[
    {
        "title": "Polyphonic Music Classification on Symbolic Data Using Dissimilarity Functions.",
        "author": [
            "Yoko Anan",
            "Kohei Hatano",
            "Hideo Bannai",
            "Masayuki Takeda",
            "Ken Satoh"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415672",
        "url": "https://doi.org/10.5281/zenodo.1415672",
        "ee": "https://zenodo.org/records/1415672/files/AnanHBTS12.pdf",
        "abstract": "This paper addresses the polyphonic music classification problem on symbolic data. A new method is proposed which converts music pieces into binary chroma vector se- quences and then classifies them by applying the dissimilarity- based classification method TWIST proposed in our previ- ous work. One advantage of using TWIST is that it works with any dissimilarity measure. Computational experiments show that the proposed method drastically outperforms SVM and k-NN, the state-of-the-art classification methods.",
        "zenodo_id": 1415672,
        "dblp_key": "conf/ismir/AnanHBTS12",
        "keywords": [
            "Polyphonic Music Classification",
            "Symbolic Data",
            "Dissimilarity Functions",
            "TWIST",
            "Chroma Vector Sequences",
            "SVM",
            "k-NN",
            "Music Information Retrieval",
            "Machine Learning Techniques",
            "Feature Vectors",
            "Preprocessing Polyphonic Music",
            "Audio Data"
        ],
        "content": "POLYPHONIC MUSIC CLASSIFICATION ON SYMBOLIC DATA USING\nDISSIMILARITY FUNCTIONS\nY\noko Anan, Kohei Hatano, Hideo Bannai, Masayuki Takeda\nDepartment of Informatics, Kyushu University\nfyoko.anan, hatano, bannai, takeda g@inf.kyushu-u.ac.jpKen Satoh\nNational Institute of Informatics\nksatoh@nii.ac.jp\nABSTRACT\nThis paper addresses the polyphonic music classiﬁcation\nproblem on symbolic data. A new method is proposed\nwhich converts music pieces into binary chroma vector se-\nquences and then classiﬁes them by applying the dissimilarity-\nbased classiﬁcation method TWIST proposed in our previ-\nous work. One advantage of using TWIST is that it works\nwith anydissimilarity measure. Computational experiments\nshow that the proposed method drastically outperforms SVM\nandk-NN, the state-of-the-art classiﬁcation methods.\n1. INTRODUCTION\nClassiﬁcation of music is one of the most fundamental prob-\nlems in music information retrieval research and has been\nstudied extensively (e.g., [4, 5, 7, 19, 20, 25, 28]). Music is\nusually polyphonic in the sense that more than one tone\nsounds simultaneously and thus a single time interval is\nmade up of two or more simultaneous tones. Classifying\npolyphonic music pieces seems to be more difﬁcult than\nclassifying monophonic music pieces.\nThe difﬁculty in classifying polyphonic music stems from\ntwo issues. The ﬁrst issue is in determining what kind of\ninformation needs to be extracted from polyphonic music.\nMany previous research (e.g., [13, 18]) reduce the classiﬁ-\ncation problem of polyphonic music to that of monophonic\nmusic by converting the data in some way. For example,\nthe so-called skyline method converts polyphonic music to\nmonophonic music by choosing the highest pitches among\nmultiple pitches. This approach is effective to an extent,\nbut it does not fully exploit the information which can be\nobtained from multiple pitches.\nThe second issue is how to classify the preprocessed\ndata. A major approach of machine learning techniques\nis to represent data as feature vectors and then applying\nlearning algorithms. There are several known features such\nas performance worm [9], performance alphabet [26] and\nothers [4, 19, 20, 28]. Then, it is non-trivial to construct ef-\nfective features from data, since such construction requires\nmuch human resource such as experts’ knowledge. If we\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.employ kernel-based machine learning approaches such as\nSVM (e.g., [15–17, 21, 29]), we can avoid the problem\nof explicitly constructing features by using kernels since\nkernels implicitly deﬁne features. However, the kernel-\nbased has a limitation that the kernel must be positive-\nsemideﬁnite. Therefore many popular (dis)similarity mea-\nsures such as the edit distance is not applicable since they\nare not proved to be positive-semideﬁnite.\nIn this paper, we propose a new method for classify-\ning polyphonic music, which is a combination of poly-\nphonic music preprocessing and classiﬁcation techniques.\nFor preprocessing data, our method employs chroma vec-\ntor representation which is popular for audio data (e.g., [3]).\nUnlike previous approaches such as the skyline method,\nwe preprocess the data so that information contained in\nthe original data is kept as much as possible. An advan-\ntage of the chosen approach is that it captures concurrent\nbehavior of pitches by encoding them into a new set of\nstrings, and therefore, can extract more information from\npolyphonic music data than the monophonic music reduc-\ntion approach.\nFor classiﬁcation, we propose a multi-class version of\nour classiﬁcation method named TWIST (Tug-of-War be-\ntween Instances by Soft margin optimization Technique)\nproposed in [2]. TWIST is based on the theory of Wang et\nal. [31] for learning with (dis)similarity functions. The the-\nory guarantees that under some mild assumptions, the ﬁnal\nclassiﬁer constructed from dissimilarity functions is accu-\nrate enough for future data. Further, TWIST can use any\ndissimilarity function which might not be positive semi-\ndeﬁnite.\nBy combining the two approaches described above, we\nsigniﬁcantly outperform the state-of-the-art methods such\nask-nearest neighbor ( k-NN) and SVM with string kernels\nfor composer classiﬁcation tasks of classical piano music\nand Japanese POP music given in MIDI format.\n2. OUTLINE OF OUR METHOD\nIn this section, we explain the outline of our method. Our\nmethod consists of two parts: (i) First of all, we convert\npolyphonic music data into binary chroma sequences. In\nour experiments, the original polyphonic data is given as\nMIDI data. (ii) Next, given labeled binary chroma se-\nquences and a dissimilarity function which measures the\ndiscrepancy between them, we use a multi-class version of\nTWIST to learn a classiﬁer. The details are given in Sec-tion 3.\n3. QUANTIFYING DISSIMILARITY BETWEEN\nMUSIC PIECES\nThe key to successfully\nusing TWIST is in choosing how\nto quantify dissimilarity between music pieces. We con-\nsider dissimilarity measures dthat are combinations of a\npreprocessing pof music pieces into particular sequential\nrepresentation, and a string dissimilarity measure \u000e. That\nis,dis given by d(x; y ) = \u000e(p(x); p(y )), where x; yare\nmusic pieces.\nOne popular sequential representation of polyphonic mu-\nsic is the chroma vector sequence representation. A chroma\nvector is a twelve-element vector with each dimension rep-\nresenting the intensity in a very short time interval, as-\nsociated with a particular semitone regardless of octave.\nChroma vectors model important aspect of music audio\nand have been widely used in music retrieval [22,23], mu-\nsic classiﬁcation [1, 10], and several other applications in\nmusic information processing [24].\nOn the other hand, many string (dis)similarity measures\nhave been proposed, such as the edit distance [30], the\nlongest common subsequence (LCS) length [14], the nor-\nmalized compression distance (NCD) [6], which are used\nin many applications such as automatic spelling correc-\ntion, information retrieval, gene information analysis and\nso on. String kernels such as the n-gram kernel [16], the\nmismatch kernel [15], the subsequence kernel [21] are also\nstring similarity measures.\nConsider applying such a string similarity measure to\nmusic pieces that are given in the form of sequences of\nbinary chroma vectors (or pitch sets). A naive approach\nwould be to use the so-called skyline method, where the\nhighest pitch is chosen among multiple pitches in each time\ninterval. It is essentially a reduction to monophonic music\nprocessing.\nAnother approach is a direct computation of dissimilar-\nity regarding the binary chroma vectors as just symbols.\nThere are 212= 4096 symbols. For the edit distance, we\nhave to deﬁne the weights associated with edit operations,\nnamely, the weight w(a; \") of deleting aand the weight\nw(\"; a) of inserting afor any symbol a, and the weight\nw(a; b)of replacing awith bfor any distinct symbol pair\n(a; b). The simplest way is to use the unit weight func-\ntion such that w(a; \") = w(\"; a) = 1 for any symbol a\nandw(a; b) = 1 for any distinct symbol pair (a; b)1. An-\nother possible way would be to set w(a; b) = 1 \u0000\u0012(a; b)\nwhere \u0012(a; b)is the angle between the vectors aandbfor\nany distinct symbol pair (a; b), and w(a; \") =w(\"; a) = 1\nfor any symbol a. An alternative way is to quantify resem-\nblance between chroma vectors based on musical knowl-\nedge. Harte et al. [12] proposed such a method: It converts\n12-dim. binary chroma vectors into 6-dim. real-valued vec-\ntors, called the tonal centroid vectors (TC vectors in short).\nThey claim in [12] that close harmonic relations such as\nﬁfths and thirds appear as small Euclidian distances. Thus\n1The edit distance with this weight function is often called the Lev\nen-\nshtein distance.the Euclidian distance of two TC vectors or the cosine of\nthe angle between them could be a good dissimilarity (sim-\nilarity) measure between original chroma vectors.\nUsing TC vector conversion Ahonen et al. [1] took an-\nother approach. The component values of TC vectors are\nquantized to 0 or 1 to produce 6-dim. binary vectors, which\nwe call the binary TC vectors. The resulting sequences are\nthus strings over an alphabet of size 64, and NCD with\nbzip2/PPMZ is used to quantify their dissimilarity.\nIn Section 5 we compare by computational experiments\nthe performance of all combinations of sequential repre-\nsentation and (dis)similarity measure mentioned above.\n4. CLASSIFICATION METHOD\nIn this section, we brieﬂy sketch TWIST [2] which is de-\nsigned for binary classiﬁcation. Then we show how to ex-\ntend TWIST for multi-class classiﬁcation tasks.\n4.1 binary classiﬁcation\nTWIST employs a dissimilarity-based learning framework\nof [31], which we call TW (Tug-of-War). We ﬁrst explain\nthe TW framework below.\nLetXbe the instance space. We call a pair (x; y )of\ninstance x2Xand label y2 f\u00001; 1ganexample . Sup-\npose that we are given ppositive examples (x+\n1;+1): : : ;\n(x+\np;+1)andnnegative examples (x\u0000\n1;\u00001); : : : ; (x\u0000\nn;\u00001).\nWe are also given a dissimilarity function d(x; x0)is a\nfunction from X\u0002XtoR+.\nFor each pair of positive instance x+\niand negative in-\nstance x\u0000\nj(i= 1; : : : ; p ,j= 1; : : : ; n ), we deﬁne the base\nclassiﬁer hi;j:X! f\u00001; +1g as follows:\nhij(x) = sgn( d(x\u0000\nj; x)\u0000d(x+\ni; x));\nwhere sgn(a) = 1 ifa > 0and\u00001otherwise. The base\nclassiﬁer hijclassiﬁes an instance xas positive if xis more\ndissimilar to the negative instance x\u0000\njthan the positive in-\nstance x+\ni(in other words, xis more similar to x+\nithan\nx\u0000\nj) and it classiﬁes an instance xas negative, otherwise.\nThe behavior of the base classiﬁer seems like a tug-of-war,\nwhich is why we call the framework TW. In the TW frame-\nwork, the ﬁnal classiﬁer is a weighted voting of base clas-\nsiﬁers,\nsgn[p∑\ni=1n∑\nj=1wijhij(x)]\nfor some weights wijs. TW has a theoretical guarantee\nthat, under some natural assumptions, there exist weights\nsuch that the associated ﬁnal classiﬁer is accurate enough\nfor future instances. A heuristics is used to determine weights\nin the original paper [31].\nOur previous work [2] uses a more robust method for\nﬁnding weights than the above mentioned heuristics. We\nnamed the method of [2] TWIST, which is an abbreviation\nof “Tug-of-War of Instances by Soft margin optimization\nTechnique”. TWIST employs the 1-norm soft margin opti-\nmization to determine weights wijs. The 1-norm soft mar-\ngin optimization is a standard formulation of classiﬁcationproblems in Machine Learning (see, e.g., [8, 32]), which\nis known to pro vide a robust classiﬁer. In our case, the\n1-norm soft margin optimization is formulated as follows:\nmax\n\u001a;b;w ;\u0018+;\u0018\u0000\u001a\u00001\n\u0017p∑\nk=1\u0018+\nk\u00001\n\u0017n∑\nk=1\u0018\u0000\nk(1)\nsub:to\np∑\ni=1n∑\nj=1wijhij(x+\nk) +b)\u0015\u001a\u0000\u0018+\nk(k= 1; : : : ; p );\n\u0000p∑\ni=1n∑\nj=1wijhij(x\u0000\nk) +b)\u0015\u001a\u0000\u0018\u0000\nk(\nk= 1; : : : ; n );\nw\u00150;p∑\ni=1n∑\nj=1wij= 1;\u0018+;\u0018\u0000\u00150;\nwhere each ykis+1or\u00001. An additional advantage of 1-\nnorm soft margin optimization is that the resulting weights\nare likely to be sparse since we regularize 1-norm of the\nweights This property is also useful for feature selection\ntasks.\n4.2 multi-class classiﬁcation\nWe explain how to extend TWIST for multi-class classi-\nﬁcation. We employ the standard reduction method from\nmulti-class to binary classiﬁcation, one-versus-rest. The\none-versus-rest method solves K-class classiﬁcation by re-\nducing it to Kbinary classiﬁcation problems. For each\nclass k(1\u0014k\u0014K), the associated binary classiﬁcation\nproblem is constructed by assuming the label kis positive\nand other labels are negative. Then a learning algorithm is\napplied for each binary classiﬁcation problem and it out-\nputs the classiﬁer hk:X!Rfor each class k. The ﬁnal\nclassiﬁer of the one-versus-rest method is given as\narg max\nk=1;:::;Khk(x):\n5. COMPUTATIONAL EXPERIMENT\nWe evaluated the performance of TWIST in composer clas-\nsiﬁcation of music pieces in comparison to those of the\nclassiﬁcation methods k-NN and SVM. A suitable dataset\nshould contain enough number of music pieces for each\ncomposer, and the pieces for all composers have roughly\nthe same conditions (the length, genre, instrument, etc. of\nthe piece). Although various MIDI datasets are publicly\navailable, datasets suitable for composer classiﬁcation are\nrare. For example, the classical music dataset of the RWC\nMusic Database2consists of 50 pieces written by 24 com-\nposers, only 2.08 pieces for each composer on the average.\nThe following datasets were available for our experi-\nments:\nClassical. The set of classical music MIDI ﬁles described\nin [27]. It consists of 5 sets of 25 pieces of keyboard\n2http://staff.aist.go.jp/m.goto/RWC-MDB/music, written by Bach, Beethoven, Chopin, Mozart\nand Schumann, respecti vely.\nJPOP. A set of Japanese POP (JPOP) music MIDI ﬁles for\nKARAOKE downloaded from a commercial site by\nYAMAHA. It consists of 5 sets of 25 pieces of JPOP,\nwritten by 5 composers (Tomoyasu Hotei, Tetsuya\nKomuro, Keisuke Kuwata, Takahiro Matsumoto, Kazu-\nmasa Oda).\nFrom the MIDI ﬁles, we removed the MIDI events other\nthan the NOTE ON/OFF events and quantized the NOTE\nON/OFF times with unit time corresponding to the six-\nteenth note length. The tracks/channels of the MIDI ﬁles\ncan then be viewed as sequences of sets of pitches that are\n“ON” in respective unit time intervals. Each MIDI ﬁle in\nClassical consists of two tracks, corresponding to the left\nand right hand parts. We extracted two pitch-set sequences\nand merged them into a single sequence. Each MIDI ﬁle\ninJPOP consists of a single track with several channels.\nWe chose the channel 0 corresponding to main melody part\nand obtained a single pitch-set sequence from the channel.\nWe then converted the obtained pitch-set sequences into (a)\nhighest-pitch strings, (b) binary chroma vector sequences,\nand (c) binary TC vector sequences.\nFor quantifying dissimilarities between sequences, we\nadopted several (dis)similarity measures between strings.\nFor TWIST, SVM and k-NN, we used the following two\nstring kernels: n-gram kernel with parameters n= 2; 5;10\nand mismatch kernel with parameters n= 5; 10andm=\n1;2, where mis the maximum number of errors allowed.\nFor TWIST and k-NN, we also used the following (dis)simi-\nlarity measures: edit distance, LCS, and NCD with com-\npression programs gzip andbzip2.\nFor the edit distance between binary chroma vector se-\nquences, we used the symbol-pair weight functions wof\nthe three types: (i) the unit weight (w (a; b) = 1 ifa6=b\nandw(a; b) = 0 ifa=b); (ii) w(a; b) = 1\u0000cos\u0012(a; b)\nfor binary chroma vectors a; b; and (iii) w(a; b) = 1 \u0000\ncos\u0012(a0; b0)for TC vectors a0; b0of binary chroma vec-\ntorsa; b. For highest-pitch strings and binary TC vectors,\nwe used only the unit weight. We used the cosine val-\nues for (ii) and (iii) in the case of LCS. We note that the\ncompression programs gzip andbzip2 used in NCD\ntake data ﬁles of byte-sequences as input. We encoded the\nhighest-pitch strings as one-byte-integer sequences, wrote\nthem into data ﬁles and then applied the compressors to the\nﬁles. For binary chroma vector sequences, we wrote them\nas data ﬁles of two-byte-integer sequences to be processed\nin a byte-wise manner by the compressors.\nWe evaluated the three classiﬁcation methods by per-\nforming 5-fold cross validation. We used the values 0:05,\n0:1;0:2;0:3;0:4;0:5for the parameter \u0017of the 1-norm soft\nmargin optimization formulation (1). For SVM, we used\nthe\u0017-SVM implementation of LIBSVM (version 3.11) [11].\nThe values 0:05, 0:1,0:2,0:3,0:4,0:5for the parameter \u0017\nwere used. For k-NN, we used k= 1; 3;5. Accuracies\nare obtained using the best value of \u0017for each method and\neach (dis) similarity measure.Table 1. Comparison of classiﬁcation accuracy for dataset Classical (in %).\n(a) highest-pitch\nstrings.\nedit distance LCS NCD n-gram kernel mismatch kernel\nunit weight unit weight bzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nm= 1m= 2m= 1m= 2\nTWIST 70.40 78.40 80.80 84.80 76.00 77.60 72.80 72.80 81.60 79.20 75.20\n1-NN 52.80 20.00 51.20 41.60 16.00 20.00 12.00 19.20 18.40 12.00 13.60\n3-NN 60.00 18.40 62.40 48.00 17.60 41.60 44.80 28.00 19.20 46.40 39.20\n5-NN 51.20 24.80 56.00 39.20 33.60 27.20 32.00 36.00 34.40 31.20 30.40\nSVM N/A N/A N/A N/A 51.20 44.00 24.80 52.00 52.80 26.40 29.60\n(b) binary chroma vector sequences.\nedit distance LCS NCD n-gram kernel mismatch kernel\nweight weightbzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nunit cosine TC unit cosine TC m= 1m= 2m= 1m= 2\nTWIST 80.00 70.40 72.00 81.60 77.60 77.60 91.20 92.00 80.00 70.40 49.60 74.40 73.60 61.60 67.20\n1-NN 46.40 50.40 44.80 29.60 30.40 27.20 63.20 53.60 20.00 22.40 24.00 20.80 21.60 18.40 17.60\n3-NN 35.20 43.20 38.40 25.60 28.80 24.00 68.00 63.20 16.80 4.80 8.00 10.40 16.80 4.80 4.80\n5-NN 25.60 40.00 37.60 23.20 23.20 21.60 65.60 56.00 25.60 1.60 9.60 3.20 11.20 4.80 4.00\nSVM N/A N/A N/A N/A N/A N/A N/A N/A 57.60 40.00 24.80 44.80 51.20 24.00 23.20\n(c) binary TC vector sequences.\nedit distance LCS NCD n-gram kernel mismatch kernel\nunit weight unit weight bzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nm= 1m= 2m= 1m= 2\nTWIST 76.00 78.40 85.60 88.80 78.40 70.40 59.20 72.00 73.60 64.80 67.20\n1-NN 42.40 28.00 56.00 59.20 20.00 20.00 17.60 19.20 19.20 16.00 17.60\n3-NN 32.00 25.60 64.80 65.60 18.40 18.40 17.60 18.40 18.40 15.20 16.00\n5-NN 28.00 25.60 57.60 52.00 22.40 1.60 6.40 3.20 12.00 1.60 0.80\nSVM N/A N/A N/A N/A 58.40 46.40 25.60 52.00 54.40 25.60 29.60\nThe experimental results for Classical are summarized\nin Table 1. TWIST drastically outperforms the\nother clas-\nsiﬁcation methods in all combinations of sequential repre-\nsentation and (dis)similarity measure. TWIST shows the\nbest accuracy when used with combination of the binary\nchroma vector sequence representation and NCD with gzip.\nThe experimental results for JPOP are summarized in\nTable 2. Again, TWIST defeats the other classiﬁcation\nmethods in most combinations of sequential representa-\ntions and (dis)similarity measures. This time TWIST us-\ning NCD with bzip2 shows good accuracies. In fact the\nbest and the second best are achieved by NCD with bzip2\ncombined with the highest-pitch strings and with the bi-\nnary chroma vector sequence representations, respectively.\nNow we discuss the effects of preprocessing for clas-\nsiﬁcation of JPOP andClassical. For JPOP , the classi-\nﬁcation accuracy with highest-pitch strings is better than\nthat with chroma vector sequences. This might be due\nto the fact that JPOP is almost like monophonic music.\nMore precisely, each music of JPOP is characterized with\nhighest-pitch sequence mostly corresponding to the lead\nvocal line. On the other hand, each piano music of Clas-\nsical is characterized with a succession of simultaneously\nsounding pitches. Therefore, chroma vector representationis more advantageous for classiﬁcation of polyphonic mu-\nsic since the representation keeps more information in the\noriginal music.\n6. CONCLUSION\nIn this paper we proposed a polyphonic music classiﬁca-\ntion method as a combination of way of quantifying afﬁn-\nity between music pieces and the classiﬁcation technique\nTWIST [2]. The method converts given music data into\nbinary chroma vector sequences, and builds a classiﬁer\nbased on the similarity values between the sequences (or\ntheir converted sequences) using a string similarity mea-\nsure. One advantage is that TWIST works with anysimi-\nlarity measure, not necessarily to be positive semideﬁnite.\nThe results of computational experiments with classical\nmusic and Japanese POP music show that TWIST dras-\ntically outperforms the well-known classiﬁcation methods\nk-NN and SVM with string kernels in all combinations of\nsequential representation and similarity measure.\nAlthough the computational experiments were carried\nout on MIDI ﬁles, our classiﬁcation method can, in the-\nory, be applied to audio ﬁles, provided that an appropriate\nfunction that quantiﬁes afﬁnity between music audio dataTable 2. Comparison of classiﬁcation accuracy for dataset JPOP (in %).\n(a) highest-pitch\nstrings.\nedit distance LCS NCD n-gram kernel mismatch kernel\nunit weight unit weight bzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nm= 1m= 2m= 1m= 2\nTWIST 78.40 80.80 86.40 73.60 38.40 31.20 39.20 28.80 48.80 26.40 28.00\n1-NN 26.40 21.60 33.60 27.20 19.20 19.20 19.20 19.20 19.20 19.20 19.20\n3-NN 37.60 45.60 58.40 44.00 58.40 58.40 20.00 20.00 58.40 58.40 20.00\n5-NN 34.40 42.40 43.20 40.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00\nSVM N/A N/A N/A N/A 35.20 25.60 17.60 25.60 36.00 17.60 19.20\n(b) binary chroma vector sequences.\nedit distance LCS NCD n-gram kernel mismatch kernel\nweight weightbzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nunit cosine TC unit cosine TC m= 1m= 2m= 1m= 2\nTWIST 80.00 83.20 83.20 76.80 81.60 81.60 84.80 79.20 60.80 48.80 28.80 44.00 50.40 36.00 34.40\n1-NN 31.20 30.40 34.40 27.20 27.20 30.40 35.20 30.40 19.20 19.20 19.20 19.20 19.20 19.20 19.20\n3-NN 44.80 44.80 49.60 53.60 53.60 55.20 51.20 44.80 57.60 56.80 58.40 58.40 49.60 57.60 58.40\n5-NN 41.60 41.60 46.40 46.40 46.40 32.80 48.80 40.00 19.20 20.00 20.00 20.00 27.20 20.00 20.00\nSVM N/A N/A N/A N/A N/A N/A N/A N/A 40.80 40.00 32.00 40.80 40.80 33.60 36.00\n(c) binary TC vector sequences.\nedit distance LCS NCD n-gram kernel mismatch kernel\nunit weight unit weight bzip2 gzip n= 2n= 5n= 10n= 5 n= 10\nm= 1m= 2m= 1m= 2\nTWIST 72.80 83.20 79.20 80.80 56.00 38.40 40.00 45.60 42.40 28.80 37.60\n1-NN 28.80 28.00 31.20 30.40 19.20 19.20 19.20 19.20 19.20 19.20 19.20\n3-NN 44.00 55.20 48.80 55.20 57.60 56.00 58.40 58.40 42.40 57.60 58.40\n5-NN 44.00 48.00 45.60 40.00 20.00 21.60 20.00 20.00 27.20 20.00 20.00\nSVM N/A N/A N/A N/A 38.40 39.20 32.00 41.60 43.20 33.60 36.00\nis available. A future work is to develop\nsuch a function\nfor music audio data.\n7. ACKNOWLEDGMENTS\nWe thank anonymous referees for helpful comments and\nsuggestions.\n8. REFERENCES\n[1] Teppo E. Ahonen, Kjell Lemstr ¨om, and Simo Linkola.\nCompression-based similarity measures in symbolic,\npolyphonic music. In Proceedings of the 12th Interna-\ntional Symposium on Music Information Retrieval (IS-\nMIR’11), pages 91–96, 2011.\n[2] Yoko Anan, Kohei Hatano, Hideo Bannai, and\nMasayuki Takeda. Music genre classiﬁcation using\nsimilarity functions. In Proceedings of the 12th Inter-\nnational Symposium on Music Information Retrieval\n(ISMIR’11) , pages 693–698, 2011.\n[3] Mark A. Bartsch and Gregory H. Wakeﬁeld. Audio\nthumbnailing of popular music using chroma-basedrepresentations. IEEE Transactions on Multimedia,\n7(1):96–104, 2005.\n[4] James Bergstra, Norman Casagrande, Dumitru Erhan,\nDouglas Eck, and Bal ´azs K ´egl. Aggregate features and\nAdaBoost for music classiﬁcation. Machine Learning,\n65:473–484, 2006.\n[5] Rudi Cilibrasi, Paul Vit ´anyi, and Ronald de Wolf. Al-\ngorithmic clustering of music based on string compres-\nsion. Computer Music Journal , 28(4):49–67, 2004.\n[6] Rudi Cilibrasi and Paul M. B. Vit ´anyi. Clustering by\ncompression. IEEE Transactions on Information The-\nory, 51:1523–1545, 2005.\n[7] Christopher DeCoro, Zafer Barutcuoglu, and Re-\nbecca Fiebrink. Bayesian aggregation for hierarchical\ngenre classiﬁcation. In Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’07) , pages 77–80, 2007.\n[8] Ayhan Demiriz, Kristin P. Bennett, and John Shawe-\nTaylor. Linear programming boosting via column gen-\neration. Machine Learning , 46(1-3):225–254, 2002.[9] Simon Dixon, Werner Goebl, and Gerhard Widmer.\nThe performance worm: Real time visualisation of ex-\npression based on langner’s tempo-loudness animation.\nInProceedings of the International Computer Music\nConference (ICMC’02), pages 361–364, 2002.\n[10] Daniel P. W. Ellis. Classifying music audio with tim-\nbral and chroma features. In Proceedings of the 8th\nInternational Conference on Music Information Re-\ntrieval (ISMIR ’07) , pages 339–340, 2007.\n[11] Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.\nWorking set selection using second order information\nfor training support vector machines. Journal of Ma-\nchine Learning Research , 6:1889–1918, 2005.\n[12] Christopher Harte, Mark Sandler, and Martin Gasser.\nDetecting harmonic change in musical audio. In Pro-\nceedings of 1st ACM Workshop on Audio and Music\nComputing Multimedia (AMCMM’06), pages 21–26,\n2006.\n[13] Ruben Hillewaere, Bernard Manderick, and Darrell\nConklin. String quartet classiﬁcation with monophonic\nmodels. In Proceedings of the 11th International Soci-\nety for Music Information Retrieval (ISMIR ’10), pages\n537–542, 2010.\n[14] D.S. Hirschberg. A linear space algorithm for comput-\ning maximal common subsequences. Communications\nof the ACM , 18(6):341–343, 1975.\n[15] Christina S. Leslie, Eleazar Eskin, Adiel Cohen, Jason\nWeston, and William Stafford Noble. Mismatch string\nkernels for discriminative protein classiﬁcation. Bioin-\nformatics , 20(4):467–476, 2004.\n[16] Christina S. Leslie, Eleazar Eskin, and\nWilliam Stafford Noble. The spectrum kernel: a\nstring kernel for SVM protein classiﬁcation. In Pro-\nceedings of the Paciﬁc Symposium on Biocomputing\n(PSB’02), pages 566–575, 2002.\n[17] Christina S. Leslie and Rui Kang. Fast string kernels\nusing inexact matching for protein sequences. Journal\nof Machine Learning Research , 5:1435–1455, 2004.\n[18] Ming Li and Ronan Sleep. Melody classiﬁcation using\na similarity metric based on Kolmogorov complexity.\nInSound and Music Computing, pages 126–129, 2004.\n[19] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiﬁcation. In Proceedings of the 6th\nInternational Conference on Music Information Re-\ntrieval (ISMIR’05) , pages 34–41, 2005.\n[20] Thomas Lidy, Andreas Rauber, Antonio Pertusa, and\nJos´e Manuel I ˜nesta. Improving genre classiﬁcation by\ncombination of audio and symbolic descriptors using\na transcription system. In Proceedings of 8th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’07), pages 61–66, 2007.[21] Huma Lodhi, Craig Saunders, John Shawe-Taylor,\nNello Cristianini, Chris Watkins, and Bernhard\nScholkopf. Text classiﬁcation using string kernels.\nJournal of Machine Learning Research , 2:563–569,\n2002.\n[22] Riccardo Miotto and Nicola Orio. A music identiﬁca-\ntion system based on chroma indexing and statistical\nmodeling. In Proceedings of the 9th International Soci-\nety for Music Information Retrieval (ISMIR ’08), pages\n301–306, 2008.\n[23] Meinard M ¨uller, Frank Kurth, and Michael Clausen.\nAudio matching via chroma-based statistical features.\nInProceedings of the 6th International Conference on\nMusic Information Retrieval (ISMIR ’05), pages 288–\n295, 2005.\n[24] Laurent Oudre, Yves Grenier, and C ´edric F ´evotte.\nChord recognition by ﬁtting rescaled chroma vectors to\nchord templates. IEEE Transactions on Audio, Speech\nand Language Processing , 19(7):2222 – 2233, 2011.\n[25] Carlos P ´erez-Sancho, David Rizo, and Jos ´e\nManuel I ˜nesta Quereda. Genre classiﬁcation using\nchords and stochastic language models. Connection\nScience, 21:145–159, 2009.\n[26] Craig Saunders, David R. Hardoon, John Shawe-taylor,\nand Gerhard Widmer. Using string kernels to identify\nfamous performers from their playing style. In Pro-\nceedings of the 15th European Conference on Machine\nLearning (ECML’04) , pages 384–395, 2004.\n[27] Takuya Sawada and Ken Satoh. Composer classiﬁca-\ntion based on patterns of short note sequences. In Pro-\nceedings of the AAAI-2000 Workshop on AI and Music,\npages 24–27, 2000.\n[28] George Tzanetakis, Georg Essl, and Perry Cook. Au-\ntomatic musical genre classiﬁcation of audio signals.\nInProceedings of the 2nd International Symposium on\nMusic Information Retrieval (ISMIR’01), pages 293–\n302, 2001.\n[29] S. V . N. Vishwanathan and Alexander J. Smola. Fast\nkernels for string and tree matching. In Advances on\nNeural Information Processing Systems 15, pages 569–\n576, 2002.\n[30] Robert A. Wagner and Michael J. Fischer. The string-\nto-string correction problem. Journal of the ACM ,\n21(1):168–173, 1974.\n[31] Liwei Wang, Masashi Sugiyama, Cheng Yang, Kohei\nHatano, and Jufu Feng. Theory and algorithm for learn-\ning with dissimilarity functions. Neural Computation,\n21(5):1459–1484, 2009.\n[32] Manfred K. Warmuth, Karen Glocer, and Gunnar\nR¨atsch. Boosting algorithms for maximizing the soft\nmargin. In Advances in Neural Information Processing\nSystems 20 (NIPS’08), pages 1585–1592, 2008."
    },
    {
        "title": "Fast Identification of Piece and Score Position via Symbolic Fingerprinting.",
        "author": [
            "Andreas Arzt",
            "Sebastian Böck",
            "Gerhard Widmer"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417022",
        "url": "https://doi.org/10.5281/zenodo.1417022",
        "ee": "https://zenodo.org/records/1417022/files/ArztBW12.pdf",
        "abstract": "In this paper we present a novel algorithm that, given a short snippet of an audio performance (piano music, for the time being), identifies the piece and the score position. In- stead of using audio matching methods we propose a com- bination of a state-of-the-art music transcription algorithm and a new symbolic fingerprinting method. The resulting system is usable in both on-line and off-line scenarios and thus may be of use in many application areas. As the eval- uation shows the system operates with only minimal lag and achieves high precision even with very short queries.",
        "zenodo_id": 1417022,
        "dblp_key": "conf/ismir/ArztBW12",
        "keywords": [
            "novel algorithm",
            "music transcription",
            "symbolic fingerprinting",
            "state-of-the-art",
            "combination",
            "on-line scenarios",
            "off-line scenarios",
            "application areas",
            "minimal lag",
            "high precision"
        ],
        "content": "FAST IDENTIFICATION OF PIECE AND SCORE POSITION\nVIA SYMBOLIC FINGERPRINTING\nAndreas Arzt1, Sebastian B ¨ock1, Gerhard Widmer1;2\n1Department of Computational Perception, Johannes Kepler University, Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\nandreas.arzt@jku.at, sebastian.boeck@jku.at, gerhard.widmer@jku.at\nABSTRACT\nIn this paper we present a novel algorithm that, given a\nshort snippet of an audio performance (piano music, for the\ntime being), identiﬁes the piece and the score position. In-\nstead of using audio matching methods we propose a com-\nbination of a state-of-the-art music transcription algorithm\nand a new symbolic ﬁngerprinting method. The resulting\nsystem is usable in both on-line and off-line scenarios and\nthus may be of use in many application areas. As the eval-\nuation shows the system operates with only minimal lag\nand achieves high precision even with very short queries.\n1. INTRODUCTION\nOver the last few years efﬁcient systems for content-based\naudio retrieval have been a major topic in music infor-\nmation retrieval research. These systems allow the user\nto browse and explore large music collections without the\nneed for meta-data and other external information sources.\nIn this context methods to automatically retrieve all pieces\n(and/or all the excerpts of the pieces) matching a given ex-\nample query (in the form of a short audio clip) play an\nimportant role (and actually are in everyday commercial\nuse). This task, most commonly called audio identiﬁca-\ntion oraudio ﬁngerprinting, can be considered as solved\n(see e.g., [5, 10]).\nAudio identiﬁcation by deﬁnition only ﬁnds exact repli-\ncas of the query in the database, possibly distorted in some\nways (e.g., compression artefacts, noise). Especially for\nclassical music, this is not sufﬁcient, because there are gen-\nerally large numbers of different performances of the same\npiece, all different in terms of tempo, expressive timing,\nand other performance aspects. The relationship between\nthese performances (that they derive from a common mu-\nsical score) in general goes unnoticed by an audio identi-\nﬁcation algorithm. In this paper we propose a method for\nscore identiﬁcation: instead of identifying a particular per-\nformance it returns the musical score on which the query\nsnippet is based. For example, if we present an audio ex-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.cerpt of Vladimir Horowitz playing Chopin’s Nocturne Op.\n55 No. 1 to the system, it will return the name and data of\nthe piece (Nocturne Op. 55 No. 1 by Chopin) rather than\nthe data of the speciﬁc performance. Moreover, the sys-\ntem we propose returns not only the corresponding score,\nbut also the exact position within the score. Accordingly,\nthe database for this task does not contain audio record-\nings, but symbolic representations of musical scores (i.e.,\nto identify the piece being played, the system only uses the\nsymbolic score and has no information about the speciﬁc\nperformance by Horowitz in the database).\nThis task is related to cover song identiﬁcation (see [9]\nfor an overview), where the goal is to identify different\nversions of one and the same song, in order to detect cover\nsongs in popular music for commercial applications. To\nperform this task algorithms have to cope with large vari-\nations in parameters like timbre, tempo, timing and struc-\nture between different performances. Score identiﬁcation\ncan be seen as a special case of cover song identiﬁcation.\nA MIDI version of the score of a classical piece of music\ncan be synthesized and then be treated as a “normal” per-\nformance in this task, i.e., performances become “cover\nsongs” of the synthesized version of the score. Still, the\ndifference to our approach to score identiﬁcation is that for\nour system very short queries (e.g., 5 seconds) are sufﬁ-\ncient, while cover song identiﬁcation algorithms generally\nassign similarity values to whole pieces of music.\nAs an alternative to our symbolic approach audio match-\ningcan be considered (see e.g., [7]). In this case the score\nis again ﬁrst transformed into an audio ﬁle (or a suitable\nin-between representation). Then an audio matching al-\ngorithm, most commonly based on dynamic programming\ntechniques, retrieves all excerpts from a database which\nmusically correspond to a short query clip. In contrast\nto audio ﬁngerprinting methods audio matching can also\ncope with (non-linear) timing deviations. The downside\nof audio matching is that in general these methods are very\nslow compared to ﬁngerprinting methods. To cope with the\ncomputational costs, [6] presented clever indexing strate-\ngies that greatly reduce the computation time. Still, due\nto the coarse feature resolution, relatively large query sizes\nare needed.\nAnother related task, especially regarding the on-line\ncapabilities of the proposed algorithm, is score following\n(see e.g., [3,8] for state-of-the-art score following systems).\nIn contrast to the algorithm presented in this paper, a scorefollower needs to know a-priori which piece the perform-\ners are playing, and then tracks the on-going performance\nand continuously returns the current score position. To do\nso it relies both on access to the complete performance (up\nto the current point in time) and on the performers closely\nfollowing the score (i.e., without any additional repeats or\nany jumps). It contrast to this, the algorithm presented in\nthis paper is able to identify the piece being played, and to\nidentify the actual (or at least an identical) score position\nfrom only a small, arbitrary snippet of the performance.\nIn the following we will describe a new symbolic ap-\nproach to score identiﬁcation. Instead of creating an audio\nrepresentation of the score we create the database directly\nfrom the symbolic score information. Then we transform\nthe audio query into a symbolic representation – a list of\nnote onset times with their respective pitch – and use a\nsymbolic ﬁngerprinting algorithm, inspired by the algo-\nrithm described in [10], to ﬁnd matching positions in the\nscore database. This process is very fast, can be used in\nreal-time, on-line applications, and yields very high preci-\nsion (as can be seen in Section 4). Note that our algorithm\ninvolves music (audio) transcription – which is still basi-\ncally an unsolved problem – as a query preprocessing step.\nFor our system we use a state-of-the-art music transcrip-\ntion system for piano music which, despite many errors,\nprovides us with transcriptions of sufﬁcient quality for the\nrobust symbolic ﬁngerprinting algorithm. Still, this also\nmeans that our system currently only works for piano mu-\nsic (because of speciﬁc properties of the transcription sys-\ntem).\n2. BUILDING THE SCORE DATABASE\nBefore actually processing queries the score database has\nto be built. In our system we use deadpan MIDI ﬁles as the\nbasis for the score database. The duration of these MIDI\nﬁles is similar to the duration of a ‘typical’ performance\nof the respective piece, but without encoded timing varia-\ntions. From these ﬁles a simple ordered list of note events\nis extracted where for each note event the exact time in\nseconds and the pitch as MIDI note number is stored.\nNext, for each piece ﬁngerprint tokens are generated. In\ncontrast to [10] we create them from 3 successive events\naccording to some constraints (also see Figure 1) to make\nthem tempo independent. Given a ﬁxed event ewe pair it\nwith the ﬁrst n1events with a distance of at least dseconds\n“in the future” of e. This results in n1event pairs. For\neach of these pairs we then repeat this step and again pair\nthem with the n2future events with a distance of at least\ndseconds. This ﬁnally results in n1\u0003n2event triplets. In\nour experiments we used the values d= 0:05 seconds and\nn1=n2= 5. Also inspired by [10] we further constrain\nthe pair creation steps to notes which are at most 2 octaves\napart.\nGiven such a triplet consisting of the events e1,e2and\ne3the time difference td1;2between e1ande2and the\ntime difference td2;3between e2ande3are computed. To\nget a tempo independent ﬁngerprint token we compute the\ntime difference ratio of the time differences: tdr=td2;3\ntd1;2.\nPitch\nTime\n>d>dFigure 1. Fingerprint Token Generation: Example of 1\ngenerated Token\nThis ﬁnally leads to a ﬁngerprint token [pitch 1:pitch 2:\npitch 3:tdr] :pieceID :time :td1;2, where the hash key\n[pitch 1:pitch 2:pitch 3:tdr]can be stored in a 32 bit in-\nteger. The purpose of storing td1;2in the ﬁngerprint token\nwill be explained in the description of the search process\nitself (see Section 3.2 below).\nThe result of the score preprocessing is our score data-\nbase; a container of ﬁngerprint tokens which provides quick\naccess to the tokens via hash keys.\n3. QUERYING THE DATABASE\n3.1 Preprocessing: Transcribing the Query\nBefore querying the database the query (an audio snippet\nof a performance) has to be transformed into a symbolic\nrepresentation. The algorithm we use to transcribe musical\nnote onsets from an audio signal is based on the system de-\nscribed in [2], which exhibits state-of-the-art performance\nfor this task. It uses a recurrent neural network to simulta-\nneously detect the pitches and the onsets of the notes (see\nFigure 2 for an illustration of the algorithm).\nFor its input, a discretely sampled audio signal is split\ninto overlapping blocks before it is transferred to the fre-\nquency domain with two parallel Short-Time Fourier Trans-\nforms (STFT). Two different window lengths have been\nchosen to achieve both a good temporal precision and a\nsufﬁcient frequency resolution for the transcription of the\nnotes. Phase information of the resulting complex spec-\ntrogram is discarded and only the logarithm of the magni-\ntude values is used for further processing. To reduce the\ndimensionality of the input vector for the neural network,\nthe spectrogram representation is ﬁltered with a bank of\nﬁlters whose frequencies are equally spaced on a logarith-\nmic frequency scale and are aligned according to the MIDI\npitches. The attack phase of a note onset is characterized\nby a rise of energy, thus the ﬁrst order differences of the\ntwo spectrograms are used as additional inputs to the neu-\nral network.\nThe neural network consists of a linear input layer with\n324 units, three bidirectional fully connected recurrent hid-\nden layers, and a regression output layer with 88 units,\nwhich directly represent the MIDI pitches. Each of the\nhidden layers uses 88 neurons with hyperbolic tangent ac-\ntivation function. The use of bidirectional hidden layers\nenables the system to better model the context of the notes,\nwhich show a very characteristic envelope during their de-2 parallel STFT\nInput\nLayerOutput\nLayerRecurrent Neural Network\nHidden\nLayerHidden\nLayerHidden\nLayerFigure 2. The Transcription System\nDetection Window Precision Recall F-measure\n20 ms 0.586 0.489 0.533\n40 ms 0.812 0.678 0.739\n60 ms 0.851 0.710 0.774\n80 ms 0.864 0.720 0.786\n100 ms 0.869 0.725 0.790\nTable 1. Results of the On-line Transcription Algorithm,\nfor different detection window sizes.\ncay phase.\nThe network is trained with supervised learning and early\nstopping. The network weights are initialized with ran-\ndom values following a Gaussian distribution with mean 0\nand standard deviation 0.1. Standard gradient descent with\nbackpropagation of the errors is used for training. The\nnetwork was trained on a collection of 281 piano pieces\nrecorded on various pianos, virtual and real (seven dif-\nferent synthesizers, an upright Yamaha Disklavier, and a\nB¨osendorfer SE grand piano).\nTo make the transcriber applicable also in on-line sce-\nnarios, instead of preprocessing the whole piece of audio at\na time, the signal is split into blocks of 11 frames centered\naround the actual frame. The use of 11 frames is a trade-off\nbetween keeping the system’s ability to model the context\nof the notes and to keep the introduced delay at a mini-\nmum. In the current system the constant lag caused by the\nquery preprocessing amounts to about 210 ms.\nTable 1 shows the on-line transcription results for the\ncomplete test set described later on in Section 4.1. A note\nis considered to have been discovered correctly if its posi-\ntion is detected within a ‘detection window’ of given size\naround the annotated ground truth position. As can be seen\nin the table, the results are far from perfect (though they are\nvery good, considering the state of the art). If the proposed\nﬁngerprinting system is used in an off-line scenario, the\nuse of an off-line transcription algorithm is an option to\nslightly improve the results.\n3.2 Querying the Database\nThe transcription of the query results in a list of note pitches\nwith timestamps. This list is then processed in the same\nway as described in Section 2 above to produce query to-\n!\"#\"$\"%\"&\"'\"(\"\n!\" $\" &\" (\" )\" #!\" #$\" #&\" #(\" #)\" $!\"!\"#$%&'()#&(*&+#,-*./&\n+,-$#&'()#&(*&+#,-*./&\n!\"#\"$\"%\"&\"'!\"'#\"'$\"\n!\"'\"#\"(\"$\")\"%\"*\"&\"+\"'!\"''\"'#\"'(\"'$\"')\"'%\"'*\"'&\"'+\"#!\"!\"#$%&'\"(%)&\n*+\",$!-.$&/&0($,1!-.$&a)\nb)Figure 3. a) scatter plot of matching tokens and b) com-\nputed histogram for diagonal identiﬁcation\nkens. Of course in this case no piece ID is known and\nfurthermore each query starts at time 0. These query ﬁn-\ngerprint tokens are now used to query the database. The\nmethod described below is again very much inspired by\nthe audio ﬁngerprinting method proposed in [10].\nThe general idea is to ﬁnd regions in the score database\nwhich share a continuous sequence of tokens with the query.\nTo do so ﬁrst all the score tokens which match the query\ntokens are extracted from the database. When plotted as a\nscatter plot against their respective time stamps (see Figure\n3a) matches will be indicated by (rough) diagonals (i.e.,\nthese indicate that the query tokens match the score tokens\nover a period of time). As identifying these diagonals di-\nrectly would be computationally expensive we instead use\na simpler method described in [10]. This is based on his-\ntograms (one for each piece in the score database, with a\ntime resolution of 1 second) into which the matched tokens\nare sorted in a way such that peaks appear at the start points\nof these diagonals (i.e., the start point of a query, see Fig-\nure 3b). This is achieved by computing the bin to sort the\ntoken into as the difference between the time of the score\ntoken and time of the query token. The complete process\nwill be explained in more detail below.\nFor each of the query tokens qtwith[qpitch 1:qpitch 2:\nqpitch 3:qtdr] :qtime :qtd1;2the following process\nis repeated. First, matching tokens are extracted from the\nscore database via the hash key. To allow for local tempo\ndifferences we permit the normalized time difference to be\nwithin1\n4ofqtdr. This normally results in a large num-\nber of score tokens [spitch 1:spitch 2:spitch 3:stdr] :\nspieceID :stime :std1;2. Unfortunately directly sort-\ning these tokens into bin round (stime \u0000qtime )of the\nhistogram spieceID does not necessarily make sense be-\ncause of the query possibly having a different tempo thanData Description Number of Pieces Notes in Score Notes in Performance Performance Duration\nChopin Corpus 154 325,263 326,501 9:38:36\nMozart Corpus 13 42,049 42,095 1:23:56\nAdditional Pieces 16 68,358 – –\nTotal 183 435,670\nTable 2. Pieces in Database\nQuery Length in Notes\n5 10 20 30 40 50 60\nCorr. Piece as Top Match 22.55% 78.33% 94.07% 96.70% 97.50% 98.01% 98.42%\nCorr. Piece in Top 2 29.23% 83.22% 96.07% 97.67% 98.28% 98.64% 98.87%\nCorr. Piece in Top 3 33.00% 85.50% 96.74% 98.12% 98.57% 98.91% 99.09%\nCorr. Piece in Top 4 35.33% 86.88% 97.15% 98.32% 98.76% 99.09% 99.22%\nCorr. Piece in Top 5 37.24% 87.86% 97.44% 98.49% 98.87% 99.17% 99.32%\nCorr. Position as Top Match 14.41% 60.47% 80.35% 84.63% 84.86% 83.91% 83.70%\nCorr. Position in Top 2 21.94% 75.09% 91.11% 93.39% 93.77% 93.39% 93.17%\nCorr. Position in Top 3 25.77% 79.70% 93.69% 95.36% 95.73% 95.85% 95.84%\nCorr. Position in Top 4 28.20% 81.94% 94.69% 96.14% 96.61% 96.84% 96.93%\nCorr. Position in Top 5 30.02% 83.29% 95.22% 96.55% 97.05% 97.34% 97.47%\nMean Query Duration 0.60 sec 1.33 sec 2.78 sec 4.21 sec 5.63 sec 7.04 sec 8.48 sec\nMean Query Exec. Time 1.71 ms 5.13 ms 11.76 ms 16.86 ms 20.76 ms 26.36 ms 31.89 ms\nTable 3. Results of the proposed piece and score position identiﬁcation algorithm on the test database. Each estimate is\nbased on 50,000 random audio queries.\nexpected by the score.\nAs an illustration let us assume a slower tempo for the\nquery than for the respective score. Then the diagonal in\nFigure 3a would be steeper and when computing the bins\nviaround (stime \u0000qtime )the ﬁrst few tokens may fall into\nthe correct bins. But soon the tokens, despite belonging to\nthe same score position, would get sorted into lower bins\ninstead.\nThus we ﬁrst try to adapt the timing by estimating the\ntempo difference between the score token and the query\ntoken. First we compute the tempo ratio of both tokens\nr=std1;2\nqtd1;2and then adapt the time of the query event\nwhen computing the bin to sort the token into: bin=\nround (stime \u0000qtime \u0003r).\nWe now have a number of histograms, one for each\nscore in the database, and need a way of deciding on the\nmost probable score position(s) (and, by implication, the\nmost probable piece), for the query. We did experiments\nwith different methods of computing the matching score\nbut in the end simply taking the number of tokens in each\nbin as the score produced the best results.\n4. EVALUATION\n4.1 Dataset Description\nFor the evaluation of our algorithm a ground truth is needed.\nWe need exact alignments of performances of classical mu-\nsic to their respective scores such that we know exactly\nwhen each note given in the score is actually played in the\nperformance. This data can either be generated by a com-puter program or by extensive manual annotation but both\nways are prone to annotation errors.\nLuckily, we have access to two unique datasets where\nprofessional pianists played their performances on a com-\nputer-controlled piano1and thus every action (e.g., key\npresses, pedal movements) was recorded in a symbolic way.\nThe ﬁrst dataset (described in [11]) consists of performan-\nces of the ﬁrst movements of 13 Mozart piano sonatas by\nRoland Batik. The second, much larger, dataset consists\nof nearly the complete solo piano works by Chopin per-\nformed by Nikita Magaloff [4]. For the latter set we do\nnot have the original audio ﬁles and thus replayed the sym-\nbolic performance data on a Yamaha N2 hybrid piano and\nrecorded the resulting performances.\nAs we have both symbolic and audio information about\nthe performances, we know the exact timing of each played\nnote in the audio ﬁles. The performances were manually\naligned to electronic (symbolic) versions of the original\nsheet music. To build the score database we converted the\nsheet music to MIDI ﬁles with a constant tempo such that\nthe overall duration of the ﬁle is similar to a ‘normal’ per-\nformance of the piece.\nIn addition to these two datasets we added some more\nscores to the database, solely to provide for more diversity\nand to make the task even harder for our algorithm (these\ninclude, amongst others, the Beethoven Symphony No. 5,\nthe Mozart Oboe Quartet KV370, the First Mephisto Waltz\nby Liszt and Schoenberg Op. 23 No. 3). To the latter,\nwe have no ground truth, but this is irrelevant since we\n1B¨osendorfer SE 290Possible Applications\nLive Performance\n'Any-time' On-line Music T racker\nInstant Piece Recognizer\nNote Recognizer\n(On-line Audio-to-Pitch \nTranscriptor)\nSymbolic Music \nMatcher\n(Fingerprinter)\nMulti Agent Music Tracking System\nOutput: Score Position\nMusical Score \nDatabase\nFigure 4. The Ultimate Music Companion\ndo not actively query for them with performance data in\nour evaluation runs2. See Table 2 for an overview of the\ncomplete dataset.\n4.2 Results\nAn evaluation of the transcription stage (query preprocess-\ning) was already presented in Section 3.1 above. As Table\n1 shows the results of this stage are rather noisy. Still, the\nquality of the transcription is sufﬁcient to be used with our\nrobust ﬁngerprinting technique.\nWe tested the algorithm with different query lengths: 5,\n10, 20, 30, 40, 50 and 60 notes (in number of transcribed\nnotes during the preprocessing step). For each of the query\nlengths, we generated 50,000 queries by picking random\npoints in the performances of our test database, and used\n2Additionally we performed some non-systematic experiments with\ndata from different sources (e.g., Youtube videos, both by amateurs and\nby professional pianists, with differing recording qualities (including\nnoisy ‘old’ recordings and noisy amateur recordings)), for which we have\nno ground truth data. The general impression is that the system works\nwell too in these scenarios, but of course the performance worsens in the\npresence of noise.Score Tempo\nNormal Double Half\nCorr. Pos. as Top M. 84.63% 83.30% 85.15%\nCorr. Pos. in Top 2 93.39% 92.61% 92.70%\nCorr. Pos. in Top 3 95.36% 95.06% 94.42%\nCorr. Pos. in Top 4 96.14% 96.11% 95.00%\nCorr. Pos. in Top 5 96.55% 96.55% 95.26%\nTable 4. Results of the algorithm with score representa-\ntions with altered tempi. The results are based on query\nlengths of 30 notes.\nthem as input for the proposed algorithm.\nThe results of this experiment are shown in Table 3.\nIn this table we present two measures: the percentage of\ncorrectly identiﬁed pieces, and the percentage of cases in\nwhich both the piece and the exact position in the score\nwere correctly identiﬁed.\nFor the evaluation a score position Xis considered cor-\nrect if it marks the beginning (+/- 1.5 seconds) of a score\nsection that is identical in note content, over a time span\nthe length of the query (but at least 30 notes), to the note\ncontent of the ‘real’ score situation corresponding to the\naudio segment that the system was just listening to (we\ncan establish this as we have the correct alignment be-\ntween performance time and score positions — our ground\ntruth). This complex deﬁnition is necessary because musi-\ncal pieces may contain repeated sections or phrases, and it\nis impossible for the system (or anyone else, for that mat-\nter) to guess the ‘true’ one out of a set of identical passages\nmatching the current performance snippet, given just that\nperformance snippet as input. We acknowledge that a mea-\nsurement of musical time in a score in terms of seconds is\nrather unusual. But as the MIDI tempos in our database\ngenerally are set in a meaningful way, this seemed the best\ndecision to make errors comparable over different pieces,\nwith different time signatures – it would not be very mean-\ningful to, e.g., compare errors in bars or beats over different\npieces.\nAs can be seen, even queries of only a length of 5 notes\nlead to a surprising number of correct position identiﬁca-\ntions, and already for a query length of 20 notes (which\ncorresponds to a mean query duration of 2.78 seconds)\nthe correct position in the score is contained in the top 5\nmatches for more than 95% of the cases.\nTo show the tempo independence of our method we\nalso ran experiments with big tempo differences between\nthe score and the performance. We simulated this by ma-\nnipulating the scores to have double and half the original\ntempo. The results for these experiments are shown in\nTable 4. As can be seen the performance only decreases\nslightly and the proposed algorithm still recognizes the cor-\nrect position in the vast majority of the cases.\nA ﬂaw of the current approach is that it cannot cope\nwith non-linear tempo deviations (i.e., with tempo varia-\ntions within a query). As we are using very short queries\nand a rather coarse resolution in the histograms this is onlya minor problem. But for longer queries (e.g., with dura-\ntions of over 10 seconds) explicitly dealing with non-linear\ntempo deviations becomes more of an issue. To make our\napproach useable with longer queries we propose to split\nthe long query into smaller, overlapping queries (e.g., of\nsize 30 notes with 15 notes overlap) and then use some\nsimple tracking and scoring algorithm to combine the indi-\nvidual results into a single score. Preliminary experiments\nwith this approach suggest that this leads to a very robust\nand accurate algorithm.\n5. CONCLUSION\n5.1 Applications\nThe proposed system is useful in a wide range of applica-\ntions. For the off-line case it may either be used stand-\nalone or as a preprocessing step to audio alignment al-\ngorithms. This enables fast and robust (inter- and intra-\ndocument) searching and browsing in large collections of\nmusical scores and corresponding performances.\nOriginally this work was motivated by an on-line sce-\nnario (see [1]). In connection with an on-line score fol-\nlowing algorithm we are currently building a system that\nwe somewhat immodestly call ‘the ultimate classical mu-\nsic companion’ (see Figure 4 for a sketch of the system).\nThis system will be able to recognize arbitrary pieces of\nclassical music, immediately identify the position in the\nscore, provide meta-information, track the piece via an on-\nline score following algorithm, display the score, and vi-\nsualize musical aspects of the performance like the struc-\nture of the piece, tempo and expressive timing deviations\n– all done on-line with minimal delay. This ultimate mu-\nsic companion may be used to enhance the listening ex-\nperience but may also be of use for performers, especially\nduring rehearsal. Besides this very speciﬁc application the\nproposed algorithm may also be of use in any application\nwhich requires monitoring of an audio stream of classical\nmusic on-line with minimal delay.\n5.2 Future Work\nRegarding the improvement of the algorithm, we see some\nfuture work in making it better useable with longer queries\nincluding tempo variations. As already mentioned above a\ncombination of small overlapping subqueries with simple\ntracking of the results seems to be the way to go. Also a\nlarger scale evaluation of the algorithm (with thousands of\nclassical piano scores) has to be done.\nThe algorithm in the current state is able to recognize\nthe correct piece and the score position even for very short\nqueries of piano music. Based on this algorithm we have\nalready implemented a real-time on-line piece and score\nposition identiﬁcation system that shows a level of perfor-\nmance that even human experts in classical music will ﬁnd\nhard to match. This will be demonstrated live at the con-\nference. We are now working on the integration of the pro-\nposed algorithm in our score following system as a next\nstep towards our ultimate goal: ‘the ultimate classical mu-\nsic companion’.6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Fund\n(FWF) under project numbers Z159 and P22856-N23.\n7. REFERENCES\n[1] A. Arzt, G. Widmer, S. B ¨ock, R. Sonnleitner, and\nH. Frostel. Towards a complete classical music com-\npanion. In Proceedings of the European Conference on\nArtiﬁcial Intelligence (ECAI), 2012.\n[2] S. B ¨ock and M. Schedl. Polyphonic piano note tran-\nscription with recurrent neural networks. In Proceed-\nings of the International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2012.\n[3] A. Cont. A coupled duration-focused architecture for\nrealtime music to score alignment. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence,\n32(6):974–987, 2010.\n[4] S. Flossmann, W. Goebl, M. Grachten, B. Nie-\ndermayer, and G. Widmer. The Magaloff project:\nAn interim report. Journal of New Music Research,\n39(4):363–377, 2010.\n[5] J. Haitsma and T. Kalker. A highly robust audio ﬁn-\ngerprinting system. In Proceedings of the International\nSymposium on Music Information Retrieval (ISMIR),\n2002.\n[6] F. Kurth and M. M ¨uller. Efﬁcient index-based audio\nmatching. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 16(2):382–395, 2008.\n[7] M. M ¨uller, F. Kurth, and M. Clausen. Audio matching\nvia chroma-based statistical features. In Proceedings\nof the International Conference on Music Information\nRetrieval (ISMIR), 2005.\n[8] C. Raphael. Current directions with music plus one. In\nProceedings of the Sound and Music Computing Con-\nference (SMC), 2009.\n[9] J. Serra, E. G ´omez, and P. Herrera. Audio cover song\nidentiﬁcation and similarity: background, approaches,\nevaluation, and beyond. In Z. W. Ras and A. A. Wiec-\nzorkowska, editors, Advances in Music Information\nRetrieval, pages 307–332. Springer, 2010.\n[10] A. Wang. An industrial strength audio search algo-\nrithm. In Proceedings of the International Conference\non Music Information Retrieval (ISMIR), 2003.\n[11] G. Widmer. Discovering simple rules in complex data:\nA meta-learning algorithm and some surprising mu-\nsical discoveries. Artiﬁcial Intelligence, 146(2):129–\n148, 2003."
    },
    {
        "title": "Mel Cepstrum &amp; Ann Ova: The Difficult Dialog Between MIR and Music Cognition.",
        "author": [
            "Jean-Julien Aucouturier",
            "Emmanuel Bigand"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417179",
        "url": "https://doi.org/10.5281/zenodo.1417179",
        "ee": "https://zenodo.org/records/1417179/files/AucouturierB12.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1417179,
        "dblp_key": "conf/ismir/AucouturierB12",
        "content": "MEL CEPSTRUM & ANN OVA: THE DIFFICULT DIALOG \nBETWEEN MIR AND MUSIC COGNITION \n \nJean-Julien Aucouturier & Emmanuel Bigand \nLEAD/CNRS UMR 5022, University of Burgundy, Dijon, France. \naucouturier@gmail.com; bigand@u-bourgogne.fr \n \n  \nMel is a MIR researcher (the audio type) who's always \nbeen convinced that his field of research had something \nto contribute to the study of music cognition. His feeling, \nhowever, hasn't been much shared by the reviewers of the \nmany psychology journals he tried submitting his views \nto. Their critics, rejecting his data as irrelevant, have fru-\nstrated him - the more he tried to rebut, the more defen-\nsive both sides of the debate became. He was close to \ngive up his hopes of interdisciplinary dialog when, in one \nfinal and desperate rejection letter, he sensed an unusual \ntouch of interest in the editor's response. She, a cognitive \npsychologist named Ann, was clearly open to discussion. \nThis was the opportunity that Mel had always hoped for: \nclarifying what psychologists really think of audio MIR, \ncorrecting misconceptions that he himself made about \ncognition, and maybe, developing a vision of how both \nfields could work together. The following is the imagi-\nnary dialog that ensued. Meet Dr Mel Cepstrum, the MIR \nresearcher, and Prof. Ann Ova, the psychologist.     \n     \n1. ON AUDIO FEATURES \n \nAnn Ova: Let me start with a tentative definition of what \nwe, music cognition researchers, are interested in. To me, \ncognition is like digestion: a chain of transformations af-\nfecting a stimulus (e.g. a piece of music reaching the \nears), transforming it, breaking it into blocks and even-\ntually metabolizing it to produce a behavior (an emotion-\nal reaction, recognition, learning, etc.). As researchers, \nwe are seeking to understand this mechanism of \"stimulus \ndigestion\": what in the signal triggers it, how it is acti-\nvated, what brain/mind functions are required.  \nMel Cepstrum: When I hear this, I form the impression \nthat your collective goal is not very different from ours in \nMusic Information Retrieval. First, we study the same \nbehaviors: the recognition of music into melodies, artists, \nstyles, genres, or the prediction of emotional reactions. \nSecond, we too are looking for mechanisms, which we \nprefer to call algorithms, and we conceptualize them us-\ning similar steps: sensory transformations first (we'd call \nthis the signal processing front-end or feature extraction), \nthen linking to memory and learning (we'd say databases \nand statistical models). It is therefore surprising to me \nthat a lot of work in music cognition tends to rely on au-\ndio characteristics that can be extracted \"by ear\", thus ig-\nnoring much of our work in the past 10 to 15 years on musical signal processing. For instance, of the nearly \n1,000 pages of the Handbook of Music and Emotion [14], \nnot a single one is devoted to computerized signal analy-\nsis, but examples abound of research asking participants \nto subjectively evaluate a musical extract's tempo, com-\nplexity, height etc. on scales from 1 to 5, so these charac-\nteristics can be correlated with what you call \"behavior\". \nWhile I understand this may have been the only approach \navailable to, say, Robert Francès in 1958 [10], surely you \ndo realize that all of this (pitch extraction, beat tracking, \netc.) can now be automated with computer algorithms? \nWhat's the superiority of doing it by hand?    \nA.O. This is true, much of what we study is analyzed by \nhand, or rather \"by ear\", by participants. I believe the ad-\nvantage of doing so is that we only consider as possible \nacoustic correlates of a given behavior constructs that can \nbe cognitively assessed by the participants themselves. \nWe want to use what they really  hear, not what a com-\nputer thinks they hear, and the best way to do this is to \nsimply ask them.   \nM.C. But you'll agree that there are unique advantages to \nautomatic analysis: it's fast and cheap, you can process a \nlarge number of stimuli in just minutes, while it would \ntake a large number of participants to do the same by ear. \nA.O. I understand this is an important criteria in your \nfield - certainly one does not want to index iTunes by \nhand, but this is not an important concern for us. If a par-\nticular experimental design is expensive in terms of expe-\nrimenter and participant time, but it is the design of \nchoice, so be it.  \nM.C. Right - but isn't automatic signal analysis also more \nobjective? It can extract physical properties from the sig-\nnal, e.g. the root-mean-square  that qualifies its physical \nenergy or the zero-crossing-rate  which describes the noi-\nsiness of the waveform - without mediating these by cog-\nnitive judgments. It can also realistically simulate the au-\ndio processing chain of the peripheral auditory system. \nFor instance, Mel-Frequency Cepstrum Coefficients , a \nmathematical construct derived from the signal's Fourier \ntransform, are designed to reproduce the non-linear tono-\ntopic scale of the cochlea and the dynamical response of \nthe hair cells of the basilar membrane.  \nA.O. This is only partly correct, you see. If you look at \nMFCCs closely (take Logan [23], say), you see that parts \nof the algorithm were designed to improve their computa-\ntional value for machine learning, and not at all to im-\nprove their cognitive relevance. That final discrete cosine \ntransform, for instance, is used to reduce correlations be-\ntween coefficients, which would make their statistical \nmodeling more complex. Now, one could argue I guess  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nthat the brain uses a similar computational trick - authors \nlike Lewicki [19] are thinking along these lines, I suppose \n- but you'll agree that the responsibility rests on us, re-\nsearchers, to prove that correct. Until then, MFCCs are \nmaths. Useful maths for you perhaps, but irrelevant to our \nconcerns.  \nM.C.  Wait, that's a bit harsh.  How about that study by \nTerasawa, Slaney and their colleagues at Stanford [31]: \nthey resynthesized sounds from MFCCs and showed that \nhuman timbre dissimilarity ratings between sounds corre-\nlated exactly with the MFCCs. Doesn't that prove some-\nthing?  \nA.O. Good one. This is indeed an important study, per-\nhaps the first to tackle this problem diligently. But what \ndoes this prove, you ask? That an algorithmic construc-\ntion, the MFCC, closely predicts a cognitive judgement. \nShould we conclude the brain implements a discrete co-\nsine transform? Probably not. Just like fitting reactors on \nan airplane and seeing it take off should not lead us to \nconclude anything about how birds fly. Don't you think?  \nM.C. You're killing me. Are you seriously rejecting 10 \nyears' worth of results as mere coincidences? Our find-\nings that, say, taking the derivative of MFCCs improve \ngenre classification by 10%, or that periodicities in the \nrange 1-10 seconds (the rhythm fluctuation patterns  of \nPampalk [27]) are enough to account for timbre similari-\nty, shouldn't that, at least, give you some sort of intuition  \nabout how these behaviors are cognitively produced?   \nA.O. Sorry if I sounded dismissive. In theory, you're \nright, and actually, we have been paying attention to your \nwork (initiative like the MIRToolbox [17] have helped). \nBut in practice, it's been really difficult to use your work, \nor to derive useful intuitions from it. Let me show you \nwhat I mean here, precisely. I'm looking at this data pro-\nduced in my lab, a series of emotional valence and arous-\nal judgements produced by participants listening to very \nshort musical extracts (shorter than 500ms). At this level, \nit is unlikely that emotional reactions result from a cogni-\ntive analysis of say, melody or harmony, because the ex-\ntracts are too short to even include a single note. The \nquestion for us is therefore to understand what low-level \nfeatures of the raw sound are responsible for the emotion. \nIt's the classical Gjerdingen & Perrott scenario [12], isn't \nit? This, if I understand correctly, is the ideal use-case for \nMIR features: a quasi-stationary signal, mostly important \nby its timbre quality. Well, let's have a look.  \nTable 1 reproduces the results of a multivariate regression \nwe computed between the stimuli's valence and arousal \nand the whole batch of features offered by the MIRTool-\nbox [17]. Let's see what intuition I, the cognition re-\nsearcher, should derive from this. We see stimulus va-\nlence is very well explained by, let me get this right, the \nentropy of the period of the magnitude of the maximum \npeak detected every 50ms in the signal's chromagram  (a \nchromagram, as you know,  gives at each successive time \nposition the energy observed at the frequency correspond-\ning to each note - c, c#, d, etc., of each octave). Similarly, \nstimulus arousal seems to result from the standard devia-\ntion of the 6th MFCC  and the mean of the Table 1: Top MIR features in a regression of valence \nand arousal emotional judgements \n \n3rd, and - mind you - not the opposite. \nM.C.  Hmm. This seems a bit too complicated maybe?  \nA.O. See what I mean? That surely fits well to the data, \nbut I'm sure you realize it does not actually explain  any-\nthing. Even if we took it literally, this would be a formid-\nable mix-bag of an explanation. We have here an emo-\ntional reaction, valence, of which neuroscience tells us it \nis at least partly pre-attentive and subcortical, and which \nwe explain here with constructs requiring memory and \nstatistical learning (\"entropy\"), rhythmic entrainment \n(\"period\"), temporal integration (\"maximum peak\"), har-\nmonic analysis (\"chromagram\") and arguably a partici-\npant's musical training in a western culture (because the \nchromagram relies on the 12-tone western pitch system).     \nM.C. Well, you got a point. But isn't this exactly the \nsame problem when psychologists rely on features eva-\nluated subjectively by their participants? When they study \ncultural differences between western and Indian classical \nmusic, Balkwill & Thompson [2] argue emotions are re-\nlated to their stimuli's musical complexity, which they \nmeasure by asking participants, I quote, \" to evaluate how \nmuch was going on melodically in the except - was there \na lot of repetition as in \"Mary had a little lamb? \". Now, \nisn't that carrying a lot of assumptions too? The construct \nof  \"being like Mary had a little lamb? \" is probably only \nderivable at a cortical level, using a lot of cognitive func-\ntions such as memory, melodic representations, etc. and \ncertainly presupposes the participants know of that song \nin the first place. Are these assumptions realistic knowing \nwhat we know of emotions?  \nA.O. Well, you're probably right. And I guess one could \neven add that MIR has the advantage of not hiding these \nassumptions under their apparent lexical simplicity. But \nstill, you have to admit that the logics behind your typical \nMIR signal feature is difficult for us to follow. If we want \nto use it to prove anything, it is crucially important for us \nto know what  we're dealing with: a physical measure? a \ncognitive model? Let's have a look. In the MIR bestiary, \nwe find, first, features deriving from traditional psy-\nchoacoustics: for instance, the spectral centroid  which is \nthe traditional correlate for the first perceptual dimension \nidentified by MDS studies of timbre [13] or the log attack \ntime, which correlates with the second most-important \ndimension; then, your field offers quite a lot of mathe-\nmatical variants of these same characteristics, which \nseem to be justified only by the fact that they are concep-\n  \n \ntually close (for instance, spectral skewness , the 3rd spec-\ntral moment, which seems to be included because of the \nspecial status of the first moment, the above-mentioned \ncentroid ) or even that they are easy enough to compute \n(spectral entropy , obtained by multiplying the Fourier \nspectrum with its logarithm); other features seem to start \ntheir career as intermediary steps in the processing chain \nof another feature, gain special status and then a name of \ntheir own (for instance, the \" fluctuation pattern \" you \nmentioned earlier [27], which was originally an interme-\ndiary step in a tempo extraction algorithm); or even, as \nby-products of other algorithms, like some measures of \npulse clarity  [16] which are in fact the error estimation in \nthe output of a beat-tracking algorithm. And the list goes \non, growing every year: the sole MIRToolbox library of-\nfers more than 300 features, very few of which having a \nclear epistemological status. Now, I do not doubt they \nserve your purpose well, but I hope you see it is unclear \nwhether they can serve ours.  \n \n2. ON PRECISION AND GROUNDTRUTH \n \nM.C.  I do. And I have to admit it never occurred to me \nthat our drive to optimize our features for precision (de-\nriving such features, selecting variants that work, recom-\nbining them, etc.) had taken us so far from the cognitive \nreality. Still, isn't it paradoxical that this same process is \ntaking us closer and closer to the actual phenomenon, in \nterms of percentage of precision? I mean, work like Liu \n& Zhang  [21] simulate with more than 95% precision the \nhuman judgements of \"depression\" and \"contentement\" \nmade when listening to more than 250 extracts of music, \nby combining features describing timbre (e.g. spectral \ncentroid), rhythm (e.g. average autocorrelation peak) and \nintensity (e.g. rms). Even if their algorithm has no pre-\ntense of being a cognitive model, the fact that it agrees \nwith humans 95% of the time on hundreds of stimuli can \nonly make us think it captures a large share of the physi-\ncal and sensory features used by human cognition - right?  \nA.O. Let me ask a question. Isn't this definition of preci-\nsion, relative to a so-called ground truth, a bit illusionary? \nDoes everybody, in every culture, have the same exact \ndefinition of what is, say, \"rock music\", or of 2 songs that \n\"sound the same\"?  \nM.C. I see where you're going with this. We, MIR re-\nsearchers, have always been uneasy about this point, to be \nhonest. We're stuck between 2 research traditions: one, \nmachine learning, which is interested in the capacity of \nalgorithms to learn from a set of examples, whatever \nthese examples are. For this tradition, whether the ground \ntruth is meaningful or not is irrelevant. It is just taken as a \ntemporary gold standard, relative to which different algo-\nrithms can be compared. Whether \"rock\" is indeed \"rock\" \nor \"jazz\" does not matter - actually, we want algorithms \nthat have the flexibility to also learn that \"jazz\" is \"rock\" \nif we like them to. However, we also have the second re-\nsearch goal of being useful to electronic music distribu-\ntion systems. Now, in this world, defining a unique \nground truth is suddenly very relevant, but you soon real-ize it is also close to impossible: we have plenty of exam-\nples where what some call \"rock\", others will call \"pop\" \nor \"jazz\" and so on. I guess that's what you would call \nindividual variations. Most of our recent research tries to \naddress this paradox: for instance, how tags learned on \none dataset generalize to other datasets [24], how to per-\nsonalize music recommendations [5] or even letting uses \ndefine their own personal categories in interaction with \nthe system [26]. But one cannot just rule out the idea of \nprecision. After all, you psychologists also have to rely \non the same concept: take the psychoacoustics of musical \ntimbre. What these studies do is, similarly, consider aver-\nage similarity ratings over many users (not that many, \nincidentally, compared with the thousands of samples we \nare routinely dealing with in MIR), and select features \nthat explain the best percentage of the data's variance - \nfinding, for instance, that the spectral centroid of an ex-\ntract correlates at 93% with the first principal component \nof the human-constructed timbre space. But why should \nwe accept spectral centroid as an important \"psychologi-\ncally-validated\" characteristics of timbre, and simulta-\nneously reject, say, Liu & Zhang's average autocorrela-\ntion peak [21] (or Alluri & Toiviainen's 6th band spectral \nflux [1] ) when it allows to classify emotions at 95%? \nSometimes, I wonder if you have a bit of a \"not-invented-\nhere\" bias...  \nA.O. You may be right. Perhaps we have been disregard-\ning advances in signal processing just because they look \ncomplicated and we can't be bothered to follow what \nyou've been doing. Signal features produced by recent \nMIR research could and probably should be integrated to \nmodern psychoacoustics, especially for those problems \nthat could not yet be solved, such as dissonance [28], and \nyou'll have to teach us on that. However, your using the \nexample of psychoacoustics is interesting. I don't know if \nyou realize that the psychoacoustics methodology is de-\nsigned to investigate percepts, i.e. the immediate psycho-\nlogical gestalts corresponding to those few physical cha-\nracteristics that define an auditory object, regardless of \nthe listener and its culture. A musical sound has pitch, \nloudness and timbre. These are percepts. The same sound, \nhowever, does not have  genre or emotion - these are con-\nstructed cognitively; their value could change (e.g. if you \nstart calling the sound \"pop\" instead of \"rock\") without \nchanging the physical definition of a sound. Now, to be \nhonest, the frontier between what's a percept and what's a \ncognitive construction has been challenged in recent \nyears, with the realization that action and perception are \nintertwined, but still most cognition researchers would \nagree a fundamental difference remains between the two. \nI'm worried you're applying the psychoacoustics meta-\nphor to behaviors (genres, emotions, etc.) for which it \ndoes not apply.  \nM.C. This is fascinating. I realize just now that, all these \nyears, I have been using the terms psychoacoustics and \nmusic cognition is a nearly interchangeable way. The \nmore I think about it, the more I realize that indeed MIR \ntakes a psychoacoustics approach to, as you say, genres \nand emotions, treating these as if they were a set of phys-\nical properties of the sound. What's surprising is that it   \n \nworks so well. In fact, we're not capturing \"rock\" or \"sad\" \nmusic, we're capturing things that sound like  \"rock\", or \nthings that sound like a \"sad song\". Because music is a \nstructured human activity, there are a lot of regularities \nthere: most \"sad\" music indeed sounds the same (dark \ntimbre, low pitch, what have you). But these features do \nnot make  the music sad - \nA.O. - or at least, you're not testing whether they do - \nM.C.  right. We can potentially find music that is sad \nwithout exhibiting any of these features.  \nA.O. Take, say, that Dixieland upbeat tune they play at \nfunerals in New Orleans.  \nM.C.  Exactly. For these songs, our models will fail com-\npletely. But because such songs are rare (or at least \nthey're rare in our test databases), say there are maybe 5% \nof them, we can still reach 95% performance without ac-\ntually modeling anything specific about how, say, genre \nis cognitively constructed. \n \n3. ON PHYSICAL AND COGNITIVE MODELS \n \nA.O. It's a possibility, indeed. But you make it sound \nworse than it is, I think. It's not that your approach is bet-\nter or worse than ours, but it's important that we under-\nstand the difference, and how we can be complementary. \nYou're interested in the result, and how much algorithms \nand humans agree on it. In music cognition, I think we're \nless interested in the result than we are in the process. If \nwe were to design computer algorithms to do maths, say, \nwe're not interested in building machines that can multip-\nly numbers as well and as fast as humans, but rather in \ndoing them in such a way that multiplying 8*7 is more \ndifficult than 3*4, as it is for humans.  \nM.C.  This is indeed a true difference between our discip-\nlines. We're happy when we see our algorithms duly clas-\nsify as \"rock\" certain songs that are clearly on the border \nof that definition (Queen's Bohemian Rapsody, say) ... \nA.O. ... whereas we would rather understand what makes \na song more prototypically \"rock\" than another, or how \nmuch \"rock\" does one have to listen to form a stable re-\npresentation of what that genre is.  \nM.C. But your problem in that case is how to measure \nprototypicality, because if you ask the same participants \nto judge it subjectively then your argument becomes \ncompletely tautological...   \nA.O. You're right \nM.C. ... whereas MIR gives you a tool to do just this: a \nmeasure, let's say a physical measure, of the \"rockness\" \nof a song. How much it sounds like rock.   \nA.O. This, what you just said, is really interesting. The \nkey word here is \"physical\". I believe that music cogni-\ntion would gain a lot indeed if it had a more complete and \npowerful arsenal of tools to control stimuli physically. \nTools that do not have the pretense of infringing into \ncognitive thinking, just purely, state-of-the-art physical modeling. If we start seeing MIR in this way, a lot of re-\nsearch avenues open I think.   \nM.C.  In sum, in order to be useful to cognition, we \nshould stop trying to do any ourselves.  \n \n4. ON FOLK PSYCHOLOGY \n \nA.O. I can sense the irony, you know. This said, if I can \nmake a small request, and I'm saying this in part jokingly \nbut not solely, it would help indeed if you guys could at \nleast stop using the word \"semantics\".  \nM.C. Wait...  What?  \nA.O. \"Semantics\" - as in \"a semantic model\" of genre \nclassification, \"mixing acoustics with semantic\" informa-\ntion, \"semantic gap\". Just, what do you mean by this?  \nM.C. Well, I suppose we take it as the \"high-level\" mean-\ning of music, like saying \"rock\" is semantically related to \nyouth and rebellion, electric guitars, all that linguistic and \nsocial knowledge around music. All which is where per-\nception stops and, err..., cognition kicks in? Activating \nthe semantic networks of musical concepts, err...  \nA.O. See: that. We hate it where you do that. Folk psy-\nchology. Like there is a box in our head somewhere with \na knowledge base, and some kind of process that acti-\nvates this or that depending on the input. You lose us in-\nstantly with that kind of thinking. If you browse the psy-\nchology literature, you will not find a single cognitive \nmodel which uses a \"semantic\" layer. That single word, \nlet alone your using it assuming that it will appeal to us, \ndoes probably more harm to the dialogue between our \ndisciplines than the mathematical complexity of your \nwork. The \"entropy of the period of the magnitude\", I can \ndeal with; \"semantics\", I sincerely have no idea. It literal-\nly drives me away.   \nM.C. Interesting - that certainly explains some reviewer \nreactions when I tried to communicate MIR results in \npsychological journals! Now, on the question of musical \ngenre, you have to admit, conversely, that research in \ncognition does not have much to say about the links be-\ntween social, lexical, sensory categories - all that we \nwrongly call \"semantics\". Neuroscience research has \nshown for instance that Wagner operas could prime rec-\nognition on such words as heroism, courage, etc. [15]. \nThis has probably profound implications for everyday \nmusic perception. How come music cognition research is \nnot studying this?   \nA.O. You're right. Most of us would consider that musi-\ncal genre, as an object of study, is too complex, i.e. we \nknow in advance that studying it won't help us isolate ex-\nperimentally any particular process that could constitute \nit. For instance, if one wants to understand the sensory \nprocess by which a rock song is recognized as rock, it is \nsimpler, more elementary if you will, to study the same \nprocess in the case of the recognition of environmental \nsounds. This latter case is less plagued by cultural learn-\ning, ambiguity, subjectivity that musical genre.    \n \nM.C.  I see. Unfortunately, we in MIR don't have that \nluxury. If iTunes users want rock music, we cannot easily \njustify to study hammer noises instead.  \nA.O. Naturally. Once again, your discipline is interested \nin the result, and we are interested in the process.   \n   \n5. INSPIRING EXAMPLES \n \nM.C.  But I'd like to backtrack a bit to your argument that \nMIR could contribute to cognition as a tool for physical \nmodeling. This sounded promising.  \nA.O. Yes, I believe there is room to invent a methodolo-\ngy to use MIR tools to build a scientific proof in cogni-\ntion. Precisely, MIR can be used, I think, as a physical \nmeasure of the information available for human cognition \nin the musical signal for a given task. And this measure \ncan be used to control our stimuli and separate what's in \nthe signal from what's constructed out of it by cognition.   \nM.C.  I think there is work that already goes in the direc-\ntion. In the speech domain, de Boer & Kuhl [8] for in-\nstance have shown that speech recognition algorithms \n(hidden Markov models) have better word recognition \nperformance when they are trained and tested on infant-\ndirected speech (IDS, or \"motherese\") than adult speech, \nwhich they claim validates the argument that the deve-\nlopmental value of IDS is to bootstrap language learning.  \nA.O. It's a lovely result, and indeed a very good example \nof how to integrate a physical, holistic recognition algo-\nrithm into a cognitive argument. What's important here is \nthat the algorithm is not presented as a cognitive model: \nnobody here is pretending that the human brain imple-\nments a hidden Markov model. It only gives a proof of \nfeasibility: from a purely physical point of view, the in-\nformation exists in the IDS signal to allow for an easier \ntreatment than adult speech. It would be very difficult to \nreplace the machine by a human measure in this argument \n- computer modeling was really the clever thing to use.  \nM.C.  There are a few other examples. For instance, Kap-\nlan and colleagues [25] show that machine learning can \nclassify dog barks into contexts like being afraid, playful, \netc. This was taken to indicate, for the first time, that dog \nvocalizations contain communicative features. Like you \nsaid, from a purely physical, objective point of view, the \npoint is to show that the information exists in the signal \nto allow for a potential communicative use.  \nA.O. I think we have discovered a design pattern here: \none could probably imagine a similar application with \nmusic.  \nM.C. Let's see. Could we show for instance with a ma-\nchine's good recognition performance that there exist \nenough harmonic information in, say, Indian classical \nmusic to explain the good performance (e.g. [2]) of west-\nern listeners when they are asked to classify emotions in \nraags (even though they are not familiar with this musical \ntradition)?  \nA.O. I confirm: this is exactly the type of question that's \ninteresting for us, music cognition researchers, and in-deed, I wouldn't know how to prove this without MIR. \nProvided the algorithm uses a representation of \"harmo-\nny\" which is both plausible biologically and agnostic cul-\nturally, of course. Some low-level measure of conson-\nance/dissonance rather than a chromagram, perhaps?  \nM.C. Studying this would be pretty interesting from our \npoint of view too. These characteristics found \"enough to \nexplain\" a behavior would allow us to improve the per-\nformance of our algorithms in cross-cultural contexts, \nwhich are becoming a key issue in MIR. A recent interna-\ntional project, CompMusic [29], has even brought forth \nthe question whether our most trusted algorithms have a \nwestern bias, because they were \"evolved\" with corpuses \ncomposed mostly of western music. By the way, you see, \nwe're perhaps less naive than you first thought... \n \n6. A SECOND LOOK AT BIOLOGICAL \nPLAUSIBILITY \n \nA.O.  I agree a lot of questions overlap between our 2 \ndisciplines, perhaps more than I had first assumed. On the \nother hand, you'll also have to recognize that we are less \nhermetic to computational modeling than you think. I al-\nready mentioned the MIRToolbox, which is gaining in-\nterest among music psychologists. But most of our recent \nwork include some element of computational modeling, \noften inspired by neuroscience. For instance, recent stu-\ndies in harmonic priming [3] rely on fairly advanced \ncomputational models of auditory short-term memory \n[18]. Recently, human performance in tempo tracking \nwas even explained in Journal of Experimental Psycholo-\ngy by a non-linear oscillation model [22].  \nM.C. This is true. Curiously, we MIR researchers are \naware of these algorithms (auditory models by Leman \n[18], Large [22] or Cariani [4]), but for some reason they \nare not widely used. The criteria here is, again, precision. \nIn our experience, when we first try them, \"cognitively \nplausible\" algorithms tend to work less well than brute-\nforce engineer solutions, so they quickly drift into obli-\nvion before we spend much time with them. One example \nwas Lidy and Rauber's study [20] applying a wide range \nof \"psychoacoustical\" optimizations for genre recognition \nand finding very little improvement, if any. So we con-\nclude, a bit hastingly maybe, that they're below our stan-\ndards. But we've already discussed the difference be-\ntween optimizing precision and modeling an underlying \ncognitive process.    \nA.O. Indeed. But even if you consider precision alone, \nthe claim that cognitively plausible models will always \nperform poorly is not necessarily true, I think. In the im-\nage processing community, models which follow biologi-\ncal constraints radically are now giving comparable per-\nformance to their non-biologically-plausible alternatives \n(e.g. Serre, Poggio and colleagues [30]), and even faster \nlearning rates. Now, you could argue that more is known \nin the psychophysiology of the visual cortex than for the \nauditory cortex, and it is therefore logical that machine \nvision should be ahead, but it is less and less the case, I \nreckon. We now have a good understanding of the re-  \n \nsponse patterns of neurons throughout the auditory path-\nway (see e.g. the idea of spectro-temporal receptive fields \n[11]), and computational models even exist to model \nthem [6].       \nM.C.  That's right. I have seen one application of these \nmodels to instrument timbre classification, but this was \nby researchers outside the MIR community [9], and I \ndon't think we have really picked it up. I guess we should \nlook into these more seriously.   \nA.O. I think we should indeed. The potential is not only \nbetter precision, but better interdisciplinary dialogue. \nAgain, let's turn to machine vision for an example. The \nvisual cognition community has now started to take inspi-\nration from models like Serre's [30] to explain experi-\nmental results. For instance, I'm looking at a recent paper \nby Crouzet, Thorpe and colleagues [7], finding that hu-\nmans are capable of ultra-fast face categorization. The \nauthors write: \" Our ability to initiate directed saccades \ntoward faces as early as 100–110 ms after stimulus onset \nclearly leaves little time for anything other than a feed-\nforward pass. [Conveniently,] there is recent evidence \n(Serre et al. 2007) that such a purely feed-forward hie-\nrarchical processing mechanisms may be sufficient to ac-\ncount for at least some forms of rapid categorization\". In \nterms of interdisciplinary collaboration, I look at this with \nenvy.  \nM.C.  This is inspiring indeed! Let's work together so \nthat, in a few years' time, we can write similar arguments \nin a similar article, linking a MIR model with some as-\npect of music cognition to derive a common scientific \nconclusion.  \n \n7. REFERENCES \n[1] Alluri, V. & Toiviainen, P. (2010). Exploring perceptual and \nacoustic correlates of polyphonic timbre. Music Percep-\ntion, 27(3), 223-241. \n[2] Balkwill, L.& Thompson, W. F. (1999). A cross-cultural \ninvestigation of the perception of emotion in music: Psycho-\nphysical and cultural cues. Music Perception, 17, 43-64 \n[3] Bigand, E. & Poulin-Charronnat, B. (2006). Are we \"expe-\nrienced listeners\"? A review of the musical capacities that do \nnot depend on formal musical training. Cognition, 100(1), 100-\n130  \n[4] Cariani, P. (2001). Temporal Codes, Timing Nets, and Mu-\nsic Perception. Journal of New Music Research, 30(2), 107-136.  \n[5] Celma, O. & Lamere, P. (2011). If You Like Radiohead, \nYou Might Like This Article, AI Magazine, 32(3), 57-66 \n[6] Chi, T., Ru, P. & Shamma, S. (2005) Multi-resolution spec-\ntrotemporal analysis of complex sounds. Journal of Acoustical \nSociety of America, 118(2),  887-906  \n[7] Crouzet, S., Kirchner, H. & Thorpe, S. (2010). Fast saccades \ntoward faces: Face detection in just 100 ms, Journal of Vision, \n10(4):16.1-17 \n[8] De Boer, B. & Kuhl, P. (2003) Investigating the role of in-\nfant-directed speech with a computer model. Acoustics Re-\nsearch Letters On-line 4(4), 129–134 \n[9] Elhilali, M, Shamma, SA, Thorpe, SJ & Pressnitzer, D \n(2007) Models of timbre using spectro-temporal receptive \nfields: investigation of coding strategies, in proc. 19th Interna-\ntional Congress on Acoustics, Madrid, Spain. \n[10] Francès, R. (1958) La perception de la musique, Paris: \nVrin. [11] Ghazanfar, A. & Nicolelis, M. (2001). The Structure & \nFunction of Dynamic Cortical & Thalamic Receptive Fields, \nCerebral Cortex, 11(3):183-93.   \n[12] Gjerdingen, R. & Perrott, D. (2008) Scanning the Dial: The \nRapid Recognition of Music Genres, Journal of  New Music Re-\nsearch, 37(2), 93–100  \n[13] Grey, J. M. (1977). Multidimensional perceptual scaling of \nmusical timbres. Journal of the Acoustical Society of America, \n61, 1270–1277. \n[14] Juslin, P. & Sloboda, J. (2010) Handbook of Music and \nEmotion. Oxford University Press, USA \n[15] Koelsch, S., Kasper, E, Sammler, D., Schulze, K., Gunter, \nT. & Friederici, A. (2004) Music, Language and Meaning: \nBrain Signatures of Semantic Processing, Nature Neuros-\ncience 7, 302 - 307.  \n[16] Lartillot, O., Eerola, T., Toiviainen, P. & Fornari, J. (2008) \n, Multi-feature modeling of pulse clarity: Design, validation, \nand optimization, in proc 9th Int. Conference on Music Infor-\nmation Retrieval, Philadelphia PA, USA  \n[17] Lartillot, O. & Toiviainen, P. (2007) A Matlab Toolbox for \nMusical Feature Extraction From Audio, in proc. 10th Int. Con-\nference on Digital Audio Effects, Bordeaux, France.  \n[18] Leman, M. (2000). An Auditory Model of the Role of \nShort-term Memory in Probe-tone Ratings. Music Percep-\ntion, 17(4), 481-510.  \n[19] Lewicki, M.S. (2002). Efficient coding of natural \nsounds, Nature Neuroscience, 5 (4), 356-363. \n[20] Lidy, T. & Rauber, A. (2005). Evaluation of Feature Ex-\ntractors and Psycho-Acoustic Transformations for Music Genre \nClassification. in proc. 6th Int. Conference on Music Informa-\ntion Retrieval, London, UK.  \n[21] Liu, D. & Zhang, H.-J. (2006) Automatic mood detection \nand tracking of music audio signal, IEEE Transactions on \nSpeech and Audio processing, 14(1), 5-18 \n [22] Loehr, J., Large, E. & Palmer, C. (2011). Temporal coor-\ndination in music performance: Adaptation to tempo change. \nJournal of Experimental Psychology: Human Perception and \nPerformance, 37 (4), 1292–1309 \n[23] Logan, B. (2000) Mel Frequency Cepstral Coefficients for \nMusic Modeling. in Proc. 1st Int. Conf. on Music Information \nRetrieval, Plymouth, MA, USA.  \n[24] Marques G., Domingues M., Langlois T. & Gouyon F. \n(2011). Three Current Issues in Music Autotagging. in proc. \n12th Int. Conf. on Music Information Retrieval, Miami, FL, \nUSA.  \n[25] Molnár, C., Kaplan, F., Roy, P., Pachet, F., Pongrácz, P., \nDóka, A. & Miklósi, Á. (2008) Classification of dog barks: a \nmachine learning approach. Animal Cognition, 11(3):389-400.  \n[26] Pachet, F (2008). The future of content is in our-\nselves. Computers in Entertainment,6(3), 2008. \n[27] Pampalk, E., Flexer, A. & Widmer, G. (2005). Improve-\nments of Audio-Based Music Similarity and Genre Classifica-\ntion, in proc. 6th Int. Conf. on Music Information Retrieval, \nLondon, UK.  \n[28] Peretz, I. (2008). The need to consider underlying mechan-\nisms: A response from dissonance. Behavioral and Brain \nSciences, 31, 590-591  \n[29] Serra, X.  (2011). A Multicultural Approach in Music In-\nformation Research , in proc. 12th Int. Conf. on Music Informa-\ntion Retrieval - see also http://compmusic.upf.edu/node/71  \n[30] Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M. & Pog-\ngio, T. (2007). Object recognition with cortex-like mechanisms. \nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 29(3), 411-426 \n[31] Terasawa, H., Slaney, M. & Berger, J. (2005) . The Thir-\nteen Colors of Timbre, in proc. IEEE Workshop on Appl. of \nSignal Proc. to Audio and Acoustics, New Paltz, NY, USA."
    },
    {
        "title": "Analyzing Drum Patterns Using Conditional Deep Belief Networks.",
        "author": [
            "Eric Battenberg",
            "David Wessel"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417955",
        "url": "https://doi.org/10.5281/zenodo.1417955",
        "ee": "https://zenodo.org/records/1417955/files/BattenbergW12.pdf",
        "abstract": "We present a system for the high-level analysis of beat- synchronous drum patterns to be used as part of a compre- hensive rhythmic understanding system. We use a multi- layer neural network, which is greedily pre-trained layer- by-layer using restriced Boltzmann machines (RBMs), in order to model the contextual time-sequence information of a drum pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an ef- fective generative model of multi-dimensional sequences. Subsequent layers of the neural network can be pre-trained as conditional or standard RBMs in order to learn higher- level rhythmic features. We show that this model can be fine-tuned in a discriminative manner to make accurate pre- dictions about beat-measure alignment. The model gen- eralizes well to multiple rhythmic styles due to the dis- tributed state-space of the multi-layer neural network. In addition, the outputs of the discriminative network can serve as posterior probabilities over beat-alignment labels. These posterior probabilities can be used for Viterbi decoding in a hidden Markov model in order to maintain temporal con- tinuity of the predicted information.",
        "zenodo_id": 1417955,
        "dblp_key": "conf/ismir/BattenbergW12",
        "keywords": [
            "beat- synchronous",
            "drum patterns",
            "high-level analysis",
            "comprehensive rhythmic understanding",
            "multi- layer neural network",
            "generative model",
            "conditional RBMs",
            "rhythmic features",
            "discriminative manner",
            "Viterbi decoding"
        ],
        "content": "ANALYZING DRUM PATTERNS USING CONDITIONAL DEEP BELIEF\nNETWORKS\nEric Battenberg\nUniversity of California, Berkeley\nDept. of Electrical Engineering and Computer Sciences\nericb@eecs.berkeley.eduDavid Wessel\nUniversity of California, Berkeley\nCenter for New Music and Audio Technologies\nwessel@cnmat.berkeley.edu\nABSTRACT\nWe present a system for the high-level analysis of beat-\nsynchronous drum patterns to be used as part of a compre-\nhensive rhythmic understanding system. We use a multi-\nlayer neural network, which is greedily pre-trained layer-\nby-layer using restriced Boltzmann machines (RBMs), in\norder to model the contextual time-sequence information\nof a drum pattern. For the input layer of the network, we\nuse a conditional RBM, which has been shown to be an ef-\nfective generative model of multi-dimensional sequences.\nSubsequent layers of the neural network can be pre-trained\nas conditional or standard RBMs in order to learn higher-\nlevel rhythmic features. We show that this model can be\nﬁne-tuned in a discriminative manner to make accurate pre-\ndictions about beat-measure alignment. The model gen-\neralizes well to multiple rhythmic styles due to the dis-\ntributed state-space of the multi-layer neural network. In\naddition, the outputs of the discriminative network can serve\nas posterior probabilities over beat-alignment labels. These\nposterior probabilities can be used for Viterbi decoding in\na hidden Markov model in order to maintain temporal con-\ntinuity of the predicted information.\n1. INTRODUCTION\nDeep belief networks (DBNs) have shown promise in many\ndiscriminative tasks, such as written digit recognition [6]\nand speech recognition [8]. In addition, the generative na-\nture of DBNs makes them especially well-suited for stochas-\ntic generation of images or sequences [5, 11].\nIn this paper, we apply DBNs to the analysis of drum\npatterns. The drum pattern analysis system presented here\nis to be part of a complete live drum understanding system,\nwhich is also composed of a drum detection front-end [1]\nand a low-level multi-hypothesis beat tracker. The goal\nof the drum understanding system is to go beyond simple\nbeat tracking by providing additional high-level rhythmic\ninformation, such as time signature or style information,\nwhile being robust to expressive embellishments and dy-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.namic song structure, such as tempo ﬂuctuations or time\nsignature changes. The pattern analysis system we present\nhere can help achieve these goals, not only by providing\nthe desired high-level information, but also by communi-\ncating with the low-level beat tracker to help it correct beat\nperiod and phase errors.\nTo demonstrate the effectiveness of this model, we fo-\ncus on a speciﬁc discriminative task: identifying the align-\nment of beats within a measure. Measure alignment infor-\nmation is particularly important to high-level drum pattern\nanalysis since each beat has a speciﬁc meaning depend-\ning upon the musical style. For example, song transitions\ntypically occur on the ﬁrst beat of a measure, and in rock\nmusic and relate styles, beats 2 and 4 typically feature an\naccented snare drum back beat .\nPrevious work on beat-measure alignment has focused\non simple heuristic rules. In [7], Klapuri presents a beat\ntracker that determines beat-measure alignment by corre-\nlating multi-band onset patterns with two different back\nbeat measure templates. In [3], Goto addresses beat align-\nment by detecting chord change locations and by align-\nment with 8 drum pattern templates. Approaches like these\nwork well for the typical pop song but are ineffective when\npresented with exotic rhythm styles or many types of pro-\ngressive music. To deal with these situations, rather than\naccruing a large list of hand-written heuristic rules, we can\nautomatically encode a large amount of musical knowl-\nedge into the distributed state-space [2] of a deep belief\nnetwork, which we introduce in the next section.\n2. DEEP BELIEF NETWORKS\n2.1 The Restricted Boltzmann Machine\nThe deep belief network is a probabilistic multi-layer neu-\nral network composed of restricted Boltzmann machines ,\nor RBMs [2, 5]. The RBM, as shown in Figure 1, is a two\nlayer probabilistic graphical model with undirected con-\nnections between visible layer units, vi, and hidden layer\nunits,hj. The “restricted” part of the name points to the\nfact that there are no connections between units in the same\nlayer. This allows the conditional distribution of the units\nof one layer given all the units of the other layer to be com-h1h2hMv1v2v3vN......hidden layervisiblelayera1a2a3aNb1b2bMWFigure 1 . A restricted Boltzmann machine with Nvisible\nunits andMhidden units.\npletely factorial, i.e.\nP(vjh) =Y\niP(vijh) (1)\nP(hjv) =Y\njP(hjjv) (2)\nThe RBM is a probabilistic energy-based model, meaning\nthe probability of a speciﬁc conﬁguration of the visible and\nhidden units is proportional to the negative exponentiation\nof an energy function, E(v;h)\nP(v;h) =e\u0000E(v;h)\nZ(3)\nWhereZ=P\nv;hexp(\u0000E(v;h))is a normalizing con-\nstant referred to as the partition function . Note that be-\ncauseZis difﬁcult to compute, it is typically intractable to\ncompute the joint distribution P(v;h).\nFor binary-valued visible and hidden units, the energy\nfunction,E(v;h), can be written as:\nE(v;h) =\u0000aTv\u0000bTh\u0000vTWh (4)\nWhere aandbare vectors containing the visible and hid-\nden unit biases, respectively, and Wis the weight matrix\nthat connects the two layers.\nThe goal in training an RBM is to maximize the like-\nlihood of the training data under the model, P(v). The\nactual log-likelihood gradient is difﬁcult to compute be-\ncause it involves the intractable partition function Z; how-\never, stochastic estimates of the gradient can be made by\ndrawing Gibbs samples from the joint distribution P(v;h)\nusing the factorial conditional distributions in (5),(6).\nP(vi= 1jh) = \u0016\u001b(ai+P\njWijhj) (5)\nP(hj= 1jv) = \u0016\u001b(bj+P\niWijvi) (6)\nWhere \u0016\u001b(x)is the logistic sigmoid function:\n\u0016\u001b(x) =1\n1 +e\u0000x(7)\nh￿1h￿2h￿M￿...\nRBM1h1h2hM...v1v2v3vN...RBM2\nvisible layerlayer 2 layer 3 W1W2Figure 2 . A 3-layer deep belief network comprised of 2\nRBMs\nThe Gibbs sampling Markov chain can take quite a long\ntime to produce actual samples from the joint distribution,\nso in practice the chain is started at a training example and\nrun for a small number of iterations. Using this estimate\nof the log-likelihood gradient, we are instead minimizing a\nquantity referred to as the contrastive divergence between\nthe training data and the model [2, 5]. Contrastive diver-\ngence updates for the RBM parameters are shown below:\n\u0001Wij/ hvihji0\u0000hvihjik (8)\n\u0001ai/ hvii0\u0000hviik (9)\n\u0001bj/ hhji0\u0000hhjik (10)\nWhereh\u0001ikdenotes the value of the quantity after kitera-\ntions of Gibbs sampling, and for k= 0,viis simply the\ntraining data and hjis a sample from (6) given the training\ndata. Typically, these updates are performed using multiple\ntraining examples at a time by averaging over the updates\nproduced by each example. This helps to smooth the learn-\ning signal and also helps take advantage of the efﬁciency\nof larger matrix operations. As k! 1 , these updates\napproach maximum likelihood learning.\n2.2 Stacking RBMs\nA deep belief network is formed when multiple RBMs are\nstacked on top of each other as shown in Figure 2. Af-\nter training a ﬁrst-level RBM using the training data, we\ncan perform a deterministic up-pass by setting the hidden\nunits to their real-valued activation probabilities using (6)\nfor each visible training vector. This is the same as what\nis done in the up-pass in a deterministic neural network.\nThese deterministic hidden unit values are then used as the\nvisible data in a subsequent higher-level RBM, which is\nalso trained using contrastive divergence learning. This\nRBM stacking continues until the network reaches the de-\nsired depth. This greedy layer-by-layer training approach\nis a useful procedure for learning a set of non-linear fea-\ntures in an unsupervised manner [4], and it has been shownh1h2hM...v1v2vN...y1y2yK...baWBAconditioning unitsvisible unitshidden unitsFigure 3 . A conditional restricted Boltzmann machine.\nThe correct label unit activations are provided as part of\nthe visible unit data during training.\nto be a beneﬁcial pre-training procedure when followed by\ndiscriminative backpropagation [6].\n2.3 The Conditional Restricted Boltzmann Machine\nThe conditional restricted Boltzmann machine (CRBM) takes\nthe RBM a step further by adding directed connections be-\ntween additional visible units, yi, and the existing visible\nand hidden units, as shown in Figure 3. These additional\nunits can represent any type of additional information, in-\ncluding visible data from the recent past. Because of this,\nthe CRBM is an effective generative model of time se-\nquence data [11]. This fact is what motivated our use of\nthe CRBM to model drum patterns.\nThe directed connections, which are represented by weight\nmatricesAandB, replace the bias terms, aandbin (5),(6),\nwith dynamic bias terms, ^ aand^b.\n^ a=a+Ay (11)\n^b=b+By (12)\nWhere yis a vector containing the conditioning data. This\nmodiﬁed RBM models the distribution P(v;hjy), and the\nlearning rules in (8)–(10) are unchanged except for the ad-\ndition of the dynamic bias terms to the sampling expres-\nsions. The learning rules for the conditional weight matri-\nces also have a familiar form:\n\u0001Aij/ hviyji0\u0000hviyjik (13)\n\u0001Bij/ hhiyji0\u0000hhiyjik (14)\nNote that the yjabove are simply the training values and\nare not stochastically sampled in any way.\n3. MODELLING AND ANALYZING DRUM\nPATTERNS\n3.1 Bounded Linear Units\nDrum patterns are not simply a series of ones and zeros,\nonset or no onset. Most drum patterns contain an appre-\nciable sonic difference between accented and unaccented\nnotes on every drum or cymbal, and it is these differences\nwhich give drum patterns their character. In order to ef-\nfectively model drum patterns using the CRBM, we must\nmodify the binary-valued visible units to be real-valued.\n−10−5 0 5 10 15 20 25 30\nInput ( x)05101520Expected value, E[v|x]Bounded Linear Unit Activation with N=20Figure 4 . Bounded linear unit activation function\nThere are many options for getting real-valued visible\nactivations out of RBMs; in fact, it has been shown that\nevery distribution in the exponential family is a viable can-\ndidate [13]. A popular choice is the Gaussian distribution\ndue to its familiarity and ubiquity; however, the unbound-\nedness of Gaussian samples does not translate well to the\nspace of dynamic levels possible within a drum pattern.\nIn order to model the bounded nature of playing dy-\nnamics, we use a modiﬁed version of the rectiﬁed linear\nunits (RLUs) described in [9]. RLUs are constructed from\na series of binary units with identical inputs but with ﬁxed,\nincreasing bias offsets. If the bias offsets are chosen appro-\npriately, the expected value and variance of the number of\nactive units out of these Ntied binary units with common\ninputxis:\nE[vjx] = log(1 + ex)\u0000log(1 +ex\u0000N)(15)\nVar(vjx) = \u0016\u001b(x)\u0000\u0016\u001b(x\u0000N) (16)\nAs can be seen in Figure 4, (15) yields a sigmoidal curve\nthat saturates when x>N , bottoms out when x<0, and\nis approximately linear in between. In the linear region,\nthe variance is equal to one, so the value of Nis chosen to\nachieve the desired level of noisiness in the samples, and\nthe training data can be rescaled to the interval [0;N]. In\nthis work, we have chosen N= 20 , so that a value of 20\nrepresents the loudest possible drum strike, while zero rep-\nresents the absence of a drum strike. To sample from these\nbounded linear units (BLUs), instead of actually sampling\nfromNbinary units with stepped offsets, we approximate\ntheir total output with:\nP(vjx)\u0018h\nN\u0000\nE[vjx];Var(vjx)\u0001iN\n0(17)\nwhereN(\u0001)is a normal distribution with mean and vari-\nance provided by (15) and (16), and\u0002\n\u0001\u0003N\n0snaps values\noutside of the interval [0;N]to the boundaries of the in-\nterval. Because these BLUs are constructed from logistic\nbinary units, all of the RBM learning rules from Section 2\nare still valid; the only thing that changes is how we sample\nfrom the visible BLUs.\n3.2 Label Units\nIf bounded linear units give us a way to get drum onset\ninformation into the network, label units are how we get\ninformation out of the network. A standard approach to\nmulti-class classiﬁcation with neural networks is to use ah1h2hM...v1v2vN...l1l2lL...bainput datalabel unitsWhidden layervisiblelayerFigure 5 . An RBM with an added group of visible label\nunits.\ngroup of softmax output units, which assigns a value to\neach of its units (each with input xi) based on the softmax\nactivation function shown in (18). This activation function\nis convenient for classiﬁcation because the activation val-\nues of the group sum to one, which allows the output values\nto be interpreted as posterior class probabilities given the\ninput data.\nSmax(xi;x) =exi\nP\njexj(18)\nIn the realm of RBMs and deep learning, a different ap-\nproach can be used which entails providing the ground\ntruth class labels as part of the visible data during training.\nThis approach has been shown to be more effective than us-\ning a separate softmax output layer in certain cases [6], and\nit indeed achieves better results for our application. Instead\nof adding the label units to a separate output layer, we aug-\nment the visible layer in the top-level RBM of a deep belief\nnetwork with a group of softmax label units, as shown in\nFigure 5. This allows us to train the top-level RBM us-\ning the label units as visible data, by turning on only the\ncorrect label unit during training. Once this labeled RBM\nhas been trained, we can compute the posterior activation\nprobability under the model of each of the label units given\nthe data,P(ljv), using (19) and (20) (see [5]):\nF(vjl) =\u0000X\niv(l)\niai\u0000X\njlog\u0010\n1 + exp\u0010\nx(l)\nj\u0011\u0011\n(19)\nP(ljv) =e\u0000F(vjl)\nP\nke\u0000F(vjk)(20)\nWherex(l)\nj=bj+P\niWijv(l)\ni, andv(l)\nidenotes the visible\ndata but with only unit lof the label unit group activated.\nThis calculation is tractable due to the typically small num-\nber of label units being evaluated.\n3.3 Modelling Drum Patterns\nIn our drum pattern analysis network, we always start with\na CRBM at the ﬁrst layer. This CRBM models the current\ndrum beat or subdivision using one BLU visible unit per\ndrum or cymbal. In our experiments, we use a minimal\nthree-drum setup: bass drum, snare drum, and hi-hat, but\nthis can be expanded to work with any percussion setup.\nThe conditioning units, yj, of the CRBM contain drum ac-\ntivations from the recent past. In our experiments, yis\nfed with drum activations from the most recent two mea-\nsures (or 32 subdivisions given a 4/4 time signature with\nsixteenth note subdivisions).Subsequent layers use binary visible units instead of\nBLUs. Intermediate layers of the DBN can be made up of\neither additional CRBMs or standard RBMs, and the ﬁnal\nlayer must have visible label units to represent the classi-\nﬁer output. Using an intermediate-layer CRBM allows the\nlayer to take into account past hidden unit activations of the\nlayer below it, which allows it to learn higher-level time de-\npendencies. In doing so, it increases the past time context\nthat the top-level layer sees, since the past hidden unit acti-\nvations of a ﬁrst-level CRBM have been conditioned on the\npast relative to themselves. In order to make a fair compar-\nison between DBNs that use different numbers of CRBM\nlayers, we must make sure that the top layer always has ac-\ncess to the same amount of visible ﬁrst-layer data from the\npast.\nIn our experiments, we train the label units to detect the\ncurrent sixteenth note beat subdivision within the current\n4/4 measure. In the next section, we give details on the\nconﬁguration and training of the various DBNs that we test\nfor this task.\n4. TRAINING THE SYSTEM\n4.1 Training Data\nThe dataset consists of 173 twelve-measure sequences com-\nprising a total of 33,216 beat subdivisions, each of which\ncontains bass drum, snare drum, and hi-hat activations.\nThe data was collected using electronic Roland V-drums1,\nquantized to exact sixteenth note subdivisions, and con-\nverted to a subdivision-synchronous drum activation ma-\ntrix.\nThe sequences were intended to span a sizeable, but by\nno means complete, collection of popular rhythmic styles.\nThere is a strong rock bias, with many beats featuring a\nprominent back beat; however, also included are more syn-\ncopated styles such as funk and drum ‘n’ bass as well as the\nBrazilian styles samba and bossa nova. We use a random-\nized 70/20/10 split for training, testing, and validation data,\nrespectively.\n4.2 DBN Conﬁgurations\nWe test four DBN conﬁgurations. For each of the four\nnetwork architectures, we tested multiple hidden unit con-\nﬁgurations and have chosen to present only those which\nperformed best on the test data for each architecture. They\nare as follows:\n1.1-layer: Labeled-CRBM\n3 visible data units + 16 visible label units, 100 hid-\nden units, and 32 past subdivisions of context (96\nconditioning units)\n2.2-layers: CRBM!Labeled-RBM\nEach with 100 hidden units. The CRBM again has a\ncontext of 32 subdivisions.\n1http://rolandus.com3.2-layers: CRBM!Labeled-CRBM\nWith 100 and 200 hidden units respectively. Each\nCRBM has a context of 16.\n4.3-layers: CRBM!CRBM!Labeled-RBM\nWith 100, 200, and 100 hidden units respectively.\nBoth CRBMs have a context of 16 subdivisions.\n4.3 DBN Training\nEach non-labeled layer was trained using contrastive di-\nvergence with k= 1 (CD-1) for 300 sweeps through the\ntraining data with an update batch size of 100. The order\nof the training data was randomized in order to smooth the\nlearning.\nTop-level labeled layers were trained with the correct\nvisible label unit switched on and the other label units switched\noff. We pre-trained each labeled layer using CD with k=\n1for 150 epochs and then kwas linearly increased from 1\nto 10 for an additional 150 epochs.\nAfter pre-training each layer, we used discriminative\nbackpropagation to ﬁne-tune the network by backpropa-\ngating the cross-entropy label unit error to the lower lay-\ners [6]. Backpropagation was run for 400 epochs, but in\nthe end we used the model parameters which produced the\nlowest cross-entropy validation error during training.\nThis type of training relies heavily on multiplying large\nmatrices, which can be done considerably faster using highly\ndata-parallel graphics processing units (GPUs). We use\nGnumpy [12], a Python module which provides Numpy-\nlike2bindings for matrix operations on Nvidia GPUs. Us-\ning an Nvidia Tesla C2050 GPU, training the single-layer\nmodel (#1) took around 20 minutes, while the 3-layer model\n(#4) took around 30 minutes. The typical split between\npre-training time and backpropagation time was around\n60%/40%.\n4.4 Viterbi Decoding\nIn addition to simply classifying each subdivision individ-\nually, we can take into account additional sequential con-\ntext by providing the label probabilities as posterior state\nprobabilities in a hidden Markov model (HMM) [10]. In\norder to maximize coherence between successive beat sub-\ndivision estimates, we assign a high probability of a tran-\nsition to the next successive beat and give an equal divi-\nsion of the remaining probability to other transitions. Since\nour system is designed for live use, we use strictly causal\nViterbi decoding to estimate the current beat subdivision.\n5. RESULTS\n5.1 Independent Subdivision Classiﬁcation\nHere we present the classiﬁcation results for beat-measure\nalignment. The test data contains 16 beat subdivisions per\n4/4 measure, so we use 16 separate label units in the train-\ning. We were concerned with the prevalence of half-note\nsymmetry in most back-beat-oriented drum patterns. For\n2http://numpy.org\nModel 1Example Posteriors [test sequence 20]\nModel 2\nModel 3\nModel 4\n0 20 40 60 80 100 120 140\nBeat subdivision [16 per measure]Ground\nTruthFigure 6 . Example posteriors subdivision probabilities\nfrom the four models and the ground truth labels. The\ncolumns in each matrix show the posterior probability of\neach label for a particular beat subdivision.\nexample, distinguishing between the ﬁrst quarter note and\nthe third quarter note of many basic rock patterns is virtu-\nally impossible without additional contextual information.\nEven though this phenomenon caused the majority of clas-\nsiﬁcation errors, the networks seemed to do well on the\nwhole despite it.\nTable 1 shows the classiﬁcation results for each model.\nThe single layer model was signiﬁcantly outperformed by\nall multi-layer models, and adding a third layer did not\nseem to provide additional beneﬁt on our test data. Exam-\nple posterior label probabilities for each model are shown\nin Figure 6.\nDBN Conﬁguration Train Test\nAccuracy Accuracy\nL-CRBM 97.2 76.2\nCRBM!L-RBM 94.9 80.8\nCRBM!L-CRBM 91.0 83.7\nCRBM!CRBM!L-RBM 89.6 81.1\nTable 1 . Subdivision classiﬁcation accuracy for each net-\nwork conﬁguration\n5.2 With Viterbi Decoding\nNow we present the classiﬁcation results when using Viterbi\ndecoding. We were concerned there would be a tendency\nfor the high sequential state transition probability to in-\ncrease the number of classiﬁcation errors in the presence\nof half-note offset ambiguities; however, the decoding only\nseemed to help classiﬁcation. Strong half-note ambiguities\nseemed to provide strong enough evidence for both alterna-\ntives that the original independent classiﬁcation decisions\nwere typically unaffected by the Viterbi decoding.\nAs shown in Figure 7, increasing the sequential tran-\nsition probability increases the overall beat classiﬁcation\naccuracy; however, in a real-world application, one cannot\nsimply set this probability to one or else the decoder could0.2 0.4 0.6 0.8 1.0\nSequential Transition Probability0.760.780.800.820.840.860.880.90Classiﬁcation AccuracyViterbi Decoding Accuracy\n1: L-CRBM\n2: CRBM→L-RBM\n3: CRBM→L-CRBM\n4: CRBM→CRBM→L-RBMFigure 7 . Viterbi decoding classiﬁcation accuracy with in-\ncreasing sequential transition probability\nPosteriorExample Viterbi Decoding [model 2, test sequence 20]\nMax\nPosterior\nViterbi\nDecoded\n0 20 40 60 80 100 120 140\nBeat subdivision [16 per measure]Ground\nTruth\nFigure 8 . Row 1: Example posterior probabilities. Row\n2: Independent classiﬁcations. Row 3: Viterbi decoded\nclassiﬁcations. Row 4: Ground truth labels.\npossibly be locked into an incorrect beat-measure align-\nment for the entire song. The decoder must also be allowed\nto adjust its estimates when beats are purposely left out by\nthe drummer. Therefore, this parameter should be set with\nthe end use case in mind. Example viterbi decoding results\nare shown in Figure 8.\n6. DISCUSSION\nOur results show the beneﬁt of using a multi-layer neural\nnetwork that is pre-trained as a deep belief network for an-\nalyzing drum patterns; however, it is likely that the actual\noptimal network conﬁguration will be highly dependent on\nthe diversity of the drum patterns in the dataset.\nThe results in Table 1 show an inverse relationship be-\ntween training and test accuracy, which suggests overﬁt-\nting was occurring during backpropagation. Subsequent\nwork should focus on a more robust evaluation of the re-\nsults using a larger dataset, cross-validation, and more at-\ntention to regularization techniques. In addition, compari-\nson with existing drum pattern analysis methods is neces-\nsary.\nAlthough, we do not objectively evaluate the use of these\nmodels for generating drum patterns, it is important to note\nthat because the RBM is inherently a generative model,these networks are especially well-suited to serve as stochas-\ntic drum machines. Even a single labeled-CRBM works\nwell for this purpose, and turning on the label unit of the\ndesired subdivision during Gibbs sampling helps increase\nthe metric stability of the generated patterns.\nThis type of model has signiﬁcant potential to be of use\nin many music information retrieval and computer music\ntasks. We plan to explore the ability of the model to dis-\ncriminate between rhythmic styles, discern between dif-\nferent time signatures, or to detect rhythmic transitions or\nﬁlls. We also plan to do a more in-depth evaluation of the\ngenerative abilities of the model as well as to pursue meth-\nods which will allow interactive improvisation between hu-\nman and computer performers.\nAdditional information as well as the dataset and code\nused in this work will be made available at:\nhttp://www.eecs.berkeley.edu/ ˜ericb/ .\n7. REFERENCES\n[1] E Battenberg, V Huang, and D Wessel. Toward live drum separa-\ntion using probabilistic spectral clustering based on the itakura-saito\ndivergence. Proc. AES Conference on Time-Frequency Processing ,\n2011.\n[2] Y Bengio. Learning deep architectures for AI. Foundations and\nTrends in Machine Learning , 2009.\n[3] M Goto. An audio-based real-time beat tracking system for mu-\nsic with or without drum-sounds. Journal of New Music Research ,\n30(2):159–171, 2001.\n[4] P Hamel and D Eck. Learning features from music audio with deep\nbelief networks. Proc. of the 11th International Society for Music In-\nformation Retrieval Conference (ISMIR 2010) , pages 339–344, 2010.\n[5] G Hinton and S Osindero. A fast learning algorithm for deep belief\nnets. Neural Computation , 2006.\n[6] G. E. Hinton. To recognize shapes, ﬁrst learn to generate images.\nProgress in brain research , 165:535–547, 2007.\n[7] AP Klapuri, AJ Eronen, and JT Astola. Analysis of the meter of\nacoustic musical signals. IEEE Transactions on Speech and Audio\nProcessing , 14(1):342, 2006.\n[8] A Mohamed and G Dahl. Deep belief networks for phone recogni-\ntion. NIPS Workshop on Deep Learning for Speech Recognition and\nRelated Applications , 2009.\n[9] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\nboltzmann machines. Proc. 27th International Conference on Ma-\nchine Learning , 2010.\n[10] L.R. Rabiner. A tutorial on hidden markov models and selected appli-\ncations in speech recognition. Proceedings of the IEEE , 77(2):257–\n286, 1989.\n[11] G Taylor and G Hinton. Two Distributed-State Models For Gener-\nating High-Dimensional Time Series. Journal of Machine Learning\nResearch , 2011.\n[12] T Tieleman. Gnumpy: an easy way to use GPU boards in Python.\nTechnical Report 2010-002, University of Toronto, 2010.\n[13] M Welling and M Rosen-Zvi. Exponential family harmoniums with\nan application to information retrieval. Advances in Neural Informa-\ntion Processing Systems , 2005.\nResearch supported by Microsoft (Award #024263) and Intel (Award #024894)\nfunding and by matching funding by U.C. Discovery (Award #DIG07-\n10227). Additional support comes from Par Lab afﬁliates National In-\nstruments, Nokia, NVIDIA, Oracle, and Samsung."
    },
    {
        "title": "Second Fiddle is Important Too: Pitch Tracking Individual Voices in Polyphonic Music.",
        "author": [
            "Mert Bay",
            "Andreas F. Ehmann",
            "James W. Beauchamp",
            "Paris Smaragdis",
            "J. Stephen Downie"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418121",
        "url": "https://doi.org/10.5281/zenodo.1418121",
        "ee": "https://zenodo.org/records/1418121/files/BayEBSD12.pdf",
        "abstract": "Recently, there has been much interest in automatic pitch estimation and note tracking of polyphonic music. To date, however, most techniques produce a representation where pitch estimates are not associated with any particular in- strument or voice. Therefore, the actual tracks for each instrument are not readily accessible. Access to individ- ual tracks is needed for more complete music transcrip- tion and additionally will provide a window to the anal- ysis of higher constructs such as counterpoint and instru- ment theme imitation during a composition. In this paper, we present a method for tracking the pitches (F0s) of indi- vidual instruments in polyphonic music. The system uses a pre-learned dictionary of spectral basis vectors for each note for a variety of musical instruments. The method then formulates the tracking of pitches of individual voices in a probabilistic manner by attempting to explain the input spectrum as the most likely combination of musical instru- ments and notes drawn from the dictionary. The method has been evaluated on a subset of the MIREX multiple-F0 estimation test dataset, showing promising results.",
        "zenodo_id": 1418121,
        "dblp_key": "conf/ismir/BayEBSD12",
        "keywords": [
            "Automatic pitch estimation",
            "Note tracking of polyphonic music",
            "Pitch estimates not associated with instruments",
            "Individual instrument tracks",
            "Complete music transcription",
            "Analysis of higher constructs",
            "Counterpoint and instrument theme imitation",
            "Pre-learned dictionary of spectral basis vectors",
            "Probabilistic pitch tracking",
            "MIREX multiple-F0 estimation test dataset"
        ],
        "content": "SECOND FIDDLE IS IMPORTANT TOO:\nPITCH TRACKING INDIVIDUAL VOICES IN POLYPHONIC MUSIC\nMert Bay2, Andreas F. Ehmann2, James W. Beauchamp2, Paris Smaragdis1,2and J. Stephen Downie3\n1Department of Computer Science, University of Illinois at Urbana-Champaign\n2Department of Electrical & Computer Eng., University of Illinois at Urbana-Champaign\n3Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign\nfmertbay,aehmann,jwbeauch,paris,jdownieg@illinois.edu\nABSTRACT\nRecently, there has been much interest in automatic pitch\nestimation and note tracking of polyphonic music. To date,\nhowever, most techniques produce a representation where\npitch estimates are not associated with any particular in-\nstrument or voice. Therefore, the actual tracks for each\ninstrument are not readily accessible. Access to individ-\nual tracks is needed for more complete music transcrip-\ntion and additionally will provide a window to the anal-\nysis of higher constructs such as counterpoint and instru-\nment theme imitation during a composition. In this paper,\nwe present a method for tracking the pitches (F0s) of indi-\nvidual instruments in polyphonic music. The system uses\na pre-learned dictionary of spectral basis vectors for each\nnote for a variety of musical instruments. The method then\nformulates the tracking of pitches of individual voices in\na probabilistic manner by attempting to explain the input\nspectrum as the most likely combination of musical instru-\nments and notes drawn from the dictionary. The method\nhas been evaluated on a subset of the MIREX multiple-F0\nestimation test dataset, showing promising results.\n1. INTRODUCTION\nOne of the most important classes of information to be re-\ntrieved from music is its polyphonic pitch content. Re-\ncently, many researchers have attempted multiple-F0 esti-\nmation (MFE) [11, 14, 18]. (A review of state-of-the-art\nMFE systems can be found in [2].) Many current MFE\nsystems such as [14] restrict themselves to the estimation\nof a certain number of F0s for each frame, while ignoring\nwhich instrument/timbre corresponds to each F0. While\napproaches for tracking solo melodic voice/timbres exist\n[15], the pitch tracking of additional lines and parts in poly-\nphonic music opens new possibilities. By exposing the in-\ndividual lines produced by each instrument in polyphonic\nmusic, aspects such as counterpoint, the appearance of leit-\nmotifs across instruments in a piece, and hidden thematic\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.references across musical pieces can be uncovered. There-\nfore, instruments and their timbres are very important com-\nponents in music and should be tracked along with their\nF0s. Moreover, knowing each instrument’s F0 track can\nbe very beneﬁcial for a variety of MIR user applications,\nsuch as music transcription, score alignment, audio music\nsimilarity, cover song identiﬁcation, active music listening,\nmelodic similarity, harmonic analysis, intelligent equaliza-\ntion, and F0-guided source separation.\nAs stated previously, most MFE systems produce low-\nlevel representations of the polyphonic pitch content pre-\nsent in music audio which report only what fundamen-\ntal frequencies are present at each given time. However\nhigher-level representations can attempt to track and link\nthese fundamental frequencies over time to form notes such\nas described in [3]. It is important to note that such track-\ning can also improve the accuracy of MFE’s based purely\non individual frames, as reported reported in [18]. In [12] a\nclassiﬁcation approach is used to determine singing voice\nportions in music audio so as to build the pitch-track corre-\nsponding to a vocal melody. In [5, 6], a frame level multi-\nF0 estimation method is used followed by a constrained\nclustering method that uses harmonic amplitude-based fea-\ntures to cluster pitches into pitch tracks. In these cases, to\nbuild instrument or timbre-speciﬁc pitch tracks, bottom-up\nmethods were used that ﬁrst produced frame-based pitch\nestimates and subsequently sometimes attempted to build\nnote-level representations, which may form solo phrases.\nIn the method presented in this paper, we use an alter-\nnative approach. Instead of building timbre tracks from\nthe pitch content, our proposed approach uses timbre infor-\nmation to guide the formation of instrument-speciﬁc pitch\ntracks. This paper is organized as follows. Section 2 de-\ntails the proposed method. Section 3 describes the evalua-\ntion datasets, measures, and results. The evaluation results\nare discussed in Section 4 and conclusions and future work\ncovered in Section 5.\n2. PROPOSED METHOD\nTo make a system that tracks pitches attributed to differ-\nent musical instruments, we borrow an idea from the su-\npervised sound source separation domain: using a spectral\nlibrary [1, 13]. By training our system on example soundsfrom different instruments, we can track them in complex\nsound mixtures. The proposed method is based on prob-\nabilistic latent component analysis (PLCA) [17]. PLCA\nis a variant of non-negative matrix factorization (NMF)\nand has been used widely to model sounds in the spec-\ntral domain. Its probabilistic interpretation makes it ex-\ntensible to using priors and statistical techniques. We use\nregular PLCA to build dictionaries of spectra indexed by\nF0 and instrument where the spectra are analyzed from\nrecordings of individual notes of different musical instru-\nments. We extend the model of [16] to represent each\nsource/instrument not by only one spectral dictionary, but\nrather with a collection of dictionaries, each of which is\nan ensemble of spectral basis vectors that have the same\nF0. We model the input music signal’s spectrum as a sum\nof basis vectors from F0-specﬁc spectrum dictionaries for\ndifferent instruments. Update rules are designated to cal-\nculate the model parameters, which are estimated proba-\nbilities for the occurrence of each spectral basis vector, F0-\nspectrum dictionary, and instrument in the input mixture at\na given time. Finally, we perform the Viterbi algorithm [8]\nto track the most likely pitch sequence for each instrument.\nA sinusoidal model is used for the time-frequency rep-\nresentation because of its compactness and also for rep-\nresenting each note independent of the intonation errors or\ntuning differences between training and test set performers.\nWe begin by performing a short-time Fourier transform on\nthe audio signal. Peaks in the spectrum are then deter-\nmined using a frequency-dependent threshold as described\nin [7]. We then reﬁne each peak’s amplitude and frequency\nusing a signal derivative method proposed by [4].\n2.1 Model\nWe model the audio input mixture’s spectrum for each fra-\nme as a sum of instrument tones where each tone is rep-\nresented by a dictionary of spectral basis vectors that are\nlearned in advance.\nIt is assumed that all instrument tones have harmon-\nically related frequencies which are integer multiples of\nan F0 frequency. It turns out that in a mixture of such\ntones there is a high probability that harmonics will over-\nlap. The input spectrum can be modeled as a distribu-\ntion/histogram over a range of frequencies. E.g., each com-\nponent is viewed as the probability of occurrence of that\nparticular frequency. The magnitude at any particular fre-\nquency can be thought as an accumulation of magnitudes\nfrom various instruments due to component overlapping.\nThe scaled version of the input mixture spectrum is mod-\neled as a discrete distribution. The generative process is\nmodeled as follows:\nXt(f)\u0018=Pt(f)=IP\niPt(i)NP\nn2piPt(pji)KP\nz2zpiPt(zjp)Ppi(\nfjz)(1)\nwhereXt(fj)is the spectral magnitude of the peak jat\nfrequencyfjfor the observed input mixture\nspectrum at\ntimet;Pt(f)is an approximation of the input spectrum;\nPt(i)is the estimated probability of occurrence of instru-\nmentiat timet, whereasPt(pji)is the estimated prob-\nability that pitch pis produced by instrument i;P(fjz)is the learned spectral basis vector for the pitch pof in-\nstrumenti; andPt(zjp)is the probability (weight) of that\nbasis vector. The above model explains the mixture mag-\nnitude spectrum hierarchically as the sum of Nindividual\npitches from Idifferent instruments where the dictionary\ncorresponding to each pitch/instrument has Kelements.\nThe independence relationships of the model can be repre-\nsented by the graph I!P!Z!F. The process that gener-\nates the frequency components in the observed magnitude\nspectrum is as follows: First, an individual instrument li-\nbrary is selected from a group of instrument libraries. Sec-\nond, a spectrum dictionary is drawn for each pitch from\nthe instrument library. Third, a spectral basis vector is\ndrawn from a particular F0-spectrum dictionary for the in-\nstrument. Fourth, a spectral component at a particular fre-\nquency is drawn from the spectral basis vector. Although\nit is possible that this spectral component will be generated\nby only a single instrument, most often it only contributes\na fraction to the magnitude of the observed spectrum at that\nfrequency.\n2.2 Parameter Estimation\nParameters for the model \u0012=fPt(zjp);Pt(pji);Pt(i)g\ncan be estimated using an expectation-maximization (EM)\nalgorithm. In the E-step, current parameter values \u0012oldare\nused to calculate the posterior distribution.\nPt(i;p;zjf;\u0012old) =Pt(fji;p;z )Pt(i;p;z )\nPt(f)(2)\nBecause of the structure of the model, Pt(fji;p;z) =Pt(fjz)\nandPt(i;p;z ) =Pt(i)Pt(pji)Pt(zjp\n). We can write the\nposterior as\nPt(i;p;zjf;\u0012old) =P(fjz)Pt(zjp)\nPt(pji)Pt(i)P\niPt(i)P\nn2piPt(pji)P\nz2zpiPt(zjp)Ppi(fjz)(3)\nThe posterior is used to calculate the expectation of the\ncomplete data log\nlikelihoodQ\nQ(\u0012;\u0012old) =P\nfXfP\niP\nn2piP\nz2zpiP(i;p;zjf;\n\u0012old)log(P(i;p;z;fj\u0012)(4)\nIn the M-step, new parameters are estimated by maximiz-\ning the abov\ne function according to \u0012, resulting in the fol-\nlowing update rules:\nPt(zjp)new P\nfPt(i;p;zjf)Xt(f)\nP\nz2zpiP\nfPt(i;p;zjf)Xt(f)(5)\nPt(pji)new P\nz2zpiP\nfPt(i;p;zjf)Xt(f)\nP\np2piP\nz2zpiP\nfPt(i;p;zjf)Xt(f)(6)\nPt(i)new P\np2piP\nz2zpiP\nfPt(i;p;zjf)Xt(f)\nP\niP\np2piP\nz2zpiP\nfPt(i;p;zjf)Xt(f)(7)We then use the new estimates to calculate the posterior in\nan iterative manner and repeat until convergence is achieved.\n2.3 Sparsity Prior\nAt any given time, we expect each instrument active to be\nplaying a single pitch. Even though the parameter esti-\nmation method would allow multiple pitches per frame for\neach instrument, in this project our goal is to track a mono-\nphonic pitch contour for each instrument. We also expect\nthat not all instruments are active at the same time.\nEnforcing sparsity constraints on note and instrument\nprobabilities Pt(pji)’s andPt(i )’s reinforce this behavior.\nWe use the following prior (the normalizing constant is\ndropped for convenience)\nP(\u001e)= \nP\n\u001e(P(\u001e))\u000b!\f\n(8)\nAdding the above prior to the expectation of the\ncomplete\ndata log-likelihood and maximizing it with respect to P(i)\nandP(pji), we arrive at the following update rules\nP(pji)new P\nz2zpiP\nfP(i;p;zjf)Xf+\fp(Pt(pji))\u000b\nP\np2pi(Pt(pji))\u000b(9)\nPt(i)new P\np2piP\nz2zpiP\nfPt(i;p;zjf)Xt(f) +\fp(Pt(i))\u000b\nP\np2i(Pt(i))\u000b(10)\n(We have to rescale explicitly so that it sums\nup to one.)\nP(pji)new P(pji)new\nP\np2piP(i)newandP(i)new P(i)new\nP\niP(i)new (11)\n2.4 Enforcing Continuity\nOur goal is to track each instrument’s F0\nthrough time. We\nexpect the pitch contour to be smooth, not changing drasti-\ncally from frame to frame except at note transitions. Using\nthe Viterbi algorithm, we treat the pitches as hidden states\nand pose the instrument tracking problem as one of infer-\nring the most likely pitch state sequence for each instru-\nment. Above estimated pitch distributions (which maxi-\nmize the mixture likelihood P(Xt(f)ji;p;z )) for each in-\nstrument are considered to be the emission probability of\nthe hidden state in a hidden Markov model (HMM). Tran-\nsition probabilities are modeled as normal distributions gi-\nven by\nP(ptjp0\nt\u00001)=1p\n2\u0019\u001be\u0000\u0012\nf0t\u0000f0\n0t\u00001\u00132\n2\u001b2 (12)\nwhereptdenotes the hidden pitch state for instrument i\nat timet.f0tdenotes the F0 associated with\nthe hidden\npitch state for instrument iat timet. Transitions are calcu-\nlated within the same instrument. We empirically choose\n\u001b= 7 +f=100Hz . The above distribution enforces the\ncontinuity of the active notes from frame to frame.3. EVALUATION ON REAL WORLD DATA\nWe trained a dictionary for each pitch of each instrument\nusing the RWC musical instrument database [9] using non-\nnegative matrix factorization with Kullback-Leibler diver-\ngence which is numerically equal to the regular PLCA met-\nhod in 2 dimensions [17]. For each pitch from the RWC\ndataset, 20 representative spectra were derived from 27 dif-\nferent tones corresponding to 3 players, 3 dynamics (pi-\nano, mezzo, forte), and 3 articulations (normal, staccato,\nvibrato). In the pitch tracking stage, we limit the num-\nber of instrument libraries to choose from by designating\nthe instruments expected to be in the input mixture as in-\nput to the algorithm. We also limit the search range for F0\n(which F0-spectrum dictionaries to use) of each instrument\nby only using the peaks estimated by the sinusoidal model\nthat are between 50 Hz to 2500 Hz as pitch candidates.\nEvaluations are performed at frame level. Multi-F0 track-\ning problem is similar to melody extraction problem ex-\ntended to multiple melodies as opposed multi-F0 estima-\ntion problem where the estimated number of F0s for each\nframe can be different than the ground-truth F0s, which is\nnot the case in the tracking problem, where the number of\nestimated F0s and the reference F0s are simply equal to\ntotal number of frames. We extended the evaluation met-\nrics from MIREX melody extraction task to our problem.\nComparing the voiced (nonzero F0) and unvoiced (zero F0)\nvalues for each frame of the estimated and ground-truth\nF0 tracks, the counts for true positives (TP), true negatives\n(TN), false positives (FP) and false negatives (FN) are cal-\nculated according to Table 1\nEstimated\nvoiced unvoiced\nGround voiced TP FN\ntruth unvoiced FP TN\nTable 1. Evaluation\nTP’s further break down into ones with correct F0 and the\nones with incorrect F0 as TP =TPC +TPI . Estimated\nvoiced F0 is correct if it is within a quarter tone (+-2.93%)\nrange of a positive ground-truth F0 for that frame.\nThe precision, recall, F-measure, and accuracy for each\ninput test ﬁle is then calculated over all frames and all in-\nstruments as:\nPrecision =P\niP\ntTPCi;t\nP\niP\ntTPi;t+FPi;t(13)\nRecall =P\niP\ntTPCi;t\nP\niP\ntTPi;t+FNi;t(14)\nF-measure =2\u0002precision\u0002recall\nprecision +recall(15)\nAcc. =P\niP\ntTPCi;t+TNi;t\nP\niP\ntTPi;t+FPi;t+TNi;t+FNi;t(16)wheretis the frame index and iis the instrument index.\nWe tested the proposed method on different datasets. The\nground-truths for these datasets were estimated using mono-\nphonic pitch estimators (Wavesurfer, Praat and YIN ) on the\nsingle-instrument recordings prior to mixing. The results\nof the monophonic pitch estimators are manually corrected\nwhere necessary.\nFor preliminary testing and development, we applied\nthe method on a 11 second excerpt taken from a real world\nperformance by bassoon, clarinet and oboe which was taken\nfrom a MIREX multitrack dataset (standard woodwind quin-\ntet transcribed from L. van Beethoven ”Variations for String\nQuartet”, Op.18, N.5). The three separate tracks were mixed\nto monaural. The results can be seen in Table 2. The pro-\nposed method scored 0.83 accuracy on average. Figure 1\nshows the multiple-F0 tracks for each instrument. Without\ntracking, the F0’s would not be connected from frame to\nframe.\nBassoon Clarinet Oboe Ave.\nAccuracy 0.76 0.85 0.89 0.83\nPrecision 0.76 0.85 0.89 0.83\nRecall 0.82 0.92 0.90 0.88\nF-measure 0.79 0.88 0.89 0.86\nTable 2. Evaluation performances for a 11-s woodwind\ntrio excerpt (bassoon, clarinet, oboe).\n0 2 4 6 8 1050556065707580\nTime (sec)Midi note number\n  \nGround−truth oboe\nEstimated oboe\nGround−truth clarinet\nEstimated clarinet\nGround−truth Bassoon\nEstimated Bassoon\nFigure 1. Pitch (in midi note numbers) vs. time using\nthe proposed system on the 11-s woodwind trio excerpt\n(bassoon (lower), clarinet (middle), oboe (upper)). Thin\nlines represent the ground-truth.\nWe then tested the proposed method on two datasets\nused in the MIREX multiple-F0 task test set [2]. The ﬁrst\none is a multitrack recording of the woodwind quintet men-\ntioned above [2]. The piece is highly contrapuntal as op-\nposed to consisting of a lone melodic voice plus accom-\npaniment. The predominant melodies alternate between\ninstruments. The F0 tracks often cross each other. Fivenon-overlapping 30-second sections were chosen from the\nrecording. Isolated instruments were mixed from solo tracks\nto polyphonies ranging from 2 (duo) to 5 (quintet), result-\ning in a total of 20 tracks (4 different polyphonies times\nﬁve sections). The average pitch-tracking results over all\ntracks for polyphony 2 to 5 are shown in Table 3.\nAccuracy Precision Recall F-measure\n0.52 0.50 0.56 0.53\nTable 3. Average performance on the MIREX woodwind\nquintet dataset.\nFigure 2 shows a bar graph of the performance of the\nmethod for different polyphonies. Figure 3 shows the av-\nerage accuracies for different instruments in various poly-\nphonies.\n2 3 4 500.10.20.30.40.50.60.7\nPolyphony  \nAccuracy\nPrecision\nRecall\nF−measure\nFigure 2. Performance vs. polyphony for the\nMIREX woodwind quintet dataset (Five 30-s segments per\npolyphony).\n2 3 4 50.20.40.60.8\nPolyphonyAve. Accuracy\n  \nBassoon\nClarinet\nHorn\nOboe\nFlute\nFigure 3. Ave. accuracy of individual instruments for\ndifferent polyphonies for the MIREX woodwind quintet\ndataset. (Five 30-s segments)\nThe second dataset we tested our method on is a record-\ning of a four-part J.S. Bach chorales created by [5] consist-ing of bassoon, clarinet, saxophone, and viola. Four 30\nseconds sections were mixed from duo to quartet, resulting\nin 12 tracks (2 different polyphonies times 4 sections). The\naverage results over all tracks in this dataset can be seen in\nTable 4.\nAccuracy Precision Recall F-measure\n0.59 0.55 0.55 0.55\nTable 4. Average performance for the MIREX Bach\nchorales dataset\nFigure 4 shows a bar graph of the performance of the\nmethod for different polyphonies. Figure 5 shows the av-\nerage accuracies of different instruments in various poly-\nphonies. The RWC dataset of the MIREX multiple-F0 task\nwas not evaluated because RWC samples were used for\ntraining. Also the piano dataset was not used because our\nproject’s goal is to track distinct timbres.\n2 3 400.10.20.30.40.50.60.70.8\nPolyphony  \nAccuracy\nPrecision\nRecall\nF−measure\nFigure 4. Ave. Performance vs. polyphony for the\nMIREX Bach chorales dataset. (Four 30-s segments)\n2 3 40.40.60.8\nPolyphonyAve. Accuracy\n  \nBassoon\nClarinet\nSaxophone\nViolin\nFigure 5. Ave. accuracy of instruments in differ-\nent polyphonies for the MIREX Bach chorales quintet\ndataset.(Four 30-s segments)4. DISCUSSION\nThe proposed method on average performed with 83% ac-\ncuracy on identifying the instrument tracks for the trio case\n(Figure 1) which was used for the development of the al-\ngorithm. Accuracies were 52% and 59% for the MIREX\nwoodwind quintet and Bach chorales quartet datasets. By\nexamining the MIREX dataset results, we see that most\nproblems are caused by an inactive instrument following\nthe dominant instrument’s F0 track. Some instruments in\nthis dataset are inactive during 70-80% of the entire dura-\ntion of the input. Accuracy-vs.-instrument results for the\nMIREX woodwind quintet (Figure 3), indicate that instru-\nments horn and clarinet have lower performance in the 4\nand 5 polyphony cases, due to their F0 tracks being highly\nsparse. In addition to remaining silent much of the time,\nthey often play soft notes when they are active.\nThe tracking system reports note probabilities for every\nnon-silent frame, which are then used in an HMM to es-\ntimate the F0 tracks. V oicing decisions are based strictly\non the rms amplitude of the mixture input signal to de-\ncide whether the signal is silent. This results in a high\nnumber of false positives when an individual instrument\nis silent in parts of a track, a condition which happens of-\nten in the MIREX dataset. This behavior also results in a\nvery low number of true and false negatives (since the sys-\ntem reports very few negatives) resulting in a higher recall\nthan lower precision which can be seen in Figures 2 and\n4. In the future, we would like to explore methods to infer\nwhether instruments are active or not at any given point in\ntime.\nAnother kind of error that frequently occurs is when a\nless dominant instrument tracks a more dominant one that\nhas a similar timbre. This is probably caused by the instru-\nment spectra having signiﬁcant correlation with each other.\nLooking at the performance-vs.-polyphony results for each\ninstrument in Figure 5, we see that instruments like violin,\nwhich have a unique timbre, have better average accuracy.\nPossible solutions include training the instrument spec-\ntrum dictionaries not in isolation but in combination with\nother instrument spectra, which may assist the basis vec-\ntors behaving in a more discriminant way. Methods for dis-\ncriminant non-negative tensor factorizations are explored\nin [19]. This issue can also be addressed in the testing part.\nPitch probabilities in EM iterations can be estimated to be\nas maximally different as possible, while still explaining\nthe overall mixture by appropriate use of priors. Another\nmethod that might improve this issue would be to use a\nfactorial HMM to jointly estimate the pitch tracks.\nOn average, the proposed method scored 0.53 for Ac-\ncuracy on the MIREX dataset, which is an improvement\nover the only past result (0.21) for the multiple-F0 track-\ning task evaluated at MIREX [10]. However, we note that\nthe MIREX multiple-F0 task did not offer the opportunity\nto utilize instrument names for the mixture input test ﬁles.\nOne reason the proposed method works comparatively\nwell for the trio and the Bach chorales case (see Table 1\nand Figure 1) is that most instruments were active most of\nthe time. The encouraging results from this case lead us tobelieve that the pitch tracking problems can be improved\neffectively by addressing the issues discussed above.\nFinally, we note that the restriction that sounds must\nconsist soley of harmonic partials can be relaxed. E.g.,\npitch estimates for instruments like xylophone, which do\nnot have strict harmonic structures but do have predictable\ninharmonic structures, can be determined.\n5. CONCLUSION\nA new method for pitch and instrument tracking of indi-\nvidual instruments in polyphonic music has been designed\nand evaluated on an established dataset that has previously\nbeen used for multiple-F0 estimation under MIREX. Cur-\nrent results are encouraging, but several problems need to\nbe resolved (as described in the Discussion section) for the\nmethod to be an effective tool.\nAs mentioned in the Introduction, knowing the F0 tracks\ncan be very beneﬁcial for a variety of MIR tasks. In the fu-\nture, we would like to explore the use of voicing detection\nto determine which instruments are active. We would also\nlike to perform instrument identiﬁcation in the front-end\nso the method does not require prior knowledge of the in-\nstrumentation. We also plan to experiment with different\ndiscriminant learning methods and to re-infer the dictio-\nnaries based on the input mixture. Finally, we propose to\nexplore using an automatic key detection algorithm and a\nmore musicologically informed pitch transition matrix for\nthe hidden Markov model.\n6. REFERENCES\n[1] M. Bay and J. Beauchamp. Harmonic source separa-\ntion using prestored spectra. Proc. Independent Com-\nponent Analysis and Blind Signal Separation, pages\n561–568, 2006.\n[2] M. Bay, A.F. Ehmann, and J.S. Downie. Evaluation of\nmultiple-F0 estimation and tracking systems. In Proc.\n10th Int. Conf. for Music Information Retrieval (ISMIR\n2009), pages 315–320, 2009.\n[3] W.C. Chang, A.W.Y . Su, C. Yeh, A. Roebel, and\nX. Rodet. Multiple-f0 tracking based on a high-order\nhmm model. In Proc. 11th Int. Conf. Digital Audio Ef-\nfects (DAFx-08), pages 379–386, 2008.\n[4] M. Desainte-Catherine and S. Marchand. High-\nprecision Fourier analysis of sounds using signal\nderivatives. J. Audio Eng. Soc, 48(7/8):654–667, 2000.\n[5] Z. Duan, J. Han, and B. Pardo. Harmonically informed\nmulti-pitch tracking. In Proc. 10th Int. Conf. on Music\nInformation Retrieval (ISMIR 2009), pages 333–338,\n2009.\n[6] Z. Duan, J. Han, and B. Pardo. Song-level multi-pitch\ntracking by heavily constrained clustering. In Proc.\nIEEE Int. Conf. on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP 2010), pages 57–60, 2010.[7] M.R. Every and J.E. Szymanski. Separation of syn-\nchronous pitched notes by spectral ﬁltering of harmon-\nics.IEEE Trans. Audio, Speech and Language Process-\ning, 14(5):1845–1856, 2006.\n[8] G.D. Forney Jr. The Viterbi algorithm. Proceedings of\nthe IEEE, 61(3):268–278, 1973.\n[9] M. Goto. Development of the RWC music database.\nInProc. 18th Int. Congress on Acoustics (ICA 2004),\nvolume 1, pages 553–556, 2004.\n[10] IMIRSEL. Multiple fundamental frequency es-\ntimation and tracking results wiki. http:\n//www.music-ir.org/mirex/wiki/2010:\nMultiple_Fundamental_Frequency_\nEstimation_%26_Tracking_Results, 2010.\n[11] A. Klapuri. Multipitch analysis of polyphonic music\nand speech signals using an auditory model. IEEE\nTrans. on Audio, Speech, and Language Processing,\n16(2):255–266, 2008.\n[12] R. Marxer, J. Janer, and J. Bonada. Low-latency instru-\nment separation in polyphonic audio using timbre mod-\nels.Proc. 10th Int. Conf. on Latent Variable Analysis\nand Signal Separation, pages 314–321, 2012.\n[13] G. Mysore, P. Smaragdis, and B. Raj. Non-negative\nhidden markov modeling of audio with application to\nsource separation. Proc. 8th Int. Conf. on Latent Vari-\nable Analysis and Signal Separation, pages 140–148,\n2010.\n[14] A. Pertusa and J.M. Inesta. Multiple fundamental fre-\nquency estimation using gaussian smoothness. In Proc.\nIEEE Int. Conf. on Acoustics, Speech and Signal Pro-\ncessing,(ICASSP 2008), pages 105–108, 2008.\n[15] G.E. Poliner, D.P.W. Ellis, A.F. Ehmann, E. G ´omez,\nS. Streich, and B. Ong. Melody transcription from mu-\nsic audio: Approaches and evaluation. IEEE Trans. on\nAudio, Speech, and Language Processing, 15(4):1247–\n1256, 2007.\n[16] B. Raj and P. Smaragdis. Latent variable decomposi-\ntion of spectrograms for single channel speaker sep-\naration. In Proc. IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics, (WASPAA\n2005), pages 17–20, 2005.\n[17] P. Smaragdis, B. Raj, and M. Shashanka. A probabilis-\ntic latent variable model for acoustic modeling. Proc.\n20th Conf. on Neural Information Processing Systems\n(NIPS 2006), 148, 2006.\n[18] C. Yeh, A. Roebel, and X. Rodet. Multiple fundamental\nfrequency estimation and polyphony inference of poly-\nphonic music signals. IEEE Trans. on Audio, Speech,\nand Language Processing, 18(6):1116–1126, 2010.\n[19] S. Zafeiriou. Discriminant nonnegative tensor factor-\nization algorithms. IEEE Trans. on Neural Networks,\n20(2):217–235, 2009."
    },
    {
        "title": "Automatic Music Transcription: Breaking the Glass Ceiling.",
        "author": [
            "Emmanouil Benetos",
            "Simon Dixon",
            "Dimitrios Giannoulis",
            "Holger Kirchhoff",
            "Anssi Klapuri"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415088",
        "url": "https://doi.org/10.5281/zenodo.1415088",
        "ee": "https://zenodo.org/records/1415088/files/BenetosDGKK12.pdf",
        "abstract": "Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, al- though the field is still very active. In this paper we analyse limitations of current methods and identify promising di- rections for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to over- come the limited performance of transcription systems, al- gorithms have to be tailored to specific use-cases. Semi- automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich po- tential source of training data, via forced alignment of au- dio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects.",
        "zenodo_id": 1415088,
        "dblp_key": "conf/ismir/BenetosDGKK12",
        "keywords": [
            "Automatic music transcription",
            "Holy Grail",
            "Human expert performance",
            "Current methods limitations",
            "Specific use-cases",
            "Semi-automatic approaches",
            "Training data",
            "Forced alignment",
            "Integration of methods",
            "Musical aspects"
        ],
        "content": "AUTOMATIC MUSIC TRANSCRIPTION:\nBREAKING THE GLASS CEILING\nEmmanouil Benetos, Simon Dixon, Dimitrios Giannoulis, Holger Kirchhoff, and Anssi Klapuri †\nCentre for Digital Music, Queen Mary University of London\nfemmanouilb,simond,dimitrios,holger,anssikg@eecs.qmul.ac.uk\nABSTRACT\nAutomatic music transcription is considered by many to\nbe the Holy Grail in the ﬁeld of music signal analysis.\nHowever, the performance of transcription systems is still\nsigniﬁcantly below that of a human expert, and accuracies\nreported in recent years seem to have reached a limit, al-\nthough the ﬁeld is still very active. In this paper we analyse\nlimitations of current methods and identify promising di-\nrections for future research. Current transcription methods\nuse general purpose models which are unable to capture\nthe rich diversity found in music signals. In order to over-\ncome the limited performance of transcription systems, al-\ngorithms have to be tailored to speciﬁc use-cases. Semi-\nautomatic approaches are another way of achieving a more\nreliable transcription. Also, the wealth of musical scores\nand corresponding audio data now available are a rich po-\ntential source of training data, via forced alignment of au-\ndio to scores, but large scale utilisation of such data has\nyet to be attempted. Other promising approaches include\nthe integration of information across different methods and\nmusical aspects.\n1. INTRODUCTION\nAutomatic music transcription (AMT) is the process of\nconverting an audio recording into some form of musical\nnotation. AMT applications include automatic retrieval of\nmusical information, interactive music systems, as well as\nmusicological analysis [28]. Transcribing polyphonic mu-\nsic is a nontrivial task and while the problem of automatic\npitch estimation for monophonic signals can be considered\nsolved, the creation of an automated system able to tran-\nscribe polyphonic music without restrictions on the degree\nof polyphony or the instrument type still remains open. In\nthis work we will be addressing the problem of polyphonic\ntranscription; for an overview of melody transcription ap-\nproaches the reader can refer to [39].\n† Equally contributing authors. We acknowledge the support of\nthe MIReS project, supported by the European Commission, FP7, ICT-\n2011.1.5 Networked Media and Search Systems, grant agreement No\n287711. E. Benetos is funded by a Queen Mary University of London\nWestﬁeld Trust Research Studentship. D. Giannoulis and H. Kirchhoff\nare funded by a Queen Mary University of London CDTA Studentship.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2012 International Society for Music Information Retrieval.The core problem for creating an AMT system is the\ndetection of multiple concurrent pitches. In past years the\nmajority of multi-pitch detection methods employed a com-\nbination of audio feature extraction and heuristic techniques,\nwhich also produced the best results in the MIREX multi-\nF0 (frame-wise) and note tracking evaluations [5,33]. One\ncommonly used technique of these methods is the iterative\nspectral subtraction approach of [27]. The best performing\nmethod in the MIREX multi-F0 and note tracking task is\nthe work by Yeh [45], who proposed a joint pitch estima-\ntion algorithm based on a pitch candidate set score func-\ntion, which is based on several audio features.\nAnother set of approaches formulates the frame-wise\nmultiple-F0 estimation problem within a statistical frame-\nwork. The problem can then be viewed as a maximum a\nposteriori (MAP) estimation problem:\n^c= arg max\nc2CL(cjx) (1)\nwhere c=fF1\n0;:::;FN\n0gis a set of fundamental frequen-\ncies,Cis the set of all possible F0 mixtures, and xis the\nobserved audio signal within a single analysis frame. If\nno prior information is speciﬁed, the problem can be ex-\npressed as a maximum likelihood (ML) estimation prob-\nlem using Bayes’ rule, e.g. [11, 14]. A related method\nwas proposed in [37], using a generative model with a non-\nhomogeneous Poisson process.\nFinally, the majority of recent transcription papers utilise\nand expand spectrogram factorisation techniques (e.g. [7,\n10]). Non-negative matrix factorisation (NMF) is a tech-\nnique ﬁrst introduced as a tool for music transcription in\n[43]. In its simplest form, the NMF model decomposes an\ninput spectrogram X2RK\u0002N\n+ withKfrequency bins and\nNframes as:\nX=WH (2)\nwhere W2RK\u0002R\n+ contains the spectral bases for each\nof the Rpitches and H2RR\u0002N\n+ is the pitch activity\nmatrix across time. An alternative formulation of NMF\ncalled probabilistic latent component analysis (PLCA) has\nalso been employed for transcription (e.g. [22]). In PLCA\nthe matrices in the model are considered to be probability\ndistributions, thus allowing for a model that can be easily\nextended and formalised. Additional transcription meth-\nods have been proposed in the literature, employing sparse\ncoding techniques (e.g. [1]), genetic algorithms (e.g. [40]),\nand machine learning algorithms (e.g. [38]), which due to\nspace limitations cannot be detailed here.\nFor note tracking, hidden Markov models (HMMs) are\nfrequently used at a postprocessing stage (e.g. [38]). OtherParticipants 2009 2010 2011\nYeh and Roebel 0.69 0.69 0.68\nDressler - - 0.63\nBenetos and Dixon - 0.47 0.57\nDuan, Han, and Pardo 0.57 0.55 -\nTable 1. Best results using the accuracy metric for the\nMIREX Multi-F0 estimation task, from 2009-2011. De-\ntails about the employed metric can be found in [33].\ntechniques include temporal smoothing (e.g. using a me-\ndian ﬁlter) and minimum duration pruning [10].\nIn the remainder of this paper we analyse limitations of\ncurrent approaches and identify promising directions for\novercoming the obstacles in current performance.\n2. CHALLENGES\nDespite signiﬁcant progress in AMT research, there ex-\nists no end-user application that can accurately and reli-\nably transcribe music containing the range of instrument\ncombinations and genres available in recorded music. The\nperformance of even the most recent systems is still clearly\nbelow that of a human expert, who requires multiple takes,\nmakes extensive use of prior knowledge and complex in-\nference, and produces imperfect results. Furthermore, cur-\nrent test sets are limited in their complexity and coverage.\nTable 1 gives the results for the frame-based multiple-F0\nestimation task of the MIREX evaluation [33]. Results\nfor the note tracking task are much inferior, in the range\nof 0.2–0.35 average F-measure with onset-offset detection\nand 0.4–0.55 average F-measure with onset detection only.\nAs we propose in Section 3, informing transcription via\nuser-assistance or by providing a draft score in some ap-\nplications are ways to increase systems’ performance and\novercome the observed plateau.\nCurrently proposed systems also fall short in ﬂexibility\nto deal with diverse target data. Music genres like classical,\nheavy metal, hip-hop, ambient electronic and traditional\nChinese music have little in common. Furthermore styles\nof notation vary with genre. For example Pop/Rock no-\ntation might represent melody, chords and (perhaps) bass\nline, whereas a classical score would usually contain all the\nnotes to be played, and electroacoustic music has no stan-\ndard means of notation. The task of tailoring AMT systems\nto speciﬁc styles has yet to be addressed. In Section 4 we\npropose systems focusing on instrument- or genre-speciﬁc\ntranscription.\nAlgorithms are developed independently to carry out in-\ndividual tasks such as multiple-F0 detection, beat tracking\nand instrument recognition. Although this is necessary,\nconsidering the complexity of each task, the challenge re-\nmains in combining the outputs of the algorithms, or better,\ncombining the algorithms themselves to perform joint es-\ntimation of all parameters, in order to avoid the cascading\nof errors when the algorithms are combined sequentially.\nIn Section 5, we propose the fusion of information across\nmultiple musical aspects and the combination of methods\ntargeting the same feature.\nAnother challenge concerns the availability of data fortraining and evaluation. Although there is no shortage of\ntranscriptions and scores in standard music notation, hu-\nman effort is required to digitise and time-align them to\nthe recordings. Except for the case of solo piano, data\nsets currently employed for evaluation are small: a small\nsubset from the RWC database [20] which contains only\n12 tracks is commonly used (although the RWC database\ncontains many more recordings) and the MIREX multi-F0\nrecording lasts only 54 seconds. Such small datasets can-\nnot be considered representative; the danger of overﬁtting\nand thus overestimating system performance is high. It has\nbeen observed for several tasks that dataset developers tend\nto attain the best MIREX results [33]. In Section 6, we dis-\ncuss ways to generate more training data.\nAt present, there is no established single unifying frame-\nwork for music transcription as HMMs are for speech recog-\nnition. Likewise, there is no standard method for front end\nprocessing of the signal, with various approaches includ-\ning STFT, constant Q transform [8] and auditory models,\neach leading to different mid-level representations. The\nchallenge in this case is to characterise the impact of such\ndesign decisions on the AMT results. In Section 7, we con-\nsider the implications and steps required to progress from\nexisting systems to complete transcription.\nIn addition to the above, the research community shares\ncode and data on an ad hoc basis, with poor management\nand restrictive licensing limiting the level of re-use of re-\nsearch outputs. Many PhD students, for example, start\nfrom scratch spending valuable time “reinventing wheels”\nbefore proceeding to address current research issues. The\nlack of standard methodology is a contributing factor, with\nthe multiplicity of approaches to AMT making it difﬁcult\nto develop a useful shared code-base. The Reproducible\nResearch movement [9], with its emphasis on open soft-\nware and data, provides examples of best practice which\nare worthy of consideration by our community.\nFinally, present research in AMT introduces certain chal-\nlenges in itself that might constrain the evolution of the\nﬁeld. Advances in AMT research have mainly come from\nengineers and computer scientists, particularly those spe-\ncialising in machine learning. Currently there is minimal\ncontribution from computational musicologists, music psy-\nchologists or acousticians. Here the challenge is to inte-\ngrate knowledge from these ﬁelds, either from the litera-\nture or by engaging these experts as collaborators in AMT\nresearch.\nAMT research is quite active and vibrant at present, and\nwe do not presume to predict what the state of the art will\nbe in the next years and decades. In the remainder of the\npaper we propose promising techniques that can be utilised\nand further investigated in order to address the aforemen-\ntioned limitations in transcription performance. In Fig. 1\nwe provide a general diagram of transcription, incorporat-\ning techniques discussed in the following sections.\n3. INFORMED TRANSCRIPTION\n3.1 Semi-automatic Approaches\nSemi-automatic oruser-assisted transcription refers to ap-\nproaches where the user provides a certain amount of prior\ninformation to facilitate the transcription process [26]. Al-though such systems are not applicable to the analysis of\nlarge music databases, they can be of use for musicians,\nmusicologists, and—if a suitable synthesis method exists—\nfor intelligent audio manipulation.\nAMT systems usually have to solve a number of tasks,\nthe nature of which depends on the type of music anal-\nysed and the level of detail required for the score repre-\nsentation. While some of these tasks might be quite easy\nfor a human listener, it is often difﬁcult to ﬁnd an algo-\nrithmic formulation. The advantage of semi-automatic ap-\nproaches is the fact that certain tasks that are inherently\ndifﬁcult to solve algorithmically can be assisted by the user\nof the system. Semi-automatic transcription systems might\nalso pave the way for more robust fully-automatic ones,\nbecause the possibility of replacing the human part by an\nequally-performing computational solution always exists.\nIn principle any acoustic or score-related information\nthat can facilitate the transcription process can act as prior\ninformation for the system. However, to be of use in a\npractical application, it is important that it does not require\ntoo much time and effort, and that the required information\ncan be reliably extractable by the user, who might not be\nan expert musician.\nDepending on the expertise of the targeted users, infor-\nmation that is easy to provide could include key, tempo\nand time signature of the piece, structural information, in-\nformation about the instrument types in the recording, or\neven asking the user to label a number of notes for each\ninstrument. Although many proposed transcription sys-\ntems -often silently- make assumptions about certain pa-\nrameters, such as the number or types of instruments in the\nrecording, not many published systems explicitly incorpo-\nrate prior information from a human user. In the context of\nsource separation, Ozerov et al. [36] proposed a framework\nthat enables the incorporation of prior knowledge about\nthe number and types of sources, and the mixing model.\nThe authors showed that by using prior information, a bet-\nter separation can be achieved than with completely blind\nsystems. A system for user-assisted music transcription\nwas proposed in [26], where the user provides informa-\ntion about the instrument identities or labels a number of\nnotes for each instrument. This knowledge enabled the\nauthors to sidestep the error-prone task of source identi-\nﬁcation or timbre modelling, and to evaluate the proposed\nnon-negative framework in isolation.\n3.2 Score-informed Approaches\nContrary to speech, only a small fraction of music is fully\nspontaneous, as musical performances are typically based\non an underlying composition or song. Although transcrip-\ntion is usually associated with the analysis of an unknown\npiece, there are certain applications for which a score is\navailable, and in these cases the AMT system can exploit\nthis additional knowledge [42]. For example in automatic\ninstrument tutoring [6, 44], a system evaluates the perfor-\nmance of a student based on a reference score and pro-\nvides feedback. Thus, the correctly played passages need\nto be identiﬁed, along with any mistakes made by the stu-\ndent, such as missed or extra played notes. Another exam-\nple application is the analysis of expressive performance,\nwhere the tempo, dynamics, and timing deviations relative\nUser InteractionPrior Information\n(genre etc.)\nTranscription Method(s)\nSupporting T\noolsAudio Score\nAcoustic and\nmusicalmodelsTraining\nDataFigure 1. General overview of transcription. Supporting\ntools refer to techniques which can facilitate transcription,\ne.g. key estimation, instrument recognition.\nto the score are the focus of the analysis. There are of-\nten small differences between the reference score and the\nperformance, and in most cases, the score will not contain\nthe absolute timing of notes and thus will need to be time-\naligned with the recording as a ﬁrst step.\nOne way to utilise the automatically-aligned score is for\ninitialising the pitch activity matrix Hin a spectrogram\nfactorisation-based model (see Eq. (2)), and keeping these\nﬁxed while the spectral templates Ware learnt, as in [16].\nAfter the templates are learnt, the gain matrix can also be\nupdated in order to cater for note differences between the\nscore and the recording.\n4. INSTRUMENT- AND GENRE-SPECIFIC\nTRANSCRIPTION\nCurrent approaches for AMT usually employ instrument\nmodels that are not restricted to speciﬁc instrument types,\nbut applicable and adaptable to a wide range of musical\ninstruments. In fact, most transcription algorithms that\nare based on heuristic rules and those that employ human\nsound perception models even deliberately disregard spe-\nciﬁc timbral characteristics in order to enable an instrument-\nindependent detection of notes. Even many so-called pi-\nano transcription methods are not so much tailored to pi-\nano music as tested on such music; they do not implement\na piano-speciﬁc instrument model. Similarly, the aim of\nmany transcription methods is to be applicable to a broad\nrange of musical genres.\nThe fact that only a small number of publications on\ninstrument- and genre-speciﬁc transcription exist, is par-\nticularly surprising when we compare AMT to the more\nmature discipline of automatic speech recognition. Con-\ntinuous speech recognition systems are practically always\nlanguage-speciﬁc and typically also domain-speciﬁc, and\nmany modern speech recognisers include speaker adapta-\ntion [24].\nTranscription systems usually try to model a wide range\nof musical instruments using a single set of computational\nmethods, thereby assuming that those methods can be ap-\nplied equally well to different kinds of musical instruments.However, depending on the sound production mechanism\nof the instruments, their characteristics can differ consid-\nerably and might not be captured equally well by the same\ncomputational model or might at least require instrument-\nspeciﬁc parameters and constraints if a common model\nis used. Furthermore, acoustic instruments incorporate a\nwide range of playing styles, which can differ notably in\nsound quality. On the other hand we can revert to the ex-\ntensive literature on the physical modelling of musical in-\nstruments. A promising direction is to incorporate these\nmodels in the transcription process or at least use them as\nprior information that can then be adapted to the record-\ning under analysis. Some examples of instrument-speciﬁc\ntranscription are for violin [29], bells [30], tabla [19] and\nguitar [3].\nThe application of instrument-speciﬁc models, however,\nrequires the target instrumentation either to be known or\ninferred from the recording. Instrument identiﬁcation in\na polyphonic context, as opposed to monophonic, is ren-\ndered difﬁcult by the way the different sources blend with\neach other, with a high degree of overlap in the time-fre-\nquency domain. The task is closely related to sound source\nseparation and as a result, many systems operate by ﬁrst\nseparating the signals of different instruments from the mix-\nture or by generating time-frequency masks that indicate\nspectral regions that belong only to a particular instrument\nwhich can then be classiﬁed more accurately [13]. There\nare also systems that try to extract features directly from\nthe mixture or by focusing on time-frequency regions with\nisolated note partials [4]. A review of instrument identiﬁ-\ncation methods can be found in [34, sect. IV].\nThe advantage of restricting a transcription system to a\ncertain musical genre lies in the fact that special (expert)\nknowledge about that genre can be incorporated. Musico-\nlogical knowledge about structure (e.g. sonata form), har-\nmony progressions (e.g. 12-bar blues) or speciﬁc instru-\nments (e.g. Irish folk music) can enhance the transcrip-\ntion accuracy. Genre-speciﬁc AMT systems have been de-\nsigned for genres such as Australian aborginal music [35].\nIn order to build a general-purpose AMT system, several\ngenre-speciﬁc transcription systems could be combined and\nselected based on a preliminary genre classiﬁcation stage.\n5. INFORMATION INTEGRATION\n5.1 Fusing information across the aspects of music\nMany systems for note tracking combine multiple-F0 esti-\nmation with onset and offset detection, but disregard con-\ncurrent research on other aspects of music, such as instru-\nmentation, rhythm, or tonality. These aspects are highly\ninterdependent and they could be analysed jointly, combin-\ning information across time and across features to improve\ntranscription performance.\nA human transcriber interprets the performed notes in\nthe context of a metrical structure consisting of a semi-\nregular, hierarchical system of accents. Extensive research\nhas been performed into tempo induction, beat tracking\nand rhythm parsing [21], but transcription rarely takes ad-\nvantage of this knowledge. An exception is the use of beat-\nsynchronous features in chord transcription [31], where the\naudio is segmented according to the location of beats, andfeatures are averaged over these beat-length intervals. The\nadvantage of a more robust feature (less overlap between\nsucceeding chords) is balanced by a loss in temporal res-\nolution (harmonic change is assumed not to occur within\na beat). For note transcription, it is unrealistic to assume\nthat notes do not change within beats, but a promising ap-\nproach would be to use a similar technique at a lower (i.e.\nsub-beat) metrical level, corresponding to the fastest note\nsequences. The resulting features would be more robust\nthan frame-level features, and advantage could be taken of\nknown (or learnt) rhythmic patterns and effects of metrical\nposition.\nKey is another high-level musical cue that, if known\nor estimated from the signal, provides useful prior infor-\nmation for the extraction of notes and chords. Key can\nbe modelled as imposing a probability distribution over\nnotes and chords for different metrical positions and du-\nrations. Therefore, by speciﬁcally modelling key, tran-\nscription accuracy can be improved, e.g. by giving more\nweight to notes which belong to the current key. Genre\nand style are also inﬂuential factors for modelling the dis-\ntribution of pitch classes in a key. Several key estimation\napproaches have been proposed, but these are rarely ex-\nploited for AMT, with the exception of [41], which gave\nthe best results for the MIREX 2008 note tracking task.\nLikewise, local harmony (the current chord) can be used\nto inform note transcription. The converse problem, de-\ntermining the chord given a set of detected notes, is also\na transcription task. A chord transcription system which\nuses a probabilistic framework to jointly model the key,\nmetre, chord and bass notes is presented in [31].\nFinally, information can also be integrated over time.\nMost AMT systems to date have modeled only short-term\ndependencies, often using Markov models to describe ex-\npected melodic, harmonic and rhythmic sequences. As a\nnotable exception, [32] utilized structural repetitions for\nchord transcription. Also the musical key establishes a\nlonger-term (tonal) context for pitch analysis.\n5.2 Combining methods targeting the same feature\nInformation could also be integrated by combining multi-\nple estimators or detectors for a single feature, for instance\ncombining two multi-pitch estimators, especially if these\nare based on different acoustic cues or different processing\nprinciples. This could help overcome weak points in the\nperformance of the individual estimators, offer insight on\nthe weaknesses of each and raise the overall system accu-\nracy. In a different context, several pitched instrument on-\nset detectors, which individually have high precision and\nlow recall, have been successfully combined in order to\nobtain an improved detection accuracy [23]. For classiﬁ-\ncation, adaptive boosting (AdaBoost) provides a powerful\nframework for fusing different classiﬁers in order to im-\nprove the performance [17].\n5.3 Joint transcription and source separation\nSource separation could be of beneﬁt to transcription-related\ntasks such as instrument identiﬁcation, where both tasks\nare interdependent, and accomplishing one of them could\nsigniﬁcantly ease the other. In this spirit, joint source sepa-\nration and musical instrument identiﬁcation methods havebeen proposed using signal model-based probabilistic in-\nference in the score-informed case [25]. Also, ideas and al-\ngorithms from the ﬁeld of source separation can be utilised\nfor AMT, especially regarding the exploitation of spatial\ninformation, if this is available [12, 36].\nHowever, for most AMT tasks there is only one or two\nmixture signals available, and the number of sources is\nlarger than the number of mixtures. In this case, the sep-\naration task is underdetermined, and can only be solved\nby requiring certain assumptions to hold for the sources.\nThese could include sparsity, non-negativity and indepen-\ndence or they could involve structured spectral models like\nNMF models [22], spectral Gaussian scaled mixture mod-\nels (Spectral-GSMMs) [2] or the source-ﬁlter model for\nsound production. Further constraints such as temporal\ncontinuity or harmonicity can be applied on spectral mod-\nels. Techniques that employ spectral source modelling or\nan NMF-based framework that explicitly models the mix-\ning process of the sources have been shown to perform well\nbecause they exploit the statistical diversity of the source\nspectrograms [2].\nFinally, source separation can be fully utilised in a semi-\nsupervised system like [12], where the user initially selects\nthe desired audio source through the estimated F0 track\nof that source and subsequently the system reﬁnes the se-\nlected F0 tracks, and estimates and separates the relevant\nsource.\n6. CREATING TRAINING DATA\nA large subset of AMT approaches perform experiments\nonly on piano data, e.g. [10, 14, 38]. One reason is be-\ncause it is relatively easy to create recordings with aligned\nground-truth using e.g. a Disklavier. However, this em-\nphasis on piano music sometimes leads to models that are\ntailored for pitched percussive instruments and could also\nbe a cause for overﬁtting. Thus, ground-truth for multiple-\ninstrument recordings is crucial for the further develop-\nment of sophisticated transcription systems.\nIf musical scores become widely available in digital form\n(for example via crowd-sourced transcriptions), they pro-\nvide valuable side-information for signal analysis, and in\nthe extreme cases reduce the transcription task to the align-\nment of an existing score to the input audio, although it\nshould be noted that different renditions of a song often\nvary considerably in their instrumentation and arrangement.\nOne such example is the set of syncRWC annotations1.\nMost of the current AMT methods involve a training\nstage, where the parameters of the method are optimised\nusing manually annotated data. The availability of recorded\nmusic with the exact underlying score opens up huge and\nlargely unutilised opportunities for training complex mod-\nels. In the case of genre- and instrument-speciﬁc transcrip-\ntion, separate parameter sets can be trained for different\ntarget material.\n7. TOWARDS A COMPLETE TRANSCRIPTION\nMost of the aforementioned transcription approaches tackle\nthe problems of multiple-F0 estimation and note onset and\n1http://staff.aist.go.jp/m.goto/RWC-MDB/\nAIST-Annotation/SyncRWC/offset detection. However, in order to fully solve the AMT\nproblem and have a system that provides an output that\nis equivalent to sheet music, additional issues need to be\naddressed, such as metre induction, rhythm parsing, key\nﬁnding, note spelling, dynamics, ﬁngering, expression, ar-\nticulation and typesetting. Although there are approaches\nthat address many of these individual problems, there ex-\nists no ‘complete’ AMT system to date.\nRegarding typesetting, current tools produce readable\nscores from MIDI data only (e.g. Lilypond2), however,\ncues from the music signal could also assist in incorpo-\nrating additional information into the ﬁnal score (e.g. ex-\npressive features for note phrasing). As far as dynamics\nare concerned, in [15] a method was proposed for estimat-\ning note intensities in a score-informed scenario. However,\nestimating note dynamics in an unsupervised way has not\nbeen tackled. Another issue would be the fact that most ex-\nisting ground-truth does not include note intensities, which\nis difﬁcult to annotate manually, except for datasets created\nusing reproducing pianos (e.g. [38]), which automatically\ncontain intensity information such as MIDI note velocities.\nRecent work [3] addresses the problem of automatically\nextracting the ﬁngering conﬁgurations for guitar record-\nings in an AMT framework. For computing ﬁngering, in-\nformation from the transcribed signal as well as instrument-\nspeciﬁc knowledge is needed. Thus, a robust instrument\nidentiﬁcation system would need to be incorporated for\ncomputing ﬁngerings in multi-instrument recordings.\nFor extracting expressive features, some work has been\ndone in the past, mostly in the score-informed case. In [18]\na framework for extracting expressive features both from\na score-informed and an uninformed perspective is pro-\nposed. For the latter, an AMT system is used prior to the\nextraction of expressive features. It should be mentioned\nthough that the extracted features (e.g. auditory loudness,\nattack) do not necessarily correspond to expressive nota-\ntion. Thus, additional work needs to be done in order to\nprovide a mapping between mid-level features and actual\nexpressive markings in a transcribed music score.\n8. CONCLUSIONS\nAutomatic music transcription is a rapidly developing re-\nsearch area where several different approaches are still be-\ning actively investigated. However from the perspective\nof evaluation results, the performance seems to converge\ntowards a level that is not satisfactory for all applications.\nOne viable way of breaking the glass ceiling is to insert\nmore information into the problem. For example, genre- or\ninstrument-speciﬁc transcription allows the utilisation of\nhigh-level models that are more precise and powerful than\ntheir more general counterparts. A promising research di-\nrection is to combine several processing principles, or to\nextract various types of musical information, such as the\nkey, metrical structure, and instrument identities, and feed\nthat into a model that provides context for the note detec-\ntion process. To enable work in this area, sharing code and\ndata between researchers becomes increasingly important.\nNote detection accuracy is not the only determining fac-\ntor that enables meaningful end-user applications. Often\n2http://lilypond.org/it is possible to circumvent the limitations of the under-\nlying technology in creative ways. For example in semi-\nautomatic transcription, the problem is redeﬁned as achiev-\ning the required transcription accuracy with minimal user\neffort. It is important to have end-user applications that\ndrive the development of AMT technology and provide it\nwith relevant feedback.\n9. REFERENCES\n[1] S. A. Abdallah and M. D. Plumbley. Polyphonic transcription\nby non-negative sparse coding of power spectra. In ISMIR,\npages 318–325, 2004.\n[2] S. Arberet, A. Ozerov, F. Bimbot, and R. Gribonval. A\ntractable framework for estimating and combining spectral\nsource models for audio source separation. Signal Process-\ning, 92(8):1886–1901, 2012.\n[3] A.M. Barbancho, A. Klapuri, L.J. Tardon, and I. Barbancho.\nAutomatic transcription of guitar chords and ﬁngering from\naudio. IEEE TASLP, 20(3):915–921, 2012.\n[4] J.G.A. Barbedo and G. Tzanetakis. Musical instrument clas-\nsiﬁcation using individual partials. IEEE TASLP, 19(1):111–\n122, 2011.\n[5] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation of\nmultiple-F0 estimation and tracking systems. In ISMIR, pages\n315–320, 2009.\n[6] E. Benetos, A. Klapuri, and S. Dixon. Score-informed tran-\nscription for automatic piano tutoring. In EUSIPCO, 2012.\n[7] N. Bertin, R. Badeau, and E. Vincent. Enforcing harmonicity\nand smoothness in Bayesian non-negative matrix factoriza-\ntion applied to polyphonic music transcription. IEEE TASLP,\n18(3):538–549, 2010.\n[8] J.C. Brown. Calculation of a constant Q spectral transform.\nJASA, 89(1):425–434, 1991.\n[9] J. B. Buckheit and D. L. Donoho. WaveLab and reproducible\nresearch. Technical Report 474, Dept of Statistics, Stanford\nUniv., 1995.\n[10] A. Dessein, A. Cont, and G. Lemaitre. Real-time polyphonic\nmusic transcription with non-negative matrix factorization\nand beta-divergence. In ISMIR, pages 489–494, 2010.\n[11] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamental fre-\nquency estimation by modeling spectral peaks and non-peak\nregions. IEEE TASLP, 18(8):2121–2133, 2010.\n[12] J.L. Durrieu and J.P. Thiran. Musical audio source separation\nbased on user-selected F0 track. In LVA/ICA, pages 438–445,\n2012.\n[13] J. Eggink and G.J. Brown. A missing feature approach to in-\nstrument identiﬁcation in polyphonic music. In ICASSP, vol-\nume 5, pages 553–556, 2003.\n[14] V . Emiya, R. Badeau, and B. David. Multipitch estimation of\npiano sounds using a new probabilistic spectral smoothness\nprinciple. IEEE TASLP, 18(6):1643–1654, 2010.\n[15] S. Ewert and M. M ¨uller. Estimating note intensities in music\nrecordings. In ICASSP, pages 385–388, 2011.\n[16] S. Ewert and M. M ¨uller. Using score-informed constraints for\nNMF-based source separation. In ICASSP, pages 129–132,\n2012.\n[17] Y . Freund, R. Schapire, and N. Abe. A short introduction to\nboosting. JSAI, 14(771-780):1612, 1999.\n[18] R. Gang, G. Bocko, J. Lundberg, S. Roessner, D. Headlam,\nand M.F. Bocko. A real-time signal processing framework of\nmusical expressive feature extraction using MATLAB. In IS-\nMIR, pages 115–120, 2011.\n[19] O. Gillet and G. Richard. Automatic labelling of tabla signals.\nInISMIR, 2003.\n[20] M. Goto. Development of the RWC music database. In 18th\nInt. Congress Acoustics, pages 553–556, 2004.\n[21] F. Gouyon and S. Dixon. A review of automatic rhythm de-\nscription systems. CMJ, 29(1):34–54, 2005.[22] G. Grindlay and D. Ellis. Transcribing multi-instrument\npolyphonic music with hierarchical eigeninstruments. IEEE\nJSTSP, 5(6):1159–1169, 2011.\n[23] A. Holzapfel, Y . Stylianou, A.C. Gedik, and B. Bozkurt.\nThree dimensions of pitched instrument onset detection.\nIEEE TASLP, 18(6):1517–1527, 2010.\n[24] X. Huang, A. Acero, and H.-W. Hon, editors. Spoken Lan-\nguage Processing: A guide to theory, algorithm and system\ndevelopment. Prentice Hall, 2001.\n[25] K. Itoyama, M. Goto, K. Komatani, T. Ogata, and H.G.\nOkuno. Simultaneous processing of sound source separation\nand musical instrument identiﬁcation using Bayesian spectral\nmodeling. In ICASSP, pages 3816–3819, 2011.\n[26] H. Kirchhoff, S. Dixon, and A. Klapuri. Shift-variant non-\nnegative matrix deconvolution for music transcription. In\nICASSP, 2012.\n[27] A. Klapuri. Multiple fundamental frequency estimation based\non harmonicity and spectral smoothness. IEEE TASLP,\n11(6):804–816, 2003.\n[28] A. Klapuri and M. Davy, editors. Signal Processing Methods\nfor Music Transcription. Springer, 2006.\n[29] A. Loscos, Y . Wang, and W.J.J. Boo. Low level descriptors\nfor automatic violin transcription. In ISMIR, pages 164–167,\n2006.\n[30] M. Marolt. Automatic transcription of bell chiming record-\nings. IEEE TASLP, 20(3):844–853, 2012.\n[31] M. Mauch and S. Dixon. Simultaneous estimation of chords\nand musical context from audio. IEEE TASLP, 18(6):1280–\n1289, 2010.\n[32] M. Mauch, K. Noland, and S. Dixon. Using musical structure\nto enhance automatic chord transcription. In ISMIR, pages\n231–236, 2009.\n[33] Music Information Retrieval Evaluation eXchange (MIREX).\nhttp://music-ir.org/mirexwiki/, 2011.\n[34] M. M ¨uller, D. Ellis, A. Klapuri, and G. Richard. Signal pro-\ncessing for music analysis. IEEE JSTSP, 5(6):1088–1110,\n2011.\n[35] A. Nesbit, L. Hollenberg, and A. Senyard. Towards automatic\ntranscription of Australian aboriginal music. In ISMIR, pages\n326–330, 2004.\n[36] A. Ozerov, E. Vincent, and F. Bimbot. A general ﬂexible\nframework for the handling of prior information in audio\nsource separation. IEEE TASLP, 20(4):1118–1133, 2012.\n[37] P.H. Peeling and S.J. Godsill. Multiple pitch estimation\nusing non-homogeneous Poisson processes. IEEE JSTSP,\n5(6):1133–1143, 2011.\n[38] G. Poliner and D. Ellis. A discriminative model for poly-\nphonic piano transcription. EURASIP JASP, 8:154–162,\n2007.\n[39] G. Poliner, D. Ellis, A. Ehmann, E. Gomez, S. Streich, and\nB. Ong. Melody transcription from music audio: Approaches\nand evaluation. IEEE TASLP, 15(4):1247–1256, 2007.\n[40] G. Reis, N. Fonseca, F. F. de Vega, and A. Ferreira. Hy-\nbrid genetic algorithm based on gene fragment competition\nfor polyphonic music transcription. In Conf. Applications of\nEvolutionary Computing, pages 305–314. 2008.\n[41] M.P. Ryyn ¨anen and A. Klapuri. Polyphonic music transcrip-\ntion using note event modeling. In WASPAA, pages 319–322,\n2005.\n[42] E.D. Scheirer. Using musical knowledge to extract expressive\nperformance information from audio recordings. In H. Okuno\nand D. Rosenthal, editors, Readings in Computational Audi-\ntory Scene Analysis. Lawrence Erlbaum, 1997.\n[43] P. Smaragdis and J. C. Brown. Non-negative matrix factoriza-\ntion for polyphonic music transcription. In WASPAA, pages\n177–180, 2003.\n[44] Y . Wang and B. Zhang. Application-speciﬁc music transcrip-\ntion for tutoring. IEEE MultiMedia, 15(3):70–74, 2008.\n[45] C. Yeh. Multiple fundamental frequency estimation of poly-\nphonic recordings. PhD thesis, Universit ´e Paris VI - Pierre et\nMarie Curie, France, 2008."
    },
    {
        "title": "Large-Scale Cover Song Recognition Using the 2D Fourier Transform Magnitude.",
        "author": [
            "Thierry Bertin-Mahieux",
            "Daniel P. W. Ellis"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414956",
        "url": "https://doi.org/10.5281/zenodo.1414956",
        "ee": "https://zenodo.org/records/1414956/files/Bertin-MahieuxE12.pdf",
        "abstract": "Large-scale cover song recognition involves calculating item- to-item similarities that can accommodate differences in timing and tempo, rendering simple Euclidean measures unsuitable. Expensive solutions such as dynamic time warp- ing do not scale to million of instances, making them inap- propriate for commercial-scale applications. In this work, we transform a beat-synchronous chroma matrix with a 2D Fourier transform and show that the resulting representa- tion has properties that fit the cover song recognition task. We can also apply PCA to efficiently scale comparisons. We report the best results to date on the largest available dataset of around 18,000 cover songs amid one million tracks, giving a mean average precision of 3.0%.",
        "zenodo_id": 1414956,
        "dblp_key": "conf/ismir/Bertin-MahieuxE12",
        "keywords": [
            "cover song recognition",
            "item-to-item similarities",
            "Euclidean measures",
            "dynamic time warping",
            "million instances",
            "commercial-scale applications",
            "beat-synchronous chroma matrix",
            "2D Fourier transform",
            "PCA",
            "mean average precision"
        ],
        "content": "LARGE-SCALE COVER SONG RECOGNITION\nUSING THE 2D FOURIER TRANSFORM MAGNITUDE\nThierry Bertin-Mahieux\nColumbia University\nLabROSA, EE Dept.\ntb2332@columbia.eduDaniel P.W. Ellis\nColumbia University\nLabROSA, EE Dept.\ndpwe@ee.columbia.edu\nABSTRACT\nLarge-scale cover song recognition involves calculating item-\nto-item similarities that can accommodate differences in\ntiming and tempo, rendering simple Euclidean measures\nunsuitable. Expensive solutions such as dynamic time warp-\ning do not scale to million of instances, making them inap-\npropriate for commercial-scale applications. In this work,\nwe transform a beat-synchronous chroma matrix with a 2D\nFourier transform and show that the resulting representa-\ntion has properties that ﬁt the cover song recognition task.\nWe can also apply PCA to efﬁciently scale comparisons.\nWe report the best results to date on the largest available\ndataset of around 18,000 cover songs amid one million\ntracks, giving a mean average precision of 3.0%.\n1. INTRODUCTION\nMusic videos are abundant on the web, and tracing the dis-\nsemination of a given work is a challenging task. Audio\nﬁngerprinting [26] can be used to ﬁnd an exact copy of a\ngiven music track, but the same technology will not work\nfor ﬁnding novel versions of the original work, i.e. “cover\nsongs”. Since cover songs are typically recorded by musi-\ncians other than those who originally commercialized the\ntrack, one motivation for identifying such “covers” is to\nensure the correct handling of songwriting royalties. For\ninstance, on YouTube, the copyright holder of the musical\nwork can have the covers of her work removed, or she can\nreceive part of the advertising revenue from the video1.\nFigure 1 shows how easy it is to ﬁnd thousands of such cov-\ners online from the metadata alone, but many more are not\nidentiﬁed as covers. Another reason to study cover song\nrecognition is that ﬁnding and understanding transforma-\ntions of a musical piece that retain its essential identity can\nhelp us to develop intelligent audio algorithms that recog-\nnize common patterns among musical excerpts.\nUntil recently, cover recognition was studied on a small\nscale (a few hundred tracks) due in part to the scarcity of\n1http://www.youtube.com/t/faq\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\nFigure 1. Search result for self-identiﬁed cover songs of Lady\nGaga on YouTube on November 22nd, 2011. This simple query\nproduces about 35;900results.\ngenerally-available databases. Most current algorithms are\nbased on comparisons between chroma patterns, or a re-\nlated feature, using some form of dynamic time-warping\n(DTW) [23]. Chromas are derived from the spectrogram\nand provide a coarse approximation of the musical score\n(see Section 2), which make them suitable for our task. Re-\ncently, the release of the Million Song Dataset [1] (MSD),\nwhich contains metadata and audio features (including chro-\nmas) for one million songs, has spurred the investigation of\nlarge-scale music information retrieval techniques. Linked\nto this dataset, the SecondHandSongs dataset (SHS) iden-\ntiﬁes approximately eighteen thousand cover songs. The\ntask of ﬁnding these cover songs within one million tracks\nmakes it much closer to a commercial-scale application.\nVery large datasets constrain the amount of computa-\ntion that can be devoted to individual comparisons, making\nDTW an increasingly infeasible choice. To work with mil-\nlions of examples, we would ideally reduce each compari-\nson to a simple operation in a low-dimensional space. Such\na space may be deﬁned via hash codes extracted from the\nsignal in the spirit of ﬁngerprinting algorithms [2]. Hash\ncodes can be efﬁciently indexed, and ﬁnding a song that\ncontains particular hash codes is extremely fast. Another\noption is to project the entire song into a small ﬁxed dimen-\nsion space in which nearest neighbors are our candidate\ncovers. Nearest neighbor methods are easy to parallelizeand scale, and working with a ﬁxed-dimensional represen-\ntation (instead of a variable-length audio signal) is a great\nconvenience.\nDisappointed by the results presented in [2], we focus\non the second option. Beat-synchronous chroma repre-\nsentations form a relatively compact description that re-\ntains information relevant to covers, and may be cropped\nto a constant size. Unfortunately, direct comparison of\nchroma patterns using common metrics is poorly behaved\n[3]. Summarizing a song by extracting a few chroma patches\nand comparing them with Euclidean distance gives unus-\nable results. In order to obtain an efﬁcient nearest-neighbor\nalgorithm, we need a representation for the chroma patches\nwith the following properties:\n\u000frepresentation vectors can be compared using a sim-\nple metric, e.g. Euclidean distance;\n\u000frepresentation vectors are compact, i.e. low-dimensional;\n\u000frepresentation must be robust to semitone rotations\n(musical transpositions);\n\u000frepresentation should be robust to different pattern\noffsets (time alignments).\nThe last condition would allow us to match patches with-\nout having to identify a universal time alignment which\nis very difﬁcult in a large set. Our candidate representa-\ntion is the two-dimensional Fourier transform magnitude\n(2DFTM), a simpliﬁed version of the representation used\nin [18] and discussed in Subsection 3.2. The Fourier trans-\nform separates patterns into different levels of detail, which\nis useful for compacting energy (as in image compression\nschemes such as JPEG) and for matching in Euclidean space.\nDiscarding the phase component provides invariance both\nto transposition (rotation) in the pitch axes and skew (mis-\nalignment) on the beat (time) axis. Thus, taking the 2DFTM\nof a chroma patch, we obtain a transformation of chroma\nfeatures that makes Euclidean distance quite useful, even\nafter dimensionality reduction through PCA. Our method\nencodes each track as a 50-dimensional vector and pro-\nvides a large improvement over the hash code-based method\n[2]. On the SHS, this approach gives the best result re-\nported so far.\nThe rest of this work is organized as follows: In Section\n2, we present the chroma feature and its variants, its metric\nissues, and the task of cover song recognition. In Section\n3, we describe how we transform the chroma matrices and\nlook at the resulting invariance properties. Section 4 details\nour experiments on large-scale cover song recognition, and\nwe conclude in Section 5.\n2. PREVIOUS WORK\n2.1 Chroma feature and distance\nChroma features were introduced as pitch-class proﬁles\n(PCP) [9]. Many variants have been derived, including\nHPCP [10] and CENS [21]; an overview can be found\nin [15]. There is even evidence that chromas can be learned\nfrom a simple similarity task [12].Unfortunately, chroma matrices (or chroma patches, our\nterm for chroma matrices with a ﬁxed number of time sam-\nples) are high-dimensional features that are difﬁcult to com-\npare with usual metrics [3]. Previous work has experi-\nmented with Euclidean distance [3, 23], cosine distance\n[23], Kullback-Leibler divergence [3,22] and Itakura-Saito\ndivergence [22]. None of the results were fully satisfying\nfor the task of cover song recognition.\n2.2 Cover song recognition\nCover song recognition has been widely studied in recent\nyears, including a speciﬁc task within MIREX since 2007\n[6]. An early system is Ellis and Poliner [8] and a good\noverview is in Serr `a’s thesis [23]. A signiﬁcant amount of\nwork has been done with classical music [16, 19–21] but\npopular music can present a richer range of variation in\nstyle and instrumentation.\nMost cover song works were tested on a few hundred or\nthousand songs, a size not comparable to commercial col-\nlections (Spotify2claims more than 15M tracks). How-\never, some of the work was made to scale and could be\nextended to larger sets. Kurth and M ¨uller [17] use a code-\nbook of CENS features to encode songs, thus creating an\nindex that can be searched efﬁciently. Casey and Slaney\n[5] use locally-sensitive hashing (LSH) to quickly com-\npare chroma patches, or shingles. Yu et al. [27] also use\nLSH to compare different statistics about a song. Kim and\nNarayanan [16] look at chroma changes over time and sug-\ngest using these changes as hash codes. Finally, our previ-\nous method [2] uses hash codes inspired by the ﬁngerprint\nsystem of [26], i.e., identifying peaks in the chromagram\nand encode their relative positions. This was the ﬁrst result\nreported on the SHS.\nThe idea of using the magnitude of the two-dimensional\nFourier transform has been explored in [14, 18]. As with\nthe methods above, these were tested on a few hundreds\nor thousands examples. The differences with the method\nused in this work are highlighted in Subsection 3.4\n3. CHROMA FEATURE AND 2DFTM\nOur feature representation is the magnitude of the two-\ndimensional Fourier transform of beat-aligned chroma patches.\nBelow, we explain how they are computed and discuss their\nproperties.\n3.1 Chroma\nWe use the chroma features computed by the Echo Nest\nAPI [7] as described in [13]. They are available as part of\nthe Million Song Dataset [1] and were used in [2].\nA chromagram is similar in spirit to a constant-Q spec-\ntrogram except that pitch content is folded into a single\noctave of 12discrete bins, each corresponding to a par-\nticular semitone (e.g., one key of the octave on a piano).\nFor each song in the MSD, the Echo Nest analyzer gives a\nchroma vector (length 12) for every music event (called\n2http://www.spotify.com0 20 40 60 800246810semitones\n45\n40\n35\n30\n25\n20\n15\n10\n5\nloudnesssegments (chromas / loudness) and beats\n0 10 20 30 40 50 60 700246810semitonesbeat aligned chromas\n0 10 20 30 40 50 60 700246810semitonesbeat aligned chromas after raising high valuesFigure 2. Beat-aligned chroma features.\n“segment”), and a segmentation of the song into beats.\nBeats may span or subdivide segments. Averaging the per-\nsegment chroma over beat times results in a beat-synchronous\nchroma feature representation similar to that used in [8].\nEcho Nest chroma vectors are normalized to have the largest\nvalue in each column equal to 1.\nEmpirically, we found improved results from raising the\nhighest values relative to the lowest ones by a power-law\nexpansion. We believe this accentuates the main patterns\nin the signal. Figure 2 illustrates the stages in converting\nsegment-aligned chroma features and their loudness to cre-\nate beat-aligned chroma features for our task.\n5 10 1551015\n5\n 10\n 15\n−5 0 5−505\n5 10 1551015\n5\n5\n 5\n 10\n 15\n−5 0 5−505\n5 10 1551015\n5\n5\n 5\n 10\n 15\n−5 0 5−505Image 2DFTM\nFigure 3. Examples of images and the corresponding 2DFTMs.\nFT images have the low-frequency values in the center.\n3.2 Magnitude of the 2D Fourier Transform\nTaking the two-dimensional Fourier transform is a com-\nmon task in digital image processing where it is useful for\ndenoising and compression, among other things [11]. As\nillustrated in the examples of Figure 3, a single point in the\n2D Fourier transform corresponds to a sinusoidal grid of a\nparticular period and orientation in the image (transform)\ndomain; more complex images are built up out of multiple\nsinusoid grids.We skip the deﬁnition of the 2D Fourier transform and\nits magnitude since it is widely known. Note that for visu-\nalization purposes, the bins are shifted so that the center of\nthe axis system is in the middle of the image3. In that rep-\nresentation, the transformed image is symmetrical about\nthe origin. Figure 4 gives an example of the transform ap-\nplied to the chroma patch from Figure 2.\n0 10 20 30 40 50 60 700246810semitonesbeat-aligned chroma patch\n-14 -12 0 12 14-4-20242DFTM of the chroma patch\nFigure 4. 2DFTM over a beat-synchronous chroma patch. Bins\nhave been shifted so the maximum energy is in the center and\nmagnitude values have been raised to the power 0:5for visualiza-\ntion purposes. In the 2DFTM, the darker columns at \u00009and+9\ncan be explained by the repetitive pattern in time in the chroma\npatch (bottom semitone) whose period is approximately 9beats.\n3.3 Rotation invariance\nAnalyzing nearest neighbors or clusters of chroma patches\nhelps understand the properties of a given representation.\nQuantizing the chroma vector can be a useful step in an al-\ngorithm [20], but it can also be a goal on its own with the\nhope of seeing meaningful music patterns emerge [4]. The\nuse of the 2DFTM introduces an interesting invariance, not\nonly in key as in [4], but also in time, since small time\nshifts within a large patch correspond primarily to phase\nshifts in the Fourier components, with only slight changes\nin magnitude arising from edge effects. Figure 5 shows, for\na randomly-chosen chroma patch at the top-left of the ﬁg-\nure, the nearest neighbors obtained from the 2DFTM rep-\nresentation. For visualization purposes, we used 16-beat\npatches for this experiment. The result is noisy, but we see\na clear square wave pattern that is repeated with different\nonsets in the ﬁrst three neighbors.\n3.4 Comparison with Similar Previous Approaches\nOur method resembles those of [14,18], but some steps are\nsimpliﬁed in view of the size of the collection at hand. In\n[14], instead of beat-aligned chromagrams, the authors use\ntwo-dimensional autocorrelation, then, for each semitone,\ntake 17samples spaced logarithmically in time (to normal-\nize tempo) building a 12x17 feature matrix for each song.\nAutocorrelation is the Fourier transform of the Fourier Trans-\nform magnitude we use.\n3fftshift in MATLAB, scipy.fftpack.fftshift in Python024681012140246810\n024681012140246810\n024681012140246810\n024681012140246810\n024681012140246810\n024681012140246810Figure 5. Neighbors of the upper left patch using 2DFTM rep-\nresentation (including raising values to power 1.96). The square\nsignal shape, shifted in time, is visible in the ﬁrst two neighbors\n(upper right, middle left) and partially in the middle right neigh-\nbor.\nCompared to [18], we do not extract melodic lines, and\nwe represent each track with an aggregate patch rather than\nstoring many overlapping patches. We use PCA as the do-\nmain for comparison to improve generalization. Marolt de-\nscribes time-shift invariance of the 2DFTM as an issue, but\nfor us it is the main attraction. We found 75beat patches\nto be most useful compared to the optimum at 32in [18],\nperhaps because of PCA. A main emphasis of his work is\nthe use of LSH, but reducing each track to a single pat-\ntern makes such indexing unneccessary, even for our much\nlarger database.\n4. COVER SONG RECOGNITION\nOur goal is to ﬁnd cover songs among mostly western pop\nmusic. The experiments are conducted on the Second-\nHandSongs dataset (SHS). The SHS was created by match-\ning the MSD with the online database4that aggregates\nuser-submitted cover songs. The SHS training set contains\n12;960cover songs from 4;128cliques (musical works),\nand the testing set has 5;236tracks in 726cliques. Note\nthat we use the term “cover” in a wide sense. For instance,\ntwo songs derived from the same original work are consid-\nered covers of one another.\n4.1 Method\nWe will represent each song by a vector of ﬁxed-length,\ndeﬁning a point in Euclidean space. The closer two feature\nvectors lie in this space, the more likely the songs are cov-\ners. The steps to compute this feature vector for a song are\nsummarized as follows:\n1. obtain chroma from the MSD\n2. resample onto beat grid\n3. apply power-law expansion\n4. extract FFT patches and keep magnitude\n5. take the median\n4www.secondhandsongs.com6. apply PCA\nThe ﬁrst step is done using the code provided with the\nMSD. One can also call The Echo Nest API [7] if start-\ning from audio (see Subsection 4.4). Beat estimation is\nalso provided in the MSD, and the second step is mostly\na linear scaling. The third step, in which we enhance the\ncontrast between strong and weak chroma bins by raising\nthe values to an exponent, was determined to be useful in\nour experiments. In the fourth step, we take the 2DFTM as\nshown in Figure 4 for every 75-beat long chroma patch. In\nthe ﬁfth step we keep the median, within each bin, across\nall the transformed patches. Finally, in the sixth step, we\nuse PCA to reduce the dimensionality. PCA is done with-\nout normalization, the data is simply centered to zero and\nrotated. In our experiments PCA was computed on 250K\nsongs that were not identiﬁed as covers (i.e., in neither the\nSHS train or test sets).\nMany of the parameters above were chosen empirically.\nWe do not present the list of parameters we tried for lack\nof space and the little interest they carry. Simply note that\nthe following parameters were chosen based on their abil-\nity to identify 500 train covers (see Subsection 4.3, ﬁrst\nexperiment):\n\u000fpatches of size 75beats, we tried number of beats\nranging from 8to100;\n\u000fmedian as opposed to average;\n\u000fraising to the power 1:96, we tried values between\n0:25 and3:0.\nThe number of PCs we keep after PCA is also a parameter,\nbut choosing the best one is more difﬁcult. The number of\nPCs is a trade-off between accuracy and feature vector size\n(and hence speed). We believe 50is the “best” trade-off,\nbut we report results for other numbers of PCs. Still re-\ngarding PCA, since we use chroma patches of 75beats, we\nhave 12\u000275 = 900 principal components. Note that half\nof the components (450) are redundant due to the symme-\ntry in the 2DFTM, and have a variance of zero associated\nto them.\n4.2 Reference methods\nWe compare our algorithm to other methods, at different\nscales depending on the speed of the algorithm. We start\nwith our previous work [2], the only reported result on the\nSHS to our knowledge. Also taken from that work, we\nreport again a comparison with the method from [8].\nWe also test a DTW-based method based on [24] using\ncode from S. Ravuri5. This is more of a sanity check than\na full comparison; the authors in [24] used up to 36semi-\ntones instead of the 12we possess, we did not re-estimate\nthe DTW parameters, etc. It is likely that the full system\nfrom [24] outperforms our method, the problem being the\nexecution time which is prohibitive on a large scale. In our\nimplementation, each DTW comparison takes on the order\nof10ms. One query on the MSD would therefore take\n5http://infiniteseriousness.weebly.com/\ncover-song-detection.htmlabout 2:7hours. Thus we do this comparison only on our\n500binary queries.\nFinally, we compare with pitch histograms, a feature\nthat was suggested for music identiﬁcation in [25]. The\npitch histogram of a song is the sum of the energy in the\n12semitones normalized to one. In our case, we compute\nit from beat-aligned chroma features. This feature is not\npowerful enough to perform cover recognition on its own,\nbut it gives an idea of how much more information our\nmethod can encode in a \u001810-dimensional feature.\n4.3 Experiments\nMethod accuracy\nrandom 50:0%\npitch hist. 73:6%\ncorrelation 76:6%\nDTW 80:0%\njcodes 1 [2] 79:8%\njcodes 2 [2] 77:4%\n2DFTM (full) 82:0%\n2DFTM (200 PC) 82:2%\n2DFTM (50 PC) 82:2%\n2DFTM (10 PC) 79:6%\n2DFTM (1 PC) 66:2%\nTable 1. Results on 500binary tasks. PC is the number of\nprincipal components we retain after PCA. Empirically, 50\nPC appear to be the best trade-off between accuracy and\nsize.\nWe follow the methodology of [2] where the parame-\nters are ﬁrst tuned on a subset of 500binary tasks created\nwithin the SHS training set (we use the same 500queries).\nThe goal is: Given a query song Aand two songs Band\nC, ﬁnd which of BorCis a cover of A. The result is the\npercentage of trials where the algorithm succeeds. We then\npresent the results testing on the training set, mostly as a\nsanity check. Finally, we report result on the SHS test set\nusing the full MSD.\nIn [2], the main reported result was the average rank of\na known cover given a query. For instance, on 1M songs,\npicking at random would give 500K. We again report this\nmeasure to permit comparison, but for practical purposes\nthis number may be misleading since it is dominated by\nthe most difﬁcult covers, of which there will always be a\nnumber, and hides differences in performance near the top\nof the ranking. We now prefer to report results in terms of\nmean average precision (meanAP), which puts emphasis\non results that rank high, i.e., the covers that are in the top\nkresults. present the recall curves for a few algorithms\n(see Figure 6).\nAs we see in Table 3, the method based on our 2DFTM\nrepresentation provides a signiﬁcant improvement over the\nmethod in [2] for both measures. In particular, based on\nmeanAP, many more covers are ranked in the ﬁrst few hun-\ndred results, which makes it much more valuable in a com-\nmercial system. Note that a second, slower step could be\napplied to the top kresults, kbeing 1K or 10k, similar to\nthe progressive reﬁnement in [27]. A good candidate for\nthis second step would be the full system of [24].Method average rank mean AP\npitch hist. 4;032:9 0:01851\n2DFTM (full) 3;096:7 0:08912\n2DFTM (200 PC) 3;005:1 0:09475\n2DFTM (50 PC) 2;939:8 0:07759\n2DFTM (10 PC) 3;229:3 0:02649\n2DFTM (1 PC) 4;499:1 0:00186\nTable 2. Results on the training set (12; 960songs). For\naverage rank, lower is better. For meanAP, higher is better.\nMethod average rank mean AP\nrandom 500; 000 \u00180:00001\npitch hist. 268; 063 0:00162\njcodes 2 308; 370 0:00213\n2DFTM (200 PC) 180; 304 0:02954\n2DFTM (50 PC) 173; 117 0:01999\n2DFTM (10 PC) 190; 023 0:00294\n2DFTM (1 PC) 289; 853 0:00003\nTable 3. Results on 1M songs. For average rank, lower is\nbetter. For meanAP, higher is better.\nUsing the system with 50principal components, Figure\n6 shows us that more than half of the covers are ranked in\nthe top 100k and more than a quarter of them are in the top\n10k. For 112queries, the ﬁrst song returned was a known\ncover. We ignore here the songs that might have ranked\nsecond or third after other known covers. This includes the\npairs (Hayley Westenra, Kate Bush) performing “Wuther-\ning Heights” and (The Boomtown Rats, G4) performing “I\ndon’t like Mondays”, matches considered easy in [2].\nIn terms of speed, with a 50-dimensional vector per\nsong, ranking all one million for all 1;726test covers in\nPython took 1h46min on a machine with plenty of RAM.\nThis is around 3-4seconds per query without any optimiza-\ntion or parallelization. Compared to the 2:7hours of the\nDTW method, that is a \u00182;500x speedup.\n4.4 Out of collection data\nUsing audio as the query input with the SHS is a chal-\nlenge, beat-synchronous features relies on a consistent beat\ntracker. Fortunately, The Echo Nest provides an open API\nwhich will convert any uploaded audio into the format pro-\nvided in the Million Song Dataset. We experimented with\nthis using cover songs found on YouTube. For instance,\nthe song “Summertime” by Ella Fitzgerald and Louis Arm-\nstrong6was correctly associated with a cover version by\n6http://www.youtube.com/watch?v=MIDOEsQL7lA\n0 200000 400000 600000 800000 1000000\ncutoff0.00.20.40.60.81.0proportion query - cover pairs50 PC\n200 PC\npitch hist.\njcodes\nFigure 6. Recall using (query, cover) pairs on the full million\nsongs for different systems. Legend is in order from best (upper\nleft corner) to worst.Joshua Redman (ﬁrst match). The Ella Fitzgerald and\nLouis Armstrong version present in the SHS was found at\nrank 9. The fact that this was not the ﬁrst match might\nbe explained by the lower audio quality on YouTube, or it\ncould be a different version.\n5. CONCLUSION\nThe 2DFTM allows us to pose the search for cover songs\nas estimating an Euclidean distance. We show that this rep-\nresentation exhibits some nice properties and improves the\nstate-of-the-art on large-scale cover song recognition. Fur-\nthermore, obtaining good results using patches of 75beats\nsuggests an easy way to include more time dependency in\nthe harmonic representation. Future work will look into\nthe usefulness of this representation for other tasks, such\nas music tagging and segmentation. Finally, all our code is\navailable online7.\n6. ACKNOWLEDGMENTS\nThe authors would like to thank The Echo Nest and SecondHand-\nSongs for making the MSD and the SHS available respectively.\nThanks to S. Ravuri for his DTW code. TBM is a NSERC PGD\nscholar. This work has been supported by a gift from Google and\nby the NSF under grant IIS-0713334.\n7. REFERENCES\n[1] T. Bertin-Mahieux, D. Ellis, B. Whitman, and P. Lamere. The\nmillion song dataset. In Proceedings of ISMIR, 2011.\n[2] T. Bertin-Mahieux and D.P.W. Ellis. Large-scale cover song\nrecognition using hashed chroma landmarks. In Proceedings\nof WASPAA, New Platz, NY , 2011.\n[3] T. Bertin-Mahieux, G. Grindlay, R. Weiss, and D. Ellis. Eval-\nuating music sequence models through missing data. In Pro-\nceedings of ICASSP, 2011.\n[4] T. Bertin-Mahieux, R. Weiss, and D. Ellis. Clustering beat-\nchroma patterns in a large music database. In Proceedings of\nISMIR, 2010.\n[5] M. Casey and M. Slaney. Fast recognition of remixed music\naudio. In Proceedings of ICASSP. IEEE Signal Processing\nSociety, 2007.\n[6] J.S. Downie. The music information retrieval evaluation\nexchange (2005–2007): A window into music informa-\ntion retrieval research. Acoustical Science and Technology,\n29(4):247–255, 2008.\n[7] The Echo Nest Analyze, API, http://developer.\nechonest.com.\n[8] D. Ellis and G. Poliner. Identifying cover songs with chroma\nfeatures and dynamic programming beat tracking. In Pro-\nceedings of ICASSP. IEEE Signal Processing Society, 2007.\n[9] T. Fujishima. Realtime chord recognition of musical sound:\na system using common lisp music. In Proceedings of the\nInternational Computer Music Conference (ICMC), 1999.\n[10] E. G ´omez. Tonal description of polyphonic audio for mu-\nsic content processing. INFORMS Journal on Computing,\n18(3):294–304, 2006.\n7http://www.columbia.edu/ ˜tb2332/proj/\ncoversongs.html[11] R.C. Gonz ´alez and R.E. Woods. Digital image processing.\nPearson/Prentice Hall, 2008.\n[12] ¨O.˙Izmirli and R.B. Dannenberg. Understanding features and\ndistance functions for music sequence alignment. In Proceed-\nings of ISMIR, 2010.\n[13] T. Jehan. Creating music by listening. PhD thesis, Mas-\nsachusetts Institute of Technology, 2005.\n[14] J.H. Jensen, M.G. Christensen, and S.H. Jensen. A chroma-\nbased tempo-insensitive distance measure for cover song\nidentiﬁcation using the 2d autocorrelation function. MIREX\ncover song identiﬁcation contest, 2008.\n[15] N. Jiang, P. Grosche, V . Konz, and M. M ¨uller. Analyzing\nchroma feature types for automated chord recognition. In\nProceedings of the 42nd AES Conference, 2011.\n[16] S. Kim and S. Narayanan. Dynamic chroma feature vectors\nwith applications to cover song identiﬁcation. In Multimedia\nSignal Processing, 2008 IEEE 10th Workshop on, pages 984–\n987. IEEE, 2008.\n[17] F. Kurth and M. M ¨uller. Efﬁcient index-based audio match-\ning.Audio, Speech, and Language Processing, IEEE Trans-\nactions on, 16(2):382–395, 2008.\n[18] M. Marolt. A mid-level representation for melody-based re-\ntrieval in audio collections. Multimedia, IEEE Transactions\non, 10(8):1617–1625, 2008.\n[19] R. Miotto, N. Montecchio, and N. Orio. Content-based cover\nsong identiﬁcation in music digital libraries. In Digital Li-\nbraries, pages 195–204. Springer, 2010.\n[20] R. Miotto and N. Orio. A music identiﬁcation system based\non chroma indexing and statistical modeling. In Proceedings\nof ISMIR, 2008.\n[21] M. M ¨uller, F. Kurth, and M. Clausen. Audio matching via\nchroma-based statistical features. In Proceedings of ISMIR,\nLondon, UK, 2005.\n[22] L. Oudre, Y . Grenier, and C. F ´evotte. Chord recognition\nby ﬁtting rescaled chroma vectors to chord templates. IEEE\nTransactions on Audio, Speech and Language Processing,\n19(7):2222 – 2233, Sep. 2011.\n[23] J. Serr `a.Identiﬁcation of versions of the same musical com-\nposition by processing audio descriptions. PhD thesis, Uni-\nversitat Pompeu Fabra, Barcelona, 2011.\n[24] J. Serr `a, E. G ´omez, P. Herrera, and X. Serra. Chroma binary\nsimilarity and local alignment applied to cover song identiﬁ-\ncation. IEEE Transactions on Audio, Speech, and Language\nProcessing, 16(6):1138–1151, 2008.\n[25] G. Tzanetakis, A. Ermolinskyi, and P. Cook. Pitch histograms\nin audio and symbolic music information retrieval. Journal of\nNew Music Research, 32(2):143–152, 2003.\n[26] A. Wang. An industrial strength audio search algorithm. In\nProceedings of ISMIR, 2003.\n[27] Y . Yu, M. Crucianu, V . Oria, and L. Chen. Local summariza-\ntion and multi-level LSH for retrieving multi-variant audio\ntracks. In Proceedings of the seventeen ACM international\nconference on Multimedia, pages 341–350. ACM, 2009."
    },
    {
        "title": "Semiotic Structure Labeling of Music Pieces: Concepts, Methods and Annotation Conventions.",
        "author": [
            "Frédéric Bimbot",
            "Emmanuel Deruty",
            "Gabriel Sargent",
            "Emmanuel Vincent 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416412",
        "url": "https://doi.org/10.5281/zenodo.1416412",
        "ee": "https://zenodo.org/records/1416412/files/BimbotDSV12.pdf",
        "abstract": "Music structure description, i.e. the task of representing the high-level organization of music pieces in a concise, generic and reproducible way, is currently a scientific challenge both algorithmically and conceptually. In this paper, we focus on semiotic structure, i.e. the description of similarities and internal relationships within a music piece, as a low-rate stream of arbitrary symbols from a limited alphabet and we address methodological ques- tions related to annotation. We formulate the labeling task as a blind demodulation problem, whose goal is to identify a minimal set of semi- otic codewords, whose realizations within the music piece are subject to a number of connotative variations viewed as modulations. The determination of labels is achieved by combining morphological, paradigmatic and syntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to de- fine their individual properties, (ii) the support of proto- typical structural patterns to guide the comparison be- tween blocks and (iii) a methodology for the determina- tion of distinctive features across semiotic classes. Specific notations are introduced to account for unresolv- able semiotic ambiguities, which are occasional but must be considered as inherent to the music matter itself. A set of 500 music pieces labeled in accordance with the pro- posed concepts and annotation conventions is being re- leased with this article.",
        "zenodo_id": 1416412,
        "dblp_key": "conf/ismir/BimbotDSV12",
        "keywords": [
            "Music structure description",
            "semiotic structure",
            "algorithmic challenge",
            "conceptual challenge",
            "semiotic description",
            "low-rate stream",
            "arbitrary symbols",
            "limited alphabet",
            "labeling task",
            "blind demodulation problem"
        ],
        "content": "SEMIOTIC STRUCTURE LABELING OF MUSIC PIECES : \nCONCEPTS, METHODS AND ANNOTATION CONVENTIONS  \nFrédéric BIMBOT  Emmanuel DERUTY  Gabriel SARGENT Emmanuel VINCENT \nIRISA / METISS  \nCNRS - UMR 6074, France  \nfrederic.bimbot@irisa.fr  INRIA / METISS  \nRennes , France  \nemma nuel.deruty@gmail.com  IRISA / METISS  \nUniversité Rennes 1, France  \ngabriel.sargent@irisa.fr INRIA / METISS  \nRennes, France  \nemmanuel.vincent@inria.fr \nABSTRACT \nMusic structure description, i.e. the task of representing \nthe high-level organization of music pie ces in a concise, \ngeneric and reproducible way, is currently a scientific challenge both algorithmically and conceptuall y. In this \npaper, we focus on semiotic structure , i.e. the description \nof similarities and internal relationships within a music \npiece, as a low-rate stream of arbitrary symbols from a \nlimited alphabet and we address methodological que s-\ntions related to annotation. \nWe formulate the labeling task as a blind demodulation \nproblem, whose goal is to identify a minimal set of sem i-\notic codewords, whose realizations within the music \npiece are subject to a number of connotative variations  \nviewed as modulations . The determination of labels is \nachieved by combining morphological, paradigmatic and \nsyntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to d e-\nfine their individual properties, (ii) the support of prot o-\ntypical structural patterns to guide the comparison b e-\ntween blocks and (iii) a methodology for the determin a-\ntion of distinctive features across semiotic classes. \nSpecific notations are introduced to account for unresol v-\nable semiotic ambiguities, which are occasional but must \nbe considered as inherent to the music matter itself. A set \nof 500 music pieces labeled in accordance with the pr o-\nposed concepts and annotation conventions is being r e-\nleased with this article. \n1. INTRODUCTION \nMusic can be defined as “the art, process and result of deliberately arranging sound items with the purpose of reflecting and affecting senses, emotions and intellect” \n[1]. From a more operative viewpoint, music can be a p-\nproached as a set of sounds organized by human com pos-\ners for human listeners. From these definitions, the role of \nstructure in the musical process appears as rather esse n-\ntial, as it is both a constituent and a support of the mus i-\ncal discourse.  \nIn the domain of MIR, music structure is frequently con-\nsidered as a central element to music description and \nmodeling, but also as a scientific challenge, both alg o-\nrithmically and conceptually [2]. This situation has tri g-\ngered significant effort in the MIR community, towards \nthe production of annotated resources [3][4] and the o r-\nganization of evaluation campaigns [5 ]. At the scale of an entire piece, music structure is a co n-\ncept which can be approached in several ways : \na. The acoustic structure which consists in describing \nthe course and turns of active sources and/or timbral textures within the piece : singer(s) , lead entries, i n-\nstrumentation , etc…  \nb. The functional structure which is based on usual \ndesignations of the different parts in terms of their \nrole in the music piece, for instance : intro – verse – \nchorus – bridge – etc… (cf. [6], for instance) ,  \nc. The semiotic structure which aims at representing, \nby a limited set of arbitrary symbols (called labels), \nthe similaritie s (and interrelations) of structural \nsegments within the piece. \nThese various views of music structure have influenced \nthe design of methods and algorithms for the automatic \nanalysis of audio data , for instance [7][8][9]. \nHowever, in spite of a need and an interest for method o-\nlogical and operational concepts [ 10][11], there is no \nwell-established principles for the structural annotation of music pieces, either in terms of problem statement, pr o-\ncedure, or annotation conventions, even in “simple” cases \nlike pop songs. \nIn this context, some of our former work [ 12][13] has \nbeen focused on the definition of structural block bound a-\nries. In this article, we address the labeling task, i.e. the \ndetermination of equivalence classes between structural \nsegments so as to obtain a symbolic transcription of the \npiece’s structure. Our methodology is primarily designed for audio data but can also be applied to written music. \nBy approaching a music piece as a “communication sys-\ntem”, we formulate (section 2) the labeling task as the \nresolution of an ill-posed problem, for which the solution \nis seeked by assuming that recurring properties and sys-\ntematic differences across structural blocks are more prone to be semiotically relevant than irregular and occ a-\nsional variations. \nWithin this scope, semiotic analysis aims at ensuring a \ntrade-off between : \n- coverage : i.e. to encompass the largest possible nu m-\nber of musical properties in the semiotic description \n- regularity : i.e. to obtain a transcription as regular as \npossible and relating to a simple prototypical pattern. \n- accuracy : i.e. to account as faithfully as possible for \nthe distinctive properties across semiotic elements. \n- compactness  : i.e. to limit the semiotic alphabet to a reasonable number (and distribution) of elements. \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted withou t fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval  This trade-off is obtained by combining methodological \ncriteria based on morphologic, syntagmatic and paradi g-\nmatic considerations (section 3). \nIn section 4, we introduce annotation conventions which \ncover the most typical situations but which also handle \noccasional semiotic ambiguities, i.e. segments undecide d-\nly belonging to two classes at the same time. \nWe release, with this article, a set of approximately 500 \nmusic pieces, annotated with the proposed conventions, and accompanied with additional documentation. \n2. CONCEPTUAL APPROACH \nThe semiotic annotation of a music piece consists in su m-\nmarizing its high-level structure as a short sequence of arb i-\ntrary symbols drawn from a limited alphabet, for instance : \nA  B  C  D  A  B  C  D  E  C  D  C  D  D \nIn the scope of this work, w e suppose that the elements \nthus indexed are structural segments (or blocks) of com-\nparable size and at a typical timescale between 10 and 25 seconds. \n A music piece viewed as a communication system  2.1\nAssuming the existence of a semiotic description of music \nstructure is intimately linked to the hypothesis of an unde r-\nlying communication scheme which governs, at the stru c-\ntural scale, the global narrative organization of the piece. \nIt is rather commonplace to consider music in general a s \na means of communication, based on a set of rules and \nconventions (which clearly depend on the type of music \nunder consideration). Here, we consider that each music \npiece can itself be viewed as the output of a particular \ncommunication system, with its own constituents : a \nsender (the composer), one (or several ) receiver(s)  (i.e. \nlisteners), a transmission channel (which can take various \nforms), a message (the musical narration ) and a code, \nnamely the alphabet of semiotic elements  (or codewords) \nwith which is built and developed the narration. \nFrom this viewpoint, the codewords are fundamentally \npiece-specific and they are discovered (and inferred) grad-\nually by the listener while the piece unfolds, i.e. while the musical narration develops. Describing the semiotic struc-ture of a music piece can thus be viewed as a deciphering \ntask based on the observation of the output of a commun i-\ncation system (possibly with the help of more or less co n-\nscious knowledge of composition conventions). \n Semiotic structure : an ill-posed inverse problem 2.2\nSemiotic structure description falls in the category of ill-\nposed inverse problems , and therefore cannot be solved \nunequivocally unless additional constraints are incorp o-\nrated to condition the solution. \nOne option could be to try to formulate the conditioning \nconstraints in terms of particular properties of the music \nsubstance, more or less specific to the genre of the piece. \nHowever, this approach would require sharp expertise, \ngeneral concordance and a stable status of the compos i-\ntional rules for all genres, which is difficult to imagine and which would certainly raise major problems of com-\nparability and compatibility of the result across pieces \n(and annotators). \nOur objective is to formulate the properties of semiotic \nelements (i.e. the conditioning constraints ) as generically \nas possible. This is why, we propose that the structural description of the piece should be approached as some sort of blind information demodulation problem , i.e. the \nseparation of a carrier (the sequence of codewords) and a \nmodulation  (the variations in the realization of the code-\nwords), solely based on the behavior and relationships  of \nmusical properties, but not on their actual substance. \n Inferring a semiotic code : an intuitive example 2.3\nLet’s consider the following sequence : \n \nA careful study of this sequence reveals that : \n- Almost all items are directional. \n- Only 4 distinct orientations are observed (say N, E, \nSE and NW).  \n- Gray level varies but does not show any regular pat-\ntern, nor does the tail of the objects, nor their size. \n- Orientation is driven by a strong syntax (for instance, \nSE is always followed by NW, NW never by E, etc…) \n- The shape of item #9 is singular (could be interpreted as pointing SE or NW, though) \nLet’s now consider this second sequence : \n \nHere, we observe that : \n- Almost all items are (also) directional but \n- Direction is taking all sort of random, unquantizable \nvalues \n- Gray level takes only 4 different values (say 20, 40, 60 or 80 %) \n- The tail of the objects (still) does not show any reg u-\nlar pattern, nor their size. \n- Gray level is driven by a strong syntax (for instance \n20 % is always followed by 80 %) \n- The gray-level of item #9 is singular / undecided \nThe successive values or states taken by the various pro p-\nerties constitute information layers  and, in both cases, a \nparticular layer exhibits some systematic and organized \nbehavior : “orientation” in the first sequence, “gray-level” \nin the second one . The knowledge of their behavior con-\nveys, at a low explicative cost, significant information on the overall sequence. At the same time, the other prope r-\nties appear mostly as creating different variants (or co n-\nnotations) in the realization of these properties. \nIn both cases, the symbolic representation of the most o r-\nganized information layer is : \nA  B  C  D  A  B  C  D  E  C  D  C  D  D  \nand in fact, considering any other property (or combin a-\ntion of properties) would create rather uninformative , \nmuch less regular or almost trivial descriptions , such as \nABCDE …KLMN, AABCBCDAEBADAA , or AAA… ABAAAAA . \n The carrier-modulation model 2.4\nIn the proposed approach, we assume that the sequence of \nstructural elements can be decomposed into : \n- A carrier component  which is built on information \nlayers whose behavior, is periodic, cyclic, regular, r e-\ncurrent, repeated, correlated, quantized, organized…  \n- A modulation component  which corresponds to i n-\nformation layers which appear as aperiodic, acyclic, \nirregular, occasional, erratic, sporadic, uncorrelated , \nisolated, continu ous-valued, scattered, etc…  \nThe sequence of semiotic labels describes the succession \nof property values  taken by the carrier component. The \nmodulation component represents circumstantial or inc i-\ndental variations of the semiotic elements \n Application to music 2.5\nAt the level of a short musical passage, successions of \nnotes belonging to a given musical scale form an acoustic \nmelody whose properties (amplitude, duration, attack, …) are modulated over time to convey expressivity. \nSimilarly, at the level of the whole piece, the succession \nof structural blocks forms some sort of semiotic tune , \nbuild on the “scale” of semiotic elements and whose \nmodulation constitut e connotative variants  across diffe r-\nent stages of the musical narration. However, as opposed to conventional music units, those “macro-notes” are \npiece-dependent and they are primarily inferred by d e-\ntecting and comparing structural elements over the whole \npiece.  \nAt this point, it is very important to note that the partic u-\nlar status of a property as being either semiotic (carrier) \nor connotative (modulation) cannot be a priori decided \non the single basis of its nature.  \nIndeed, in a number of pieces , structural blocks are built \non a few distinct harmonic progressions which recur \nthroughout the piece together with a strong variability of the melodic line, whereas other pieces may be bu ilt on a \nunique harmonic cycle from the beginning to the end, the melodic line being the only distinctive feature between blocks. Some techno or electro pieces rather use the ti m-\nbre or the texture to create structural patterns over a co n-\nstant melodic-harmonic loop. In percussive pieces, the structural organization usually stems from rhythmic \nproperties, etc…  \nTherefore, the analysis of semiotic structure primarily r e-\nquires to identify , for each music piece , which are the mu-\nsical information layers  (melody, harmony, rhythm, etc…) \ntaking part to  the semiotic component. \n Semiotic features 2.6\nLet us now consider a third toy example, which we su p-\npose to be an other realization of ABCDABCDECDCDD  : \n \nHere we have a slightly more complex situation than in \nsection 2.3, in the sense that the structure of the sequence is based on a switch of the semiotic property on which the \ncarrier component is built, as summarized in the table b e-\nlow  : \nSymbol Orientation Gray-level Tail-shape Size \nA North any any any \nB East any any any \nC any 20 % any any \nD any 80 % any any \nE Indistinct Multiple any any \nSuch a situation is very common in music : for instance in \na song, verses 1 and 2 may consist of two distinct melodies based on a same harmonic progression while chorus es 1 \nand 2 may be based on two distinct chord loops (while the melodic line would be identical or almost). Indeed, in the \ncase of music, semiotic structure relies on musical prope r-\nties whose nature varies across pieces but which may also \nvary within a given piece.  \nSemiotic annotation therefore requires the determination \nof what we call semiotic features , i.e. not only the semio t-\nic properties but also the particular values taken by these \nproperties to form the carrier component describing the sequence of structural blocks . In the case above , the se-\nmiotic properties are orientation  and gray-level , while the \nsemiotic features are North orientation , East orientation , \n20 % gray-level  and 80 % gray-level  (the other values of \norientation and gray-level being irrelevant to the carrier \ncomponent ). \nIt is also worth noting that, in this example, the synta g-\nmatic organization of the sequence plays an essential role \nin guiding the determination of the relevant semiotic fe a-\ntures. Indeed, East is always followed by 20 %, itself a l-\nways followed by 80 %, and more globally, the pattern \nN-E-20-80 is observed twice. \n The three ingredients of semiotic structure 2.7\nTo sum up the underlying process at work behind semio t-\nic analysis, we can distinguish 3 levels of reasoning which jointly participate to the determination of the s e-\nmiotic description : \n- Morphological analysis : intrinsic features of the \nstructural elements composing the sequence (i.e, the properties and the values of these properties). \n- Syntagmatic analysis : local relations that elements \nexhibit with their neig hbors within the piece \n- Paradigmatic analysis :  similarities and differences  \nwhich they exhibit with other elements.  \nThe following sections investigate in details these three facets of semiotic structure analysis and how they interact with one another. \n3. METHODOLOGICAL AXES \n Morphological analysis 3.1\nThe morphological analysis of structural blocks is based on the System & Contrast  (S&C) model [14]. \nUnder this approach, each structural block is assumed to \nbe built around 4 morphological elements  (of typically 2 \nbars each) forming a square carrier system. These ele-\nments relate through a (usually 2x2) matrix of simple r e-\nlationships. Structural blocks can be more complex, but \nthey usually can be reduced to a square stem. \nIn general, on some musical information layers, the 4th \nelement departs from the logical sequence formed by the \nfirst three (thus creating some sort of punctuation).  \nThe morphology of a square S&C can be written as : \n    ( )   ( )   ( ( ( )))  \nwhere a is the “seed” morphological element, f and g are \nthe internal relations between the elements forming the \ncarrier system and   a contrast function that represents \nthe (relative) disparity of the 4th element. S&Cs exist on \nseveral musical layers in different forms and at different \ntimescales simultaneously. Their synchronization con-\ntributes to the musical consistency of the segment and to \nits autonomy [12]. Identifying S&Cs is thus very useful \nto locate, at the chosen timescale, the boundaries of the \nstructural blocks.  \nA S&C can be summarized as a quadruplet :        , \nwhich can be view ed as the “genetic code” of the stru c-\ntural block. Moreover , in many situations, either f or g (or \nboth) are “identity” ( id), resulting in well-identifiable \nmorphological patterns such as aaaa, abab, aabb  (for \n    ) or aaab, abac, aabc  (for     ). These patterns \ncan straightforwardly be extended to “close-to-id” or \n“begins-like” functions : aaa’b, aba’c, aa’bc, …  \nThe morphology of structural blocks can therefore be \nprimarily characterized by the various systems followed \nby its (say p) active musical layers, i.e. as a multi-\ndimensional quadruplet  (       )       . In many ca s-\nes, this quadruplet can be represented more simply as the \nmorphological pattern governing each layer : for instance, \n         for the melodic line ,           for the harmony, \n         for the drum loop, etc… \n Syntagmatic analysis 3.2\nMoving back now to the timescale of the entire piece, we \ndiscuss how the position and context of structural blocks \nwithin the piece can be taken into account in order to \nguide semiotic labeling. \nIndeed two structural blocks will be considered to be a \npriori more likely to belong to the same equivalence class \nif they appear in similar contexts in the piece, i.e. if the y \nare located beside similar left and/or right segments with-\nin the piece. For instance, in the sequence ABxDAB y-\nDECDCDD, x and y are more likely to belong to the same \nsemiotic class than in ABxDyBCDECDCDD . This criterion \npartly relates to commutability, often used in semiotic \nanalysis. \nA second syntagmatic factor to take into account is that \ndifferences between two blocks should not be appreciated \nin the same way if the two blocks are immediately next to \neach other or if they appear at some distance in the piece : \na slight difference observed between two successive sim i-lar blocks may be distinctive (especially if this opposition \nis recurrent in the piece) whereas a stronger difference at \na long distance may just be a connotative variation, esp e-\ncially if the two blocks occur in similar contexts. \nThe guidance of a prototypical structural pattern  (see \nTable I) is also an essential element of syntagmatic anal y-\nsis, for weighing similarities and differences between and \nacross blocks and interpreting them with respect to the \nglobal organization of the piece. \nHowever, while the semiotic structure of music tends to \nbe based on recurrent patterns, the actual realization  of a \nstructural pattern in a music piece generally shows irreg u-\nlarities (which are bound to increase when getting t o-\nwards the end of the piece). For instance the structural \ndescription ABCDABCDECDCDD can be viewed as a real i-\nzation of 4 cycles of an (ABCD) structural pattern with \ngrowing irregularities towards the end of the piece. \nIn practice, structural patterns can prove to be very eff i-\ncient to guide the annotation for some musical genres, but \nthey can also turn out to be totally useless for others. \n Paradigmatic analysis 3.3\nThe goal of paradigmatic analysis is to determine the set \nof semiotic features within the population of structural \nblocks, i.e. what are the semiotic properties (and the val-\nues taken by these properties) which characterize the \nequivalence classes (cf. subsection 2.6 ). \nA key concept of this process is that, rather than comparing \nthe surface properties of the structural segments, the sem i-\notic comparison of blocks is based primarily on the co m-\nparison of their carrier system , i.e. the triplet       (as \ndefined in subsection 3.1) resulting from the ir morpholog i-\ncal analysis. Note that the contrast function   is treated \nseparately, as a special form of modulation (see 4.1).  \nFor a given music information layer, the carrier systems \nof two structural blocks x and y are considered as homo l-\nogous, if there exist a property of that layer for which the \ntriplets(        ) and (        )  are similar. For i n-\nstance, if the musical layer is the melody, the property \ncan be the melody itself, the support notes of the melody, \nthe shape of the melodic line, etc… \nThis comparison is carried out for all musical layers \nwhich show some morphological organization and the \nsubset of common properties that emerges from the co m-\nparison provides the characteristics of a potential class \nencompassing x and y. In particular, if the similarity of \nthe systems holds for all music information layers active \nin x and y, it is considered that these segments should be \ngrouped into  a single semiotic clas s. \nSemiotic features can thus be hypothesized as conjun c-\ntions of properties ( together with  their particular values) \noccurring in similar S&Cs and these features can be o r-\ndered (at least partly) according to their coverage of the \nvarious musical information layers. \nA global solution is then searched (empirically) as a part i-\ntion of the population of structural segments grouping \nthose with equivalent  carrier systems in the subspace of their semiotic featur es. In case of several possible sol u-\ntions, the one yielding the most regular sequence of labels \nis chosen. \nFinally, the set of distinctive features  is established as the \nminimal subset of properties (and their particular values) \nwhich is necessary and sufficient to distinguish each s e-\nmiotic element from all the others. \nOf course, the trade-off between accuracy of the descri p-\ntion and compactness of the semiotic set is an essential stopping condition. It is conjectured that a “good” a pos-\nteriori distribution of labels should follow some sort of \nZipf law, or at least should not depart too much from it. \n4. ANNOTATION CONVENTIO NS \n Primary notation of semiotic labels  4.1\nQuite naturally, two blocks with non-equivalent  carrier \nsystems are denoted by 2 distinct alphabetic capital le t-\nters : A vs B. \nWhen two blocks show equivalent carrier systems but \ndifferent contrasts (  ), this difference is noted as a sub-\nscript : A1 vs A2. Blocks showing no (or extremely weak) \ncontrasts are denoted A0 and conversely, blocks showing \nexceptional contrasts are denoted A* (they usually tend to \noccur at the end of the piece). \nWhen two block s have equivalent carrier systems , but \nthey significantly differ in their (surface) realizations \n(connotative variants) , they are denoted with distinct s u-\nperscripts, for instance : A’ vs A’’. Optionally, the supe r-\nscript may be chosen specifically to indicate the na ture of \nthe variant. For instance, the notations A+ and A- are used \nto indicate more or less rich occurrences, and A~ can be \nused to denote exceptional variants of  A. \nWhen a property evolves gradually within a block , this is \ndenoted as a fade- in or a fade-out. This is denoted as <A, \nA> and it may apply to surface properties such as the i n-\ntensity, the instrumentation support, or to strengthening \nor vanishing properties of the carrier sys tem. \nIf a block is realized only in a half-form, specific not a-\ntions are used : A/2 for a half-size block, |A and A| for \nleft (resp. right) truncated half block, more general cases \nof incomplete blocks being denoted as …A or A…. \nTable II summarizes these annotation conventions. \n Composite labels for handling ambiguities 4.2\nInevitably, semiotic labeling leads to some situations \nwhich exhibit ambiguities, resulting from the combina-\ntion or the mutation of semiotic items to create new h y-\nbrid ones, namely some sort of chord, at the level of the semiotic structure. This is indee d a natural process in m u-\nsic at many time-scales  \nTherefore, a set of additional notations were designed to \nrender these ambiguities through composite labels (whose \nconfigurations are schematized on figure 1). AB (vertical hybrid ) : block showing undecided prev a-\nlence of properties of A and B, for instance, superposition. \nA&B (intrication ) : block showing intertwined portions of \nA and B. The size of block A&B  is the total of that of A \nand B. \nB|A (horizontal hybrid ) : a specific system ( B) is present \nin the first half of the block but the second half recall s the \nsystem of A (sometimes with a different contrast). \nB<A\n  (kinship) : block B is acceptable as autonomous at the \ncurrent timescale but it exhibits strong cohesion with the \nprevious block A via some commo n property or super-\nsystem. The sequence [A][B<A] could be described as a \nsingle morphological system at the immediately upper \ntimescale.  \nB/A (mutation) : morphological system partly similar to A \n(rooted in A) but with a subset of properties whose sy s-\ntem strongly departs from that of the other elements of \nclass A. This is typically the case in some types of solos , \nwhere the harmony is common to some former block but \nthe melody of the lead becomes freestyle . This situation \nalso includes cases when the subset of properties is sim p-\nly void, for example a passage (in particular, an intro) \nwhere the instrumental background is played alone, wit h-\nout any main lead. \nA?B (indetermination ) : ambiguous segment which ca n-\nnot be annotated unequivocally . A typical situation like \nthis is B?A~, when it is impossible for the annotator to \nconclude whether the segment is a specific semiotic el e-\nment B or a very particular connotative variant of item  A. \nWe also introduce notations for short segments that occ a-\nsionally intervene in between  regular ones : \nA_B (overlap) : i.e. tiling segment corresponding to a pa r-\ntial superposition between A and B. \n_AB_ (connexion ) : short segment  located between A and \nB which cannot decidedly be related primarily to A or B. \n \nFigure 1 : composite labels (schematic configurations) \n Proto-functional symbols 4.3\nEven though semiotic structure description is distinct \nfrom functional structure description, we consider that it \ncan be informative to choose the semiotic labels in such a \nway that they somehow reflect the proto-functional  status \nof the block within the piece. We therefore propose to \nuse, as much as possible , the alphabetic letters with the \ncorrespondence given in Table III. \n Transcription example 4.4\nSemiotic symbols can be put into brackets to facilitate \nvisual parsing, especially when they are composite. Due \nto a lack of space, we leave it to the reader to “decode” \nthe semiotic transcription represented below : \n[I/A] [A 1] [A 2] [B] [C] [J/2 ] [A’ 1] [|A’2] [B’] [C ] [X/C] [Y/2] [C *] \n Concluding remark 4.5\nWe want to underline that a primary objective of these \nnotations is to provide a consistent communication lan-\nguage for describing the most obvious aspects of the s e-\nmiotic structure of music, while also being able to reflect \nsome of its subtle ambiguities. \n5. ANNOTATION EFFORT AND FUTURE WORK \nThe set of approximately 500 music pieces, for which a n-\nnotations in terms of structural boundaries have been r e-\nleased in 2011 [13], has been updated and complemented \nwith semiotic labels obtained with the present methodo l-\nogy and is accessible at [ 15]. These annotations have \nbeen produced manually. They come with additional \ndocumentation and with the analysis of some difficulties \nmet during the annotation process. \nFuture work will be turned towards the formalization of \nthe concepts and methodology presented in this article in \nterms of information theory criteria, and their investig a-\ntion for the design of models and algorithm for the aut o-\nmatic inference of music structure. \nPrototype  Illustration  Codification  \nTrivial AAAAAAAA…  (A) \nBinary ABABABABAB…  (AB)  \nTernary ABCABCABC…  (ABC)  \nQuaternary  ABCDABCDABCD…  (ABCD)  \nAlternate  AABCCDAABCCD…  (2A,B,2C,D)  \nCyclic  ABBCDDDABBCDDD…  (A,2B,C,3D)  \nAcyclic  ABBCDDDEEF…  A,2B,C,3D,2E,…  \nErgodic  ABCDBADAAACBCC…  {ABCD}  \nTable I : Most common prototypical structural patterns  \n REFERENCES \n[1] Mixture of several definitions from various sources. \n[2] J. Paulus, M. Müller, A. Klapuri , Audio-based music stru c-\nture analysis, Proc. ISMIR 2010. \n[3] SALAMI Project : http://salami.music.mcgill.ca \n[4] QUAERO Project : http://www.quaero.org \n[5] MIREX 2011 : http://www.music-ir.org/mirex/2011 \n[6] Ten Minute Master n°18 : Song Structure - Music Tech \nMagazine, October 2003, pp. 62- 63. \n[7] J. Foote. Automatic audio segmentation using a measure of \naudio novelty. IEEE ICME , pp. 452–455, Aug. 2000. \n[8] J. Paulus. Signal Processing Methods for Drum Transcri p-\ntion and Music Structure Analysis. PhD Thesis , 2009. \n[9] M. Müller and F. Kurth. Towards structural analysis of a u-\ndio recordings in the presence of musical variations. EURASIP \nJournal on Advances  in Signal Processing , 2007. \n[10] G. Peeters, E. Deruty : Is Music Structure Annotation Multi \nDimensional ? LSAS, Graz (Austria) 2009. \n[11] J.B.L. Smith, J.A. Burgoyne, I. Fujinaga, D. De Roure, J.S. \nDownie : Design and Creation of a Large-Scale Database of \nStructural Annotations.  Proc. ISMIR 2011. \n[12] F. Bimbot, O. Le Blouch, G. Sargent, E. Vincent : Decom-\nposition into Autonomous and Comparable Blocks : A Structu r-\nal Description of Music Pieces, Proc. ISMIR 2010. \n[13] F. Bimbot, E. Deruty, G. Sargent, E. Vincent : Methodology \nand Resources for The Structural Segmentation of Music Pieces \ninto Autonomous and Comparable Blocks, Proc. ISMIR 2011. \n[14] F. Bimbot, E. Deruty, G. Sargent, E. Vincent : Compleme n-\ntary report to the article \"Semiotic Structure Labeling of Music \nPieces : Concepts, Methods and Annotations Conventions”. \nIRISA Internal Report n° 1996, June 2012. \n[15] http://musicdata.gforge.inria.fr \nAcknowledgements \nPart of this work was achieved in the context of the \nQUAERO project, funded by OSEO. \n \n Regular Specific \nSemiotic \nvariants A1, A2, A3, …, Ai, Aj A0, A* \nConnotative  \nvariants A’, A”, …, A(i), A(j) A+, A-, A~ \n \nFade-in / out Non-square Incomplete  \n<A \nA> A/2 or (1/2)  A \n(3/4)  A, (5/4)  A |A, A|  \n…A, A…  \nTable II : Main set of semiotic labels \n \n Intro Pre-\ncentral Central Post-\ncentral Relay Other \n(recurrent)  Other \n(sporadic)  Outro \nPrimary set  I, J A,B C,D E,F J,K M,N  X,Y,Z  K, L \nSecondary set  G,H P,Q R,S T,U G,H V,W  G,H \n Table III : Proto-functional semiotic symbols"
    },
    {
        "title": "Evaluating the Online Capabilities of Onset Detection Methods.",
        "author": [
            "Sebastian Böck",
            "Florian Krebs",
            "Markus Schedl"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416036",
        "url": "https://doi.org/10.5281/zenodo.1416036",
        "ee": "https://zenodo.org/records/1416036/files/BockKS12.pdf",
        "abstract": "In this paper, we evaluate various onset detection algo- rithms in terms of their online capabilities. Most meth- ods use some kind of normalization over time, which ren- ders them unusable for online tasks. We modified existing",
        "zenodo_id": 1416036,
        "dblp_key": "conf/ismir/BockKS12",
        "keywords": [
            "evaluation",
            "onset detection",
            "algorithms",
            "online capabilities",
            "normalization",
            "tasks",
            "existing",
            "methods",
            "modified",
            "online"
        ],
        "content": "EVALUATING THE ONLINE CAPABILITIES OF ONSET DETECTION\nMETHODS\nSebastian B ¨ock, Florian Krebs and Markus Schedl\nDepartment of Computational Perception\nJohannes Kepler University, Linz, Austria\nABSTRACT\nIn this paper, we evaluate various onset detection algo-\nrithms in terms of their online capabilities. Most meth-\nods use some kind of normalization over time, which ren-\nders them unusable for online tasks. We modiﬁed existing\nmethods to enable online application and evaluated their\nperformance on a large dataset consisting of 27,774 an-\nnotated onsets. We focus particularly on the incorporated\npreprocessing and peak detection methods. We show that,\nwith the right choice of parameters, the maximum achiev-\nable performance is in the same range as that of ofﬂine\nalgorithms, and that preprocessing can improve the results\nconsiderably. Furthermore, we propose a new onset detec-\ntion method based on the common spectral ﬂux and a new\npeak-picking method which outperforms traditional meth-\nods both online and ofﬂine and works with audio signals\nof various volume levels.\n1. INTRODUCTION AND RELATED WORK\nOnset detection, the task of ﬁnding musically meaningful\nevents in audio signals, is fundamental to many applica-\ntions: Real-time applications such as automatic score fol-\nlowers [7] can be enhanced by incorporating (online) onset\ndetectors that look for note onsets in a live performance,\nwhile (ofﬂine) onset detection is used increasingly to im-\nprove digital audio workstations with a view to event-wise\naudio processing.\nMany different methods of solving this task have been\nproposed and evaluated over the years. Comprehensive\noverviews of onset detection methods were presented by\nBello et al. in [2] and Collins in [6] (with special empha-\nsis on psychoacoustically motivated methods in the latter).\nDixon proposed enhancements to several of these in [9].\nAll methods were evaluated in an ofﬂine setting, using a\nnormalization over the whole length of the signal or apply-\ning averaging techniques which require future information.\nForonline onset detection, only few evaluations have\nbeen carried out: Brossier et al. [5] compared four on-\nset functions based on spectral features and proposed a\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.method for dynamic thresholding in online scenarios, us-\ning a dataset of 1,066 onsets for evaluation. Stowell and\nPlumbley [18] proposed adaptive whitening as an improve-\nment to short-time Fourier transform (STFT) based onset\ndetection methods and evaluated eight detection functions\nusing a dataset of 9,333 onsets. Glover at al. [12] applied\nlinear prediction and sinusoidal modeling to online onset\ndetection, but used a relatively small dataset of approxi-\nmately 500 onsets for evaluation.\nThese traditional onset detection methods usually incor-\nporate only spectral and/or phase information of the sig-\nnal, are easy to implement, and have modest computational\ncost. In contrast, methods based on machine learning tech-\nniques (e.g., neural networks in [11,15]) or on probabilistic\ninformation (e.g., Hidden Markov models in [8]) depend\non large datasets for training and are in general computa-\ntionally more demanding, which makes them unsuited for\nonline processing.\nThe onset detection process is usually divided into three\nparts (as shown in Figure 1): signal preprocessing, compu-\ntation of the actual onset detection function (ODF), and\npeak detection.\nODF Peak detection Signal Onsets Preprocessing\nFigure 1. Basic onset detection workﬂow.\nThere are generally two normalization steps that require\nspecial attention in an online context: The ﬁrst can be\nfound in the preprocessing step where many implementa-\ntions normalize the audio input prior to further processing.\nThe second and more widespread use of normalization\nis in the peak detection stage, where the whole ODF is\nnormalized before being processed further. An exception\nto this rule are some machine learning approaches like the\nneural network-based methods, since their detection func-\ntion can be considered as a probability function which al-\nready has the range [0::1]. Furthermore, most ofﬂine meth-\nods use smoothing or averaging over (future) time to com-\npute dynamic thresholds for the ﬁnal peak-picking.\nThis paper is structured as follows: We combine the\nODFs described in Section 2.2 with different preprocessing\nmethods from Section 2.1 and evaluate them on the dataset\ndescribed in Section 3.1 using the peak-picking method\ngiven in Section 2.3.4. In Section 4 we discuss the results,and we give conclusions in Section 5.\n2. COMPARED METHODS\nPreviously, onset detection algorithms used to work di-\nrectly with the time signal x(t). However, all current onset\ndetection algorithms use a frequency representation of the\nsignal. We used frames of 23 ms length (2048 samples at a\nsample rate of 44.1 kHz) that are ﬁltered with a Hann win-\ndow before transfer into the frequency domain by means of\nSTFT. The hopsize between two consecutive frames was\nset to 10 ms, which results in a frame rate of 100 frames\nper second. The resulting spectrogram X(n;k)(ndenot-\ning the frame and kthe frequency bin number) was then\nprocessed further by the individual preprocessing and on-\nset detection algorithms.\n2.1 Preprocessing\n2.1.1 Filtering\nScheirer [17] stated that, in onset detection, it is advanta-\ngeous if the system divides the frequency range into fewer\nsub-bands as done by the human auditory system. Filter-\ning has been applied by many authors (e.g. [6,14,17]), and\nneural network based approaches also use ﬁlter banks to\nreduce the dimensionality of the STFT spectrogram [11].\n2.1.2 Logarithmic magnitude\nUsing the logarithmic magnitude instead of the linear re-\npresentation was found to yield better results in many cases,\nindependently of the ODF used [11,14]. \u0015is a compression\nparameter and was adjusted for each method separately.\nAdding a constant value of 1results in only positive val-\nues:\nXlog(n;k) =log(\u0015\u0001X(n;k) + 1) (1)\n2.1.3 Adaptive whitening\nProposed in [18], adaptive whitening normalizes the mag-\nnitudesjX(n;k)jof each frequency bin separately by past\npeak values. The iterative algorithm (with rbeing a ﬂoor\nparameter and mthe memory coefﬁcient) is given as fol-\nlows:\nPn;k=(\nmax(jX (n;k)j;r;m\u0001Pn\u00001;k)ifn\u00151\nmax(jX (n;k)j;r) otherwise\njX(n;k)j \u0000jX(n;k)j\nPn;k(2)\n2.2 Onset detection functions\nWe have chose to omit other common methods such as\nphase deviation (PD) [3], high frequency content (HFC)\n[16] or rectiﬁed complex domain (RCD) [9], since they\nexhibited inferior performance in our tests.2.2.1 Spectral Flux\nThe spectral ﬂux (SF) [16] describes the temporal evolu-\ntion of the magnitude spectrogram by computing the dif-\nference between two consecutive short-time spectra. This\ndifference is determined separately for each frequency bin,\nand all positive differences are then summed to yield the\ndetection function.\nSF(n) =k=N\n2X\nk=1H(jX(n;k)j\u0000jX (n\u00001;k)j) (3)\nwithH(x) =x+jxj\n2being the half-wave rectiﬁer function.\nVariants of this method use the L2-norm instead of the\nL1-norm or the logarithmic magnitude [14] (cf. Section\n2.1.2).\n2.2.2 Weighted Phase Deviation\nAnother class of detection function utilizes the phase of the\nsignal [3, 9]. The change in the instantaneous frequency\n(the second order derivative of the phase '(n;k )) is an\nindicator of a possible onset. In [9], an improvement to\nthe phase deviation ODF called weighted phase deviation\n(WPD) was proposed. The WPD function weights each\nfrequency bin of the phase deviation function with its mag-\nnitude.\nWPD (n) =2\nNk=N\n2X\nk=1jX(n;k)\u0001'00(n;k)j (4)\n2.2.3 Complex Domain\nAnother way to incorporate both magnitude and phase in-\nformation (as in the WPD detection function) was pro-\nposed in [10]. First, the expected target amplitude and\nphaseXT(n;k)for the current frame are estimated based\non the values of the two previous frames assuming constant\namplitude and rate of phase change. The complex domain\n(CD) ODF is then deﬁned as:\nCD(n) =k=N\n2X\nk=1jX(n;k)\u0000XT(n;k)j (5)\n2.3 Peak detection\nIllustrated in Figure 2 and common to all onset detection\nmethods is the ﬁnal thresholding and peak-picking step to\ndetect the onsets in the ODF. Various methods have been\nproposed in the literature; we give an overview of the dif-\nferent components and modiﬁcations needed to make them\nsuitable for online processing.\nPreprocessing ThresholdingOnset\ndetection\nfunctionOnsets Peak-picking\nFigure 2. Peak detection process.2.3.1 Preprocessing\nThe preprocessing stage of the peak detection process con-\nsists mainly of two components: smoothing of the peaky\nODF and normalization. Both of them cannot be used in\nan online scenario. Instead, moving average techniques as\noutlined in Section 2.3.2 are applied to normalize the ODF\nlocally. To prevent detecting many false positives due to a\npeaky ODF, the effect of smoothing can be approximated\nby introducing a minimal distance from the last onset w5\nas proposed in Section 2.3.4.\n2.3.2 Thresholding\nBefore picking the ﬁnal onsets from the ODF, thresholding\nis performed to discard the non-onset peaks. Most methods\nuse dynamic thresholding to take into account the loudness\nvariations of a music piece. Mean [9], median [3,11,18] or\ncombinations [5, 12] are commonly used to ﬁlter the ODF.\nIf only information about the present or past is used, the\nthresholding function is suitable for online processing.\n2.3.3 Peak-picking\nTwo peak-picking methods are commonly used for ﬁnal\ndetection of onsets. One selects all local maxima in the\nthresholded detection function as the ﬁnal onset positions.\nSince detecting a local maximum requires both past and\nfuture information, this method is only applicable to ofﬂine\nprocessing.\nThe other method selects all values above the previously\ncalculated threshold as onsets and is also suitable for on-\nline processing. The downside of this approach is its rel-\natively high false positive rate because the threshold pa-\nrameter must be set to a very low level to detect the onsets\nreliably.\n2.3.4 Proposed peak detection\nWe use a modiﬁed version of the peak picking method pro-\nposed in [9] to also satisfy the constraints for online onset\npeak detection. A frame nis selected as an onset if the\ncorresponding ODF (n)fulﬁlls the following three condi-\ntions:\n1.ODF (n) =max(ODF (n\u0000w1:n+w2)\n2.ODF (n)\u0015mean(ODF (n\u0000w3:n+w4)) +\u000e\n3.n\u0000nlast onset>w 5\nwhere\u000eis a ﬁxed threshold and w1::w5are tunable\npeak-picking parameters. For online detection, we set w2=\nw4= 0. Our online experiments experiments showed that,\non average, onsets are detected one frame earlier than an-\nnotated in the dataset (using the values speciﬁed in Section\n3.3). As we want to ﬁnd the perceptual onset times (as an-\nnotated), we report the onset one frame later than detected.\nNote that this does not mean that we predict the onset, it\nonly means that the onset can be recognized in the signal\nbefore it is perceived.\nUnlike in previous studies [5, 12, 18] we do not use the\nsame thresholding parameters for all ODFs. This is mainly\nbecause some of the ODFs have fewer peaks and hence\nneed less averaging in the thresholding stage than others.2.4 Neural network based methods\nFor reference, we compare the presented methods with two\nstate-of-the-art algorithms, the OnsetDetector [11] and its\nonline variant OnsetDetector.LL [4]:\nOnsetDetector uses a bidirectional neural network which\nprocesses the signal both in a forward and backward man-\nner, making it an ofﬂine algorithm. The algorithms showed\nexceptional performance compared to other algorithms in-\ndependently of the type of onsets in the audio material,\nespecially in its latest version tested during the MIREX\ncontest in 2011 [1].\nOnsetDetector.LL incorporates a unidirectional neural\nnetwork to model the sequence of onsets based solely on\ncausal audio signal information.\nSince these methods show very sharp peaks (represent-\ning the propability of an onset) at the actual onset positions,\nthe before mentioned peak detection method is not applied,\nand a simple thresholding is used instead.\n2.5 New method\nWe propose a new onset detection method which is based\non the spectral ﬂux (cf. Section 2.2.1), drawing on various\nother author’s ideas.\nAs a ﬁrst step, we ﬁlter the linear magnitude spectro-\ngramjX(n;k)jwith a ﬁlter bank. We investigated dif-\nferent types of ﬁlter banks (Mel, Bark, Constant-Q) and\nfound that they all outperform the standard spectral ﬂux.\nSince they all perform approximately equally well when\nusing a similar number of ﬁlter bands, we chose a pseudo\nConstant-Q, where the frequencies are aligned according\nto the frequencies of the semitones of the western music\nscale over the frequency range from 27.5 Hz to 16 kHz, but\nusing a ﬁxed window length for the STFT. Overlapping\ntriangular ﬁlters sum all STFT bins belonging to one ﬁlter\nbin (similarly to Mel ﬁltering). The resulting ﬁlter bank\nF(k;b) hasB= 82 frequency bins with bdenoting the bin\nnumber of the ﬁlter and kthe bin number of the linear spec-\ntrogram. The ﬁlters have not been normalized, resulting in\nan emphasis of the higher frequencies, similar to the HFC\nmethod. The resulting ﬁltered spectrogram Xfilt(n;b) is\ngiven by:\nXfilt(n;b) =jX(n;k)j\u0001F(k;b) (6)\nApplying Equation 1 to the ﬁltered linear magnitude spec-\ntrogramXfilt(n;b) yields the logarithmic ﬁltered spectro-\ngramXlog\nfilt(n;b). The ﬁnal ODF Ois then given by:\nO(n) =k=N\n2X\nk=1H\u0010\f\f\fXlog\nfilt(n;b)\f\f\f\u0000\f\f\fXlog\nfilt(n\u00001;b)\f\f\f\u0011\n(7)\nwhereHis the half-wave rectiﬁer function deﬁned in Sec-\ntion 2.2.1.\n3. EXPERIMENTS\nTo evaluate the methods described, we conducted three ex-\nperiments: First, the methods were evaluated under on-\nline conditions: no future information was used to decidewhether there is an onset at the current time point. Second,\nthe same methods were evaluated under ofﬂine conditions\n(enabling prior data normalization or computing averages\nthat incorporate future information) to determine the max-\nimum performance achievable by each method. Third, we\nattenuated the volume of the audio data to an increasing\ndegree to test the online methods’ abilities to cope with\nsignals of different volume without access to normaliza-\ntion.\n3.1 Dataset\nTo evaluate the presented onset detection and peak-picking\nmethods we use a dataset of real world recordings.\nAn onset is usually deﬁned as the exact time a note or\ninstrument starts sounding after being played. However,\nthis timing is hard to determine, and thus it is impossible\nto annotate the real onset timing in complex audio record-\nings with multiple instruments, voices, and effects. Thus,\nthe most commonly used method for onset annotation is\nmarking the earliest time point at which a sound is audible\nby humans. This instant cannot be deﬁned in pure terms\n(e.g., minimum increase of volume or sound pressure), but\nis a rather complex mixture of various factors.\nThe annotation process is very time-consuming because\nit is performed in multiple passes. First, onsets are an-\nnotated manually during slowed down playback. In the\nsecond pass, visualization support is used to reﬁne the on-\nset positions. Spectrograms obtained with different STFT\nlengths are used in combination to capture the precise tim-\ning of an onset without missing any onset due to insuf-\nﬁcient frequency resolution. This multi-resolution proce-\ndure seems to be a good approach since the best onset de-\ntection algorithms also use this mechanism. If multiple\nonsets are located in close vicinity, they are annotated as\nmultiple onsets.\nThe dataset contains 321 audio excerpts taken from var-\nious sources. 87 tracks were taken from the dataset used in\n[11], 23 from [2], and 92 from [13]. All annotations were\nmanually checked and corrected to match the annotation\nstyle outlined above. The remaining 119 ﬁles were newly\nannotated and contain the vast majority of the 27,774 on-\nsets of the complete set.\nAlthough musically correct, the precise annotations (raw\nonsets) do not necessarily represent human perceptions of\nonsets. Thus, all onsets within 30 ms were combined into a\nsingle one located at the arithmetic mean of the positions1,\nwhich resulted in 25,966 combined onsets used for evalu-\nation. The dataset can be roughly divided into six main\ngroups (Table 3.1).\n3.2 Measures\nFor evaluation, the standard measures precision, recall, and\nF-measure were used. An onset is considered to be cor-\nrectly detected if there is a ground truth annotation within\n1To better predict the perceived position of an onset, psychoacoustical\nknowledge must be applied. Since the masking effects involved depend\non both loudness and frequency of an onset, they are not applied here.\nFor the evaluation of onset detection methods as in this paper, the selected\nmethod of combination is adequate.Type of audio Files Raw onsets Combined\nComplex mixtures 193 21,091 19,492\nPitched percussive 60 2,981 2,795\nNon-pitched perc. 17 1,390 1,376\nWind instruments 25 822 820\nBowed strings 23 1,180 1,177\nV ocal 3 310 306\nALL 321 27,774 25,966\nTable 1. Description of the used dataset: Pitched per-\ncussive (e.g., piano, guitar), non-pitched percussive (e.g.,\npercussion), wind instruments (e.g., sax, trumpet), bowed\nstring instruments (e.g., violin, kemence), monophonic vo-\ncal music and complex mixtures (e.g., jazz, pop, classical\nmusic)\n\u000625 ms around the predicted position. This rather strict\nevaluation method (also used in [11] and [6] for percus-\nsive sounds) was chosen because it gives more meaningful\nresults - especially in online onset detection - than an eval-\nuation window of\u000650 ms as used in [2, 9, 18].\nAn important factor in the evaluation is how false pos-\nitives and negatives are counted. Let us assume that two\nonsets are detected inside the detection window around a\nsingle annotation. If tolerant counting is used, no false pos-\nitives are counted. Every single detection is considered a\ntrue positive, since there is an annotated onset within the\ndetection window. This is often referred to as merged on-\nsets. If counted in a strict way, all annotated onsets can\nonly be matched once, i.e., two detections within the de-\ntection window of a single onset are counted as one true\npositive and one false positive detection.\nSince many papers do not explicitly describe the crite-\nria, it must be assumed that the results were obtained with\nthe ﬁrst method (usually yielding better results). In this\npaper, we evaluated the stricter way, but with combined\nannotated onsets (not to be confused with merged onsets).\nThe combining of onsets leads to less false negative de-\ntections if the algorithm reports only a single onset where\nmultiple ones are annotated. Since most of the algorithms\nare not capable reporting multiple consecutive onsets, this\nresults in a more fair comparison.\n3.3 Parameter selection\nThe peak-picking parameters w1:::w 5and the ﬁxed thresh-\nhold\u000eintroduced in Section 2.3.3 were optimized by a grid\nsearch over the whole set for each method separately. As\nin [2, 9], we report the best performance for each method\nusing the optimized global parameter set. For online de-\ntection (w 2=w4= 0), the optimal values for w3were\nfound to be between 4 and 12, w1= 3, andw5= 3. For\nthe ofﬂine case, w2= 3,w4= 1 andw5= 0 yielded the\nbest results (w 1andw3were left unchanged). The adaptive\nwhitening parameters m= 10 andr= 0:005 were found\nto be generally good settings and were used for all ODFs\nin the experiments. The compression parameter \u0015(Section\n2.1.2) was chosen to be between 0:01 and20. The neu-ral networks are trained and evaluated using 8-fold cross\nvalidation on disjoint training, validation, and test sets.\nAll parameters were optimized on the dataset and left\nunchanged for the unnormalized penalty task.\n4. RESULTS AND DISCUSSION\n4.1 Comparison of different ODFs\nTable 2 lists the results for all algorithms working in on-\nline mode on the complete dataset using the peak detection\nmethod described in Section 2.3.4. It shows that applica-\ntion of adaptive whitening and use of a logarithmic magni-\ntude both outperform the traditional methods without any\npreprocessing. Both preprocessing methods compress the\nmagnitude and hence emphasize higher frequency bands\nthat are important for detecting percussive onsets. Further-\nmore, our proposed method (SF log ﬁltered) clearly out-\nperforms all the other methods (apart from the reference\nOnsetDetector.LL ). In particular, it is characterized by a\nhigh precision value due to the reduced number of false\npositives compared to the other methods. We believe that\nthe ﬁltering process reduces the spectrum to the most rel-\nevant components for onset detection. This may facilitate\nbetter distinction between signal changes that are arising\nfrom an onset and spurious, non-onset-related changes.\nOnline algorithm %F-meas. %Prec. %Rec.\nSF 74.5 76.3 72.8\nSF aw 75.7 78.0 73.4\nSF log 76.1 78.3 74.0\nSF log ﬁltered 80.3 88.3 73.5\nCD 71.1 72.4 69.8\nCD aw 75.8 76.4 75.1\nCD log 74.1 77.4 71.1\nWPD 69.7 68.8 70.6\nWPD aw 71.4 70.8 72.0\nWPD log 70.9 74.6 67.5\nOnsetDetector.LL [4] 81.7 85.0 78.7\nTable 2. F-measure, precision and recall of different on-\nset detection algorithms using online peak-picking, where\nawdenotes adaptive whitening, logdenotes the use of a\nlogarithmic magnitude and SF log ﬁltered is the method\nproposed in Section 2.5\n.\nOur tests showed that - if the parameters are properly\nchosen - the ofﬂine results are in the same range as the\nonline results2. We deem this is a remarkable ﬁnding\nand think that the reasons for this behavior are the fol-\nlowing: First, the audio tracks of the dataset have similar\nvolume levels, which renders the normalization step less\nimportant. Second, when looking only at single indepen-\ndent frames, it seems reasonable that frames after the cur-\nrent onset frame do not carry much additional information.\nHowever, the superior results of the ofﬂine OnsetDetec-\ntor(F-measure 86:6, precision 90:6, recall 83:0 ) suggest\n2We observed an average gain in F-measure of 0:25% in ofﬂine modethat using both past and future information contained in\nthe magnitude spectrogram can be valuable to detect also\nthe “harder” onsets (as reﬂected by the much higher recall\nvalue of this method).\n4.2 Unnormalized penalty\nWhen dealing with unnormalized data, the investigated on-\nset detection methods experience different levels of perfor-\nmance loss. As shown in Figure 3, our proposed onset de-\ntection method exhibits superior performance at all attenu-\nation levels and is only beaten by the OnsetDetector:LL ,\nthat is unaffected by any volume changes. This shows the\npower of machine learning techniques that do not depend\non predeﬁned peak-picking threshholds. The methods us-\ning adaptive whitening score third, which seems reason-\nable as these methods include an implicit normalization us-\ning past frames. Computing the difference of two adjacent\nframes of the logarithmic spectrum (SF log) has the effect\nof dividing the magnitude at frame nby that at frame n\u00001,\nresulting in the relative magnitude change rather than the\nabsolute difference. This makes the spectral ﬂux obtained\nwith logarithmic magnitudes more robust against absolute\nvolume changes, compared to the standard variant (SF ).\nFinally, the compression parameter \u0015was found to in-\nﬂuence greatly the shape of the performance curve (when\nusing the logarithmic magnitude spectrum), especially at\nlower volume levels.\n−15 −10 −5 00102030405060708090\nAttenuation [dB]F−measure [%]SF\nSF aw\nSF log\nSF log filt.\nCD\nCD aw\nCD log\nWPD\nWPD aw\nWPD log\nOnsetDetector.LL\nFigure 3. Performance of the online methods at different\nattenuation levels.\n4.3 Remarks\nIn this paper, we give only results for the complete dataset.\nResults for subsets (organized by audio type and author)\nobtained with different detection window sizes can be found\nonline at http://www.cp.jku.at/people/boeck/\nISMIR2012.html.5. CONCLUSIONS\nIn this paper we have evaluated various onset detection al-\ngorithms in terms of their suitability for online use, focus-\ning on the preprocessing and peak detection algorithms.\nWe have shown that using logarithmic magnitudes or adap-\ntive whitening as a preprocessing step results in improved\nperformance in all methods investigated. When the param-\neters for peak detection are chosen carefully, online meth-\nods can achieve results in the same range as those of ofﬂine\nmethods.\nFurther, we have introduced a new algorithm which out-\nperforms other preprocessing methods. It copes better with\naudio signals of various volume levels, which is of major\nimportance for onset detection in real-time scenarios.\nApart from that, machine learning techniques like neu-\nral network based methods are much more robust against\nvolume changes in online scenarios and are the methods of\nchoice if enough training data is available.\n6. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Funds\n(FWF) under the projects P22856-N23, TRP-109 and Z159\n“Wittgenstein Award”. For this research, we have made\nextensive use of free software, in particular python and\nGNU/Linux. Further, we are grateful to the authors Bello\nand Holzapfel for making their onset dataset publicly avail-\nable.\n7. REFERENCES\n[1] MIREX 2011 onset detection results. http:\n//nema.lis.illinois.edu/nema_out/\nmirex2011/results/aod/.\n[2] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M. B. Sandler. A tutorial on onset de-\ntection in music signals. IEEE Transactions on Speech\nand Audio Processing, 13(5):1035–1047, 2005.\n[3] J. P. Bello, C. Duxbury, M. Davies, and M. B. Sandler.\nOn the use of phase and energy for musical onset detec-\ntion in the complex domain. IEEE Signal Processing\nLetters, 11(6):553–556, 2004.\n[4] S. B ¨ock, A. Arzt, F. Krebs, and M. Schedl. Online real-\ntime onset detection with recurrent neural networks. In\nProceedings of the 15th International Conference on\nDigital Audio Effects (DAFx), 2012.\n[5] P. Brossier, J. P. Bello, and M. D. Plumbley. Real-time\ntemporal segmentation of note objects in music signals.\nInIn Proceedings of the International Computer Music\nConference (ICMC), 2004.\n[6] N. Collins. A comparison of sound onset detection al-\ngorithms with emphasis on psychoacoustically moti-\nvated detection functions. In Proceedings of the 118th\nAES Convention, pages 28–31, 2005.[7] R. B. Dannenberg. An on-line algorithm for real-time\naccompaniment. In Proceedings of the 1984 Interna-\ntional Computer Music Conference, pages 193–198,\n1984.\n[8] N. Degara, M. Davies, A. Pena, and M. D. Plumbley.\nOnset event decoding exploiting the rhythmic structure\nof polyphonic music. IEEE Journal of Selected Topics\nin Signal Processing, 5(6):1228–1239, 2011.\n[9] S. Dixon. Onset detection revisited. In Proceedings of\nthe 9th International Conference on Digital Audio Ef-\nfects (DAFx), pages 133–137, 2006.\n[10] C. Duxbury, J. P. Bello, M. Davies, and M. B. Sandler.\nComplex domain onset detection for musical signals.\nInProceedings of the 6th International Conference on\nDigital Audio Effects (DAFx), 2003.\n[11] F. Eyben, S. B ¨ock, B. Schuller, and A. Graves. Uni-\nversal onset detection with bidirectional long short-\nterm memory neural networks. In Proceedings of the\n11th International Conference on Music Information\nRetrieval (ISMIR), pages 589–594, 2010.\n[12] J. Glover, V . Lazzarini, and J. Timoney. Real-time de-\ntection of musical onsets with linear prediction and si-\nnusoidal modeling. EURASIP Journal on Advances in\nSignal Processing, 2011(1):1–13, 2011.\n[13] A. Holzapfel, Y . Stylianou, A.C. Gedik, and\nB. Bozkurt. Three dimensions of pitched instrument\nonset detection. IEEE Transactions on Audio, Speech,\nand Language Processing, 18(6):1517–1527, 2010.\n[14] A. Klapuri. Sound onset detection by applying psy-\nchoacoustic knowledge. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), volume 6, pages 3089–3092,\n1999.\n[15] A. Lacoste and D. Eck. A supervised classiﬁcation al-\ngorithm for note onset detection. EURASIP Journal on\nApplied Signal Processing, pages 153–153, 2007.\n[16] P. Masri. Computer Modeling of Sound for Transfor-\nmation and Synthesis of Musical Signals. PhD thesis,\nUniversity of Bristol, UK, 1996.\n[17] E. D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. The Journal of the Acoustical Society\nof America, 103(1):588–601, 1998.\n[18] D. Stowell and M. D. Plumbley. Adaptive whitening\nfor improved real-time audio onset detection. In Pro-\nceedings of the International Computer Music Confer-\nence (ICMC), 2007."
    },
    {
        "title": "Finding Repeating Stanzas in Folk Songs.",
        "author": [
            "Ciril Bohak",
            "Matija Marolt"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417597",
        "url": "https://doi.org/10.5281/zenodo.1417597",
        "ee": "https://zenodo.org/records/1417597/files/BohakM12.pdf",
        "abstract": "Folk songs are typically composed of repeating parts - stan- zas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several meth- ods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method con- sists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate begin- nings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate begin- nings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated simi- larities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive.",
        "zenodo_id": 1417597,
        "dblp_key": "conf/ismir/BohakM12",
        "keywords": [
            "breathing pauses",
            "pitch changes",
            "tempo variations",
            "candidate stanzas",
            "stanza boundaries",
            "scored function",
            "peak picking method",
            "global thresholding",
            "Slovenian folk songs",
            "EthnoMuse archive"
        ],
        "content": "FINDING REPEATING STANZAS IN FOLK SONGS\nCiril Bohak\nUniversity of Ljubljana\nciril.bohak@fri.uni-lj.siMatija Marolt\nUniversity of Ljubljana\nmatija.marolt@fri.uni-lj.si\nABSTRACT\nFolk songs are typically composed of repeating parts - stan-\nzas. To ﬁnd such parts in audio recordings of folk songs,\nsegmentation methods can be used that split a recording\ninto separate parts according to different criteria. Most\naudio segmentation methods were developed for popular\nand classical music, however these do not perform well\non folk music recordings. This is mainly because folk\nsong recordings contain a number of speciﬁc issues that\nare not considered by these methods, such as inaccurate\nsinging of performers, variable tempo throughout the song\nand the presence of noise. In recent years several meth-\nods for segmentation of folk songs were developed. In this\npaper we present a novel method for segmentation of folk\nsongs into repeating stanzas that does not rely on additional\ninformation about an individual stanza. The method con-\nsists of several steps. In the ﬁrst step breathing (vocal)\npauses are detected, which represent the candidate begin-\nnings of individual stanzas. Next, a similarity measure is\ncalculated between the ﬁrst and all other candidate stanzas,\nwhich takes into account pitch changes between stanzas\nand tempo variations. To evaluate which candidate begin-\nnings represent the actual boundaries between stanzas, a\nscoring function is deﬁned based on the calculated simi-\nlarities between stanzas. A peak picking method is used in\ncombination with global thresholding for the ﬁnal selection\nof stanza boundaries. The presented method was tested\nand evaluated on a collection of Slovenian folk songs from\nEthnoMuse archive.\n1. INTRODUCTION\nFolk music is receiving increased attention of the music\ninformation retrieval (MIR) community, as our awareness\nof the need for preserving cultural heritage and making it\navailable to the general public grows. In order to process\nlarge quantities of folk song recordings gathered in ehtno-\nmusicological archives, automated methods for analysis of\nthese recordings need to be developed. Usually, such anal-\nysis starts with segmentation of recordings. Namely, folk\nsongs are typically found within ﬁeld recordings, which\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.are integral documents of the process of folk music gather-\ning and can, besides music, contain other kinds of content\nsuch as interviews with performers and background infor-\nmation. High-level segmentation of ﬁeld recordings from\nEthnoMuse archive [13] into individual units can be done\nmanually or by using automated methods [7].\nFor accurate analysis of individual songs, further seg-\nmentation into shorter parts is desirable. As folk songs\ntypically consist of repetitions of melodically similar stan-\nzas, segmentation boils down to ﬁnding the boundaries be-\ntween repeating stanzas. This is quite different to segmen-\ntation of popular music, where songs typically consist of\ndifferent parts, such as intro, verse, bridge and chorus.\nWhile the structure of a popular song is usually more\ncomplex than the structure of a typical folk song, the seg-\nmentation of folk songs contains other challenges. While\npopular music is recorded by professional musicians in stu-\ndios, folk songs are recorded in an everyday noisy envi-\nronment (talking in the background, wind and other en-\nvironmental noises, clapping . . . ) and singers are mostly\nuntrained and usually older people that may sing out of\ntune, forget parts of lyrics or melody, interrupt their per-\nformances, switch to speaking etc.\nIn this paper, a novel approach for segmentation of folk\nsongs into stanzas is presented. The algorithm is based on\nﬁnding the vocal pauses in a folk song recording, derive the\nlikely candidates for stanza beginnings from the pauses,\nscore these candidates and select the best matching ones to\nobtain the ﬁnal segmentation.\n2. RELATED WORK\nMost segmentation algorithms were developed for segmen-\ntation of popular or classical music. A broad overview\nof implemented methods is given in [11], where authors\npresent state-of-the-art approaches and results of segmen-\ntation and structure discovery in music recordings. Typi-\ncally approaches are based on calculating different sound\nfeatures, which are used for construction of similarity or\nself-similarity matrices. First use of such matrices in MIR\nis presented in [4]. By ﬁnding repeating parts in matri-\nces the structure of a musical piece can be inferred. Ap-\nproaches use different features for construction of self-si-\nmilarity matrices, two of the more popular are Mel-fre-\nquency cepstral coefﬁcients [2, 5, 12] and chroma vectors\n[1, 6].\nIn recent years, several approaches to segmentation of\nfolk music were presented. In [10] authors present a method\nfor robust segmentation and annotation of folk songs. TheFigure 1. Outline of the proposed segmentation method.\npresented approach uses chroma vectors in combination\nwith a newly deﬁned distance function for calculating the\ndistance between individual stanzas and provided MIDI\ntemplate. The method uses a MIDI representation of a\nsingle stanza to determine the length and expected pitch\nof each individual stanzas. Authors also present enhance-\nments of the presented method, in which shifted chroma\nvectors are used to improve similarity between parts with\nshifted pitch.\nA newer approach for detecting repetitive structures in\nmusic, presented in [9], introduces a novel ﬁtness measure\nfor deﬁning the most representative part of song with the\nuse of enhanced self-similarity matrix constructed from a\nvariation of chroma-based audio features.\nIn [14], authors do not perform segmentation to search\nfor repeating structure in folk songs, but are rather look-\ning for their meaningful parts. Meaningful parts of folk\nsongs are taken to be separated by breathing pauses, which\nare deﬁned as parts of audio recording without detectable\npitch.\nEthnoMuse archive is a collection of audio ﬁeld record-\nings, images, video recordings and metadata from Sloven-\ninan folklore. Archive contains more than 13.000 manu-\nscripts, 1000 dance recordings, photos and more than 300\nﬁeld recordings. Archive is not publicly accessible with\nexception of selected content. However parts of archive\nare published in book collections.\n3. METHODOLOGY\nOur segmentation method takes a folk song recording, as\nits input and outputs a set of boundaries, representing be-\nginnings of individual stanzas in the recording. The method\ntakes into account that performers are not professional sin-\ngers, which may lead to pitch drifting over the duration of\nthe piece, as well as considerable differences in tempo of\nindividual stanzas.\nThe method consists of several steps: preprocessing,\nsearch for vocal pauses, search for possible beginnings of\nstanzas and selection of actual stanza beginnings. The gen-\neral diagram of the suggested method is shown in Figure 1.\n3.1 Preprocessing\nThe input audio signal is mixed from stereo to a single\nchannel, the sample rate reduced to 11025 Hz and the am-\nplitude normalized.3.2 Detecting vocal pauses\nPerformers of folk songs are typically amateur singers which\nmake characteristic breathing pauses, reﬂected in audio re-\ncordings as silence. These pauses are longer between stan-\nzas, so they can be used to detect boundaries between stan-\nzas. We use three approaches for detection of vocal pauses:\nshort-term signal energy, amplitude envelope of the signal\nand the detected pitch. One can conﬁrm such assumptions\nby listening to audio data from presented collection. This\nholds for solo and choir singing music, but does not hold\nfor instrumental music.\n3.2.1 Detecting vocal pauses according to signal energy\nV ocal pauses in the audio signal are determined as parts\nof the signal where energy is below an experimentally de-\ntermined threshold. Energy of the signal is computed on\n200ms long frames and the threshold is set to \u00181=E\n120,\nwhereEis the average energy of the signal. Consequent\nframes with energy values below the speciﬁed threshold\nare merged into one vocal pause. V ocal pauses shorter than\n\u00182= 0:7 times the average detected vocal pause length are\nignored, to avoid the detection of short breathing pauses\nduring singing. Parameter \u00182was also determined experi-\nmentally. Endings of detected vocal pauses, displayed red\nin Figure 2(a) (green are beginnings of vocal pauses), are\nlater used as candidates for beginnings of stanzas.\n3.2.2 Detecting vocal pauses according to signal\nenvelope\nThe amplitude envelope of a signal is obtained by ﬁltering\nthe full-wave rectiﬁed signal using 4th order Butterworth\nﬁlter with a normalized cutoff frequency of 0:001. V o-\ncal pauses are parts of the signal where the envelope falls\nbelow the threshold \u00183=\u000060dB, which was determined\nexperimentally. Such parts of the signal are similarly as\nbefore merged into a single vocal pause, whereby we ad-\nditionally merge all non-consequent parts that are less than\n\u00184= 0:5s apart, where the value \u00184was deﬁned experi-\nmentally as well. As in the previous case, endings of de-\ntected vocal pauses are used as candidates for beginnings\nof stanzas and are displayed red in Figure 2(b) (green are\nthe beginnings of vocal pauses).\n3.2.3 Detecting vocal pauses according to relative\ndifference of pitch\nFor detecting parts of the signal without any detectable\nfundamental frequency we are using the approach presented\nin [14]. The input signal is ﬁrst resampled to fs= 1024Hz.\nThe resampled signal is then used as input for the YIN\nalgorithm [3] that calculates fundamental frequencies for\neach frame of the signal. Fundamental frequencies are\nsmoothed with a low-pass ﬁlter. Parts of the signal that\ndiffer more than 20 semitones from the average signal fre-\nquency are selected as vocal pauses.\nIn our approach we are merging vocal pauses longer\nthan an experimentally obtained value \u00185= 4ms, while\nshorter vocal pauses are ignored. We are also taking into\naccount the minimal length of a vocal pause which is inour case\u00186= 250ms. Again, endings of vocal pauses are\nused as candidates for stanza beginnings. In Figure 2(c) the\ndetected vocal pauses are shown (green are the beginnings\nand red are endings) for a sample recording.\n(a) Detected parts with low energy.\n(b) Detected parts with low amplitude envelope.\n(c) Parts with no detectable fundamental frequency.\n(d) All detected stanza boundary candidates.\nFigure 2. Comparison of methods for vocal pause detec-\ntion. In images (a), (b) and (c) green are beginnings of\nvocal pauses and red are vocal pauses endings. In image\n(d) green are candidates for stanza boundaries and red is\nthe value of ﬁtness function for the candidates.\n3.3 Finding candidates for stanza boundaries\nIn search for candidates for stanza boundaries we merge\nall sets of vocal pauses obtained with the previously de-\nscribed methods. An example of such a merged set is dis-\nplayed in Figure 2(d) where the beginnings of vocal pauses\nare omitted and only their endings, which we consider as\ncandidates for stanza boundaries, are shown in green. If\ncandidates are present in several sets before merging, they\nare merged into a single candidate boundary.\nWe assume that the ﬁrst candidate from the set repre-\nsents the actual beginning of the ﬁrst stanza. We then\ncalculate the distance of the ﬁrst \u00187= 10 s of this ﬁrst\nstanza to the 10s beginnings of all other stanza candidates\ndetermined by the candidates for stanza boundaries. The\nvalue of\u00187was chosen to represent approximately half of\nthe average stanza length in our dataset. The calculation\nof distances between different stanza candidates takes the\npitch drifting and tempo variations into consideration and\nis composed of four steps and is illustrated in Figure 3.\n3.3.1 Step 1\nWe calculate 12 dimensional chromagrams, as deﬁned in\n[8], for all stanza candidates using a window size of 50ms.\nFigure 3. Outline of the algoritm for evaluating candidate\nstanza beginnings.\n3.3.2 Step 2\nWe deﬁne a distance function between each pair of 12 di-\nmensional chroma vectors as the root mean square (RMS)\ndistance, which was also used for chorus detection in [6]:\nc(a;b) =p\n(P\ni(ai\u0000bi)2\np\n12; (1)\nwherecis the distance function between two chroma vec-\ntorsaandb,aiandbiarei-th elements of chroma vectors.\n3.3.3 Step 3\nThe deﬁned distance function is used by the Dynamic Time\nWarping (DTW) algorithm for calculation of the total dis-\ntance between the selected stanzas as:\ncp(p1;p2) =LX\nl=1c(p1(l);p2(l)) (2)\nwherep1andp2are candidate stanza beginnings. p1(l)and\np2(l)are the corresponding chroma vectors (previously la-\nbeled as aandb), the index ltakes values from the ﬁrst\n(1) to the last (L) chroma vector in the selected audio part.\nThe DTW is used for calculating the total distance between\ntwo stanza candidates:\ncmin(dj) =DTW (d0;dj) =minfc p(d0;dj)g; (3)\nwherecminis the minimal cost between parts d0anddj.\nA similar approach that uses DTW for calculating the cost\nwas used in [10].\n3.3.4 Step 4\nTo account for pitch drifting during singing, we also calcu-\nlate distances between stanza candidates with shifted chr-\noma vectors. The chroma vectors are circularly shifted up\nto two semitones up and down to compensate for the out-\nof-tune singing. We then select the lowest DTW distance\nas:\ndistmin(d0) = 0;\ndistmin(dj) =min\ndf\nj;f2[\u00002;2]fcmin(d0;df\nj)g; (4)Figure 4. Fitness function for evaluating the candidate\nstanza beginnings.\nwheredf\njrepresents a rotation of chroma vectors for the\nselected stanza candidate from two semitones downwards\nto two semitones upwards in steps of one semitone.\nFinally, we deﬁne a ﬁtness function for scoring the can-\ndidate stanza beginnings kias:\np(i) =(\n0; dj=2D\n1\u0000(distmin(dj)\nmax jdistmin(dj))2; dj2D: (5)\nFigure 4 shows such a ﬁtness function (Eq. 5) plotted on\ntop of the audio signal. As the function is inversely pro-\nportional to the distance between the ﬁrst stanza and a\nstanza candidate, higher ﬁtness function values correspond\nto stanza boundaries which are more likely - stanzas are\nmore similar and the candidate thus more likely represents\na repetition of the original ﬁrst stanza.\n3.4 Selection of actual stanza beginnings\nThe selection of actual stanza beginnings is made with a\nsimple peak picking algorithm in combination with a global\nthreshold. In the deﬁned ﬁtness function, peaks represent\nthe most likely stanza beginnings, so all peaks above a\nglobal threshold, corresponding to the average value of the\nﬁtness function, are picked as the actual boundaries be-\ntween stanzas.\n4. EXPERIMENTS AND RESULTS\nThe proposed method was tested on a set of folk songs\nfrom an ethnomusicological archive labeled as solo or choir\nsinging, totalling 190 minutes in length and containing 135\nunits of solo or choir singing with an average duration of\n100 seconds per unit. The average number of stanzas per\nunit was approximately 4, the average length of a stanza 18\nseconds.\n4.1 Evaluation of developed method\nWe performed an evaluation of vocal pause detection algo-\nrithms, as well as an evaluation of the whole segmentation\nmethod using the different detection algorithms.\nThe values of algorithm parameters \u00181. . .\u00187, used in\nvocal pause detection algorithms, were determined on a\nsmall set of recordings by evaluating a range of parameter\nvalues and choosing the ones for which the segmentation\nalgorithm performed best. The algorithm itself is not very\nsensitive to changes in these parameters.4.1.1 Evaluation of vocal pause detection\nWe evaluated individual approaches for detecting vocal pau-\nses on the dataset containing 545 annotated vocal pauses.\nA detected vocal pause was considered as correctly de-\ntected, if it was within 2 seconds of the annotated vocal\npause. Table 1 shows precision, recall and F-Measures of\ndetection for individual methods and their combination. As\nshown in the table, combining all of the methods yields\nhigh recall and low precision. This is what we are aim-\ning for at this ﬁrst stage of the segmentation algorithm,\nbecause the second stage of the algorithm removes irrele-\nvant vocal pauses and thus ﬁnding as many vocal pauses as\npossible is a priority.\nTable 1. Comparison of vocal pause detection algorithms.\nAlgorithm Precision Recall F-Measure\nEnergy 0,3336 0,8276 0,4755\nAmplitude 0,5066 0,3729 0,4296\nNoPitch 0,2793 0,5908 0,3793\nCombination 0,0894 0,9866 0,1639\n4.1.2 Evaluation of the method as a whole\nTable 2 shows accuracy of the whole segmentation algo-\nrithm by using the four approaches to vocal pause detec-\ntion described previously. One can see that the method,\nwhich uses the combination of all vocal pause detection al-\ngorithms, signiﬁcantly outperforms the others. This result\nis expected, since individual vocal pause detection algo-\nrithms have lower recall, which means that we are already\nmissing a number of annotated segment boundaries.\nTable 2. Segmentation accuracy with different vocal pause\ndetection algorithms.\nMethod Precision Recall F-Measure\nEnergy 0,7592 0,4430 0,5595\nAmplitude 0,6886 0,2574 0,3747\nNoPitch 0,7447 0,3597 0,3597\nCombination 0,6773 0,6435 0,6600\nOur method performs well on songs that have strong vo-\ncal pauses, songs that consist of melodically similar stan-\nzas and songs where the singing is approximately in tune.\nOne can see an example of such vocal pause detection in\nFigure 5(a) where our method ﬁnds 9 out of 10 annotated\nvocal pauses. The 10th vocal pause is also clearly seen in\nthe plotted ﬁtness function, however the global threshold-\ning prevents its detection.\nThe method fails on songs where the ﬁrst stanza is in-\ncorrectly detected, because all stanzas are always com-\npared to the ﬁrst stanza. It also has difﬁculties in cases\nwhere stanzas are melodically very different, because com-\nparison of chroma vectors relies on melodically similar\nstanzas. An example of such failure is shown in Figure 5(b).In this example song consists of melodically signiﬁcantly\ndistinguished parts. First part of the song consists of three\nmelodically similar parts that are also detected by our met-\nhod, while same is true for the second part, its repeating\nparts are not similar to ﬁrst part of the song.\n(a) An example where our method performs well.\n(b) An example where our method fails to ﬁnd any annotated vocal pauses.\nFigure 5. The ﬁgure shows an example where our method\nperforms well (a) and an example where our method fails\n(b). The sound signal is plotted in blue, the ﬁtness function\nis plotted in orange, true stanza beginnings are plotted with\ngreen stars (*) and the detected beginnings are plotted with\npink plus signs (+).\n4.2 Comparison with existing methods\nWe compared the proposed method with two existing folk\nsong segmentation methods, results are shown in Table 3.\nThe ﬁrst method [14] only relies on detection of vocal pauses\nfor segmentation and does not consider repetitions. Thus,\nthe method covers most of the annotated vocal pauses (high\nrecall), however it also detects many false positives, result-\ning in low precision.\nWe also compared our method with the method pre-\nsented in [10] that uses symbolic transcription of a typical\nstanza as its input. Due to this additional prior knowledge,\nthe method outperforms ours by a signiﬁcant margin, as\nthe approximate stanza melody and length are known to\nthe algorithm. But, as this prior knowledge is not always\navailable, the method cannot be used in all cases.\n5. CONCLUSION AND FUTURE WORK\nIn this paper we presented a novel method for ﬁnding re-\npeating stanzas in recordings of folk songs. The method re-\nlies on the detection of vocal pauses, which represent can-\ndidate stanza beginnings, which are then evaluated accord-Table 3. Comparison of our method to other approaches.\nMethod Precision Recall F-Measure\nKranenburg 0,149 0,930 0,257\nMueller (\u0001fluc) 0,865 0,748 0,802\nOur method 0,442 0,646 0,525\ning to melodic similarity with the ﬁrst stanza. The vocal\npause detection algorithms and the method as a whole are\nseparately evaluated on a dataset of folk song recordings.\nThe method performs well, however several extensions are\nplanned.\nOur future work will include improvements with detec-\ntion of the ﬁrst stanza and processing of the ﬁtness func-\ntion, as well as adaptation of the method to instrumental\ntunes, where vocal pauses are not present. We also plan\nto integrate the developed method with our algorithm for\nhigh-level ﬁeld recording segmentation, where it could be\nused to improve the accuracy of high-level segmentation.\nWe will also try using more advanced preprocessing of the\naudio signal for environmental noise reduction. Other pos-\nsibility is to try extracting pitch from the raw audio data\nand try ﬁnding repeating stanzas in symbolic domain.\n6. ACKNOWLEDGEMENTS\nAuthors would like to thank all anonymous reviewers for\nthorough reviews with many suggestions on possible im-\nprovements and recommendations on how to improve this\npaper. This work could not been done without data pro-\nvided by Institute of Ethnomusicology ar Research Cen-\ntre of Slovenian Academy of Sciences and Arts. Research\nwork was done as part of basic research and application\nproject ETNOKATALOG: retrieval of semantic data from\nfolk song and music, based on melodic and metro-rhythmic\nanalysis funded by Slovenian research agency.\n7. REFERENCES\n[1] Mark A. Bartsch and Gregory H. Wakeﬁeld. To catch a\nchorus: Using chroma-based representations for audio\nthumbnailing. In Applications of Signal Processing to\nAudio and Acoustics, pages 15–19, New Platz, NY ,\nUSA, October 2001.\n[2] Matthew L. Cooper and Jonathan Foote. Automatic\nmusic summarization via similarity analysis. In Pro-\nceedings of the 3rd International Conference on Mu-\nsic Information Retrieval (ISMIR 2002), pages 81–85,\nParis, France, October 2002.\n[3] Alain de Cheveign ´e and Hideki Kawahara. YIN, a\nfundamental frequency estimator for speech and mu-\nsic.The Journal of the Acoustical Society of America,\n111(4):1917–1930, 2002.\n[4] Jonathan T. Foote. Visualizing music and audio using\nself-similarity. In Proceedings of the 7th ACM interna-\ntional conference on Multimedia, pages 77–80, 1999.[5] Jonathan T. Foote and Matthew L. Cooper. Media seg-\nmentation using self-similarity decomposition. In Pro-\nceedings of SPIE Storage and Retrieval for Multimedia\nDatabases, pages 167–175, 2003.\n[6] Masataka Goto. A chorus section detection method for\nmusical audio signals and its application to a music lis-\ntening station. IEEE Transactions on Audio, Speech &\nLanguage Processing, 14(5):1783–94, 2006.\n[7] Matija Marolt. Probabilistic segmentation and labeling\nof ethnomusicological ﬁeld recordings. In Proceedings\nof the 10th International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2009), pages 75–80,\nKobe, Japan, October 2009.\n[8] Meinard M ¨uller. Information Retrieval for Music and\nMotion. Springer Verlag, 2007.\n[9] Meinard M ¨uller, Peter Grosche, and Nanzhu Jiang. A\nsegment-based ﬁtness measure for capturing repetitive\nstructures of music recordings. In Proceedings of the\n12th International Conference on Music Information\nRetrieval (ISMIR), 2011.\n[10] Meinard M ¨uller, Peter Grosche, and Frans Wiering.\nRobust segmentation and annotation of folk song\nrecordings. In Proceedings of the 10th International\nSociety for Music Information Retrieval Conference\n(ISMIR 2009), pages 735–740, Kobe, Japan, October\n2009.\n[11] Jouni Paulus, Meinard M ¨uller, and Anssi Klapuri.\nAudio-based music structure analysis. In Proceedings\nof the 11th International Conference on Music In-\nformation Retrieval (ISMIR 2010), pages 625–636,\nUtrecht, Netherlands, August 2010.\n[12] Geoffroy Peeters. Toward automatic music audio sum-\nmary generation from signal analysis. In Proceedings\nof the 3rd International Conference on Music Infor-\nmation Retrieval (ISMIR 2002), pages 94–100, Paris,\nFrance, October 2002.\n[13] Gregor Strle and Matija Marolt. The ethnomuse dig-\nital library: conceptual representation and annota-\ntion of ethnomusicological materials. International\nJournal on Digital Libraries, pages 1–15, 2011.\n10.1007/s00799-012-0086-z.\n[14] Peter van Kranenburg and George Tzanetakis. A com-\nputational approach to the modeling and employment\nof cognitive units of folk song melodies using audio\nrecordings. In Proceedings of the 11th International\nConference on Music Perception and Cognition, pages\n794–797, 2010."
    },
    {
        "title": "A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals.",
        "author": [
            "Juan J. Bosch",
            "Jordi Janer",
            "Ferdinand Fuhrmann",
            "Perfecto Herrera"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416076",
        "url": "https://doi.org/10.5281/zenodo.1416076",
        "ee": "https://zenodo.org/records/1416076/files/BoschJFH12.pdf",
        "abstract": "The authors address the identification of predominant music instruments in polytimbral audio by previously di- viding the original signal into several streams. Several strategies are evaluated, ranging from low to high com- plexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typ- ically pose problems to state-of-art source separation al- gorithms. The recognition results are improved a 19% with a simple sound segregation pre-step using only pan- ning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The re- sults showed that the performance was only enhanced if the recognition models are trained with the features ex- tracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original in- strument recognition algorithm is improved in up to 32%.",
        "zenodo_id": 1416076,
        "dblp_key": "conf/ismir/BoschJFH12",
        "keywords": [
            "polytimbral",
            "audio",
            "music",
            "instruments",
            "identification",
            "streaming",
            "segregation",
            "algorithms",
            "dataset",
            "recording"
        ],
        "content": "A COMPARISON OF SOUND SEGREGATION TECHNIQUES \nFOR PREDOMINANT \nINSTRUMENT RECOGNITION  \nIN MUSICAL AUDIO SIGNALS \nJuan J. Bosch, Jordi Janer, Ferdinand Fuhrmann and Perfecto Herrera \nUniversitat Pompeu Fabra, Music Technology Group, Roc Boronat 138, Barcelona \njuanjo.bosch@gmail.com, jordi.janer@upf.edu,  \nferdinand.fu\nhrmann@gmail.com, perfecto.herrera@upf.edu  \n \nABSTRACT \nThe authors  \naddress the identification of predominant \nmusic instruments in polytimbral audio by previously di-\nviding the original signal into several streams. Several \nstrategies are evaluated, ranging from low to high com-\nplexity with respect to the segregation algorithm and \nmodels used for classification. The dataset of interest is \nbuilt from professionally produced recordings, which typ-\nically pose problems to state-of-art source separation al-\ngorithms. The recognition results are improved a 19% \nwith a simple sound segregation pre-step using only pan-\nning information, in comparison to the original algorithm. \nIn order to further improve the results, we evaluated the \nuse of a complex source separation as a pre-step. The re-\nsults showed that the performance was only enhanced if \nthe recognition models are trained with the features ex-\ntracted from the separated audio streams. In this way, the \ntypical errors of state-of-art separation algorithms are \nacknowledged, and the performance of the original in-\nstrument recognition algorithm is improved in up to 32%. \n1. INTRODUCTION \nThe amount of music available has dramatically increased \nin recent years. There is thus a clear need of effectively \norganizing and retrieving this content. Music Information \nRetrieval (MIR) is a research field dealing with the ex-\ntraction of music content information, and can be used for \nsuch purposes. Instrumentation is a very useful descrip-\ntion of musical data, since it can be exploited successfully \nin different forms; songs can be retrieved using the infor-\nmation about the presence of an instrument, and the iden-\ntification of the musical genre is easier when knowledge \nabout the instrumentation is available (e.g. a banjo makes \nthe piece more likely to be country than classical music). \nAdditionally, instrumentation is a key aspect for the per-\nceived similarity in music [1]. \nAudio source separation deals with the recuperation of \nthe original signals from the acoustical sources constitut-ing an audio mixture by computational means. Even \nthough there is still much room for improvement when \napplied to real world music, state-of-art separation algo-\nrithms can be used to, at least, increasing the presence of \na source or a group of sources in a mixture, such as har-\nmonic-percussive separation [10]. They can potentially be \na useful pre-step to improve the results of MIR tasks, such \nas chord detection, melody extraction, etc. \nThe automatic recognition of instruments is usually \nbased on timbre models or features such as MFCCs or \nMPEG-7 combined with statistical classifiers. An exten-\nsive review of approaches for isolated musical instrument \nclassification can be found in [8], with several classifica-\ntion techniques, a number of instrumental categories be-\nlow ten, and accuracies that reach up to 90%. \nMore recent works deal with instrument recognition in \npolytimbral musical signals, which is a more realistic and \ndemanding problem. For instance, Tzanetakis focused on \nthe detection of voice [12], while Essid [4] presented an \napproach using a taxonomy-based hierarchical classifica-\ntion, in which the classifiers were trained on combinations \nof instruments such as: piano, tenor sax, double bass and \ndrums. Kitahara et al. [9] proposed several techniques to \nimprove instrument recognition in duo and trio music by \ndealing with three issues: the feature variations caused by \nsound mixtures, the pitch dependency of timbres, and the \nuse of musical context. With the proposed techniques, \nthey achieved an 85.8% average recognition rate in trio \nmusic. Fuhrmann [5] proposed a method for automatic \nrecognition of predominant instruments with Support \nVector Machine (SVM) classifiers trained with features \nextracted from real musical audio signals. One of the \nproblems identified in this system is that it often missed \nsome of the labels in excerpts containing more than one \npredominant instrument. \nThe recognition of the instruments present in a mixture \nbecomes more complex as the number of instruments in-\ncreases. Reducing the number of instruments in the audio \nto be analyzed should thus help in the recognition of in-\nstruments, and the idea of using source separation as a \npre-step has already been investigated in previous re-\nsearch. Heittola et al. [6] use Non-Negative Matrix Fac-\ntorization (NMF) with a source filter model, based on  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nprevious wor\nk by Virtanen and Klapuri [13]. Klapuri’s \nmultipitch estimation is used in the separation, with aid of \nan optional streaming algorithm which organizes individ-\nual notes into sound sources. The Viterbi algorithm is \nthen employed to find the most likely sequence of notes. \nThe classifiers use MFCC’s (with a 40 channel filter \nbank) along with their first time derivatives. A Gaussian \nMixture Model (GMM) is used to model the instrument \nconditional densities of the features, and the parameters \nare estimated using the Expectation Maximization (EM) \nalgorithm from the training material. A Maximum Likeli-\nhood classifier is then used for classification. The dataset \nwas artificially created from the RWC dataset, with a \nmaximum of six note polyphony, and 19 different pitched \ninstruments, reaching a 59.1% F1-measure. Burred [3] \nalso presents an instrument classification approach with a \nstereo blind source separation pre-step, using Gaussian \nlikelihood as a timbre similarity measure. The reported \naccuracy reaches 86.7%, with polyphony of 2 instru-\nments, and 5 classes. Results are significantly better than \nin monaural separation, with 79.8% accuracy. \nIn this paper, we address the combination of source \nseparation and instrument recognition, in order to im-\nprove the identification of predominant instruments in \nprofessionally produced western music recordings. As \nopposed to audio data created by artificially mixing sev-\neral instrument with no musical relation between them, \nthis real world scenario adds more complexity to source \nseparation algorithms, due to several reasons. First, in real \nworld western music, instruments are harmonically relat-\ned, and thus their spectral components usually share some \nof the frequencies. Furthermore, effects such as reverbs, \ndelays, etc. make the separation much more difficult. On \nthe other hand, such scenario allows the algorithms to \ntake advantage of the spatial information present in stere-\nophonic recordings. \n2. METHOD \nThis section introduces the methodology proposed to in-\nvestigate if the performance of an instrument recognition \nalgorithm can be improved with a previous audio segrega-\ntion step, as introduced in subsection 2.1. The dataset is \ndescribed in subsection 2.2, and the evaluation methodol-\nogy is introduced in subsection 2.3. \n2.1 Audio segregation for instrument recognition \nThe algorithm used by Fuhrmann in [5] is considered as \nthe baseline instrument recognition. It is conceived to \noutput a set of labels corresponding to the predominant \ninstruments in an excerpt of polytimbral music. Ten \npitched instruments are used in this study: cello, clarinet, \nflute, acoustic guitar, electric guitar, organ, piano, saxo-\nphone, trumpet, violin, and additionally human singing \nvoice. The original system uses SVM, which outputs probabilistic estimates for each of the modeled categories. \nAs previously introduced, the main problem is that it \nsometimes misses some labels in excerpts with multiple \ninstruments. \nThe hypothesis is that in order to enhance its perfor-\nmance, a previous step could be performed, separating \ninput audio data into several streams. These streams are \nthen separately processed by the instrument recognition \nalgorithm, resulting in several sets of labels. The sets of \nlabels are then combined and given as output labels. Sev-\neral segregation methods are considered, as well as dif-\nferent strategies for the label combination, and also sever-\nal models used for instrument recognition. Figure 1 illus-\ntrates the combination of a segregation process followed \nby the instrument recognition in each of the streams. \n \nFigure 1. Generic flow diagram for the application of \naudio segregation as a previous step to the instrument \nrecognition \nWe consider two different segregation methods. The \nfirst is FASST (A Flexible Audio Source Separation \nFramework), presented by Ozerov et al. [10]. It is based \non structured source models, which allow the introduction \nof constrains according to the available prior knowledge \nabout the separation problem. It aims at generalizing sev-\neral existing source separation methods, and allows creat-\ning new ones.  The second segregation method is a simple \nLeft/Right-Mid/Side (LRMS) separation based on pan-\nning information, where M = L+R and S = L-R.  \nIn this research, the FASST algorithm is used in a con-\nfiguration which separates the polytimbral audio input \ninto four streams: “drums”, “bass”, “melody”, and “other” \n(dbmo).  This is a default configuration provided with the \nFASST framework, and it fits our interest in the recogni-\ntion of the predominant pitched instruments, as the classi-\nfier neither considers bass nor drums. After the separa-\ntion, the “melody” stream would ideally contain the main \ninstrument to be recognized, and the “other” stream \nwould contain the rest of the instruments, with no pres-\nence of bass and percussive instruments.  Recognition of \nthe predominant instruments in these streams of audio \nshould be easier than in the case of the original \npolytimbral mixture. However, there are limitations in \nmost separation algorithms, especially when applied to \nreal world music. They commonly create artifacts and er-\nrors in the separation, producing some leakage of instru-\nments in streams where they should not be present. This   \n \ncould affect\n the recognition of instruments due to the \nchanges the artifacts produce in timbre. In order to deal \nwith these errors, we investigate if a classifier could learn \nhow a source separation algorithm behaves, and \nacknowledge the errors by training models on the separat-\ned audio estimations. In simple words, the models would \nlearn, with the features of the estimated “drums”, “bass”, \n“melody”, “other” stream, when the predominant instru-\nment of the audio is a cello, clarinet, flute, acoustic guitar, \nelectric guitar, organ, piano, saxophone, trumpet, violin, \nor voice. We consider the use of different models for each \nof the separated streams, in order to allow the usage of a \ndifferent set of (automatically selected) audio features, as \nwell as different parameters for training the classifiers.  \nFinally, the strategy for the combination of the labels \ngiven as output by the individual instrument recognition \nmodels is also important. Two strategies are explored in \nour experiments: 1) selecting some of the classifiers’ out-\nput only (e.g. only the sets of labels from the “melody” \nand “other” streams), and 2) requiring a degree of agree-\nment (overlap) between all sets of labels. In the second \nstrategy, output labels correspond to the ones present in \nmore than N sets of labels predicted by the models. \n2.2 Data \nTwo different datasets have been created for training and \ntesting, based on the database originally compiled by \nFuhrmann [6]. Firstly, the training dataset contains 6700 \nannotated excerpts of 3 seconds in which only one in-\nstrument is predominant. These data are unevenly distrib-\nuted among the modeled categories, ranging from mini-\nmum 388 to a maximum of 778. A second training dataset \nis derived by separating the original one into dbmo \nstreams with FASST. Secondly, the testing set consists of \naround 3000 excerpts annotated with one to five instru-\nments. This set was created by dividing the original music \npieces of the original database [6] into segments with the \nfollowing properties: 1) the predominant instruments are \nthe same in the whole excerpt, 2) the length is between 5 \nand 20 seconds, and 3) the excerpts are stereo. The first \nproperty allows us to disregard segmentation according to \nthe instrumentation into the recognition evaluation, since \nthe predominance of instruments typically changes \namongst or even within sections of a music piece. The \nsecond property ensures that the instrument labeling pro-\ncess has enough information to output the labels with a \ncertain confidence. The third property corresponds to the \nuse case of interest: professionally produced music re-\ncordings, in stereo format. \n2.3 Evaluation methodology \nThe evaluation method is based on comparing the output \nlabels against the manually annotated ground-truth labels. \nFollowing the traditional information retrieval evaluation \nmeasures, we calculate: true positives (tp), true negatives (tn), false positives (fp) and false negatives (fn) for each \nof the instruments (labels). We consider L the closed set \nof labels {}iL l=, with 1...i N=, N the number of in-\nstruments, and the dataset { }iX x=, with 1...i M=, and \nM the number of excerpts. We define ˆˆ{ }iY y=, with \n1...i M=  as the set of ground-truth labels, and \n{ }iY y=, with 1...i M=, and iyL⊆, the set of pre-\ndicted labels assigned to each instance i. Precision and \nrecall are defined for each of the labels l in Las: \n , ,\n1\n,\n1ˆM\nli li\nli\nl M\nl l\nli\niy ytpP\ntp fpy=\n== =\n+∑\n∑ (1) \n , ,\n1\n,\n1ˆ\nˆM\nli li\nli\nl M\nl l\nli\niy ytpR\ntp fny=\n== =\n+∑\n∑ (2) \nwhere ,liy and ,ˆliy are boolean variables referring to in-\nstance i, which indicate the presence of the label l in the \nset of predicted labels, or in the set of ground-truth labels \nrespectively. Additionally, we define the F1 as the har-\nmonic mean between precision and recall: \n 2l l\nl\nl lPRF\nP R=\n+ (3) \nWe also d\nefine macro and micro averages of the previous \nmetrics, in order to obtain more general performance met-\nrics, which consider all labels. The macro is here under-\nstood as an unweighted average of the precision or recall \ntaken separately for each label (average over labels).  \n \n11L\nmacro l\nlP\nLP\n==∑, \n11L\nmacro l\nlLRR\n==∑ (4) \nOn the other hand, the micro average is an average over \ninstances, and thus, giving more weight to the labels with \na higher number of instances: \n \n( ), ,\n1 1 1\n,\n1 1 1ˆL L M\nl li li\nl l i\nmicroL M\nl l liL\nl l itpy y\nP\ntp fp y= = =\n= = ===\n+∑ ∑∑\n∑ ∑∑ (5) \n \n( ), ,\n1 1 1\n,\n1 1 1ˆ\nˆL L M\nl li li\nl l i\nmicroL M\nl l liL\nl l itpy y\nR\ntp fn y= = =\n= = ===\n+∑ ∑∑\n∑ ∑∑ (6) \nThe mac\nro\n and micro F1 are defined as the harmonic \nmean of respectively, the macro and micro averages. \n 2 2,macro macro micro micro\nmac\nro micro\nmacro macro micro microP R P RF F\nP R P R=  =\n+ + (7) \nThe follo\nwing section details the experiments performed \naccording to the presented methodology.   \n \n3. EXPERIMENTS\n \nWe conducted five experiments to investigate the benefits \nof the segregation of the audio signal into different \nstreams prior to the application of an instrument recogni-\ntion algorithm. In the first four experiments, the SVM \nmodels used for the instrument recognition were trained \nwith parameters that optimized the performance in Exper-\niment 1: a polynomial kernel of degree 4 and a cost pa-\nrameter = 0.1. In each experiment, we consider all combi-\nnations of sets of labels to find the best recognition per-\nformance. These are notated with the initials of the \nstreams considered, e.g.: “Exp3:dbo” refers to the combi-\nnation of the labels outputted from the recognition of the \nd (drums) + b (bass) + o (other) streams in Experiment 3. \nThe combination strategy was initially the union of the \nlabels predicted by each of the models. Then, in Experi-\nment 5 we explored a partial overlap strategy, and we op-\ntimized recognition performance by tuning the parameters \nfor each of the models. \n3.1 Experiment 1: original algorithm \nThe original algorithm is employed without a previous \nseparation step, as shown in Figure 2: \n \nFigure 2: Original instrument recognition algorithm \nThe labels ob\ntained in this experiment are named “n” \nfor “no separation”. In this configuration, the stereo audio \ninput is transformed into mono, by adding the left and \nright channels. We obtain the following micro averages: \nprecision = 0.708, recall = 0.258 and F1 = 0.378. \n3.2 Experiment 2: Left/Right-Mid/Side separation + \noriginal models \nIn this experiment, audio was segregated into four streams \nwith l = Left, r = Right, n = l+r (Mid), and s = l-r (Side), \nand the original model was used for classification, as de-\npicted in Figure 5. \n \nFigure 5. LRMS separation into lrns streams, used as \ninput of the original instrument recognition models. \nThe label “n” is used for the “Mid” stream in order to be \nconsistent with the notation in Experiment 1, also per-\nformed in the addition of the Left and Right channels.  Evaluation results showed that the best combination is \nwith “Exp2:lrns”, obtaining a micro F1 = 0.451. This rep-\nresents an absolute improvement of 7.3 percent points in \nthe micro F1 with respect to the original algorithm \n“Exp1:n”, or in relative terms, a 19.3%. This is a consid-\nerable improvement, especially taking into account that \nthis is a very simple segregation method which could even \nbe performed in real time. \n3.3 Experiment 3: FASST + original models \nIn this experiment, FASST separation into the bass (b), \ndrums (d), melody (m)  and other (o) streams is used, \nalong with the original models for the instrument recogni-\ntion, as shown in Figure 3. \n \nFigure 3. FASST separation into the drum, bass, melody \nand other streams, combined with the original instru-\nment recognition models. \nThe evaluation showed that the original algorithm \nwithout source separation provides better results than any \nof the combinations of the labels obtained in Experiment \n3. The best micro F1 (0.355) is obtained with a combina-\ntion of all separated streams (“Exp3:dbmo”). In this case, \nrecall (0.385) is better than with the original algorithm \n(“Exp1:n”), but precision is quite worse (0.330), so the \nF1-measure is lower. There is thus a decrease in perfor-\nmance when using source separation as a pre-step of the \noriginal instrument recognition models. This is probably \ndue to the fact that the separation is not perfect, there is \nsome energy of instruments in streams where there should \nnot be present, and their timbre is modified. Additionally, \nthe separation algorithm has the drawback of its complex-\nity and execution time, which is above one minute per se-\ncond of to-be-separated audio (Intel Core 2 Duo @ 2.4 \nGHz, 4 GB RAM with Windows XP – 32 bits). \n3.4 Experiment 4: FASST + models trained with sep-\narated audio \nIn this experiment, the instrument recognition models \nhave been trained with the dbmo audio streams obtained \nfrom separating the training dataset with FASST. Four \ndifferent models have been created; one for each of the \noutput streams of the FASST bdmo separation algorithm, \nas shown in Figure 4. A different set of features has been \nautomatically selected for each of the SVM models dur-\ning the training process.   \n \n \nFigure 4. FASST separation into the drum, bass, melody \nand other streams, combined with the instrument recog-\nnition using models trained on the separated audio. \nThe evaluation showed that using the models trained \non each of the streams of separated audio provides better \nresults than using the original models, and better than the \noriginal algorithm without any sound segregation. The \ncombination of the “m” and “o” labels already improves \nthe results obtained in “Exp1:n”, obtaining a micro F1 = \n0.411. The best micro F1-measure (0.446) is obtained \nwith the bdmo combination. If the “n” labels are addition-\nally combined, the micro F1 increases to 0.480. \nIf we analyze the recognition results per instrument, \nthe best are obtained with the voice, achieving 0.902 pre-\ncision, 0.574 recall and a 0.701 F1-measure. Clarinet \nseems to be the most challenging instrument to be recog-\nnized, with a F1-measure = 0.113. A further observation \nis that there is a relation between the stream and the in-\nstruments which are better recognized. For instance, the \nrecognition in the bass stream is better for instruments \nwith low frequency content, such as the cello, which is \nnot so well recognized in the rest of the streams. \n3.5 Experiment 5: Optimizing the performance of \nFASST + models trained with separated audio \nIn this experiment, we aimed at improving the results ob-\ntained in Experiment 4 — FASST dbmo separation + \nmodels trained with separated audio. Different models are \nused for the recognition of each of the four audio streams, \nand thus it is possible to optimize the parameters of each \nof them. Additionally, we also investigate the requirement \nof a certain degree N of overlap in the combination of la-\nbels. The evaluation showed that if the value of N is in-\ncreased, the precision increased as well, at the expense of \na lower recall. With N = 0 (no overlap required), the ob-\ntained micro F1 is equal to 0.446. If N = 1, which is \nequivalent to outputting only the labels which had been \npredicted by at least two of the classifiers, we obtain the \nbest precision found in all experiments (0.733), but the \nrecall is considerably reduced (0.354), and the F1-\nmeasure is thus smaller. Therefore, the overall perfor-\nmance is considered to be worse when N increases.  \nAs in all previous experiments, the minimum degree of \noverlap between labels was set to N=0 in Experiment 5, \nwhich provided the best results in terms of the F1-\nmeasure. The output labels were thus the union of all la-bels predicted by each of the models. On the other hand, \nthe use of a different configuration for the training of each \nof the four models led to some improvements in the re-\nsults, achieving a micro F1= 0.497. In order to further \nimprove the results we tried combining the labels derived \nfrom both, source separation and panning-based segrega-\ntion streams. The combination Exp5:dbmonslr achieved \nthe best F1-measure from all experiments, equal to 0.503.  \nThe most relevant results of all experiments are pre-\nsented in Table 1. \n \n Mac \nPrec Mac \nRec Mic \nPrec Mic \nRec Mac \nF1 Mic \nF1 \nExp1:n 57.8 24.9 70.8 25.8 34.9 37.8  \nExp2:lrns 48.5 33.8 58.2 36.7 39.8 45.1  \nExp3:dbmo 31.0 37.0 33.0 38.5 33.7 35.5  \nExp4:dbmo 49.0 30.6 62.5 34.7 37.7 44.6  \nExp4:dbmon 47.5 37.3 59.3 40.3 41.8 48.0  \nExp5:dbmon 44.0 41.5 54.9 45.5 42.7 49.7  \nExp5:dbmolrns 41.0 45.5 50.4 50.1 43.2 50.3  \nTable 1. Instrument recognition measures (in %). See \ntext for details regarding the studied experimental meth-\nods and their acronyms \nIn the following section we analyze the obtained results, \nand compare the evaluated approaches. \n4. DISCUSSION \nThe highest precision is obtained with the instrument \nrecognition algorithm [5] by itself (“Exp1:n”), at the ex-\npense of having a low recall, which provides a medium \nF1-measure. In “Exp2:nslr” we considerably improve the \nresults with a simple panning-based segregation, achiev-\ning a 19.2% relative increase in the micro-F1 with respect \nto the original algorithm. Experiment 3 makes use of the \nFASST dbmo separation as a pre-step to the instrument \nrecognition. In this experiment, the precision drops, and \nthe recognition performance is worse. After training the \nrecognition models with source separated data, we obtain \nconsiderably better results in “Exp4:dbmo” compared to \n“Exp3:dbmo” and also “Exp1: n” in terms of F1-measure. \nWith the aggregation of the sets of labels obtained with \nthe original algorithm, we obtain a further increase in the \nperformance in “Exp4:dbmon”. The results from \n“Exp5:dbmon” show that it is possible to further improve \nthe instrument recognition by tuning the parameters of \neach of the dbmo models. Finally “Exp5:dbmonslr” cor-\nresponds to the best results obtained in any of the auto-\nmatic instrument recognition experiments, by combining \n“dbmo” sets of labels from the tuned models trained with \nseparated streams, and the “Exp2:nslr” sets of labels ob-\ntained with the LRMS separation. The detailed results for \nall possible combination of labels and experiments can be \nfound in [2]. The best micro F1-measure = 0.503, thanks \nto the recall gained by the combination of all labels. The   \n \nmicro F1-mea\nsure obtained with the original algorithm \nwithout segregation was 0.378, so we were able to im-\nprove 12.2 percent points, which represents a 32.3% rela-\ntive to the initial value. It is interesting to note that the \nmicro averages are better than the macro averages, since \nthe majority of categories with the most frequent instanc-\nes (e.g. voice) are more easily recognized than the rest. \n5. CONCLUSIONS \nWe presented novel methods to improve the automatic \nrecognition of predominant musical instruments, by its \ncombination with audio segregation algorithms. A com-\nparison with previous similar approaches is not straight-\nforward, since the number of classes and datasets are dif-\nferent. However, if we compare the performance of the \noriginal algorithm with the best of our presented ap-\nproaches combining source separation and instrument \nrecognition, there is around 32% improvement of the mi-\ncro F1-measure. The way in which the combination is \nmade is very important to be able to improve the results \nof the algorithms: we found that the application of a \nsource separation pre-step may not provide a better \nrecognition of the instruments if the models do not con-\nsider the limitations and errors of the separation algo-\nrithms. Training the classification models with the differ-\nent streams of separated audio has been found to be an \neffective strategy for acknowledging the typical source \nseparation errors. This leads to a better performance, \nwhich can be further enhanced by tuning the parameters \nof each of the different models used in the instrument \nrecognition. A drawback of the use of the proposed sepa-\nration algorithm is its computational complexity. As a \nsimple, fast and efficient alternative, we propose the de-\ncomposition of the stereophonic polytimbral audio into \nthe left, right, mid and side streams, and the combination \nof the labels identified by the instrument recognition algo-\nrithms in each of the streams. This increased a 19.2% the \nperformance of the predominant instrument recognition. \n \nAcknowledgements: This research was partially support-\ned by “La Caixa” Fellowship Program, and the following \nprojects: Classical Planet: TSI-070100-2009-407 \n(MITYC), DRIMS: TIN2009-14247-C02-01 (MICINN) \nand MIRES: EC-FP7 ICT-2011.1.5 Networked Media \nand Search Systems, grant agreement No. 287711. \n6. REFERENCES \n[1] V. Alluri and P. Toiviainen, “Exploring perceptual \nand acoustical correlates of polyphonic timbre,” \nMusic Perception , vol. 27, no. 3, pp. 223–242, 2010. \n[2] J.J. Bosch, “Synergies between Musical Source \nSeparation and Instrument Recognition”, Master’s \nThesis, Universitat Pompeu Fabra, 2011. [3] J. J. Burred, “From sparse models to timbre learning: \nnew methods for musical source separation,” PhD \nThesis, Technical University of Berlin, Berlin, 2008. \n[4] S. Essid, G. Richard, and B. David, “Instrument \nrecognition in polyphonic music based on automatic \ntaxonomies,” IEEE Transactions On Audio Speech \nand Language Processing , vol. 14, no. 1, pp. 68-80, \n2006. \n[5] F. Fuhrmann, M. Haro, and P. Herrera, “Scalability, \ngenerality and temporal aspects in automatic \nrecognition of predominant musical instruments in \npolyphonic music,” in Proc. of ISMIR , 2009. \n[6] F. Fuhrmann and P. Herrera, “Polyphonic Instrument \nRecognition for exploring semantic Similarities in \nMusic,” in Proc. of 13th Int. Conference on Digital \nAudio Effects DAFx10,  Graz Austria, 2010, pp. 1-8. \n[7] T. Heittola, A. Klapuri, and T. Virtanen, “Musical \ninstrument recognition in polyphonic audio using \nsource-filter model for sound separation,” in Proc. of \nISMIR, 2009. \n[8] P. Herrera-Boyer, G. Peeters, and S. Dubnov, \n“Automatic Classification of Musical Instrument \nSounds,” Journal of New Music Research , vol. 32, \nno. 1, pp. 3-21, Mar. 2003. \n[9] T. Kitahara , M. Goto, K. Komatani, T. Ogata, and \nH. G. Okuno, “Instrument Identiﬁcation in \nPolyphonic Music: Feature Weighting to Minimize \nInﬂuence of Sound Overlaps”, EURASIP Journal on \nAdvances in Signal Processing , 2007. \n[10] N. Ono, K. Miyamoto, H. Kameoka, J. Le Roux, Y. \nUchiyama, E. Tsunoo, T. Nishomoto, and S. \nSagayama, “Harmonic and percussive sound \nseparation and its application to MIR-related tasks” \nin Advances in Music Information Retrieval . Vol. \n274 Springer, 2010, pp. 213–236. \n[11] A. Ozerov, E. Vincent, and F. Bimbot, “A general \nﬂexible framework for the handling of prior \ninformation in audio source separation”, IEEE \nTransactions on Audio, Speech and Language \nProcessing , vol. 20, no. 4, pp. 1118 – 1133. \n[12] G. Tzanetakis, “Song-specific bootstrapping of \nsinging voice structure,” in Proc IEEE International \nConference on Multimedia and Expo ICME , 2004. \n[13] T. Virtanen and A. Klapuri, “Analysis of poly-\nphonic audio using source-filter model and non-\nnegative matrix factorization,” in Advances in \nModels for Acoustic Processing , Neural Information \nProcessing Systems Workshop, 2006."
    },
    {
        "title": "Discriminative Non-negative Matrix Factorization for Multiple Pitch Estimation.",
        "author": [
            "Nicolas Boulanger-Lewandowski",
            "Yoshua Bengio",
            "Pascal Vincent"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417435",
        "url": "https://doi.org/10.5281/zenodo.1417435",
        "ee": "https://zenodo.org/records/1417435/files/Boulanger-LewandowskiBV12.pdf",
        "abstract": "In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to ex- tend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in or- der to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive po- tential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and or- chestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation.",
        "zenodo_id": 1417435,
        "dblp_key": "conf/ismir/Boulanger-LewandowskiBV12",
        "keywords": [
            "supervised",
            "multiple pitch estimation",
            "non-negative matrix factorization",
            "musical scores",
            "pitch information",
            "sparse NMF",
            "latent variable",
            "deterministic autoencoder",
            "logistic regressors",
            "basis spectra"
        ],
        "content": "DISCRIMINATIVE NON-NEGATIVE MATRIX FACTORIZATION FOR\nMULTIPLE PITCH ESTIMATION\nNicolas Boulanger-Lewandowski, Yoshua Bengio and Pascal Vincent\nDept. IRO, Universit ´e de Montr ´eal\nMontr ´eal, Qu ´ebec, Canada H3C 3J7\nfboulanni, bengioy, vincentpg@iro.umontreal.ca\nABSTRACT\nIn this paper, we present a supervised method to improve\nthe multiple pitch estimation accuracy of the non-negative\nmatrix factorization (NMF) algorithm. The idea is to ex-\ntend the sparse NMF framework by incorporating pitch\ninformation present in time-aligned musical scores in or-\nder to extract features that enforce the separability between\npitch labels. We introduce two discriminative criteria that\nmaximize inter-class scatter and quantify the predictive po-\ntential of a given decomposition using logistic regressors.\nThose criteria are applied to both the latent variable and the\ndeterministic autoencoder views of NMF, and we devise\nefﬁcient update rules for each. We evaluate our method\non three polyphonic datasets of piano recordings and or-\nchestral instrument mixes. Both models greatly enhance\nthe quality of the basis spectra learned by NMF and the\naccuracy of multiple pitch estimation.\n1. INTRODUCTION\nNon-negative matrix factorization (NMF) is an unsuper-\nvised technique to discover parts-based representations un-\nderlying non-negative data [12], i.e. a set of characteristic\ncomponents that can be combined additively to reconsti-\ntute the observations. When applied to the magnitude spec-\ntrogram of a polyphonic audio signal, NMF can discover a\nbasis of interpretable recurring note events and their asso-\nciated time-varying encodings, or activities, that together\noptimally reconstruct the original spectrogram.\nIn general, the extracted representation will converge to\nindividual note spectra provided the following conditions\nare met [5]. First, each observed spectrogram frame must\nbe representable as a non-negative linear combination of\nthe isolated note spectra, an approximation that depends on\nthe interference between overlapping harmonic partials in\na polyphonic mix but that is nevertheless reasonable [22].\nThe second condition requires that basis spectra be linearly\nindependent, and the third condition requires that all com-\nbinations of individual notes be present in the database.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.This last assumption is of course difﬁcult to achieve com-\npletely but partial combinations seem sufﬁcient in practice.\nConsequently, the activities extracted by NMF have proven\nuseful as features to detect individual note pitches played\nsimultaneously at a given instant in a polyphonic audio sig-\nnal, a task known as multiple pitch estimation, and for the\nrelated task of transcribing audio excerpts into musical no-\ntation [1, 3, 4, 19]. Sparsity, temporal and spectral priors\nhave proven useful to enhance the accuracy of multiple\npitch estimation [3, 7, 20].\nSince NMF is an unsupervised technique, it can be ap-\nplied in principle to an unlimited number of musical record-\nings without the need for ground-truth pitch labels. How-\never, such information is often readily available as recorded\nexpressive performances, symbolic sequences (e.g. a MIDI\nﬁle) or time-aligned musical scores. In those cases, we\nwould like to exploit the pitch information to steer the NMF\ndecomposition in a supervised way to obtain discrimina-\ntive features more useful for multiple pitch estimation. A\nfew attempts have been made in this direction, notably\nby adding a linear discriminant analysis (LDA) stage to\nthe activities extracted by NMF [23], or by embedding\nFisher-like discriminant constraints inside the decomposi-\ntion [9, 21, 23]. Discriminative dictionaries have also been\ndeveloped for sparse coding [15]. Those methods how-\never are designed for classiﬁcation, which means choosing\na single label, whereas multiple pitch estimation is a multi-\nlabel task, i.e. multiple pitch labels can be associated with\na single spectrogram frame. In this context, we propose\ntwo discriminative criteria that maximize inter-class scatter\nfor each label separately and estimate the predictive power\nof a given decomposition using logistic regressors. Those\nideas are applied in the conventional latent variables frame-\nwork of NMF and in a deterministic autoencoder model\nto directly maximize test-time discriminative performance.\nEfﬁcient update rules are devised for each, and we show\nthat our method greatly improves the quality of the basis\nspectra learned by NMF and the accuracy of multiple pitch\nestimation on three polyphonic datasets of piano record-\nings and orchestral instrument mixes.\nThe remainder of this paper is organized as follows. In\nSections 2 and 3, we review the NMF algorithm and its ap-\nplication to multiple pitch estimation. In Sections 4 and 5\nwe introduce the latent variables and autoencoder discrim-\ninative models. We describe our experiments and evaluate\nour method in Sections 6 and 7.Spectrogram X Dictionary W Activity matrix H≈ x\n0 10 20 30 40 50 60\ndictionary column index0.00.51.01.52.02.53.0frequency (kHz)\n0 2 4 6 8 10 12 14\ntime (s)0.00.20.40.60.8frequency (kHz)\n0 2 4 6 8 10 12 14\ntime (s)0102030405060dictionary column index\n0 2 4 6 8 10 12 14 16\ntime (s)45505560657075MIDI note number\nTarget score YFigure 1. Illustration of the sparse NMF decomposition (\u0015 = 0:01,\u0016= 10\u00005) of an excerpt of Drigo’s Serenade. Using\na dictionary Wpretrained on a polyphonic piano dataset, the spectrogram Xis transformed into an activity matrix Hap-\nproximating the piano-roll transcription Y. The columns of Wwere sorted by increasing estimated pitch for visualization.\n2. NON-NEGATIVE MATRIX FACTORIZATION\nThe NMF method aims to discover an approximate factor-\nization of an input matrix X:\nnf\u0002nt\nX'nf\u0002nt\n\u0003\u0011nf\u0002m\nW\u0001m\u0002n tH (1)\nwhereXis the observed magnitude spectrogram with time\nand frequency dimensions ntandnfrespectively, \u0003is the\nreconstructed spectrogram, Wis a dictionary matrix of m\nbasis spectra and His the activity matrix. Non-negativity\nconstraintsWi;j\u00150;Hi;j\u00150apply on both matrices.\nNMF seeks to minimize the reconstruction error, a distor-\ntion measure between the observed spectrogram Xand the\nreconstruction \u0003. A popular choice is the Euclidean dis-\ntance:\nCLS\u0011jjX\u0000\u0003jj2(2)\nwith which we will demonstrate our method although it can\nbe easily generalized to other distortion measures in the \f-\ndivergence family [11]. Minimizing CLScan be achieved\nby alternating multiplicative updates to HandW[13]:\nH H\u000eWTX\nWT\u0003(3)\nW W\u000eXHT\n\u0003HT(4)\nwhere the\u000eoperator denotes element-wise multiplication,\nand division is also element-wise. These updates are guar-\nanteed to decrease the reconstruction error assuming a lo-\ncal minimum is not already reached. While the objective\nis convex in either WorHseparately, it is non-convex in\nWandHtogether and thus ﬁnding the global minimum is\nintractable in general.\n2.1 Sparsity constraints\nIn a polyphonic signal with relatively few notes played at\nany given instant, it is reasonable to assume that active ele-\nmentsHijshould be limited to a small subset of the avail-\nable basis spectra. To encourage this behavior, a sparsity\npenaltyCScan be added to the total SNMF objective [10]:\nCS=\u0015jHj (5)wherej\u0001jdenotes theL1norm and\u0015speciﬁes the relative\nimportance of sparsity. In order to eliminate underdeter-\nmination associated with the invariance of WH under the\ntransformation W!WD ,H!D\u00001H, whereDis a\ndiagonal matrix, we impose the constraint that the basis\nspectra have unit norm. Equation (3) becomes:\nH H\u000eWTX\nWT\u0003 +\u0015(6)\nand the multiplicative update to W(equation 4) is replaced\nby projected gradient descent [14]:\nW W\u0000\u0016(\u0003\u0000X)HT(7)\nW:i W:i\njjW:ijj(8)\nwhereW:iis thei-th column of W,\u0016is the learning rate\nand1\u0014i\u0014m.\n3. NMF FOR MULTIPLE PITCH ESTIMATION\nThe ability of NMF to extract fundamental note events\nfrom a polyphonic mixture makes it an obvious stepping\nstone for multiple pitch estimation. In the ideal scenario,\nthe dictionary Wcontains the spectrum proﬁles of indi-\nvidual notes composing the mix and the activity matrix H\napproximately corresponds to the ground-truth score. An\nexample of the sparse NMF decomposition of an excerpt of\nDrigo’s Serenade using a dictionary pretrained on a simple\npolyphonic piano dataset is illustrated in Figure 1. The\ndictionary contains mostly monophonic basis spectra that\nwere sorted by increasing estimated pitch for visualization.\nWe also observe a clear similarity between the activity ma-\ntrix and the target score in a piano-roll representation Y.\nThere are many options to exploit the NMF decomposi-\ntion to perform actual multiple pitch estimation. The dic-\ntionary inspection approach [1, 18, 19] consists in estimat-\ning the pitch (or lack thereof) of each column of W, which\ncan be done automatically using harmonic combs [20], and\nto transcribe all pitches for which the associated Hijactiv-\nities exceed a threshold \u0011:\nYkj= 1,X\nijL(i)=kHij\u0015\u0011 (9)whereL(i)is the estimated pitch label (index) of the i-th\nbasis spectrum. For this method, a new factorization can\nbe performed adaptively for each new piece to analyze,\nor the dictionary can be pretrained from an extended cor-\npus and kept ﬁxed during testing. Dictionaries can also be\nconstructed from the concatenation of isolated note spec-\ntra [3, 4].\nAnother option is to predict each column of Yfrom\nthe corresponding column of Husing a general-purpose\nmulti-label classiﬁer or a set of binary classiﬁers, one for\neach label (note) in the designated range. This obviously\nrequires the use of a ﬁxed dictionary and the availability\nof annotated pieces to train the classiﬁers. In this work, we\nwill exclusively employ pretrained dictionaries and we will\nconsider both dictionary inspection and multi-label classi-\nﬁcation with linear support vector machines (SVM) [17].\n4. DISCRIMINATIVE CRITERIA\nThe simple interpretation of the activity matrix as an ap-\nproximate transcription usually deteriorates when we in-\ncrease instrumental diversity, pitch range or polyphony. In\nthis section, we introduce two discriminative criteria ex-\nploiting the aligned score information Yto ensure that NMF\nextracts meaningful features into WandH.\nThe ﬁrst criterion is inspired from linear discriminant\nanalysis in that we aim to maximize the inter-class scatter\nof theHij, where the classes here refer to the presence or\nabsence of a given pitch label at a given time. We encour-\nage the activities associated with a given basis spectrum to\nbe maximal when its pitch is present in the score and mini-\nmal otherwise, such that a unidimensional decision thresh-\nold is sufﬁcient to estimate the presence of a note. We ﬁrst\nassign a pitch label L(i)to each column iofW, or set\nL(i) =\u00001to denote an unpitched basis spectrum. Due to\nthe invariance of WH under the column permutation of W\nand the equivalent row permutation of H, this assignment\ncan be done arbitrarily as long as the number of basis spec-\ntra describing each pitch (q ) and the number of unpitched\nspectra ( \u0016q) remain constant. More precisely, this criterion\nhas the form:\nCd(H) =X\nij8\n><\n>:\u0000\f+Hij ifYL(i);j = 1\n\f\u0000Hij ifYL(i);j = 0\n0 ifL(i) =\u00001(10)\nwhere the\f+and\f\u0000parameters quantify respectively the\nimportance of presence and absence of an Hijelement.\nNote that the limit \f\u0000!1 corresponds to setting Hij=\n0forYL(i);j = 0.\nThe second proposed criterion does not impose a pre-\ndetermined structure on the activity matrix, but rather at-\ntempts to determine whether His a good predictor for Y.\nWe introduce a stage of logistic regressors with weight ma-\ntrixVand bias vector busingHas input:\npkj=\u001b((VH )kj+bk) (11)\nwhere\u001b(x)\u0011(1+e\u0000x)\u00001is the element-wise logistic sig-\nmoid function and pis an output matrix of note probabili-\nX\nYW\nH*Minimization\nProblem�\nOutput\nPredictionFigure 2. In the DNMF autoencoder model, the input is\nencoded via a deterministic minimization procedure. The\ncodeH\u0003is trained to reconstruct Xand to predict Y.\nties, or probabilistic piano-roll. We use the cross-entropy\nas a discriminative criterion for H:\nCl(H) =\u0000\u000bX\nkjYkjlogpkj+ (1\u0000Ykj) log(1\u0000pkj)\n(12)\nwhere\u000bis a weighting coefﬁcient. Adding our criteria to\nthe total objective yields the DNMF model:\nC=CLS+CS+Cd+Cl: (13)\nIt is easy to show that the Hessian matrices r2\nHCd(H)\nandr2\nHCl(H)are both positive semi-deﬁnite and that the\nDNMF objective remains convex in WorHseparately.\nThe multiplicative update rule for H(equation 6) becomes:\nH H\u000eWTX\nWT\u0003 +\u0015+@Cd(H)\n@H+@Cl(H)\n@H(14)\nwhere the gradients are given by:\n@Cd(H)\n@Hij=8\n><\n>:\u0000\f+ifYL(i);j= 1\n\f\u0000ifYL(i);j= 0\n0 ifL(i) =\u00001(15)\n@Cl(H)\n@H=\u000bVT(p\u0000Y): (16)\nThe update rules for Ware the same as for sparse NMF and\nare given by (7) and (8). The Vandbparameters are opti-\nmized via stochastic gradient descent using the updates:\nV V\u0000\u0016(p\u0000Y)HT(17)\nbk bk\u0000\u0016X\nj(pkj\u0000Ykj): (18)\n5. AUTOENCODER MODEL\nIn the probabilistic latent variables model (LV) underly-\ning NMF, the activities are regarded as hidden variables\nwith joint negative log probability given by (13) and the\nuse of equations (14) and (7-8) during training corresponds\nto the expectation and maximization phases of an EM al-\ngorithm [12]. A subtlety associated with this interpretation\narises in testing conditions when the labels Yare unknown.\nWe can resort to equation (6) to infer H, but it is possible\nto address this issue in a more principled manner with the\nautoencoder model (AE) presented in this section.\nLet us consider the value of Hobtained in testing con-\nditions, denoted H\u0003:\nH\u0003(W)\u0011arg min\nH(CLS+CS) (19)and let us apply the same discriminative criteria Cd(H\u0003)\nandCl(H\u0003)on that variable. Since H\u0003is a purely de-\nterministic function of the input with Wthe only learned\nparameter, this model can be assimilated to an autoencoder\nwith the encoding step consisting in a complex minimiza-\ntion problem (equation 19) and the decoding step is the\nusual linear input reconstruction (equation 1). In addition,\nthe discriminative criteria encourage H\u0003to be a good pre-\ndictor ofY. The overall model is depicted in Figure 2. The\nprojected gradient descent update for Wbecomes:\nW W\u0000\u0016@C(H\u0003)\n@W(20)\nW:i W:i\njjW :ijj(21)\nSinceH\u0003(W)is the result of an optimization process,\nthe gradient of C(H\u0003)with respect to Wis not trivial to\ncompute. We can exploit the convergence guarantee of the\nmultiplicative update (6) to express H\u0003as an inﬁnite se-\nquence truncated to Kiterations:\nH\u0003= lim\nk!1Hk'HK(22)\nwhere:\nHk+1=Hk\u000eWTX\nWTWHk+\u0015(23)\nfrom which the gradients are easily computed by back-\npropagation through iteration kin an efﬁcient O(K)time:\n@C\n@Hk=@C\n@Hk+1\u000eHk+1\nHk\u0000WTWBk(24)\nfor0\u0014k<K , where the auxiliary variable Bkis:\nBk=@C\n@Hk+1\u000eHk+1\nWTWHk+\u0015: (25)\nThe initial conditions are:\n@C\n@HK=WT(WHK\u0000X) +\u0015+@Cd\n@HK+@Cl\n@HK(26)\nwhere the two rightmost terms are given by (15) and (16)\nwithH=HK. The gradient with respect to Wis then\ngiven by:\n@C\n@W=K\u00001X\nk=0h\nX\u0010@C\n@Hk+1\u000eHk+1\nWTX\u0011\n\u0000\nW(BkHkT+HkBkT)i\n+ (WHK\u0000X)HKT:(27)\nWhen computing @C=@W , the ﬁnite-sequence approx-\nimation (22) needs only be accurate in the vicinity of the\ncurrent value of W, denotedW0. We can increase efﬁ-\nciency without sacriﬁcing precision by initializing H0\u0011\nH\u0003(W0)and keeping Ksmall (< 10). Note also that this\ngradient may become inﬁnite when Wis rank deﬁcient,\na condition that arises when combinations of basis spectra\nmomentarily align [8]. This optimization issue is alleviated\nin practice by two facts: the basis spectra are renormalized\nafter each update (equation 21), and the use of a ﬁnite se-\nquence to approximate the gradient tends to smooth out\nsingularities.6. EVALUATION\nWe use three datasets to evaluate our method:\nRAND is a piano dataset of random chords part of the\nlarger MAPS database [6]. Each chord contains from 2 to 7\nnotes sampled from the whole piano range with heteroge-\nneous loudnesses. We randomly split the data into training,\nvalidation and test sets using a 4:1:1 ratio.\nORC is a random polyphonic dataset similar to RAND,\nbut that includes common orchestral instruments such as\nviolin, cello, trumpet, French horn, saxophone, oboe, bas-\nsoon, clarinet, ﬂute and piccolo, in addition to piano and\norgan. Each of the 3000 tracks contains 5 instruments si-\nmultaneously playing in their respective range for 16 sec-\nonds and was rendered with the FluidR3 SoundFont1.\nMUS is a collection of classical piano pieces also in-\ncluded in MAPS [6], that contains nine sets created by\nhigh-quality software synthesizers (7 sets) and a Yamaha\nDisklavier (2 sets). Five synthesizer sets were selected for\ntraining, with the remaining two held out for validation\nto avoid overﬁtting the speciﬁc piano tones heard during\ntraining. We used the ﬁrst 30 seconds of each piece from\nthe Disklavier sets for test. The average polyphony for this\ndataset is 2.9.\nThe magnitude spectrogram was computed for all data-\nsets by the short-term Fourier transform using a 93 ms slid-\ning Blackman window at 10 ms intervals. Each spectro-\ngram frame (column of X) was normalized and square root\ncompressed to reduce the dynamic range. The ground truth\nYwas directly inferred from the MIDI ﬁles [6].\nWe evaluate multiple pitch estimation performance with\nthe standard metrics of accuracy, precision, recall and F-\nmeasure [2]. Either dictionary inspection or linear SVMs\nusingH\u0003orXas input serve to estimate the pitches. The\nSVMs can optionally be replaced by multilayer percep-\ntrons (MLP) [16] for comparison. For each NMF model,\nthe parameters are ﬁrst selected to maximize accuracy on\nthe validation set and we report the ﬁnal performance on\nthe test set. Parameters are optimized over predetermined\nsearch grids on the following intervals:\nq2[1;7] \u0016 q2[0;12] \u00112[0;20]\n\f\u00062[10\u00006;10]\u000b2[10\u00002;102]\n\u00152[10\u00007;2]\u00162[10\u00006;10\u00003]\n7. RESULTS\nTo illustrate the effectiveness of our approach, we ﬁrst eval-\nuate qualitatively the learned basis and pitch activities on\npolyphonic piano data. The dictionary matrices obtained\non RAND via unsupervised NMF (Fig. 3(a)) and DNMF\n(Fig. 3(b)) are presented in Figure 3 after sorting the columns\nby increasing estimated pitch. From these results, it is clear\nthat DNMF extracted basis spectra – from a purely poly-\nphonic mix – that correspond much closely to the expected\nspectrum of individual piano notes. It is thus not surprising\nthat applying those dictionaries to extract pitch activities\nH\u0003from an excerpt of the MUS test set (Fig. 4(a)) yielded\n1http://www.hammersound.net01020304050607080\ndictionary column index01234frequency (kHz)(a)\n01020304050607080\ndictionary column index01234frequency (kHz)(b)Figure 3. Dictionaries trained (q = 1,\u0016q= 0) on the RAND dataset via NMF (a) and DNMF (b). Columns were sorted by\nincreasing estimated pitch for visualization.\n2 4 6 810 12 14\ntime (s)0.51.01.52.02.5frequency (kHz)(a)\n2 4 6 810 12 14\ntime (s)30405060708090100MIDI note number(b)\n2 4 6 810 12 14\ntime (s)01020304050607080dictionary column index(c)\n2 4 6 810 12 14\ntime (s)01020304050607080dictionary column index(d)\nFigure 4. Spectrogram (a) and piano-roll score (b) for the ﬁrst 15 seconds of an arpeggiated version of Silent Night, Holy\nNight from the MUS test set. Pitch activities H\u0003(c-d) were estimated for that signal using the pretrained dictionaries in\nFigure 3(a-b) respectively.\nless noisy estimates much closer to the ground-truth score\n(Fig. 4(b)), as can be observed from Figure 4(c-d).\nA more quantitative measure of the discriminative qual-\nity of the learned basis is the discriminative ratio r:\nr(H) =0\n@X\ni;jjYL(i);j =1Hij,X\ni;jjYL(i);j =0Hij1\nA:(28)\nAccording to this deﬁnition, we obviously favor higher ra-\ntios. Whilercan be made arbitrarily high in training con-\nditions simply by increasing \f\u0006, what we really care about\nis its value in testing conditions r(H\u0003). Figure 5 shows\na signiﬁcant increase in the test discriminative ratio with\nour latent variable algorithm compared to the sparse NMF\nbaseline, which indicates a much better pitch label sepa-\nrability. The additional improvement provided by the au-\ntoencoder model demonstrates that directly optimizing H\u0003\nis useful to increase discriminative performance.\nIn the next experiments we verify if the discriminative\nfeatures learned by our models translate in good pitch esti-\nmation performance. Frame-level accuracies on the RAND\nand ORC datasets are presented in Table 1 using dictionary\ninspection and in Table 2 for multi-label classiﬁcation. The\nproposed models outperform the baselines in all cases, es-\npecially DNMF-AE used in conjunction with SVMs. Ta-\nble 3 shows frame-level precision, recall and F-measure\nresults on the MUS test set for common existing NMF\nvariants. Our approach surpasses adaptive unconstrained\nNMF and is competitive with NMF trained on isolated pi-\nano notes and NMF with spectral constraints [20].\n0 20 40 60 80 100 120 140\ntraining iteration ( ×103)10−1100101102ratior\nDNMF-LV (tr.)\nDNMF-AE\nDNMF-LV\nSNMFFigure 5. Evolution of the ratio r(H\u0003)during training on\nthe RAND dataset. “tr” stands for training conditions.\nMethod RAND ORC\nNMF 27.6% 30.0%\nSNMF 32.3% 43.8%\nDNMF-LV 53.2% 58.8%\nDNMF-AE 53.4% 58.6%\nTable 1. Multiple pitch estimation accuracy obtained by\ndictionary inspection on the RAND and ORC datasets.\n8. CONCLUSION\nWe have shown that by exploiting pitch information present\nin time-aligned musical scores to encourage the extracted\nfeatures to discriminate against the pitch labels, we can im-\nprove the multiple pitch estimation performance of NMF\non three datasets of polyphonic music. Interestingly, theFeatures RAND ORC\nSpectrogram 50.9% 55.9%\nNMF 56.2% 59.4%\nSNMF 55.5% 59.5%\nDNMF-LV 60.4% 63.3%\nDNMF-AE 61.6% 65.5%\nSpectrogram (MLP) 52.7% 62.0%\nTable 2. Multiple pitch estimation accuracy obtained on\nthe RAND and ORC datasets via linear SVMs using the\nspeciﬁed feature extraction technique.\nNMF variant Prec. Rec. F-meas.\nNo training\nUnconstrainedy 58.9% 60.0% 57.8%\nSpectral constraints [20] 71.6% 65.5% 67.0%\nPretrained dictionary\nIsolated note spectra y 68.6% 66.7% 66.0%\nProposed (DNMF-LV) 68.1% 65.9% 66.9%\nProposed (DNMF-AE) 66.8% 68.7% 67.8%\nOther methods\nSONIC [16] 74.5% 57.6% 63.6%\nTable 3. Average multiple pitch estimation performance\nof common NMF variants on the MUS (MAPS) piano\ndataset.yThese results are from Vincent [20].\nresulting basis spectra closely resemble the spectrum of\nindividual piano notes, even though they were trained on\npurely polyphonic data without explicit harmonicity con-\nstraints. Once that discriminative basis is learned, relevant\npitch activity features can be efﬁciently computed using\nonly standard multiplicative updates.\n9. ACKNOWLEDGMENTS\nThe authors would like to thank NSERC, CIFAR and the\nCanada Research Chairs for funding.\n10. REFERENCES\n[1] S.A. Abdallah and M.D. Plumbley. Unsupervised anal-\nysis of polyphonic music by sparse coding. IEEE\nTrans. on Neural Networks, 17(1):179–196, 2006.\n[2] M. Bay, A.F. Ehmann, and J.S. Downie. Evaluation\nof multiple-F0 estimation and tracking systems. In IS-\nMIR, 2009.\n[3] A. Cont. Realtime multiple pitch observation using\nsparse non-negative constraints. In ISMIR, 2006.\n[4] A. Dessein, A. Cont, and G. Lemaitre. Real-time poly-\nphonic music transcription with non-negative matrix\nfactorization and beta-divergence. In ISMIR, 2010.\n[5] D. Donoho and V . Stodden. When does non-negative\nmatrix factorization give a correct decomposition into\nparts. In NIPS 16, 2003.\n[6] V . Emiya, R. Badeau, and B. David. Multipitch estima-\ntion of piano sounds using a new probabilistic spectralsmoothness principle. IEEE Trans. on Audio, Speech,\nand Language Processing, 18(6):1643–1654, 2010.\n[7] D. Fitzgerald, M. Cranitch, and E. Coyle. Generalised\nprior subspace analysis for polyphonic pitch transcrip-\ntion. In DAFX 8, 2005.\n[8] G.H. Golub and V . Pereyra. The differentiation of\npseudo-inverses and nonlinear least squares problems\nwhose variables separate. SIAM Journal on Numerical\nAnalysis, pages 413–432, 1973.\n[9] N. Guan, D. Tao, Z. Luo, and B. Yuan. Manifold regu-\nlarized discriminative nonnegative matrix factorization\nwith fast gradient descent. IEEE Transactions on Im-\nage Processing, 20(7):2030–2048, 2011.\n[10] P.O. Hoyer. Non-negative sparse coding. In Neural\nNetworks for Signal Processing, pages 557–565, 2002.\n[11] R. Kompass. A generalized divergence measure for\nnonnegative matrix factorization. Neural computation,\n19(3):780–791, 2007.\n[12] D.D. Lee and H.S. Seung. Learning the parts of ob-\njects by non-negative matrix factorization. Nature,\n401(6755):788–791, 1999.\n[13] D.D. Lee and H.S. Seung. Algorithms for non-negative\nmatrix factorization. In NIPS 13, 2001.\n[14] C.J. Lin. Projected gradient methods for non-\nnegative matrix factorization. Neural computation,\n19(10):2756–2779, 2007.\n[15] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisser-\nman. Discriminative learned dictionaries for local im-\nage analysis. In CVPR, pages 1–8, 2008.\n[16] M. Marolt. A connectionist approach to automatic tran-\nscription of polyphonic piano music. IEEE Transac-\ntions on Multimedia, 6(3):439–449, 2004.\n[17] G.E. Poliner and D.P.W. Ellis. A discriminative model\nfor polyphonic piano transcription. EURASIP Journal\non Applied Signal Processing, 2007(1):154–164, 2007.\n[18] P. Smaragdis. Polyphonic pitch tracking by example.\nInIEEE WASPAA, pages 125–128, 2011.\n[19] P. Smaragdis and J.C. Brown. Non-negative matrix fac-\ntorization for polyphonic music transcription. In IEEE\nWASPAA, pages 177–180, 2003.\n[20] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-\nmonic spectral decomposition for multiple pitch esti-\nmation. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 18(3):528–537, 2010.\n[21] Y . Wang and Y . Jia. Fisher non-negative matrix factor-\nization for learning local features. In ACCV, 2004.\n[22] C. Yeh and A. R ¨obel. The expected amplitude of over-\nlapping partials of harmonic sounds. In ICASSP, 2009.\n[23] S. Zafeiriou, A. Tefas, I. Buciu, and I. Pitas. Exploiting\ndiscriminant information in nonnegative matrix fac-\ntorization with application to frontal face veriﬁcation.\nIEEE Transactions on Neural Networks, 17(3):683–\n695, 2006."
    },
    {
        "title": "Neon.js: Neume Editor Online.",
        "author": [
            "Gregory Burlet",
            "Alastair Porter",
            "Andrew Hankinson",
            "Ichiro Fujinaga"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.49873",
        "url": "https://doi.org/10.5281/zenodo.49873",
        "ee": "https://zenodo.org/records/49873/files/Programming_Historian_Print_Edition_-_Feb_2016.pdf",
        "abstract": "The Programming Historian (http://programminghistorian.org) offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research.\n\nThis PDF version of the project is a snapshot of all published lessons as they appeared in February 2016. It contains 48 tutorials introducing topics ranging from:\n\n\n\tsetting up (5 lessons)\n\tacquiring data (7 lessons)\n\ttransforming data (17 lessons)\n\tanalyzing data (6 lessons)\n\tpresenting history (10 lessons)\n\tsustaining data (3 lessons)\n\n\nAll content is licensed under a creative commons license. We encourage you to use, distribute, and print out lessons (or the whole book) as it suits you. Go forth and learn!\n\nAuthor List:\n\nAmanda Morton, Spencer Roberts, James Baker, Sarah Simpkin, Dennis Tenen, Grant Wythoff, Ian Milligan, Seth van Hooland, Ruben Verborgh, Max De Wilde, Doug Knox, Laura Turner O&#39;Hara, Seth Bernstein, Jon Crump, Adam Crymble, Heather Froehlich, Vilja Hulden, Shawn Grahan, Scott Weingart, Fred Gibbs, Matthew Lincoln, Jim Clifford, Josh MacFadyen, Daniel Macfarlane, Marten Dring, Miriam Posner, Caleb McDaniel, Kellen Kurschinski, Jeri Wieringa, William J. Turkel.",
        "zenodo_id": 49873,
        "dblp_key": "conf/ismir/BurletPHF12",
        "keywords": [
            "Programming Historian",
            "novice-friendly tutorials",
            "humanists",
            "digital tools",
            "techniques",
            "workflows",
            "research",
            "creative commons license",
            "lessons",
            "lessons as it suits you"
        ],
        "content": "2   The Programming Historian http://programminghistorian.org    Printed Edition.   Edited by  Adam Crymble, Fred Gibbs, Allison Hegel, Caleb McDaniel, Ian Milligan, Miriam Posner, Evan Taparata, Jeri Wieringa, Jeremy Boggs, and William J. Turkel.     Print Edition collated by  Adam Crymble.                            2012-2015   \n 3     The Programming Historian and all lessons therein are released under the CC-BY license.  The project is published by the Editorial Board of the Programming Historian.  \n 4  Editorial Board of the Programming Historian  Albuquerque, United States Houston, United States Fairfax, United States London, United Kingdom Los Angeles, United States Minneapolis, United States Waterloo, Canada.    The Editorial Board of the Programming Historian is a non-legal entity, controlled entirely by its members. It exists solely to bring high quality technical tutorials to humanities scholars looking to improve their research skills.  http://programminghistorian.org  All text in this volume, including tutorials, has been released under a Creative Commons ‘BY’ license. We encourage you to use, reuse, and redistribute this work as widely and creatively as you can. This license does not necessarily extend to datasets, images created by third parties and used within tutorials, or external web content.  First printed on paper 2016. Printed in the United Kingdom  ISSN 2397-2068 Online  The Editorial Board of the Programming Historian has no responsibility for the persistence or accuracy of URLs for external or third-party Internet websites referred to in this publication, and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.  \n 5        This book is dedicated to our  authors, reviewers, editors, and readers.   And to anyone who wants to learn.  \n 6 Contents   Introduction\t13\t Part\tOne:\tSetting\tUp\t16\t1.\tPython\tIntroduction\tand\tInstallation\t17\t2.\tSetting\tUp\tan\tIntegrated\tDevelopment\tEnvironment\tfor\tPython\t(Mac)\t19\t3.\tSetting\tup\tan\tIntegrated\tDevelopment\tEnvironment\tfor\tPython\t(Linux)\t23\t4.\tSetting\tUp\tan\tIntegrated\tDevelopment\tEnvironment\tfor\tPython\t(Windows)\t27\t5.\tInstalling\tPython\tModules\twith\tpip\t33\t\tPart\tTwo:\tAcquiring\tData\t36\t6.\tDownloading\tWeb\tPages\twith\tPython\t37\t7.\tAutomated\tDownloading\twith\tWget\t43\t8.\tApplied\tArchival\tDownloading\twith\tWget\t52\t9.\tDownloading\tMultiple\tRecords\tUsing\tQuery\tStrings\t60\t10.\tIntro\tto\tthe\tZotero\tAPI\t84\t11.\tData\tMining\tthe\tInternet\tArchive\tCollection\t88\t12.\tUsing\tSPARQL\tto\taccess\tLinked\tOpen\tData\t101\t Part\tThree:\tTransforming\tData\t119\t13.\tWorking\twith\tText\tFiles\tin\tPython\t120\t14.\tCode\tReuse\tand\tModularity\tin\tPython\t125\t15.\tManipulating\tStrings\tin\tPython\t128\t16.\tFrom\tHTML\tto\tList\tof\tWords\t(part\t1)\t133\t17.\tFrom\tHTML\tto\tList\tof\tWords\t(part\t2)\t138\t18.\tNormalizing\tTextual\tData\twith\tPython\t145\t19.\tKeywords\tin\tContext\t(Using\tn-grams)\twith\tPython\t150\t20.\tCreating\tNew\tItems\tin\tZotero\t156\t21.\tIntro\tto\tBeautiful\tSoup\t159\t22.\tIntroduction\tto\tthe\tBash\tCommand\tLine\t172\t23.\tCounting\tand\tmining\tresearch\tdata\twith\tUnix\t189\t24.\tCleaning\tData\twith\tOpenRefine\t196\t25.\tUsing\tGazetteers\tto\tExtract\tSets\tof\tKeywords\tfrom\tFree-Flowing\tTexts\t205\t26.\tUnderstanding\tRegular\tExpressions\t225\t \n 7 27.\tCleaning\tOCR’d\ttext\twith\tRegular\tExpressions\t244\t28.\tGenerating\tan\tOrdered\tData\tSet\tfrom\tan\tOCR\tText\tFile\t253\t29.\tTransliterating\tnon-ASCII\tcharacters\twith\tPython\t284\t Part\tFour:\tAnalyzing\tData\t294\t30.\tCounting\tWord\tFrequencies\twith\tPython\t295\t31.\tCounting\tFrequencies\tfrom\tZotero\tItems\t303\t32.\tGetting\tStarted\twith\tTopic\tModeling\tand\tMALLET\t308\t33.\tCorpus\tAnalysis\twith\tAntconc\t323\t34.\tData\tExtraction\tand\tNetwork\tVisualization\tof\tHistorical\tSources\t338\t35.\tSupervised\tClassification:\tThe\tNaive\tBayesian\tReturns\tto\tthe\tOld\tBailey\t357\t Part\tFive:\tPresenting\tHistory\t385\t36.\tUnderstanding\tWeb\tPages\tand\tHTML\t386\t37.\tOutput\tData\tas\tan\tHTML\tFile\twith\tPython\t390\t38.\tCreating\tand\tViewing\tHTML\tFiles\twith\tPython\t395\t39.\tOutput\tKeywords\tin\tContext\tin\tan\tHTML\tFile\twith\tPython\t399\t40.\tUp\tand\tRunning\twith\tOmeka.net\t406\t41.\tCreating\tan\tOmeka.net\tExhibit\t418\t42.\tIntro\tto\tGoogle\tMaps\tand\tGoogle\tEarth\t422\t43.\tInstalling\tQGIS\t2.0\tand\tAdding\tLayers\t441\t44.\tCreating\tNew\tVector\tLayers\tin\tQGIS\t2.0\t458\t45.\tGeoreferencing\tin\tQGIS\t2.0\t470\t Part\tSix:\tSustaining\tData\t482\t46.\tPreserving\tYour\tResearch\tData\t483\t47.\tGetting\tStarted\twith\tMarkdown\t491\t48.\tSustainable\tAuthorship\tin\tPlain\tText\tusing\tPandoc\tand\tMarkdown\t499\t Epilogue\t514\t Editorial\tBoard\t515\t    \n 8 Preface  Humanists need somewhere to learn digital skills.  In 2007, William J. Turkel and Alan MacEachern provided the solution. The pair published The Programming Historian, a series of tutorials designed to teach historians programming skills.1 In 2011, their research assistant at the Network in Canadian History & Environment (NiCHE), Adam Crymble, put forth the following ‘supersecret’ proposal, which became the Programming Historian that you’re reading today:    [SUPERSECRET]  10 February 2011  The Programming Historian 2: A Sustainable, Collaborative Open Access Model for Academic Publishing  - A Proposal -  Academic publishing, meet the Internet. Most academics are suspicious of online work. The criticisms are justified; online publishing lacks peer review, the writing tends to be unpolished, and works are impermanent or frequently changed, frustrating attempts to cite. Those who support open access publishing – of whom there are many amongst digital humanists – dismiss the cost, sluggishness and rigidity of the academic publishing model. And yet relatively little work has been done to bring the two sides together. Instead, the conversations are dismissive and divisive. Digital humanists publish to their blogs or Twitter feeds, and traditional academics ignore them, while opting for more respectable places to publish. Rather than perpetuate the divide, a positive solution-based model is needed, one that addresses the concerns of those who support academic publishing and leads by example. This solution is a collaborative, open access version of The Programming Historian, which maintains rigorous standards, peer review, and relative permanence, while taking advantage of the flexibility of Web 2.0, Drupal and an engaged community.  This web-based project will provide an infrastructure to allow members of the community to contribute modules (chapters) for review under a                                                         1\tWilliam\tJ.\tTurkel\t&\tAlan\tMacEachern,\tThe\tProgramming\tHistorian\t(2007):\thttp://niche-canada.org/wp-content/uploads/2013/09/programming-historian-1.pdf.\t\n \n 9 Creative Commons “by-sa” license. The editor(s) will try each module to ensure it is accurate and work with the author to ensure the lesson is comprehensive for our target user: an educated but non-specialist researcher interested in learning new techniques. Approved modules will then added to the collection. Quality modules will be sought over quantity of content.  To ensure the site does not become a code repository, mechanisms will be put in place to promote a linear learning model similar to a book, but with a bit of choose your own adventure thrown in. Authors seeking to make submissions will be required to build directly upon an existing module, or to ensure that their module is entirely self-sufficient. In this way readers can always find the beginning of a set of lessons, and contributors are encouraged to work within the existing structure. The image to the left shows how this collaborative building might look. The entire project may have several starting branches, depending on what type of project the authors seek to contribute: physical computing, text mining, website building etc.    That ‘super secret’ plan became our open project.  We have not created exactly the project first envisioned by Crymble, but the spirit is still there. Fred Gibbs, who joined as an editor in 2012, was instrumental in adding extra flexibility, dropping the requirement to build directly upon an existing lesson and thus bringing in a much more diverse set of tutorials. Subsequently, we opted for CC-BY licenses instead of CC-BY-SA. And we never used Drupal for our website, though we could have done.  Most importantly, the project, and this book, is not designed to be read in order. Instead, read the lessons that are useful for your own needs. Jump around sections, and stop reading a tutorial when you’ve taken from it what you need to know. This is not a linear reading experience – though you are of course welcome to take that approach as well.  Since we first went public with the new site in 2012, we have published 48 tutorials with the help of dozens of authors, reviewers, workshop attendees, and students. More than 200,000 people have logged in to learn a new skill. And while we received tremendous support from the Network in Canadian \n \n 10 History and Environment in our early days, we’re proud to say, we’ve been able to build the project without an active budget. Digital Humanities projects have a reputation for being expensive. The Programming Historian was built with passion, pitched on a whim, and has grown to become one of the field’s most important publications. It’s our attempt to share skills, build a field, and break some moulds.  We hope you will find a new skill within these pages. If we have missed what you’re looking for, please get in touch and contribute a tutorial so that those who come after you can benefit from what you have learned. We’re building this together, for each other. \n 11 Acknowledgments  Thank you to everyone who made this project possible. To William J. Turkel and Alan MacEachern for writing the first rendition of the Programming Historian and supporting its subsequent re-envisioning. Thank you to our former editors, Jeremy Boggs and Carrie Sanders. To our current editors, Adam Crymble, Fred Gibbs, Allison Hegel, Caleb McDaniel, Ian Milligan, Miriam Posner, Evan Taparata, and Jeri Wieringa. Also to their friends and families and they who provide support while we volunteer our energy to this cause. Thank you to our authors Kellen Kurschinski, Seth Bernstein, James Baker, Amanda Morton, Dennis Tenen, Grant Wythoff, Shawn Graham, Scott Weingart, Jim Clifford, Daniel Macfarlane, Josh MacFadyen, Jon Crump, Vilja Hulden, Marten Düring, Max De Wilde, Ruben Verborgh, Seth van Hooland, Jonathan Reeve, Spencer Roberts, Doug Knox, Heather Froehlich, Laura Turner O'Hara, Matthew Lincoln, Sarah Simpkin, Adam Crymble, Fred Gibbs, Miriam Posner, Jeri Wieringa, Ian Milligan, and Caleb McDaniel. Thank you to our reviewers Matthew Lincoln, Daniel van Strien, Andrew Dunning, Robert Blades, Mila Oiva, Amanda Morton, Taryn Dewar, James Baker, Jonathan Reeve, Jim Clifford, Jacob Greene, Luke Bergmann, Kim Pham, John Fink, Thomas J. Duck, Patrick Burns, M. H. Beals, William J Turkel, Eduardo Sanchez, John Laudun, Tim Hitchcock, Konrad Lawson, Nora McGregor, Dennis Tenen, Mary Beth Start, Finn Arne Jørgensen, Jeff Bain-Conkin, Nick Pearce, Eli Pousson, Sarah Simpkin, Aurélien Berra, Chris Forster, Nick Ruest, Dave Shepard, Silvia Gutiérrez, Spencer Roberts, Sheila Brennan, Sara Palmer, John Levin, Amanda Visconti, Eli Pousson, Andrew Dunning, Tod Robbins, Jon Crump, Mauro Bieg, and Benjamin Schmidt. Thank you to the many people who contributed openly and anonymously to our discussions about improving the accessibility of the project, including Heather Froehlich, Mia Ridge, Molly Desjardin, Scott Weingart, Fiona Tweedie, Matthew Lincoln, James Baker, Rebecca Sutton Koeser, Shawn Graham, Nora McGregor, M.H. Beals, Andromeda Yelton, Erin Bush, Daniel van Strein, Patrick Murray-John, Amanda Visconti, Eric Phetteplace, and Jessica Lord. And thank you to the 200,000 readers of the Programming Historian around the world. \n 12  \n 13 Introduction  \n The Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research. We regularly publish new lessons, and we always welcome proposals for new lessons on any topic. Our editorial mentors will be happy to work with you throughout the lesson writing process. If you’d like to be a reviewer or if you have suggestions to make Programming Historian a more useful resource, please see the Contribute page on our website. Our editors and peer reviewers work collaboratively with authors to craft tutorials that illustrate fundamental digital and programming principles and techniques. We have lessons on Acquiring Data, Transforming Data, Analyzing Data, Presenting Data, Sustaining Data, in addition to a suite of lessons introducing the Python programming language. If you can’t find what you’re looking for, we welcome your online feedback. Better yet, get in touch via our website and contribute a lesson! The Programming Historian (ISSN 2397-2068) aims to set a new standard for openness and collaboration in scholarly publishing, and you can help!  \n \n 14 Open Source The Programming Historian is committed to open source and open access principles. All contributed lessons must make use of open source programming languages and open source software whenever possible. This policy is meant to minimize costs for all parties, and to allow the greatest possible level of participation. We believe everyone should be able to benefit from these tutorials, not just those with large research budgets for expensive proprietary software. Gold Open Access All submissions to The Programming Historian are published under a Creative Commons ‘BY’ license. This adheres to a ‘Gold’ open access model of publishing, which is fully compliant with RCUK funding and HEFCE publishing requirements for scholars in the UK,2 as well as the Canadian Tri-Agency Open Access Policy.3 'Gold’ open access means that the version of record is made freely available without subscription fee or restrictions on access. Authors are permitted to republish their tutorials anywhere. And so can anyone, as long as they cite the original author and respect his or her moral rights. We do not charge Article Processing Charges (APCs), nor do we charge library subscriptions. Peer Review All tutorials that appear on The Programming Historian have been rigorously peer reviewed and copy edited. Each lesson is guided through the review process by one of our editors who are assigned to the piece. Review involves a thorough exchange with the editor to ensure the lesson works as intended and that all concepts are explained fully for a non-specialist reader, before the tutorial is sent to external reviewers to test it and provide further comments. We aim to return reviewed material to authors quickly, but our first priority is always to ensure a quality product. Our peer review process is a bit different from what might be considered the ‘traditional’ peer review process. We do not solicit reviews to judge whether a tutorial is ‘good enough’ to be published. Rather, we consider the review process an integral component of a collaborative, productive, and sustainable effort for scholars to create useful technical resources for each other. Once a tutorial slips into our editorial workflow, our goal is to do                                                         2\t‘Open\tAccess’,\tResearch\tCouncils\tUK\t(2014):\thttp://www.rcuk.ac.uk/research/openaccess/;\t‘Open\tAccess\tResearch’,\tHigher\tEducation\tFunding\tCouncil\tfor\tEngland:\thttp://www.hefce.ac.uk/rsrch/oa/\t3\t‘Tri-Agency\tOpen\tAccess\tPolicy\ton\tPublications’,\tScience.gc.ca\t(2015):\thttp://www.science.gc.ca/default.asp?lang=En&n=F6765465-1.\t \n 15 everything we can to make sure the tutorial becomes as useful as possible and published in a reasonable amount of time.  Once the peer review has commenced, the role of the editor is to mediate between reviewers and authors, and keep the process on track in a timely manner. We strive to ensure all tutorials are functional on their date of publication. From time to time technology changes and tutorials cease to function as intended. If this happens, please report it using the ‘Give Feedback’ link in the website footer, and we will assign an editor to fix the problem. Funding & Ownership The Programming Historian is a volunteer-led initiative, controlled entirely by the 'Editorial Board of the Programming Historian’ with the help of community contributors. It is not a legal entity, and does not currently receive direct funding from any source.  The project is grateful for past support by the Network in Canadian History & Environment (NiCHE), and for hosting support from the Roy Rosenzweig Centre for New Media (RRCHNM). If you would like to provide financial support to help the project grow, please contact one of the Editorial Board members listed on the website. This project is our attempt to demonstrate what open access academic publishing can and should be. Please tell your librarian to include the project in your library catalogue.   \n 16 Part One: Setting Up The lessons in this first section are about setting up your computer for digital history work. You do not need to install these programs, but some of the lessons in the rest of the volume do require the software described herein. You can either start off by getting completely set up, or you can do it as and when you need to.  \n 17 1. Python Introduction and Installation William J. Turkel and Adam Crymble – 2012    Lesson Goals This first lesson in our section on dealing with Online Sources is designed to get you and your computer set up to start programming. We will focus on installing the relevant software – all free and reputable – and finally we will help you to get your toes wet with some simple programming that provides immediate results. In this opening module you will install the Python programming language,4 the Beautiful Soup HTML/XML parser,5 and a text editor. Screencaps provided here come from Komodo Edit,6 but you can use any text editor capable of working with Python. Here's a list of other options: Python Editors.7 Once everything is installed, you will write your first programs, \"Hello World\" in Python and HTML. the Python Programming Language The first programming language we will introduce in the Programming Historian is Python, a free, open source language. Unless otherwise noted, we will be using Python v.2 throughout. Version 3 is available but we have elected to stick with version 2 for now because it’s the most widely used version and it is the one that ships preinstalled on new Macs. Python 3 has different syntax (think grammar rules) and if you are trying to use Python 3 with the Programming Historian, you may run into difficulties. We welcome version 3 translations of any of our lessons. Backup Your Work! Before you download or install any new software, it is crucial that you make backups of your work. Each day before you do any programming, make sure to back up your work. At the end of a day’s work, make another backup of any programs that you’ve written that day. You should back up your whole computer at least weekly, and preferably more frequently. It is also a good idea to make off-site backups of your work, so that you don't lose everything if something happens to your computer or to your home or                                                         4\t‘Python’:\thttps://www.python.org/\t5\t‘Beautiful\tSoup’:\thttp://www.crummy.com/software/BeautifulSoup/\t6\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/\t7\t‘Python\tEditors’,\tPython:\thttps://wiki.python.org/moin/PythonEditors/\t \n 18 office. Sites like Jungle Disk and Dropbox provide easy-to-use and relatively inexpensive online backup options.8 Choose Your Operating System Step 1 – Install and Set Up Software In order to work through the techniques in this website, you will need to download and install some freely available software. We have provided instructions for Mac, Windows and Linux in the following three lessons. Once you have installed the software for your operating system, move on to 'Understanding Web Pages and HTML'.9 If you run into trouble with our instructions or find something that doesn't work on your platform, please let us know. About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire. \n                                                        8\t‘Jungle\tDisk’:\thttps://www.jungledisk.com/;\t‘Dropbox’:\thttp://dropbox.com/.\t9\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’\tThe\tProgramming\tHistorian\t(2012).\t \n 19 2. Setting Up an Integrated Development Environment for Python (Mac) William J. Turkel and Adam Crymble – 2012   Back up your computer Mac users can take advantage of the Time Machine for this.10 Install Python v.2 As of May 2012, Mac OS X comes preinstalled with Python 2. You can check to see if you have Python installed by launching the Terminal in the ‘Applications/Utilities’ directory and entering which\tpython followed by the Enter key. Pushing the Enter key sends the command to the computer when using the terminal. If you see ‘/usr/bin/python’ or something similar containing the word ‘python’ and a bunch of slashes, then you are all set. If not, close the Terminal, download the latest stable release of the Python programming language (Version 2.7.3 as of May 2012) and install it by following the instructions on the Python website.11 Create a Directory To stay organized, it’s best to have a dedicated directory (folder) on your computer where you will keep your Python programs (e.g., programming-historian) and save it anywhere you like on your hard drive. Beautiful Soup Download the latest version of Beautiful Soup and copy it to the directory where you are going to put your own programs.12 Beautiful Soup is a library (a collection of prewritten code) that makes it easy for Python programs to break web pages down into meaningful chunks that can be further processed. Install Komodo Edit Komodo Edit is a free and open source code editor, but as we said in the introduction, you have many other text editing options.13 Some of our testers prefer a program called TextWrangler.14 Which you use is up to you,                                                         10\t‘Use\tTime\tMachine\tto\tback\tup\tor\trestore\tyour\tMac’\tApple\tSupport:\thttps://support.apple.com/en-gb/HT201250\t11\t‘Python’:\thttps://www.python.org/\t12\t‘Beautiful\tSoup’:\thttp://www.crummy.com/software/BeautifulSoup/\t13\t‘Python\tEditors’\tPython:\thttps://wiki.python.org/moin/PythonEditors/\t14\t‘TextWrangler’:\thttp://www.barebones.com/products/textwrangler/\t \n 20 but for the sake of consistency in our lessons, we will be using Komodo Edit. You can download a copy of Komodo Edit from the Komodo Edit website.15 Install it from the .DMG file Start Komodo Edit It should look something like this: \n\tscreenshot of Komodo Exit on OS X If you don’t see the Toolbox pane on the right hand side, choose View->Tabs\t&\tSidebars\t->Toolbox. It doesn’t matter if the Project pane is open or not. Take some time to familiarize yourself with the layout of the Komodo editor. The Help file is quite good Configure Komodo Edit Now you need to set up the editor so that you can run Python programs. In the Toolbox window, click on the gear icon and select “New\tCommand…“. This will open a new dialog window. Rename your command to “Run\tPython” and feel free to change the icon if you like. In the “Command” box, type %(python)\t%f\tand under \"Start in,\" enter %D\tClick OK. Your new Run Python command should appear in the Toolbox pane.                                                         15\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/\t\n \n 21 Step 2 – “Hello World” in Python It is traditional to begin programming in a new language by trying to create a program that says ‘hello world’ and terminates. We will show you how to do this in Python and HTML. Python is a good programming language for beginners because it is very high-level. It is possible, in other words, to write short programs that accomplish a lot. The shorter the program, the more likely it is for the whole thing to fit on one screen, and the easier it is to keep track of all of it in your mind. The languages that we will be using are all interpreted. This means that there is a special computer program (known as an interpreter) that knows how to follow instructions written in that language. One way to use the interpreter is to store all of your instructions in a file, and then run the interpreter on the file. A file that contains programming language instructions is known as a program. The interpreter will execute each of the instructions that you gave it in your program and then stop. Let’s try this. In your text editor, create a new file, enter the following two-line program and save it to your programming-historian directory as hello-world.py #\thello-world.py\tprint\t'hello\tworld'\tYour chosen text editor should have a “Run” button that will allow you to execute your program. If you are using TextWrangler, click on the “#!” button and Run. If all went well, it should look something like this: \tTextWrangler-hello-world Interacting with a Python shell Another way to interact with an interpreter is to use what is known as a shell. You can type in a statement and press the Enter key, and the shell will respond to your command. Using a shell is a great way to test statements to make sure that they do what you think they should. This is done slightly differently on Mac, Linux and Windows. You can run a Python shell by launching the “terminal”. On the Mac, open the Finder and double-click on Applications\t->\tUtilities\t->\tTerminal \n \n 22 then typing “python” into the window that opens on your screen. At the Python shell prompt, type print\t'hello\tworld'\tand press Enter. The computer will respond with hello\tworld\tWhen we want to represent an interaction with the shell, we will use -> to indicate the shell’s response to your command, as shown below: print\t'hello\tworld'\t->\thello\tworld\tOn your screen, it will look more like this: \n\thello world terminal on a Mac Now that you and your computer are up and running, we can move onto some more interesting tasks. If you are working through the Python lessons in order, we suggest you next try ‘Understanding Webb Pages and HTML’.16 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire. \n                                                        16\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’\tThe\tProgramming\tHistorian\t(2012).\t\n \n 23 3. Setting up an Integrated Development Environment for Python (Linux) William J. Turkel and Adam Crymble – 2012   Thanks to John Fink for providing the basis of this section. These instructions are for Ubuntu 12.04 LTS, but should work for any apt based system such as Debian, or Linux Mint, provided you have sudo installed. Back up your computer It is always important to make sure you have regular and recent backups of your computer. This is just good advice for life, and is not limited to times when you are engaged in programming. Install Python v. 2 and Python “Beautiful Soup” module Open a terminal (Dash\tHome, then type Terminal, then click on the Terminal icon). Now type: sudo\tapt-get\tinstall\tpython2.7\tpython-beautifulsoup Enter your password, and then type Y to finish the install. Note that you probably have Python 2.7 installed already, so don’t be alarmed if Ubuntu tells you that. Create a directory You will keep your Python programs in this directory. It can be anywhere you like, but it is probably best to put it in your home folder. Something like this in your open terminal window should do the trick: cd\t~\tmkdir\tprogramming-historian\tInstall Komodo Edit Komodo Edit is a free and open source code editor, but as we said in the introduction, you have many other text editing options.17 You can download Komodo Edit at the Komoto Edit Website.18 Once you’ve downloaded it, open it with Ubuntu’s package manager, extract it to your home directory, and follow the installation instructions. If you are following along with these instructions and have installed Komodo Edit, open the home folder, go to the Komodo-Edit-7/bin directory, and click on komodo. You can also                                                         17\t‘PythonEditors’\tPython:\thttps://wiki.python.org/moin/PythonEditors/\t18\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/\t \n 24 right click on the Komodo icon in your launcher and click “Lock\tto\tLauncher” to have Komodo saved permanently to your launcher bar. Make a “Run Python” Command in Komodo Edit In Komodo Edit, click the gear icon under Toolbox and select New\tCommand. In the top field type “Run\tPython\tFile“ In the Command field, type: %(python)\t%F Then hit the OK button at the bottom of the Add Command window. Step 2 – “Hello World” in Python It is traditional to begin programming in a new language by trying to create a program that says “hello world” and terminates. We will show you how to do this in Python and HTML. Python is a good programming language for beginners because it is very high-level. It is possible, in other words, to write short programs that accomplish a lot. The shorter the program, the more likely it is for the whole thing to fit on one screen, and the easier it is to keep track of all of it in your mind. The languages that we will be using are all interpreted. This means that there is a special computer program (known as an interpreter) that knows how to follow instructions written in that language. One way to use the interpreter is to store all of your instructions in a file, and then run the interpreter on the file. A file that contains programming language instructions is known as a program. The interpreter will execute each of the instructions that you gave it in your program and then stop. Let’s try this. In your text editor, create a new file, enter the following two-line program and save it to your programming-historian directory as hello-world.py #\thello-world.py\tprint\t'hello\tworld'\tYour chosen text editor should have a “Run” button that will allow you to execute your program. If all went well, it should look something like this (Example as seen in Komodo Edit. Click on the image to see a full-size copy):  \n 25 \n\thello world in Komodo Edit on a Mac Interacting with a Python shell Another way to interact with an interpreter is to use what is known as a shell. You can type in a statement and press the Enter key, and the shell will respond to your command. Using a shell is a great way to test statements to make sure that they do what you think they should. You can run a Python shell by launching the “terminal”. For Linux, go to Applications->\tAccessories\t->\tTerminaland do the same. At the Python shell prompt, type python\tThis will open up the Python prompt, meaning that you can now use Python commands in the shell. Now type print\t'hello\tworld'\tand press Enter. The computer will respond with hello\tworld\tWhen we want to represent an interaction with the shell, we will use -> to indicate the shell’s response to your command, as shown below: print\t'hello\tworld'\t->\thello\tworld\tOn your screen, it will look more like this: \n \n 26 \n\thello world terminal on a Mac Now that you and your computer are up and running, we can move onto some more interesting tasks. If you are working through the Python lessons in order, we suggest you next try ‘Understanding Web Pages and HTML’.19 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        19\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’\tThe\tProgramming\tHistorian\t(2012).\t\n \n 27 4. Setting Up an Integrated Development Environment for Python (Windows) William J. Turkel and Adam Crymble – 2012  Back up your computer It is always important to make sure you have regular and recent backups of your computer. This is just good advice for life, and is not limited to times when you are engaged in programming. Install Python v.2 Go to the Python website, download the latest stable release of the Python programming language (Version 2.7.3 as of May 2012) and install it by following the instructions on the Python website.20 Create a Directory To stay organized, it’s best to have a dedicated directory (folder) on your computer where you will keep your Python programs (e.g., programming-historian) and save it anywhere you like on your hard drive. Install Komodo Edit Komodo Edit is a free and open source code editor, but as we said in the introduction, you have many other text editing options.21 You can download a copy from the Komodo Edit website.22 Start Komodo Edit It should look something like this: \n\tKomodo Edit on Windows                                                         20\t‘Python’:\thttps://www.python.org/\t21\t‘PythonEditors’\tPython:\thttps://wiki.python.org/moin/PythonEditors/\t22\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/\t\n \n 28 If you don’t see the Toolbox pane on the right hand side, choose View\t->\tTabs\t->\tToolbox. It doesn’t matter if the Project pane is open or not. Take some time to familiarize yourself with the layout of the Komodo editor. The Help file is quite good Configure Komodo Edit Now you need to set up the editor so that you can run Python programs. 1. Choose Edit\t->\tPreferences. This will open a new dialog window. Select the Python category and set the “Default\tPython\tInterpreter” (it should be C:\\Python27\\Python.exe) If it looks like this, click OK:  \n   Set the Default Python Interpreter    2. Next, in the Preferences section select Internationalization. Select Python from the drop-down menu titled Language-specific Default Encoding and make sure that UTF-8 is selected as the default encoding method.23 \n                                                        23\t‘UTF-8’,\tWikipedia:\thttps://en.wikipedia.org/wiki/UTF-8\t\n \n 29 \n   Set the Language to UTF-8 Next choose Toolbox->Add->New\tCommand. This will open a new dialog window. Rename your command to ‘Run\tPython’. Under ‘Command’, type: %(python)\t%f\tIf you forget this command, Python will hang mysteriously because it isn’t receiving a program as input. Under ‘Start\tin’, enter: %D If it looks like this, click OK: \n\tRun Python Command Windows \n \n 30 “Run Python” Command Your new command should appear in the Toolbox pane. You may need to restart your machine after completing this step before Python will work with Komodo Edit Step 2 – “Hello World” in Python It is traditional to begin programming in a new language by trying to create a program that says “hello world” and terminates. We will show you how to do this in Python and HTML. Python is a good programming language for beginners because it is very high-level. It is possible, in other words, to write short programs that accomplish a lot. The shorter the program, the more likely it is for the whole thing to fit on one screen, and the easier it is to keep track of all of it in your mind. The languages that we will be using are all interpreted. This means that there is a special computer program (known as an interpreter) that knows how to follow instructions written in that language. One way to use the interpreter is to store all of your instructions in a file, and then run the interpreter on the file. A file that contains programming language instructions is known as a program. The interpreter will execute each of the instructions that you gave it in your program and then stop. Let’s try this. In your text editor, create a new file, enter the following two-line program and save it to your programming-historian directory as hello-world.py #\thello-world.py\tprint\t'hello\tworld'\tYour chosen text editor should have a “Run” button that will allow you to execute your program. If all went well, it should look something like this (Example as seen in Komodo Edit. Click on the image to see a full-size copy):  \n 31 \n\thello world in Komodo Edit Interacting with a Python shell Another way to interact with an interpreter is to use what is known as a shell. You can type in a statement and press the Enter key, and the shell will respond to your command. Using a shell is a great way to test statements to make sure that they do what you think they should. You can run a Python Shell by double-clicking on the python.exe file. If you installed version 2.7 (the most recent as of May 2012), then this file is probably located in the C:\\Python27\\python.exe directory. In the shell window that opens on your screen type: print\t'hello\tworld'\tand press Enter. The computer will respond with hello\tworld\tWhen we want to represent an interaction with the shell, we will use -> to indicate the shell’s response to your command, as shown below: print\t'hello\tworld'\t->\thello\tworld\tOn your screen, it will look more like this: \n \n 32 \n\tPython Shell on Windows Python Shell in Windows Now that you and your computer are up and running, we can move onto some more interesting tasks. If you are working through the Python lessons in order, we suggest you next try ‘Understanding Web Pages and HTML’.24 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        24\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’,\tProgramming\tHistorian\t(2012).\t\n \n 33 5. Installing Python Modules with pip Fred Gibbs – 2013   Lesson Goals This lesson shows you how to download and install Python modules. There are many ways to install external modules, but for the purposes of this lesson, we’re going to use a program called pip.25 As of Python 2.7.9 and newer, pip is installed by default. This tutorial will be helpful for anyone using older versions of Python (which are still quite common). Introducing Modules One of the great things about using Python is the number of fantastic code libraries that are widely and easily available that can save you a lot of coding, or simply make a particular task (like creating a CSV file, or scraping a webpage) much easier. When Googling for solutions to problems, you’ll often find sample code that uses code libraries you haven’t heard about before. Don’t let these scare you away! Once these libraries are installed on your computer, you can use them by importing them at the beginning of your code; you can import as many libraries as you’d like, such as import\tcsv\timport\trequests\timport\tkmlwriter\timport\tpprint\tFor new Python users, it can be a bit intimidating to download and install external modules for the first time. There are many ways of doing it (thus adding to the confusion); this lesson introduces one of the easiest and most common ways of installing python modules. The goal here is to install software on your computer that can automatically download and install Python modules for us. We’re going to use a program called pip. Note: As of Python 3.4, pip will be included in the regular install. There are many reasons why you might not have this version yet, and in case you don’t, these instructions should help. Mac and Linux instructions As per the pip documentation, we can download a python script to install pip for us. Using a Mac or Linux, we can install pip via the command line                                                         25\t‘pip’:\thttps://pip.pypa.io/en/stable/\t \n 34 by using the curl command, which downloads the pip installation perl script.26 curl\thttps://bootstrap.pypa.io/get-pip.py\tonce you’ve downloaded the get-pip.py file, you need to execute it with the python interpreter. However, if you try to execute the script with python like python\tget-pip.py\tthe script will most likely fail because it won’t have permissions to update certain directories on your filesystem that are by default set so that random scripts cannot change important files and give you viruses. In this case—and in all cases where you need to allow a script that you trust to write to your system folders—you can use the sudo command (short for “Super User DO”) in front of the python command, like sudo\tpython\tget-pip.py\tWindows Instructions As with the above platforms, the easiest way to install pip is through the use of a python program called get-pip.py, which you can download at https://bootstrap.pypa.io/get-pip.py. When you open this link, you might be scared of the massive jumble of code that awaits you. Please don’t be. Simply use your browser to save this page under its default name, which is get-pip.py. It might be a good idea to save this file in your python directory, so you know where to find it. Once you have saved this file, you need to run it, which can be done in two ways. If you prefer using your python interpreter, just right-click on the file get-pip.py and choose “open with” and then choose whatever python interpreter you care to use. If you prefer to install pip using the windows command line, navigate to whatever directory you’ve placed python and get-pip.py. For this example, we’ll assume this directory is python27, so we’ll use the command C:\\>cd python27. Once you are in this directory, run the command python\tget-pip.py\tto\tinstall\tpip\tIf you are looking for more information, check out the StackOverflow page that seems to be regularly updated.27                                                         26\tLakshmanan\tGanapathy,\t’15\tPractical\tLinux\tcurl\tCommand\tExamples\t(curl\tDownload\tExamples)’\tThe\tGeek\tStuff\t(11\tApril\t2012):\thttp://www.thegeekstuff.com/2012/04/curl-examples/\t27\t‘How\tdo\tI\tinstall\tpip\ton\tWindows?’\tStack\tOverflow:\thttp://stackoverflow.com/questions/4750806/how-do-i-install-pip-on-windows\t \n 35 Installing Python Modules Now that you have pip, it is easy to install python modules since it does all the work for you. When you find a module that you want to use, usually the documentation or installation instructions will include the necessary pip command, such as pip\tinstall\trequests\tpip\tinstall\tbeautifulsoup4\tpip\tinstall\tsimplekml\tRemember, for the same reasons explained above, you will probably need to run pip with sudo, like sudo\tpip\tinstall\trequests\tHappy installing! About the Author Fred Gibbs is an assistant professor of history at the University of New Mexico.  \n 36 Part Two: Acquiring Data After your machine is set up, often the first stage of a digital humanities project involves getting the data – the stuff – that you will be working with. Increasingly the web is littered with juicy data that could become the basis of a great project. Having the skills to acquire that data efficiently can save you tremendous amounts of time. The seven lessons in this first section provide various ways of extracting targeted information from the web and saving it to your computer.  \n 37 6. Downloading Web Pages with Python William J. Turkel and Adam Crymble – 2012    Editor’s Note: This lesson was originally written as part of a series of introduction to Python lessons. You may find it easier to complete this lesson if you have already competed the previous lesson in this series: ‘Code Reuse and Modularity in Python’.28  Lesson Goals This lesson introduces Uniform Resource Locators (URLs) and explains how to use Python to download and save the contents of a web page to your local hard drive. About URLs A web page is a file that is stored on another computer, a machine known as a web server. When you \"go to\" a web page, what is actually happening is that your computer, the client, sends a request to the server (the host) out over the network, and the server replies by sending a copy of the page back to your machine. One way to get to a web page with your browser is to follow a link from somewhere else. You also have the ability, of course, to paste or type a Uniform Resource Locator (URL) directly into your browser. The URL tells your browser where to find an online resource by specifying the server, directory and name of the file to be retrieved, as well as the kind of protocol that the server and your browser will agree to use while exchanging information (like HTTP, the Hypertext Transfer Protocol). The basic structure of a URL is protocol://host:port/path?query\tLet's look at a few examples. http://oldbaileyonline.org\tThe most basic kind of URL simply specifies the protocol and host. If you give this URL to your browser, it will return the main page of The Old                                                         28\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Code\tReuse\tand\tModularity\tin\tPython’\tThe\tProgramming\tHistorian\t(2012).\t \n 38 Bailey Online website.29 The default assumption is that the main page in a given directory will be named index, usually index.html. The URL can also include an optional port number. Without getting into too much detail at this point, the network protocol that underlies the exchange of information on the Internet allows computers to connect in different ways. Port numbers are used to distinguish these different kinds of connection. Since the default port for HTTP is 80, the following URL is equivalent to the previous one. http://oldbaileyonline.org:80\tAs you know, there are usually many web pages on a given website. These are stored in directories on the server, and you can specify the path to a particular page. The \"About\" page for The Old Bailey Online has the following URL. http://oldbaileyonline.org/static/Project.jsp\tFinally, some web pages allow you to enter queries. The Old Bailey Online website, for example, is laid out in such a way that you can request a particular page within it by using a query string. The following URL will take you to a search results page for criminal record trials containing the word \"arsenic\". http://www.oldbaileyonline.org/search.jsp?form=custom&_divs_fulltext=arsenic\tThe snippet after the \"?\" represents the query. You can learn more about building queries in Downloading Multiple Records Using Query Strings.30 Opening URLs with Python As a digital historian you will often find yourself wanting to use data held in scholarly databases online. To get this data you could open URLs one at a time and copy and paste their contents to a text file, or you can use Python to automatically harvest and process webpages. To do this, you're going to need to be able to open URLs with your own programs. The Python language includes a number of standard ways to do this. As an example, let's work with the kind of file that you might encounter while doing historical research. Say you're interested in race relations in eighteenth century Britain. The Old Bailey Online (OBO) is a rich resource that provides trial transcripts from 1674 to 1913 and is one good place to seek sources.                                                         29\tTim\tHitchcock,\tRobert\tShoemaker,\tClive\tEmsley,\tSharon\tHoward\tand\tJamie\tMcLaughlin,\tet\tal.,\tThe\tOld\tBailey\tProceedings\tOnline,\t1674-1913\t(www.oldbaileyonline.org,\tversion\t7.0,\t24\tMarch\t2012).\t30\tAdam\tCrymble,\t‘Downloading\tMultiple\tRecords\tUsing\tQuery\tStrings’,\tThe\tProgramming\tHistorian\t(2012).\t \n 39 \n The Old Bailey Online Homepage  For this example, we will be using the trial transcript of Benjamin Bowsey, a “black moor” who was convicted of breaking the peace during the Gordon Riots of 1780.31 The URL for the entry is http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&div=t17800628-33\tBy studying the URL we can learn a few things. First, The OBO is written in JSP (JavaServer Pages, a web programming language which outputs HTML), and it’s possible to retrieve individual trial entries by making use of the query string. Each is apparently given a unique ID number (id=t in the URL), built from the date of the trial session in the format (YYYYMMDD) and the trial number from within that court session, in this case: 33. If you change the two instances of 33 to 34 in your browser and press Enter, you should be taken to the next trial. Unfortunately, not all websites have such readable and reliable URLs. \n Trial Account of Benjamin Bowsey, 1780                                                         31\t‘Gordon\tRiots’\tWikipedia:\thttps://en.wikipedia.org/wiki/Gordon_Riots\t\n \n 40  Spend a few minutes looking at Benjamin Bowsey’s trial page. Here we are not so much interested in what the transcript says, but what features the page has. Notice the \"View as XML\" link at the bottom that takes you to a heavily marked up version of the text which may be useful to certain types of research. You can look at a scan of the original document, which was transcribed to make this resource.32 And you can access a \"Print-friendly version\" by clicking a link near the top of the entry. When you are processing web resources automatically, it is often a good idea to work with printable versions if you can, as they tend to have less formatting. Since we have that option, we will use the printable version in this lesson. http://www.oldbaileyonline.org/print.jsp?div=t17800628-33\tNow let's try opening the page using Python. Copy the following program into Komodo Edit and save it as open-webpage.py. When you execute the program, it will open the trial file, read its contents into a Python string called webContent and then print the first three hundred characters of the string to the \"Command Output\" pane. Use the View\t->\tWeb\tDeveloper\t->\tView\tPage\tSource command in Firefox to verify that the HTML source of the page is the same as the source that your program retrieved. Each browser has a different shortcut key to open the page source. In Firefox on PC it is CTRL+u. If you cannot find it on your browser, try using a search engine to find where it is. (See the Python library reference to learn more about urllib2.)33 #\topen-webpage.py\t\timport\turllib2\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\twebContent\t=\tresponse.read()\t\tprint\twebContent[0:300]\tThese five lines of code achieve an awful lot very quickly. Let us take a moment to make sure that everything is clear and that you can recognize the building blocks that allow us to make this program do what we want it to do. url, response, and webContent are all variables that we have named ourselves.                                                         32\t‘Page\tImage:\tProceedings\tof\tthe\tOld\tBailey,\t28th\tJune\t1780,\tpage\t84.’\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/images.jsp?doc=178006280084\t33\t‘urllib2’\tPython:\thttps://docs.python.org/2/library/urllib2.html\t \n 41 url holds the URL of the web page that we want to download. In this case, it is the trial of Benjamin Bowsey. On the following line, we call the function urlopen, which is stored in a Python module named urllib2.py, and we have asked that function to open the website found at the URL we just specified. We then saved the result of that process into a variable named response. That variable now contains an open version of the requested website. We then use the read method, which we used earlier, to copy the contents of that open webpage into a new variable named webContent. Make sure you can pick out the variables (there are 3 of them), the modules (1), the methods (2), and the parameters (1) before you move on. In the resulting output, you will notice a little bit of HTML markup: \t<!--\tMAIN\tCONTENT\t-->\t\t<div\tid=\"main\"\tclass=\"full\"><div\tid=\"main2\">\t\t\t\t\t\t\t\t\t<div\tstyle=\"font-family:serif;\"><i>Old\tBailey\tProceedings\tOnline</i>\t(www.oldbaileyonline.org,\tversion\t6.0,\t16\tMarch\t2013),\tJune\t1780,\ttrial\tof\tBENJAMIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tBOWSEY\tBecause we are using the printable version there is a lot less than most web pages have, but there is still more than we need. Don't worry; you will soon learn how to remove that excess markup. Saving a Local Copy of a Web Page Given what you already know about writing to files, it is quite easy to modify the above program so that it writes the contents of the webContent string to a local file on our computer rather than to the \"Command Output\" pane. Copy the following program into Komodo Edit, save it as save-webpage.py and execute it. Using the File\t->\tOpen\tFile command in Firefox, open the file on your hard drive that it creates (obo-t17800628-33.html) to confirm that your saved copy is the same as the online copy. #\tsave-webpage.py\t\timport\turllib2\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\twebContent\t=\tresponse.read()\t\tf\t=\topen('obo-t17800628-33.html',\t'w')\tf.write(webContent)\tf.close\tSo, if you can save a single file this easily, could you write a program to download a bunch of files? Could you step through trial IDs, for example,  \n 42 and make your own copies of a whole bunch of them? Yep. We'll get there soon. Suggested Readings Lutz, Mark. “Ch. 4: Introducing Python Object Types”, Learning Python (O’Reilly, 1999). Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each lesson you can download the \"programming-historian\" zip file to make sure you have the correct code. programming-historian-1 (zip): programminghistorian.org/assets/programming-historian1.zip  If you are following the learning Python lessons, the next tutorial in this series is ‘Manipulating Strings in Python’.34  About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.   \n                                                        34\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Manipulating\tStrings\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 43 7. Automated Downloading with Wget Ian Milligan – 2012   Editor's Note This lesson requires you to use the command line. If you have no previous experience using the command line you may find it helpful to work through the Scholar's Lab Command Line Bootcamp tutorial.35 Lesson Goals This is a lesson designed for intermediate users, although beginner users should be able to follow along. Wget is a useful program, run through your computer's command line, for retrieving online material. \nThe Mac Command Line, Terminal It can be useful in the following situations: Retrieving or mirroring (creating an exact copy of) an entire website. This website might contain historical documents, or it may simply be your own                                                         35\t‘The\tCommand\tLine’\tThe\tPraxis\tProgram\tat\tthe\tScholar’s\tLab:\thttp://praxis.scholarslab.org/scratchpad/bash/\t\n \n 44 personal website that you want to back up. One command can download the entire site onto your computer. Downloading specific files in a website's hierarchy (all websites within a certain part of a website, such as every page that is contained within the /papers/ directory of a website). In this lesson, we will work through three quick examples of how you might use wget in your own work. At the end of the lesson, you will be able to quickly download large amounts of information from the Internet in an automated fashion. If you find a repository of online historical information, instead of right-clicking on every file and saving it to build your dataset, you will have the skills to craft a single command to do so. First, a caution is in order. You need to be careful about how you use wget. If you consult the manual when in doubt, and work through the lessons here, you should be okay. You should always build a delay into your commands so that you do not overload the servers, and should also always put a limit on the speed to which you download. This is all part of being a good Internet citizen, and can be seen as analogous to sipping from a firehose rather than turning it on all at once (it's not good for you, or the water company). Be as specific as possible when formulating your download. One joke suggests that you can accidentally download the entire Internet with wget. While that's a bit of an exaggeration, it isn't too far off! Let's begin. Step One: Installation Linux Instructions If you are using a Linux system, you should already have wget installed. To check if you have it, open up your command line. Type 'wget' and press enter. If you have wget installed the system will respond with: ->\tMissing\tURL.\tIf you do not have wget installed, it will respond with ->\tcommand\tnot\tfound.\tIf you are on OS X or Windows, you will need to download the program. If on Linux, you receive the error message indicating that you do not have wget installed, follow the OS X instructions below.  \n 45 OS X Instructions OS X Option One: The Preferred Method On OS X, there are two ways to get wget and install it. The easiest is to install a package manager and use it to automatically install wget. There is a second method, discussed below, that involves compiling it. Both, however, require that you install Apple's 'Command Line Tools' to use properly. This requires downloading XCode. If you have the 'App Store', you should be able to just download XCode.36  If not, the following instructions will work. To download this, go to the Apple Developer website,37 register as a developer, and then in the downloads for Apple developers section you will need to find the correct version. It is a big download, and will take some time. Once you have the file, install it. You will need to install the 'Command Line Tools' kit in XCode. Open up the 'Preferences' tab, click on 'Downloads,' and then click 'Install' next to Command Line Tools. We are now ready to install a package manager. The easiest package manager to install is Homebrew. Go to http://mxcl.github.io/homebrew/ and review the instructions. There are many important commands, like wget, that are not included by default in OS X. This program facilitates the downloading and installation of all required files. To install Homebrew, open up your terminal window and type the following: ruby\t-e\t\"$(curl\t-fsSL\thttps://raw.github.com/mxcl/homebrew/go)\"\tThis uses the ruby programming language, built into OS X, to install Homebrew. To see if the installation worked, type the following into your terminal window: brew\tA list of documentation options should appear if it has been installed. We have one more command to run to make sure everything is working, which is: brew\tdoctor\tWith Homebrew installed, we now have to install wget. This is now an easy step.                                                         36\t‘Xcode’\tApp\tStore:\thttps://itunes.apple.com/en/app/xcode/id497799835?mt=12\t37\t‘Xcode’\tApple\tDeveloper:\thttps://developer.apple.com/xcode/\t \n 46 brew\tinstall\twget\tIt will proceed to download the most recent version of wget, which is wget 1.14. After the script stops running, and you are back to your main window, enter the following command into the terminal: wget\tIf you have installed it, you will see: ->\tMissing\tURL.\tIf not, you will see: ->\tcommand\tnot\tfound.\tAt this point, you should have installed wget successfully. We are now ready to keep going! OS X Option Two If for some reason you do not want to install a package manager, you are able to simply download wget alone. This will be applicable if you are using a different packet manager (such as Mac Ports) or if you want to keep your infrastructure to a minimum. Follow the same instructions again to install xcode and the Command Line Tools set. Then you can subsequently download an uncompiled version of wget from the GNU website38 (I chose to download the file 'wget-1.13.tar.gz', which you can find by following the link to either the HTTP or FTP39 download pages), unzip it (by double-clicking on it) into your home directory (on a Mac, this will be your /user/ directory – for example, my user name is ianmilligan and it appears next to a house icon in my Finder), and then open up Terminal. For this tutorial, we have downloaded wget-1.13. First, we will need to navigate to the directory that the wget files are in. At the terminal, type: cd\twget-1.13\tNote that if you have downloaded a different version of wget, the following steps will work but you may have to replace the above version number (i.e. 1.13) with your own. We now need to generate the instructions, or makefile, for the file. This is sort of a blueprint for what the final file is going to look like. Accordingly, type: ./configure\t–with-ssl=openssl\t                                                        38\t‘GNU\tWget’\tGNU\tOperating\tSystem:\thttp://www.gnu.org/software/wget/\t39\t‘GNU\tWget\t–\tftp\tGNU\tOperating\tSystem:\thttp://ftp.gnu.org/gnu/wget/;\tOR\tftp://ftp.gnu.org/gnu/wget/\t \n 47 Now that we have the blueprints, let\\'s tell our computer to follow them. Type: make\tThen, you need to make the final file. By pre-pending the command sudo, you are running the command with highest security privileges. This lets you actually install the file into your system. sudo\tmake\tinstall\tAt this point, you will be prompted for your computer's password. Type it. You should now have wget installed. Windows Instructions The easiest way is to download a working version. To do so, visit the eternallybored website40 and, download wget.exe (as of writing it is version 1.16.3, and you should download the 32-bit binary). If you place wget.exe in your C:Windows directory, you can then use wget from anywhere on your computer. This will make your life easier as you will not have to worry about always running wget from only one place on your system. If it is in this directory, Windows will know that the command can be used anywhere in your terminal window. Step Two: Learning about the Structure of Wget – Downloading a Specific Set of Files At this point, users of all three platforms should be on the same page. We use wget through our operating system's command line interface (introduced previously as Terminal for Mac and Linux users, where you have been playing around with some Python commands). You need to use your command line, instead of the Komodo Edit client you may have used in other lessons. The comprehensive documentation for wget can be found on the GNU wget manual page.41 Let's take an example dataset. Say you wanted to download all of the papers hosted on the website ActiveHistory.ca. They are all located at: http://activehistory.ca/papers/; in the sense that they are all contained within the /papers/ directory: for example, the 9th paper published on the website is http://activehistory.ca/papers/historypaper-9/. Think of this structure in the same way as directories on your own computer: if you have a folder labeled /History/, it likely contains several files within it. The same structure holds true for websites, and we are using this logic to tell our computer what files we want to download.                                                         40\t‘Windows\tbinaries\tof\tGNU\tWget’\tEternally\tBored:\thttps://eternallybored.org/misc/wget/\t41\t‘GNU\tWget\t1.17.1\tManual’\tGNU:\thttp://www.gnu.org/software/wget/manual/wget.html\t \n 48 If you wanted to download them all manually, you would either need to write a custom program, or right-click every single paper to do so. If the files are organized in a way that fits your research needs, wget is the quickest approach. To make sure wget is working, try the following. In your working directory, make a new directory. Let's call it wget-activehistory. You can make this using your Finder/Windows, or if you are at a Terminal window at that path, you can type: mkdir\twget-activehistory\tEither way, you now have a directory that we will be working in. Now open up your command line interface and navigate to the wget-activehistory directory. As a reminder, you can type: cd\t[directory]\tto navigate to a given directory. If you've made this directory in your home directory, you should be able to type cd\twget-activehistory to move to your new directory. Enter the following command: wget\thttp://activehistory.ca/papers/\tAfter some initial messages, you should see the following (figures, dates and some details will be different, however): Saving\tto:\t`index.html.1'\t\t[]\t37,668\t--.-K/s\tin\t0.1s\t\t2012-05-15\t15:50:26\t(374\tKB/s)\t-\t`index.html.1'\tsaved\t[37668]\tWhat you have done is downloaded just the first page of http://activehistory.ca/papers/, the index page for the papers to your new directory. If you open it, you'll see the main text on the home page of ActiveHistory.ca. So at a glance, we have already quickly downloaded something. What we want to do now, however, is to download every paper. So we need to add a few commands to wget. Wget operates on the following general basis: wget\t[options]\t[URL]\tWe have just learned about the [URL] component in the previous example, as it tells the program where to go. Options, however, give the program a bit more information about what exactly we want to do. The program knows that an option is an option by the presence of a dash before the  \n 49 variable. This lets it know the difference between the URL and the options. So let's learn a few commands now: -r\tRecursive retrieval is the most important part of wget. What this means is that the program begins following links from the website and downloading them too. So for example, the http://activehistory.ca/papers/ has a link to http://activehistory.ca/papers/historypaper-9/, so it will download that too if we use recursive retrieval. However, it will also follow any other links: if there was a link to http://uwo.ca somewhere on that page, it would follow that and download it as well. By default, -r sends wget to a depth of five sites after the first one. This is following links, to a limit of five clicks after the first website. At this point, it will be quite indiscriminate. So we need more commands: --no-parent\t\t(The double-dash indicates the full-text of a command. All commands also have a short version, this could be initiated using -np). This is an important one. What this means is that wget should follow links, but not beyond the last parent directory. In our case, that means that it won't go anywhere that is not part of the http://activehistory.ca/papers/ hierarchy. If it was a long path such as: http://niche-canada.org/projects/events/new-events/not-yet-happened-events/ It would only find files in the /not-yet-happened-events/ folder. It is a critical command for delineating your search. Here is a graphical representation: \n A graphical representation of how 'no-parent' works with wget Finally, if you do want to go outside of a hierarchy, it is best to be specific about how far you want to go. The default is to follow each link and carry \n \n 50 on to a limit of five pages away from the first page you provide. However, perhaps you just want to follow one link and stop there? In that case, you could input -l\t2, which takes us to a depth of two web-pages. Note this is a lower-case 'L', not a number 1. -l\t2\tIf these commands help direct wget, we also need to add a few more to be nice to servers and to stop any automated countermeasures from thinking the server is under attack! To that end, we have two additional essential commands: -w\t10\tIt is not polite to ask for too much at once from a web server. There are other people waiting for information, too, and it is thus important to share the load. The command -w\t10, then, adds a ten second wait in between server requests. You can shorten this, as ten seconds is quite long. In my own searches, I often use a 2 second wait. On rare occasions, you may come across a site that blocks automated downloading altogether. The website's terms of service, which you should consult, may not mention a policy on automated downloading, but steps to prohibit it may be built into their website's architecture nonetheless. In such rare cases, you can use the command ––random-wait which will vary the wait by 0.5 and 1.5 times the value you provide here. Another critical comment is to limit the bandwidth you will be using in the download: --limit-rate=20k\tThis is another important, polite command. You don't want to use up too much of the servers' bandwidth. So this command will limit the maximum download speed to 20kb/s. Opinion varies on what a good limit rate is, but you are probably good up to about 200kb/s for small files – however, not to tax the server, let us keep it at 20k. This will also keep us at ActiveHistory.ca happy! Step Three: Mirror an Entire Website Ok, with all of this, let's finally download all of the ActiveHistory.ca papers. Note that the trailing slash on the URL is critical – if you omit it, wget will think that papers is a file rather than a directory. Directories end in slashes. Files do not. The command will then download the entire ActiveHistory.ca page. The order of the options does not matter. wget\t-r\t--no-parent\t-w\t2\t--limit-rate=20k\thttp://activehistory.ca/papers/\tIt will be slower than before, but your terminal will begin downloading all of the ActiveHistory.ca papers. When it is done, you should have a directory labeled ActiveHistory.ca that contains the /papers/ sub-directory –  \n 51 perfectly mirrored on your system. This directory will appear in the location that you ran the command from in your command line, so likely is in your USER directory. Links will be replaced with internal links to the other pages you've downloaded, so you can actually have a fully working ActiveHistory.ca site on your computer. This lets you start to play with it without worrying about your internet speed. To see if the download was a success, you will also have a log in your command screen. Take a look over it to make sure that all files were downloaded successfully. If it did not download, it will let you know that it failed. If you want to mirror an entire website, there is a built-in command to wget. -m\tThis command means 'mirror,' and is especially useful for backing up an entire website. It introduces the following set of commands: time-stamping, which looks at the date of the site and doesn't replace it if you already have that version on your system (useful for repeated downloads), as well as infinite recursion (it will go as many layers into the site as necessary). The command for mirroring ActiveHistory.ca would be: wget\t-m\t-w\t2\t--limit-rate=20k\thttp://activehistory.ca\tA Flexible Tool for Downloading Internet Sources As you become increasingly comfortable with the command line, you'll find wget a helpful addition to your digital toolkit. If there is an entire set of archival documents that you want to download for text mining, if they're arranged in a directory and are all together (which is not as common as one might think), a quick wget command will be quicker than scraping the links with Python. Similarly, you can then begin downloading things directly from your command line: programs, files, backups, etc. Further Reading I've only given a snapshot of some of wget's functionalities. For more, please visit the wget manual.42 About the Author Ian Milligan is an assistant professor of history at the University of Waterloo.                                                          42\t‘GNU\tWget\t1.17.1\tManual’\tGNU:\thttp://www.gnu.org/software/wget/manual/wget.html\t \n 52 8. Applied Archival Downloading with Wget Kellen Kurchinski – 2013   Editor’s Note: you may find it easier to complete this lesson if you have already completed the previous ‘wget’ tutorial by Ian Milligan.  Background and Lesson Goals Now that you have learned how Wget can be used to mirror or download specific files from websites like ActiveHistory.ca via the command line, it's time to expand your web-scraping skills through a few more lessons that focus on other uses for Wget's recursive retrieval function. The following tutorial provides three examples of how Wget can be used to download large collections of documents from archival websites with assistance from the Python programing language. It will teach you how to parse and generate a list of URLs using a simple Python script, and will also introduce you to a few of Wget's other useful features. Similar functions to the ones demonstrated in this lesson can be achieved using curl,43 an open-source software capable of performing automated downloads from the command line. For this lesson, however, we will focus on Wget and building your Python skills. Archival websites offer a wealth of resources to historians, but increased accessibility does not always translate into increased utility. In other words, while online collections often allow historians to access hitherto unavailable or cost-prohibitive materials, they can also be limited by the manner in which content is presented and organized. Take for example the Indian Affairs Annual Reports database44 hosted on the Library and Archives Canada [LAC] website. Say you wanted to download an entire report, or reports for several decades. The current system allows a user the option to read a plaintext version of each page, or click on the \"View a scanned page of original Report\" link, which will take the user to a page with LAC's embedded image viewer. This allows you to see the original document, but it is also cumbersome because it requires you to scroll through each individual page. Moreover, if you want the document for offline viewing, the only option is to right click –> save as each image to a                                                         43\tKonrad\tM.\tLawson,\t‘Download\ta\tSequential\tRange\tof\tURLs\twith\tCurl’,\tProfHacker\t(2012):\thttp://chronicle.com/blogs/profhacker/download-a-sequential-range-of-urls-with-curl/41055\t44\t‘Indian\tAffairs\tAnnual\tReports,\t1864-1990’\tLibrary\tand\tArchives\tCanada:\thttp://www.bac-lac.gc.ca/eng/discover/aboriginal-heritage/first-nations/indian-affairs-annual-reports/Pages/introduction.aspx\t \n 53 directory on your computer. If you want several decades' worth of annual reports, you can see the limits to the current means of presentation pretty easily. This lesson will allow you to overcome such an obstacle. Recursive Retrieval and Sequential URLs: The Library and Archives Canada Example Let's get started. The first step involves building a script to generate sequential URLs using Python's ForLoop function. First, you'll need to identify the beginning URL in the series of documents that you want to download. Because of its smaller size we're going to use the online war diary for No. 14 Canadian General Hospital as our example.45 The entire war diary is 80 pages long. The URL for page 1 is http://data2.archives.ca/e/e061/e001518029.jpg and the URL for page 80 is http://data2.archives.ca/e/e061/e001518109.jpg. Note that they are in sequential order. We want to download the .jpeg images for all of the pages in the diary. To do this, we need to design a script to generate all of the URLs for the pages in between (and including) the first and last page of the diary. Open your preferred text editor (such as Komodo Edit)46 and enter the code below. Where it says 'integer 1′ type in '8029′, where it says 'integer 2′, type '8110'. The For Loop will generate a list of numbers between '8029' and '8110', but it will not print the last number in the range (i.e. 8110). To download all 80 pages in the diary you must add one to the top-value of the range because it is at this integer where the ForLoop is told to stop. This applies for any sequence of numbers you generate with this function. Additionally, the script will not properly execute if leading zeros47 are included in the range of integers, so you must exclude them by leaving them in the string (the URL). In this example I have parsed the URL so that only the last four digits of the string are being manipulated by the ForLoop. #URL-Generator.py\t\turls\t=\t'';\tf=open('urls.txt','w')\tfor\tx\tin\trange('integer1',\t'integer2'):\t\t\t\t\turls\t=\t'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n'\t%\t(x)\t\t\t\t\tf.write(urls)\tf.close\tNow replace 'integer1′ and 'integer2′ with the bottom and top ranges of URLs  you want to download. The final product should look like this:                                                         45\t‘War\tdiaries\t–\t14th\tCanadian\tGeneral\tHospital’\tLibrary\tand\tArchives\tCanada:\thttp://collectionscanada.gc.ca/pam_archives/index.php?fuseaction=genitem.displayItem&lang=eng&rec_nbr=2005110&rec_nbr_list=3366167,3203123,2005097,2005100,2005101,2005099,2005096,2005110,2005108,2005106\t46\t‘Komodo\tEdit’\tKomodo\tIDE:\thttp://komodoide.com/komodo-edit/\t47\t‘Leading\tzero’\tWikipedia:\thttps://en.wikipedia.org/wiki/Leading_zero\t \n 54 #URL-Generator.py\t\turls\t=\t'';\tf=open('urls.txt','w')\tfor\tx\tin\trange(8029,\t8110):\t\t\t\t\turls\t=\t'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n'\t%\t(x)\t\t\t\t\tf.write(urls)\tf.close\tSave the program as a .py file, and then click run the Python script. The ForLoop will automatically generate a sequential list of URLs between the range of two integers that you specified in the brackets, and will write them to a .txt file that will be saved in your Programming Historian directory. The %d appends each sequential number generated by the ForLoop to the exact position you place it in the string. Adding \\n to the end of the string removes line-breaks, allowing Wget to read the .txt file. You do not need to use all of the digits in the URL to specify the range – just the ones between the beginning and end of the sequence you are interested in. This is why only the last 4 digits of the string were selected and 00151 was left intact. Before moving on to the next stage of the downloading process, make sure you have created a directory where you would like to save your files, and, for ease of use, locate it in the main directory where you keep your documents. For both Mac and Windows users this will normally be the 'Documents' folder. For this example, we'll call our folder 'LAC'. You should move the urls.txt file your Python script created in to this directory.  To save time on future downloads, it is advisable to simply run the program from the directory you plan to download to. This can be achieved by saving the URL-Generator.py file to your 'LAC' folder. For Mac users, under your applications list, select Utilities -> Terminal. For Windows Users, you will need to open your system's Command Line utility. Once you have a shell open, you need to 'call' the directory you want to save your downloaded .jpeg files to. Type: cd\t~/Documents\tand hit enter. Then type: cd\t'LAC'\tand press enter again. You now have the directory selected and are ready to begin downloading. Based on what you have learned from Ian Milligan's Wget lesson,48 enter the following into the command line (note you can choose whatever you like                                                         48\tIan\tMilligan,\t‘Automated\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2012).\t \n 55 for your 'limit rate', but be a responsible internet citizen and keep it under 200kb/s!): wget\t-i\turls.txt\t-r\t--no-parent\t-nd\t-w\t2\t--limit-rate=100k\t(Note: including '-nd' in the command line will keep Wget from automatically mirroring the website's directories, making your files easier to access and organize). Within a few moments you should have all 80 pages of the war diary downloaded to this directory. You can copy and move them into a new folder as you please. A Second Example: The National Archives of Australia Let's try one more example using this method of recursive retrieval. This lesson can be broadly applied to numerous archives, not just Canadian ones! Say you wanted to download a manuscript from the National Archives of Australia, which has a much more aesthetically pleasing online viewer than LAC, but is still limited by only being able to scroll through one image at a time. We'll use William Bligh's \"Notebook and List of Mutineers, 1789\" which provides an account of the mutiny aboard the HMS Bounty.49 On the viewer page (see previous footnote) you'll note that there are 131 'items' (pages) to the notebook. This is somewhat misleading. Click on the first thumbnail in the top right to view the whole page. Now, right-click -> view image. The URL should be 'http://nla.gov.au/nla.ms-ms5393-1-s1-v.jpg'. If you browse through the thumbnails, the last one is 'Part 127', which is located at 'http://nla.gov.au/nla.ms-ms5393-1-s127-v.jpg'. The discrepancy between the range of URLs and the total number of files means that you may miss a page or two in the automated download – in this case there are a few URLs that include a letter in the name of the .jpeg ('s126a.v.jpg' or 's126b.v.jpg' for example). This is going to happen from time to time when downloading from archives, so do not be surprised if you miss a page or two during an automated download. Note that a potential workaround could include using regular expressions to make more complicated queries if appropriate (for more, see the Understanding Regular Expressions lesson by Doug Knox).50 Let's run the script and Wget command once more: \t\t                                                        49\t‘MS\t5393\tNotbook\tand\tlist\tof\tmutineers,\t1789\t[manuscript]’,\tDigital\tCollections\tManuscripts:\thttp://www.nla.gov.au/apps/cdview/?pi=nla.ms-ms5393-1\t50\tDoug\tKnox,\t‘Understanding\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t \n 56 #Bligh.py\t\turls\t=\t'';\tf=open('urls.txt','w')\tfor\tx\tin\trange(1,\t128):\t\t\t\t\turls\t=\t'http://www.nla.gov.au/apps/cdview/?pi=nla.ms-ms5393-1-s%d-v.jpg\\n'\t%\t(x)\t\t\t\t\tf.write(urls)\tf.close\tAnd: wget\t-i\turls.txt\t-r\t--no-parent\t-nd\t-w\t2\t--limit-rate=100k\tYou now have a (mostly) full copy of William Bligh's notebook. The missing pages can be downloaded manually using right-click -> save image as. Recursive Retrieval and Wget's 'Accept' (-A) Function Sometimes automated downloading requires working around coding barriers. It is common to encounter URLs that contain multiple sets of leading zeros, or URLs which may be too complex for someone with a limited background in coding to design a Python script for. Thankfully, Wget has a built-in function called 'Accept' (expressed as '-A') that allows you to define what type of files you would like to download from a specific webpage or an open directory. For this example we will use one of the many great collections available through the Library of Congress website: The Thomas Jefferson Papers. As with LAC, the viewer for these files is outdated and requires you to navigate page by page. We're going to download a selection from Series 1: General Correspondence. 1651-1827.51 Open the link and then click on the image (the .jpeg viewer looks awful familiar doesn't it?) The URL for the image also follows a similar pattern to the war diary from LAC that we downloaded earlier in the lesson, but the leading zeros complicate matters and do not permit us to easily generate URLs with the first script we used. Here's a workaround. Look at this webpage: http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/ The page you just opened is a sub-directory of the website that lists the .jpeg files for a selection of the Jefferson Papers. This means that we can use Wget's '–A' function to download all of the .jpeg images (100 of them) listed on that page. But say you want to go further and download the whole range of files for this set of dates in Series 1 – that's 1487 images. For a task like this where there are relatively few URLs you do not actually need                                                         51\t‘The\tThomas\tJefferson\tPapers\tSeries\t1.\tGeneral\tCorrespondence.\t1651-1827’\tThe\tLibrary\tof\tCongress:\thttp://memory.loc.gov/cgi-bin/ampage?collId=mtj1&fileName=mtj1page001.db&recNum=1&itemLink=/ammem/collections/jefferson_papers/mtjser1.html&linkText=6\t \n 57 to write a script (although you could using my final example, which discusses the problem of leading zeros). Instead, simply manipulate the URLs in a .txt file as follows: http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/ http://memory.loc.gov/master/mss/mtj/mtj1/001/0100/ http://memory.loc.gov/master/mss/mtj/mtj1/001/0200/ ... all the way up to http://memory.loc.gov/master/mss/mtj/mtj1/001/1400 This is the last sub-directory on the Library of Congress site for these dates in Series 1. This last URL contains images 1400-1487. Your completed .txt file should have 15 URLs total. Before going any further, save the file as 'Jefferson.txt' in the directory you plan to store your downloaded files in. Now, run the following Wget command: wget\t–i\tJefferson.txt\t–r\t--no-parent\t-nd\t–w\t2\t–A\t.jpg,\t.jpeg\t--limit-rate=100k\tVoila, after a bit of waiting, you will have 1487 pages of presidential papers right at your fingertips! More Complicated Recursive Retrieval: A Python Script for Leading Zeros The Library of Congress, like many online repositories, organizes their collections using a numbering system that incorporates leading zeros within each URL. If the directory is open, Wget's –A function is a great way to get around this without having to do any coding. But what if the directory is closed and you can only access one image at a time? This final example will illustrate how to use a Python script to incorporate leading into a list of URLs. For this example we will be using the Historical Medical Poster Collection,52 available from the Harvey Cushing/Jack Hay Whitney Medical Library (Yale University). First, we'll need to identify the URL of the first and last files we want to download. We also want the high-resolution versions of each poster. To locate the URL for the high res image click on the first thumbnail (top left) then look below the poster for the link that says 'Click HERE for Full Image'. If you follow the link, a high-resolution image with a complex URL will appear. As was the case in the Australian Archives example, to get the                                                         52\t‘Historical\tMedical\tPoster\tCollection’,\tHarvey\tCushing/John\tHay\tWhitney\tMedical\tLibrary:\thttp://cushing.med.yale.edu/gsdl/collect/mdposter/\t \n 58 simplified URL you must right-click -> view image using your web-browser. The URL for the first poster should be: http://cushing.med.yale.edu/images/mdposter/full/poster0001.jpg Follow the same steps for the last poster in the gallery – the URL should be: http://cushing.med.yale.edu/images/mdposter/full/poster0637.jpg. The script we used to download from LAC will not work because the range function cannot comprehend leading zeros. The script below provides an effective workaround that runs three different For Loops and exports the URLs to a .txt file in much the same way as our original script. This approach would also work with the Jefferson Papers, but I chose to use the –A function to demonstrate its utility and effectiveness as a less complicated alternative. In this script the poster URL is treated in much the same way as the URL in our LAC example. The key difference is that the leading zeros are included as part of the string. For each loop, the number of zeros in the string decreases as the digits increase from single, to double, to triple. The script can be expanded or shortened as needed. In this case we needed to repeat the process three times because we were moving from three leading zeros to one leading zero. To ensure that the script iterates properly, a '+' should be added to each For Loop as in the example below. We do not recommend actually performing this download because of the size and extent of the files. This example is merely intended to illustrate the how to build and execute the Python script. #Leading-Zeros.py\t\turls\t=\t'';\tf=open('leading-zeros.txt','w')\t\tfor\tx\tin\trange(1,10):\t\t\t\t\turls\t+=\t'http://cushing.med.yale.edu/images/mdposter/full/poster000%d.jpg\\n'\t%\t(x)\t\tfor\ty\tin\trange(10,100):\t\t\t\t\turls\t+=\t'http://cushing.med.yale.edu/images/mdposter/full/poster00%d.jpg\\n'\t%\t(y)\t\tfor\tz\tin\trange(100,638):\t\t\t\t\turls\t+=\t'http://cushing.med.yale.edu/images/mdposter/full/poster0%d.jpg\\n'\t%\t(z)\t\tf.write(urls)\tf.close\t \n 59 Conclusion These three examples only scratch the surface of Wget's potential. Digital archives organize, store, and present their content in a variety of ways, some of which are more accessible than others. Indeed, many digital repositories store files using URLs that must be manipulated in several different ways to utilize a program like Wget. Wherever your downloading may take you, new challenges and opportunities await. This tutorial has provided you with the core skills for further work in the digital archive and, hopefully, will lead you to undertake your own experiments in an effort to add new tools to the digital historian's toolkit. As new methods for scraping online repositories become available, we will continue to update this lesson with additional examples of Wget's power and potential. About the Author Kellen Kurchinski is a doctoral candidate in history at McMaster University and a Research Officer at the University of Waterloo.   \n 60 9. Downloading Multiple Records Using Query Strings Adam Crymble – 2012   Module Goals Downloading a single record from a website is easy, but downloading many records at a time – an increasingly frequent need for a historian – is much more efficient using a programming language such as Python. In this lesson, we will write a program that will download a series of records from the Old Bailey Online53 using custom search criteria, and save them to a directory on our computer. This process involves interpreting and manipulating URL Query Strings. In this case, the tutorial will seek to download sources that contain references to people of African descent that were published in the Old Bailey Proceedings between 1700 and 1750. For Whom is this Useful? Automating the process of downloading records from an online database will be useful for anyone who works with historical sources that are stored online in an orderly and accessible fashion and who wishes to save copies of those sources on their own computer. It is particularly useful for someone who wants to download many specific records, rather than just a handful. If you want to download all or most of the records in a particular database, you may find Ian Milligan’s tutorial on ‘Automated Downloading with WGET’ more suitable.54 The present tutorial will allow you to download discriminately, isolating specific records that meet your needs. Downloading multiple sources automatically saves considerable time. What you do with the downloaded sources depends on your research goals. You may wish to create visualizations or perform various data analysis methods, or simply reformat them to make browsing easier. Or, you may just want to keep a backup copy so you can access them without Internet access. This lesson is for intermediate Python users. If you have not already tried the Python Programming Basics lessons, you may find that a useful starting point.55                                                         53\tTim\tHitchcock,\tRobert\tShoemaker,\tClive\tEmsley,\tSharon\tHoward\tand\tJamie\tMcLaughlin,\tet\tal.,\tThe\tOld\tBailey\tProceedings\tOnline,\t1674-1913\t(www.oldbaileyonline.org,\tversion\t7.0,\t24\tMarch\t2012).\t54\tIan\tMilligan,\t‘Automated\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2012).\t55\tWilliam\tJ.\tTurkel\t&\tAdam\tCrymble,\t‘Python\tIntroduction\tand\tInstallation’,\tThe\tProgramming\tHistorian\t(2012).\t \n 61 Applying our Historical Knowledge In this lesson, we are trying to create our own corpus of cases related to people of African descent. From Benjamin Bowsey’s case56 at the Old Bailey in 1780, we might note that “black” can be a useful keyword for us to use for locating other cases involving defendants of African descent. However, when we search for “black” on the Old Bailey website, we find it often refers to other uses of the word: black horses, or black cloth. The task of disambiguating this use of language will have to wait for another lesson. For now, let’s turn to easier cases. As historians, we can probably think of keywords related to African descendants that would be worth pursuing. The infamous “n-word” of course is not useful, as this term did not come into regular usage until the mid-nineteenth century. “Negro” and “mulatto” are however, much more relevant to the early eighteenth century. These keywords are less ambiguous than “black” and are much more likely to be immediate references to people in our target demographic. If we try these two terms in separate simple searches on the Old Bailey website, we get results like in these screenshots: \n Search results for 'negro' in the Old Bailey Online \n                                                        56\tOld\tBailey\tProceedings\tOnline\t(www.oldbaileyonline.org,\tversion\t7.2,\t23\tFebruary\t2016),\tJune\t1780,\ttrial\tof\tBENJAMIN\tBOWSEY:\thttp://www.oldbaileyonline.org/browse.jsp?ref=t17800628-33.\t\n \n 62 \n Search results for 'mulatto' in the Old Bailey Online After glancing through these search results, it seems clear that these are references to people, rather than horses or cloth or other things that may be black. We want to download them all to use in our analysis. We could, of course, download them one at a time, manually. But let’s find a programmatic way to automate this task. The Advanced Search on OBO Every website’s search features work differently. While searches work similarly, the intricacies of database searches may not be entirely obvious. Therefore it’s important to think critically about database search options and, when available, read the documentation provided on the website. Prudent historical researchers always interrogate their sources; the procedures behind your search boxes should receive the same attention. The Old Bailey Online’s advanced search form lets you refine your searches based on ten different fields including simple keywords, a date range, and a crime type. As each website’s search feature is different it always pays to take a moment or two to play with and read about the search options available. In this case, read over the short explanation of the “Advanced” features by clicking on the “what’s this?” link, which will explain how to refine your search further. Since we have already done the simple searches for “negro” and “mulatto”, we know there will be results. However, let’s use the advanced search to limit our results to records published in the Old Bailey Proceedings trial accounts from 1700 to 1750 only. You can of course change this to whatever you like, but this will make the example easier to follow. Perform the search shown in the image below. Make sure you tick the “Advanced” radio button and include the * wildcards to include pluralized entries or those with an extra “e” on the end. \n \n 63 \n Old Bailey Advanced Search Example Execute the search and then click on the “Calculate Total” link to see how many entries there are. We now have 13 results (if you have a different number go back and make sure you copied the example above exactly). What we want to do at this point is download all of these trial documents and analyze them further. Again, for only 13 records, you might as well download each record manually. But as more and more data comes online, it becomes more common to need to download 1,300 or even 130,000 records, in which case downloading individual records becomes impractical and an understanding of how to automate the process becomes that much more valuable. To automate the download process, we need to step back and learn how the search URLs are created on the Old Bailey website, a method common to many online databases and websites. Understanding URL Queries Take a look at the URL produced with the last search results page. It should look like this: http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext=mulatto*+negro*&kwparse=advanced&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount&fromYear=1700&fromMonth=00&toYear=1750&toMonth=99&start=0&count=0\tWe had a look at URLs in ‘Understanding Web Pages and HTML’,57 but this looks a lot more complex. Although longer, it is actually not that much more complex. But it is easier to understand by noticing how our search criteria get represented in the URL.                                                         57\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’,\tThe\tProgramming\tHistorian\t(2012).\t\n \n 64 http://www.oldbaileyonline.org/search.jsp\t?foo=bar\t&form=searchHomePage\t&_divs_fulltext=mulatto*+negro*\t&kwparse=advanced\t&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount\t&fromYear=1700\t&fromMonth=00\t&toYear=1750\t&toMonth=99\t&start=0\t&count=0\tIn this view, we see more clearly our 12 important pieces of information that we need to perform our search (one per line). On the first is the Old Bailey’s base website URL, followed by a query: “?” (don’t worry about the foo=bar bit; the developers of the Old Bailey Online say that it does not do anything.) and a series of 10 name/value pairs put together with & characters. Together these 10 name/value pairs comprise the query string, which tells the search engine what variables to use in specific stages of the search. Notice that each name/value pair contains both a variable name: toYear, and then assigns that variable a value: 1750. This works in exactly the same way as Function Arguments by passing certain information to specific variables. In this case, the most important variable is \\_divs\\_fulltext= which has been given the value: mulatto*+negro*\tThis holds the search term we have typed into the search box. The program has automatically added a + sign in place of a blank space (URLs cannot contain spaces); otherwise that’s exactly what we’ve asked the Old Bailey site to find for us. The other variables hold values that we defined as well. fromYear and toYear contain our date range. Since no year has 99 months as suggested in the toMonth variable, we can assume this is how the search algorithm ensures all records from that year are included. There are no hard and fast rules for figuring out what each variable does because the person who built the site gets to name them. Often you can make an educated guess. All of the possible search fields on the Advanced Search page have their own name/value pair. If you’d like to find out the name of the variable so you can use it, do a new search and make sure you put a value in the field in which you are interested. After you submit your search, you’ll see your value and the name associated with it as part of the URL of the search results page. With the Old Bailey Online, as with many other websites, the search form (advanced or not) essentially helps you to construct URLs that tell the database what to search for. If you can understand how the search fields are represented in the URL – which is often quite straightforward – then it becomes relatively simple to programmatically construct these URLs and thus to automate the process of downloading records.  \n 65 Now try changing the “start=0” to “start=10” and hit enter. You should now have results 11-13. The “start” variable tells the website which entry should be shown at the top of the search results list. We should be able to use this knowledge to create a series of URLs that will allow us to download all 13 files. Let’s turn to that now. Systematically Downloading Files In ‘Downloading Web Pages with Python’58 we learned that Python can download a webpage as long as we have the URL. In that lesson we used the URL to download the trial transcript of Benjamin Bowsey. In this case, we’re trying to download multiple trial transcripts that meet the search criteria we outlined above without having to repeatedly re-run the program. Instead, we want a program that will download everything we need in one go. At this point we have a URL to a search results page that contains the first ten entries of our search. We also know that by changing the “start” value in the URL we can sequentially call each search results page, and ultimately retrieve all of the trial documents from them. Of course the research results don’t give us the trial documents themselves, but only links to them. So we need to extract the link to the underlying records from the search results. On the Old Bailey Online website, the URLs for the individual records (the trial transcript files) can be found as links on the search results pages. We know that all trial transcript URLs contain a trial id that takes the form: “t” followed by at least 8 numbers (e.g. t17800628-33). By looking for links that contain that pattern, we can identify trial transcript URLs. As in previous lessons, let’s develop an algorithm so that we can begin tackling this problem in a manner that a computer can handle. It seems this task can be achieved in four steps. We will need to: Generate the URLs for each search results page by incrementing the “start” variable by a fixed amount an appropriate number of times. Download each search results page as an HTML file. Extract the URLs of each trial transcript (using the trial ID as described above) from the search results HTML files. Cycle through those extracted URLs to download each trial transcript and save it to a directory on our computer You’ll recall that this is fairly similar to the tasks we achieved in ‘Downloading Web Pages with Python’59 and ‘From HTML to a List of Words 2’.60 First we download, then we parse out the information we’re after. And in this case, we download some more.                                                         58\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’\tThe\tProgramming\tHistorian\t(2012).\t59\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’\tThe\tProgramming\tHistorian\t(2012).\t60\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\ta\tList\tof\tWords\t(2)’,\tThe\tProgramming\tHistorian\t(2012).\t \n 66 Downloading the search results pages First we need to generate the URLs for downloading each search results page. We have already got the first one by using the form on the website: http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext=mulatto*+negro*&kwparse=advanced&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount&fromYear=1700&fromMonth=00&toYear=1750&toMonth=99&start=0&count=0\tWe could type this URL out twice and alter the ‘start’ variable to get us all 13 entries, but let’s write a program that would work no matter how many search results pages or records we had to download, and no matter what we decide to search for. Study this code and then add this function to a module named obo.py (create a file with that name and save it to the directory where you want to do your work). The comments in the code are meant to help you decipher the various parts. def\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth):\t\t\t\t\t\timport\turllib2\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\turl\t+=\tquery\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\tfilename\t=\t'search-result'\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\tf.write(webContent)\t\t\t\t\tf.close\tIn this function we have split up the various Query String components and used Function Arguments so that this function can be reused beyond our specific needs right now. When we call this function we will replace the arguments with the values we want to search for. We then download the search results page in a similar manner as done in ‘Downloading Web Pages with Python’.61 Now, make a new file: download-searches.py and copy into it the following code. Note, the values we have passed as arguments are exactly the same as those used in the example above. Feel free to play with these to get different results or see how they work.                                                         61\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’\tThe\tProgramming\tHistorian\t(2012).\t \n 67 #download-searches.py\timport\tobo\t\tquery\t=\t'mulatto*+negro*'\t\tobo.getSearchResults(query,\t\"advanced\",\t\"1700\",\t\"00\",\t\"1750\",\t\"99\")\tWhen you run this code you should find a new file: “search-results.html” in your programming-historian\tdirectory containing the first search results page for your search. Check that this downloaded properly and then delete the file. We’re going to adapt our program to download the other page containing the other 3 entries at the same time so we want to make sure we get both. Let’s refine our getSearchResults function by adding another function argument called “entries” so we can tell the program how many pages of search results we need to download. We will use the value of entries and some simple math to determine how many search results pages there are. This is fairly straightforward since we know there are ten trial transcripts listed per page. We can calculate the number of search results pages by dividing the value of entries by 10. We will save this result to an integer variable named pageCount. It looks like this: #determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\tpageCount\t=\tentries\t/\t10\tHowever, because pageCount is an integer and cannot have decimal places or remainders, Python will drop the remainder. You can test this by running this code in your Terminal (Mac & Linux) / Python Command Line (Windows) and printing out the value held in pageCount. (Note, from here on, we will use the word Terminal to refer to this program). entries\t=\t13\tpageCount\t=\tentries\t/\t10\tprint\tpageCount\t->\t1\tWe know this should read 2 (one page containing entries 1-10, and one page containing entries 11-13). Since there is a remainder to this problem (of 3, but it doesn’t matter what the remainder is), the last 3 results won’t be downloaded, as we’ll only grab 1 page of 10 results. To get around this problem we use the modulo operator (%) in place of the usual division operator (/).62 Modulo divides the first value by the second and returns the remainder. So if the remainder is more than 0, we know there is a partial page of results, and we need to increase the pageCount value by one. The code should now look like this:                                                           62\t‘Binary\tarithmetic\toperations’\tPython\tReference\tManual:\thttps://docs.python.org/release/2.5.2/ref/binary.html\t \n 68  #determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\tpageCount\t=\tentries\t/\t10\tremainder\t=\tentries\t%\t10\tif\tremainder\t>\t0:\t\t\t\t\tpageCount\t+=\t1\tIf we add this to our getSearchResults function just under the startValue = 0 line, our program, the code can now calculate the number of pages that need to be downloaded. However, at this stage it will still only download the first page since we have only told the downloading section of the function to run once. To correct this, we can add that downloading code to a for loop which will download once for every number in the pageCount variable. If it reads 1, then it will download once; if it reads 5 it will download five times, and so on. Immediately after the if statement you have just written, add the following line and indent everything down to f.close one additional tab so that it is all enclosed in the for loop: for\tpages\tin\trange(1,\tpageCount+1):\t\t\t\t\tprint\tpages\tSince this is a for loop, all of the code we want to run repeatedly needs to be intended as well. You can see if you have done this correctly by looking at the finished code example below. This loop takes advantage of Python’s range funciton.63 To understand this for loop it is probably best to think of pageCount as equal to 2 as it is in the example. This two lines of code then means: start running with an initial loop value of 1, and each time you run, add 1 more to that value. When the loop value is the same as pageCount, run once more and then stop. This is particularly valuable for us because it means we can tell our program to run exactly once for each search results page and provides a flexible new skill for controlling how many times a for loop runs. If you would like to practice with this new and powerful way of writing for loops, you can open your Terminal and play around. pageCount\t=\t2\tfor\tpages\tin\trange(1,\tpageCount+1):\t\t\t\t\tprint\tpages\t\t->\t1\t->\t2\tBefore we add all of this code together to our getSearchResults function, we have to make two final adjustments. At the end of the for loop (but still inside the loop), and after our downloading code has run we will need to change the startValue variable, which is used in building the URL of the page we want to download. If we forget to do this, our program will repeatedly download the first search results page since we are not actually changing anything in the initial URL. The startValue variable, as                                                         63\t‘The\trange()\tfunction’\tPython:\thttps://docs.python.org/2/tutorial/controlflow.html#the-range-function\t \n 69 discussed above, is what controls which search results page we want to download. Therefore, we can request the next search results page by increasing the value of startValue by 10 after the initial download has completed. If you are not sure where to put this line you can peek ahead to the finished code example below. Finally, we want to ensure that the name of the file we have downloaded is different for each file. Otherwise, each download will save over the previous download, leaving us with only a single file of search results. To solve this, we can adjust the contents of the filename variable to include the value held in startValue so that each time we download a new page, it gets a different name. Since startValue is an integer, we will have to convert it to a string before we can add it to the filename variable. Adjust the line in your program that pertains to the filename variable to looks like this: filename\t=\t'search-result'\t+\tstr(startValue)\tYou should now be able to add these new lines of code to your getSearchResults function. Recall we have made the following additions: Add entries as an additional function argument right after toMonth Calculate the number of search results pages and add this immediately after the line that begins with startValue = 0 (before we build the URL and start downloading) Follow this immediately with a for loop that will tell the program to run once for each search results page, and indent the rest of the code in the function so that it is inside the new loop. The last line in the for loop should now increase the value of the startValue variable each time the loop runs. Adjust the existing filename variable so that each time a search results page is downloaded it gives the file a unique name. The finished function code in your obo.py file should look like this:  \n 70 \t#create\tURLs\tfor\tsearch\tresults\tpages\tand\tsave\tthe\tfiles\tdef\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth,\tentries):\t\t\t\t\t\timport\turllib2\t\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t\t#this\tis\tnew!\tDetermine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\t\t\t\t\tpageCount\t=\tentries\t/\t10\t\t\t\t\tremainder\t=\tentries\t%\t10\t\t\t\t\tif\tremainder\t>\t0:\t\t\t\t\t\t\t\t\tpageCount\t+=\t1\t\t\t\t\t\t#this\tline\tis\tnew!\t\t\t\t\tfor\tpages\tin\trange(1,\tpageCount\t+1):\t\t\t\t\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t\t\t\t\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\t\t\t\t\turl\t+=\tquery\t\t\t\t\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t\t\t\t\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\tfilename\t=\t'search-result'\t+\tstr(startValue)\t\t\t\t\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\t#this\tlines\tis\tnew!\t\t\t\t\t\t\t\t\tstartValue\t=\tstartValue\t+\t10\tTo run this new function, add the extra argument to download-searches.py and run the program again: #download-searches.py\timport\tobo\t\tquery\t=\t'mulatto*+negro*'\t\tobo.getSearchResults(query,\t\"advanced\",\t\"1700\",\t\"00\",\t\"1750\",\t\"99\",\t13)\tGreat! Now we have both search results pages, called search-result0.html and search-result10.html. But before we move onto the next step in the algorithm, let’s take care of some housekeeping. Our programming-historian directory will quickly become unwieldy if we download multiple search results pages and trial transcripts. Let’s have  \n 71 Python make a new directory named after our search terms. Study and then copy the following to obo.py. def\tnewDir(newDir):\t\t\t\t\timport\tos\t\t\t\t\t\tdir\t=\tnewDir\t\t\t\t\t\tif\tnot\tos.path.exists(dir):\t\t\t\t\t\t\t\t\tos.makedirs(dir)\tWe want to call this new function in getSearchResults, so that our search results pages are downloaded to a directory with the same name as our search query. This will keep our programming-historian directory more organized. To do this we will create a new directory using the os library, short for “operating system”. That library contains a function called makedirs, which, unsurprisingly, makes a new directory. You can try this out using the Terminal. import\tos\t\tquery\t=\t\"myNewDirectory\"\tif\tnot\tos.path.exists(query):\t\t\t\t\tos.makedirs(query)\tThis program will check to see if your computer already has a directory with this name. If not, you should now have a directory called myNewDirectory on your computer. On a Mac this is probably located in your /Users/username/ directory, and on Windows you should be able to find it in the Python directory on your computer, the same in which you opened your command line program. If this worked you can delete the directory from your hard drive, since it was just for practice. Since we want to create a new directory named after the query that we input into the Old Bailey Online website, we will make direct use of the query function argument from the getSearchResults function. To do this, import the os directory after you have imported urllib2 and then add the code you have just written immediately below. Your getSearchResults function should now look like this:  \n 72 \t#create\tURLs\tfor\tsearch\tresults\tpages\tand\tsave\tthe\tfiles\tdef\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth,\tentries):\t\t\t\t\t\timport\turllib2,\tos\t\t\t\t\t\t#This\tline\tis\tnew!\tCreate\ta\tnew\tdirectory\t\t\t\t\tif\tnot\tos.path.exists(query):\t\t\t\t\t\t\t\t\tos.makedirs(query)\t\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t\t#Determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\t\t\t\t\tpageCount\t=\tentries\t/\t10\t\t\t\t\tremainder\t=\tentries\t%\t10\t\t\t\t\tif\tremainder\t>\t0:\t\t\t\t\t\t\t\t\tpageCount\t+=\t1\t\t\t\t\t\tfor\tpages\tin\trange(1,\tpageCount\t+1):\t\t\t\t\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t\t\t\t\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\t\t\t\t\turl\t+=\tquery\t\t\t\t\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t\t\t\t\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t#save\tthe\tresult\tto\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\tfilename\t=\t'search-result'\t+\tstr(startValue)\t\t\t\t\t\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\tstartValue\t=\tstartValue\t+\t10\tThe last step for this function is to make sure that when we save our search results pages, we save them in this new directory. To do this we can make a minor adjustment to the filename variable so that the file ends up in the right place. There are many ways we can do this, the easiest of which is just to append the new directory name plus a slash to the name of the file: filename\t=\tquery\t+\t'/'\t+\t'search-result'\t+\tstr(startValue)\t \n 73 If your computer is running Windows you will need to use a backslash instead of a forward slash in the above example. Add the above line to your getSearchResults page in lieu of the current filename description. If you are running Windows, chances are your downloadSearches.py program will now crash when you run it because you are trying to create a director with a * in it. Windows does not like this. To get around this problem we can use regular expressions64 to remove any non-Windows-friendly characters. We used regular expressions previously in ‘Counting Word Frequencies with Python’.65 To remove non-alpha-numeric characters from the query, first import the regular expressions library immediately after you have imported the os library, then use the re.sub() function to create a new string named cleanQuery that contains only alphanumeric characters. You will then have to substitute cleanQuery as the variable used in the os.path.exists(), os.makedirs(), and filename declarations. import\turllib2,\tos,\tre\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\tif\tnot\tos.path.exists(cleanQuery):\t\t\t\t\t\t\t\t\tos.makedirs(cleanQuery)\t\t...\t\tfilename\t=\tcleanQuery\t+\t'/'\t+\t'search-result'\t+\tstr(startValue)\tThe final version of your function should look like this: #create\tURLs\tfor\tsearch\tresults\tpages\tand\tsave\tthe\tfiles\tdef\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth,\tentries):\t\t\t\t\t\timport\turllib2,\tos,\tre\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\t\t#Create\ta\tnew\tdirectory\t\t\t\t\tif\tnot\tos.path.exists(cleanQuery):\t\t\t\t\t\t\t\t\tos.makedirs(cleanQuery)\t\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t\t#determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\t\t\t\t\tpageCount\t=\tentries\t/\t10\t\t\t\t\tremainder\t=\tentries\t%\t10\t\t\t\t\tif\tremainder\t>\t0:\t\t\t\t\t\t\t\t\tpageCount\t+=\t1\t\t\t\t\t\tfor\tpages\tin\trange(1,\tpageCount+1):\t\t\t\t\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t                                                        64\t‘Regular\texpression\toperations’,\tPython:\thttps://docs.python.org/2/library/re.html\t\t65\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tthe\tProgramming\tHistorian\t(2012).\t \n 74 \t\t\t\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\t\t\t\t\turl\t+=\tquery\t\t\t\t\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t\t\t\t\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t#save\tthe\tresult\tto\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\tfilename\t=\tcleanQuery\t+\t'/'\t+\t'search-result'\t+\tstr(startValue)\t\t\t\t\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\tstartValue\t=\tstartValue\t+\t10\tThis time we tell the program to download the trials and put them in the new directory rather than our programming-historian directory. Run download-searches.py once more to ensure this worked and you understand how to save files to a particular directory using Python. Downloading the individual trial entries At this stage we have created a function that can download all of the search results HTML files from the Old Bailey Online website for an advanced search that we have defined, and have done so programmatically. Now for the next step in the algorithm: Extract the URLs of each trial transcript from the search results HTML files. In the lessons that precede this one (eg, ‘Downloading Web Pages with Python’66), we have worked with the printer friendly versions of the trial transcripts, so we will continue to do so. We know that the printer friendly version of Benjamin Bowsey’s trial is located at the URL: http://www.oldbaileyonline.org/print.jsp?div=t17800628-33\tIn the same way that changing query strings in the URLs yields different search results, changing the URL for trial records – namely substituting one trial ID for another – we will get the transcript for that new trial. This means that to find and download the 13 matching files, all we need are these trial IDs. Since we know that search results pages on websites generally contain a link to the pages described, there is a good chance that we can find these links embedded in the HTML code. If we can scrape this information from the downloaded search results pages, we can then use                                                         66\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 75 that information to generate a URL that will allow us to download each trial transcript. This is a technique that you can use for most search result pages, not just Old Bailey Online! To do this, we must first find where the trial IDs are amidst the HTML code in the downloaded files, and then determine a way to consistently isolate them using code so that no matter which search results page we download from the site we are able to find the trial transcripts. First, open search-results0.html in Komodo Edit and have a look for the list of the trials. The first entry starts with “Anne Smith” so you can use the “find” feature in Komodo Edit to jump immediately to the right spot. Notice Anne’s name is part of a link: http://www.oldbaileyonline.org/browse.jsp?id=t17160113-18&div=t17160113-18&terms=mulatto|negro#highlight\t\tPerfect, the link contains the trial ID! Scroll through the remaining entries and you’ll find the same is true. Lucky for us, the site is well formatted and it looks like each link starts with “browse.jsp?id=” followed by the trial ID and finished with an &, in Anne’s case: “browse.jsp?id=t17160113-18&”. We can write a few lines of code that can isolate those IDs. Take a look at the following function. This function also uses the os library, in this case to list all of the files located in the directory created in the previous section. The os library contains a range of useful functions that mirror the types of tasks you would expect to be able to do with your mouse in the Mac Finder or Windows such as opening, closing, creating, deleting, and moving files and directories, and is a great library to master – or at least familiarize yourself with. def\tgetIndivTrials(query):\t\t\t\t\timport\tos,\tre\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tsearchResults\t=\tos.listdir(cleanQuery)\t\t\t\t\t\tprint\tsearchResults\tCreate and run a new program called extract-trial-ids.py with the following code. Make sure you input the same value into the query argument as you did in the previous example: import\tobo\t\tobo.getIndivTrials(\"mulatto*+negro*\")\tIf everything went right, you should see a list containing the names of all the files in your new “mulatto*+negro*” directory, which at this point should be the two search results pages. Ensure this worked before moving forward. Since we saved all of the search results pages with a filename that includes “search-results”, we now want to open each file with a name containing “search-results”, and extract all trial IDs found therein. In this case we know we have 2, but we want our code to be as reusable as possible (with reason, of course!) Restricting this action to files named “search- \n 76 results” will mean that this program will work as intended even if the directory contains many other unrelated files because the program will skip over anything with a different name. Add the following to your getIndivTrials() function, which will check if each file contains “search-results” in its name. If it does, the file will be opened and the contents saved to a variable named text. That text variable will then be parsed looking for the trial ID, which we know always follows “browse.jsp?id=”. If and when that trial ID is found it will be saved to a list and printed to the command output, which leaves us with all of the information we need to then write a program that will download the desired trials. \t\t\t\timport\tos,\tre\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tsearchResults\t=\tos.listdir(cleanQuery)\t\t\t\t\t\turls\t=\t[]\t\t\t\t\t\t#find\tsearch-results\tpages\t\t\t\t\tfor\tfiles\tin\tsearchResults:\t\t\t\t\t\t\t\t\tif\tfiles.find(\"search-result\")\t!=\t-1:\t\t\t\t\t\t\t\t\t\t\t\t\tf\t=\topen(cleanQuery\t+\t\"/\"\t+\tfiles,\t'r')\t\t\t\t\t\t\t\t\t\t\t\t\ttext\t=\tf.read().split(\"\t\")\t\t\t\t\t\t\t\t\t\t\t\t\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#look\tfor\ttrial\tIDs\t\t\t\t\t\t\t\t\t\t\t\t\tfor\twords\tin\ttext:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords.find(\"browse.jsp?id=\")\t!=\t-1:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#isolate\tthe\tid\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\turls.append(words[words.find(\"id=\")\t+3:\twords.find(\"&\")])\t\t\t\t\t\tprint\turls\tThat last line of the for loop may look tricky, but make sure you understand it before moving on. The words variable is checked to see if it contains the characters “id=” (without the quotes), which of course refers to a specific trial transcript ID. If it does, we use the slice string method to capture only the chunk between id= and & and append it to the url list. If we knew the exact index positions of this substring we could have used those numerical values instead. However, by using the find() string method we have created a much more flexible program. The following code does exactly the same thing as that last line in a less condensed manner. idStart\t=\twords.find(\"id=\")\t+\t3\tidEnd\t=\twords.find(\"&\")\ttrialID\t=\twords[idStart:\tidEnd]\t\turls.append(trialID)\tWhen you re-run extract-trial-ids.py, you should now see a list of all the trial IDs. We can add a couple extra lines to turn these into proper URLs and download the whole list to our new directory. We’ll also use the time library to pause our program for three seconds between downloads– a technique called throttling. It’s considered good form not to pound  \n 77 someone’s server with many requests per second; and the slight delay makes it more likely that all the files will actually download rather than time out.67 Add the following code to the end of your getIndivTrials() function. This code will generate the URL of each individual page, download the page to your computer, place it in your new directory, save the file, and pause for 3 seconds before moving on to the next trial. This work is all contained in a for loop, and will run once for every trial in your url list. def\tgetIndivTrials(query):\t\t\t\t\t#...\t\t\t\t\timport\turllib2,\ttime\t\t\t\t\t\t#import\tbuilt-in\tpython\tfunctions\tfor\tbuilding\tfile\tpaths\t\t\t\t\tfrom\tos.path\timport\tjoin\tas\tpjoin\t\t\t\t\t\tfor\titems\tin\turls:\t\t\t\t\t\t\t\t\t#generate\tthe\tURL\t\t\t\t\t\t\t\t\turl\t=\t\"http://www.oldbaileyonline.org/print.jsp?div=\"\t+\titems\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t#create\tthe\tfilename\tand\tplace\tit\tin\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\tfilename\t=\titems\t+\t'.html'\t\t\t\t\t\t\t\t\tfilePath\t=\tpjoin(cleanQuery,\tfilename)\t\t\t\t\t\t\t\t\t\t#save\tthe\tfile\t\t\t\t\t\t\t\t\tf\t=\topen(filePath,\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\t#pause\tfor\t3\tsecond\t\t\t\t\t\t\t\t\ttime.sleep(3)\tIf we put this all together into a single function it should look something like this. (Note, we’ve put all the “import” calls at the top to keep things cleaner). def\tgetIndivTrials(query):\t\t\t\t\timport\tos,\tre,\turllib2,\ttime\t\t\t\t\t\t#import\tbuilt-in\tpython\tfunctions\tfor\tbuilding\tfile\tpaths\t\t\t\t\tfrom\tos.path\timport\tjoin\tas\tpjoin\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tsearchResults\t=\tos.listdir(cleanQuery)\t\t\t\t\t\turls\t=\t[]\t\t\t\t\t\t#find\tsearch-results\tpages\t\t\t\t\tfor\tfiles\tin\tsearchResults:\t\t\t\t\t\t\t\t\tif\tfiles.find(\"search-result\")\t!=\t-1:\t                                                        67\t‘HTTP\tError\t408\tRequest\tTimeout’\tCheckupdown:\thttp://www.checkupdown.com/status/E408.html\t \n 78 \t\t\t\t\t\t\t\t\t\t\t\tf\t=\topen(cleanQuery\t+\t\"/\"\t+\tfiles,\t'r')\t\t\t\t\t\t\t\t\t\t\t\t\ttext\t=\tf.read().split(\"\t\")\t\t\t\t\t\t\t\t\t\t\t\t\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#look\tfor\ttrial\tIDs\t\t\t\t\t\t\t\t\t\t\t\t\tfor\twords\tin\ttext:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords.find(\"browse.jsp?id=\")\t!=\t-1:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#isolate\tthe\tid\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\turls.append(words[words.find(\"id=\")\t+3:\twords.find(\"&\")])\t\t\t\t\t\t#new\tfrom\there\tdown!\t\t\t\t\tfor\titems\tin\turls:\t\t\t\t\t\t\t\t\t#generate\tthe\tURL\t\t\t\t\t\t\t\t\turl\t=\t\"http://www.oldbaileyonline.org/print.jsp?div=\"\t+\titems\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t#create\tthe\tfilename\tand\tplace\tit\tin\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\tfilename\t=\titems\t+\t'.html'\t\t\t\t\t\t\t\t\tfilePath\t=\tpjoin(cleanQuery,\tfilename)\t\t\t\t\t\t\t\t\t\t#save\tthe\tfile\t\t\t\t\t\t\t\t\tf\t=\topen(filePath,\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\t#pause\tfor\t3\tseconds\t\t\t\t\t\t\t\t\ttime.sleep(3)\tLet’s add the same three-second pause to our getSearchResults function to be kind to the Old Bailey Online servers: #create\tURLs\tfor\tsearch\tresults\tpages\tand\tsave\tthe\tfiles\tdef\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth,\tentries):\t\t\t\t\t\timport\turllib2,\tos,\tre,\ttime\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tif\tnot\tos.path.exists(cleanQuery):\t\t\t\t\t\t\t\t\tos.makedirs(cleanQuery)\t\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t\t#Determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\t\t\t\t\tpageCount\t=\tentries\t/\t10\t\t\t\t\tremainder\t=\tentries\t%\t10\t\t\t\t\tif\tremainder\t>\t0:\t\t\t\t\t\t\t\t\tpageCount\t+=\t1\t\t\t\t\t\tfor\tpages\tin\trange(1,\tpageCount\t+1):\t\t\t\t\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t\t\t\t\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\t\t\t\t\turl\t+=\tquery\t\t\t\t\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t \n 79 \t\t\t\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t#save\tthe\tresult\tto\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\tfilename\t=\tcleanQuery\t+\t'/'\t+\t'search-result'\t+\tstr(startValue)\t\t\t\t\t\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\tstartValue\t=\tstartValue\t+\t10\t\t\t\t\t\t\t\t\t\t#pause\tfor\t3\tseconds\t\t\t\t\t\t\t\t\ttime.sleep(3)\tFinally, call the function in the download-searches.py program. #download-searches.py\timport\tobo\t\tquery\t=\t'mulatto*+negro*'\t\tobo.getSearchResults(query,\t\"advanced\",\t\"1700\",\t\"00\",\t\"1750\",\t\"99\",\t13)\t\tobo.getIndivTrials(query)\tNow, you’ve created a program that can request and download files from the Old Bailey website based on search parameters you define, all without visiting the site! In case a file does not download Check to make all thirteen files have downloaded properly. If that’s the case for you, that’s great! However, there’s a possibility that this program stalled along the way. That’s because our program, though running on your own machine, relies on two factors outside of our immediate control: the speed of the Internet, and the response time of the Old Bailey Online server at that moment. It’s one thing to ask Python to download a single file, but when we start asking for a file every 3 seconds there’s a greater chance the server will either time out or fail to send us the file we are after. If we were using a web browser to make these requests, we’d eventually get a message that the “connection had timed out” or something of the sort. We all see this from time to time. However, our program isn’t built to handle or relay such error messages, so instead you’ll realize it when you discover that the program has not returned the expected number of files or just seemingly does nothing. To prevent frustration and uncertainty, we want a fail-safe in our program that will attempt to download each trial. If for  \n 80 whatever reason it fails, we’ll make a note of it and move on to the next trial. To do this, we will make use of the Python try / except error handling68 mechanism, as well as a new library: socket. Try and Except are a lot like an if / else statement. When you ask Python to try something, it will attempt to run the code; if the code fails to achieve what you have defined, it will run the except code. This is often used when dealing with errors, known as error handling. We can use this to our advantage by telling our program to attempt downloading a page. If it fails, we’ll ask it to let us know which file failed and then move on. To do this we need to use the socket library, which will allow us to put a time limit on a download attempt before moving on. This involves altering the getIndivTrials function. First, we need to load the socket library, which should be done in the same way as all of our previous library imports. We will also need to set the default socket timeout length – how long do we want to try to download a page before we give up. This should go immediately after the comment that begins with #download\tthe\tpage import\tos,\turllib2,\ttime,\tsocket\t\t\t\t\t\t#...\t\t\t\t\t\t\t\t\t#download\tthe\tpage\t\t\t\t\t\t\t\t\tsocket.setdefaulttimeout(10)\tThen, we need a new python list that will hold all of the urls that failed to download. We will call this failedAttempts and you can insert it immediately after the import instructions: \tfailedAttempts\t=\t[]\tFinally, we can add the try / except statement, which is added in much the same way as an if / else statement would be. In this case, we will put all of the code designed to download and save the trials in the try statement, and in the except statement we will tell the program what we want it to do if that should fail. Here, we will append the url that failed to download to our new list, failedAttempts #...\t\t\t\t\t\t\t\t\t\tsocket.setdefaulttimeout(10)\t\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#create\tthe\tfilename\tand\tplace\tit\tin\tthe\tnew\t\"trials\"\tdirectory\t\t\t\t\t\t\t\t\t\t\t\t\tfilename\t=\titems\t+\t'.html'\t\t\t\t\t\t\t\t\t\t\t\t\tfilePath\t=\tpjoin(newDir,\tfilename)\t                                                        68\t‘Errors\tand\tExceptions’,\tPython:\thttps://docs.python.org/2/tutorial/errors.html\t \n 81 \t\t\t\t\t\t\t\t\t\t\t\t\t#save\tthe\tfile\t\t\t\t\t\t\t\t\t\t\t\t\tf\t=\topen(filePath,\t'w')\t\t\t\t\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\t\t\t\t\tfailedAttempts.append(url)\tFinally, we will tell the program to print the contents of the list to the command output so we know which files failed to download. This should be added as the last line in the function. print\t\"failed\tto\tdownload:\t\"\t+\tstr(failedAttempts)\t\tNow when you run the program, should there be a problem downloading a particular file, you will receive a message in the Command Output window of Komodo Edit. This message will contain any URLs of files that failed to download. If there are only one or two, it’s probably fastest just to visit the pages manually and use the “Save As” feature of your browser. If you are feeling adventurous, you could modify the program to automatically download the remaining files. The final version of your getSearchResults(), getIndivTrials(), and newDir() functions should now look like this: def\tgetSearchResults(query,\tkwparse,\tfromYear,\tfromMonth,\ttoYear,\ttoMonth,\tentries):\t\t\t\t\t\timport\turllib2,\tos,\tre,\ttime\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tif\tnot\tos.path.exists(cleanQuery):\t\t\t\t\t\t\t\t\tos.makedirs(cleanQuery)\t\t\t\t\t\tstartValue\t=\t0\t\t\t\t\t#determine\thow\tmany\tfiles\tneed\tto\tbe\tdownloaded.\t\t\t\t\tpageCount\t=\tentries\t/\t10\t\t\t\t\tremainder\t=\tentries\t%\t10\t\t\t\t\tif\tremainder\t>\t0:\t\t\t\t\t\t\t\t\tpageCount\t+=\t1\t\t\t\t\t\tfor\tpages\tin\trange(1,\tpageCount+1):\t\t\t\t\t\t\t\t\t\t#each\tpart\tof\tthe\tURL.\tSplit\tup\tto\tbe\teasier\tto\tread.\t\t\t\t\t\t\t\t\turl\t=\t'http://www.oldbaileyonline.org/search.jsp?foo=bar&form=searchHomePage&_divs_fulltext='\t\t\t\t\t\t\t\t\turl\t+=\tquery\t\t\t\t\t\t\t\t\turl\t+=\t'&kwparse='\t+\tkwparse\t\t\t\t\t\t\t\t\turl\t+=\t'&_divs_div0Type_div1Type=sessionsPaper%7CtrialAccount'\t\t\t\t\t\t\t\t\turl\t+=\t'&fromYear='\t+\tfromYear\t\t\t\t\t\t\t\t\turl\t+=\t'&fromMonth='\t+\tfromMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&toYear='\t+\ttoYear\t\t\t\t\t\t\t\t\turl\t+=\t'&toMonth='\t+\ttoMonth\t\t\t\t\t\t\t\t\turl\t+=\t'&start='\t+\tstr(startValue)\t\t\t\t\t\t\t\t\turl\t+=\t'&count=0'\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\tand\tsave\tthe\tresult.\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t \n 82 \t\t\t\t\t\t\t\t\tfilename\t=\tcleanQuery\t+\t'/'\t+\t'search-result'\t+\tstr(startValue)\t\t\t\t\t\t\t\t\tf\t=\topen(filename\t+\t\".html\",\t'w')\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\t\tstartValue\t=\tstartValue\t+\t10\t\t\t\t\t\t\t\t\t\t#pause\tfor\t3\tseconds\t\t\t\t\t\t\t\t\ttime.sleep(3)\t\tdef\tgetIndivTrials(query):\t\t\t\t\timport\tos,\tre,\turllib2,\ttime,\tsocket\t\t\t\t\t\tfailedAttempts\t=\t[]\t\t\t\t\t\t#import\tbuilt-in\tpython\tfunctions\tfor\tbuilding\tfile\tpaths\t\t\t\t\tfrom\tos.path\timport\tjoin\tas\tpjoin\t\t\t\t\t\tcleanQuery\t=\tre.sub(r'\\W+',\t'',\tquery)\t\t\t\t\tsearchResults\t=\tos.listdir(cleanQuery)\t\t\t\t\t\turls\t=\t[]\t\t\t\t\t\t#find\tsearch-results\tpages\t\t\t\t\tfor\tfiles\tin\tsearchResults:\t\t\t\t\t\t\t\t\tif\tfiles.find(\"search-result\")\t!=\t-1:\t\t\t\t\t\t\t\t\t\t\t\t\tf\t=\topen(cleanQuery\t+\t\"/\"\t+\tfiles,\t'r')\t\t\t\t\t\t\t\t\t\t\t\t\ttext\t=\tf.read().split(\"\t\")\t\t\t\t\t\t\t\t\t\t\t\t\tf.close()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#look\tfor\ttrial\tIDs\t\t\t\t\t\t\t\t\t\t\t\t\tfor\twords\tin\ttext:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords.find(\"browse.jsp?id=\")\t!=\t-1:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#isolate\tthe\tid\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\turls.append(words[words.find(\"id=\")\t+3:\twords.find(\"&\")])\t\t\t\t\t\tfor\titems\tin\turls:\t\t\t\t\t\t\t\t\t#generate\tthe\tURL\t\t\t\t\t\t\t\t\turl\t=\t\"http://www.oldbaileyonline.org/print.jsp?div=\"\t+\titems\t\t\t\t\t\t\t\t\t\t#download\tthe\tpage\t\t\t\t\t\t\t\t\tsocket.setdefaulttimeout(10)\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\t\t\t\t\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\t\t\t\t\t\t\t\t\t\t#create\tthe\tfilename\tand\tplace\tit\tin\tthe\tnew\tdirectory\t\t\t\t\t\t\t\t\t\t\t\t\tfilename\t=\titems\t+\t'.html'\t\t\t\t\t\t\t\t\t\t\t\t\tfilePath\t=\tpjoin(cleanQuery,\tfilename)\t\t\t\t\t\t\t\t\t\t\t\t\t\t#save\tthe\tfile\t\t\t\t\t\t\t\t\t\t\t\t\tf\t=\topen(filePath,\t'w')\t\t\t\t\t\t\t\t\t\t\t\t\tf.write(webContent)\t\t\t\t\t\t\t\t\t\t\t\t\tf.close\t\t\t\t\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\t\t\t\t\tfailedAttempts.append(url)\t\t\t\t\t\t\t\t\t#pause\tfor\t3\tseconds\t\t\t\t\t\t\t\t\ttime.sleep(3)\t\t\t\t\tprint\t\"failed\tto\tdownload:\t\"\t+\tstr(failedAttempts)\t\tdef\tnewDir(newDir):\t \n 83 \t\t\t\timport\tos\t\t\t\t\t\tdir\t=\tnewDir\t\t\t\t\t\tif\tnot\tos.path.exists(dir):\t\t\t\t\t\t\t\t\tos.makedirs(dir)\tFurther Reading For more advanced users, or to become a more advanced user, you may find it worthwhile to read about achieving this same process using Application Programming Interfaces (API). A website with an API will generally provide instructions on how to request certain documents. It’s a very similar process to what we just did by interpreting the URL Query Strings, but without the added detective work required to decipher what each variable does. If you are interested in the Old Bailey Online, they have recently released an API and the documentation can be quite helpful: Old Bailey Online API (http://www.oldbaileyonline.org/static/DocAPI.jsp) Python Best way to create directory if it doesn’t exist for file write? (http://stackoverflow.com/questions/273192/python-best-way-to-create-directory-if-it-doesnt-exist-for-file-write) About the Author Adam Crymble is a lecturer of digital history at the University of Hertfordshire.   \n 84 10. Intro to the Zotero API Amanda Morton – 2013  Lesson Goals In this lesson, you’ll learn how to use python with the Zotero API to interact with your Zotero library. The Zotero API is a powerful interface that would allow you to build a complete Zotero client from scratch if you so desired. But like most APIs, it works in small, discrete steps, so we have to build our way up to the complicated requests we might want to use to access our Zotero libraries. But this incremental building gives us plenty of time to learn as we go along. What is Zotero? Zotero is a browser-based research tool that allows you to collect and store content. If you are new to Zotero or do not regularly use it, you may want to familiarize yourself with the Zotero site69 and its helpful Quick Start Guide.70 Additionally, while you will not need it for this introductory lesson, we advise that you download the current version of the libZotero GitHub library71 and store it in the directory you have chosen to use for these lessons. Installing libZotero Using what you learned in Fred Gibbs’ lesson on ‘Installing Python Modules with pip’,72 we'll use pip73 to install libZotero, a python library that will allow us to interact with the Zotero API. To install the library, in your command line/terminal window enter: pip\tinstall\tlibZotero\t\tRemember that you may need to use the sudo preface and enter your password to allow the installation to proceed. Zotero Hello World Once libZotero is installed, we can use it to talk to the Zotero server using Python. In your text editor, run the following: #make\tthe\tlibZotero\tlibrary\tavailable\tfrom\tlibZotero\timport\tzotero\t\t                                                        69\tZotero:\thttps://www.zotero.org/\t70\t‘Quick\tStart\tGuide’,\tZotero:\thttps://www.zotero.org/support/quick_start_guide\t71\t‘libZotero\tGithub\tlibrary’,\tGithub:\thttps://github.com/fcheslack/libZotero\t72\tFred\tGibbs,\t‘Installing\tPython\tModules\twith\tpip’,\tThe\tProgramming\tHistorian\t(2013).\t73\t‘Pip’,\tPython:\thttps://pypi.python.org/pypi/pip\t \n 85 Once we've successfully imported the name zotero from the library, we can create and define a Zotero library \"object\" (zlib, in this example), which will be our means of creating a request for the Zotero server and returning its data. When we create the library object we will need to specify whether we're accessing an individual or group library and include the Zotero library's ID number. Depending on the type of library we're accessing and the things we plan to do with it, we may also need to include an authentication key, which functions sort of like a password. #create\tZotero\tlibrary\tobject\tcalled\t\"zlib\"\tzlib=zotero.Library('group','<insert\tgroup\tID>','<null>',\t'<insert\tAPI\tkey>')\t\tFor this lesson, you can use your own group or individual library, or you can use the library we’ve created for this lesson at ‘Programming Historian 2’.74 If you want to use your own group or individual library, you will need to retrieve your group or user ID and your own API key. If you use your individual library, you'll also need to replace the word group with the word user in the above code. Your group ID can be found by hovering over the RSS option on your library feed. The ID is the numeric part of the URL. Your group API key, if one has been created, is located in your account settings. If there is no key assigned to the group, and you are the end user, you can create a new key on the same page. To use the ‘Programming Historian 2’ group library, use the following: \tGroup\tID:\t155975\t\tAPI\tkey:\t9GLmvmZ1K1qGAz9QWcdlyf6L\t\tzlib=zotero.Library('group','155975','<null>',\t'9GLmvmZ1K1qGAz9QWcdlyf6L')\t\tOnce we've defined our object, we can use it to interact with the information in the library. Retrieving Item Information Zotero has parent items and child items. Parents are typically top-level objects with metadata, and children are usually things like notes and file attachments. For this portion of the lesson, we'll be pulling information from the first five top-level items in our collection. #\tretrieve\tthe\tfirst\tfive\ttop-level\titems.\titems\t=\tzlib.fetchItemsTop({'limit':\t5,\t'content':\t'json,bib,coins'})\t\t                                                        74\t‘Programming\tHistorian\t2\t–\tZotero\tGroup’,\tZotero:\thttps://www.zotero.org/groups/programming_historian_2\t \n 86 Your output for this step, if you are using our sample collection, should look like this: value\tstored\tin\tcache\t-\thttps://api.zotero.org/groups/155975/items/top?limit=5&content=\tjson%2Cbib%2Ccoins&key=9GLmvmZ1K1qGAz9QWcdlyf6L\t\tNext, we can print some basic information about these items. #\tprint\tsome\tdata\tabout\tthese\tfive\titems\tfor\titem\tin\titems:\tprint\t'Item\tType:\t%s\t|\tKey:\t%s\t|\tTitle:\t%s'\t%\t(item.itemType,\titem.itemKey,\titem.title)\t\tThis step should retrieve the item type (journal article, webpage, etc.), the key, and item title. Item\tType:\twebpage\t|\tKey:\tTK5Z4H9J\t|\tTitle:\tBenjamin\tBowsey\tItem\tType:\twebpage\t|\tKey:\t3A2RWZ8A\t|\tTitle:\tY\ta\tt-il\tune\tHistoire\tNumerique\t2.0?\tItem\tType:\twebpage\t|\tKey:\t79U2EACW\t|\tTitle:\tDigitization\tboosts\taccess,\tcollaboration,\tUCLA\tprof\tsays\tItem\tType:\tjournalArticle\t|\tKey:\t39V7A2SZ\t|\tTitle:\tHistory\tand\tthe\tsecond\tdecade\tof\tthe\tWeb\tItem\tType:\tjournalArticle\t|\tKey:\tJRCM2PM7\t|\tTitle:\tThe\tPasts\tand\tFutures\tof\tDigital\tHistory\t\tWe can also pull the bibliographic information associated with our first five items: for\titem\tin\titems:\t\t\t\t\tprint\titem.bibContent\t\tRunning this command will print the bibliographic content stored on the Zotero servers for these items: <div\tclass=\"csl-bib-body\"\tstyle=\"line-height:\t1.35;\tpadding-left:\t2em;\ttext-indent:-2em;\"\txmlns=\"http://www.w3.org/1999/xhtml\">\t<div\tclass=\"csl-entry\">“Benjamin\tBowsey.”\tAccessed\tMarch\t29,\t2013.\thttp://www.oldbaileyonline.org/print.jsp?div=t17800628-33.</div>\t</div>\t<div\tclass=\"csl-bib-body\"\tstyle=\"line-height:\t1.35;\tpadding-left:\t2em;\ttext-indent:-2em;\"\txmlns=\"http://www.w3.org/1999/xhtml\">\t\t\t<div\tclass=\"csl-entry\">Noiret,\tSerge.\t“Y\ta\tT-il\tUne\tHistoire\tNumerique\t2.0?”\tContribution\tto\tbook.\tAccessed\tJuly\t21,\t2011.\thttp://cadmus.eui.eu/handle/1814/18074.</div>\t</div>\t<div\tclass=\"csl-bib-body\"\tstyle=\"line-height:\t1.35;\tpadding-left:\t2em;\ttext-indent:-2em;\"\txmlns=\"http://www.w3.org/1999/xhtml\">\t\t\t<div\tclass=\"csl-entry\">Rushton,\tTullia.\t“Digitization\tBoosts\tAccess,\tCollaboration,\tUCLA\tProf\tSays.”\t<i>Chronicle\tof\tHigher\tEducation</i>,\tJanuary\t20,\t2010.\thttp://dukechronicle.com/article/digitization-boosts-access-collaboration-ucla-prof-says.</div>\t</div>\t<div\tclass=\"csl-bib-body\"\tstyle=\"line-height:\t1.35;\tpadding-left:\t2em;\ttext-indent:-2em;\"\txmlns=\"http://www.w3.org/1999/xhtml\">\t\t\t<div\tclass=\"csl-entry\">Cohen,\tDaniel\tJ.\t“History\tand\tthe\tSecond\tDecade\tof\t \n 87 the\tWeb.”\t<i>Rethinking\tHistory</i>\t8,\tno.\t2\t(2004):\t293.\tdoi:10.1080/13642520410001683950.</div>\t</div>\t<div\tclass=\"csl-bib-body\"\tstyle=\"line-height:\t1.35;\tpadding-left:\t2em;\ttext-indent:-2em;\"\txmlns=\"http://www.w3.org/1999/xhtml\">\t\t\t<div\tclass=\"csl-entry\">Ayers,\tEdward\tL.\t“The\tPasts\tand\tFutures\tof\tDigital\tHistory”\t(1999).\thttp://www.vcdh.virginia.edu/PastsFutures.html.</div>\t</div>\tNow that we have worked through retrieving information using the Zotero API, we can continue to use it to interact with the items stored in our library.  Editor’s Note: This lesson is the first of three lessons on the Zotero API. The next lesson in this series is ‘Creating New Items in Zotero’.75 About the Author Amanda Morton is a DH Fellow at the Center for History and New Media.\n                                                        75\tAmanda\tMorton,\t‘Creating\tNew\tItems\tin\tZotero’,\tThe\tProgramming\tHistorian\t(2013).\t \n 88 11. Data Mining the Internet Archive Collection Caleb McDaniel – 2014  The collections of the Internet Archive include many digitized historical sources.76 Many contain rich bibliographic data in a format called MARC. In this lesson, you'll learn how to use Python to automate the downloading of large numbers of MARC files from the Internet Archive and the parsing of MARC records for specific information such as authors, places of publication, and dates. The lesson can be applied more generally to other Internet Archive files and to MARC records found elsewhere. Lesson Goals The collections of the Internet Archive (IA) include many digitized sources of interest to historians, including early JSTOR journal content,77 John Adams's personal library,78 and the Haiti collection at the John Carter Brown Library.79 In short, to quote Programming Historian Ian Milligan, \"The Internet Archive rocks.\" In this lesson, you'll learn how to download files from such collections using a Python module specifically designed for the Internet Archive. You will also learn how to use another Python module designed for parsing MARC XML records, a widely used standard for formatting bibliographic metadata. For demonstration purposes, this lesson will focus on working with the digitized version of the Anti-Slavery Collection at the Boston Public Library in Copley Square.80 We will first download a large collection of MARC records from this collection, and then use Python to retrieve and analyze bibliographic information about items in the collection. For example, by the end of this lesson, you will be able to create a list of every named place from which a letter in the antislavery collection was written, which you could then use for a mapping project or some other kind of analysis.                                                         76\t‘Internet\tArchive’:\thttps://archive.org/\t77\t‘JSTOR\tEarly\tJournal\tContent’:\thttps://archive.org/details/jstor_ejc\t78\t‘The\tJohn\tAdams\tLibrary\tat\tthe\tBoston\tPublic\tLibrary’:\thttps://archive.org/details/johnadamsBPL\t79\t‘John\tCarter\tBrown\tLibrary\t–\tHaiti\tCollection’:\thttps://archive.org/details/jcbhaiti\t80\t‘Boston\tPublic\tLibrary\tAnti-Slavery\tCollection’:\thttps://archive.org/details/bplscas\t \n 89 For Whom Is This Useful? This intermediate lesson is good for users of the Programming Historian who have completed general lessons on downloading files and performing text analysis on them, but would like an applied example of these principles. It will also be of interest to historians or archivists who work with the MARC format or the Internet Archive on a regular basis. Before You Begin We will be working with two Python modules that are not included in Python's standard library. The first, internetarchive,81 provides programmatic access to the Internet Archive. The second, pymarc,82 makes it easier to parse MARC records. The easiest way to download both is to use pip, the python package manager. Begin by installing pip using Fred Gibbs’ ‘Installing Python Modules with pip’ tutorial.83 Then issue these commands at the command line: To install internetarchive: sudo\tpip\tinstall\tinternetarchive\tTo install pymarc: sudo\tpip\tinstall\tpymarc\tNow you are ready to go to work! The Antislavery Collection at the Internet Archive The Boston Public Library's anti-slavery collection at Copley Square contains not only the letters of William Lloyd Garrison, one of the icons of the American abolitionist movement, but also large collections of letters by and to reformers somehow connected to him. And by \"large collection,\" I mean large. According to the library's estimates, there are over 16,000 items at Copley. As of this writing, approximately 7,000 of those items have been digitized and uploaded to the Internet Archive. This is good news, not only because the Archive is committed to making its considerable cultural resources free for download, but also because each uploaded item is paired with a wealth of metadata suitable for machine-reading.                                                         81\t‘internetarchive’\tPython:\thttps://pypi.python.org/pypi/internetarchive\t82\t‘pymarc’\tPython:\thttps://pypi.python.org/pypi/pymarc/\t83\tFred\tGibbs,\t‘Installing\tPython\tModules\twith\tpip’,\tThe\tProgramming\tHistorian\t(2013).\t \n 90 Take a letter sent by Frederick Douglass to William Lloyd Garrison.84 Anyone can read the original manuscript online,85 without making the trip to Boston, and that alone may be enough to revolutionize and democratize future abolitionist historiography. But you can also download multiple files related to the letter that are rich in metadata,86 like a Dublin Core record87 and a fuller MARCXML record88 that uses the Library of Congress's MARC 21 Format for Bibliographic Data.89 Stop and think about that for a moment: every item uploaded from the Collection contains these things. Right now, that means historians have access to rich metadata, full images, and partial descriptions for thousands of antislavery letters, manuscripts, and publications.90 Accessing an IA Collection in Python Internet Archive (IA) collections and items all have a unique identifier, and URLs to collections and items all look like this: http://archive.org/details/[IDENTIFIER]\tSo, for example, here is a URL to the Archive item discussed above, Douglass's letter to Garrison: http://archive.org/details/lettertowilliaml00doug\tAnd here is a URL to the entire antislavery collection at the Boston Public Library: http://archive.org/details/bplscas/\tBecause the URLs are so similar, the only way to tell that you are looking at a collection page, instead of an individual item page, is to examine the page layout. An item page usually has a left-hand sidebar that says \"View the Book\" and lists links for reading the item online or accessing other file formats. A collection page will probably have a \"Spotlight Item\" in the lefthand sidebar instead. You can browse to different collections through                                                         84\t‘Letter\tfrom\tFrederick\tDouglass\tto\tWilliam\tLloyd\tGarrison,\t1846’,\tInternet\tArchive:\thttps://archive.org/details/lettertowilliaml00doug\t85\t‘Letter\tto\tWilliam\tLloyd\tGarrison\t–\tmanuscript’,\tInternet\tArchive:\thttp://archive.org/stream/lettertowilliaml00doug/39999066767938#page/n0/mode/2up\t86\t‘Index\tof\t/29/items/lettertowilliaml00doug/’,\tInternet\tArchive:\thttp://archive.org/download/lettertowilliaml00doug\t87\t‘Letter\tto\tWilliam\tLloyd\tGarrison\t–\tDublin\tCore’:\thttp://ia801703.us.archive.org/29/items/lettertowilliaml00doug/lettertowilliaml00doug_dc.xml\t88\t‘Letter\tto\tWilliam\tLloyd\tGarrison\t–\tMARCXML’:\thttp://ia801703.us.archive.org/29/items/lettertowilliaml00doug/lettertowilliaml00doug_marc.xml\t89\t‘MARC\t21\tFormat\tfor\tBibliographic\tData’,\tLibrary\tof\tCongress\t(1999):\thttp://www.loc.gov/marc/bibliographic/\t90\t‘Search\tbplscas’,\tInternet\tArchive:\thttp://archive.org/search.php?query=collection%3Abplscas&sort=-publicdate\t \n 91 the eBook and Texts portal,91 and you may also want to read a little bit about the way that items and item URLs are structured.92 Once you have a collection's identifier—in this case, bplscas—seeing all of the items in the collection is as easy as navigating to the Archive's advanced search page,93 selecting the id from the drop down menu next to \"Collection,\" and hitting the search button. Performing that search with bplscas selected, as of this writing showed 7,029 results. We can also search the Archive using the Python module that we installed,94 and doing so makes it easier to iterate over all the items in the collection for purposes of further inspection and downloading. For example, let's modify the sample code from the module's documentation to see if we can tell, with Python, how many items are in the digital Antislavery Collection. The sample code looks something like what you see below. The only difference is that instead of importing only the search_items module from internetarchive, we are going to import the whole library. import\tinternetarchive\tsearch\t=\tinternetarchive.search_items('collection:nasa')\tprint\tsearch.num_found\tAll we should need to modify is the collection identifier, from nasa to bplscas. After starting your computer's Python interpreter, try entering each of the above lines, followed by enter, but modify the collection id in the second command: search\t=\tinternetarchive.search_items('collection:bplscas')\tAfter hitting enter on the print command, you should see a number that matches the number of results you saw when doing the advanced search for the collection in the browser. Accessing an IA Item in Python The internetarchive module also allows you to access individual items using their identifiers. Let's try that using the documentation's sample code,95 modifying it in order to get the Douglass letter we discussed earlier. If you are still at your Python interpreter's command prompt, you don't need to import\tinternetarchive again. Since we imported the whole                                                         91\t‘eBooks\tand\tTexts’,\tInternet\tArchive:\thttps://archive.org/details/texts\t92\tAlexis\tRossi,\t‘How\tArchive.org\titems\tare\tstructured’,\tInternet\tArchive\tBlogs\t(31\tMarch\t2011):\thttp://blog.archive.org/2011/03/31/how-archive-org-items-are-structured/\t93\t‘Advanced\tSearch’,\tInternet\tArchive:\thttps://archive.org/advancedsearch.php\t94\t‘Searching\tfrom\tPython’,\tPython:\thttps://pypi.python.org/pypi/internetarchive#searching-from-python\t95\t‘Downloading\tfrom\tPython’,\tPython:\thttps://pypi.python.org/pypi/internetarchive#downloading-from-python\t \n 92 module, we also need to modify the sample code so that our interpreter will know that get_item is from the internetarchive module. We also need to change the sample identifier stairs to our item identifier, lettertowilliaml00doug (note that the character before the two zeroes is a lowercase L, not the number 1): item\t=\tinternetarchive.get_item('lettertowilliaml00doug')\titem.download()\tEnter each of those lines in your interpreter, followed by enter. Depending on your Internet connection speed, it will now probably take a minute or two for the command prompt to return, because your computer is downloading all of the files associated with that item, including some pretty large images. But when it's done downloading, you should be see a new directory on your computer whose name is the item identifier. To check, first exit your Python interpreter: exit()\tThen list the contents of the current directory to see if a folder now appears named lettertowilliaml00doug. If you list the contents of that folder, you should see a list of files similar to this: 39999066767938.djvu\t39999066767938.epub\t39999066767938.gif\t39999066767938.pdf\t39999066767938_abbyy.gz\t39999066767938_djvu.txt\t39999066767938_djvu.xml\t39999066767938_images.zip\t39999066767938_jp2.zip\t39999066767938_scandata.xml\tlettertowilliaml00doug_archive.torrent\tlettertowilliaml00doug_dc.xml\tlettertowilliaml00doug_files.xml\tlettertowilliaml00doug_marc.xml\tlettertowilliaml00doug_meta.mrc\tlettertowilliaml00doug_meta.xml\tlettertowilliaml00doug_metasource.xml\tNow that we know how to use the Search and Item functions in the internetarchive module, we can turn to thinking about how to make this process more effective for downloading lots of information from the collection for further analysis. Downloading MARC Records from a Collection Downloading one item is nice, but what if we want to look at thousands of items in a collection? We're in luck, because the internetarchive module's Search function allows us to iterate over all the results in a search. To see how, let's first start our Python interpreter again. We'll need to import our module again, and perform our search again:  \n 93 import\tinternetarchive\tsearch\t=\tinternetarchive.search_items('collection:bplscas')\tNow let's enter the documentation's sample code for printing out the item identifier of every item returned by our search: for\tresult\tin\tsearch:\t\t\t\tprint\tresult['identifier']\tNote that after entering the first line, your Python interpreter will automatically print an ellipsis on line two. This is because you have started a for loop, and Python is expecting there to be more. It wants to know what you want to do for each result in the search. That's also why, once you hit enter on the second line, you'll see a third line with another ellipsis, because Python doesn't know whether you are finished telling it what to do with each result. Hit enter again to end the for loop and execute the command. You should now see your terminal begin to print out the identifiers for each result returned by our bplscas search---in this case, all 7,029 of them! You can interrupt the print out by hitting Ctrl-C on your keyboard, which will return you to the prompt. If you didn't see identifiers printing out to your screen, but instead saw an error like this, you may have forgotten to enter a few spaces before your print command: for\tresult\tin\tsearch:\t\t\t\tprint\tresult['identifier']\tFile\t\"\",\tline\t2\t\t\t\tprint\tresult['identifier']\t\t\t\t\t\t\t^\tIndentationError:\texpected\tan\tindented\tblock\tRemember that whitespace matters in Python, and you need to indent the lines in a for loop so that Python can tell which command(s) to perform on each item in the loop. Understanding the for loop The for loop, expressed in plain English, tells Python to do something to each thing in a collection of things. In the above case, we printed the identifier for each result in the results of our collection search. Two additional points about the for loop: First, the word we used after for is what's called a local variable in Python. It serves as a placeholder for whatever instance or item we are going to be working with inside the loop. Usually it makes sense to pick a name that describes what kind of thing we are working with—in this case, a search result—but we could have used other names in place of that one. For example, try running the above for loop again, but substitute a different name for the local variable, such as:  \n 94 for\titem\tin\tsearch:\t\t\t\tprint\titem['identifier']\tYou should get the same results. The second thing to note about the for loop is that the indented block could could have contained other commands. In this case, we printed each individual search result's identifier. But we could have chosen to do, for each result, anything that we could do to an individual Internet Archive item. For example, earlier we downloaded all the files associated with the item lettertowilliaml00doug. We could have done that to each item returned by our search by changing the line print\tresult['identifier'] in our for loop to result.download(). We probably want to think twice before doing that, though—downloading all the files for each of the 7,029 items in the bplscas collection is a lot of files. Fortunately, the download function in the internetarchive module also allows you to download specific files associated with an item.96 If we had only wanted to download the MARC XML record associated with a particular item, we could have instead done this: item\t=\tinternetarchive.get_item('lettertowilliaml00doug')\tmarc\t=\titem.get_file('lettertowilliaml00doug_marc.xml')\tmarc.download()\tBecause Internet Archive item files are named according to specific rules,97 we can also figure out the name of the MARC file we want just by knowing the item's unique identifier. And armed with that knowledge, we can proceed to … Download All MARC XML Files from a Collection For the next section, we're going to move from using the Python shell to writing a Python script that downloads the MARC record from each item in the BPL Antislavery Collection. Try putting this script into Komodo or your preferred text editor:                                                              96\t‘Downloading\tfrom\tPython’,\tPython:\thttps://pypi.python.org/pypi/internetarchive#downloading-from-python\t97\t‘Frequently\tAsked\tQuestions’,\tInternet\tArchive:\thttps://archive.org/about/faqs.php#140\t \n 95 #!/usr/bin/python\t\timport\tinternetarchive\t\tsearch\t=\tinternetarchive.search_items('collection:bplscas')\t\tfor\tresult\tin\tsearch:\t\t\t\t\titemid\t=\tresult['identifier']\t\t\t\t\titem\t=\tinternetarchive.get_item(itemid)\t\t\t\t\tmarc\t=\titem.get_file(itemid\t+\t'_marc.xml')\t\t\t\t\tmarc.download()\t\t\t\t\tprint\t\"Downloading\t\"\t+\titemid\t+\t\"\t...\"\tThis script looks a lot like the experiments we have done above with the Frederick Douglass letter, but since we want to download the MARC record for each item returned by our collection search, we are using an itemid variable to account for the fact that the identifier and filename will be different for each result. Before running this script (which, I should note, is going to download thousands of small XML files to your computer), make a directory where you want those MARC records to be stored and place the above script in that directory. Then run the script from within the directory so that the files will be downloaded in an easy-to-find place. (Note that if you receive what looks like a ConnectionError on your first attempt, check your Internet connection, wait a few minutes, and then try running the script again.) If all goes well, when you run your script, you should see the program begin to print out status updates telling you that it is downloading MARC records. But allowing the script to run its full course will probably take a couple of hours, so let's stop the script and look a little more closely at ways to improve it. Pressing Ctrl-C while in your terminal window should make the script stop. Building Error Reporting into the Script Since downloading all of these records will take some time, we are probably going to want to walk away from our computer for a while. But the chances are high that during those two hours, something could go wrong that would prevent our script from working. Let's say, for example, that we had forgotten that we already downloaded an individual file into this directory. Or maybe your computer briefly loses its Internet connection, or some sort of outage happens on the Internet Archive server that prevents the script from getting the file it wants. In those and other error cases, Python will raise an \"exception\" telling you what the problem is. Unfortunately, an exception will also crash your script instead of continuing on to the next item.  \n 96 To prevent this, we can use what's called a try statement in Python, which does exactly what it sounds like. The statement will try to execute a certain snippet of code until it hits an exception, in which case you can give it some other code to execute instead. You can read more about handling exceptions in the Python documentation,98 but for now let's just update our above script so that it looks like this: #!/usr/bin/python\t\timport\tinternetarchive\timport\ttime\t\terror_log\t=\topen('bpl-marcs-errors.log',\t'a')\t\tsearch\t=\tinternetarchive.search_items('collection:bplscas')\t\tfor\tresult\tin\tsearch:\t\t\t\t\titemid\t=\tresult['identifier']\t\t\t\t\titem\t=\tinternetarchive.get_item(itemid)\t\t\t\t\tmarc\t=\titem.get_file(itemid\t+\t'_marc.xml')\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\tmarc.download()\t\t\t\t\texcept\tException\tas\te:\t\t\t\t\t\t\t\t\terror_log.write('Could\tnot\tdownload\t'\t+\titemid\t+\t'\tbecause\tof\terror:\t%s\\n'\t%\te)\t\t\t\t\t\t\t\t\tprint\t\"There\twas\tan\terror;\twriting\tto\tlog.\"\t\t\t\t\telse:\t\t\t\t\t\t\t\t\tprint\t\"Downloading\t\"\t+\titemid\t+\t\"\t...\"\t\t\t\t\t\t\t\t\ttime.sleep(1)\tThe main thing we've added here, after our module import statements, is a line that opens a text file called bpl-marcs-errors.log and prepares it to have text appended to it. We are going to use this file to log exceptions that the script raises. The try statement that we have added to our for loop will attempt to download the MARC record. If it can't, it will write a descriptive statement about what went wrong to our log file. That way we can go back to the file later and identify which items we will need to try to download again. If the try clause succeeds and can download the record, then the script will execute the code in the else clause. One other thing we have added, upon successful download, is this line: time.sleep(1)\tThis line uses the time module that we are now importing at the beginning to tell our script to pause for one second before proceeding, which is basically just a way for us to be nice to Internet Archive's servers by not clobbering them every millisecond or so with a request. Try updating your script to look like the above lines, and run it again in the directory where you want to store your MARC files. Don't be surprised if                                                         98\t‘Handling\tExceptions’,\tPython:\thttps://docs.python.org/2/tutorial/errors.html#handling-exceptions\t \n 97 you immediately encounter a string of error messages; that means the script is doing what it's supposed to do! Calmly go into your text editor, while leaving the script running, and open the bpl-marcs-errors.log to see what exceptions have been recorded there. You'll probably see that the script raised the exception \"File already exists\" for each of the files that you had already downloaded when running our earlier, shorter program. If you leave the program running for a little while, the script will eventually get to items that you have not already downloaded and resume collecting your MARCs! Scraping Information from a MARC Record Once your download script has completed, you should find yourself in the possession of nearly 7,000 detailed MARC XML records about items in the Anti-Slavery Collection (or whichever other collection you may have downloaded instead; the methods above should work on any collection whose items have MARC files attached to them). Now what? The next step depends on what sort of questions about the collection you want to answer. The MARC formatting language captures a wealth of data about an item, as you can see if you return to the MARC XML record for the Frederick Douglass letter mentioned at the outset.99 Notice, for example, that the Douglass letter contains information about the place where the letter was written in the datafield that is tagged 260, inside the subfield coded a. The person who prepared this MARC record knew to put place information in that specific field because of rules specified for the 260 datafield100 by the MARC standards.101 That means that it should be possible for us to look inside all of the MARC records we have downloaded, grab the information inside of datafield 260, subfield a, and make a list of every place name where items in the collection were published. To do this, we'll use the other helpful Python module that we downloaded with pip at the beginning: pymarc.102 That module makes it easy to get information out of subfields. Assuming that we have a MARC record prepared for parsing by the module assigned to the variable record, we could get the information about publication place names this way:                                                         99\t‘Letter\tto\tWilliam\tLloyd\tGarrison\t–\tMARC\tXML’,\tInternet\tArchive:\thttp://ia801703.us.archive.org/29/items/lettertowilliaml00doug/lettertowilliaml00doug_marc.xml\t100\t‘260\t–\tPublication,\tDistribution,\tetc.\t(Imprint)\t(R)’\tLibrary\tof\tCongress:\thttp://www.loc.gov/marc/bibliographic/bd260.html\t101\t‘MARC\tStandards’,\tLibrary\tof\tCongress:\thttp://www.loc.gov/marc/\t102\t‘pymarc’,\tGithub:\thttps://github.com/edsu/pymarc\t \n 98 place_of_pub\t=\trecord['260']['a']\tThe documentation for pymarc is a little less complete than that for the Internet Archive, especially when it comes to parsing XML records. But a little rooting around in the source code for the module reveals some functions that it provides for working with MARC XML records.103 One of these, called map_xml() is described this way: def\tmap_xml(function,\t*files):\t\t\t\t\t\"\"\"\t\t\t\t\tmap\ta\tfunction\tonto\tthe\tfile,\tso\tthat\tfor\teach\trecord\tthat\tis\t\t\t\t\tparsed\tthe\tfunction\twill\tget\tcalled\twith\tthe\textracted\trecord\t\t\t\t\t\t\t\t\t\tdef\tdo_it(r):\t\t\t\t\tprint\tr\t\t\t\t\t\t\t\t\t\tmap_xml(do_it,\t'marc.xml')\t\t\t\t\t\"\"\"\tTranslated into plain English, this function means that we can take an XML file containing MARC data (like the nearly 7,000 we now have on our computer), pass it to the map_xml function in the pymarc module, and then specify another function (that we will write) telling our program what to do with the MARC data retrieved from the XML file. In rough outline, our code will look something like this: import\tpymarc\t\tdef\tget_place_of_pub(record):\t\t\t\t\tplace_of_pub\t=\trecord['260']['a']\t\t\t\t\tprint\tplace_of_pub\t\tpymarc.map_xml(get_place_of_pub,\t'lettertowilliaml00doug_marc.xml')\tTry saving that code to a script and running it from a directory where you already have the Douglass letter XML saved. If all goes well, the script should spit out this: Belfast,\t[Northern\tIreland],\tVoila! Of course, this script would be much more useful if we scraped the place of publication from every letter in our collection of MARC records. Putting together what we've learned from earlier in the lesson, we can do that with a script that looks like this:                                                             103\t‘marcxml.py’,\tGithub:\thttps://github.com/edsu/pymarc/blob/master/pymarc/marcxml.py\t \n 99 #!/usr/bin/python\t\timport\tos\timport\tpymarc\t\tpath\t=\t'/path/to/dir/with/xmlfiles/'\t\tdef\tget_place_of_pub(record):\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\tplace_of_pub\t=\trecord['260']['a']\t\t\t\t\t\t\t\t\tprint\tplace_of_pub\t\t\t\t\texcept\tException\tas\te:\t\t\t\t\t\t\t\t\tprint\te\t\tfor\tfile\tin\tos.listdir(path):\t\t\t\t\tif\tfile.endswith('.xml'):\t\t\t\t\t\t\t\t\tpymarc.map_xml(get_place_of_pub,\tpath\t+\tfile)\tThis script modifies our above code in several ways. First, it uses a for loop to iterate over each file in our directory. In place of the internetarchive search results that we iterated over in our first part of this lesson, we iterate over the files returned by os.listdir(path) which uses the built-in Python module os to list the contents of the directory specified in the path variable, which you will need to modify so that it matches the directory where you have downloaded all of your MARC files. We have also added some error handling to our get_place_of_pub() function to account for the fact that some records may (for whatever reason) not contain the information we are looking for. The function will try to print the place of publication, but if this raises an Exception, it will print out the information returned by the Exception instead. In this case, if the try statement failed, the exception will probably print None. Understanding why is a subject for another lesson on Python Type errors, but for now the None printout is descriptive enough of what happened, so it could be useful to us. Try running this script. If all goes well, your screen should fill with a list of the places where these letters were written. If that works, try modifying your script so that it saves the place names to a text file instead of printing them to your screen. You could then use the Counting Frequencies with Python lesson by Turkel and Crymble,104 to figure out which place names are most common in the collection. You could work with the place names to find coordinates that could be placed on a map using the ‘Intro to Google Maps and Google Earth’ tutorial by Clifford et al.105                                                         104\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t105\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDaniel\tMacfarlane,\t‘Intro\tto\tGoogle\tMaps\tand\tGoogle\tEarth’,\tThe\tProgramming\tHistorian\t(2013).\t \n 100 Or, to get a very rough visual sense of the places where letters were written, you could do what I've done below and simply make a Wordle word cloud of the text file.106 \n Wordle wordcloud of places of publication for abolitionist letters  Of course, to make such techniques useful would require more cleaning of your data, such as those discussed by Laura Turner O’Hara in ‘Cleaning OCR’d text with Regular Expressions’.107 And other applications of this lesson may prove more useful. For example, working with the MARC data fields for personal names, you could create a network of correspondents. Or you could analyze which subjects are common in the MARC records. Now that you have the MARC records downloaded and can use pymarc to extract information from the fields, the possibilities can multiply rapidly! About the Author Caleb McDaniel is an associate professor of history at Rice University. \n                                                        106\t‘Wordle’:\thttp://www.wordle.net/\t107\tLaura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t\n \n 101 12. Using SPARQL to access Linked Open Data Matthew Lincoln – 2015    Lesson Goals This lesson explains why many cultural institutions are adopting graph databases, and how researchers can access these data though the query language called SPARQL. Table of contents Graph Databases, RDF, and Linked Open Data RDF in brief Searching RDF with SPARQL URIs and Literals Terms to review Real-world queries All the statements for one object Complex queries FILTER Aggregation Linking multiple SPARQL endpoints Working with SPARQL results Export results to CSV Export results to Palladio Further reading Graph Databases, RDF, and Linked Open Data Many cultural institutions now offer access to their collections information through web Application Programming Interfaces.108 While these APIs are a powerful way to access individual records in a machine-readable manner, they are not ideal for cultural heritage data because they are structured to work for a predetermined set of queries. For example, a museum may have information on donors, artists, artworks, exhibitions, and provenance, but                                                         108\tFor\texample,\tsee:\tAmanda\tMorton,\t‘Intro\tto\tthe\tZotero\tAPI’,\tThe\tProgramming\tHistorian\t(2013).\t \n 102 its web API may offer only object-wise retrieval, making it difficult or impossible to search for associated data about donors, artists, provenance, etc. This structure is great if you come looking for information about particular objects. However, it makes it difficult to aggregate information about every artist or donor that happens to be described in the dataset as well. RDF databases are well-suited to expressing complex relationships between many entities, like people, places, events, and concepts tied to individual objects. These databases are often referred to as \"graph\" databases because they structure information as a graph or network, where a set of resources, or nodes, are connected together by edges that describe the relationships between each resource. Because RDF databases support the use of URLs (weblinks), they can be made available online and linked to other databases, hence the term \"Linked Open Data\". Major art collections including the British Museum,109 Europeana,110 the Smithsonian American Art Museum,111 and the Yale Center for British Art112 have published their collections data as LOD. The Getty Vocabulary Program,113 has also released their series of authoritative databases on geographic place names, terms for describing art and architecture, and variant spellings of artist names, as LOD. SPARQL is the language used to query these databases. This language is particularly powerful because it does not presuppose the perspectives that users will bring to the data. A query about objects and a query about donors is basically equivalent to such a database. Unfortunately, many tutorials on SPARQL use extremely simplified data models that don't resemble the more complex datasets released by cultural heritage institutions. This tutorial gives a crash course on SPARQL using a dataset that a humanist might actually find in the wilds of the Internet. In this tutorial, we will learn how to query the British Museum Linked Open Data collection. RDF in brief RDF represents information in a series of three-part \"statements\" that comprise a subject, predicate, and an object, e.g.: <The\tNightwatch>\t<was\tcreated\tby>\t<Rembrandt\tvan\tRijn>\t.\t(Note that just like any good sentence, they each have a period at the end.)                                                         109\t‘British\tMuseum\tSemantic\tWeb\tCollection\tOnline’,\tBritish\tMuseum:\thttp://collection.britishmuseum.org/\t110\t‘Europeana\tLinked\tOpen\tData’\tEuropeana\tLabs:\thttp://labs.europeana.eu/api/linked-open-data-introduction\t111\t‘Smithsonian\tAmerican\tArt\tMuseum’:\thttp://americanart.si.edu/\t112\t‘Yale\tCentre\tfor\tBritish\tArt’:\thttp://britishart.yale.edu/collections/using-collections/technology/linked-open-data\t113\t‘The\tGetty\tVocabularies’:\thttp://vocab.getty.edu/\t \n 103 Here, the subject <The\tNightwatch> and the object <Rembrandt\tvan\tRijn> can be thought of as two nodes of the graph, with the predicate <was\tcreated\tby> defining an edge between them. (Technically, <was\tcreated\tby> can, in other queries, be treated as an object or subject itself, but that is beyond the scope of this tutorial.) A pseudo-RDF database might contain interrelated statements like these: ...\t<The\tNightwatch>\t<was\tcreated\tby>\t<Rembrandt\tvan\tRijn>\t.\t<The\tNightwatch>\t<was\tcreated\tin>\t<1642>\t.\t<The\tNightwatch>\t<has\tmedium>\t<oil\ton\tcanvas>\t.\t<Rembrandt\tvan\tRijn>\t<was\tborn\tin>\t<1606>\t.\t<Rembrandt\tvan\tRijn>\t<has\tnationality>\t<Dutch>\t.\t<Johannes\tVermeer>\t<has\tnationality>\t<Dutch>\t.\t<Woman\twith\ta\tBalance>\t<was\tcreated\tby>\t<Johannes\tVermeer>\t.\t<Woman\twith\ta\tBalance>\t<has\tmedium>\t<oil\ton\tcanvas>\t.\t...\tIf we were to visualize these statements as nodes and edges within network graph, it would appear like so: \n A network visualization of the pseud-RDF shown above. Arrows indicate the 'direction' of the predicate. For example, that 'Woman with a Balance' was created by Vermeer', and not the other way around. A traditional relational database might split attributes about artworks and attributes about artists into separate tables. In an RDF/graph database, all these data points belong to the same interconnected graph, which allows users maximum flexibility in deciding how they wish to query it. Searching RDF with SPARQL SPARQL lets us translate heavily interlinked, graph data into normalized, tabular data with rows and columns you can open in programs like Excel, or import into a visualization suite such as plot.ly or Palladio.114                                                         114\t‘Plotly’:\thttps://plot.ly/;\t‘Palladio’:\thttp://palladio.designhumanities.org/\t\n \n 104 It is useful to think of a SPARQL query as a Mad Lib - a set of sentences with blanks in them.115 The database will take this query and find every set of matching statements that correctly fill in those blanks, returning the matching values to us as a table. Take this SPARQL query: SELECT\t?painting\tWHERE\t{\t\t\t?painting\t<has\tmedium>\t<oil\ton\tcanvas>\t.\t}\t?painting in this query stands in for the node (or nodes) that the database will return. On receiving this query, the database will search for all values of ?painting that properly complete the RDF statement <has\tmedium>\t<oil\ton\tcanvas>\t.: \n A visualization of what our query is looking for When the query runs against the full database, it looks for the subjects, predicates, and objects that match this statement, while excluding the rest of the data: \n A visualization of the SPARQL query.                                                           115\t‘Mad\tLibs’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Mad_Libs\t\n \n 105 And our results might look like this table: painting The Nightwatch Woman with a Balance What makes RDF and SPARQL powerful is the ability to create complex queries that reference many variables at a time. For example, we could search our pseudo-RDF database for paintings by any artist who is Dutch: SELECT\t?artist\t?painting\tWHERE\t{\t\t\t?artist\t<has\tnationality>\t<Dutch>\t.\t\t\t?painting\t<was\tcreated\tby>\t?artist\t.\t}\tHere we've introduced a second variable, ?artist. The RDF database will return all matching combinations of ?artist and ?painting that fulfill both of these statements. \n A visualization of the SPARQL query.  artist painting Rembrandt van Rijn The Nightwatch Johannes Vermeer Woman with a Balance URIs and Literals So far, we have been looking at a toy representation of RDF that uses easy-to-read text. However, RDF is primarily stored as URIs (Uniform Resource Identifiers) that separate conceptual entities from their plain-English (or other language!) labels. (Note that a URL, or Uniform Resource Locator, is a URI for a resource that is accessible on the web) In real RDF, our original statement: <The\tNightwatch>\t\t\t<was\tcreated\tby>\t\t\t<Rembrandt\tvan\tRijn>\t.\t\n \n 106 would more likely look something like this: <http://data.rijksmuseum.nl/item/8909812347>\t<http://purl.org/dc/terms/creator>\t\t<http://dbpedia.org/resource/Rembrandt>.\tN.B. the Rijksmuseum has not (yet) built their own Linked Data site, so the URI in this query is just for demo purposes. In order to get the human-readable version of the information represented by each of these URIs, what we're really doing is just retrieving more RDF statements. Even the predicate in that statement has its own literal label: <http://data.rijksmuseum.nl/item/8909812347>\t<http://purl.org/dc/terms/title>\t\"The\tNightwatch\"\t.\t\t<http://purl.org/dc/terms/creator>\t<http://www.w3.org/1999/02/22-rdf-syntax-ns#label>\t\"was\tcreated\tby\"\t.\t\t<http://dbpedia.org/resource/Rembrandt>\t<http://xmlns.com/foaf/0.1/name>\t\"Rembrandt\tvan\tRijn\"\t.\tYou will notice that, unlike the URIs in the query that are surrounded by <>, the objects of these statements are just strings of text within quotation marks, known as literals. Literals are unlike URIs in that they represent values, rather than references. For example: <http://dbpedia.org/resource/Rembrandt> represents an entity that may reference (and be referenced by) any number of other statements (say, birth dates, students, or family members), while the text string \"Rembrandt\tvan\tRijn\" stands only for itself. Literals do not point to other nodes in the graph, and they can only ever be objects in an RDF statement. Other literal values in RDF include dates and numbers. See the predicates in these statements, with domain names like purl.org, w3.org, and xmlns.com? These are some of the many providers of ontologies that help standardize the way we describe relationships between bits of information like \"title\", \"label\", \"creator\", or \"name\". The more RDF/LOD that you work with, the more of these providers you'll find. URIs can become unwieldy when composing SPARQL queries, which is why we'll use prefixes. These are shortcuts that allow us to skip typing out entire long URIs. For example, remember that predicate for retrieving the title of the Nightwatch, <http://purl.org/dc/terms/title>? With these prefixes, we just need to type dct:title whenever we need to use a purl.org predicate. dct: stands in for http://purl.org/dc/terms/, and title just gets pasted onto the end of this link. For example, with the prefix PREFIX\trkm:\t<http://data.rijksmuseum.nl/>, appended to the start of our SPARQL query, <http://data.rijksmuseum.nl/item/8909812347> becomes rkm:item/8909812347 instead.  \n 107 Many live databases, such as the British Museum database that we will try our first new queries on, already define these prefixes for us, so we won't have to explicitly state them in our queries. However, you should now be able to recognize whenever we use one in a SPARQL query. Also be aware that, prefixes can be arbitrarily assigned with whatever abbreviations you like, different endpoints may use slightly different prefixes for the same namespace (e.g. dct vs. dcterms for <http://purl.org/dc/terms/>). Terms to review SPARQL - Protocol and RDF Query Language - The language used to query RDF graph databases RDF - Resource Description Framework - A method for structuring data as a graph or network of connected statements, rather than a series of tables. LOD - Linked Open Data - LOD is RDF data published online with dedicated URIs in such a manner than developers can reliably reference it. statement - Sometimes also called a \"triple\", an RDF statement is a quantum of knowledge comprising a subject, predicate, and object. URI - Uniform Resource Identifier - a string of characters for identifying a resource. RDF statements use URIs to link various resources together. A URL, or uniform resource locator, is a type of URI that points to resources on the web. literal - Some objects in RDF statements do not refer to other resources with a URI, but instead convey a value, such as text (\"Rembrandt\tvan\tRijn\"), a number (5), or a date (1606-06-15). These are known as literals. prefix - In order to simplify SPARQL queries, a user may specify prefixes that act as abbreviations for full URIs. These abbreviations, or QNames, are also used in namespaced XML documents. Real-world queries All the statements for one object Let's start our first query using the British Museum SPARQL endpoint.116 A SPARQL endpoint is a web address that accepts SPARQL queries and returns results. The BM endpoint is like many others: if you navigate to it in a web browser, it presents you with a text box for composing queries. \n                                                         116\t‘SPARQL\tQuery’,\tThe\tBritish\tMuseum:\thttp://collection.britishmuseum.org/sparql\t\n \n 108 The BM SPARQL endpoint webpage. For all the queries in this tutorial, make sure that you have left the 'Include inferred' and 'Expand results over equivalent URIs' boxes unchecked. When starting to explore a new RDF database, it helps to look at the relationships that stem from a single example object.117 (For each of the following queries, click on the \"Run query\" link below to see the results. Click on the \"Edit query\" link to go to a page with the query automatically pasted the query into the BM's endpoint. You can then run it as is, or modify it before requesting the results. Remember when editing the query before running to uncheck the 'Include inferred' box.) SELECT\t?p\t?o\tWHERE\t{\t\t\t<http://collection.britishmuseum.org/id/object/PPA82633>\t?p\t?o\t.\t}\tRun query118 / Edit query119 By calling SELECT\t?p\t?o we're asking the database to return the values of ?p and ?o as described in the WHERE\t{} command. This query returns every statement for which our example artwork, <http://collection.britishmuseum.org/id/object/PPA82633>, is the subject. \n                                                         117\tThe\tExample\tObject\tis:\t‘PPA82633’,\tThe\tBritish\tMuseum:\thttp://collection.britishmuseum.org/resource?uri=http://collection.britishmuseum.org/id/object/PPA82633\t118\tTo\trun\tthe\tquery,\tvisit:\thttp://collection.britishmuseum.org/sparql?query=SELECT+*%0D%0AWHERE+{%0D%0A++%3Chttp%3A%2F%2Fcollection.britishmuseum.org%2Fid%2Fobject%2FPPA82633%3E+%3Fp+%3Fo+.%0D%0A++}&_implicit=false&_equivalent=false&_form=%2Fsparql\t119\tTo\tedit\tthe\tquery,\tvisit:\thttp://collection.britishmuseum.org/sparql?sample=SELECT+*%0D%0AWHERE+{%0D%0A++%3Chttp%3A%2F%2Fcollection.britishmuseum.org%2Fid%2Fobject%2FPPA82633%3E+%3Fp+%3Fo+.%0D%0A++}\t\n \n 109 An initial list of all the predicates and objects associated with one artwork in the British Museum. The BM endpoint formats the results table with hyperlinks for every variable that is itself an RDF node, so by clicking on any one of these links you can shift to seeing all the predicates and objects for that newly-selected node. Note that BM automatically includes a wide range of SPARQL prefixes in its queries, so you will find many hyperlinks are displayed in their abbreviated versions; if you mouse over them your browser will display their unabbreviated URIs. \n Visualizing a handful of the nodes returned by the first query to the BM. Additional levels in the hierarchy are included as a preview of how this single print connects to the larger BM graph. Let's find out how they store the object type information: look for the predicate <bmo:PX_object_type> (highlighted in the figure above) and click on the link for thes:x8577 to navigate to the node describing the particular object type \"print\":  The resource page for thes:x8577 ('print') in the British Museum LOD You'll note how this node has an plain-text label, as well as ties to related artwork type nodes within the database. Complex queries To find other objects of the same type with the preferred label \"print\", we can call this query: \n \n 110 SELECT\t?object\tWHERE\t{\t\t\t\t#\tSearch\tfor\tall\tvalues\tof\t?object\tthat\thave\ta\tgiven\t\"object\ttype\"\t\t\t?object\tbmo:PX_object_type\t?object_type\t.\t\t\t\t#\tThat\tobject\ttype\tshould\thave\tthe\tlabel\t\"print\"\t\t\t?object_type\tskos:prefLabel\t\"print\"\t.\t}\tRun query120 / Edit query121 \n A one-column table returned by our query for every object with type 'print' Remember that, because \"print\" here is a literal, we enclose it within quotation marks in our query. When you include literals in a SPARQL query, the database will only return exact matches for those values. Note that, because ?object_type is not present in the SELECT command, it will not show up in the results table. However, it is essential to structuring our query, because it connects the dots from ?object to the label \"print\". FILTER In the previous query, our SPARQL query searched for an exact match for the object type with the text label \"print\". However, often we want to match literal values that fall within a certain range, such as dates. For this, we'll use the FILTER command. To find URIs for all the prints in the BM created between 1580 and 1600, we'll need to first figure out where the database stores dates in relationship                                                         120\tTo\trun\tthe\tquery,\tvisit:\thttp://collection.britishmuseum.org/sparql?query=SELECT+%3Fobject%0D%0AWHERE+{%0D%0A++%3Fobject+bmo%3APX_object_type+%3Fobject_type+.%0D%0A++%3Fobject_type+skos%3AprefLabel+%22print%22+.%0D%0A}&_implicit=false&implicit=false&_equivalent=false&_form=%2Fsparql\t(note,\tyou\tcan\talso\tfind\tthis\tas\ta\tclickable\tlink\ton\tthe\tonline\tversion\tof\tthe\ttutorial)\t121\tTo\tedit\tthe\tquery,\tvisit:\thttp://collection.britishmuseum.org/sparql?sample=PREFIX+bmo%3A+%3Chttp%3A%2F%2Fcollection.britishmuseum.org%2Fid%2Fontology%2F%3E%0APREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E%0ASELECT+%3Fobject%0D%0AWHERE+{%0D%0A++%3Fobject+bmo%3APX_object_type+%3Fobject_type+.%0D%0A++%3Fobject_type+skos%3AprefLabel+%22print%22+.%0D%0A}\t(note,\tyou\tcan\talso\tfind\tthis\tas\ta\tclickable\tlink\ton\tthe\tonline\tversion\tof\tthe\ttutorial)\t\n \n 111 to the object node, and then add references to those dates in our query. Similar to the way that we followed a single link to determine an object type, we must hop through several nodes to find the production dates associated with a given object: \n Visualizing part of the British Museum's data model where production dates are connected to objects. #\tReturn\tobject\tlinks\tand\tcreation\tdate\tSELECT\t?object\t?date\tWHERE\t{\t\t\t\t#\tWe'll\tuse\tour\tprevious\tcommand\tto\tsearch\tonly\tfor\t\t\t#\tobjects\tof\ttype\t\"print\"\t\t\t?object\tbmo:PX_object_type\t?object_type\t.\t\t\t?object_type\tskos:prefLabel\t\"print\"\t.\t\t\t\t#\tWe\tneed\tto\tlink\tthough\tseveral\tnodes\tto\tfind\tthe\t\t\t#\tcreation\tdate\tassociated\twith\tan\tobject\t\t\t?object\tecrm:P108i_was_produced_by\t?production\t.\t\t\t?production\tecrm:P9_consists_of\t?date_node\t.\t\t\t?date_node\tecrm:P4_has_time-span\t?timespan\t.\t\t\t?timespan\tecrm:P82a_begin_of_the_begin\t?date\t.\t\t\t\t#\tAs\tyou\tcan\tsee,\twe\tneed\tto\tconnect\tquite\ta\tfew\tdots\t\t\t#\tto\tget\tto\tthe\tdate\tnode!\tNow\tthat\twe\thave\tit,\twe\tcan\t\t\t#\tfilter\tour\tresults.\tBecause\twe\tare\tfiltering\tby\tdate,\t\t\t#\twe\tmust\tattach\tthe\ttag\t^^xsd:date\tafter\tour\tdate\tstrings.\t\t\t#\tThis\ttag\ttells\tthe\tdatabase\tto\tinterpret\tthe\tstring\t\t\t#\t\"1580-01-01\"\tas\tthe\tdate\t1\tJanuary\t1580.\t\t\t\tFILTER(?date\t>=\t\"1580-01-01\"^^xsd:date\t&&\t\t\t\t\t\t\t\t\t\t?date\t<=\t\"1600-01-01\"^^xsd:date)\t}\t\n \n 112 Run query / Edit query122  \n ALL BM prints made between 1580 and 1600 Aggregation So far we have only used the SELECT command to return a table of objects. However, SPARQL allows us to do more advanced analysis such as grouping, counting, and sorting. Say we would like to keep looking at objects made between 1580 and 1600, but we want to understand how many objects of each type the BM has in its collections. Instead of limiting our results to objects of type \"print\", we will instead use COUNT to tally our search results by type.   SELECT\t?type\t(COUNT(?type)\tas\t?n)\tWHERE\t{\t\t\t#\tWe\tstill\tneed\tto\tindicate\tthe\t?object_type\tvariable,\t\t\t#\thowever\twe\twill\tnot\trequire\tit\tto\tmatch\t\"print\"\tthis\ttime\t\t\t\t?object\tbmo:PX_object_type\t?object_type\t.\t\t\t?object_type\tskos:prefLabel\t?type\t.\t\t\t\t#\tOnce\tagain,\twe\twill\talso\tfilter\tby\tdate\t\t\t?object\tecrm:P108i_was_produced_by\t?production\t.\t\t\t?production\tecrm:P9_consists_of\t?date_node\t.\t\t\t?date_node\tecrm:P4_has_time-span\t?timespan\t.\t\t\t?timespan\tecrm:P82a_begin_of_the_begin\t?date\t.\t\t\tFILTER(?date\t>=\t\"1580-01-01\"^^xsd:date\t&&\t\t\t\t\t\t\t\t\t\t?date\t<=\t\"1600-01-01\"^^xsd:date)\t                                                        122\tThe\tlinks\tto\trun\tand\tedit\tthe\tquery\tcan\tbe\tfound\tin\tthe\tonline\tversion\tof\tthe\ttutorial.\tTrust\tus,\tyou\twouldn’t\twant\tto\thave\tto\ttype\tit\tout\tby\thand:\thttp://programminghistorian.org/lessons/graph-databases-and-SPARQL\t\n \n 113 }\t#\tThe\tGROUP\tBY\tcommand\tdesignates\tthe\tvariable\tto\ttally\tby,\t#\tand\tthe\tORDER\tBY\tDESC()\tcommand\tsorts\tthe\tresults\tby\t#\tdescending\tnumber.\tGROUP\tBY\t?type\tORDER\tBY\tDESC(?n)\tRun query / Edit query123  \n Counts of objects by type produced between 1580 and 1600  Linking multiple SPARQL endpoints Up until now, we have constructed queries that look for patterns in one dataset alone. In the ideal world envisioned by Linked Open Data advocates, multiple databases can be interlinked to allow very complex queries dependent on knowledge present in different locations. However, this is easier said than done, and many endpoints (the BM's included) do not yet reference outside authorities. One endpoint that does, however, is Europeana's.124 They have created links between the objects in their database and records about individuals in DBPedia125 and VIAF,126 places in GeoNames,127 and concepts in the Getty Art & Architecture thesaurus. SPARQL allows you to insert SERVICE statements that instruct the database to \"phone a friend\" and run a portion of the query on an outside dataset, using the results to complete the query                                                         123\tThe\tlinks\tto\trun\tand\tedit\tthe\tquery\tcan\tbe\tfound\tin\tthe\tonline\tversion\tof\tthe\ttutorial.\tTrust\tus,\tyou\twouldn’t\twant\tto\thave\tto\ttype\tit\tout\tby\thand:\thttp://programminghistorian.org/lessons/graph-databases-and-SPARQL\t124\t‘SPARQL\tQueries’\tEuropeana:\thttp://europeana.ontotext.com/sparql\t125\t‘DBPedia’:\thttp://wiki.dbpedia.org/\t126\t‘VIAF:\tThe\tVirtual\tInternational\tAuthority\tFile’:\thttps://viaf.org/\t127\t‘GeoNames’:\thttp://sws.geonames.org/\t\n \n 114 on the local dataset. While this lesson will go into the data models in Europeana and DBpedia in depth, the following query illustrates how a SELECT statement works. You may run it yourself by copying and pasting the query text into the Europeana endpoint. PREFIX\tedm:\t\t\t\t<http://www.europeana.eu/schemas/edm/>\tPREFIX\trdf:\t\t\t\t<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\tPREFIX\tdbo:\t\t\t\t<http://dbpedia.org/ontology/>\tPREFIX\tdbr:\t\t\t\t<http://dbpedia.org/resource/>\tPREFIX\trdaGr2:\t<http://rdvocab.info/ElementsGr2/>\t\t#\tFind\tall\t?object\trelated\tby\tsome\t?property\tto\tan\t?agent\tborn\tin\ta\t#\t?dutch_city\tSELECT\t?object\t?property\t?agent\t?dutch_city\tWHERE\t{\t\t\t\t\t?proxy\t?property\t?agent\t.\t\t\t\t\t?proxy\tore:proxyFor\t?object\t.\t\t\t\t\t\t?agent\trdf:type\tedm:Agent\t.\t\t\t\t\t?agent\trdaGr2:placeOfBirth\t?dutch_city\t.\t\t\t\t\t\t#\t?dutch_city\tis\tdefined\tby\thaving\t\"Netherlands\"\tas\tits\tbroader\t\t\t\t\t#\tcountry\tin\tDBpedia.\tThe\tSERVICE\tstatement\tasks\t\t\t\t\t#\thttp://dbpdeia.org/sparql\twhich\tcities\thave\tthe\tcountry\t\t\t\t\t#\t\"Netherlands\".\tThe\tanswers\tto\tthat\tsub-query\twill\tthen\tbe\t\t\t\t\t#\tused\tto\tfinish\toff\tour\toriginal\tquery\tabout\tobjects\tin\tthe\t\t\t\t\t#\tEuropeana\tdatabase\t\t\t\t\t\tSERVICE\t<http://dbpedia.org/sparql>\t{\t\t\t\t\t\t\t\t\t?dutch_city\tdbo:country\tdbr:Netherlands\t.\t\t\t\t}\t}\t#\tThis\tquery\tcan\tpotentially\treturn\ta\tlot\tof\tobjects,\tso\tlet's\t#\tjust\trequest\tthe\tfirst\t100\tin\torder\tto\tspeed\tup\tthe\tsearch\tLIMIT\t100\t\n Visualizing the query sequence of the above SPARQL request \n \n 115 An interlinked query like this means that we can ask Europeana questions about its objects that rely on information about geography (what cities are in the Netherlands?) that Europeana does not need to store and maintain itself. In the future, more cultural LOD will hopefully link to authority databases like the Getty's Union List of Artist Names, allowing, for example, the British Museum to outsource biographical data to the more complete resources at the Getty. Working with SPARQL results Having constructed and run a query... what do we do with the results? Many endpoints offer, like the British Museum, a web-based browser that returns human-readable results. However, SPARQL endpoints are designed to return structured data to be used by other programs. Export results to CSV In the top right corner of the results page for the BM endpoint, you will find links for both JSON and XML downloads. Other endpoints may also offer the option for a CSV/TSV download, however this option is not always available. The JSON and XML output from a SPARQL endpoint contain not only the values returned from the SELECT statement, but also additional metadata about variable types and languages. Parsing the XML verson of this output may be done with a tool like Beautiful Soup (see Wieringa’s Programming Historian lesson)128 or Open Refine.129 To quickly convert JSON results from a SPARQL endpoint into a tabular format, I recommend the free command line utility jq.130 (For a tutorial on using command line programs, see \"Introduction to the Bash Command Line\".)131 The following query will convert the special JSON RDF format into a CSV file, which you may load into your preferred program for further analysis and visualization: jq\t-r\t'.head.vars\tas\t$fields\t|\t($fields\t|\t@csv),\t(.results.bindings[]\t|\t[.[$fields[]].value]\t|\t@csv)'\tsparql.json\t>\tsparql.csv\tExport results to Palladio The popular data exploration platform Palladio132 can directly load data from a SPARQL endpoint. On the \"Create a new project\" screen, a link at the bottom to \"Load data from a SPARQL endpoint (beta)\" will provide you a field to enter the endpoint address, and a box for the query itself.                                                         128\tJeri\tWieringa,\t‘Intro\tto\tBeautiful\tSoup’,\tThe\tProgramming\tHistorian\t(2012).\t129\t‘Open\tRefine’:\thttp://openrefine.org/\t130\t‘Download\tjq’:\thttps://stedolan.github.io/jq/download/\t131\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorian\t(2014).\t132\t‘Palladio’:\thttp://palladio.designhumanities.org/#/\t \n 116 Depending on the endpoint, you may need to specify the file output type in the endpoint address; for example, to load data from the BM endpoint you must use the address http://collection.britishmuseum.org/sparql.json. Try pasting in the aggregation query we used above to count artworks by type and clicking on \"Run query\". Palladio should display a preview table. \n Palladio's SPARQL query interface  After previewing the data returned by the endpoint, click on the \"Load data\" button at the bottom of the screen to begin manipulating it. (See Düring’s lesson on networks for a more in-depth tutorial on Palladio.)133 For example, we might make a query that returns links to the images of prints made between 1580 and 1600, and render that data as a grid of images sorted by date:134 \n                                                        133\tMartin\tDüring, ‘From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources’, The Programming Historian (2015): http://programminghistorian.org/lessons/creating-network-diagrams-from-historical-sources.html#visualize-network-data-in-palladio\t134\tLink\tavailable\tfor\tthis\tquery\ton\tthe\tonline\tversion\tof\tthe\ttutorial.\tTrust\tus,\tyou\twouldn’t\twant\tto\ttype\tit\tout:\thttp://programminghistorian.org/lessons/graph-databases-and-SPARQL\t\n \n 117 \n A gallery of images with a timeline of their creation dates generated using Palladio Note that Palladio is designed to work with relatively small amounts of data (on the order of hundreds or thousands of rows, not tens of thousands), so you may have to use the LIMIT command that we used when querying the Europeana endpoint to reduce the number of results that you get back, just to keep the software from freezing. Further reading In this tutorial we got a look at the structure of LOD as well as a real-life example of how to write SPARQL queries for the British Museum's database. You also learned how to use aggregation commands in SPARQL to group, count, and sort results rather than simply list them. There are even more ways to modify these queries, such as introducing OR and UNION statements (for describing conditional queries), and CONSTRUCT statements (for inferring new links based on defined rules), full-text searching, or doing other mathematical operations more complex than counting. For a more complete rundown of the commands available in SPARQL, see these resources: How to SPARQL: http://rdf.myexperiment.org/howtosparql? Wikibooks SPARQL tutorial: https://en.wikibooks.org/wiki/XQuery/SPARQL_Tutorial Both the Europeana and Getty Vocabularies LOD sites also offer extensive, and quite complex example queries which can be good sources for understanding how to search their data: Europeana SPARQL how-to: http://labs.europeana.eu/api/linked-open-data-SPARQL-endpoint \n \n 118 Getty Vocabularies Example Queries: http://vocab.getty.edu/queries  About the Author Matthew Lincoln is a PhD candidate at the University of Maryland, College Park. He is interested in the potential for computer-aided analysis of cultural datasets to help model long-term artistic trends in iconography, art markets, and social relations between artists in the early modern period.                 \n 119  Part Three: Transforming Data Once you have data, you’ll almost invariably have to transform it in some way. Sometimes this is ‘cleaning’ messy data. Other times it’s just changing it into a format that’s useable in your analysis. Knowing how to work with data systematically and at scale can save you weeks worth of work.  \n 120 13. Working with Text Files in Python William J. Turkel and Adam Crymble – 2012   Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Setting Up an Integrated Development Environment for Python’.135  Lesson Goals In this lesson you will learn how to manipulate text files using Python. This includes opening, closing, reading from, and writing to .txt files. The next few lessons will involve downloading a web page from the Internet and reorganizing the contents into useful chunks of information. You will be doing most of your work using Python code written and executed in Komodo Edit. Working with Text Files Python makes it easy to work with files and text. Let’s begin with files. Creating and Writing to a Text File Let’s start with a brief discussion of terminology. In a previous lesson (depending on your operating system: Mac Installation, Windows Installation, or Linux Installation), you saw how to send information to the \"Command Output\" window of your text editor by using Python's print command.136 print\t'hello\tworld'\tThe Python programming language is object-oriented. That is to say that it is constructed around a special kind of entity, an object, which contains both data and a number of methods for accessing and altering that data. Once an object is created, it can interact with other objects. In the example above, we see one kind of object, the string \"hello world\". The string is the sequence of characters enclosed by quotes. You can write a string one of three ways:                                                         135\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Setting\tup\tan\tIntegrated\tDevelopment\tEnvironment\tfor\tPython’\t(Mac,\tLinux,\tor\tWindows),\tThe\tProgramming\tHistorian\t(2012).\t136\t‘The\tPrint\tstatement’,\tPython:\thttps://docs.python.org/2/reference/simple_stmts.html#the-print-statement\t \n 121 message1\t=\t'hello\tworld'\tmessage2\t=\t\"hello\tworld\"\tmessage3\t=\t\"\"\"hello\thello\thello\tworld\"\"\"\tThe important thing to note is that in the first two examples you can use single or double quotes / inverted commas, but you cannot mix the two within one string. For instance, the following are all wrong: message1\t=\t\"hello\tworld'\tmessage2\t=\t'hello\tworld\"\tmessage3\t=\t'I\tcan't\teat\tpickles'\tCount the number of single quotes in message3. For that to work you would have to escape the apostrophe: message3\t=\t'I\tcan\\'t\teat\tpickles'\tOr, rewrite the phrase as: message3\t=\t\"I\tcan't\teat\tpickles\"\tIn the third example, the triple quotes signify a string that covers more than one line. Print is a command that prints objects in textual form. The print command, when combined with the string, produces a statement. You will use print like this in cases where you want to create information that needs to be acted upon right away. Sometimes, however, you will be creating information that you want to save, to send to someone else, or to use as input for further processing by another program or set of programs. In these cases you will want to send information to files on your hard drive rather than to the \"Command Output\" pane. Enter the following program into your text editor and save it as file-output.py. #\tfile-output.py\tf\t=\topen('helloworld.txt','w')\tf.write('hello\tworld')\tf.close()\tIn Python, any line that begins with a hash mark (#) is known as a comment and is ignored by the Python interpreter. Comments are intended to allow programmers to communicate with one another (or to remind themselves of what their code does when they sit down with it a few months later). In a larger sense, programs themselves are typically written and formatted in a way that makes it easier for programmers to communicate with one another. Code that is closer to the requirements of the machine is referred to as low-level, whereas code that is closer to natural language is high-level. One of the benefits of using a language like  \n 122 Python is that it is very high level, making it easier for us to communicate with you (at some cost in terms of computational efficiency). In this program f is a file object, and open, write and close are file methods. In other words, open, write and close do something to the object f which is in this case defined as a .txt file. This is likely a different use of the term \"method\" than you might expect and from time to time you will find that words used in a programming context have slightly (or completely) different meanings than they do in everyday speech. In this case recall that methods are bits of code which perform actions. They do something to something else and return a result. You might try to think of it using a real-world example such giving commands to the family dog. The dog (the object) understands commands (i.e., has \"methods\") such as \"bark\", \"sit\", \"play dead\", and so on. We will discuss and learn how to use many other methods as we go along. f is a variable name chosen by us; you could have named it just about anything you like. In Python, variable names can be made from upper- and lowercase letters, numbers and underscores…but you can't use the names of Python commands as variables. If you tried to name your file variable \"print\" for example, your program would not work because that is a reserved\tword that is part of the programming language.137 Python variable names are also case-sensitive, which means that foobar, Foobar and FOOBAR would all be different variables. When you run this program, the open method will tell your computer to create a new text file helloworld.txt in the same folder as you have saved the file-output.py program. The w parameter says that you intend to write content to this new file using Python. Note that since both the file name and the parameter are surrounded by single quotes you know they are both stored as strings; forgetting to include the quotation marks will cause your program to fail. On the next line, your program writes the message \"hello world\" (another string) to the file and then closes it. (For more information about these statements, see the section on File\tObjects in the Python Library Reference.)138 Double-click on your \"Run Python\" button in Komodo Edit to execute the program (or the equivalent in whichever text-editor you have decided to use: e.g., click on the \"#!\" and \"Run\" in TextWrangler). Although nothing will be printed to the \"Command Output\" pane, you will see a status message that says something like `/usr/bin/python\tfile-output.py`\treturned\t0.\t                                                        137\t‘Keywords’,\tPython\tReference\tManual:\thttps://docs.python.org/release/2.5.4/ref/keywords.html\t138\t‘File\tObjects’,\tPython:\thttps://docs.python.org/2/library/stdtypes.html#bltin-file-objects\t \n 123 in Mac or Linux, or 'C:\\Python27\\Python.exe\tfile-output.py'\treturned\t0.\tin Windows. This means that your program executed successfully. If you use File -> Open -> File in your Komodo Edit, you can open the file helloworld.txt. It should contain your one-line message: Hello\tWorld!\tSince text files include a minimal amount of formatting information, they tend to be small, easy to exchange between different platforms (i.e., from Windows to Linux or Mac or vice versa), and easy to send from one computer program to another. They can usually also be read by people using a text editor like Komodo Edit. Reading From a Text File Python also has methods which allow you to get information from files. Type the following program into your text editor and save it as file-input.py. When you click on \"Run\" to execute it, it will open the text file that you just created, read the one-line message from it, and print the message to the \"Command Output\" pane. #\tfile-input.py\tf\t=\topen('helloworld.txt','r')\tmessage\t=\tf.read()\tprint\tmessage\tf.close()\tIn this case, the r parameter is used to indicate that you are opening a file to read from it. Parameters let you choose among the different options a particular method allows. Returning to the family dog example, the dog may be trained to bark once when he gets a beef-flavoured snack and twice when he gets a chicken-flavoured one. The flavour of the snack is a parameter. Each method is different in terms of what parameters it will accept. You cannot, for example, ask the dog to sing an Italian opera – unless your dog is particularly talented. You can look up the possible parameters for a particular method on the Python website, or often you can find them by typing the method into a search engine along with \"Python\". Read is another file method. The contents of the file (the one-line message) are copied into message, which is what we've decided to call this string, and then the print command is used to send the contents of message to the \"Command Output\" pane. Appending to a Pre-Existing Text File A third option is to open a pre-existing file and add more to it. Note that if you open a file and use the write method, the program will overwrite  \n 124 whatever might have been contained in the file. This isn’t an issue when you are creating a new file, or when you want to overwrite the contents of an existing file, but it might be undesirable when you are creating a log of events or compiling a large set of data into one file. So, instead of write you will want to use the append method, designated by a. Type the following program into your text editor and save it as file-append.py. When you run this program it will open the same helloworld.txt file created earlier and append a second “hello world” to the file. The '\\n' stands for new line. #\tfile-append.py\tf\t=\topen('helloworld.txt','a')\tf.write('\\n'\t+\t'hello\tworld')\tf.close()\tAfter you have run the program, open the helloworld.txt file and see what happened. Close the text file and re-run file-append.py a few more times. When you open helloworld.txt again you should notice a few extra 'hello world' messages waiting for you. In the next section, we will discuss modularity and reusing code. Suggested Readings  Non-Programmer’s Tutorial for Python 2.6/Hello, World139  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Code Reuse and Modularity in Python’.140 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        139\t‘Non-Programmer’s\tTutorial\tfor\tPython\t2.6/Hello,\tWorld’\tWikibooks:\thttps://en.wikibooks.org/wiki/Non-Programmer's_Tutorial_for_Python_2.6/Hello,_World\t140\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Code\tReuse\tand\tModularity\tin\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 125 14. Code Reuse and Modularity in Python William J. Turkel and Adam Crymble – 2012   Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Working with Text Files in Python’.141 Lesson Goals Computer programs can become long, unwieldy and confusing without special mechanisms for managing complexity. This lesson will show you how to reuse parts of your code by writing Functions and break your programs into Modules, in order to keep everything concise and easier to debug. Being able to remove a single dysfunctional module can save time and effort. Functions You will often find that you want to re-use a particular set of statements, usually because you have a task that you need to do over and over. Programs are mostly composed of routines that are powerful and general-purpose enough to be reused. These are known as functions, and Python has mechanisms that allow you to define new functions. Let’s work through a very simple example of a function. Suppose you want to create a general purpose function for greeting people. Copy the following function definition into Komodo Edit and save it as greet.py. #\tgreet.py\t\tdef\tgreetEntity\t(x):\t\t\t\t\tprint\t\"hello\t\"\t+\tx\t\tgreetEntity(\"Everybody\")\tgreetEntity(\"Programming\tHistorian\")\tThe line beginning with def is the function declaration. We are going to define (\"def\") a function, which in this case we have named \"greetEntity\". The (x) is the function's parameter. You should understand how that works in a moment. The second line contains the code of the function. This could be as many lines as we need, but in this case it is only a single line. Note that indentation is very important in Python. The blank space before the print statement tells the interpreter that it is part of the function that                                                         141\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Working\twith\tText\tFiles\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 126 is being defined. You will learn more about this as we go along; for now, make sure to keep indentation the way we show it. Run the program, and you should see something like this: hello\tEverybody\thello\tProgramming\tHistorian\tThis example contains one function: greetEntity. This function is then \"called\" (sometimes referred to as \"invoked\") two times. Calling or invoking a function just means we have told the program to execute the code in that function. Like giving the dog his chicken-flavoured treat (*woof* *woof*). In this case each time we have called the function we have given it a different parameter. Try editing greet.py so that it calls the greetEntity function a third time using your own name as a parameter. Run the program again. You should now be able to figure out what (x) does in the function declaration. Before moving on to the next step, edit greet.py to delete the function calls, leaving only the function declaration. You're going to learn how to call the function from another program. When you are finished, your greet.py file should look like this: #\tgreet.py\t\tdef\tgreetEntity\t(x):\t\t\t\t\tprint\t\"hello\t\"\t+\tx\tModularity When programs are small like the above example, they are typically stored in a single file. When you want to run one of your programs, you can simply send the file to the interpreter. As programs become larger, it makes sense to split them into separate files known as modules. This modularity makes it easier for you to work on sections of your larger programs. By perfecting each section of the program before putting all of the sections together, you not only make it easier to reuse individual modules in other programs, you make it easier to fix problems by being able to pinpoint the source of the error. When you break a program into modules, you are also able to hide the details for how something is done within the module that does it. Other modules don’t need to know how something is accomplished if they are not responsible for doing it. This need-to-know principle is called “encapsulation“. Suppose you were building a car. You could start adding pieces willy nilly, but it would make more sense to start by building and testing one module — perhaps the engine — before moving on to others. The engine, in turn, could be imagined to consist of a number of other, smaller modules like the carburettor and ignition system, and those are comprised of still smaller and more basic modules. The same is true when coding. You try to break a problem into smaller pieces, and solve those first.  \n 127 You already created a module when you wrote the greet.py program. Now you are going to write a second program, using-greet.py which will import code from your module and make use of it. Python has a special import statement that allows one program to gain access to the contents of another program file. This is what you will be using. Copy this code to Komodo Edit and save it as using-greet.py. This file is your program; greet.py is your module. #\tusing-greet.py\t\timport\tgreet\tgreet.greetEntity(\"everybody\")\tgreet.greetEntity(\"programming\thistorian\")\tWe have done a few things here. First, we have told Python to import (load) the greet.py module, which we previously created. You will also notice that whereas before we were able to run the function by calling only its name: greetEntity(\"everybody\"), we now need to include the module's name followed by a dot (.) in front of the function name. In plain English this means: run the greetEntity function, which you should find in the greet.py module. You can run your using-greet.py program with the \"Run Python\" command that you created in Komodo Edit. Note that you do not have to run your module…just the program that calls it. If all went well, you should see the following in the Komodo Edit output pane: hello\teverybody\thello\tprogramming\thistorian\tMake sure that you understand the difference between loading a data file (e.g., helloworld.txt) and importing a program file (e.g. greet.py) before moving on. Suggested Readings  Python Basics: http://www.astro.ufl.edu/~warner/prog/python.html  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Downloading Web Pages with Python’.142 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.                                                        142\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 128 15. Manipulating Strings in Python William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Downloading Web Pages with Python’.143   Lesson Goals This lesson is a brief introduction to string manipulation techniques in Python. Knowing how to manipulate strings plays a crucial role in most text processing tasks. If you’d like to experiment with the following lessons, you can write and execute short programs as we’ve been doing, or you can open up a Python shell / Terminal to try them out on the command line. Manipulating Python Strings If you have been exposed to another programming language before, you might have learned that you need to declare or type variables before you can store anything in them. This is not necessary when working with strings in Python. We can create a string simply by putting content wrapped with quotation marks into it with an equal sign (=): message\t=\t\"Hello\tWorld\"\tString Operators: Adding and Multiplying As we mentioned previously, a string is a type of object, one that consists of a series of characters. Python already knows how to deal with a number of general-purpose and powerful representations, including strings. One way to manipulate strings is by using string operators. These operators are represented by symbols that you likely associate with mathematics, such as +, -, *, /, and =. When used with strings, they perform actions that are similar to, but not the same as, their mathematical counterparts. Concatenate This term means to join strings together. The process is known as concatenating strings and it is done using the plus (+) operator. Note that you must be explicit about where you want blank spaces to occur by placing them between single quotation marks also.                                                         143\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 129 In this example, the string \"message1\" is given the content \"hello world\". message1\t=\t'hello'\t+\t'\t'\t+\t'world'\tprint\tmessage1\t->\thello\tworld\tMultiply If you want multiple copies of a string, use the multiplication (*) operator. In this example, string message2a is given the content \"hello\" times three; string message 2b is given content \"world\"; then we print both strings. message2a\t=\t'hello\t'\t*\t3\tmessage2b\t=\t'world'\tprint\tmessage2a\t+\tmessage2b\t->\thello\thello\thello\tworld\tAppend What if you want to add material to the end of a string successively? There is a special operator for that (+=). message3\t=\t'howdy'\tmessage3\t+=\t'\t'\tmessage3\t+=\t'world'\tprint\tmessage3\t->\thowdy\tworld\tString Methods: Finding, Changing In addition to operators, Python comes pre-installed with dozens of string methods that allow you to do things to strings. Used alone or in combination, these methods can do just about anything you can imagine to strings. The good news is that you can reference a list of String Methods on the Python website,144 including information on how to use each properly. To make sure that you’ve got a basic grasp of string methods, what follows is a brief overview of some of the more commonly used ones: Length You can determine the number of characters in a string using len. Note that the blank space counts as a separate character. message4\t=\t'hello'\t+\t'\t'\t+\t'world'\tprint\tlen(message4)\t->\t11\tFind You can search a string for a substring and your program will return the starting index position of that substring. This is helpful for further processing. Note that indexes are numbered from left to right and that the count starts with position 0, not 1.                                                         144\t‘String\tMethods’,\tPython:\thttps://docs.python.org/2/library/stdtypes.html#string-methods\t \n 130 message5\t=\t\"hello\tworld\"\tmessage5a\t=\tmessage5.find(\"worl\")\tprint\tmessage5a\t->\t6\tIf the substring is not present, the program will return a value of -1. message6\t=\t\"Hello\tWorld\"\tmessage6b\t=\tmessage6.find(\"squirrel\")\tprint\tmessage6b\t->\t-1\tLower Case Sometimes it is useful to convert a string to lower case. For example, if we standardize case it makes it easier for the computer to recognize that \"Sometimes\" and \"sometimes\" are the same word. message7\t=\t\"HELLO\tWORLD\"\tmessage7a\t=\tmessage7.lower()\tprint\tmessage7a\t->\thello\tworld\tThe opposite effect, raising characters to upper case, can be achieved by changing .lower() to .upper(). Replace If you need to replace a substring throughout a string you can do so with the replace method. message8\t=\t\"HELLO\tWORLD\"\tmessage8a\t=\tmessage8.replace(\"L\",\t\"pizza\")\tprint\tmessage8a\t->\tHEpizzapizzaO\tWORpizzaD\tSlice If you want to slice off unwanted parts of a string from the beginning or end you can do so by creating a substring. The same kind of technique also allows you to break a long string into more manageable components. message9\t=\t\"Hello\tWorld\"\tmessage9a\t=\tmessage9[1:8]\tprint\tmessage9a\t->\tello\tWo\tYou can substitute variables for the integers used in this example. startLoc\t=\t2\tendLoc\t=\t8\tmessage9b\t=\tmessage9[startLoc:\tendLoc]\tprint\tmessage9b\t->\tllo\tWo\t \n 131 This makes it much easier to use this method in conjunction with the find method as in the next example, which checks for the letter \"d\" in the first six characters of \"Hello World\" and correctly tells us it is not there (-1). This technique is much more useful in longer strings – entire documents for example. Note that the absence of an integer before the colon signifies we want to start at the beginning of the string. We could use the same technique to tell the program to go all the way to the end by putting no integer after the colon. And remember, index positions start counting from 0 rather than 1. message9\t=\t\"Hello\tWorld\"\tprint\tmessage9[:5].find(\"d\")\t->\t-1\tThere are lots more, but the string methods above are a good start. Note that in this last example, we are using square brackets instead of parentheses. This difference in syntax signals an important distinction. In Python, parentheses are usually used to pass an argument to a function. So when we see something like print\tlen(message7)\tit means pass the string message7 to the function len then send the returned value of that function to the print statement to be printed. If a function can be called without an argument, you often have to include a pair of empty parentheses after the function name anyway. We saw an example of that, too: message7\t=\t\"HELLO\tWORLD\"\tmessage7a\t=\tmessage7.lower()\tprint\tmessage7a\t->\thello\tworld\tThis statement tells Python to apply the lower function to the string message7 and store the returned value in the string message7a. The square brackets serve a different purpose. If you think of a string as a sequence of characters, and you want to be able to access the contents of the string by their location within the sequence, then you need some way of giving Python a location within a sequence. That is what the square brackets do: indicate a beginning and ending location within a sequence as we saw when using the slice method. Escape Sequences What do you do when you need to include quotation marks within a string? You don’t want the Python interpreter to get the wrong idea and end the string when it comes across one of these characters. In Python, you can put a backslash (\\) in front of a quotation mark so that it doesn't terminate the string. These are known as escape sequences.  \n 132 print\t'\\\"'\t->\t\"\tprint\t'The\tprogram\tprinted\t\\\"hello\tworld\\\"'\t->\tThe\tprogram\tprinted\t\"hello\tworld\"\tTwo other escape sequences allow you to print tabs and newlines: print\t'hello\\thello\\thello\\nworld'\t->hello\thello\thello\tworld\tSuggested Reading Lutz, Learning Python Ch. 7: Strings Ch. 8: Lists and Dictionaries Ch. 10: Introducing Python Statements Ch. 15: Function Basics Code Syncing To follow along with future lessons it is important that you have the right files and programs in your programming-historian directory. At the end of each chapter you can download the programming-historian zip file to make sure you have the correct code. Note we have removed unneeded files from earlier lessons. Your directory may contain more files and that’s ok! programming-historian-2 (zip): http://programminghistorian.org/assets/programming-historian2.zip  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘From HTML to List of Words (part 1)’.145 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        145\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\tList\tof\tWords\t(part\t1)’,\tThe\tProgramming\tHistorian,\t2012.\t \n 133 16. From HTML to List of Words (part 1) William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Manipulating Strings in Python’.146  Lesson Goals In this two-part lesson, we will build on what you’ve learned about ‘Downloading Web Pages with Python’,147 learning how to remove the HTML markup from the webpage of Benjamin Bowsey’s 1780 criminal trial transcript.148 We will achieve this by using a variety of string operators, string methods and close reading skills. We introduce looping and branching so that programs can repeat tasks and test for certain conditions, making it possible to separate the content from the HTML tags. Finally, we convert content from a long string to a list of words that can later be sorted, indexed, and counted. The Challenge To get a clearer picture of the task ahead, open the obo-t17800628-33.html file that you created in ‘Downloading Web Pages with Python’149 (or download and save the trial if you do not already have a copy)150, then look at the HTML source by clicking on Tools -> Web Developer -> Page Source. As you scroll through the source code you’ll notice that there are a few HTML tags mixed in with the text. Because this is a printable version there is far less HTML than you will find on the other versions of the transcript (see the HTML151 and XML152 versions to compare). While not                                                         146\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Manipulating\tStrings\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t147\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’\tThe\tProgramming\tHistorian\t(2012).\t148\t‘Trial\tof\tBenjamin\tBowsey,\tJune\t1780,\t(t17800628-33)’,\tThe\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/print.jsp?div=t17800628-33\t\t149\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’\tThe\tProgramming\tHistorian\t(2012).\t150\t‘Trial\tof\tBenjamin\tBowsey,\tJune\t1780,\t(t17800628-33)’,\tThe\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/print.jsp?div=t17800628-33\t151\tOld\tBailey\tProceedings\tOnline\t(www.oldbaileyonline.org,\tversion\t7.2,\t26\tFebruary\t2016),\tJune\t1780,\ttrial\tof\tBENJAMIN\tBOWSEY\t(t17800628-33):\twww.oldbaileyonline.org/browse.jsp?id=t17800628-33-defend448&div=t17800628-33\t152http://www.oldbaileyonline.org/browse.jsp?foo=bar&path=sessionsPapers/17800628.xml&div=t17800628-33&xml=yes\t \n 134 mandatory, we recommend that at this point you take the W3 Schools HTML tutorial153 to familiarize yourself with HTML markup. If your work often requires that you remove HTML markup, it will certainly help to be able to understand it when you see it. Files Needed For This Lesson  obo-t17800628-33.html If you do not have these files, you can download programming-historian-2, the (zip - http://programminghistorian.org/assets/programming-historian2.zip) file from the previous lesson. Devising an Algorithm Since the goal is to get rid of the HTML, the first step is to create an algorithm that returns only the text (minus the HTML tags) of the article. An algorithm is a procedure that has been specified in enough detail that it can be implemented on a computer. It helps to write your algorithms first in plain English; it’s a great way to outline exactly what you want to do before diving into code. To construct this algorithm you are going to use your close reading skills to figure out a way to capture only the textual content of the biography. Looking at the source code of obo-t17800628-33.html you will notice the actual transcript does not start right away. Instead there are a couple of HTML tags and some citation information. In this case: <div\tstyle=\"font-family:serif;\"><i>Old\tBailey\tProceedings\tOnline</i>\t(www.oldbaileyonline.org,\tversion\t6.0,\t01\tJuly\t2011),\tJune\t1780,\ttrial\tof\tBENJAMIN\tBOWSEY\t(t17800628-33).<hr/><h2>BENJAMIN\tBOWSEY...\tWe are only interested in the transcript itself, not the extra metadata contained in the tags. However, you will notice that the end of the metadata corresponds with the start of the transcript. This makes the location of the metadata a potentially useful marker for isolating the transcript text. At a glance, we can see that the metadata ends with two HTML tags: <hr/><h2>. We might be able to use those to find the starting point of our transcript text. We are lucky in this case because it turns out that these tags are a reliable way to find the start of transcript text in the printable versions (if you want, take a look at a few other printable trials to check). We are also lucky because other than a few HTML tags at the end of the transcript, there is no further information on the page. Had there been other unrelated content, we would take a similar approach and look for some way of isolating the end of the desired text. Well-formatted websites                                                         153\t‘HTML(5)\tTutorial’:\thttp://www.w3schools.com/html/\t \n 135 will almost always have some unique way of signalling the end of the content. You often just need to look closely. The next thing that you want to do is strip out all of the HTML markup that remains mixed in with the content. Since you know HTML tags are always found between matching pairs of angle brackets, it’s probably a safe bet that if you remove everything between angle brackets, you will remove the HTML and be left only with the transcript. Note that we are making the assumption that the transcript will not contain the mathematical symbols for “less than” or “greater than.” If Bowsey was a mathematician, this assumption would not be as safe. The following describes our algorithm in words. To isolate the content: Download the transcript text Search the HTML for and store the location of <hr/><h2> Save everything after the <hr/><h2> tags to a string: pageContents At this point we have the trial transcript text, plus HTML markup. Next: Look at every character in the pageContents string, one character at a time If the character is a left angle bracket (<) we are now inside a tag so ignore each following character If the character is a right angle bracket (>) we are now leaving the tag; ignore the current character, but look at each following character If we’re not inside a tag, append the current character to a new variable: text Finally: Split the text string into a list of individual words that can later be manipulated further. Isolating Desired Content The following step uses Python commands introduced in the ‘Manipulating Strings in Python’154 lesson to implement the first half of the algorithm: removing all content before the <hr/><h2> tags. To recap, the algorithm was as follows: Download the transcript text Search the HTML for and store the location of <hr/><h2> Save everything after the <hr/><h2> tags to a string: pageContents                                                         154\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble\t‘Manipulating\tStrings\tin\tPython’\tThe\tProgramming\tHistorian\t(2012).\t \n 136 To achieve this, you will use the find string method and create a new substring containing only the desired content using the index as start point for the substring. As you work, you will be developing separate files to contain your code. One of these will be called obo.py (for “Old Bailey Online”). This file is going to contain all of the code that you will want to re-use; in other words, obo.py is a module. We discussed the idea of modules in ‘Code Reuse and Modularity in Python’155 when we saved our functions to greet.py. Create a new file named obo.py and save it to your programming-historian directory. We are going to use this file to keep copies of the functions needed to process The Old Bailey Online. Type or copy the following code into your file. #\tobo.py\t\tdef\tstripTags(pageContents):\t\t\t\t\tstartLoc\t=\tpageContents.find(\"<hr/><h2>\")\t\t\t\t\t\tpageContents\t=\tpageContents[startLoc:]\t\t\t\t\treturn\tpageContents\tCreate a second file, trial-content.py, and save the program shown below. #\ttrial-content.py\t\timport\turllib2,\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\tHTML\t=\tresponse.read()\t\tprint\tobo.stripTags(HTML)\tWhen you run trial-content.py it will get the web page for Bowsey’s trial transcript, then look in the obo.py module for the stripTags function. It will use that function to extract the stuff after the <hr/><h2> tags. With any luck, this should be the textual content of the Bowsey transcript, along with some of HTML markup. Don’t worry if your Command Output screen ends in a thick black line. Komodo Edit’s output screen has a maximum number of characters it will display, after which characters start literally writing over one another on the screen, giving the appearance of a black blob. Don’t worry, the text is in there even though you cannot read it; you can cut and paste it to a text file to double check. Let’s take a moment to make sure we understand how trial-contents.py is able to use the functions stored in obo.py. The stripTags function that we saved to obo.py requires one argument. In other words, to run properly it needs one piece of information to be supplied. Recall the trained dog                                                         155\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Code\tReuse\tand\tModularity\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 137 example from a previous lesson. In order to bark, the dog needs two things: air and a delicious treat. The stripTags function in obo.py needs one thing: a string called pageContents. But you’ll notice that when we call stripTags in the final program (trialcontents.py) there’s no mention of “pageContents“. Instead the function is given HTML as an argument. This can be confusing to many people when they first start programming. Once a function has been declared, we no longer need to use the same variable name when we call the function. As long as we provide the right type of argument, everything should work fine, no matter what we call it. In this case we wanted pageContents to use the contents of our HTML variable. You could have passed it any string, including one you input directly between the parentheses. Try rerunning trial-content.py, changing the stripTags argument to “I am quite fond of dogs” and see what happens. Note that depending on how you define your function (and what it does) your argument may need to be something other than a string: an integer for example. Suggested Reading  Lutz, Learning Python156 Ch. 7: Strings Ch. 8: Lists and Dictionaries Ch. 10: Introducing Python Statements Ch. 15: Function Basics Code Syncing To follow along with future lessons it is important that you have the right files and programs in your programming-historian directory. At the end of each chapter you can download the programming-historian zip file to make sure you have the correct code. Note we have removed unneeded files from earlier lessons. Your directory may contain more files and that’s ok! programming-historian-2 (zip - http://programminghistorian.org/assets/programming-historian2.zip)   If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘From HTML to List of Words (part 2)’.157 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.                                                        156\tMark\tLutz,\tLearning\tPython\t(5th\tedition),\tO’Reilley,\t2013.\t157\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\tList\tof\tWords\t(part\t2)’,\tThe\tProgramming\tHistorian,\t2012.\t \n 138 17. From HTML to List of Words (part 2) William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘From HTML to List of Words (part 1)’.158  Lesson Goals In this lesson, you will learn the Python commands needed to implement the second part of the algorithm begun in the ‘From HTML to a List of Words (part 1)’. The first half of the algorithm gets the content of an HTML page and saves only the content that follows the <hr/><h2> tags. The second half of the algorithm does the following: Look at every character in the pageContents string, one character at a time If the character is a left angle bracket (<) we are now inside a tag so ignore each following character If the character is a right angle bracket (>) we are now leaving the tag; ignore the current character, but look at each following character If we’re not inside a tag, append the current character to a new variable: text Split the text string into a list of individual words that can later be manipulated further. Files Needed For This Lesson obo.py trial-content.py If you do not have these files, you can download programming-historian-2, a (zip - http://programminghistorian.org/assets/programming-historian2.zip) file from the previous lesson. Repeating and Testing in Python The next stage in implementing the algorithm is to look at every character in the pageContents string, one at a time and decide whether the character belongs to HTML markup or to the content of the trial transcript. Before you can do this you’ll have to learn a few techniques for repeating tasks and for testing conditions.                                                         158\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\tList\tof\tWords\t(part\t1)’,\tThe\tProgramming\tHistorian\t(2012).\t \n 139 Looping Like many programming languages, Python includes a number of looping mechanisms. The one that you want to use in this case is called a for loop. The version below tells the interpreter to do something for each character in a string named pageContents. The variable char will contain each character from pageContents in succession. We gave char its name; it does not have any special significance and could have been named jingles or k if we had felt so inclined. You can use the colour-coding in Komodo Edit as a guideline for deciding if a word is a variable with a user-given name (such as “char“) or a Python-defined name that serves a specific purpose (such as “for“). It is usually a good idea to give variables names that provide information about what they contain. This will make it much easier to understand a program that you haven’t looked at for a while. With this in mind, “jingles” is probably not a very good choice for a variable name in this case. for\tchar\tin\tpageContents:\t\t\t\t\t#\tdo\tsomething\twith\tchar\tBranching Next you need a way of testing the contents of a string, and choosing a course of action based on that test. Again, like many programming languages, Python includes a number of branching mechanisms. The one that you want to use here is called an if statement. The version below tests to see whether the string named char consists of a left angle bracket. As we mentioned earlier, indentation is important in Python. If code is indented, Python will execute it when the condition is true. Note that Python uses a single equals sign (=) for assignment, that is for setting one thing equal to something else. In order to test for equality, use double equals signs (==) instead. Beginning programmers often confuse the two. if\tchar\t==\t'<':\t\t\t\t\t#\tdo\tsomething\tA more general form of the if statement allows you to specify what to do in the event that your test condition is false. if\tchar\t==\t'<':\t\t\t\t\t#\tdo\tsomething\telse:\t\t\t\t\t#\tdo\tsomething\tdifferent\tIn Python you have the option of doing further tests after the first one, by using an elif statement (which is shorthand for else if).    \n 140 if\tchar\t==\t'<':\t\t\t\t\t#\tdo\tsomething\telif\tchar\t==\t'>':\t\t\t\t\t#\tdo\tanother\tthing\telse:\t\t\t\t\t#\tdo\tsomething\tcompletely\tdifferent\tUse the Algorithm to Remove HTML Markup You now know enough to implement the second part of the algorithm: removing all HTML tags. In this part of the algorithm we want to: Look at every character in the pageContents string, one character at a time If the character is a left angle bracket (<) we are now inside a tag so ignore the character If the character is a right angle bracket (>) we are now leaving the tag; ignore the character If we’re not inside a tag, append the current character to a new variable: text To do this, you will use a for loop to look at each successive character in the string. You will then use an if / elif statement to determine whether the character is part of HTML markup or part of the content, then append the content characters to the text string. How will we keep track of whether or not we’re inside a tag? We can use an integer variable, which will be 1 (true) if the current character is inside a tag and 0 (false) if it’s not (in the example below we have named the variable inside). The stripTags Routine Putting it all together, the final version of the routine is shown below. Note that we are expanding the stripTags function created above. Make sure you maintain the indentation as shown when you replace the old stripTags routine in obo.py with this new one. Your routine may look slightly different and as long as it works that’s fine. If you’ve elected to experiment, it’s probably best to try our version as well to make sure that your program does what ours does.         \n 141 #\tobo.py\tdef\tstripTags(pageContents):\t\t\t\t\tstartLoc\t=\tpageContents.find(\"<hr/><h2>\")\t\t\t\t\tpageContents\t=\tpageContents[startLoc:]\t\t\t\t\t\tinside\t=\t0\t\t\t\t\ttext\t=\t''\t\t\t\t\t\tfor\tchar\tin\tpageContents:\t\t\t\t\t\t\t\t\tif\tchar\t==\t'<':\t\t\t\t\t\t\t\t\t\t\t\t\tinside\t=\t1\t\t\t\t\t\t\t\t\telif\t(inside\t==\t1\tand\tchar\t==\t'>'):\t\t\t\t\t\t\t\t\t\t\t\t\tinside\t=\t0\t\t\t\t\t\t\t\t\telif\tinside\t==\t1:\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\ttext\t+=\tchar\t\t\t\t\t\treturn\ttext\tThere are two new Python concepts in this new code: continue and return. The Python continue statement tells the interpreter to jump back to the top of the enclosing loop. So if we are processing characters inside of a pair of angle brackets, we want to go get the next character in the pageContents string without adding anything to our text variable. In our previous examples we have used print extensively. This outputs the result of our program to the screen for the user to read. Often, however, we wish to allow one part of the program to send information to another part. When a function finishes executing, it can return a value to the code which called it. If we were to call stripTags using another program, we would do so like this: #understanding\tthe\tReturn\tstatement\t\timport\tobo\t\tmyText\t=\t\"This\tis\tmy\t<h1>HTML</h1>\tmessage\"\t\ttheResult\t=\tobo.stripTags(myText)\tBy using return, we have been able to save the output of the stripTags function directly into a variable, which we can then resume processing as needed using additional code. Note that in the stripTags example from the start of this sub-section, the value that we want to return now is not pageContents, but rather the content which has had the HTML markup stripped out. To test our new stripTags routine, you can run trial-content.py again. Since we’ve redefined stripTags, the trial-content.py program now does something different (and closer to what we want). Before you continue, make sure that you understand why the behaviour of trial-content.py would change when we only edited obo.py.  \n 142 Python Lists Now that you have the ability to extract raw text from web pages, you’re going to want to get the text in a form that is easy to process. So far, when you’ve needed to store information in your Python programs, you’ve usually used strings. There were a couple of exceptions, however. In the stripTags routine, you also made use of an integer159 named inside to store a 1 when you were processing a tag and a 0 when you weren’t. You can do mathematical operations on integers but you cannot store fractions or decimal numbers in integer variables. inside\t=\t1\tAnd whenever you’ve needed to read from or write to a file, you’ve used a special file handle like f in the example below. f\t=\topen('helloworld.txt','w')\tf.write('hello\tworld')\tf.close()\tOne of the most useful types160 of object that Python provides, however, is the list, an ordered collection of other objects (including, potentially, other lists). Converting a string into a list of characters or words is straightforward. Type or copy the following program into your text editor to see two ways of achieving this. Save the file as string-to-list.py and execute it. Compare the two lists that are printed to the Command Output pane and see if you can figure out how the code works. #\tstring-to-list.py\t\t#\tsome\tstrings\ts1\t=\t'hello\tworld'\ts2\t=\t'howdy\tworld'\t\t#\tlist\tof\tcharacters\tcharlist\t=\t[]\tfor\tchar\tin\ts1:\t\t\t\t\tcharlist.append(char)\tprint\tcharlist\t\t#\tlist\tof\t'words'\twordlist\t=\ts2.split()\tprint\twordlist\tThe first routine uses a for loop to step through each character in the string s1, and appends the character to the end of charlist. The second routine makes use of the split operation to break the string s2 apart wherever there is whitespace (spaces, tabs, carriage returns and similar characters). Actually, it is a bit of a simplification to refer to the objects in the second                                                         159\t‘Numeric\tTypes\t–\tint,\tfloat,\tlong,\tcomplex’\tPython\tLibrary\tReference:\thttps://docs.python.org/2.4/lib/typesnumeric.html\t160\t‘Types’\tPython:\thttps://docs.python.org/3/library/types.html\t \n 143 list as words. Try changing s2 in the above program to ‘howdy world!’ and running it again. What happened to the exclamation mark? Note, that you will have to save your changes before using Run Python again. Given what you’ve learned so far, you can now open a URL, download the web page to a string, strip out the HTML and then split the text into a list of words. Try executing the following program. #html-to-list1.py\timport\turllib2,\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\thtml\t=\tresponse.read()\ttext\t=\tobo.stripTags(html)\twordlist\t=\ttext.split()\t\tprint\twordlist[0:120]\tYou should get something like the following. ['BENJAMIN',\t'BOWSEY,',\t'Breaking',\t'Peace',\t'>',\t'riot,',\t'28th',\t'June',\t'1780.',\t'324.',\t'BENJAMIN',\t'BOWSEY',\t'(a',\t'blackmoor',\t')',\t'was',\t'indicted',\t'for',\t'that',\t'he',\t'together',\t'with',\t'five',\t'hundred',\t'other',\t'persons',\t'and',\t'more,',\t'did,',\t'unlawfully,',\t'riotously,',\t'and',\t'tumultuously',\t'assemble',\t'on',\t'the',\t'6th',\t'of',\t'June',\t'to',\t'the',\t'disturbance',\t'of',\t'the',\t'public',\t'peace',\t'and',\t'did',\t'begin',\t'to',\t'demolish',\t'and',\t'pull',\t'down',\t'the',\t'dwelling',\t'house',\t'of',\t'Richard',\t'Akerman',\t',',\t'against',\t'the',\t'form',\t'of',\t'the',\t'statute,',\t'&c.',\t'ROSE',\t'JENNINGS',\t',',\t'Esq.',\t'sworn.',\t'Had',\t'you',\t'any',\t'occasion',\t'to',\t'be',\t'in',\t'this',\t'part',\t'of',\t'the',\t'town,',\t'on',\t'the',\t'6th',\t'of',\t'June',\t'in',\t'the',\t'evening?',\t'-',\t'I',\t'dined',\t'with',\t'my',\t'brother',\t'who',\t'lives',\t'opposite',\t'Mr.',\t\"Akerman's\",\t'house.',\t'They',\t'attacked',\t'Mr.',\t\"Akerman's\",\t'house',\t'precisely',\t'at',\t'seven',\t\"o'clock;\",\t'they',\t'were',\t'preceded',\t'by',\t'a',\t'man']\tSimply having a list of words doesn’t buy you much yet. As human beings, we already have the ability to read. You’re getting much closer to a representation that your programs can process, however. Suggested Reading  Lutz, Learning Python161 Ch. 7: Strings Ch. 8: Lists and Dictionaries Ch. 10: Introducing Python Statements                                                         161\tMark\tLutz,\tLearning\tPython\t(5th\tedition),\tO’Reilley,\t2013.\t \n 144 Ch. 15: Function Basics Code Syncing To follow along with future lessons it is important that you have the right files and programs in your programming-historian directory. At the end of each chapter you can download the programming-historian zip file to make sure you have the correct code. Note we have removed unneeded files from earlier lessons. Your directory may contain more files and that’s ok! programming-historian-2 (zip - http://programminghistorian.org/assets/programming-historian2.zip)   If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Normalizing Textual Data with Python’.162 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        162\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Normalizing\tTextual\tData\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 145 18. Normalizing Textual Data with Python William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘From HTML to List of Words (part 2)’.163  Lesson Goals The list that we created in the ‘From HTML to a List of Words (2)’ needs some normalizing before it can be used further. We are going to do this by applying additional string methods, as well as by using regular expressions. Once normalized, we will be able to more easily analyze our data. Files Needed For This Lesson  html-to-list-1.py obo.py If you do not have these files from the previous lesson, you can download a zip file from the previous lesson here:  http://programminghistorian.org/lessons/from-html-to-list-of-words-2#code-syncing. Cleaning up the List In ‘From HTML to a List of Words (2)’, we wrote a Python program called html-to-list-1.py which downloaded a web page,164 stripped out the HTML formatting and metadata and returned a list of “words” like the one shown below. Technically, these entities are called “tokens” rather than “words”. They include some things that are, strictly speaking, not words at all (like the abbreviation &c. for “etcetera”). They also include some things that may be considered composites of more than one word. The possessive “Akerman’s,” for example, is sometimes analyzed by linguists as two words: “Akerman” plus a possessive marker. Is “o’clock” one word or two? And so on.                                                         163\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\tList\tof\tWords\t(part\t2)’,\tThe\tProgramming\tHistorian\t(2012).\t164\tThe\tpage\twas\ta\tcriminal\ttrial\ttranscript\tfrom\t1780:\t‘Trial\tof\tBenjamin\tBowsey,\tJune\t1780\t(t17800628-33)’,\tthe\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/print.jsp?div=t17800628-33\t \n 146 Turn back to your program html-to-list-1.py and make sure that your results look something like this: ['BENJAMIN',\t'BOWSEY,',\t'Breaking',\t'Peace',\t'>',\t'riot,',\t'28th',\t'June',\t'1780.',\t'324.',\t'BENJAMIN',\t'BOWSEY',\t'(a',\t'blackmoor',\t')',\t'was',\t'indicted',\t'for',\t'that',\t'he',\t'together',\t'with',\t'five',\t'hundred',\t'other',\t'persons',\t'and',\t'more,',\t'did,',\t'unlawfully,',\t'riotously,',\t'and',\t'tumultuously',\t'assemble',\t'on',\t'the',\t'6th',\t'of',\t'June',\t'to',\t'the',\t'disturbance',\t'of',\t'the',\t'public',\t'peace',\t'and',\t'did',\t'begin',\t'to',\t'demolish',\t'and',\t'pull',\t'down',\t'the',\t'dwelling',\t'house',\t'of',\t'Richard',\t'Akerman',\t',',\t'against',\t'the',\t'form',\t'of',\t'the',\t'statute,',\t'&c.',\t'ROSE',\t'JENNINGS',\t',',\t'Esq.',\t'sworn.',\t'Had',\t'you',\t'any',\t'occasion',\t'to',\t'be',\t'in',\t'this',\t'part',\t'of',\t'the',\t'town,',\t'on',\t'the',\t'6th',\t'of',\t'June',\t'in',\t'the',\t'evening?',\t'-',\t'I',\t'dined',\t'with',\t'my',\t'brother',\t'who',\t'lives',\t'opposite',\t'Mr.',\t\"Akerman's\",\t'house.',\t'They',\t'attacked',\t'Mr.',\t\"Akerman's\",\t'house',\t'precisely',\t'at',\t'seven',\t\"o'clock;\",\t'they',\t'were',\t'preceded',\t'by',\t'a',\t'man']\tBy itself, this ability to separate the document into words doesn’t buy us much because we already know how to read. We can use the text, however, to do things that aren’t usually possible without special software. We’re going to start by computing the frequencies of tokens and other linguistic units, a classic measure of a text. It is clear that our list is going to need some cleaning up before we can use it to count frequencies. In keeping with the practices established in ‘From HTML to a List of Words (1)’,165 let’s try to describe our algorithm in plain English first. We want to know the frequency of each meaningful word that appears in the trial transcript. So, the steps involved might look like this: Convert all words to lower case so that “BENJAMIN” and “benjamin” are counted as the same word Remove any strange or unusual characters Count the number of times each word appears Remove overly common words such as “it”, “the”, “and”, etc. Convert to Lower Case Typically tokens are folded to lower case when counting frequencies, so we’ll do that using the string method lower which was introduced in ‘Manipulating Strings in Python’.166 Since this is a string method we will have to apply it to the string: text in the html-to-list1.py program. Amend                                                         165\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘From\tHTML\tto\ta\tList\tof\tWords\t(part\t1)’,\tThe\tProgramming\tHistorian\t(2012).\t166\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Manipulating\tStrings\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 147 html-to-list1.py by adding the string tag lower() to the the end of the text string. #html-to-list1.py\timport\turllib2,\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\thtml\t=\tresponse.read()\ttext\t=\tobo.stripTags(html).lower()\t#add\tthe\tstring\tmethod\there.\twordlist\t=\ttext.split()\t\tprint\t(wordlist)\tYou should now see the same list of words as before, but with all characters changed to lower case. By calling methods one after another like this, we can keep our code short and make some pretty significant changes to our program. Like we said before, Python makes it easy to do a lot with very little code! At this point, we might look through a number of other Old Bailey Online entries and a wide range of other potential sources to make sure that there aren’t other special characters that are going to cause problems later. We might also try to anticipate situations where we don’t want to get rid of punctuation (e.g., distinguishing monetary amounts like “$1629” or “£1295” from dates, or recognizing that “1629-40” has a different meaning than “1629 40”.) This is what professional programmers get paid to do: try to think of everything that might go wrong and deal with it in advance. We’re going to take a different approach. Our main goal is to develop techniques that a working historian can use during the research process. This means that we will almost always prefer approximately correct solutions that can be developed quickly. So rather than taking the time now to make our program robust in the face of exceptions, we’re simply going to get rid of anything that isn’t an accented or unaccented letter or an Arabic numeral. Programming is typically a process of “stepwise refinement”. You start with a problem and part of a solution, and then you keep refining your solution until you have something that works better. Python Regular Expressions We’ve eliminated upper case letters. That just leaves all the punctuation to get rid of. Punctuation will throw off our frequency counts if we leave them in. We want “evening?” to be counted as “evening” and “1780.” as “1780”, of course. It is possible to use the replace string method to remove each type of punctuation:  \n 148 text\t=\ttext.replace('[',\t'')\ttext\t=\ttext.replace(']',\t'')\ttext\t=\ttext.replace(',',\t'')\t#etc...\tBut that’s not very efficient. In keeping with our goal of creating short, powerful programs, we’re going to use a mechanism called regular expressions. Regular expressions are provided by many programming languages in a range of different forms. Regular expressions allow you to search for well defined patterns and can drastically shorten the length of your code. For instance, if you wanted to know if a substring matched a letter of the alphabet, rather than use an if/else statement to check if it matched the letter “a” then “b” then “c”, and so on, you could use a regular expression to see if the substring matched a letter between “a” and “z”. Or, you could check for the presence of a digit, or a capital letter, or any alphanumeric character, or a carriage return, or any combination of the above, and more. In Python, regular expressions are available as a Python module. To speed up processing it is not loaded automatically because not all programs require it. So, you will have to import the module (called re) in the same way that you imported your obo.py module. Since we’re interested in only alphanumeric characters, we’ll create a regular expression that will isolate only these and remove the rest. Copy the following function and paste it into the obo.py module at the end. You can leave the other functions in the module alone, as we’ll continue to use those. #\tGiven\ta\ttext\tstring,\tremove\tall\tnon-alphanumeric\t#\tcharacters\t(using\tUnicode\tdefinition\tof\talphanumeric).\t\tdef\tstripNonAlphaNum(text):\t\t\t\t\timport\tre\t\t\t\t\treturn\tre.compile(r'\\W+',\tre.UNICODE).split(text)\tThe regular expression in the above code is the material inside the string, in other words W+. The W is shorthand for the class of non-alphanumeric characters. In a Python regular expression, the plus sign (+) matches one or more copies of a given character. The re.UNICODE tells the interpreter that we want to include characters from the world’s other languages in our definition of “alphanumeric”, as well as the A to Z, a to z and 0-9 of English. Regular expressions have to be compiled before they can be used, which is what the rest of the statement does. Don’t worry about understanding the compilation part right now. When we refine our html-to-list1.py program, it now looks like this:    \n 149 #html-to-list1.py\timport\turllib2,\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\thtml\t=\tresponse.read()\ttext\t=\tobo.stripTags(html).lower()\twordlist\t=\tobo.stripNonAlphaNum(text)\t\tprint\twordlist[0:500]\tWhen you execute the program and look through its output in the “Command Output” pane, you’ll see that it has done a pretty good job. This code will split hyphenated forms like “coach-wheels” into two words and turn the possessive “s” or “o’clock” into separate words by losing the apostrophe. But it is a good enough approximation to what we want that we should move on to counting frequencies before attempting to make it better. (If you work with sources in more than one language, you need to learn more about the Unicode standard167 and about Python support for it.)168 For extra practice with Regular Expressions, you may find Chapter 7 of Mark Pilgrim’s “Dive into Python” a useful tutorial.169     If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Counting Word Frequencies with Python’.170 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        167\t‘The\tUnicode\tConsortium’:\thttp://unicode.org/\t168\t‘Unicode’,\tDive\tinto\tPython:\thttp://www.diveintopython.net/xml_processing/unicode.html\t169\t‘Chapter\t7.\tRegular\tExpressions’,\tDive\tInto\tPython:\thttp://www.diveintopython.net/regular_expressions/index.html\t170\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 150 19. Keywords in Context (Using n-grams) with Python William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Output Data as an HTML File with Python’.171  Lesson Goals Like in ‘Output Data as HTML File’, this lesson takes the frequency pairs collected in ‘Counting Word Frequencies with Python’172 and outputs them in HTML. This time the focus is on keywords in context (KWIC) which creates n-grams from the original document content – in this case a trial transcript from the Old Bailey Online. You can use your program to select a keyword and the computer will output all instances of that keyword, along with the words to the left and right of it, making it easy to see at a glance how the keyword is used. Once the KWICs have been created, they are then wrapped in HTML and sent to the browser where they can be viewed. This reinforces what was learned in ‘Output Data as HTML File with Python’,173 opting for a slightly different output. At the end of this lesson, you will be able to extract all possible n-grams from the text. In the next lesson, you will be learn how to output all of the n-grams of a given keyword in a document downloaded from the Internet, and display them clearly in your browser window. Files Needed For This Lesson obo.py If you do not have these files from the previous lesson, you can download programming-historian-3, a zip file from the previous lesson: http://programminghistorian.org/assets/programming-historian3.zip                                                          171\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Output\tData\tas\tan\tHTML\tFile\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t172\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t173\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Output\tData\tas\tan\tHTML\tFile\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 151 From Text to N-Grams to KWIC Now that you know how to harvest the textual content of a web page automatically with Python, and have begun to use strings, lists and dictionaries for text processing, there are many other things that you can do with the text besides counting frequencies. People who study the statistical properties of language have found that studying linear sequences of linguistic units can tell us a lot about a text. These linear sequences are known as bigrams (2 units), trigrams (3 units), or more generally as n-grams. You have probably seen n-grams many times before. They are commonly used on search results pages to give you a preview of where your keyword appears in a document and what the surrounding context of the keyword is. This application of n-grams is known as keywords in context (often abbreviated as KWIC). For example, if the string in question were \"it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\" then a 7-gram for the keyword \"wisdom\" would be: the\tage\tof\twisdom\tit\twas\tthe\tAn n-gram could contain any type of linguistic unit you like. For historians you are most likely to use characters as in the bigram \"qu\" or words as in the trigram \"the dog barked\"; however, you could also use phonemes, syllables, or any number of other units depending on your research question. What we're going to do next is develop the ability to display KWIC for any keyword in a body of text, showing it in the context of a fixed number of words on either side. As before, we will wrap the output so that it can be viewed in Firefox and added easily to Zotero. From Text to N-grams Since we want to work with words as opposed to characters or phonemes, it will be much easier to create n-grams using a list of words rather than strings. As you already know, Python can easily turn a string into a list using the split operation. Once split it becomes simple to retrieve a subsequence of adjacent words in the list by using a slice, represented as two indexes separated by a colon. This was introduced when working with strings in ‘Manipulating Strings in Python’.174 message9\t=\t\"Hello\tWorld\"\tmessage9a\t=\tmessage9[1:8]\tprint\tmessage9a\t->\tello\tWo\t                                                        174\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Manipulating\tStrings\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 152 However, we can also use this technique to take a predetermined number of neighbouring words from the list with very little effort. Study the following examples, which you can try out in a Python Shell. wordstring\t=\t'it\twas\tthe\tbest\tof\ttimes\tit\twas\tthe\tworst\tof\ttimes\t'\twordstring\t+=\t'it\twas\tthe\tage\tof\twisdom\tit\twas\tthe\tage\tof\tfoolishness'\twordlist\t=\twordstring.split()\t\tprint\twordlist[0:4]\t->\t['it',\t'was',\t'the',\t'best']\t\tprint\twordlist[0:6]\t->\t['it',\t'was',\t'the',\t'best',\t'of',\t'times']\t\tprint\twordlist[6:10]\t->\t['it',\t'was',\t'the',\t'worst']\t\tprint\twordlist[0:12]\t->\t['it',\t'was',\t'the',\t'best',\t'of',\t'times',\t'it',\t'was',\t'the',\t'worst',\t'of',\t'times']\t\tprint\twordlist[:12]\t->\t['it',\t'was',\t'the',\t'best',\t'of',\t'times',\t'it',\t'was',\t'the',\t'worst',\t'of',\t'times']\t\tprint\twordlist[12:]\t->\t['it',\t'was',\t'the',\t'age',\t'of',\t'wisdom',\t'it',\t'was',\t'the',\t'age',\t'of',\t'foolishness']\tIn these examples we have used the slice method to return parts of our list. Note that there are two sides to the colon in a slice. If the right of the colon is left blank as in the last example above, the program knows to automatically continue to the end – in this case, to the end of the list. The second last example above shows that we can start at the beginning by leaving the space before the colon empty. This is a handy shortcut available to keep your code shorter. You can also use variables to represent the index positions. Used in conjunction with a for loop, you could easily create every possible n-gram of your list. The following example returns all 5-grams of our string from the example above. i\t=\t0\tfor\titems\tin\twordlist:\t\t\t\t\tprint\twordlist[i:\ti+5]\t\t\t\t\ti\t+=\t1\tKeeping with our modular approach, we will create a function and save it to the obo.py module that can create n-grams for us. Study and type or copy the following code:    \n 153 #\tGiven\ta\tlist\tof\twords\tand\ta\tnumber\tn,\treturn\ta\tlist\t#\tof\tn-grams.\t\tdef\tgetNGrams(wordlist,\tn):\t\t\t\t\treturn\t[wordlist[i:i+n]\tfor\ti\tin\trange(len(wordlist)-(n-1))]\tThis function may look a little confusing as there is a lot going on here in not very much code. It uses a list comprehension to keep the code compact. The following example does exactly the same thing: def\tgetNGrams(wordlist,\tn):\t\t\t\t\tngrams\t=\t[]\t\t\t\t\tfor\ti\tin\trange(len(wordlist)-(n-1)):\t\t\t\t\t\t\t\t\tngrams.append(wordlist[i:i+n])\t\t\t\t\treturn\tngrams\tA concept that may still be confusing to you are the two function arguments. Notice that our function has two variable names in the parentheses after its name when we declared it: wordlist, n. These two variables are the function arguments. When you call (run) this function, these variables will be used by the function for its solution. Without these arguments there is not enough information to do the calculations. In this case, the two pieces of information are the list of words you want to turn into n-grams (wordlist), and the number of words you want in each n-gram (n). For the function to work it needs both, so you call it in like this (save the following as useGetNGrams.py and run): #useGetNGrams.py\t\timport\tobo\t\twordstring\t=\t'it\twas\tthe\tbest\tof\ttimes\tit\twas\tthe\tworst\tof\ttimes\t'\twordstring\t+=\t'it\twas\tthe\tage\tof\twisdom\tit\twas\tthe\tage\tof\tfoolishness'\tallMyWords\t=\twordstring.split()\t\tprint\tobo.getNGrams(allMyWords,\t5)\tNotice that the arguments you enter do not have to have the same names as the arguments named in the function declaration. Python knows to use allMyWords everywhere in the function that wordlist appears, since this is given as the first argument. Likewise, all instances of n will be replaced by the integer 5 in this case. Try changing the 5 to a string, such as \"elephants\" and see what happens when you run your program. Note that because n is being used as an integer, you have to ensure the argument sent is also an integer. The same is true for strings, floats or any other variable type sent as an argument. You can also use a Python shell to play around with the code to get a better understanding of how it works. Paste the function declaration for getNGrams (either of the two functions above) into your Python shell.    \n 154 test1\t=\t'here\tare\tfour\twords'\ttest2\t=\t'this\ttest\tsentence\thas\teight\twords\tin\tit'\t\tgetNGrams(test1.split(),\t5)\t->\t[]\t\tgetNGrams(test2.split(),\t5)\t->\t[['this',\t'test',\t'sentence',\t'has',\t'eight'],\t['test',\t'sentence',\t'has',\t'eight',\t'words'],\t['sentence',\t'has',\t'eight',\t'words',\t'in'],\t['has',\t'eight',\t'words',\t'in',\t'it']]\tThere are two concepts that we see in this example of which you need to be aware. Firstly, because our function expects a list of words rather than a string, we have to convert the strings into lists before our function can handle them. We could have done this by adding another line of code above the function call, but instead we used the split method directly in the function argument as a bit of a shortcut. Secondly, why did the first example return an empty list rather than the n-grams we were after? In test1, we have tried to ask for an n-gram that is longer than the number of words in our list. This has resulted in a blank list. In test2 we have no such problem and get all possible 5-grams for the longer list of words. If you wanted to you could adapt your function to print a warning message or to return the entire string instead of an empty list. We now have a way to extract all possible n-grams from a body of text. In the next lesson, we can focus our attention on isolating those n-grams that are of interest to us. Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each chapter you can download the \"programming-historian\" zip file to make sure you have the correct code. If you are following along with the Mac / Linux version you may have to open the obo.py file and change \"file:///Users/username/Desktop/programming-historian/\" to the path to the directory on your own computer. programming-historian [Mac / Linux]  (zip: http://programminghistorian.org/assets/programming-historian-mac-linux.zip) programming-historian [Windows]  (zip: http://programminghistorian.org/assets/programming-historian-windows.zip)   \n 155 If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Output Keywords in Context into an HTML File with Python’.175 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        175\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Output\tKeywords\tin\tContext\tinto\tan\tHTML\tFile\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 156 20. Creating New Items in Zotero Amanda Morton – 2013   Editor’s Note: This is the second of three lessons on the ‘Zotero API’. You may find it easier to complete this tutorial if you have already completed the previous one: ‘Intro to the Zotero API’.176  Using Python to Create an New Zotero Item In ‘Intro to the Zotero API’, you learned a little bit about Zotero;177 now you can access some of its functions using Python scripts. In this lesson, you will create a new item in a Zotero library and add some basic metadata such as title and date. Creating a new Zotero Item It will be helpful to remember that Zotero began as a citation management system, and that an item on Zotero contains only metadata; it’s a bit like a library calling card. To upload file contents into Zotero, you would create an attachment to that item. But for now you will start by creating a new Zotero Item and assigning some information to metadata fields. Your first step is to import the python modules that you will need for this program. from\tlibZotero\timport\tzotero\timport\turllib2\timport\tdatetime\tYour next line of code will connect to the Zotero group library for this lesson using the unique group id and API key. (You can also replace the first number in the line with your own group or user ID, but if you are trying to connect to an individual user library, you must change the word group to the word user and create your own API key.) #links\tto\tzotero\tgroup\tlibrary\tzlib\t=\tzotero.Library('group',\t'155975','<null>',\t'f4Bfk3OTYb7bukNwfcKXKNLG')\tNow that you have imported the required modules and connected to your Zotero library, you can create a new item and assign it some metadata. Start by using the following code to create a new item of the type document and set the title to Python Lesson Document.                                                         176\tAmanda\tMorton,\t‘Intro\tto\tthe\tZotero\tAPI’,\tthe\tProgramming\tHistorian\t(2013).\t177\tZotero:\thttps://www.zotero.org/\t \n 157 #create\ta\tnew\titem\tof\ttype\tdocument\tnewItem\t=\tzotero.getTemplateItem('document')\t\t#sets\tthe\ttitle\tof\tthe\titem\tto\tPython\tLesson\tDocument\tnewItem.set('title',\t'Python\tLesson\tDocument')\tNext you will add two more types of metadata to your item. First, you will add an abstract note, which is basically a short description of the item you have created. Then you will set the item’s creation date to the current date. #adds\ta\tnew\tabstract\tnote\tnewItem.set('abstractNote',\t'Created\tusing\ta\tzotero\tpython\tlibrary\tand\tthe\twrite\tapi')\t\t#sets\tdate\tto\tcurrent\tdate\tnow\t=\tdatetime.datetime.today().strftime(\"%Y-%m-%d\")\tnewItem.set('date',\tnow)\tNow that you have set the important metadata for your item, you can make a request to the API to create that item. This code has set the writeFailure property to display an error message if the item is not successfully created. #\tmake\tthe\trequest\tto\tthe\tAPI\tto\tcreate\tthe\titem\t#\ta\tZotero\tItem\tobject\twill\tbe\treturned\t#\tif\tthe\tcreation\twent\tokay\tit\twill\thave\ta\twriteFailure\tproperty\tset\tto\tFalse\tcreatedItem\t=\tzlib.createItem(newItem)\tif\tcreatedItem.writeFailure\t!=\tFalse:\t\t\t\tprint(createdItem.writeFailure['code'])\t\t\t\tprint(createdItem.writeFailure['message'])\tYour last step is to add a tag to your new item. The following code will tag your item as python lesson and update the item with the new tag. Just as in the last segment, this code contains a writeFailure property that will print an error message if the item has not updated correctly. #adds\ta\tnew\ttag\tto\tthe\tnew\titem\ttagname\t=\t'python\tlesson'\t\t#in\tthe\tbracket\t(tagname,\t'<tag\ttype:0>')\tcreatedItem.addTag(tagname,\t'0')\t\t#updates\tthe\titem\twith\tthe\tnew\ttag\tupdatedItem\t=\tzlib.writeUpdatedItem(createdItem)\tif\tupdatedItem.writeFailure\t!=\tFalse:\t\t\t\tprint(\"Error\tupdating\titem\")\t\t\t\tprint(updatedItem.writeFailure['code'])\t\t\t\tprint(updatedItem.writeFailure['message'])\tAt last, you have created a new item with a title and a tag name. This last line of code will confirm the item you have just created. print\t'Created\tnew\titem\t<%s>\twith\tnew\ttag\t<%s>'\t%\t(createdItem.title,\ttagname)\tIf all has gone according to plan, your output should look like this:  \n 158 Created\tnew\titem\t<Python\tLesson\tDocument>\twith\tnew\ttag\t<python\tlesson>\tYou can also check your Zotero library to find the document that you made using Python. The title, abstract, and date should be filled out, and the tag should appear also. By editing the program above, you can create items with different types (such as books, journal articles, or newspapers) and specify more precise titles, creation dates, and tags. To see a list of all the Item Types available in the Zotero API, use your browser to navigate to this URL: https://api.zotero.org/itemTypes\tYou can then see the fields available in each Item Type template by navigating to the following URL, replacing document with the key for the Item Type that interests you: https://api.zotero.org/items/new?itemType=document\tFor example, the list of Item Types returned by the first URL shows a type called videoRecording. In our code above, you could request a template for that type by changing the document argument in our getItemTemplate() function with videoRecording. To see which fields are available in this template, you could navigate in your browser to the appropriate URL: https://api.zotero.org/items/new?itemType=videoRecording\tFor more details, see the documentation on write requests for the Zotero API.178   Editorial Note: This is the second of three lessons on the ‘Zotero API’. The final lesson in this sequence is ‘Counting Frequencies from Zotero Items’.179 About the Author Amanda Morton is a DH Fellow at the Center for History and New Media. \n                                                        178\t‘Zotero\tWeb\tAPI\tWrite\tRequests’,\tZotero:\thttps://www.zotero.org/support/dev/web_api/v3/write_requests\t179\tSpencer\tRoberts,\t‘Counting\tFrequencies\tfrom\tZotero\tItems’,\tThe\tProgramming\tHistorian\t(2013).\t \n 159 21. Intro to Beautiful Soup Jeri Wieringa – 2012  What is Beautiful Soup? This tutorial uses Python 2.7.2 and BeautifulSoup 4.180 It assumes basic knowledge of HTML, CSS, and the Document Object Model.181 It also assumes some knowledge of Python. For a more basic introduction to Python, see Working with Text Files.182 Most of the work is done in the terminal. For an introduction to using the terminal, see the Scholar’s Lab Command Line Bootcamp tutorial.183 “You didn’t write that awful page. You’re just trying to get some data out of it. Beautiful Soup is here to help.” (Opening lines of Beautiful Soup).184 Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. Say you’ve found some webpages that display data relevant to your research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web. The Beautiful Soup documentation will give you a sense of variety of things that the Beautiful Soup library will help with, from isolating titles and links, to extracting all of the text from the html tags, to altering the HTML within the document you’re working with. Installing Beautiful Soup Installing Beautiful Soup is easiest if you have pip or another Python installer already in place. If you don’t have pip, run through a quick tutorial by Fred Gibbs (2013) on installing python modules to get it                                                         180\t‘Python\t2.7\tRelease’,\thttps://www.python.org/download/releases/2.7/\t;\t‘Beautiful\tSoup\t4.4.0\tDocumentation’,\thttp://www.crummy.com/software/BeautifulSoup/bs4/doc/\t181\t‘HTML’,\tWikipedia:\thttps://en.wikipedia.org/wiki/HTML;\t‘Cascading\tStyle\tSheets’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Cascading_Style_Sheets\t;\t‘Document\tObject\tModel’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Document_Object_Model;\t182\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Working\twith\tText\tFiles\tin\tPython’,\tProgramming\tHistorian\t2012.\t183\t‘The\tCommand\tLine’,\tThe\tPraxis\tProgram\tat\tthe\tScholar’s\tLab:\thttp://praxis.scholarslab.org/scratchpad/bash/\t184\t‘Beautiful\tSoup\t4.4.0\tDocumentation’,\thttp://www.crummy.com/software/BeautifulSoup/bs4/doc/\t \n 160 running.185 Once you have pip installed, run the following command in the terminal to install Beautiful Soup: pip\tinstall\tbeautifulsoup4\tYou may need to preface this line with “sudo”, which gives your computer permission to write to your root directories and requires you to re-enter your password. This is the same logic behind you being prompted to enter your password when you install a new program. With sudo, the command is: sudo\tpip\tinstall\tbeautifulsoup4\t \n The Power of Sudo: 'Sandwich' by XKCD. Application: Extracting names and URLs from an HTML page Preview: Where we are going Because I like to see where the finish line is before starting, I will begin with a view of what we are trying to create. We are attempting to go from a search results page where the html page looks like this: <table\tborder=\"1\"\tcellspacing=\"2\"\tcellpadding=\"3\">\t<tbody>\t<tr>\t<th>Member\tName</th>\t<th>Birth-Death</th>\t</tr>\t<tr>\t<td><a\thref=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\">ADAMS,\tGeorge\tMadison</a></td>\t<td>1837-1920</td>\t</tr>\t<tr>\t<td><a\thref=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\">ALBERT,\tWilliam\tJulian</a></td>\t<td>1816-1879</td>\t</tr>\t                                                        185\tFred\tGibbs,\t‘Installing\tPython\tModules\twith\tpip’,\tThe\tProgramming\tHistorian\t(2013).\t\n \n 161 <tr>\t<td><a\thref=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077\">ALBRIGHT,\tCharles</a></td>\t<td>1830-1880</td>\t</tr>\t</tbody>\t</table>\tto a CSV file with names and urls that looks like this: \"ADAMS,\tGeorge\tMadison\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\t\"ALBERT,\tWilliam\tJulian\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\t\"ALBRIGHT,\tCharles\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077\tusing a Python script like this: from\tbs4\timport\tBeautifulSoup\timport\tcsv\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\tf\t=\tcsv.writer(open(\"43rd_Congress.csv\",\t\"w\"))\tf.writerow([\"Name\",\t\"Link\"])\t\t\t\t#\tWrite\tcolumn\theaders\tas\tthe\tfirst\tline\t\tlinks\t=\tsoup.find_all('a')\tfor\tlink\tin\tlinks:\t\t\t\t\tnames\t=\tlink.contents[0]\t\t\t\t\tfullLink\t=\tlink.get('href')\t\t\t\t\t\tf.writerow([names,fullLink])\tThis tutorial explains to how to assemble the final code. Get a webpage to scrape The first step is getting a copy of the HTML page(s) want to scrape. You can combine BeautifulSoup with urllib3 to work directly with pages on the web.186 This tutorial, however, focuses on using BeautifulSoup with local (downloaded) copies of html files. The Congressional database that we’re using is not an easy one to scrape because the URL for the search results remains the same regardless of what you’re searching for. While this can be bypassed programmatically, it is easier for our purposes to go to:  http://bioguide.congress.gov/biosearch/biosearch.asp And search for Congress number 43, and to save a copy of the results page.                                                         186\t‘urllib3\tDocumentation’\turllib3:\thttp://urllib3.readthedocs.org/en/latest/\t \n 162 \n BioGuide Interface Search for 43rd Congress  \n BioGuide Results We want to download the HTML behind this page. Selecting “File” and “Save Page As …” from your browser window will accomplish this (life will be easier if you avoid using spaces in your filename). I have used “43rd-congress.html”. Move the file into the folder you want to work in. (To learn how to automate the downloading of HTML pages using Python, see Ian Milligan’s tutorial on ‘Automated Downloading with Wget’ or Adam Crymble’s ‘Downloading Multiple Records Using Query Strings’.)187                                                         187\tIan\tMilligan,\t‘Automated\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2012);\tAdam\tCrymble,\t‘Downloading\tMultiple\tRecords\tUsing\tQuery\tStrings’,\tThe\tProgramming\tHistorian\t(2012).\t\n \n 163 Identify content One of the first things Beautiful Soup can help us with is locating content that is buried within the HTML structure. Beautiful Soup allows you to select content based upon tags (example: soup.body.p.b finds the first bold item inside a paragraph tag inside the body tag in the document). To get a good view of how the tags are nested in the document, we can use the method “prettify” on our soup object. Create a new text file called “soupexample.py” in the same location as your downloaded HTML file. This file will contain the Python script that we will be developing over the course of the tutorial. To begin, import the Beautiful Soup library, open the HTML file and pass it to Beautiful Soup, and then print the “pretty” version in the terminal. from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup(open(\"43rd-congress.html\"))\t\tprint(soup.prettify())\tSave “soupexample.py” in the folder with your HTML file and go to the command line. Navigate (use ‘cd’) to the folder you’re working in and execute the following: python\tsoupexample.py\tYou should see your terminal window fill up with a nicely indented version of the original html text (see Figure below). This is a visual representation of how the various tags relate to one another.  \n 'Pretty' print of the BioGuide results  \n \n 164 Using BeautifulSoup to select particular content Remember that we are interested in only the names and URLs of the various member of the 43rd Congress. Looking at the “pretty” version of the file, the first thing to notice is that the data we want is not too deeply embedded in the HTML structure. Both the names and the URLs are, most fortunately, embedded in “<a>” tags. So, we need to isolate out all of the “<a>” tags. We can do this by updating the code in “soupexample.py” to the following: from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tlinks\t=\tsoup.find_all('a')\t\tfor\tlink\tin\tlinks:\t\t\t\t\tprint\tlink\tSave and run the script again to see all of the anchor tags in the document. python\tsoupexample.py\tOne thing to notice is that there is an additional link in our file – the link for an additional search.  \n The URLs and names, plus one addition  We can get rid of this with just a few lines of code. Going back to the pretty version, notice that this last “<a>” tag is not within the table but is within a “<p>” tag. \n \n 165 \n The Rogue Link  Because Beautiful Soup allows us to modify the HTML, we can remove the “<a>” that is under the “<p>” before searching for all the “<a>” tags. To do this, we can use the “decompose” method, which removes the specified content from the “soup”. Do be careful when using “decompose”—you are deleting both the HTML tag and all of the data inside of that tag. If you have not correctly isolated the data, you may be deleting information that you wanted to extract. Update the file as below and run again. from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\tlinks\t=\tsoup.find_all('a')\t\tfor\tlink\tin\tlinks:\t\t\t\t\tprint\tlink\tSuccess! We have isolated out all of the links we want and none of the links we don’t! \n \n 166 \n Successfully isolated only names and URLs  Stripping Tags and Writing Content to a CSV file But, we are not done yet! There are still HTML tags surrounding the URL data that we want. And we need to save the data into a file in order to use it for other projects. In order to clean up the HTML tags and split the URLs from the names, we need to isolate the information from the anchor tags. To do this, we will use two powerful, and commonly used Beautiful Soup methods: contents and get. Where before we told the computer to print each link, we now want the computer to separate the link into its parts and print those separately. For the names, we can use link.contents. The “contents” method isolates out the text from within html tags. For example, if you started with <h2>This\tis\tmy\tHeader\ttext</h2>\tYou would be left with “This is my Header text” after applying the contents method. In this case, we want the contents inside the first tag in “link”. (There is only one tag in “link”, but since the computer doesn’t realize that, we must tell it to use the first tag.) For the URL, however, “contents” does not work because the URL is part of the HTML tag. Instead, we will use “get”, which allow us to pull the text associated with (is on the other side of the “=” of) the “href” element.  \t\n \n 167 from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\tlinks\t=\tsoup.find_all('a')\tfor\tlink\tin\tlinks:\t\t\t\t\tnames\t=\tlink.contents[0]\t\t\t\t\tfullLink\t=\tlink.get('href')\t\t\t\t\tprint\tnames\t\t\t\t\tprint\tfullLink\t\n All HTML tags have been removed. Finally, we want to use the CSV library to write the file. First, we need to import the CSV library into the script with “import csv.” Next, we create the new CSV file when we “open” it using “csv.writer”. The “w” tells the computer to “write” to the file. And to keep everything organized, let’s write some column headers. Finally, as each line is processed, the name and URL information is written to our CSV file. from\tbs4\timport\tBeautifulSoup\timport\tcsv\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\tf\t=\tcsv.writer(open(\"43rd_Congress.csv\",\t\"w\"))\tf.writerow([\"Name\",\t\"Link\"])\t#\tWrite\tcolumn\theaders\tas\tthe\tfirst\tline\t\tlinks\t=\tsoup.find_all('a')\tfor\tlink\tin\tlinks:\t\t\t\t\tnames\t=\tlink.contents[0]\t\t\t\t\tfullLink\t=\tlink.get('href')\t\t\t\t\t\tf.writerow([names,\tfullLink])\t\n \n 168 When executed, this gives us a clean CSV file that we can then use for other purposes. \n CSV file of results We have solved our puzzle and have extracted names and URLs from the HTML file.  But wait! What if I want ALL of the data? Let’s extend our project to capture all of the data from the webpage. We know all of our data can be found inside a table, so let’s use “<tr>” to isolate the content that we want. from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\ttrs\t=\tsoup.find_all('tr')\tfor\ttr\tin\ttrs:\t\t\t\t\tprint\ttr\tLooking at the print out in the terminal, you can see we have selected a lot more content than when we searched for “<a>” tags. Now we need to sort through all of these lines to separate out the different types of data. \n \n 169 \n All of the Table Row Data Extracting the Data We can extract the data in two moves. First, we will isolate the link information; then, we will parse the rest of the table row data. For the first, let’s create a loop to search for all of the anchor tags and “get” the data associated with “href”. from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\ttrs\t=\tsoup.find_all('tr')\t\tfor\ttr\tin\ttrs:\t\t\t\t\tfor\tlink\tin\ttr.find_all('a'):\t\t\t\t\t\t\t\t\tfulllink\t=\tlink.get\t('href')\t\t\t\t\t\t\t\t\tprint\tfulllink\t#print\tin\tterminal\tto\tverify\tresults\tWe then need to run a search for the table data within the table rows. (The “print” here allows us to verify that the code is working but is not necessary.) from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\ttrs\t=\tsoup.find_all('tr')\t\tfor\ttr\tin\ttrs:\t\n \n 170 \t\t\t\tfor\tlink\tin\ttr.find_all('a'):\t\t\t\t\t\t\t\t\tfulllink\t=\tlink.get\t('href')\t\t\t\t\t\t\t\t\tprint\tfulllink\t#print\tin\tterminal\tto\tverify\tresults\t\t\t\t\t\ttds\t=\ttr.find_all(\"td\")\t\t\t\t\tprint\ttds\tNext, we need to extract the data we want. We know that everything we want for our CSV file lives within table data (“td”) tags. We also know that these items appear in the same order within the row. Because we are dealing with lists, we can identify information by its position within the list. This means that the first data item in the row is identified by [0], the second by [1], etc. Because not all of the rows contain the same number of data items, we need to build in a way to tell the script to move on if it encounters an error. This is the logic of the “try” and “except” block. If a particular line fails, the script will continue on to the next line. from\tbs4\timport\tBeautifulSoup\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\ttrs\t=\tsoup.find_all('tr')\t\tfor\ttr\tin\ttrs:\t\t\t\t\tfor\tlink\tin\ttr.find_all('a'):\t\t\t\t\t\t\t\t\tfulllink\t=\tlink.get\t('href')\t\t\t\t\t\t\t\t\tprint\tfulllink\t#print\tin\tterminal\tto\tverify\tresults\t\t\t\t\t\ttds\t=\ttr.find_all(\"td\")\t\t\t\t\t\ttry:\t#we\tare\tusing\t\"try\"\tbecause\tthe\ttable\tis\tnot\twell\tformatted.\tThis\tallows\tthe\tprogram\tto\tcontinue\tafter\tencountering\tan\terror.\t\t\t\t\t\t\t\t\tnames\t=\tstr(tds[0].get_text())\t#\tThis\tstructure\tisolate\tthe\titem\tby\tits\tcolumn\tin\tthe\ttable\tand\tconverts\tit\tinto\ta\tstring.\t\t\t\t\t\t\t\t\tyears\t=\tstr(tds[1].get_text())\t\t\t\t\t\t\t\t\tpositions\t=\tstr(tds[2].get_text())\t\t\t\t\t\t\t\t\tparties\t=\tstr(tds[3].get_text())\t\t\t\t\t\t\t\t\tstates\t=\tstr(tds[4].get_text())\t\t\t\t\t\t\t\t\tcongress\t=\ttds[5].get_text()\t\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\tprint\t\"bad\ttr\tstring\"\t\t\t\t\t\t\t\t\tcontinue\t#This\ttells\tthe\tcomputer\tto\tmove\ton\tto\tthe\tnext\titem\tafter\tit\tencounters\tan\terror\t\t\t\t\t\tprint\tnames,\tyears,\tpositions,\tparties,\tstates,\tcongress\tWithin this we are using the following structure: years\t=\tstr(tds[1].get_text())\tWe are applying the “get_text” method to the 2nd element in the row (because computers count beginning with 0) and creating a string from the result. This we assign to the variable “years”, which we will use to create  \n 171 the CSV file. We repeat this for every item in the table that we want to capture in our file. Writing the CSV file The last step in this file is to create the CSV file. Here we are using the same process as we did in Part I, just with more variables. As a result, our file will look like:  from\tbs4\timport\tBeautifulSoup\timport\tcsv\t\tsoup\t=\tBeautifulSoup\t(open(\"43rd-congress.html\"))\t\tfinal_link\t=\tsoup.p.a\tfinal_link.decompose()\t\tf=\tcsv.writer(open(\"43rd_Congress_all.csv\",\t\"w\"))\t\t\t#\tOpen\tthe\toutput\tfile\tfor\twriting\tbefore\tthe\tloop\tf.writerow([\"Name\",\t\"Years\",\t\"Position\",\t\"Party\",\t\"State\",\t\"Congress\",\t\"Link\"])\t#\tWrite\tcolumn\theaders\tas\tthe\tfirst\tline\t\ttrs\t=\tsoup.find_all('tr')\t\tfor\ttr\tin\ttrs:\t\t\t\t\tfor\tlink\tin\ttr.find_all('a'):\t\t\t\t\t\t\t\t\tfullLink\t=\tlink.get\t('href')\t\t\t\t\t\ttds\t=\ttr.find_all(\"td\")\t\t\t\t\t\ttry:\t#we\tare\tusing\t\"try\"\tbecause\tthe\ttable\tis\tnot\twell\tformatted.\tThis\tallows\tthe\tprogram\tto\tcontinue\tafter\tencountering\tan\terror.\t\t\t\t\t\t\t\t\tnames\t=\tstr(tds[0].get_text())\t#\tThis\tstructure\tisolate\tthe\titem\tby\tits\tcolumn\tin\tthe\ttable\tand\tconverts\tit\tinto\ta\tstring.\t\t\t\t\t\t\t\t\tyears\t=\tstr(tds[1].get_text())\t\t\t\t\t\t\t\t\tpositions\t=\tstr(tds[2].get_text())\t\t\t\t\t\t\t\t\tparties\t=\tstr(tds[3].get_text())\t\t\t\t\t\t\t\t\tstates\t=\tstr(tds[4].get_text())\t\t\t\t\t\t\t\t\tcongress\t=\ttds[5].get_text()\t\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\tprint\t\"bad\ttr\tstring\"\t\t\t\t\t\t\t\t\tcontinue\t#This\ttells\tthe\tcomputer\tto\tmove\ton\tto\tthe\tnext\titem\tafter\tit\tencounters\tan\terror\t\t\t\t\t\tf.writerow([names,\tyears,\tpositions,\tparties,\tstates,\tcongress,\tfullLink])\tYou’ve done it! You have created a CSV file from all of the data in the table, creating useful data from the confusion of the html page. About the Author Jeri Wieringa is the digital publishing production lead for the George Mason University Libraries and a doctoral candidate in history at George Mason University.  \n 172 22. Introduction to the Bash Command Line Ian Milligan and James Baker – 2014    Introduction Many of the lessons at the Programming Historian require you to enter commands through a Command-Line Interface. The usual way that computer users today interact with their system is through a Graphical-User Interface, or GUI. This means that when you go into a folder, you click on a picture of a file folder; when you run a program, you click on it; and when you browse the web, you use your mouse to interact with various elements on a webpage. Before the rise of GUIs in the late 1980s, however, the primary way to interact with a computer was through a command-line interface. \n GUI of Ian Milligan's Computer Command-line interfaces have advantages for computer users who need more precision in their work -- such as digital historians. They allow for more detail when running some programs, as you can add modifiers to specify exactly how you want your program to run. Furthermore, they can \n \n 173 be easily automated through scripts,188 which are essentially recipes of text-based commands. There are two main command-line interfaces, or 'shells,' that many digital historians use. On OS X or many Linux installations, the shell is known as bash, or the 'Bourne-again shell.' For users on Windows-based systems, the command-line interface is by default MS-DOS-based, which uses different commands and syntax,189 but can often achieve similar tasks. This tutorial provides a basic introduction to the bash terminal, and Windows users can follow along by installing popular shells such as Cygwin190 or Git Bash (see below). This lesson uses a Unix\tshell,191 which is a command-line interpreter that provides a user interface for the Unix operating system192 and for Unix-like systems. This lesson will cover a small number of basic commands. By the end of this tutorial you will be able to navigate through your file system and find files, open them, perform basic data manipulation tasks such as combining and copying files, as well as both reading them and making relatively simple edits. These commands constitute the building blocks upon which more complex commands can be constructed to fit your research data or project. Readers wanting a reference guide that goes beyond this lesson are recommended to read Deborah S. Ray and Eric J. Ray, Unix and Linux: Visual Quickstart Guide, 4th edition (2009). Windows Only: Installing Git Bash For those on OS X, and most Linux installations, you're in luck — you already have a bash shell installed. For those of you on Windows, you'll need to take one extra step and install Git Bash. This can be installed by downloading the most recent 'Full installer' at the the msysgit featured downloads list.193 Instructions for installation are available at Open Hatch.194 Opening Your Shell Let's start up the shell. In Windows, run Git Bash from the directory that you installed it in. You will have to run it as an administrator - to do so, right click on the program and select 'Run as Administrator.' In OS X, by default the shell is located in: Applications\t->\tUtilities\t->\tTerminal                                                         188\tSee\t‘Chapter\t1.\tBash\tand\tBash\tscripts’,\tBash\tGuide\tfor\tBeginners:\thttp://www.tldp.org/LDP/Bash-Beginners-Guide/html/chap_01.html\t189\t‘Syndax’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Syntax\t190\t‘Cygwin’:\thttps://www.cygwin.com/\t191\t‘Unix\tShell’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Unix_shell\t192\t‘Unix’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Unix\t193\t‘Git-1.9.5’:\thttps://github.com/msysgit/msysgit/releases/\t194\t‘Install\tGit\tBash’,\tOpen\tHatch:\thttps://openhatch.org/missions/windows-setup/install-git-bash\t \n 174  Termina.app program on OS X When you run it, you will see this window. \n A blank terminal screen on our OS X workstation You might want to change the default visual appearance of the terminal, as eyes can strain at repeatedly looking at black text on a white background. In the default OS X application, you can open the 'Settings' menu in 'Preferences' under Terminal. Click on the 'Settings' tab and change it to a new colour scheme. We personally prefer something with a bit less contrast between background and foreground, as you'll be staring at this a great deal. 'Novel' is a soothing one as is the popular Solarized suite of colour palettes.195 For Windows users, a similar effect can be achieved using the Git Bash Properties tab. To reach this, right-click anywhere in the top bar and select Properties. \n The Settings Screen on the OS X Terminal Shell Application                                                         195\t‘Solarized’:\thttp://ethanschoonover.com/solarized\t\n \n 175 Once you are happy with the interface, let's get started. Moving Around Your Computer's File System If, when opening a command window, you are unsure of where you are in a computer's file system, the first step is to find out what directory you are in. Unlike in a graphical system, when in a shell you cannot be in multiple directories at once. When you open up your file explorer on your desktop, it's revealing files that are within a directory. You can find out what directory you are in through the pwd command, which stands for \"print working directory.\" Try inputting: pwd and hitting enter. If you're on OS X or Linux, your computer will probably display /users/USERNAME with your own user name in place of USERNAME. For example, Ian's path on OS X is /users/ianmilligan1/. Here is where you realize that those on Windows and those on OS X/Linux will have slightly different experiences. On Windows, James is at: c/users/jbaker There are minor differences, but fear not; once you're moving and manipulating files, these platform divergences can fade into the background. To orient ourselves, let's get a listing of what files are in this directory. Type ls and you will see a list of every file and directory within your current location. Your directory may be cluttered or it may be pristine, but you will at a minimum see some familiar locations. On OS X, for example, you'll see Applications, Desktop, Documents, Downloads, Library, Pictures, etc. You may want more information than just a list of files. You can do this by specifying various flags to go with our basic commands. These are additions to a command that provide the computer with a bit more guidance of what sort of output or manipulation you want. To get a list of these, OS X/Linux users can turn to the built-in help program. OS X/Linux users type man\tls\t \n 176 \n The Manual page for the LS command Here, you see a listing of the name of the command, the way that you can format this command and what it does. Many of these will not make sense at this stage, but don't worry; over time you will become more familiar with them. You can explore this page in a variety of ways: the spacebar moves down a page, or you can arrow down and arrow up throughout the document. To leave the manual page, press q and you will be brought back to the command line where you were before entering the manual page. Try playing around with the man page for the other command you have learned so far, pwd. Windows users can use the help command, though this command has fewer features than man on OS X/Linux. Enter help to see the help available, and help\tpwd for an example of the command's output. Let's try using a few of those options you saw in the man page for ls. Perhaps you only want to see TXT files that are in our home directory. Type ls\t*.txt which returns a list of text files, if you have any in your home directory (you may not, and that is OK as well). The * command is a wildcard — it stands for 'anything.' So, in this case, you're indicating that anything that fits the pattern: [anything.txt] \n \n 177 will be displayed. Try out different combinations. If, for example, you had several files in the format 1-Canadian.txt, 2-Canadian.txt, and so forth, the command ls\t*-Canadian.txt would display them all but exclude all other files (those that do not match the pattern). Say you want more information. In that long man page, you saw an option that might be useful: -l\t\t\t\t\t\t(The\tlowercase\tletter\t``ell''.)\t\tList\tin\tlong\tformat.\t\t(See\tbelow.)\t\tIf\t\t\t\t\t\t\t\t\tthe\toutput\tis\tto\ta\tterminal,\ta\ttotal\tsum\tfor\tall\tthe\tfile\tsizes\tis\tout-\t\t\t\t\t\t\t\t\tput\ton\ta\tline\tbefore\tthe\tlong\tlisting.\tSo, if you type ls\t-l the computer returns a long list of files that contains information similar to what you'd find in your finder or explorer: the size of the files in bites, the date it was created or last modified, and the file name. However, this can be a bit confusing: you see that a file test.html is '6020' bits large. In commonplace language, you are more used to units of measurement like bytes, kilobytes, megabytes, and gigabytes. Luckily, there's another flag: -h\t\t\t\t\t\tWhen\tused\twith\tthe\t-l\toption,\tuse\tunit\tsuffixes:\tByte,\tKilobyte,\t\t\t\t\t\t\t\t\tMegabyte,\tGigabyte,\tTerabyte\tand\tPetabyte\tin\torder\tto\treduce\tthe\tnumber\t\t\t\t\t\t\t\t\tof\tdigits\tto\tthree\tor\tless\tusing\tbase\t2\tfor\tsizes.\tWhen you want to use two flags, you can just run them together. So, by typing ls\t-lh you receive output in a human-readable format; you learn that that 6020 bits is also 5.9KB, that another file is 1 megabyte, and so forth. These options are very important. In other lessons within the Programming Historian, you'll see them. ‘Wget’, ‘MALLET’, and ‘Pandoc’ all use the same syntax.196 Luckily, you do not need to memorize syntax; instead, keep these lessons handy so you can take a quick peek if you need to tweak something. These lessons can all be done in any order. You've now spent a great deal of time in your home directory. Let's go somewhere else. You can do that through the cd or Change Directory command.                                                         196\tSee:\tKellen\tKurchinski,\t‘Applied\tArchival\tDownloading\twith\tWget’\t(2013);\tShawn\tGraham,\tScott\tWeingart,\tand\tIan\tMilligan,\t‘Getting\tStarted\twith\tTopic\tModeling\tand\tMALLET’\t(2012);\tand\tDennis\tTenen\tand\tGrant\tWythoff,\t‘Sustainable\tAuthorship\tin\tPlain\tText\tusing\tPandoc\tand\tMarkdown’\tThe\tProgramming\tHistorian\t(2014).\t \n 178 If you type cd\tdesktop you are now on your desktop. This is akin to you 'double-clicking' on the 'desktop' folder within a file explorer. To double check, type pwd and you should see something like: /Users/ianmilligan1/desktop Try playing around with those earlier commands: explore your current directory using the ls command. If you want to go back, you can type cd\t.. This moves us 'up' one directory, putting us back in /Users/ianmilligan1/. If you ever get completely lost, the command cd\t-- will bring you right back to the home directory, right where you started. Try exploring: visit your documents directory, your pictures, folders you might have on your desktop. Get used to moving in and out of directories. Imagine that you are navigating a tree structure.197 If you're on the desktop, you won't be able to cd\tdocuments as it is a 'child' of your home directory, whereas your Desktop is a 'sibling' of the Documents folder. To get to a sibling, you have to go back to the common parent. To do this, you will have to back up to your home directory (cd\t..) and then go forward again to cd\tdocuments. Being able to navigate your file system using the bash shell is very important for many of the lessons at the Programming Historian. As you become more comfortable, you'll soon find yourself skipping directly to the directory that you want. In our case, from anywhere on our system, you could type cd\t/users/ianmilligan1/mallet-2.0.7 or, on Windows, something like cd\tc:\\mallet-2.0.7\\ and be brought to our MALLET directory for topic modeling.198 Finally, try open\t.                                                         197\t‘Tree\tStructure’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Tree_structure\t198\tShawn\tGraham,\tScott\tWeingart,\tand\tIan\tMilligan,\t‘Getting\tStarted\twith\tTopic\tModeling\tand\tMALLET’\tThe\tProgramming\tHistorian\t(2012).\t \n 179 in OS X or explorer\t. in Windows. That command will open up your GUI at the current directory. Make sure to leave a space between open or explorer and the period. Interacting with Files As well as navigating directories, you can interact with files on the command line: you can read them, open them, run them, and even edit them, often without ever having to leave the interface. There is some debate over why one would do this. The primary reason is the seamless experience of working on the command line: you never have to pick up your mouse or touch your track pad, and, although it has a steep learning curve it can eventually become a sole writing environment. Furthermore, many programs require you to use the command line to operate with them. Since you'll be using programs on the command line, it can often be quicker to make small edits without switching into a separate program. For some of these arguments, see Jon Beltran de Heredia's \"Why, oh WHY, do those #?@! nutheads use vi?\".199 Here's a few basic ways to do interact with files. First, you can create a new directory so you can engage with text files. We will create it on your desktop, for convenience's sake. You can always move it later. Navigate to your desktop using your shell, and type: mkdir\tProgHist-Text This creates a directory named, you guessed it, 'ProgHist-Text.' In general, it's good to avoid putting spaces in your filenames and directories when using the command line (there are workarounds, of course, but this approach is simpler). You can look at your desktop to verify it has worked. Now, move into that directory (remember, that would be cd\tProgHist-Text). But wait! There's a trick to make things a bit quicker. Go up one directory (cd\t.. - which will take you back to the Desktop). To navigate to the ProgHist-Text directory you could type cd\tProgHist-Text. Alternatively, you could type cd\tProg and then hit tab. You will notice that the interface completes the line to cd\tProgHist-Text. Hitting tab at any time within the shell will prompt it to attempt to auto-complete the line based on the files or sub-directories in the current directory. This is case sensitive, however (i.e. in the previous example, cd\tprog would not auto complete to ProgHist-Text. Where two or more files have the same characters, the auto-complete will only fill up to the first point of difference. We would encourage using this method throughout the lesson to see how it behaves.                                                         199\tJon\tBeltran\tde\tHeredia,\t‘Why,\toh\tWHY\tdo\tthose\t#?@!\tNutheads\tuse\tvi?’,\tViEmu\t(16\tMay\t2007):\thttp://www.viemu.com/a-why-vi-vim.html\t \n 180 Now you need to find a basic text file to help us with the example. Why don't you use a book that you know is long, such as Leo Tolstoy's epic War and Peace. The text file is availiable via Project Gutenberg.200 If you have already installed wget,201 you can just type: wget\thttp://www.gutenberg.org/cache/epub/2600/pg2600.txt If you do not have wget installed, download the text itself using your browser. Go to the link above, and, in your browser, use the 'Save Page as..' command in your 'file menu.' Save it in your new 'ProgHist-Text directory.' Now, when you type ls\t-lh you see -rw-r--r--+\t1\tianmilligan1\tstaff\t3.1M\t1\tMay\t10:03\tpg2600.txt\tYou can read the text within this file in a few different ways. First, you can tell our computer that you want to read it using the standard program that you use to open text files. By default, this may be TextEdit on OS X or Notepad in Windows. To open a file, just type open\tpg2600.txt on OS X, or explorer\tpg2600.txt in Windows. This selects the default program to open that type of file, and opens it. However, you often want to just work on the command line without leaving it. You can read files within this environment as well. To try this, type: cat\tpg2600.txt The terminal window erupts and War and Peace cascades by. That's great, in theory, but you can't really make any sense of that amount of text? Instead, you may want to just look at the first or the last bit of the file. head\tpg2600.txt Provides a view of the first ten lines, whereas tail\tpg2600.txt provides a perspective on the last ten lines. This is a good way to quickly determine the contents of the file. You could add a command to change the                                                         200\tgraf\tLeo\tTolstoy,\t‘War\tand\tPeace’,\tProject\tGutenberg:\thttp://www.gutenberg.org/ebooks/2600?msg=welcome_stranger\t201\tKellen\tKurchinski,\t‘Applied\tArchival\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2013).\t \n 181 amount of lines displayed: head\t-20\tpg2600.txt, for example, would show the first twenty lines. You may also want to change the file name to something more descriptive. You can 'move' it to a new name by typing mv\tpg2600.txt\ttolstoy.txt Afterwards, when you perform a ls command, you will see that it is now tolstoy.txt. Had you wanted to duplicate it, you could also have run the copy command by typing cp\tpg2600.txt\ttolstoy.txt you will revisit these commands shortly. Now that you have used several new commands, it's time for another trick. Hit the up arrow on your keyboard. Notice that cp\tpg2600.txt\ttolstoy.txt appears before your cursor. You can continue pressing the up arrow to cycle through your previous commands. The down arrow cycles back toward your most recent command. After having read and renamed several files, you may wish to bring their text together into one file. To combine, or concatenate, two or more files, you can use the cat command. First, let's duplicate the Tolstoy file ( cp\ttolstoy.txt\ttolstoy2.txt). Now that you have two copies of War and Peace, let's put them together to make an even longer book. To combine, or concatenate, two or more files use the cat command. Type cat\ttolstoy.txt\ttolstoy2.txt and press enter. This prints, or displays, the combined files within the shell. However, it is too long to read on this window! Luckily, by using the > command, you can send the output to a new file, rather than the terminal window. Type cat\ttolstoy.txt\ttolstoy2.txt\t>\ttolstoy-twice.txt. Now, when you type ls you'll see tolstoy-twice.txt appear in your directory. When combining more than two files, using a wildcard can help avoid having to write out each filename individually. As you have seen above, *, is a place holder for zero or more characters or numbers. So, if you type cat\t*.txt\t>\teverything-together.txt and hit enter, a combination of all the .txt files in the current directory are combined in alphabetical order as everything-together.txt. This can be very useful if you need to combine a large number of smaller files within a directory so that you can work with them in a text analysis program.  \n 182 Another wildcard worth remembering is ? which is a place holder for a single character or number. Editing Text Files Directly on the Command Line If you want to read a file in its entirety without leaving the command line, you can fire up vim.202 Vim is a very powerful text editor, which is perfect for using with programs such as Pandoc203 to do word processing, or for editing your code without having to switch to another program. Best of all, it comes included with bash on both OS X and Windows. Vim has a fairly steep learning curve, so we will just touch on a few minor points. Type vim\ttolstoy.txt You should see vim come to life before you, a command-line based text editor. \n Vim If you really want to get into Vim, there is a good Vim guide available.204                                                         202\t‘Vim\t(text\teditor)’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Vim_%28text_editor%29\t203\t‘Pandoc’:\thttp://pandoc.org/\t204\t‘Vim\tDocumentation:\tquickref’:\thttp://vimdoc.sourceforge.net/htmldoc/quickref.html\t\n \n 183 Using Vim to read files is relatively simple. You can use the arrow keys to navigate around and could theoretically read War and Peace through the command line (one should get an achievement for doing that). Some quick basic navigational commands are as follows: Ctrl+F (that is, holding down your 'control key' and pressing the letter F) will move you down a page (Shift+UpArrow for Windows). Ctrl+B will move you up a page. (Shift+DownArrow for Windows users). If you want to rapidly move to the end of a line, you can press: $ and to move to the start of one, 0. You can also move between sentences by typing ) (forward) or ( (backwards). For paragraphs, use } and {. Since you are doing everything with your keyboard, rather than having to hold your arrow key down to move around a document, this lets you zip quickly back and forth. Let's scroll to the top and do a minor change, such as adding a Reader field in the heading. Move your cursor in between Author: and Translators:, like so: \n About to Insert a Field  If you just start typing, you'll get an error message or the cursor will begin jumping around. This is because you have to specify that you want to do an edit. Press the letter \n \n 184 a At the bottom of the screen, you will see --\tINSERT\t-- This means you are in insert mode. You can now type and edit text as if you are in a standard text editor. Press enter twice, then arrow\tup, and type Reader:\tA\tProgramming\tHistorian When you are done, press ESC to return to reading mode. To leave vim or to make saves, you have to enter a series of commands. Press : and you'll move to the command input line of Vim. you can enter a variety of commands here. If you want to save the file, type w to 'write' the file. If you execute that command, you will see \"tolstoy.txt\"\t[dos]\t65009L,\t3291681C\twritten\t\n After Writing the File, with our minor change  If you want to quit, type : again and then q. It will return you to the command line. As with the rest of bash, you could have also combined the two commands. Pressing : and then typing wq would have written the file and then quit. Or, if you wanted to exit without saving, q! would have quit vim and overriden the default preference to save your changes. \n \n 185 Vim is different than you are likely used to and will require more work and practice to become fluent with it. But if you are tweaking minor things in files, it is a good way to get started. As you become more comfortable, you might even find yourself writing term papers with it, by harnessing the footnoting and formatting power of Pandoc and Markdown.205 Moving, Copying, and Deleting Files Let's say you are done with this directory, and you would like to move tolstoy.txt somewhere else. First, you should create a backup copy. The shell is quite unforgiving with mistakes, and backing up is even more important than with GUIs. If you delete something here, there's no recycling bin to fish it out of. To create a backup, you can type cp\ttolstoy.txt\ttolstoy-backup.txt Now when you run a ls command you will see five files, two of which are the same: tolstoy.txt and tolstoy-backup.txt. Let's move the first of these somewhere else. By way of example, let's create a second directory on your desktop. Move up to your desktop (cd\t..) and mkdir another directory. Let's call it proghist-dest. To copy tolstoy.txt you have a few different options. you could run these commands from anywhere in the shell, or you could visit either the origin or destination directories. For this example, let's just run it from here. The basic format of the copy command is cp\t[source]\t[destination]. That is, you type cp first, and then enter the file or files that you want to copy followed by where they should go. In this case, the command cp\t/users/ianmilligan1/desktop/proghist-text/tolstoy.txt\t/users/ianmilligan1/desktop/proghist-dest/ will copy Tolstoy from the first directory to the second directory. You will have to insert your own username in place of 'ianmilligan1'. This means you now have three copies of the novel on our computer. The original, the backup and the new copy in the second directly. If you wanted to move the file, that is, not leave a copy behind, you could run the command again, swapping cp for mv; let's not do this yet. You can also copy multiple files with a single command. If you wanted to copy both the original and the backup file, you could use the wildcard command. cp\t/users/ianmilligan1/desktop/proghist-text/*.txt\t/users/ianmilligan1/desktop/proghist-dest/                                                         205\tSee:\tDennis\tTenen\tand\tGrant\tWythoff,\t‘Sustainable\tAuthorship\tin\tPlain\tText\tusing\tPandoc\tand\tMarkdown’,\tThe\tProgramming\tHistorian\t(2014).\t \n 186 This command copies all the text files from the origin directory into the destination directory. Note: If you are in the directory that you either want to move things to or from, you do not have to type out the whole directory structure. Let's do two quick examples. Change your directory to the proghist-text directory. From this location, if you wanted to copy these two files to proghist-dest, this command would work: cp\t*.txt\t/users/ianmilligan1/desktop/proghist-dest/  (on OS X, substitute the directory on Windows) Alternatively, if you were in the proghist-dest directory, this command would work: cp\t/users/ianmilligan1/desktop/proghist-text/*.txt\t./ The ./ command refers to the current directory you're in. This is a really valuable command. Finally, if you want to delete a file, for whatever reason, the command is rm, or remove. Be careful with the rm command, as you don't want to delete files that you do not mean to. Unlike deleting from within your GUI, there is no recycling bin or undo options. For that reason, if you are in doubt, you may want to exercise caution or maintain a regular backup of your data. Move to proghist-text and delete the original file by typing rm\ttolstoy.txt Check that the file is gone using the ls command. If you wanted to delete an entire directory, you have two options. you can use rmdir, the opposite of mkdir, to delete an empty directory. To delete a directory with files, you could use from the desktop: rm\t-r\tproghist-text Conclusions You may want to take a break from the terminal at this point. To do so, enter exit and you'll close your session. There are more commands to try as you get more comfortable with the command line. Some of our other favourites are du, which is a way to find out how much memory is being used (du\t-h makes it human readable — as with other commands). For those of you on OS X, top provides an overview of what processes are running (mem on Windows) and touch\tFILENAME can create a basic text file on both systems  \n 187 By this point, we hope you have a good, basic understanding of how to move around using the command line, move basic files, and make minor edits here and there. This beginner-level lesson is designed to give you some basic fluency and confidence. In the future, you may want to get involved with scripting. Have fun! Before you know it, you may find yourself liking the convenience and precision of the command line - for certain applications, at least - far more than the bulkier GUI that your system came with. Reference Guide For your convenience, here are the commands that you have learned in this lesson: Command What it does pwd Prints the 'present working directory,' letting you know where you are. ls Lists the files in the current directory man\t* Lists the manual for the command, substituted for the * cd\t* Changes the current directory to * mkdir\t* Makes a directory named * open or explorer On OS X, open followed by a file opens it; in Windows, the command explorer followed by a file name does the same thing. cat\t* cat is a versatile command. It will read a file to you if you substitute a file for *, but can also be used to combine files. head\t* Displays the first ten lines of * tail\t* Displays the last ten lines of * mv Moves a file cp Copies a file  \n 188 rm Deletes a file vim Opens up the vim document editor.  About the Authors Ian Milligan is an assistant professor of history at the University of Waterloo. James Baker is a lecturer of digital history at the University of Sussex.  \n 189 23. Counting and mining research data with Unix James Baker and Ian Milligan – 2014    Introduction This lesson will look at how research data, when organised in a clear and predictable manner, can be counted and mined using the Unix shell. The lesson builds on the lessons \"Preserving Your Research Data: Documenting and Structuring Data\"206 and \"Introduction to the Bash Command Line\".207 Depending on your confidence with the Unix shell, it can also be used as a standalone lesson or refresher. Having accumulated research data for one project, a historian might ask different questions of that same data when returning to it during a subsequent project. If this data is spread across multiple files - a series of tabulated data, a set of transcribed text, a collection of images - it can be counted and mined using simple Unix commands. The Unix shell gives you access to a range of powerful commands that can transform how you count and mine research data. This lesson will introduce you to a series of commands that use counting and mining of tabulated data, though they only scratch the surface of what the Unix shell can do. By learning just a few simple commands you will be able to undertake tasks that are impossible in Libre Office Calc, Microsoft Excel, or other similar spreadsheet programs. These commands can be easily extended for use with non-tabulated data. This lesson will also demonstrate that the options for manipulating, counting and mining data available to you will often depend on the amount of metadata, or descriptive text, contained in the filenames of the data you are using as much as the range of Unix commands you have learnt to use. Thus, even if it is not a prerequisite of working with the Unix shell, taking the time to structure your research data and filenaming conventions in a consistent and predictable manner is certainly a significant step towards getting the most out of Unix commands and being able to count and mine your research data. For the value of taking the time to make your data                                                         206\tJames\tBaker,\t‘Preserving\tYour\tResearch\tData’,\tThe\tProgramming\tHistorian\t(2014).\t207\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorian\t(2014).\t \n 190 consistent and predictable beyond matters of preservation, see \"Preserving Your Research Data\".208  Software and setup Windows users will need to install Git Bash. This can be installed by downloading the most recent installer at the git for windows webpage.209 Instructions for installation are available at Open Hatch.210 OS X and Linux users will need to use their terminal shells to follow this lesson, as discussed in \"Introduction to the Bash Command Line.\"211 This lesson was written using Git Bash 1.9.0 and the Windows 7 operating system. Equivalent file paths for OS X/Linux have been included where possible. Nonetheless, as commands and flags can change slightly between operating systems OS X/Linux users are referred to Deborah S. Ray and Eric J. Ray, \"Unix and Linux: Visual Quickstart Guide\", 4th edition (2009)212 which covers interoperability in greater detail. The files used in this lesson are available on \"Figshare\".213 The data contains the metadata for journal articles categorised under 'History' in the British Library ESTAR database. The data is shared under a CC0 copyright waiver. Download the required files, save them to your computer, and unzip them. If you do not have default software installed to interact with .zip files, we recommend 7-zip for this purpose.214 On Windows, we recommend unzipping the folder provided to your c: drive so the files are at c:\\proghist\\. However, any location will work fine, but you may have to adjust your commands as you are following along with this lesson if you use a different location. On OS X or Linux, we similarly recommend unzipping them to your user directory, so that they appear at /user/USERNAME/proghist/. In both cases, this means that when you open up a new terminal window, you can just type cd\tproghist to move to the correct directory.                                                          208\tJames\tBaker,\t‘Preserving\tYour\tResearch\tData’,\tThe\tProgramming\tHistorian\t(2014).\t209\t‘Git\tfor\tWindows’:\thttps://git-for-windows.github.io/\t210\t‘Install\tGit\tBash’,\tOpen\tHatch:\thttps://openhatch.org/missions/windows-setup/install-git-bash\t211\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorian\t(2014).\t212\tDeborah\tS.\tRay\tand\tEric\tJ.\tRay,\t‘Unix\tand\tLinux’\t(4th\tEdition),\tPeachpit\tPress\t(2009).\t213\t‘Data\tfor\t‘Counting\tand\tmining\tresearch\tdata\twith\tUnix’,\tThe\tProgramming\tHistorian’\tFigshare:\thttps://figshare.com/articles/Data_for_Counting_and_mining_research_data_with_Unix_The_Programming_Historian_/1172094\t214\t‘7-Zip’:\thttp://www.7-zip.org/\t \n 191 Counting files You will begin this lesson by counting the contents of files using the Unix shell. The Unix shell can be used to quickly generate counts from across files, something that is tricky to achieve using the graphical user interfaces (GUI) of standard office suites. In Unix the wc command is used to count the contents of a file or of a series of files. Open the Unix shell and navigate to the directory that contains our data, the data subdirectory of the proghist directory. Remember, if at any time you are not sure where you are in your directory structure, type pwd and use the cd command to move to where you need to be. The directory structure here is slightly different between OS X/Linux and Windows: on the former, the directory is in a format such as ~/users/USERNAME/proghist/data and on Windows in a format such as c:\\proghist\\data. Type ls and then hit enter. This prints, or displays, a list that includes two files and a subdirectory. The files in this directory are the dataset 2014-01_JA.csv that contains journal article metadata and a file containing documentation about 2014-01_JA.csv called 2014-01_JA.txt. The subdirectory is named derived_data. It contains four .tsv files215 derived from 2014-01_JA.csv. Each of these includes all data where a keyword such as africa or america appears in the 'Title' field of 2014-01_JA.csv. The derived_data directory also includes a subdirectory called results. Note: CSV files216 are those in which the units of data (or cells) are separated by commas (comma-separated-values) and TSV files are those in which they are separated by tabs. Both can be read in simple text editors or in spreadsheet programs such as Libre Office Calc or Microsoft Excel. Before you begin working with these files, you should move into the directory in which they are stored. Navigate to c:\\proghist\\data\\derived_data on Windows or ~/users/USERNAME/proghist/data/derived_data on OS X. Now that you are here you can count the contents of the files. The Unix command for counting is wc. Type wc\t-w\t2014-01-31_JA_africa.tsv and hit enter. The flag -w combined with wc instructs                                                         215\t‘Tab-separated\tvalues’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Tab-separated_values\t216\t‘Comma-separated\tvalues’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Comma-separated_values\t \n 192 the computer to print a word count, and the name of the file that has been counted, into the shell. As was seen in \"Introduction to the Bash Command Line\",217 flags such as -w are an essential part of getting the most out of the Unix shell as they give you better control over commands. If your research is more concerned with the number of entries (or lines) than the number of words, you can use the line count flag. Type wc\t-l\t2014-01-31_JA_africa.tsv and hit enter. Combined with wc the flag -l prints a line count and the name of the file that has been counted. Finally, type wc\t-c\t2014-01-31_JA_africa.tsv and hit enter. This uses the flag -c in combination with the command wc to print a character count for 2014-01-31_JA_africa.tsv. Note: OS X and Linux users should replace the -c flag with -m. With these three flags, the most obvious thing historians can use wc for is to quickly compare the shape of sources in digital format - for example word counts per page of a book, the distribution of characters per page across a collection of newspapers, the average line lengths used by poets. You can also use wc with a combination of wildcards and flags to build more complex queries. Type wc\t-l\t2014-01-31_JA_a*.tsv and hit enter. This prints the line counts for 2014-01-31_JA_africa.tsv and 2014-01-31_JA_america.tsv, offering a simple means of comparing these two sets of research data. Of course, it may be faster to compare the line count for the two documents in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. But when wishing to compare the line count for tens, hundreds, or thousands of documents, the Unix shell has a clear speed advantage. Moreover, as our datasets increase in size you can use the Unix shell to do more than copy these line counts by hand, by the use of print screen, or by copy and paste methods. Using the > redirect operator you can export your query results to a new file. Type wc\t-l\t2014-01-31_JA_a*.tsv\t>\tresults/2014-01-31_JA_a_wc.txt and hit enter. This runs the same query as before, but rather than print the results within the Unix shell it saves the results as 2014-01-31_JA_a_wc.txt. By prefacing this with results/ it moves the .txt file to the results sub-directory. To check this, navigate to the results subdirectory, hit enter, type ls, and hit enter again to see this file listed within c:\\proghist\\data\\derived_data\\results on Windows or /users/USERNAME/proghist/data/derived_data/results on OS X/Linux.                                                         217\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorian\t(2014).\t \n 193 Mining files The Unix shell can do much more than count the words, characters, and lines within a file. The grep command (meaning 'global regular expression print') is used to search across multiple files for specific strings of characters. It is able to do so much faster than the graphical search interface offered by most operating systems or office suites. And combined with the > operator, the grep command becomes a powerful research tool can be used to mine your data for characteristics or word clusters that appear across multiple files and then export that data to a new file. The only limitations here are your imagination, the shape of your data, and - when working with thousands or millions of files - the processing power at your disposal. To begin using grep, first navigate to the derived_data directory (cd\t..). Here type grep\t1999\t*.tsv and hit enter. This query looks across all files in the directory that fit the given criteria (the .tsv files) for instances of the string, or character cluster, '1999'. It then prints them within the shell. Note: there is a large amount of data to print, so if you get bored hit ctrl+c to cancel the action. Ctrl+c is used to cancel any process in the Unix shell. Press the up arrow once in order to cycle back to your most recent action. Amend grep\t1999\t*.tsv to grep\t-c\t1999\t*.tsv and hit enter. The shell now prints the number of times the string 1999 appeared in each .tsv file. Cycle to the previous line again and amend this to grep\t-c\t1999\t2014-01-31_JA_*.tsv\t>\tresults/2014-01-31_JA_1999.txt and hit enter. This query looks for instances of the string '1999' across all documents that fit the criteria and saves them as 2014-01-31_JA_1999.txt in the results subdirectory. Strings need not be numbers. grep\t-c\trevolution\t2014-01-31_JA_america.tsv\t2014-02-02_JA_britain.tsv, for example, counts the instances of the string revolution within the defined files and prints those counts to the shell. Run this and then amend it to grep\t-ci\trevolution\t2014-01-31_JA_america.tsv\t2014-02-02_JA_britain.tsv. This repeats the query, but prints a case insensitive count (including instances of both revolution and Revolution). Note how the count has increased nearly 30 fold for those journal article titles that contain the keyword 'revolution'. As before, cycling back and adding >\tresults/, followed by a filename (ideally in .txt format), will save the results to a data file. You can also use grep to create subsets of tabulated data. Type grep\t-i\trevolution\t2014-01-31_JA_america.tsv\t2014-02-02_JA_britain.tsv\t>\tYEAR-MONTH-DAY_JA_america_britain_i_revolution.tsv (where YEAR-MONTH-DAY is the date you are completing this lesson) and hit enter. This command looks in both of the defined files and exports any lines containing revolution (without regard to case) to the specified .tsv file.  \n 194 The data has not been saved to to the results directory because it isn't strictly a result; it is derived data. Depending on your research project it may be easier to save this to another subdirectory. For now have a look at this file to verify its contents and when you are happy, delete it using the rm command. Note: the rm common is very powerful and should be used with caution. Please refer to \"Introduction to the Bash Command Line\" for instructions on how to use this command correctly.218 Finally, you can use another flag, -v, to exclude data elements when using the grep command. Type grep\t-iv\trevolution\t2014*_JA_a*.tsv\t>\t2014_JA_iv_revolution.csv and hit enter. This query looks in the defined files (three in total) and exports all lines that do not contain revolution or Revolution to c:\\proghist\\data\\derived_data\\2014_JA_iv_revolution.csv. Note that you have transformed the data from one format to another - from .tsv to .csv. Often there is a loss of data structure when undertaking such transformations. To observe this for yourself, run grep\t-iv\trevolution\t2014*_JA_a*.tsv\t>\t2014_JA_iv_revolution.tsv and open both the .csv and .tsv files in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. Note the differences in column delineation between the two files. Summary Within the Unix shell you can now: use the wc command with the flags -w and -l to count the words and lines in a file or a series of files. use the redirector and structure >\tsubdirectory/filename to save results into a subdirectory. use the grep command to search for instances of a string. use with grep the -c flag to count instances of a string, the -i flag to return a case insensitive search for a string, and the -v flag to exclude a string from the results. combine these commands and flags to build complex queries in a way that suggests the potential for using the Unix shell to count and mine your research data and research projects.  Conclusion In this lesson you have learnt to undertake some basic file counting, to query across research data for common strings, and to save results and derived data. Though this lesson is restricted to using the Unix shell to count and mine tabulated data, the processes can be easily extended to free text. For this we recommend two guides written by William Turkel:                                                         218\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorian\t(2014).\t \n 195 William Turkel, 'Basic Text Analysis with Command Line Tools in Linux' (15 June 2013).219 William Turkel, 'Pattern Matching and Permuted Term Indexing with Command Line Tools in Linux' (20 June 2013).220 As these recommendations suggest, the present lesson only scratches the surface of what the Unix shell environment is capable of. It is hoped, however, that this lesson has provided a taster sufficient to prompt further investigation and productive play. For many historians, the full potential of these tools may only emerge upon embedding these skills into a real research project. Once your research grows, and, with it, your research data, being able to manipulate, count and mine thousands of files will be extremely useful. For if you choose to build on this lesson and investigate the Unix shell further you will find that even a large collection of files which do not contain any alpha-numeric data elements, such as image files, can be easily sorted, selected and queried in the Unix shell. About the Authors Ian Milligan is an assistant professor of history at the University of Waterloo. James Baker is a lecturer of digital history at the University of Sussex. \n                                                        219\tWilliam\tJ.\tTurkel,\t‘Basic\tText\tAnalysis\twith\tCommand\tLine\tTools\tin\tLinux’,\tWilliam\tJ.\tTurkel\t(2013):\thttp://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/\t220\tWilliam\tJ.\tTurkel,\t‘Pattern\tMatching\tand\tPermuted\tTerm\tIndexing\twith\tCommand\tLine\tTools\tin\tLinux’,\tWilliam\tJ.\tTurkel\t(2013):\thttp://williamjturkel.net/2013/06/20/pattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux/\t \n 196 24. Cleaning Data with OpenRefine Seth van Hooland, Ruben Verborgh, and Max De Wilde – 2013    Lesson goals Don’t take your data at face value. That is the key message of this tutorial which focuses on how scholars can diagnose and act upon the accuracy of data. In this lesson, you will learn the principles and practice of data cleaning, as well as how OpenRefine221 can be used to perform four essential tasks that will help you to clean your data: Remove duplicate records Separate multiple values contained in the same field Analyse the distribution of values throughout a data set Group together different representations of the same reality These steps are illustrated with the help of a series of exercises based on a collection of metadata from the Powerhouse museum,222 demonstrating how (semi-)automated methods can help you correct the errors in your data. Why should historians care about data quality? Duplicate records, empty values and inconsistent formats are phenomena we should be prepared to deal with when using historical data sets. This lesson will teach you how to discover inconsistencies in data contained within a spreadsheet or a database. As we increasingly share, aggregate and reuse data on the web, historians will need to respond to data quality issues which inevitably pop up. Using a program called OpenRefine, you will be able to easily identify systematic errors such as blank cells, duplicates, spelling inconsistencies, etc. OpenRefine not only allows you to quickly diagnose the accuracy of your data, but also to act upon certain errors in an automated manner. Description of the tool: OpenRefine In the past, historians had to rely on information technology specialists to diagnose data quality and to run cleaning tasks. This required custom computer programs when working with sizeable data sets. Luckily, the advent of Interactive Data Transformation tools (IDTs) now allows for rapid and inexpensive operations on large amounts of data, even by professionals lacking in-depth technical skills.                                                         221\t‘Open\tRefine’:\thttp://openrefine.org/\t222\t‘Museum\tof\tApplied\tArts\tand\tSciences’:\thttps://maas.museum/\t \n 197 IDTs resemble the desktop spreadsheet software we are all familiar with, and they share some common functionalities. You can for example use an application such as Microsoft Excel to sort your data based on numerical, alphabetical and custom-developed filters, which allows you to detect errors more easily. Setting up these filters in a spreadsheet can be cumbersome, as they are a secondary functionality. On a more general level, we could say that spreadsheets are designed to work on individual rows and cells, whereas IDTs operate on large ranges of data at once. These 'spreadsheets on steroids' offer an integrated and user-friendly interface through which end users can detect and correct errors. Several general purpose tools for interactive data transformation have been developed in recent years, such as Potter’s Wheel ABC223 and Wrangler.224 Here we want to focus specifically on OpenRefine225 (formerly Freebase Gridworks and Google Refine), as in the opinion of the authors, it is the most user-friendly tool to efficiently process and clean large amounts of data in a browser-based interface. On top of data profiling226 and cleaning operations, OpenRefine extensions allow users to identify concepts in unstructured text, a process referred to as named-entity recognition (NER)227, and can also reconcile their own data with existing knowledge bases. By doing so, OpenRefine can be a practical tool to link data with concepts and authorities which have already been declared on the Web by parties such as Library of Congress228 or OCLC.229 Data cleaning is a prerequisite to these steps; the success rate of NER and a fruitful matching process between your data and external authorities depends on your ability to make your data as coherent as possible. Description of the exercise: Powerhouse Museum The Powerhouse Museum in Sydney provides a freely available metadata export of its collection on its website.230 The museum is one of the largest science and technology museums worldwide, providing access to almost 90,000 objects, ranging from steam engines to fine glassware and from haute couture to computer chips.                                                         223\t‘Potter’s\tWheel\tA-B-C:\tAn\tInteractive\tTool\tfor\tData\tAnalysis,\tCleansing,\tand\tTransformation’:\thttp://control.cs.berkeley.edu/abc/\t224\tSean\tKandel,\tAndreaas\tPaepcke,\tJoseph\tHellerstein,\tJeffrey\tHeer,\t‘Wrangler:\tInteractive\tVisual\tSpecification\tof\tData\tTransformation\tScripts’:\thttp://vis.stanford.edu/papers/wrangler/\t225\t‘Open\tRefine’:\thttp://openrefine.org/\t226\t‘Data\tprofiling’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Data_profiling\t227\t‘Named-entity\trecognition’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Named-entity_recognition\t228\t‘Library\tof\tCongress’:\thttps://www.loc.gov/\t229\t‘OCLC’:\thttp://www.oclc.org/home.en.html\t230\t‘API,\tData\tAccess\tand\t3D\tScans’,\tPowerhouse\tMuseum\tCollection\tSearch:\thttp://www.powerhousemuseum.com/collection/database/download.php\t \n 198 The Powerhouse has been very actively disclosing its collection online and making most of its data freely available. From the museum website, a tab-separated text file under the name phm-collection.tsv can be downloaded, which you can open as a spreadsheet. The unzipped file (58MB) contains basic metadata (17 fields) for 75,823 objects, released under a Creative Commons Attribution Share Alike (CCASA) license. In this tutorial we will be using a copy of the data that we have archived for you to download (in a moment). This ensures that if the Powerhouse Museum updates the data, you will still be able to follow along with the Lesson. Throughout the data profiling and cleaning process, the case study will specifically focus on the Categories field, which is populated with terms from the Powerhouse museum Object Names Thesaurus (PONT). PONT recognizes Australian usage and spelling, and reflects in a very direct manner the strengths of the collection. In the collection you will find better representations of social history and decorative arts, and comparably few object names relating to fine arts and natural history. The terms in the Categories field comprise what we call a Controlled vocabulary.231 A controlled vocabulary consists of keywords describing the content of a collection using a limited number of terms, and is often a key entry point into data sets used by historians in libraries, archives and museums. That is why we will give particular attention to the 'Categories' field. Once the data has been cleaned, it should be possible to reuse the terms in the controlled vocabulary to find additional information about the terms elsewhere online, which is known as creating Linked Data.232 Getting started: installing OpenRefine and importing data Download OpenRefine and follow the installation instructions.233 OpenRefine works on all platforms: Windows, Mac, and Linux. OpenRefine will open in your browser, but it is important to realise that the application is run locally and that your data won't be stored online. The data files are available on our FreeYourMetadata website,234 which will be used throughout this tutorial. Please download the phm-collection.tsv file before continuing (also archived on the Programming Historian site: http://programminghistorian.org/images/phm-collection.tsv). On the OpenRefine start page, create a new project using the downloaded data file and click Next. By default, the first line will be correctly parsed as the name of a column, but you need to unselect the 'Quotation marks are used to enclose cells containing column separators' checkbox, since the quotes inside the file do not have any meaning to OpenRefine. Now click on 'Create project'. If all goes well, you will see 75,814 rows. Alternatively,                                                         231\t‘Controlled\tvocabulary’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Controlled_vocabulary\t232\t‘Linked\tdata’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Linked_data\t233\tDownload\tOpen\tRefine’:\thttp://openrefine.org/#download_openrefine\t234\t‘Index\tof\t/powerhouse-museum/’:\thttp://data.freeyourmetadata.org/powerhouse-museum/\t \n 199 you can download the initial OpenRefine project directly (http://data.freeyourmetadata.org/powerhouse-museum/phm-collection.google-refine.tar.gz). The Powerhouse museum data set consists of detailed metadata on all the collection objects, including title, description, several categories the item belongs to, provenance information, and a persistent link to the object on the museum website. To get an idea of what object the metadata corresponds to, simply click the persistent link and the website will open. \n Screenshot of a Sample Object on the Powerhouse Museum Website Get to know your data The first thing to do is to look around and get to know your data. You can inspect the different data values by displaying them in facets. You could consider a facet235 like a lense through which you view a specific subset of the data, based on a criterion of your choice. Click the triangle in front of the column name, select Facet, and create a facet. For instance, try a Text facet or a Numeric facet, depending on the nature of the values contained in the fields (numeric values are in green). Be warned, however, that text facets are best used on fields with redundant values (Categories for instance); if you run into a 'too many to display' error, you can choose to raise the choice count limit above the 2,000 default, but too high a limit can slow down the application (5,000 is usually a safe choice). Numeric facets do not have this restriction. For more options, select Customized facets: facet by blank, for instance, comes handy to find out how many values were filled in for each field. We'll explore these further in the following exercises.                                                         235\t‘faceted\tsearch’:\thttp://en.wikipedia.org/wiki/Faceted_search\t\n \n 200 Remove blank rows One thing you notice when creating a numeric facet for the Record ID column, is that three rows are empty. You can find them by unselecting the Numeric checkbox, leaving only Non-numeric values. Actually, these values are not really blank but contain a single whitespace character, which can be seen by moving your cursor to where the value should have been and clicking the 'edit' button that appears. To remove these rows, click the triangle in front of the first column called 'All', select 'Edit rows', and then 'Remove all matching rows'. Close the numeric facet to see the remaining 75,811 rows. Removing duplicates A second step is to detect and remove duplicates. These can be spotted by sorting them by a unique value, such as the Record ID (in this case we are assuming the Record ID should in fact be unique for each entry). The operation can be performed by clicking the triangle left of Record ID, then choosing 'Sort'… and selecting the 'numbers' bullet. In OpenRefine, sorting is only a visual aid, unless you make the reordering permanent. To do this, click the Sort menu that has just appeared at the top and choose 'Reorder rows permanently'. If you forget to do this, you will get unpredictable results later in this tutorial. Identical rows are now adjacent to each other. Next, blank the Record ID of rows that have the same Record ID as the row above them, marking them duplicates. To do this, click on the Record ID triangle, choose Edit cells > Blank down. The status message tells you that 84 columns were affected (if you forgot to reorder rows permanently, you will get only 19; if so, undo the blank down operation in the 'Undo/Redo' tab and go back to the previous paragraph to make sure that rows are reordered and not simply sorted). Eliminate those rows by creating a facet on 'blank cells' in the Record ID column ('Facet' > 'Customized facets' > 'Facet by blank'), selecting the 84 blank rows by clicking on 'true', and removing them using the 'All' triangle ('Edit rows' > 'Remove all matching rows'). Upon closing the facet, you see 75,727 unique rows. Be aware that special caution is needed when eliminating duplicates. In the above mentioned step, we assume the dataset has a field with unique values, indicating that the entire row represents a duplicate. This is not necessarily the case, and great caution should be taken to manually verify wether the entire row represents a duplicate or not. Atomization Once the duplicate records have been removed, we can have a closer look at the Categories field. On average each object has been attributed 2.25 categories. These categories are contained within the same field, separated by a pipe character '|'. Record 9, for instance, contains three: 'Mineral samples|Specimens|Mineral Samples-Geological'. In order to analyze in  \n 201 detail the use of the keywords, the values of the Categories field need to be split up into individual cells on the basis of the pipe character , expanding the 75,727 records into 170,167 rows. Choose 'Edit cells', 'Split multi-valued cells', entering '|' as the value separator. OpenRefine informs you that you now have 170,167 rows. It is important to fully understand the rows/records paradigm. Make the Record ID column visible to see what is going on. You can switch between 'rows' and 'records' view by clicking on the so-labelled links just above the column headers. In the 'rows view', each row represents a couple of Record IDs and a single Category, enabling manipulation of each one individually. The 'records view' has an entry for each Record ID, which can have different categories on different rows (grouped together in grey or white), but each record is manipulated as a whole. Concretely, there now are 170,167 category assignments (rows), spread over 75,736 collection items (records). You maybe noticed that we are 9 records up from the original 75,727, but don't worry about that for the time being, we will come back to this small difference later. Facetting and clustering Once the content of a field has been properly atomized, filters, facets, and clusters can be applied to give a quick and straightforward overview of classic metadata issues. By applying the customized facet 'Facet\tby\tblank', one can immediately identify the 461 records that do not have a category, representing 0.6% of the collection. Applying a text facet to the Categories field allows an overview of the 4,934 different categories used in the collection (the default limit being 2,000, you can click 'Set choice count limit' to raise it to 5,000). The headings can be sorted alphabetically or by frequency ('count'), giving a list of the most used terms to index the collection. The top three headings are 'Numismatics' (8,041), 'Ceramics' (7,390) and 'Clothing and dress' (7,279). After the application of a facet, OpenRefine proposes to cluster facet choices together based on various similarity methods. As the Figure below illustrates, the clustering allows you to solve issues regarding case inconsistencies, incoherent use of either the singular or plural form, and simple spelling mistakes. OpenRefine presents the related values and proposes a merge into the most recurrent value. Select values you wish to cluster by selecting their boxes individually or by clicking 'Select all' at the bottom, then chose 'Merge Selected and Re-Cluster'.   \n 202 \n Overview of some clusters The default clustering method is not too complicated, so it does not find all clusters yet. Experiment with different methods to see what results they yield. Be careful though: some methods are too aggressive, so you might end up clustering values that do not belong together. Now that the values have been clustered individually, we can put them back together in a single cell. Click the Categories triangle and choose Edit cells, Join multi-valued cells, OK. Choose the pipe character (|) as a separator. The rows now look like before, with a multi-valued Categories field. Applying ad-hoc transformations through the use of regular expressions You may remember there was an increase in the number of records after the splitting process: nine records appeared out of nowhere. In order to find the cause of this disparity, we need to go back in time before we split the categories into separate rows. To do so, toggle the Undo/Redo tab right of the Facet/Filter tab, and you will get a history of all the actions that you performed since the project was created. Select the step just before 'Split multi-valued cells in column Categories' (if you followed our example this should be 'Remove 84 rows') then go back to the Facet/Filter tab. The issue arose during the splitting operation on the pipe character, so there is a strong chance that whatever went wrong is linked to this character. Let's apply a filter on the Categories column by selecting 'Text filter' in the menu. First type a single | in the field on the left: OpenRefine informs you that there are 71,064 matching records (i.e. records containing a pipe) out of a total of 75,727. Cells that do not contain a pipe can be blank ones, but also cells containing a single category with no separator, such as record 29 which only has 'Scientific instruments'. Now enter a second | after the first one to get || (double pipe): you can see that 9 records are matching this pattern. These are likely the 9 records guilty of our discrepancy: when OpenRefine splits these up, the double pipe is interpreted as a break between two records instead of a meaningless \n \n 203 double separator. Now how do we correct these values? Go to the menu of the 'Categories' field, and choose 'Edit cells' > 'Transform'…. Welcome to the custom text tranform interface, a powerful functionality of OpenRefine using the OpenRefine Expression Language (GREL). The word 'value' in the text field represents the current value of each cell, which you can see below. We can modify this value by applying functions to it (see the GREL documentation for a full list).236 In this case, we want to replace double pipes with a single pipe. This can be achieved by entering the following regular expression237 (be sure not to forget the quotes): value.replace('||',\t'|')\tUnder the 'Expression' text field, you get a preview of the modified values, with double pipes removed. Click OK and try again to split the categories with 'Edit cells' > 'Split multi-valued cells', the number of records will now stay at 75,727 (click the 'records' link to double-check).  Another issue that can be solved with the help of GREL is the problem of records for which the same category is listed twice. Take record 41 for instance, whose categories are 'Models|Botanical specimens|Botanical Specimens|Didactic Displays|Models'. The category 'Models' appears twice without any good reason, so we want to remove this duplicate. Click the Categories triangle and choose Edit cells, Join multi-valued cells, OK. Choose the pipe character as a separator. Now the categories are listed as before. Then select 'Edit cells' > 'Transform', also on the categories column. Using GREL we can successively split the categories on the pipe character, look for unique categories and join them back again. To achieve this, just type the following expression: value.split('|').uniques().join('|')\tYou will notice that 32,599 cells are affected, more than half the collection. Exporting your cleaned data Since you first loaded your data into OpenRefine, all cleaning operations have been performed in the software memory, leaving your original data set untouched. If you want to save the data that you have been cleaning, you need to export them by clicking on the 'Export' menu top-right of the screen. OpenRefine supports a large variety of formats, such as CSV, HTML or Excel: select whatever suits you best or add your own export template by clicking 'Templating'. You can also export your project in the internal OpenRefine format in order to share it with others.                                                         236\tJoe\tWicentowski,\t‘GREL\tFunctions’:\thttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Functions\t237\t‘Regular\texpression’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Regular_expression\t \n 204 Building on top of your cleaned data Once your data has been cleaned, you can take the next step and explore other exciting features of OpenRefine. The user community of OpenRefine has developed two particularly interesting extensions which allow you to link your data to data that has already been published on the Web. The RDF Refine extension238 transforms plaintext keywords into URLs. The NER extension239 allows you to apply named-entity recognition (NER), which identifies keywords in flowing text and gives them a URL. Conclusions If you only remember on thing from this lesson, it should be this: all data is dirty, but you can do something about it. As we have shown here, there is already a lot you can do yourself to increase data quality significantly. First of all, you have learned how you can get a quick overview of how many empty values your dataset contains and how often a particular value (e.g. a keyword) is used throughout a collection. This lessons also demonstrated how to solve recurrent issues such as duplicates and spelling inconsistencies in an automated manner with the help of OpenRefine. Don't hesitate to experiment with the cleaning features, as you're performing these steps on a copy of your data set, and OpenRefine allows you to trace back all of your steps in the case you have made an error. About the Authors Seth van Hooland is an associate professor at at the Information and Communication Science department of the Université libre de Bruxelles.  Ruben Verborgh is a post-doc researcher at the Multimedia Lab of the Universtiteit Gent.  Max De Wilde is a PhD student at the Information and Communication Science department of the Université libre de Bruxelles.  \n                                                        238\t‘RDF\tRefine’:\thttp://refine.deri.ie/docs\t239\tRubenVerborgh,\t‘Refine-NER-Extension’:\thttps://github.com/RubenVerborgh/Refine-NER-Extension\t \n 205 25. Using Gazetteers to Extract Sets of Keywords from Free-Flowing Texts Adam Crymble – 2015    Lesson Goals If you have a copy of a text in electronic format stored on your computer, it is relatively easy to keyword search for a single term. Often you can do this by using the built-in search features in your favourite text editor. However, scholars are increasingly needing to find instances of many terms within a text or texts. For example, a scholar may want to use a gazetteer240 to extract all mentions of English placenames within a collection of texts so that those places can later be plotted on a map. Alternatively, they may want to extract all male given names, all pronouns, stop words,241 or any other set of words. Using those same built-in search features to achieve this more complex goal is time consuming and clunky. This lesson will teach you how to use Python to extract a set of keywords very quickly and systematically from a set of texts. It is expected that once you have completed this lesson, you will be able to generalise the skills to extract custom sets of keywords from any set of locally saved files. For Whom is this Useful? This lesson is useful for anyone who works with historical sources that are stored locally on their own computer, and that are transcribed into mutable electronic text (eg, .txt, .xml, .rtf, .md). It is particularly useful for people interested in identifying subsets of documents containing one or more of a fairly large number of keywords. This might be useful for identifying a relevant subset for closer reading, or for extracting and structuring the keywords in a format that can be used in another tool: as input for a mapping exercise, for example. The present tutorial will show users how to extract all mentions of English and Welsh county names from a series of 6,692 mini-biographies of individuals who began their studies at the University of Oxford during the reign of James I of England (1603-1625). These records were transcribed by                                                         240\t‘Gazetteer’,\tWikipedia:\thttp://programminghistorian.org/lessons/extracting-keywords\t241\t‘Stop\twords’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Stop_words\t \n 206 British History Online,242 from the printed version of Alumni Oxonienses, 1500-1714. These biographies contain information about each graduate, which includes the date of their studies and the college(s) they attended. Often entries contain additional information when known, including date or birth and death, the name or occupation of their father, where they originated, and what they went on to do in later life. The biographies are a rich resource, providing reasonably comparable data about a large number of similar individuals (rich men who went to Oxford). The 6,692 entries have been pre-processed by the author and saved to a CSV file243 with one entry per row. In this tutorial, the dataset involves geographical keywords. Once extracted, these placenames could be geo-referenced to their place on the globe and then mapped using digital mapping. This might make it possible to discern which colleges attracted students from what parts of the country, or to determine if these patterns changed over time. For a practical tutorial on taking this next step, see the lesson by Fred Gibbs mentioned at the end of this lesson. Readers may also be interested in ‘Georeferencing in QGIS 2.0’, also available from the Programming Historian.244 This approach is not limited to geographical keywords, however. It could also be used to extract given names, prepositions, food words, or any other set of terms defined by the user. This process could therefore be useful for someone seeking to isolate individual entries containing any of these keywords, or for someone looking to calculate the frequency of their keywords within a corpus of texts. This tutorial provides pathways into textual or geospatial analyses, rather than research answers in its own right. Data management skills are increasingly crucial for historians and textual scholars, and anyone working with particularly messy or difficult texts might consider looking into ‘Cleaning Data with OpenRefine’ by Seth van Hooland et al.245 The approach outlined in this tutorial is not optimised for working with poorly transcribed texts such as text converted through Optical Character Recognition246 if the software has a high error rate. Readers working with early modern texts with non-standardised spelling may also find this approach challenging, as the gazetteer one uses must contain exact matches of the words sought. However, with a good enough gazetteer, this approach could prove quite useful for early modernites, and                                                         242\t‘Alumni\tOxonienses\t1500-1714’,\tBritish\tHistory\tOnline:\thttp://www.british-history.ac.uk/alumni-oxon/1500-1714\t243\t‘Comma-separated\tvalues’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Comma-separated_values\t244\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDaniel\tMacfarlane,\t‘Georeferencing\tin\tQGIS\t2.0’,\tThe\tProgramming\tHistorian\t(2013).\t245\t‘Seth\tvan\tHooland,\tRuben\tVerborgh,\tand\tMax\tDe\tWilde,\t‘Cleaning\tData\twith\tOpenRefine’,\tThe\tProgramming\tHistorian\t(2013).\t246\t‘Optical\tCharacter\tRecognition’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Optical_character_recognition\t \n 207 may exceed what's practical with traditional keyword searching by making fuzzy searching possible.247 This tutorial assumes that you have already installed Python version 2 on your computer. The lesson will use the Command Line to run Python, as this is more flexible and makes it possible for use in classrooms or computer labs in which students do not have the ability to download and install interactive development environments (IDEs) like Komodo Edit. Readers who would prefer to use an IDE might like to first read Python Introduction and Installation, but this is optional.248 The tutorial also makes some basic assumptions about your Python skills. It assumes you know what the following Python data structures are (though not knowing will not prevent the code from working should you follow all of the steps in the tutorial): List249 For Loop250 String251 The lesson touches on Regular Expressions, so some readers may find it handy to have the relevant Programming Historian lessons by Doug Knox or Laura Turner O'Hara open to consult as needed.252 Familiarising yourself with the data The first step of this process is to take a look at the data that we will be using in the lesson. As mentioned, the data includes biographical details of approximately 6,692 graduates who began study at the University of Oxford in the early seventeenth century. The_Dataset_-_Alumni_Oxonienses-Jas1.csv (1.4MB)  http://programminghistorian.org/assets/The_Dataset_-_Alumni_Oxonienses-Jas1.csv \n                                                        247\t‘Approximate\tstring\tmatching’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Approximate_string_matching\t248\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Python\tIntroduction\tand\tInstallation’\tThe\tProgramming\tHistorian\t(2012).\t249\t‘Data\tStructures’,\tPython:\thttps://docs.python.org/2/tutorial/datastructures.html\t250\t‘More\tControl\tFlow\tTools’,\tPython:\thttps://docs.python.org/2/tutorial/controlflow.html\t251\t‘String’,\tPython:\thttps://docs.python.org/2/library/string.html\t252\tDoug\tKnox,\t‘Understanding\tRegular\tExpressions’\t(2013);\tLaura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t \n 208 \n Screenshot of the first forty entries in the dataset Download the dataset and spend a couple of minutes looking at the types of information available. You should notice three columns of information. The first, 'Name', contains the name of the graduate. The second: 'Details', contains the biographical information known about that person. The final column, 'Matriculation Year', contains the year in which the person matriculated (began their studies). This final column was extracted from the details column in the pre-processing stage of this tutorial. The first two columns are as you would find them on the British History Online version of the Alumni Oxonienses. If you notice more than three columns then your spreadsheet programme has incorrectly set the delimiter253 between columns. It should be set to \",\" (double quotes, comma). How you do this depends on your spreadsheet programme, but you should be able to find the solution online. Most (but not all) of these bibliographic entries contain enough information to tell us what county the graduate came from. Notice that a large number of entries contain placenames that correspond to either major cities ('of London', in the first entry) or English counties ('of Middlesex' in entry 5 or 'of Wilts' - short for Wiltshire in entry 6). If you are not British you may not be familiar with these county names. You can find a list of historic counties of England on Wikipedia.254 Unfortunately, the information is not always available in the same format. Sometimes it's the first thing mentioned in an entry. Sometimes it's in the middle. Our challenge is to extract those counties of origin from within this messy text, and store it in a new column next to that person's entry.                                                         253\t‘Delimiter’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Delimiter\t254\t‘Historic\tcounties\tof\tEngland’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Historic_counties_of_England\t\n \n 209 Build your gazetteer In order to extract the relevant place names, we first have to decide what they are. We need a list of places, often called a gazetteer. Many of the place names mentioned in the records are shortforms, such as 'Wilts' instead of 'Wiltshire', or 'Salop' instead of 'Shropshire'. Getting all of these variations may be tricky. For a start, let's build a basic gazetteer of English counties. Make a new directory (folder) on your computer where you will save all of your work. Create a text file called gazetteer.txt and using the entries listed on the Wikipedia page listed above, add each county to a new line on the text file. It should look something like this: Bedfordshire\tBerkshire\tBuckinghamshire\tCambridgeshire\tCheshire\tCornwall\tCumberland\tDerbyshire\tDevon\tDorset\tDurham\tEssex\tGloucestershire\tHampshire\tHerefordshire\tHertfordshire\tHuntingdonshire\tKent\tLancashire\tLeicestershire\tLincolnshire\tMiddlesex\tNorfolk\tNorthamptonshire\tNorthumberland\tNottinghamshire\tOxfordshire\tRutland\tShropshire\tSomerset\tStaffordshire\tSuffolk\tSurrey\tSussex\tWarwickshire\tWestmorland\tWiltshire\tWorcestershire\tYorkshire\tMake sure that there are no blank lines in the gazetteer file. If there are, your program will think all spaces are a matching keyword. Some text  \n 210 editing programs (particularly in Linux) will want to add a blank line at the end of your file. If this is the case, try another text editor. It's best to use software that puts you in control. If you ever need to add to this set of keywords, you can open this file in your text editor and add new words, each on their own line. Komodo Edit is a good text editor for this task, especially if you have set it up to run with Python, but you can also use any plain text editor as long as it is not a word processor such as Microsoft Word or Open Office.255 Word processors are inappropriate for writing code because of how they stylise apostrophes and quotes, causing havoc for your code. Loading your texts The next step is to put the texts that you want to search into another text file, with one entry per line. The easiest way to do that is to open the spreadsheet and select all of the second (details) column, then paste the contents into a .txt file. Call the file texts.txt and save it to the same directory as your gazetteer.txt file. Your directory should look something like this:  Contents of your working directory The reason we do this is to keep the original data (the .CSV file) away from the Python program we are about to write, on the off-chance that we accidentally change something without noticing. In my opinion, this approach also makes for easier to read code, which is important when learning. It is not strictly necessary to use a .txt file for this step, as Python does have ways of opening and reading CSV files. At the end of this lesson we will look at how to use the CSV reading and writing features in Python, but this is an optional advanced step. Write your Python program The last step is to write a program that will check each of the texts for each of the keywords in the gazetteer, and then to provide an output that will tell us which entries in our spreadsheet contain which of those words. There are lots of ways that this could be achieved. When planning to write a program, it is always a good idea to devise an algorithm. An algorithm is a set of human-readable steps that will solve the problem. It's a list of what you are going to do, which you then convert into the appropriate                                                         255\t‘Word\tprocessor’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Word_processor\t\n \n 211 programmatic instructions. The approach we will take here uses the following algorithm: Load the list of keywords that you've created in gazetteer.txt and save them each to a Python list Load the texts from texts.txt and save each one to another Python list Then for each biographical entry, remove the unwanted punctuation (periods, commas, etc) Then check for the presence of one or more of the keywords from your list. If it finds a match, store it while it checks for other matches. If it finds no match, move on to the next entry Finally, output the results in a format that can be easily transferred back to the CSV file. Step 1: Load the Keywords Using your text editor, create a new empty file, and add the following lines: #Import\tthe\tkeywords\tf\t=\topen('gazetteer.txt',\t'r')\tallKeywords\t=\tf.read().lower().split(\"\\n\")\tf.close()\t\tprint\tallKeywords\tprint\tlen(allKeywords)\tThe first line is a comment for our own benefit, to tells us (the human) what the code does. All Python lines beginning with a # are a comment. When the code runs it will ignore the comments. They are for human use only. A well commented piece of code is easier to return to later because you will have the means of decyphering your earlier creation. The second line opens the gazetteer.txt file, and reads it, which is signified by the 'r' (as opposed to 'w' for write, or 'a' for append). That means we will not be changing the contents of the file. Only reading it. The third line reads everything in that file, converts it to lower() case, and splits the contents into a Python list, using the newline character as the delimiter.256 Effectively that means each time the program comes across a new line, it stores it as a new entry. We then save that Python list containing the 39 counties into a variable that we have called allKeywords. The fourth line closes the open text file. The fifth line prints out the results, and the sixth line tells us how many results were found. Save this file as extractKeywords.py, again to the same folder as the other files, and then run it with Python. To do this from the command line, first you need to launch your command line terminal. On Mac OS X, this is found in the Applications folder and is called Terminal. On Windows it is                                                         256\t‘New\tline\tPython’,\tStack\tOverflow:\thttp://stackoverflow.com/questions/11497376/new-line-python\t \n 212 called Command\tPrompt. Instructions here are given for Mac Terminal users, but Windows users should be able to follow along. Once the Terminal window is open, you need to point your Terminal at the directory that contains all of the files you have just created. I have called my directory 'ExtractingKeywordSets' and I have it on my computer's Desktop. To change the Terminal to this directory, I use the following command: cd\tDesktop/ExtractingKeywordSets\tYou would need to change the above to reflect the name you gave your directory, and where you put it on your machine. Note that Windows uses '' instead of '/' in file paths. If you get stuck, rename your directory to ExtractingKeywordSets and place it on the Desktop so that you can follow along. You can now run the program you've written with the following command: python\textractKeywords.py\tOnce you have run the program you should see your gazetteer printed as a Python list in the command output, along with the number of entries in your list (39). If you can, great! Move on to step 2. If the last line of your output tells you that there was 1 result, that means the code has not worked properly, since we know that there should be 39 keywords in your gazetteer. Double check your code to make sure you havn't included any typos. If you still can't solve the problem, try changing \"\" to \"\" on line three. Some text editors will use carriage returns257 instead of 'newline characters' when creating a new line. The means 'carriage return' and should solve your problem if you're experiencing one. Step 2: Load the texts The second step is very similar to the first. Except this time we will load the texts.txt in addition to the gazetteer.txt file Add the following lines to the end of your code: #Import\tthe\ttexts\tyou\twant\tto\tsearch\tf\t=\topen('texts.txt',\t'r')\tallTexts\t=\tf.read().lower().split(\"\\n\")\tf.close()\t\tprint\tallTexts\tIf you got step 1 to work, you should understand this bit as well. Before you run the code, make sure that you have saved your program or you may accidentally run the OLD version and will be confused with the result. Once you've done that, rerun the code. As a shortcut, instead of writing out                                                         257\t‘Carriage\treturn’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Carriage_return\t \n 213 the command again in the Terminal, you can press the up arrow, which should display the last command you entered. If you keep pressing the up or down arrows, you can scroll through previous commands, saving yourself the time needed to retype them. Once you've found the command for running the program, press the return key to run the code. If the code worked, you should see a big wall of text. Those are the texts we input into the program. As long as you see them, you're ok. Before moving on to the next step, delete the three lines from your code beginning with 'print'. Now that we know they are printing the contents of these files properly we do not need to continue to check. Move on to step 3. Step 3: Remove unwanted punctuation When matching strings, you have to make sure the punctuation doesn't get in the way. Technically, 'London.' is a different string than 'London' or ';London' because of the added punctuation. These three strings which all mean the same thing to us as human readers will be viewed by the computer as distinct entities. To solve that problem, the easiest thing to do is just to remove all of the punctuation. You can do this with regular expressions,258 and Doug Knox and Laura Turner O'Hara have provided great introductions at Programming Historian for doing so.259 To keep things simple, this program will just replace the most common types of punctuation with nothing instead (effectively deleting punctuation). Add the following lines to your program: for\tentry\tin\tallTexts:\t\t\t\t\t#for\teach\tentry:\t\t\t\t\tallWords\t=\tentry.split('\t')\t\t\t\t\tfor\twords\tin\tallWords:\t\t\t\t\t\t\t\t\t\t#remove\tpunctuation\tthat\twill\tinterfere\twith\tmatching\t\t\t\t\t\t\t\t\twords\t=\twords.replace(',',\t'')\t\t\t\t\t\t\t\t\twords\t=\twords.replace('.',\t'')\t\t\t\t\t\t\t\t\twords\t=\twords.replace(';',\t'')\tThe 'allTexts' list variable contains all of our texts. Using a for loop, we will look at each entry in turn. Since we are interested in single words, we will split the text into single words by using the .split() method, looking explicitly for spaces: entry.split('\t'). Note that there is a single space between those quotation marks. Since words are generally separated by spaces, this should work fairly well. This means we now have a Python list called 'allWords' that contains each word in a single bibliographic entry.                                                         258\t‘Regular\texpressions’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Regular_expression\t259\tDoug\tKnox,\t‘Understanding\tRegular\tExpressions’\t(2013);\tLaura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t \n 214 We use another for loop to look through each word in that list, and wherever we find a comma, period, or semi-colon, we replace it with nothing, effectively deleting it. Note that there is no space between those quotation marks in the last three lines. We now have a clean set of words that we can compare against our gazetteer entries, looking for matches. Step 4: Look for matching keywords As the words from our text are already in a list called 'allWords', and all of our keywords are in a list called 'allKeywords', all we have to do now is check our texts for the keywords. First, we need somewhere to store details of any matches we have. Immediately after the 'for entry in allTexts:' line, at one level of indentation, add the following two lines of code: \t\t\t\tmatches\t=\t0\t\t\t\t\tstoredMatches\t=\t[]\tIndentation is important in Python. The above two lines should be indented one tab deeper than the for loop above it. That means the code is to run every time the for loop runs - it is part of the loop. If your text editor does not allow tabs, you can use spaces instead. The 'storedMatches' variable is a blank list, where we can store our matching keywords. The 'matches' variable is known as a 'flag', which we will use in the next step when we start printing the output. To do the actual matching, add the following lines of code to the bottom of your program, again minding the indentation (2 levels from the left margin), making sure you save: \t\t\t\t\t\t\t\t#if\ta\tkeyword\tmatch\tis\tfound,\tstore\tthe\tresult.\t\t\t\t\t\t\t\t\tif\twords\tin\tallKeywords:\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords\tin\tstoredMatches:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstoredMatches.append(words)\t\t\t\t\t\t\t\t\t\t\t\t\tmatches\t+=\t1\t\t\t\t\tprint\tmatches\tIf you are worried that you have your indentation wrong, scroll ahead towards the bottom of the lesson and check the finished code. Take a look at your whole program. These lines follow immediately after the last section in which you removed the punctuation. So each time a word had its punctuation removed (if it had punctuation to remove in the first place) it was immediately checked to see if it was in the list of keywords in your gazetteer file. If it was a match, we check that we do not already have this word recorded in our 'storedMatches' variable. If we do, we skip ahead to the next word. If it is not already recorded, we append it to the  \n 215 'storedMatches' list. This is keeping track of the matching words for us for each text. When we find a match, we also increase our 'matches' flag by 1. This lets us know how many matches we have found for that entry. This code will automatically check each word in a text, keeping track of matches in the 'storedMatches' list. When it gets to the end of a text, it will empty out the 'storedMatches' variable and start again. Printing out the 'matches' variable just lets us see how many matches we got for each text. When you run this code you should see somewhere between 0 and 2 for most entries. If it says 0 for everything then check your code again. If you only have one entry outputting then go back to step one and make sure your program is identifying the right number of keywords (39). \n Correct output of the code to this point  If it looks like it worked, delete the 'print matches' line and move to the next step. Step 5: Output results If you have got to this stage, then your Python program is already finding the matching keywords from your gazetteer. All we need to do now is print them out to the command output pane in a format that's easy to work with. Add the following lines to your program, minding the indentation as always:    \t\t\t\t#if\tthere\tis\ta\tstored\tresult,\tprint\tit\tout\t\t\t\t\tif\tmatches\t==\t0:\t\t\t\t\t\t\t\t\tprint\t'\t'\t\t\t\t\telse:\t\n \n 216 \t\t\t\t\t\t\t\tmatchString\t=\t''\t\t\t\t\t\t\t\t\tfor\tmatches\tin\tstoredMatches:\t\t\t\t\t\t\t\t\t\t\t\t\tmatchString\t=\tmatchString\t+\tmatches\t+\t\"\\t\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprint\tmatchString\tThis code checks if the number of matches is equal to 0. If so, then we havn't found any keywords and we don't need to print them out. However, we are going to print a blank space, because we want our output to contain the same number of lines as did our input (we want 1 line of output per line of text that we searched). This will make it easier to paste the output directly into our CSV file and have all of the entries line up properly with their corresponding text. If there IS a match, then the program creates a new variable called 'matchString' (it could have been called just about anything. That's just the name I chose because it's a string of matches). Then for each of the matching keywords that were kept in 'storedMatches', it appends the keyword to 'matchString', along with a tab (\"\") character. The tab character is useful for CSV files because when you paste it into a spreadsheet, content separated by a tab will automatically go into an adjacent cell. This means that if a single text has more than one match, we'll be able to automatically paste one match per cell. This makes it easier to keep the keywords separate once we have them back in our CSV file. When all of the matching keywords have been added to 'matchString', the program prints it out to the command output before moving on to the next text. If you save your work and run the program, you should now have code that achieves all of the steps from the algorithm and outputs the results to your command output. The finished code should look like this:      \n 217 Finished Code #Import\tthe\tkeywords\tf\t=\topen('gazetteer.txt',\t'r')\tallKeywords\t=\tf.read().lower().split(\"\\n\")\tf.close()\t\t#Import\tthe\ttexts\tyou\twant\tto\tsearch\tf\t=\topen('texts.txt',\t'r')\tallTexts\t=\tf.read().lower().split(\"\\n\")\tf.close()\t\t#Our\tprogramme:\tfor\tentry\tin\tallTexts:\t\t\t\t\tmatches\t=\t0\t\t\t\t\tstoredMatches\t=\t[]\t\t\t\t\t\t\t\t\t\t#for\teach\tentry:\t\t\t\t\tallWords\t=\tentry.split('\t')\t\t\t\t\tfor\twords\tin\tallWords:\t\t\t\t\t\t\t\t\t\t#remove\tpunctuation\tthat\twill\tinterfere\twith\tmatching\t\t\t\t\t\t\t\t\twords\t=\twords.replace(',',\t'')\t\t\t\t\t\t\t\t\twords\t=\twords.replace('.',\t'')\t\t\t\t\t\t\t\t\twords\t=\twords.replace(';',\t'')\t\t\t\t\t\t\t\t\t\t\t#if\ta\tkeyword\tmatch\tis\tfound,\tstore\tthe\tresult.\t\t\t\t\t\t\t\t\tif\twords\tin\tallKeywords:\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords\tin\tstoredMatches:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstoredMatches.append(words)\t\t\t\t\t\t\t\t\t\t\t\t\tmatches\t+=\t1\t\t\t\t\t\t\t\t\t\t#if\tthere\tis\ta\tstored\tresult,\tprint\tit\tout\t\t\t\t\tif\tmatches\t==\t0:\t\t\t\t\t\t\t\t\tprint\t'\t'\t\t\t\t\telse:\t\t\t\t\t\t\t\t\tmatchString\t=\t''\t\t\t\t\t\t\t\t\tfor\tmatches\tin\tstoredMatches:\t\t\t\t\t\t\t\t\t\t\t\t\tmatchString\t=\tmatchString\t+\tmatches\t+\t\"\\t\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprint\tmatchString\tIf you do not like the output format, you can change it by adjusting the second last line of code. For example, you could save each entry to a new line in a .txt file rather than to the screen. To do that you would replace 'print matchString' with the following code (make sure it is at one level of indentation, just as was the replaced line): \t\t\t\tf\t=\topen('output.txt',\t'a')\t\t\t\t\tf.write(matchString)\t\t\t\t\tf.close()\tNote the 'a' instead of the 'r' we used earlier. This 'appends' the text to the file called output.txt, which will be saved in your working directory. You  \n 218 will have to take care, because running the program several times will continue to append all of the outputs to this file, creating a very long file. There are ways around this, which we will cover in a moment, and you might consider looking into how the 'w' (write) feature works, and experimenting with output formats. There is more information related to these features in 'Working with Text Files in Python'.260 Refining the Gazetteer You can copy and paste that output directly into your spreadsheet next to the first entry. Check that the matches lined up properly. Your last entry of your spreadsheet should correspond to the correctly extracted keywords. In this case, the last entry should be blank, but the second last one should read 'dorset'. \n The output pasted back into the CSV file At this point, you might like to refine the gazetteer, as a lot of place names have been missed. Many of them are shortforms, or archaic spellings (Wilts, Salop, Sarum, Northants, etc). You could go through looking at all the empty cells and seeing if you can find keywords that you've missed. It may help to know that you can find the next empty cell in a column in Excel by pressing CTRL + down arrow (CMD + down arrow on Mac). One of the easiest ways to find all of the missing entries is to sort your spreadsheet by the new columns you've just added. If you sort the matches alphabetically for each of the new columns, then the entries at the bottom of the spreadsheet will all be unclassified. You can do this by selecting the whole spreadsheet and clicking on the Data -> Sort menu item. You can then sort a-z for each of the new columns. Before you sort a spreadsheet, it's often a good idea to add an 'original order' column in case you want to sort them back. To do this, add a new column, and in the first 3 rows, type 1, 2, and 3 respectively. Then highlight the three cells and put your cursor over the bottom right corner.                                                         260\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Working\twith\tText\tFiles\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t\n \n 219 If you are using Microsoft Excel your cursor will change into a black cross. When you see this, click and hold the mouse button and drag the cursor down until you reach the bottom of the spreadsheet (down to the last entry) before you let go. This should automatically number the rows consecutively so that you can always re-sort your entries back to the original order. \n Adding an original order column and sorting the entries Now you can sort the data and read some of the entries for which no match was found. If you find there is a place name in there, add it to your 'gazetteer.txt' file, one entry per line. You don't have to be exhaustive at this stage. You could add a handful more entries and then try the code again to see what impact they had on the result. \n Missed place name words highlighted Before you re-run your Python code, you'll have to update your texts.txt file so that the program runs on the texts in the correct order. Since the code will output the matches in the same order that it receives the files in texts.txt, it's important not to get this jumbled up if you've been sorting your spreadsheet where you intend to store your outputs. You can either re-sort the spreadsheet back to the original order before you run the program, or you can copy all of the cells in the 'details' column again and paste and save them into the texts.txt file. \n \n 220 I'd challenge you to make a few refinements to your gazetteer before moving ahead, just to make sure you have the hang of it. Once you are happy with that, you can snag my completed list of English and Welsh counties, shortforms, and various other cities (London, Bristol etc) and places (Jersey, Ireland, etc).261 My completed list contains 157 entries, and should get you all of the entries that can be extracted from the texts in this collection. At this point you could stop, as you've achieved what you set out to do. This lesson taught you how to use a short Python program to search a fairly large number of texts for a set of keywords defined by you. With the outputs from this lesson, you could fairly quickly map these entries by geolocating the place names. This might reveal new insights into spatial patterns of Oxford alumni. Having the ability to search for large numbers of keywords at the same time opens up flexibility for your research process, and makes it feasible to do work that might otherwise just have seemed like it would take too long. You could try a completely different set of words, or use this technique on another set of texts entirely. The research questions are of course, endless. If you would like to refine the program further, we can use Python to read directly from the CSV file and to print the results to a new CSV file so that everything happens automatically from the Terminal window without the need for cutting and pasting. Printing the Results Back to the CSV File Using Python Python has a built in code library that can handle working with CSV files, called csv To use it and its features, you first have to import it. At the top of your extractKeywords.py program, add the following line: \t\t\t\timport\tcsv\tNow we are going to make some changes to our original program. Instead of cutting all of the texts into a texts.txt file, we'll use Python to read the data directly into our 'allTexts' variable. Replace: #Import\tthe\ttexts\tyou\twant\tto\tsearch\tf\t=\topen('texts.txt',\t'r')\tallTexts\t=\tf.read().lower().split(\"\\n\")\tf.close()\tWith this:                                                         261\thttp://programminghistorian.org/assets/extracting-keywords-final-gazetteer.txt\t \n 221 #Import\tthe\t'Details'\tcolumn\tfrom\tthe\tCSV\tfile\tallTexts\t=\t[]\tfullRow\t=\t[]\twith\topen('The_Dataset_-_Alumni_Oxonienses-Jas1.csv')\tas\tcsvfile:\t\t\t\t\treader\t=\tcsv.DictReader(csvfile)\t\t\t\t\tfor\trow\tin\treader:\t\t\t\t\t\t\t\t\t#the\tfull\trow\tfor\teach\tentry,\twhich\twill\tbe\tused\tto\trecreate\tthe\timproved\tCSV\tfile\tin\ta\tmoment\t\t\t\t\t\t\t\t\tfullRow.append((row['Name'],\trow['Details'],\trow['Matriculation\tYear']))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#the\tcolumn\twe\twant\tto\tparse\tfor\tour\tkeywords\t\t\t\t\t\t\t\t\trow\t=\trow['Details'].lower()\t\t\t\t\t\t\t\t\tallTexts.append(row)\tAs this is an advanced option, I won't explain what every line does in detail, but you can take a look at the comments in the code to get an idea. Effectively this uses Python to read the CSV file and stores all of the information in the 'Details' column in the same 'allTexts' variable that we had it in previously, in exactly the same format as before. This code also stores each row of the CSV file into another list called 'fullRow', which will be used for writing a new CSV file containing our program's outputs. There are a few extra lines of code here, but you didn't need to cut and paste anything into the texts.txt file, and there's no risk here of your sorting of your spreadsheet causing any issues about the order of inputs and outputs. This is therefore a more robust option. You can print out either of these variables using the 'print' feature, to make sure they contain what you'd expect of them.  TROUBLESHOOTING: If you get the following error when you attempt to read your CSV file using Python, the CSV file may have been saved on a Mac, and the Python CSV library is only able to read Windows-compatible CSV files (Error:\tnew-line\tcharacter\tseen\tin\tunquoted\tfield\t-\tdo\tyou\tneed\tto\topen\tthe\tfile\tin\tuniversal-newline\tmode?).\tTo solve this problem, open your CSV file in a spreadsheet program (eg., Excel) and 'Save As' and under format chose 'Windows Comma Separated (csv)'. This should solve the problem. To read more on this issue, see Stack Overflow.262                                                          262\t‘CSV\tnew-line\tcharacter\tseen\tin\tunquoted\tfield\terror’,\tStack\tOverflow:\thttp://stackoverflow.com/questions/17315635/csv-new-line-character-seen-in-unquoted-field-error\t \n 222 Creating a new CSV file Next we need to create a new CSV file where the results of the analysis can be stored. It's always a good idea to make a new file rather than try to append it to your only copy of the original data. It's also a good idea to append the current date and time to the filename for your new file. That way you can run the code lots of times as you refine everything and it will always be clear which file contains your most recent ouputs. To do this, import the 'time' library just below where you imported the 'csv' library. import\ttime\tAnd then add the following two lines of code right below where you were just working with the new CSV code: \t#use\tthe\tcurrent\tdate\tand\ttime\tto\tcreate\ta\tunique\toutput\tfilename\ttimestr\t=\ttime.strftime(\"%Y-%m-%d-(%H:%M:%S)\")\tfilename\t=\t'output-'\t+\tstr(timestr)\t+\t'.csv'\tThis will create a variable called 'filename', which we'll use when we make the new output file. The rest of the process involves creating that new output file, putting in the correct headers, pasting in the original data, and then pasting in our new outputs from our gazetteer matching. That involves quite a few tweaks to the original code, so to keep everything as clear as possible, I've included the finished code below. I have appended 'NEW!', 'OLD!' and 'CHANGED!' in the comments for each section so that you can see at a glance which bits have changed: #NEW!\timport\tcsv\timport\ttime\t\t#OLD!\tImport\tthe\tkeywords\tf\t=\topen('gazetteer.txt',\t'r')\tallKeywords\t=\tf.read().lower().split(\"\\n\")\tf.close()\t\t\t#CHANGED!\tImport\tthe\t'Details'\tcolumn\tfrom\tthe\tCSV\tfile\tallTexts\t=\t[]\tfullRow\t=\t[]\twith\topen('The_Dataset_-_Alumni_Oxonienses-Jas1.csv')\tas\tcsvfile:\t\t\t\t\treader\t=\tcsv.DictReader(csvfile)\t\t\t\t\tfor\trow\tin\treader:\t\t\t\t\t\t\t\t\t#the\tfull\trow\tfor\teach\tentry,\t\t\t#which\twill\tbe\tused\tto\trecreate\tthe\timproved\tCSV\tfile\tin\ta\tmoment\t\t\t\t\t\t\t\t\tfullRow.append((row['Name'],\trow['Details'],\trow['Matriculation\tYear']))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#the\tcolumn\twe\twant\tto\tparse\tfor\tour\tkeywords\t\t\t\t\t\t\t\t\trow\t=\trow['Details'].lower()\t\t\t\t\t\t\t\t\tallTexts.append(row)\t\t \n 223 #NEW!\ta\tflag\tused\tto\tkeep\ttrack\tof\twhich\trow\tis\tbeing\tprinted\tto\tthe\tCSV\tfile\t\t\t\tcounter\t=\t0\t\t#NEW!\tuse\tthe\tcurrent\tdate\tand\ttime\tto\tcreate\ta\tunique\toutput\tfilename\ttimestr\t=\ttime.strftime(\"%Y-%m-%d-(%H:%M:%S)\")\tfilename\t=\t'output-'\t+\tstr(timestr)\t+\t'.csv'\t\t#NEW!\tOpen\tthe\tnew\toutput\tCSV\tfile\tto\tappend\t('a')\trows\tone\tat\ta\ttime.\twith\topen(filename,\t'a')\tas\tcsvfile:\t\t\t\t\t\t#NEW!\tdefine\tthe\tcolumn\theaders\tand\twrite\tthem\tto\tthe\tnew\tfile\t\t\t\t\tfieldnames\t=\t['Name',\t'Details',\t'Matriculation\tYear',\t'Placename']\t\t\t\t\twriter\t=\tcsv.DictWriter(csvfile,\tfieldnames=fieldnames)\t\t\t\t\twriter.writeheader()\t\t\t\t\t\t\t\t\t\t#NEW!\tdefine\tthe\toutput\tfor\teach\trow\tand\tthen\tprint\tto\tthe\toutput\tcsv\tfile\t\t\t\t\twriter\t=\tcsv.writer(csvfile)\t\t\t\t\t\t#OLD!\tthis\tis\tthe\tsame\tas\tbefore,\tfor\tcurrentRow\tin\tfullRow:\t\t\t\t\tfor\tentry\tin\tallTexts:\t\t\t\t\t\t\t\t\t\tmatches\t=\t0\t\t\t\t\t\t\t\t\tstoredMatches\t=\t[]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#for\teach\tentry:\t\t\t\t\t\t\t\t\tallWords\t=\tentry.split('\t')\t\t\t\t\t\t\t\t\tfor\twords\tin\tallWords:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#remove\tpunctuation\tthat\twill\tinterfere\twith\tmatching\t\t\t\t\t\t\t\t\t\t\t\t\twords\t=\twords.replace(',',\t'')\t\t\t\t\t\t\t\t\t\t\t\t\twords\t=\twords.replace('.',\t'')\t\t\t\t\t\t\t\t\t\t\t\t\twords\t=\twords.replace(';',\t'')\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#if\ta\tkeyword\tmatch\tis\tfound,\tstore\tthe\tresult.\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords\tin\tallKeywords:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\twords\tin\tstoredMatches:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstoredMatches.append(words)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmatches\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#CHANGED!\tsend\tany\tmatches\tto\ta\tnew\trow\tof\tthe\tcsv\tfile.\t\t\t\t\t\t\t\t\tif\tmatches\t==\t0:\t\t\t\t\t\t\t\t\t\t\t\t\tnewRow\t=\tfullRow[counter]\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\tmatchTuple\t=\ttuple(storedMatches)\t\t\t\t\t\t\t\t\t\t\t\t\tnewRow\t=\tfullRow[counter]\t+\tmatchTuple\t\t\t\t\t\t\t\t\t\t#NEW!\twrite\tthe\tresult\tof\teach\trow\tto\tthe\tcsv\tfile\t\t\t\t\t\t\t\t\twriter.writerows([newRow])\t\t\t\t\t\t\t\t\tcounter\t+=\t1\tThe code is heavily commented so if you spend some time, you should be able to figure it out. Save this code and rerun it using Python and you should get a file called output.csv appearing in your working directory, which if you open it should contain all of the same information as you had before, but without the need to do any cutting or pasting. To give a brief outline of what has been changed from the original version: The texts were extracted automatically from the original datafile instead of having to paste them into a texts.txt file.  \n 224 Using the 'time' library, we used the current date and time to create a unique and easily decypherable filename for our output file. Using the 'csv' library we created a new .csv file using that filename, and put in the column headers we wanted to use. We then ran the same matching code as before, checking 'allTexts' against 'allWords' and storing the results. Instead of printing the results to the screen, we stored each row's original data (Name, Details, Matriculation Year) + the matches to a tuple263 called 'newRow'. Using the 'csv' library we wrote the 'newRow' data to the new CSV file, one row at a time. This approach created longer and more complex code, but the result is a powerful program that reads from a CSV file, matches the texts against the contents of a gazetteer, and then automatically writes the output to a clean new CSV file with no intermediary steps for you the user. You didn't have to go that extra mile, but hopefully you can see the advantages if you made it all the way through. Suggested Further Reading Readers who have completed this lesson might be interested in then geo-referencing the output using the Google API and mapping the results. You can learn more about this process from Fred Gibbs's tutorial, Extract and Geocode Placenames from a Text File.264 This will let you visualise the practical outputs of this tutorial. Alternatively, readers may be interested in Jim Clifford et. al's tutorial on georeferencing in QGIS 2.0,265 an open source GIS program.266 About the Author Adam Crymble is a lecturer of digital history at the University of Hertfordshire. \n                                                        263\t‘Tuples\tand\tSequences’,\tPython:\thttps://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences\t264\tFred\tGibbs,\t‘Extract\tand\tGeocode\tPlacenames\tfrom\ta\tText\tFile’\t(12\tOctober\t2012):\thttp://fredgibbs.net/tutorials/extract-geocode-placenames-from-text-file.html\t265\tJim\tClifford,\tJosh\tMacFadyen\tand\tDaniel\tMacfarlane,\t‘Georeferencing\tin\tQGIS\t2.0’,\tThe\tProgramming\tHistorian,\t2013.\t266\t‘Geographic\tinformation\tsystem’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Geographic_information_system\t \n 225 26. Understanding Regular Expressions Doug Knox – 2013    Lesson Goals In this exercise we will use advanced find-and-replace capabilities in a word processing application in order to make use of structure in a brief historical document that is essentially a table in the form of prose. Without using a general programming language, we will gain exposure to some aspects of computational thinking, especially pattern matching, that can be immediately helpful to working historians (and others) using word processors, and can form the basis for subsequent learning with more general programming environments. We will start with something like this: Arizona.\t—\tQuarter\tended\tJune\t30,\t1907.\tEstimated\tpopulation,\t\t\t\t122,931.\tTotal\tnumber\tof\tdeaths\t292,\tincluding\tdiphtheria\t1,\tenteric\t\t\t\tfever\t4,\tscarlet\tfever\t11,\tsmallpox\t2,\tand\t49\tfrom\ttuberculosis.\tAnd use pattern matching to transform it to something like this: Arizona. Quarter ended June 30, 1907. Deaths diphtheria 1 Arizona. Quarter ended June 30, 1907. Deaths enteric fever 4 Arizona. Quarter ended June 30, 1907. Deaths scarlet fever 11 Arizona. Quarter ended June 30, 1907. Deaths smallpox 2 Arizona. Quarter ended June 30, 1907. Deaths tuberculosis 49 What Are Regular Expressions and for Whom Is this Useful? Perhaps you are not sure yet you want to be a programming historian, you just want to work more effectively with your sources. Historians, librarians, and others in the humanities and social sciences often work with textual sources that have implicit structure. It is also not unheard of in the humanities to have to do tedious textual work with semi-structured notes and bibliographic references, where it can help to have some knowledge of pattern-matching options. As a simple example, if we want to find a reference to a particular year, say 1877, in a document, it's easy enough to search for that single date. But if we want to find any references to years in latter half of the 19th century, it is impractical to search several dozen times for 1850, 1851, 1852, etc., in  \n 226 turn. By using regular expressions we can use a concise pattern like \"18[5-9][0-9]\" to effectively match any year from 1850 to 1899. In this exercise we will use LibreOffice Writer and LibreOffice Calc, which are free software desktop applications for word processing and spreadsheets, respectively. Installation packages for Linux, Mac, or Windows can be downloaded from http://www.libreoffice.org/download. Other word processing software and programming languages have similar pattern-matching capabilities. This exercise uses LibreOffice because it is freely available, and its regular expression syntax is closer to what you will find in programming environments than Microsoft Office's syntax. If you complete this exercise and find regular expressions useful, however, it should be relatively easy to adapt what you learn and apply it in other contexts. While we will start with simple patterns, we will get to more complicated or intimidating-looking ones fairly quickly. The aim here is to share what is involved in doing useful work with a plausible example, and not to linger too long on first principles with simplified toy examples. If you are impatient, it should be possible to go through the examples fairly quickly by copying and pasting the patterns offered, without necessarily following every detail, in order to get a general sense of what is possible. If the result is promising, you could go through a second time to decide what details could be useful to pick up for your own work. But typing everything yourself is the best way to make it your own. Getting the Text \n\tScreenshot of the unstructured text  The Internet Archive has copies of hundreds of early 20th-century public domain U.S. public health reports digitized through JSTOR and organized under the title 'Early Journal Content.' These are of a convenient length for an exercise and can plausibly represent broad classes of textual resources that are useful in many kinds of historical research. For our exercise, we will use a five-page report of monthly morbidity and mortality statistics for states and cities in the United States, published in February 1908, available at http://archive.org/details/jstor-4560629/. \n \n 227 Take a moment to scan the pages through the Read Online link267 to become familiar with it. This document is organized as paragraphs rather than tables, but there are clearly latent structures that can help us tabulate this ourselves. Nearly every paragraph of the report starts with geographic information, specifies a time span for the statistics, optionally includes a population estimate, and then reports deaths and nonlethal cases of illness. The page-flipping interface shows us what the original document looked like. But if we want to tabulate figures and enable ourselves to make comparisons and calculations over geography, we will need to represent the document as text and numbers, and not just images. In addition to offering several image formats for download, the Internet Archive makes available plain-text versions that have been created by means of Optical Character Recognition (OCR) software. OCR of old texts is often imperfect, but what it produces is useful in ways images can't be; it can be searched, copied, and edited as text. Switch to the Full Text view268. We will start from this base, ignoring the last part of the previous report. Copy the text from \"STATISTICAL REPORTS…\" to the end into a new LibreOffice document. When working with material you care about, be sure to save a copy somewhere separately from your working copy, so that you can get back to your original if something goes wrong. Ordinary search and replace We can see some Optical Character Recognition (OCR) errors, where the Internet Archive's automated transcription software has made mistakes, although for the most part this looks like a good transcription. There are two places where the OCR has inserted double quotation marks into this file mistakenly, in both cases by putting them between a comma following a month and a four-digit year, as in December,\"\t1907.\tWe can find these by doing a search (Edit\t→\tFind with shortcut Ctrl-F or Cmd-F on a Mac) for double quotation marks, and confirm that these are the only two instances of quotation marks in the file. In this case we can simply delete them. Rather than do so by hand, just for practice try using LibreOffice's find-and-replace function (Ctrl-H or Cmd-Alt-F on Mac). Replace \" with nothing.                                                         267\t‘Statistical\tReports\tof\tMorbidity\tand\tMortality,\tStates\tand\tCities\tof\tthe\tUnited\tStates:\tUntablulated’,\tInternet\tArchive:\thttp://archive.org/stream/jstor-4560629/4560629#page/n0/mode/2up\t268\t‘Full\ttext\tof\t“Statistical\tReport\ton\tMorbidity\tand\tMortality,\tStates\tand\tCities\tof\tthe\tUnited\tStates:\tUntabulated”’,\tInternet\tArchive:\thttp://archive.org/stream/jstor-4560629/4560629_djvu.txt\t \n 228 \n\tScreenshot of Find and Replace feature  Finding structure for rows We are just getting started, but to estimate how far we have to go, select the full text from LibreOffice Writer (Ctrl-A) and paste it into LibreOffice Calc (File->New->Spreadsheet). Each line of text becomes a single-celled row of the spreadsheet. What we would like is for each row of the spreadsheet to represent one kind of record in a consistent form. It would take a lot of tedious work to tabulate this by hand with this as our starting point. In what follows we will be doing all our work with regular expressions in Writer, but keep Calc open in the background. We can return to it to paste future iterations and gauge our progress. Returning to Writer, we will want to get rid of the line breaks that we don't need — but there are some end-of-line hyphenations we should clean up first. This time we will start using regular expressions. On the Find & Replace box show More\tOptions (Other Options on Mac) and make sure the Regular\texpressions checkbox is selected. This will enable us to use special symbols to define general patterns to match. Using find-and-replace, replace -\t$ (hyphen-space-dollar-sign) with nothing. \n\tThe 'More Options' tab in Open Office Find & Replace \n \n 229 The dollar sign symbol is a special symbol in this case that matches the end of each line. You might start by clicking Find and then Replace when you see that the highlighted selection matches your expectations. After repeating this a few times you can click Replace\tAll to replace all the rest at once. If you make a mistake or are uncertain, you can undo recent steps with Edit\t→\tUndo from the menu bar, or keyboard shortcut Ctrl+Z (Cmd+Z on Mac). In this document there are 27 total matches for this particular pattern. Next, again using find-and-replace, replace all $ (just a dollar sign) with nothing. There are 225 replacements with this pattern. At first it may not be clear what happened here, but this has in fact made each paragraph a single paragraph or logical line. In LibreOffice (and similar word processing programs) you can turn on nonprinting characters (View→Nonprinting Characters with shortcut Ctrl-F10\ton\tWindows\tor\tLinux) to see line and paragraph breaks. \tNon-Printing Characters in LibreOffice As a last way of confirming that we are starting to get a more useful structure from this, let's copy the full text from Writer again and paste it into a blank spreadsheet. This should confirm that each health record is now a separate row in the spreadsheet (although we also have page headings and footnotes mixed in — we will clean those up shortly). \tThe improved structure, shown in LibreOffice Calc  Finding structure for columns Spreadsheets organize information in two dimensions, rows and columns. We have seen that lines in Writer correspond to rows in Calc. How do we make columns? \n \n 230 Spreadsheet software can read and write plain-text files using any of several conventions for representing breaks between columns. One common format uses commas to separate columns, and such files are often stored with the extension \".csv\" for \"comma-separated values.\" Another common variant is to use a tab character, a special kind of space, to separate columns. Because our text contains commas, to avoid confusion we will use a tab character to separate columns. Though one could save a intermediate plain-text file, in this exercise we will assume we are copying and pasting directly from Writer to Calc. Back in Writer, let's start making columns by splitting the place-and-time information from the reported numbers. Almost all reports include the words Total\tnumber\tof\tdeaths Search for this and replace it with exactly the same phrase, but with \"\\t\" at the front of the string representing a tab character: \\tTotal\tnumber\tof\tdeaths After making this replacement (which makes 53 changes), select all the text and copy and paste it into an empty spreadsheet again. Does it look like nothing changed? LibreOffice Calc is putting the full text of each paragraph in a single cell, tabs and all. We need to insist on a plain-text interpretation to get Calc to ask us what to do with tabs. Let's try again. You can empty the spreadsheet conveniently by selecting all (Ctrl-A) and deleting the selection. In an empty spreadsheet, select Edit\t→\tPaste\tSpecial, (or right-click to reach the same) and then select \"unformatted text\" from the options in the window appears. That should result in a popup \"Text Import\" window. Make sure the Tab checkbox is selected under Separator options and then click \"OK\". (Before clicking OK you may want to try checking and unchecking Comma and Space as separators to preview what they would do here, but we do not want to treat them as separators in this context.) Now we see the promising start of a table structure, with geography and time span still in column A, but with \"Total number of deaths\" and subsequent text clearly aligned in a separate column. \tThe newly tab-delimited version of the data shown in LibreOffice Calc \n \n 231 Do you have any instances that moved over into a third column or beyond? In that case you may inadvertently have put in too many tabs. In the structure we have right now we don't expect to ever see two tab characters in a row. Back in LibreOffice Writer we can check for this and fix the problem by searching for \\t\\t and replacing with \\t repeating as needed until no more double-tabs are found. Sometimes multiple applications of a replacement pattern introduce additional changes after the first, which may or may not be what we intend, and sometimes multiple applications will have no effect beyond the first application. It is worth keeping this distinction in mind while working with regular expressions. Regular Expression Meaning [Ab1] a character class, matching one instance of any of A, b, or 1 in this case [a-z] all lowercase letters within a range [0-9] all digits . any character * zero or more + one or more (\t) if contents within parentheses match, define a group for future reference $1 refer to a matched group (this is the notation in LibreOffice; other notations such as  are sometimes used elsewhere) \\t Tab ^ beginning of line $ end of line  \n 232 The general idea of regular expressions Before doing any more practical work with the file, this is a good time for a brief introduction to regular expressions. Regular expressions (or \"regexes\" for short) are a way of defining patterns that can apply to sequences of things. They have the funny name that they do because of their origins in computer science and formal language theory, and they are incorporated into most general programming languages. Regexes are also often available in some form in advanced word processors, providing a more powerful means of find-and-replace than matching exact sequences letter by letter. There are different syntaxes and implementations of regular expressions, and what we have available in word processing programs often isn't as extensive, robust, or in conformance with wider practice as what one finds in programming language contexts, but there are essential common principles. LibreOffice for the most part follows notational conventions that you will see in other contexts. If you use a proprietary word processor you will likely find similar functionality even if the notation differs. For a more complete list of regular expressions in LibreOffice, see their List of Regular Expressions.269 Applying regular expressions Let's start to use some of these to remove the page headings with date and page number. Switch back to your LibreOffice Writer window. Replace:  ^.*February\t21.*1908.*$  with nothing (4 matches). Replace  ^.*Received\tout\tof\tregular\torder.*$  with nothing (2 matches). Here ^ (caret) matches the beginning of the line, . (period) matches any character, .* (period-asterisk) matches any sequence of zero or more characters, and $ (dollar-sign) matches the end of the line. By spelling out the date, we will match only the lines where that sequence appears, letter by letter, and by using .* at both ends we match all lines with that sequence regardless of what else is before or after it on the line. After making this replacement, we will be left with some blank lines. To remove the blank lines in LibreOffice,                                                         269\t‘List\tof\tRegular\tExpressions’:\thttps://help.libreoffice.org/Common/List_of_Regular_Expressions\t \n 233 Replace  ^$  with nothing (5 matches). (In other regular expression environments, other techniques for working with line endings will be necessary; some may be more convenient than what LibreOffice offers, but this will work now for our purposes.) Some records list a state, some a city with the state implicit, some a state and city together. The text does not have enough structure to give us a reliable way of distinguishing the California and Oakland records so that we will be able automatically to put California in a state column and Oakland in a city column. We will eventually need to do some editing by hand, drawing on our own knowledge. But there is a lot of consistency in the references to spans of time. We can use those references to develop structures that will help keep similar segments aligned across rows. For convenience, let's put some markers in the text that won't be confused with anything already present. We can easily distinguish these markers from existing text, and easily remove them later when we don't need them. Let's match time span references and put \"<t>\" at the beginning of them and \"</t>\" at the end, with the mnemonic \"t\" for time. We could put a more verbose marker in, like \"<time>\" or a more meaningless and untidy-looking one, like \"asdfJKL;\" as long as that sequence wasn't for some reason already in our text. But in this exercise we will use markers like \"<t>\" If you have seen HTML or XML, these look a lot like the tags that mark elements. We are not creating acceptable HTML or well-formed XML by doing this, and we will remove these markers quickly, but there is a resemblance. Obligatory warning: Regular expressions are powerful, but they do have their limits and (when used to modify material that someone cares about) they can be dangerous, in that a mistake can inadvertently remove or scramble a lot of information quickly. Also, as XML aficionados may passionately tell you, regular expressions are not up to the job of general-purpose parsing of XML. After one sees how useful regular expressions are at dealing with certain kinds of patterns, there is a temptation to think, whenever we see a pattern that a computer ought to be able to help with, that regular expressions are all we need. In many cases that will turn out not to be true. Regular expressions are not adequate to deal with hierarchically nested patterns that XML is good at describing. But that's OK. In the context of this tutorial, we don't claim to know anything in particular about XML, or to care about formal language grammars. We just want to put some convenient markers into a text in order to get some leverage in making a relatively simple implicit structure a bit more explicit, and we will take those markers out before we are done. There is a reason why such markers are useful. If you find yourself  \n 234 intrigued by what can be done with patterns in this exercise, you may want to learn more about HTML and XML, and learn what can be done with appropriate methods that their more explicit structure makes possible. Defining segments The next few patterns will rapidly get more complicated. If you slow down to consult the reference to how the symbols define patterns, however, the patterns should start to make sense. Geographic references in our text are followed by emdashes (dashes that are roughly the width of the letter 'm'; wider than endashes.) We can replace these with tab characters, which will effectively help us put states and cities in separate columns of the spreadsheet. Replace  [\t]?—[\t]? with  \\t You should have 42 matches. (One easy way to get the emdash into your pattern is to copy and paste from an existing emdash in the text itself. The square brackets aren't entirely necessary here, but help make visible the fact that we are matching a blank space — optionally matching it, thanks to the question mark. That means our pattern will accept an emdash with or without a space on either or both sides of it.) Now we will look for explicit references to time and wrap them in \"<t>\" and \"</t>\" markers before and after. Once we have those markers they will provide some scaffolding on which we can build further patterns. Note that in the next pattern we want to be sure to apply the replacement just once, otherwise some time references may be repeatedly wrapped. It will be most efficient to use Replace\tAll just once for each wrapping pattern. Replace  (Month\tof\t[A-Z][a-z,\t0-9]+\t19[0-9][0-9].) with  <t>$1</t>  \n 235 \n\tFinding time using Regular Expressions Here we are using parentheses to define everything that we match in the search pattern as a single group, and in the replacement pattern we use $1 to simply repeat that match, with a few additional characters before and after it. In addition to months, we need to match quarterly reports with a similar approach: Replace  ([-A-Za-z\t]+\tended\t[A-Z][a-z,\t0-9]+\t19[0-9][0-9].) with  <t>$1</t> You should have 7 more matches. It looks like we have references to time accounted for. Extending this strategy to other kinds of information here, let's use \"<p>\" for population estimates, \"<N>\" for total number of deaths, and \"<c>\" for the word \"Cases,\" which separates mortality from morbidity. (If you are familiar with HTML or XML, you may recognize \"<p>\" as a paragraph marker. We're not using it in the same way here.) Here are some patterns to wrap each of those kinds of information, all using the same strategy we just used: Replace  (Estimated\tpopulation,\t[0-9,]+.) with  <p>$1</p> (34 matches). Replace  (Total\tnumber\tof\tdeaths[A-Za-z\t,]*\t[0-9,]+) with  \n \n 236 <N>$1</N> (48 matches). Replace  (Cases\t?:) with  <c>$1</c> (49 matches). This next part is a little trickier. It would be great if we could get hold of the disease (let's use \"<d>\") and count (\"<n>\") segments. Because the prose in this document is so formulaic, especially following the indication of total number of deaths, in this case we will be able to get pretty far without having to match each disease name explicitly, one by one. First match the disease-count pair after the word \"including\": Replace  </N>\tincluding\t([A-Za-z\t]+)\t([0-9]+), with  </N>\tincluding\t<d>$1</d>\t<n>$2</n> (29 matches). And then iteratively match disease-count pairs that appear after existing markers: Replace  >\t([A-Za-z\t]+)\t([0-9]+)([.,]) with  >\t<d>$1</d>\t<n>$2</n> Note that we are getting rid of commas after the disease counts by ignoring the third match in our replacement. Repeat this replacement as many times as necessary until there are no further matches. It should take you seven iterations. Our patterns have not done anything with phrases like 'and 3 from tuberculosis.' We can match those phrases and reverse the order so that the disease name appears before the count: Replace  and\t([0-9])+\tfrom\t([a-z\t]+) with  <d>$2</d>\t<n>$1</n> (32 matches).  \n 237 It looks like our markers are now capturing a lot of the semantic structure that we are interested in. Now let's copy and paste (\"paste special … unformatted\") into LibreOffice Calc to see how close we are to getting a table. We are successfully separating location data into cells, but the cells are not aligned vertically yet. We want to get all of the time references into the third column. \n\tMeasuring progress using LibreOffice Calc  The instances with two columns of location information should already be OK. The rows with one location need an extra column. Most are cities, so we will put the locations into the second column, and in a few instances we will need to move state names back to the first column by hand. Go back to your LibreOffice Writer window and: Replace  ^([A-Za-z\t.]+\\t<t>) with  \\t$1 (30 matches). Now fix the cases with no location information, where the location is implicitly the same as the row above, and the time span is different. Replace  ^<t> with  \\t\\t<t> (19 matches) \n \n 238 \n\tFurther refining the results  The first few columns should look better after pasting this again into Calc. The Writer text is still our working copy, so if you want to fix up the state names, you could do so now in Writer by deleting the tab character before a state name and introducing a new tab character after it. Or you could wait until we are done with our work in Writer, and fix them in Calc after we are ready for that to be our live working copy. But we are not there yet. We need to decide how to handle the lists of diseases. The rows have different lists of varying lengths. While it would be easy enough now to insert tab characters to put each disease and mortality or morbidity count into a separate column, the columns would not be that helpful. Diseases and tallies would not be vertically aligned. What we can do instead is make a new row for each disease. The reports distinguish between mortality counts and morbidity counts, which are already conveniently separated by \"Cases:\". (There is one case, Indiana, where the text marks this section with the word \"Morbidity\". Our searching patterns missed this. You can fix the markup there by hand now, if you like, or ignore it since this is an exercise. It's a good example of how automated tools aren't a full substitute for editing or looking at your sources, and it won't be the last such example.) We can start by making a new row for \"cases\" lists, so that we can handle them separately. Head back to LibreOffice Writer. \n \n 239 \n\tMaking a new row for 'cases' Replace  ^(.*\\t)(.*\\t)(<t>.*</t>)(.*)(<c>.*) with  $1$2$3$4\\n$1$2$3\\t$5 (47 matches). One thing to notice here is that we are using some of the replacement patterns twice. We are matching the three fields up to the time reference, then matching everything before \"<c>\" in a fourth group, and everything from \"<c>\" on in a fifth. In the replacement pattern, we put groups 1-4 back in order, then introduce a newline and print groups 1-3 again, followed by a tab and group 5. We've effectively moved the case listings to their own lines, and copied the place and time fields verbatim. Let's go further, and split all the case lists into separate rows: Replace  ^(.*\\t)(.*\\t)(<t>.*</t>)(.*<c>.*)(<d>.*</d>)\t(<n>.*</n>) with  $1$2$3$4\\n$1$2$3\\tCases\\t$5$6 and repeat as many times as necessary until there are no more replacements (seven iterations). Now similarly split all the mortality lists into separate rows: Replace  ^(.*\\t)(.*\\t)(<t>.*</t>)(.*<N>.*)(<d>.*</d>)\t(<n>.*</n>) with  \n \n 240 $1$2$3$4\\n$1$2$3\\tDeaths\\t$5$6 and repeat as many times as necessary until there are no more replacements (eight iterations). This is getting very close now to a tabular structure, as you can see if you paste again into Calc, though if you want to wait just a bit, some cleanup work with short and simple patterns will get us most of the rest of the way: Replace  .*</c>\t$  with nothing Replace  ^$  with nothing Replace  <n> with  \\t Replace  </n>  with nothing Replace  <d>and with  <d> Replace  </?[tdp]>  with nothing  \n 241 \n\tThe final view in LibreOffice Writer  Now copy and paste this into Calc, and you should see a (mostly) well-structured table. \n\tThe final view in LibreOffice Calc  If this were not an exercise but a source we were editing for research or publication, there are still things that we would need to fix. We didn't do anything with estimated population figures. Our pattern-matching wasn't sophisticated enough to manage everything. In lines that didn't have patterns like \"Total number of deaths 292, including,\" we missed all subsequent patterns that assumed we had already put in an \"</N>\" marker. Next possibilities Some of these problems could be fixed by additional pattern-matching steps, some by hand-editing of the source document at particular points along the way, and some by later editing of the data in spreadsheet or similar tabular form. We might want to consider other structures for the table, too — perhaps mortality and morbidity would be more convenient to tally if they were in different columns. Word processors are not the best tools for making use of these kinds of structures. Spreadsheets, XML, and programmatic tools for working with data are much more likely to be helpful. But word processors do have advanced find-and-replace functions that are good to get to know. Regular expressions and advanced pattern matching can be helpful in editing, and can provide a bridge between sequences with implicit structure and more explicit structures that we may want to match or create. \n \n 242 There are more than 400 public health reports like this one available from the Internet Archive. If we wanted to tabulate all of them, LibreOffice would not be the best primary tool. It would be better to learn a little Python, Ruby, or shell scripting. Programmer-oriented plain text editors, including classic ones such as Emacs and Vi or Vim, have great regular expression support as well as other features useful for dealing with plain text in a programmatic way. If you are comfortable opening up a Unix-like shell command line (in Mac or Linux, or on Windows through a virtual machine or the Cygwin environment), you can learn and use regular expressions very well with tools like \"grep\" for searching and \"sed\" for line-oriented replacing. Regular expressions can be immensely useful in dealing with patterns across hundreds of files at once. The patterns we have used in this example would need to be refined and extended to deal with assumptions that are certain to be mistaken when applied to longer texts or larger sets of texts, but with a programming language we could record what we are doing in a short script, and refine and rerun it repeatedly to get closer to what we want. To learn more The Wikipedia page on regular expressions270 is a useful place to find a brief history of regular expressions and their relation to formal language theory, as well as an overview of syntactic variants and formal standardization efforts. The documentation for whatever tools you use will be invaluable for practical use, especially for work in word processing environments where regular expression implementations may be especially idiosyncratic. There are many resources available to learn how to use regular expressions in programming contexts; which is best for you may depend on what programming language is most familiar or convenient to start with. There are a number of freely available web-based regular expression editors. Rubular,271 built on the Ruby programming language, has a helpful interface that lets you test regular expressions against a sample text and dynamically shows matches and matched groups. David Birnbaum, Chair of the Department of Slavic Languages and Literatures at the University of Pittsburg, has some good materials on how to work with regular expressions and XML tools272 to help mark up plain-text files in TEI XML. Zed Shaw has begun developing a book, freely available online, Learn Regex the Hard Way.273 The book's exercises are built around a Python-based program developed by the author.                                                         270\t‘Regular\tExpressions’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Regular_expression\t271\t‘Rubular:\ta\tRuby\tregular\texpression\teditor’:\thttp://rubular.com/\t272\t‘Regular\texpressions\t(regex)’:\thttp://dh.obdurodon.org/regex.html\t273\tZed\tShaw,\t‘Learn\tRegex\tthe\tHard\tWay’:\thttp://regex.learncodethehardway.org/book/\t \n 243 About the Author Doug Knox is the Assistant Director of the Humanities Digital Workshop at Washington University in St. Louis.  \n 244 27. Cleaning OCR’d text with Regular Expressions Laura Turner O’Hara – 2013     Optical Character Recognition (OCR)—the conversion of scanned images to machine-encoded text—has proven a godsend for historical research. This process allows texts to be searchable on one hand and more easily parsed and mined on the other. But we’ve all noticed that the OCR for historic texts is far from perfect. Old type faces and formats make for unique OCR. Take for example, this page from the Congressional Directory from the 50th Congress (1887). The PDF scan downloaded from HeinOnline274 looks organized: \n This is a screenshot of the PDF page                                                         274\t‘HeinOnline’:\thttp://home.heinonline.org/\t\n \n 245  However, the OCR layer (downloaded as a text file*) shows that the machine-encoded text is not nearly as neat: \n This is a screenshot of the OCR Note:\tIf\tyou\tdo\tnot\thave\tthe\toption\tto\tdownload\ta\ttext\tfile,\tyou\tcan\tuse\tthe\tpdfminer\tmodule275\tto\textract\ttext\tfrom\tthe\tpdf.\tSince I want to use this to map the Washington residences for Members of these late 19th-century Congresses, how might I make this data more useable? The answer is Regular Expressions or “regex.” Here’s what regex did for me. Though this is not a “real” CSV file (the commas are not quite right), it can be easily viewed in Excel and prepped for geocoding. Much better than the text file from above, right?                                                         275\t‘PDFMiner’:\thttp://www.unixuser.org/~euske/python/pdfminer/index.html\t\n \n 246 Aldrich,\tN.\tW,Providence,\tR.\tI\tAllison,\tWilliam\tB,\tDubuque,\tIowa,24Vermont\tavenue,\tBate,\tWilliam,Nashville,\tTen,\tEbbitt\tHouse\tBeck,\tJames\tB,Lexington,\tKy\tBerry,\tJames\tI,\tBentonville,\tArk,\tNational\tHotel,\tBlair,\tI\tlenry\t\\V,\tManchester,\tN.\tH,2o\tEast\tCapitol\tstree_._'\tBlodgett,\tRufus,Long\tBranch,\tN.\tJ\tBowen,\tThomas\tM,Del\tNorte,\tColo\tBrown,\tJoseph\tE,\tAtlanta,\tGa,\tWoodmont\tFlats,\tButler,\tM.\tC,Edgefield,\tS.\tC,\t1751\tP\tstreet\tNW\tCall,\tWilkinson,\tJacksonville,\tFla,\t1903\tN\tstreet\tNW\tCameron,\tJ.\tD,Harrisburg,\tPa,\t21\tLafayette\tSquare,\tChace,\tJonathan,Providence,\tR,\tI\tChandler,\tWilliam\tE,\tConcord,\tN.\tH,\t1421\tI\tstreet\tNW\tCockrell,\tFrancis\tM,Warrensburgh,Mo,\tI518\tR\tstreet\tNW\tCoke,\tRichard,Waco,\tTex,\t419\tSixth\tstreet\tNW\tColquitt,\tAlfred\tI\tI,Atlanta,\tGa,\t920\tNew\tYork\tavenue\tCullom,\tShelby\tM,Springfield,\tIll,\t1402\tMassachusetts\tavenue\tDaniel,\tJohn\tW,,Lynchburgh,\tVa,\tI7OO\tNineteenth\tst.\tNW\tDavis,\tCushman\tK,\tSaint\tPaul,\tMinn,\t17oo\tFifteenth\tstreet\tNW\tDawes,\tHenry\tL,Pittsfield,\tMass,\t1632Rhode\tIsland\tavenue.\tDolph,\tJoseph\tN,Portland,\tOregon,\t8\tLafayette\tSquare,\tEdmunds,\tGeorge\tF,\tBurlington,\tVt,\t2111\tMassachusetts\tavenue\tEustis,\tJames\tB,,New\tOrleans,\tLa,\t1761\tN\tstreet\tNW\tEvarts,\tWilliam\tM,New\tYork,\tN.\tY,\ti6oi\tK\tstreet\tNW\tFarwell,\tCharles\tB,\tChicago,\tIll,\tFaulkner,\tCharles\tJames,\tMartinsburgh,\tW.\tVa,\tFrye,\tWilliam\tP,Lewiston,\tMe,\tHamilton\tHouse,\tGeorge,\tJames\tZ,Jackson,\tMiss,\tMetropolitan\tHotel\tGibson,\tRandall\tLee,\tNew\tOrleans,\tLa,\t1723\tRhode\tIsland\tavenue.\tGorman,\tArthur\tP,\tLaurel,\tMd\t.,1403\tK\tstreet\tNW\tGray,\tGeorge,Wilmington,\tDel,\tHale,\tEugene,Ellsworth,\tMe,\t917\tSixthteenth\tst.\tNW\tHampton,\tWade,\tColumbia,\tS.\tC,\tHarris,\tIsham\tG,\tMemphis,Tenn,\t13\tFirst\tstreet\tNE\tHawley,\tJoseph\tR,Hartford,\tCorn,\t1514\tK\tstreet\tNW\tHearst,\tGeorge,San\tFrancisco,\tCal,\tHiscock,\tFrank,\tSyracuse,\tN.\tY,\tArlington\tHotel\tHoar,\tGeorge\tF,\tWorcester,\tMass,\t1325\tK\tstreet\tNW\tIngalls,\tJohn\tJames,\tAtchison,\tKans,\tI\tB\tstreet\tNW\tJones,\tJames\tK,Washington,\tArk,\t915\tM\tstreet\tNW\tJones,\tJohn\tP,Gold\tHill,\tNev\tKenna,\tJohn\tE,Charleston,\tW.\tVa,\t14o\tB\tstreet\tNW\tMcPherson,\tJohn\t,Jersey\tCity,\tN.\tJ,\t1014\tVermont\tavenue,\tManderson,\tCharlesF.\tOmaha,\tNebr,The\tPortland\tMorgan,\tJohn\tT,.Selma,\tAla,I\t13\tFirst\tstreet\tNE\tMorrill,\tJustin\tS,\tStratford,\tVt,\tx\tThomas\tCircle\tRegular Expressions (Regex) Regex is not a programming language. Rather it follows a syntax used in many different languages, employing a series of characters to find and/or replace precise patterns in texts. For example, using this sample text: Let's\tget\tall\tthis\tbad\tOCR\tand\t$tuff.\tGr8!\t1. You could isolate all the capital letters (L, O, C, R, G) with this regex: [A-Z]\t2. You could isolate the first capital letter (L) with this regex:  \n 247 ^[A-Z]\t3. You could isolate all characters BUT the capital letters with this regex: [^A-Z]\t4. You could isolate the acronym “OCR” with this regex: [A-Z]{3}\t5. You could isolate the punctuation using this regex: [[:punct:]]\t6. You could isolate all the punctuation, spaces, and numbers this way: [[:punct:],\t,0-9]\tThe character set is not that large, but the patterns can get complicated. Moreover, different characters can mean different things depending on their placement. Take for example, the difference between example 2 and example 3 above. In example 2, the caret (^) means isolate the pattern at the beginning of the line or document. However, when you put the caret inside the character class (demarcated by []) it means “except” these sets of characters. The best way to understand Regular Expressions is to learn what the characters do in different positions and practice, practice, practice. And since experimentation is best way to learn, I suggest using a regex tester tool and experiment with the syntax. For Mac users, I had a lot of luck with the Patterns App276 (Mac Store $2.99), which allowed me to see what the regular expressions were doing in real time. It also comes with a built-in cheat sheet for the symbols, but I actually found this generic (meaning it works across languages) cheat sheet277 more comprehensive. Python and Regex In this tutorial, I use the Regular Expressions Python module to extract a “cleaner” version of the Congressional Directory text file. Though the documentation278 for this module is fairly comprehensive, beginners will have more luck with the simpler Regular Expression HOWTO documentation.279 Two things to note before you get started                                                          276\t‘Patterns’,\tKrillapps:\thttp://krillapps.com/patterns/\t277\tDave\tChild,\t‘Regular\tExpressions\tCheat\tSheet’,\tCheatography:\thttp://www.cheatography.com/davechild/cheat-sheets/regular-expressions/\t278\t‘Regular\texpression\toperations’,\tPython:\thttps://docs.python.org/2/library/re.html\t279\t‘Regular\tExpression\tHOWTO’,\tPython:\thttps://docs.python.org/2/howto/regex.html#regex-howto\t \n 248 From what I’ve observed, Python is not the most efficient way to use Regular Expressions if you have to clean a single document. Command Line programs like sed or grep280 appear to be more efficient for this process. (I will leave it to the better grep/sed users to create tutorials on those tools.) I use Python for several reasons: 1) I understand the syntax best; 2) I appreciate seeing each step written out in a single file so I can easily backtrack mistakes; and 3) I want a program I could use over and over again, since I am cleaning multiple pages from the Congressional Directory. The OCR in this document is far from consistent (within a single page or across multiple pages). Thus, the results of this cleaning tutorial are not perfect. My goal is to let regex do the heavy lifting and export a document in my chosen format that is more organized than the document with which I started. This significantly reduces, but does not eliminate, any hand-cleaning I might need to do before geocoding the address data. My example Python File Here’s the Python file that I used to created to clean my document: #cdocr.py\t#strip\tthe\tpunctuation\tand\textra\tinformation\tfrom\tHeinOnline\ttext\tdocument\t\t#import\tre\tmodule\timport\tre\t\t#Open\tthe\ttext\tfile\twith\tthe\tocr\tocr\t=\topen('../../data/txt/50-1-p1.txt')\t#read\tthe\ttext\tfile\tinto\ta\tlist\tText\t=\tocr.readlines()\t\t#Create\tan\tempty\tlist\tto\tfill\twith\tlines\tof\tcorrected\ttext\tCleanText\t=\t[]\t\t#\tchecks\teach\tline\tin\tthe\timported\ttext\tfile\tfor\tall\tthe\tfollowing\tpatterns\tfor\tline\tin\tText:\t\t\t\t\t#lines\twith\tmulti-dashes\tcontain\tdata\t-\tsearches\tfor\tthose\tlines\t\t\t\t\t#\t--\tdoes\tnot\tisolate\tintro\ttext\tlines\twith\tone\tdash.\t\t\t\t\tdashes\t=\tre.search('(--+)',\tline)\t\t\t\t\t\t#isolates\tlines\twith\tdashes\tand\tcleans\t\t\t\t\tif\tdashes:\t\t\t\t\t\t\t\t\t#replaces\tdashes\twith\tmy\tchosen\tdelimiter\t\t\t\t\t\t\t\t\tnodash\t=\tre.sub('.(-+)',\t',',\tline)\t\t\t\t\t\t\t\t\t#strikes\tmultiple\tperiods\t\t\t\t\t\t\t\t\tnodots\t=\tre.sub('.(\\.\\.+)',\t'',\tnodash)\t\t\t\t\t\t\t\t\t#strikes\textra\tspaces\t\t\t\t\t\t\t\t\tnospaces\t=\tre.sub('(\t\t+)',\t',',\tnodots)\t\t\t\t\t\t\t\t\t#strikes\t*\t\t\t\t\t\t\t\t\tnostar\t=\tre.sub('.[*]',\t'',\tnospaces)\t\t\t\t\t\t\t\t\t#strikes\tnew\tline\tand\tcomma\tat\tthe\tbeginning\tof\tthe\tline\t                                                        280\t‘GNU\tsed’:\thttp://www.gnu.org/software/sed/;\t‘GNU\tGrep’:\thttp://www.gnu.org/software/grep/\t \n 249 \t\t\t\t\t\t\t\tflushleft\t=\tre.sub('^\\W',\t'',\tnostar)\t\t\t\t\t\t\t\t\t#getting\trid\tof\tdouble\tcommas\t(i.e.\t-\tEvarts)\t\t\t\t\t\t\t\t\tcomma\t=\tre.sub(',{2,3}',\t',',\tflushleft)\t\t\t\t\t\t\t\t\t#cleaning\tup\tsome\twords\tthat\tare\tstuck\ttogether\t(i.e.\t-\t\tDawes,\tManderson)\t\t\t\t\t\t\t\t\t#skips\tdouble\tOO\tthat\twas\tput\tin\tplace\tof\t00\tin\taddress\t\t\t\t\t\t\t\t\tcaps\t=\tre.sub('[A-N|P-Z]{2,}',\t',',\tcomma)\t\t\t\t\t\t\t\t\t#Clean\tup\tNE\tand\tNW\tquadrant\tindicators\tby\tremoving\tperiods\t\t\t\t\t\t\t\t\tne\t=\tre.sub('(\\,*?\tN\\.\t?E.)',\t'\tNE',\tcaps)\t\t\t\t\t\t\t\t\tnw\t=\tre.sub('(\\,*?\tN\\.\t?W[\\.\\,]*?_?)$',\t'\tNW',\tne)\t#MAKE\tVERBOSE\t\t\t\t\t\t\t\t\t#Replace\tperiods\twith\tcommas\tbetween\tlast\tand\tfirst\tnames\t(i.e.\t-\tChace,\tCockrell)\t\t\t\t\t\t\t\t\tmatch\t=\tre.search('^([A-Z][a-z]+\\.\t)',\tnw)\t#MAKE\tVERBOSE\t\t\t\t\t\t\t\t\tif\tmatch:\t\t\t\t\t\t\t\t\t\t\t\t\tnames\t=\tre.sub('\\.',\t',',\tnw)\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\tnames\t=\tnw\t\t\t\t\t\t\t\t\t\t\t\t#Append\teach\tline\tto\tCleanText\tlist\twhile\tit\tloops\tthrough\t\t\t\t\t\t\t\t\tCleanText.append(names)\t\t#Saving\tinto\ta\t'fake'\tcsv\tfile\tfcsv\t=\topen('cdocr2/50-1p1.csv',\t'w')\t#Write\teach\tline\tin\tCleanText\tto\ta\tfile\tfor\tline\tin\tCleanText:\t\t\t\t\tfcsv.write(line)\tI’ve commented it pretty extensively, so I will explain why I structured the code the way I did. I will also demonstrate a different way to format long regular expressions for better legibility. Lines 16-22 – Notice in my original text file that my data is all on lines with multiple dashes. This code effectively isolates those lines. I use the re.search() function281 to find all lines with multiple dashes. The “if” statement on line 20 only works with the lines with dashes in the rest of the code. (This eliminates all introductory text and the rows of page numbers that follow the data I want.) Lines 23-40 – This is the long process by which I eliminate all of the extraneous punctuation and put the pieces of my data (last name, first name, home post office, washington address) into different fields for a csv document. I use the re.sub() function,282 which substitutes pattern with another character. I comment extensively here, so you can see what each piece does. This may not be the most efficient way of doing this, but by doing this piece by piece, I could check my work as I went. As I built loop, I checked each step by printing the variable in the command line. So, for example, after line 24 (when I eliminate the dashes), I would add “print nodash” (inside the if loop) before I ran the file in the command line. I checked each step to make sure my patterns were only changing the things I wanted and not changing things I did not want changed. Lines 41-46 - I used a slightly different method here. The OCR in the text file separated some names with a period (for example, Chace.Jonathan vs.                                                         281\t‘re.search’,\tPython\tDocumentation:\thttps://docs.python.org/2/library/re.html#re.search\t282\t‘re.sub’,\tPython\tDocumentation:\thttps://docs.python.org/2/library/re.html#re.sub\t \n 250 Chase,Jonathan). I wanted to isolate the periods that came up in this pattern and change those periods to commas. So I searched for the pattern ^([A-Z][a-z]+\\.), which looks at the beginning of a line (^) and finds a pattern with one capital letter, multiple lowercase letters and a period. After I had isolated that pattern, I substitute the period those lines that fit the pattern with a comma. Using Verbose Mode Most regular expressions are difficult to read. But lines 39 and 40 look especially bad. How might you clarify these patterns for people who might look at your code (or for yourself when you are staring at them at 2:00 AM someday)? You can use the module’s verbose mode.283 By putting your patterns in verbose mode, python ignores white space and the # character, so you can split the patterns across multiple lines and comment each piece. Keep in mind that, because it ignores spaces, if spaces are part of your pattern, you need to escape them with a backslash (\\). Also note that re.VERBOSE and re.X are the same thing. Here are lines 39 and 40 in verbose mode: #This\tis\tthe\tsame\tas\t(\\,*?\tN\\.\t?E.)\t#All\tspaces\tneed\tto\tbe\tescaped\tin\tverbose\tmode.\tne_pattern\t=\tre.compile(r'''\t\t\t\t\t(\t\t\t\t\t\t\t\t\t\t\t#start\tgroup\t\t\t\t\t\t\t\t\t\\,*?\t\t\t\t#look\tfor\tcomma\t(escaped);\t*?\t=\t0\tor\tmore\tcommas\twith\tfewest\tresults\t\t\t\t\t\t\t\t\t\\\tN\\.?\t\t#look\tfor\t(escaped)\tspace\t+\tN\tthat\tmight\thave\tan\t(escaped)\tperiod\tafter\tit\t\t\t\t\t\t\t\t\t\\\t?E\t\t\t\t#look\tfor\tan\tE\tthat\tmay\tor\tmay\tnot\thave\tan\tspace\tin\tfront\tof\tit\t\t\t\t\t\t\t\t\t.\t\t\t\t\t\t\t#the\tE\tmight\tbe\tfollowed\tby\tanother\tcharacter.\t\t\t\t\t)\t\t\t\t\t\t\t\t\t\t\t#close\tgroup\t\t\t\t\t$\t\t\t\t\t\t\t\t\t\t\t#ONLY\tlook\tat\tthe\tend\tof\ta\tline\t''',\tre.VERBOSE)\t\t#This\tis\tthe\tsame\tas\t(\\,*?\tN\\.\t?W[\\.\\,]*?_?)$\tnw_pattern\t=\tre.compile(r'''\t\t\t\t\t(\t\t\t\t\t\t\t\t\t\t\t#start\tgroup\t\t\t\t\t\t\t\t\t\\,*?\t\t\t\t#look\tfor\tcomma\t(escaped);\t*?\t=\t0\tor\tmore\tcommas\twith\tfewest\tresults\t\t\t\t\t\t\t\t\t\\\tN\\.?\t\t#look\tfor\tescaped\tspace+N\tthat\tmight\thave\tan\tescaped\tperiod\tafter\tit\t\t\t\t\t\t\t\t\t\\\t?W\t\t\t\t#look\tfor\tan\tW\tthat\tmay\tor\tmay\tnot\thave\tan\tspace\tin\tfront\tof\tit\t\t\t\t\t\t\t\t\t[\\.\\,]*?\t#look\tfor\tcommas\tor\tperiods\t(both\tescaped)\tthat\tmight\tcome\tafter\tW\t\t\t\t\t\t\t\t\t_?\t\t\t\t\t\t#look\tfor\tunderscore\tthat\tcomes\tafter\tNW\tquadrant\tindicators\t\t\t\t\t)\t\t\t\t\t\t\t\t\t\t\t#close\tgroup\t\t\t\t\t$\t\t\t\t\t\t\t\t\t\t\t#ONLY\tlook\tat\tthe\tend\tof\ta\tline\t''',\tre.X)\tIn above example, I use the re.compile() function284 to save the pattern for future use. So, adjusting my full python code to use verbose mode would look like the following. Note that I define my verbose patterns on lines 17-39 and store them in variables (ne_pattern and nw_pattern). I use them in my loop on lines 65 and 66. #cdocrverbose.py\t#strip\tthe\tpunctuation\tand\textra\tinformation\tfrom\tHeinOnline\ttext\tdocument\t                                                        283\t‘re.verbose’,\tPython\tDocumentation:\thttps://docs.python.org/2/library/re.html#re.VERBOSE\t284\t‘re.compile’,\tPython\tDocumentation:\thttps://docs.python.org/2/library/re.html#re.compile\t \n 251 \t#import\tre\tmodule\timport\tre\t\t#Open\tthe\ttext\tfile\twith\tthe\tocr\tocr\t=\topen('../../data/txt/50-1-p1.txt')\t#read\tthe\ttext\tfile\tinto\ta\tlist\tText\t=\tocr.readlines()\t\t#Create\tan\tempty\tlist\tto\tfill\twith\tlines\tof\tcorrected\ttext\tCleanText\t=\t[]\t\t##Creating\tverbose\tpatterns\tfor\tthe\tmore\tcomplicated\tpieces\tthat\tI\tuse\tlater\ton.##\t\t#This\tis\tthe\tsame\tas\t(\\,*?\tN\\.\t?E.)\t#All\tspaces\tneed\tto\tbe\tescaped\tin\tverbose\tmode.\tne_pattern\t=\tre.compile(r'''\t\t\t\t\t(\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#start\tgroup\t\t\t\t\t\t\t\t\t\\,*?\t\t\t\t\t\t\t\t#look\tfor\tcomma\t(escaped);\t*?\t=\t0\tor\tmore\tcommas\twith\tfewest\tresults\t\t\t\t\t\t\t\t\t\\\tN\\.?\t\t\t\t\t\t#look\tfor\t(escaped)\tspace\t+\tN\tthat\tmight\thave\tan\tescaped\tperiod\tafter\tit\t\t\t\t\t\t\t\t\t\\\t?E\t\t\t\t\t\t\t\t#look\tfor\tan\tE\tthat\tmay\tor\tmay\tnot\thave\tan\tspace\tin\tfront\tof\tit\t\t\t\t\t\t\t\t\t.\t\t\t\t\t\t\t\t\t\t\t#the\tE\tmight\tbe\tfollowed\tby\tanother\tcharacter.\t\t\t\t\t)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#close\tgroup\t\t\t\t\t$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#ONLY\tlook\tat\tthe\tend\tof\ta\tline\t''',\tre.VERBOSE)\t\t#This\tis\tthe\tsame\tas\t(\\,*?\tN\\.\t?W[\\.\\,]*?_?)$\tnw_pattern\t=\tre.compile(r'''\t\t\t\t\t(\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#start\tgroup\t\t\t\t\t\t\t\t\t\\,*?\t\t\t\t\t\t\t\t#look\tfor\tcomma\t(escaped);\t*?\t=\t0\tor\tmore\tcommas\twith\tfewest\tresults\t\t\t\t\t\t\t\t\t\\\tN\\.?\t\t\t\t\t\t#look\tfor\tescaped\tspace\t+\tN\tthat\tmight\thave\tan\tescaped\tperiod\tafter\tit\t\t\t\t\t\t\t\t\t\\\t?W\t\t\t\t\t\t\t\t#look\tfor\tan\tW\tthat\tmay\tor\tmay\tnot\thave\tan\tspace\tin\tfront\tof\tit\t\t\t\t\t\t\t\t\t[\\.\\,]*?\t\t\t\t#look\tfor\tcommas\tor\tperiods\t(both\tescaped)\tthat\tmight\tcome\tafter\tW\t\t\t\t\t\t\t\t\t_?\t\t\t\t\t\t\t\t\t\t#look\tfor\tunderscore\tthat\tcomes\tafter\tone\tof\tthese\tNW\tquadrant\tindicators\t\t\t\t\t)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#close\tgroup\t\t\t\t\t$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#ONLY\tlook\tat\tthe\tend\tof\ta\tline\t''',\tre.VERBOSE)\t\t#\tchecks\teach\tline\tin\tthe\timported\ttext\tfile\tfor\tall\tthe\tfollowing\tpatterns\tfor\tline\tin\tText:\t\t\t\t\t#lines\twith\tmulti-dashes\tcontain\tdata\t-\tsearches\tfor\tthose\tlines\t\t\t\t\t#\t--\tdoes\tnot\tisolate\tintro\ttext\tlines\twith\tone\tdash.\t\t\t\t\tdashes\t=\tre.search('(--+)',\tline)\t\t\t\t\t\t#isolates\tlines\twith\tdashes\tand\tcleans\t\t\t\t\tif\tdashes:\t\t\t\t\t\t\t\t\t#replaces\tdashes\twith\tmy\tchosen\tdelimiter\t\t\t\t\t\t\t\t\tnodash\t=\tre.sub('.(-+)',\t',',\tline)\t\t\t\t\t\t\t\t\t#strikes\tmultiple\tperiods\t\t\t\t\t\t\t\t\tnodots\t=\tre.sub('.(\\.\\.+)',\t'',\tnodash)\t\t\t\t\t\t\t\t\t#strikes\textra\tspaces\t\t\t\t\t\t\t\t\tnospaces\t=\tre.sub('(\t\t+)',\t',',\tnodots)\t\t\t\t\t\t\t\t\t#strikes\t*\t\t\t\t\t\t\t\t\tnostar\t=\tre.sub('.[*]',\t'',\tnospaces)\t\t\t\t\t\t\t\t\t#strikes\tnew\tline\tand\tcomma\tat\tthe\tbeginning\tof\tthe\tline\t\t\t\t\t\t\t\t\tflushleft\t=\tre.sub('^\\W',\t'',\tnostar)\t\t\t\t\t\t\t\t\t#getting\trid\tof\tdouble\tcommas\t(i.e.\t-\tEvarts)\t\t\t\t\t\t\t\t\tcomma\t=\tre.sub(',{2,3}',\t',',\tflushleft)\t\t\t\t\t\t\t\t\t#cleaning\tup\tsome\twords\tthat\tare\tstuck\ttogether\t(i.e.\t-\t\tDawes,\tManderson)\t\t\t\t\t\t\t\t\t#skips\tdouble\tOO\tthat\twas\tput\tin\tplace\tof\t00\tin\taddress\t\t\t\t\t\t\t\t\tcaps\t=\tre.sub('[A-N|P-Z]{2,}',\t',',\tcomma)\t\t\t\t\t\t\t\t\t#Clean\tup\tNE\tand\tNW\tquadrant\tindicators\tby\tremoving\t\t\t\t\t\t\t\t\t\t#periods\t(using\tVerbose\tregex\tdefined\tabove)\t\t\t\t\t\t\t\t\tne\t=\tre.sub(ne_pattern,\t'\tNE',\tcaps)\t\t\t\t\t\t\t\t\tnw\t=\tre.sub(nw_pattern,\t'\tNW',\tne)\t\t\t\t\t\t\t\t\t#Replace\tperiods\twith\tcommas\tbetween\tlast\tand\tfirst\tnames\t\t \n 252 \t\t\t\t\t\t\t\t#(i.e.\t-\tChace,\tCockrell)\t\t\t\t\t\t\t\t\tmatch\t=\tre.search('^([A-Z][a-z]+\\.)',\tnw)\t\t\t\t\t\t\t\t\tif\tmatch:\t\t\t\t\t\t\t\t\t\t\t\t\tnames\t=\tre.sub('\\.',\t',',\tnw)\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\tnames\t=\tnw\t\t\t\t\t\t\t\t\t\t#Append\teach\tline\tto\tCleanText\tlist\twhile\tit\tloops\tthrough\t\t\t\t\t\t\t\t\tCleanText.append(names)\t\t#Saving\tinto\ta\t'fake'\tcsv\tfile\tfcsv\t=\topen('cdocr2/50-1p1.csv',\t'w')\t#Write\teach\tline\tin\tCleanText\tto\ta\tfile\tfor\tline\tin\tCleanText:\t\t\t\t\tfcsv.write(line)\tIn conclusion, I will note that this is not for the faint of heart. Regular Expressions are powerful. Yes, they are powerful enough to completely destroy your data. So practice on copies and take it one itty bitty step at a time. About the Author Laura Turner O’Hara works in the Office of the Historian at the U.S. House of Representatives.  \n 253 28. Generating an Ordered Data Set from an OCR Text File Jon Crump – 2014    Lesson goals This tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it. These illustrations are specific to a particular text, but the overall strategy, and some of the individual procedures, can be adapted to organize any scanned text, even if it doesn't look like this one. Table of Contents Preliminaries Levenshtein Distance Function Roman Numerals Function Imports Regex Review Deploying the code snippets Iterative processing of OCR output texts Chunk up the text by pages Chunk up the text by charter Fixing folio markers Fixing the summary lines Fixing the footnotes Creating the dictionary Generate the skeleton dictionary Add marginal notation and summary lines Assign footnotes to their respective charters Parse and assign dates Completed dictionary Applications Introduction It is often the case that historians involved in digital projects wish to work with digitized texts, so they think \"OK, I'll just scan this fabulously rich and useful collection of original source material and do wonderful things with the digital text that results\". (Those of us who have done this, now  \n 254 smile ruefully). Such historians quickly discover that even the best OCR results in unacceptably high error rates. So the historian now thinks \"OK I'll get some grant money, and I'll enlist the help of an army of RAs/Grad students/Undergrads/Barely literate street urchins, to correct errors in my OCR output. (We smile again, even more sadly now). 1. There is little funding for this kind of thing. Increasingly, projects in the humanities have focused upon NLP/Data Mining/Machine Learning/Graph Analysis, and the like, frequently overlooking the fundamental problem of generating useable digital texts. The presumption has often been, well, Google scanned all that stuff didn't they? What's the matter with their scans? 2. Even if you had such an army of helpers, proof-reading the OCR output of, say, a collection of twelfth century Italian charters transcribed and published in 1935, will quickly drive them all mad, make their eyes bleed, and the result will still be a great wad of text containing a great many errors, and you will still have to do something to it before it becomes useful in any context. Going through a text file line by line and correcting OCR errors one at a time is hugely error-prone, as any proof reader will tell you. There are ways to automate some of this tedious work. A scripting language like Perl or Python can allow you to search your OCR output text for common errors and correct them using \"Regular Expressions\", a language for describing patterns in text. (So called because they express a \"regular language\".285 See L.T. O'Hara's tutorial on Regular Expressions)286 Regular Expressions, however, are only useful if the expressions you are searching for are ... well ... regular. Unfortunately, much of what you have in OCR output is highly irregular. If you could impose some order on it: create an ordered data set out of it, your Regular Expression tools would become much more powerful. Consider, for example, what happens if your OCR interpreted a lot of strings like this \"21 July, 1921\" as \"2l July, 192l\", turning the integer '1' into an 'l'. You would love to be able to write a search and replace script that would turn all instances of 2l into 21, but then what would happen if you had lots of occurrences of strings like this in your text: \"2lb. hammer\". You'd get a bunch of 21b. hammers; not what you want. If only you could tell your script: only change 2l into 21 in sections where there are dates, not weights. If you had an ordered data set, you could do things like that. Very often the texts that historians wish to digitize are, in fact, ordered data sets: ordered collections of primary source documents, or a legal code say, or a cartulary. But the editorial structure imposed upon such resources is usually designed for a particular kind of data retrieval technology i.e., a codex, a book. For a digitized text you need a different kind of structure. If                                                         285\t‘Regular\tlanguage’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Regular_language\t286\t‘Laura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t \n 255 you can get rid of the book related infrastructure and reorganize the text according to the sections and divisions that you're interested in, you will wind up with data that is much easier to do search and replace operations on, and as a bonus, your text will become immediately useful in a variety of other contexts as well. This is where a scripting language like Python comes very much in handy. For our project we wanted to prepare some of the documents from a 12th century collection of imbreviatura from the Italian scribe known as Giovanni Scriba287 so that they could be marked up by historians for subsequent NLP analysis or potentially for other purposes as well. The pages of the 1935 published edition look like this. \n\tGS page 110 The OCR output from such scans look like this even after some substantial clean-up (I've wrapped the longest lines so that they fit here): 110\tMARIO\tCHIAUDANO\tMATTIA\tMORESCO\t\t\t\t\tprofessi\tsunt\tAlvernacium\thabere\tde\ti;psa\tsocietate\tlb.\t.c.,\tin\treditu\t\t\t\t\ttracto\tpredicto\tcapitali\t.ccc.\tlb.\tproficuum.\tdebent\tdividere\tper\tmedium\t\t\t\t.\tUltra\tvero\t.cc.\tlb.\tcapitalis\tIngo\tde\tVolta\tlb.\t.xiv.\thabet\tquas\tcum\tI\t\t\t\tpso\tcapitali\tde\tscicietate\textrahere\tdebet.\tDedit\tpreterea\tprefatus\tIngo\t\t\t\tde\tVolta\tlicenciam\t(1)\t\t\t\t\tipsi\tIngoni\tNocentio\tportandi\tlb.\t.xxxvII.\t2\tOberti\tSpinule\tet\tIb.\t.xxvI\t\t\t\tI.\t                                                        287\tGiovanni\tScriba,\t‘Il\tcartolare\tdi\tGiovanni\tScriba…’,\tWorldCat:\thttp://www.worldcat.org/title/cartolare-di-giovanni-scriba/oclc/17591390\t\n \n 256 \t\t\t\tWuilielmi\tAradelli.\tActum\tante\tdomum\tW.\tBuronis\t.MCLVII.,\t.iiii.\tkalenda\t\t\t\tsiulias,\tindicione\tquarta\t(2).\t\tL\tf\to.\t26\tv.]\t.\tCCVIII.\tIngone\tDella\tVolta\tsi\tobbliga\tverso\tIngone\tNocenzio\tdi\tindennizzarlo\tdi\togni\tdanno\tche\tgli\tfosse\tderivato\tdalle\tsocieta\tche\tegli\taveva\tcon\ti\tsuoi\tfigli\t(28\tgiugno\t1157).\t\tTestes\tIngonis\tNocentii]\t.\t\t\t\t\tDie\tloco\t(3)\t,predicto\tet\ttestibus\tWuilielmo\tBurone,\tBono\tIohanne\t\t\t\t\tMalfiiastro,\tAnselmo\tde\tCafara,\tW.\tde\tRacedo,\tWuilielmo\tCallige\tPallii.\t\t\t\t\tEgo\tIngo\tde\tVolta\tpromitto\ttibi\tIngoni\tNocentio\tquod\tsi\taliquod\tdampnum\t\t\t\t\tacciderit\ttibi\tpro\tsocietate\tvel\tsocietatibus\tquam\tolim\thabueris\tcum\tfil\t\t\t\tiis\tmeis\tego\tillud\ttotum\ttibi\trestaurato\tet\thoc\ttibi\tpromitto\tsub\tpena\td\t\t\t\tupli\tde\tquanto\tinde\tdampno\thabueris.\tDo\ttibi\tpreterea\tlicentiam\taccipien\t\t\t\tdi\tbisancios\tquos\tultra\tmare\tacciipere\tdebeo\tet\tinde\tfacias\ttona\tfide\tqu\t\t\t\ticquid\ttibi\tvidebitur\tet\tinde\tab\tomni\tdanpno\tte\tabsolvo\tquicquid\tinde\tco\t\t\t\tntingerit.\t\tCCIX.\t\t\t\t\tGuglielmo\tdi\tRazedo\tdichiara\td'aver\tricevuto\tin\tsocietatem\tda\tGuglielmo\t\t\t\t\tBarone\tuna\tsomma\tdi\tdenaro\tche\tportera\tlaboratum\tultramare\t(28\tgiugno\t11\t\t\t\t57).\tWuilielmi\tBuronis]\t.\t\t\t\t\t\t\t\t\tTestes\tAnselmus\tde\tCafara,\tAlbertus\tde\tVolta,\tW.\tCapdorgol,\tCorsus\tSerre,\tAngelotus,\tIngo\tNoncencius.\tEgo\tW.\tde\tRaeedo\tprofiteor\tme\taccepisse\tate\tWuilielmo\tBurone\tlb.\tduocentum\tsexaginta\ttre\tet\ts.\t.XIII.\t1/2\tin\tsocietatem\tad\tquartam\tproficui,\teas\tdebeo\tportare\tlaboratum\tultra\tmare\tet\tinde\tquo\tvoluero,\tin\treditu,\t(11\tLicentiam\tin\tsopralinea\tin\tpotestatem\tcancellato.\t(2)\tA\tmargine\tle\tpostille:\tPro\tIngone\tNocentio\tscripta\te\tdue\tpro\tAlvernacio.\t(3)\tCancellato:\tet\ttestibus\tsupradictis.\tIn the scan of the original, the reader's eye readily parses the page: the layout has meaning. But as you can see, reduced to plain text like this, none of the metadata implied by the page layout and typography can be differentiated by automated processes. You can see from the scan that each charter has the following metadata associated with it. Charter number Page number Folio number An Italian summary, ending in a date of some kind A line, usually ending with a ']' that marks a marginal notation in the original Frequently a collection of in-text numbered footnote markers, whose text appears at the bottom of each page, sequentially numbered, and restarting from 1 on each new page. The Latin text of the charter itself  \n 257 This is typical of such resources, though editorial conventions will vary widely. The point is: this is an ordered data set, not just a great big string of characters. With some fairly straightforward Python scripts, we can turn our OCR output into an ordered data set, in this case, a python dictionary,288 before we start trying to proofread the Latin charter texts. With such an ordered data set in hand, we can do proofreading, and potentially many other kinds of tasks, much more effectively. So, the aim of this tutorial is to take a plain text file, like the OCR output above and turn it into a python dictionary with fields for the Latin text of the charter and for each of the metadata elements mentioned above: {\t.\t.\t.\t\t\t52:\t{'chid':\t'GScriba_LII',\t\t\t\t\t\t\t'chno':\t52,\t\t\t\t\t\t\t'date':\tdatetime.date(1156,\t3,\t27),\t\t\t\t\t\t\t'folio':\t'[fo.\t6\tr.]',\t\t\t\t\t\t\t'footnotes':\t[(1,\t'Cancellato:\tm.')],\t\t\t\t\t\t\t'marginal':\t'no\tmarginal]',\t\t\t\t\t\t\t'pgno':\t29,\t\t\t\t\t\t\t'summary':\t'I\tconsoli\tdi\tGenova\triconoscono\tcon\tsentenza\til\tdiritto\tdi\tRomano\tdi\tCasella\tdi\tpagarsi\tsui\tbeni\tdi\tGerardo\tConfector\tper\tun\tcredito\tche\taveva\tverso\til\tmedesimo\t(27\tmarzo\t1156).',\t\t\t\t\t\t\t'text':\t['\t\t\t\tIn\tpontili\tcapituli\tconsules\tE.\tAurie,\tW.\tBuronus,\tOgerius\tVentus\tlaudaverunt\tquod\tRomanus\tde\tCasella\thaberet\tin\tbonis\tGerardi\tConfectoris\ts.\t.xxvi.\tdenariorum\tet\tpossit\teos\taccipere\tsine\tcontradicione\teius\tet\tomnium\tpro\teo.\tHoc\tideo\tquia,\tcum;\tRomanus\tante\tipsos\tinde\tconquereretur,\tipso\tGerardo\tdebitum\tnon\tnegante,\tsed\tquod\tde\tusura\tesset\tobiiciendo,\tiuravit\tnominatus\tRomanus\tquod\tcapitalis\terat\t(1)\tet\tnon\tde\tusura,\tunde\tut\tsupra\tlaudaverunt\t,\t.MCLVI.,\tsexto\tkalendas\taprilis,\tindicione\ttercia.\\n']},\t\t53:\t{'chid':\t'GScriba_LIII',\t\t\t\t\t\t\t'chno':\t53,\t\t\t\t\t\t\t'date':\tdatetime.date(1156,\t3,\t27),\t\t\t\t\t\t\t'folio':\t'[fo.\t6\tr.]',\t\t\t\t\t\t\t'footnotes':\t[],\t\t\t\t\t\t\t'marginal':\t'Belmusti]',\t\t\t\t\t\t\t'pgno':\t29,\t\t\t\t\t\t\t'summary':\t\"Maestro\tArnaldo\te\tGiordan\tnipote\tdel\tfu\tGiovanni\tdi\tPiacenza\tsi\tobbligano\tdi\tpagare\tuna\tsomma\tnell'ottava\tdella\tprossima\tPasqua,\tper\tmerce\tricevuta\t(27\tmarzo\t1156).\",\t\t\t\t\t\t\t'text':\t['\t\tTestes\tConradus\tPorcellus,\tAlbericus,\tVassallus\tGambalixa,\tPetrus\tArtodi.\tNos\tArnaldus\tmagister\tet\tIordan\tnepos\tquondam\tIohannis\tPlacentie\taccepimus\ta\tte\tBelmusto\ttantum\tbracile\tunde\tpromittimus\tdare\ttibi\tvel\ttuo\tcerto\tmisso\tlb.\t.xLIII.\tdenariorum\tusque\toctavam\tproximi\tpasce,\tquod\tsi\tnon\tfecerimus\tpenam\tdupli\ttibi\tstipulanti\tpromittimus,\tbona\tpignori,\tpossis\tunumquemque\tconvenire\tde\ttoto.\tActum\tprope\tcampanile\tSancti\tLaurentii,\tmillesimo\tcentesimo\t.Lv.,\tsexto\tkalendas\taprilis,\tindictione\ttercia.\\n']},\t.\t.\t.\tetc.\t}\t                                                        288\t‘Dictionaries’,\tPython:\thttps://docs.python.org/2/tutorial/datastructures.html#dictionaries\t \n 258 Remember, this is just a text representation of a data structure that lives in computer memory. Python calls this sort of structure a 'dictionary', other programming languages may call it a 'hash', or an 'associative array'. The point is that it is infinitely easier to do any sort of programmatic analysis or manipulation of a digital text if it is in such a form, rather than in the form of a plain text file. The advantage is that such a data structure can be queried, or calculations can be performed on the data, without first having to parse the text.  A couple of useful functions before we start: We're going to borrow a couple of functions written by others. They both represent some pretty sophisticated programming. Understanding what's going on in these functions is instructive, but not necessary. Reading and using other people's code is how you learn programming, and is the soul of the Open-Source movement. Even if you don't fully understand how somebody does it, you can nevertheless test functions like this to see that they reliably do what they say they can, and then just apply it to your immediate problem if they are relevant.  Levenshtein distance You will note that some of the metadata listed above is page-bound and some of it is charter-bound. Getting these untangled from each other is our aim. There is a class of page-bound data that is useless for our purposes, and only meaningful in the context of a physical book: page headers and footers. In our text, these look like this on recto leaves (in a codex, a book, recto is the right-side page, and verso its reverse, the left-side page) \trecto header and this on verso leaves: \tverso header We'd like to preserve the page number information for each charter on the page, but the header text isn't useful to us and will just make any search and replace operation more difficult. So we'd like to find header text and replace it with a string that's easy to find with a Regular Expression, and store the page number. Unfortunately, regular expressions won't help you much here. This text can appear on any line of our OCR output text, and the ways in which OCR software can foul it up are effectively limitless. Here are some examples of page headers, both recto and verso in our raw OCR output. \n \n 259 260\t11141110\tCH[AUDANO\tMATTIA\tMORESCO\tIL\tCIRTOL4RE\tDI\tCIOVINN1\tSt'Itlltl\t\t269\tIL\tCJIRTOL.%RE\tDI\tG:OVeNNl\tFIM\tP%\t\t\t297\tIL\tCIP.TQLIRE\tDI\tG'OVeNNI\tSCI\tDt\t\t\t\tr.23\t332\tT1uu:0\tCHIAUDANO\tM:11TIA\tMGRESCO\tIL\tCIRTOL.'RE\tDI\tG:OV.I\\N(\tsca:FR\t\t\t339\t342\tNI\t.\\ßlO\tCHIAUDANO\t9LtTTIA\tMORESCO\tThese strings are not regular enough to reliably find with regular expressions; however, if you know what the strings are supposed to look like, you can compose some kind of string similarity algorithm to test each string against an exemplar and measure the likelihood that it is a page header. Fortunately, I didn't have to compose such an algorithm, Vladimir Levenshtein did it for us in 1965 (see: http://en.wikipedia.org/wiki/Levenshtein_distance). A computer language can encode this algorithm in any number of ways; here's an effective Python function that will work for us: def\tlev(seq1,\tseq2):\t\t\t\t\t\"\"\"\tReturn\tLevenshtein\tdistance\tmetric\t\t\t\t\t(ripped\tfrom\thttp://pydoc.net/Python/Whoosh/2.3.2/whoosh.support.levenshtein/)\t\t\t\t\t\t\"\"\"\t\t\t\t\toneago\t=\tNone\t\t\t\t\tthisrow\t=\trange(1,\tlen(seq2)\t+\t1)\t+\t[0]\t\t\t\t\tfor\tx\tin\txrange(len(seq1)):\t\t\t\t\t\t\t\t\ttwoago,\toneago,\tthisrow\t=\toneago,\tthisrow,\t[0]\t*\tlen(seq2)\t+\t[x\t+\t1]\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ty\tin\txrange(len(seq2)):\t\t\t\t\t\t\t\t\t\t\t\t\tdelcost\t=\toneago[y]\t+\t1\t\t\t\t\t\t\t\t\t\t\t\t\taddcost\t=\tthisrow[y\t-\t1]\t+\t1\t\t\t\t\t\t\t\t\t\t\t\t\tsubcost\t=\toneago[y\t-\t1]\t+\t(seq1[x]\t!=\tseq2[y])\t\t\t\t\t\t\t\t\t\t\t\t\tthisrow[y]\t=\tmin(delcost,\taddcost,\tsubcost)\t\t\t\t\t\t\t\t\t\t\t\t\t#\tThis\tblock\tdeals\twith\ttranspositions\t\t\t\t\t\t\t\t\t\t\t\t\tif\t(x\t>\t0\tand\ty\t>\t0\tand\tseq1[x]\t==\tseq2[y\t-\t1]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tand\tseq1[x-1]\t==\tseq2[y]\tand\tseq1[x]\t!=\tseq2[y]):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tthisrow[y]\t=\tmin(thisrow[y],\ttwoago[y\t-\t2]\t+\t1)\t\t\t\t\treturn\tthisrow[len(seq2)\t-\t1]\tAgain, this is some pretty sophisticated programming, but for our purposes all we need to know is that the lev() function takes two strings as parameters and returns a number that indicates the 'string distance' between them, or, how many changes had to be made to turn the first string into the second. So: lev(\"fizz\",\t\"buzz\") returns '2'  Roman to Arabic numerals You'll also note that in the published edition, the charters are numbered with roman numerals. Converting roman numerals into arabic is an instructive puzzle to work out in Python. Here's the cleanest and most elegant solution I know:    \n 260 def\trom2ar(rom):\t\t\t\t\t\"\"\"\tFrom\tthe\tPython\ttutor\tmailing\tlist:\t\t\t\t\tJános\tJuhász\tjanos.juhasz\tat\tVELUX.com\t\t\t\t\treturns\tarabic\tequivalent\tof\ta\tRoman\tnumeral\t\"\"\"\t\t\t\t\troman_codec\t=\t{'M':1000,\t'D':500,\t'C':100,\t'L':50,\t'X':10,\t'V':5,\t'I':1}\t\t\t\t\troman\t=\trom.upper()\t\t\t\t\troman\t=\tlist(roman)\t\t\t\t\troman.reverse()\t\t\t\t\tdecimal\t=\t[roman_codec[ch]\tfor\tch\tin\troman]\t\t\t\t\tresult\t=\t0\t\t\t\t\t\twhile\tlen(decimal):\t\t\t\t\t\t\t\t\tact\t=\tdecimal.pop()\t\t\t\t\t\t\t\t\tif\tlen(decimal)\tand\tact\t<\tmax(decimal):\t\t\t\t\t\t\t\t\t\t\t\t\tact\t=\t-act\t\t\t\t\t\t\t\t\tresult\t+=\tact\t\t\t\t\t\treturn\tresult\t(run <this little script>289 to see in detail how rome2ar works. Elegant programming like this can offer insight; like poetry.)  Some other things we'll need: At the top of your Python module, you're going to want to import some python modules that are a part of the standard library. (see Fred Gibbs's tutorial ‘Installing Python Modules with pip’).290 1. First among these is the \"re\" (regular expression) module import\tre. Regular expressions are your friends. However, bear in mind Jamie Zawinski's quip: Some\tpeople,\twhen\tconfronted\twith\ta\tproblem,\tthink\t\"I\tknow,\tI'll\tuse\tregular\texpressions.\"\tNow\tthey\thave\ttwo\tproblems.\t(Again, have a look at L.T. O'Hara's introduction here at the Programming Historian ‘Cleaning OCR’d text with Regular Expressions’)291 2. Also: from\tpprint\timport\tpprint. pprint is just a pretty-printer for python objects like lists and dictionaries. You'll want it because python dictionaries are much easier to read if they are formatted. 3. And: from\tcollections\timport\tCounter. We'll want this for the Find and normalize footnote markers and texts section below. This is not really necessary, but we'll do some counting that would require a lot of lines of fiddly code and this will save us the trouble. The collections module has lots of deep magic in it and is well worth getting familiar with. (Again, see Doug Hellmann's PyMOTW for the ‘collections’                                                         289\tAvailable\tat:\thttp://programminghistorian.org/assets/Roman_to_Arabic.txt\t290\tFred\tGibbs,\t‘Installing\tPython\tModules\twith\tpip’,\tThe\tProgramming\tHistorian,\t(2013).\t291\tLaura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t \n 261 module.292 I should also point out that his book The Python Standard Library By Example293 is one well worth having.)  A very brief review of regular expressions as they are implemented in python L.T. O'Hara's introduction to using python flavored regular expressions is invaluable.294 In this context we should review a couple of basic facts about Python's implementation of regular expressions, the re module, which is part of Python's standard library. re.compile() creates a regular expression object that has a number of methods. You should be familiar with .match(), and .search(), but also .findall() and .finditer() Bear in mind the difference between .match() and .search(): .match() will only match at the beginning of a line, whereas .search() will match anywhere in the line but then it stops, it'll only return the first match it finds. .match() and .search() return match objects. To retrieve the matched string you need mymatch.group(0). If your compiled regular expression has grouping parentheses in it (like our 'slug' regex below), you can retrieve those substrings of the matched string using mymatch.group(1) etc. .findall() and .finditer() will return all occurrences of the matched string; .findall() returns them as a list of strings, but .finditer() returns an iterator of match objects. (read the docs on the method .finditer()).295  Iterative processing of text files We'll start with a single file of OCR output. We will iteratively generate new, corrected versions of this file by using it as input for our python scripts. Sometimes our script will make corrections automatically, more often, our scripts will simply alert us to where problems lie in the input file, and we will make corrections manually. So, for the first several operations we're going to want to produce new and revised text files to use as input for our subsequent operations. Every time you produce a text file, you should version it and duplicate it so that you can always return to it. The next time you run your code (as you're developing it) you might alter the file in an unhelpful way and it's easiest just to restore the old version.                                                         292\t‘Collections\t–\tContainer\tdata\ttypes’:\thttps://pymotw.com/2/collections/index.html#module-collections\t293\tDoug\tHellmann,\tThe\tPython\tStandard\tLibrary\tby\tExample:\thttps://doughellmann.com/blog/the-python-standard-library-by-example/\t294\tLaura\tTurner\tO’Hara,\t‘Cleaning\tOCR’d\ttext\twith\tRegular\tExpressions’,\tThe\tProgramming\tHistorian\t(2013).\t295\t‘re.finditer’,\tPython:\thttps://docs.python.org/2/library/re.html#re.finditer\t \n 262 The code in this tutorial is highly edited; it is not comprehensive. As you continue to refine your input files, you will write lots of little ad hoc scripts to check on the efficacy of what you've done so far. Versioning will ensure that such experimentation will not destroy any progress that you've made.  A note on how to deploy the code in this tutorial: The code in this tutorial is for Python 2.7.x, Python 3 is quite a different animal. When you write code in a text file and then execute it, either at the command line, or from within your text editor or IDE, the Python interpreter executes the code line by line, from top to bottom. So, often the code on the bottom of the page will depend on code above it. One way to use the code snippets in section 2 might be to have all of them in a single file and comment out the bits that you don't want to run. Each time you execute the file, you will want to be sure that there is a logical control flow from the #! line at the top, through your various imports and assignment of global variables, and each loop, or block. Or, each of the subsections in section 2 can also be treated as a separate script, each would then have to do its own importing and assignment of global variables. In section 3, \"Creating the Dictionary\", you will be operating on a data set in computer memory (the charters dictionary) that will be generated from the latest, most correct, input text you have. So you will want to maintain a single python module in which you define the dictionary at the top, along with your import statements and the assignment of global variables, followed by each of the four loops that will populate and then modify that dictionary. #!/usr/bin/python\t\timport\tre\tfrom\tpprint\timport\tpprint\tfrom\tcollections\timport\tCounter\t\t#\tfollowed\tby\tany\tglobal\tvariables\tyou\twill\tneed,\tlike:\t\tn\t=\t0\tthis_folio\t\t=\t'[fo.\t1\tr.]'\tthis_page\t=\t1\t\t#\tcompiled\tregular\texpressions\tlike:\tslug\t=\tre.compile(\"(\\[~~~~\\sGScriba_)(.*)\\s::::\\s(\\d+)\\s~~~~\\]\")\tfol\t=\tre.compile(\"\\[fo\\.\\s?\\d+\\s?[rv]\\.\\s?\\]\")\tpgbrk\t=\tre.compile(\"~~~~~\tPAGE\t(\\d+)\t~~~~~\")\t\t#\tthe\tcanonical\tfile\tyou\twill\tbe\treading\tfrom\tfin\t=\topen(\"/path/to/your/current/canonical.txt\",\t'r')\t \n 263 GScriba\t=\tfin.readlines()\t\t\t#\tthen\tthe\tempty\tdictionary:\tcharters\t=\tdict()\t\t#\tfollowed\tby\tthe\t4\t'for'\tloops\tin\tsection\t2\tthat\twill\tpopulate\tand\tthen\tmodify\tthis\tdictionary\t Chunk up the text by pages First of all, we want to find all the page headers, both recto and verso and replace them with consistent strings that we can easily find with a regular expression. The following code looks for lines that are similar to what we know are our page headers to within a certain threshold. It will take some experimentation to find what this threshold is for your text. Since my recto and verso headers are roughly the same length, both have the same similarity score of 26. NOTA\tBENE:\tThe\tlev()\tfunction\tdescribed\tabove\treturns\ta\tmeasure\tof\tthe\t'distance'\tbetween\ttwo\tstrings,\tso,\tthe\tshorter\tthe\tpage\theader\tstring,\tthe\tmore\tlikely\tit\tis\tthat\tthis\ttrick\twill\tnot\twork.\tIf\tyour\tpage\theader\tis\tjust\t\"Header\",\tthen\tany\tline\tcomprised\tof\ta\tsix\tletter\tword\tmight\tgive\tyou\ta\tstring\tdistance\tof\t6,\teg:\tlev(\"Header\", \"Foobar\")\treturns\t'6',\tleaving\tyou\tnone\tthe\twiser.\tIn\tour\ttext,\thowever,\tthe\theader\tstrings\tare\tlong\tand\tcomplex\tenough\tto\tgive\tyou\tmeaningful\tscores,\teg:\tlev(\"RANDOM\tSTRING\tOF\tSIMILAR\tLENGTH:\t\t\t\t38\",\t'IL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA') returns 33, but one of our header strings, even badly mangled by the OCR, returns 20: lev(\"IL\tCIRTOL4RE\tDI\tCIOVINN1\tSt'Itlltl\t\t\t\t\t269\",\t'IL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA') So we can use lev() to find and modify our header strings thus: #\tAt\tthe\ttop,\tdo\tthe\timporting\tyou\tneed\tand\tdefine\tthe\tlev()\tfunction\tas\tdescribed\tabove,\tand\tthen:\t\tfin\t=\topen(\"our_base_OCR_result.txt\",\t'r')\t#\tread\tour\tOCR\toutput\ttext\tfout\t=\topen(\"out1.txt\",\t'w')\t#\tcreate\ta\tnew\ttextfile\tto\twrite\tto\twhen\twe're\tready\tGScriba\t=\tfin.readlines()\t#\tturn\tour\tinput\tfile\tinto\ta\tlist\tof\tlines\t\tfor\tline\tin\tGScriba:\t\t\t\t\t#\tget\ta\tLevenshtein\tdistance\tscore\tfor\teach\tline\tin\tthe\ttext\t\t\t\t\trecto_lev_score\t=\tlev(line,\t'IL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA')\t\t\t\t\tverso_lev_score\t=\tlev(line,\t'MARIO\tCHIAUDANO\t-\tMATTIA\tMORESCO')\t\t\t\t\t\t\t\t\t\t#\tyou\twant\tto\tuse\ta\tscore\tthat's\tas\thigh\tas\tpossible,\t\t\t\t\t#\tbut\tstill\tfinds\tonly\tpotential\tpage\theader\ttexts.\t\t\t\t\tif\trecto_lev_score\t<\t26\t:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tIf\twe\tincrement\ta\tvariable\t'n'\tto\t\t\t#count\tthe\tnumber\tof\theaders\twe've\tfound,\t\t\t\t\t\t\t\t\t#\tthen\tthe\tvalue\tof\tthat\tvariable\tshould\tbe\tour\tpage\tnumber.\t\t\t\t\t\t\t\t\tn\t+=\t1\t \n 264 \t\t\t\t\t\t\t\tprint\t\"recto:\t%s\t%s\"\t%\t(recto_lev_score,\tline)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tOnce\twe've\tfigured\tout\tour\toptimal\t'lev'\tscore,\twe\tcan\t'uncomment'\t\t\t\t\t\t\t\t\t#\tall\tthese\t`fout.write()`\tlines\tto\twrite\tout\tour\tnew\ttext\tfile,\t\t\t\t\t\t\t\t\t#\treplacing\teach\theader\twith\tan\teasy-to-find\tstring\tthat\tcontains\t\t\t\t\t\t\t\t\t#\tthe\tpage\tnumber:\tour\tvariable\t'n'.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#fout.write(\"~~~~~\tPAGE\t%d\t~~~~~\\n\\n\"\t%\tn)\t\t\t\t\telif\tverso_lev_score\t<\t26\t:\t\t\t\t\t\t\t\t\tn\t+=\t1\t\t\t\t\t\t\t\t\tprint\t\"verso:\t%s\t%s\"\t%\t(verso_lev_score,\tline)\t\t\t\t\t\t\t\t\t#fout.write(\"~~~~~\tPAGE\t%d\t~~~~~\\n\\n\"\t%\tn)\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t#fout.write(line)\t\t\t\t\t\t\t\t\tpass\t\tprint\tn\tThere's a lot of calculation going on in the lev() function. It isn't very efficient to call it on every line in our text, so this might take some time, depending on how long our text is. We've only got 803 charters in vol. 1. That's a pretty small number. If it takes 30 seconds, or even a minute, to run our script, so be it. If we run this script on our OCR output text, we get output that looks like this: .\t.\t.\tverso:\t8\t426\t\t\t\tMARIO\tCHIAUDANO\tMAITIA\tMORESCO\trecto:\t5\tIL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA\t\t\t\t427\tverso:\t11\t,\t,\t\t\t428\tMARIO\tCHIAUDANO\tMATTIA\tMORESCO\trecto:\t5\tIL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA\t\t\t\t499\tverso:\t7\t430\t\t\t\tMARIO\tCHIAUDANO\tMATTIA\tMORESCO\trecto:\t5\tIL\tCARTOLARE\tDI\tGIOVANNI\tSCRIBA\t\t\t\t431\tverso:\t8\t432\t\t\t\tMARIO\tCHIAUDASO\tMATTIA\tMORESCO\t430\tFor each line, the output tells us that it's page verso or recto, the Levenshtein \"score\", and then the text of the line (complete with all the errors in it. Note that the OCR misread the pg. number for pg. 429). The lower the Levenshtein \"score\", the closer the line is to the model you've given it. This tells you that the script found 430 lines that are probably page headers. You know how many pages there should be, so if the script didn't find all the headers, you can go through the output looking at the page numbers, find the pages it missed, and fix the headers manually, then repeat until the script finds all the page headers. Once you've found and fixed the headers that the script didn't find, you can then write out the corrected text to a new file that will serve as the basis for the other operations below. So, instead of  \n 265 quicquid\tvolueris\tsine\tomni\tmea\tet\t(1)\tSpazio\tbianco\tnel\tms.\t\t12\t\tMARIO\tCSIAUDANO\tMATTIA\tMORESCO\theredum\tmeorum\tcontradicione.\tActum\tin\tcapitulo\t.MCLV.,\tmensis\tiulii,\tindicione\tsecunda.\twe'll have a textfile like this: quicquid\tvolueris\tsine\tomni\tmea\tet\t(1)\tSpazio\tbianco\tnel\tms.\t\t~~~~~\tPAGE\t12\t~~~~~\t\theredum\tmeorum\tcontradicione.\tActum\tin\tcapitulo\t.MCLV.,\tmensis\tiulii,\tindicione\tsecunda.\tNote that for many of the following operations, we will use GScriba\t=\tfin.readlines() so GScriba will be a python list of the lines in our input text. Keep this firmly in mind, as the for loops that we will use will depend on the fact that we will iterate through the lines of our text In Document Order.  Chunk up the text by charter (or sections, or letters, or what-have-you) The most important functional divisions in our text are signaled by upper case roman numerals on a separate line for each of the charters. So we need a regex to find roman numerals like that. Here's one: romstr\t=\tre.compile(\"\\s*[IVXLCDM]{2,}\"). We'll put it at the top of our module file as a 'global' variable so it will be available to any of the bits of code that come later. The script below will look for capital roman numerals that appear on a line by themselves. Many of our charter numbers will fail that test and the script will report there's\ta\tcharter\troman\tnumeral\tmissing?, often because there's something before or after it on the line; or, KeyError, often because the OCR has garbled the characters (e.g. CCG for 300, XOII for 492). Run this script repeatedly, correcting out1.txt as you do until all the charters are accounted for. #\tAt\tthe\ttop,\tdo\tthe\timporting\tyou\tneed,\tthen\tdefine\trom2ar()\t\t#as\tdescribed\tabove,\tand\tthen:\t\tn\t=\t0\tromstr\t=\tre.compile(\"\\s*[IVXLCDM]{2,}\")\t\tfin\t=\topen(\"out1.txt\",\t'r')\tfout\t=\topen(\"out2.txt\",\t'w')\tGScriba\t=\tfin.readlines()\t\tfor\tline\tin\tGScriba:\t\t\t\t\tif\tromstr.match(line):\t\t\t\t\t\t\t\t\trnum\t=\tline.strip().strip('.')\t\t\t\t\t\t\t\t\t#\teach\ttime\twe\tfind\ta\troman\tnumeral\tby\titself\t\t \n 266 \t\t#on\ta\tline\twe\tincrement\tn:\t\t\t\t\t\t\t\t\t#\tthat's\tour\tcharter\tnumber.\t\t\t\t\t\t\t\t\tn\t+=\t1\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\t\t\t\t\t#\ttranslate\tthe\troman\tto\tthe\tarabic\tand\tit\tshould\tbe\tequal\tto\tn.\t\t\t\t\t\t\t\t\t\t\t\t\tif\tn\t!=\trom2ar(rnum):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tif\tit's\tnot,\tthen\talert\tus\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"%d,\tthere's\ta\tcharter\troman\tnumeral\tmissing?,\tbecause\tline\tnumber\t%d\treads:\t%s\"\t%\t(n,\tGScriba.index(line),\tline)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthen\tset\t'n'\tto\tthe\tright\tnumber\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tn\t=\trom2ar(rnum)\t\t\t\t\t\t\t\t\texcept\tKeyError:\t\t\t\t\t\t\t\t\t\t\t\t\tprint\tn,\t\"KeyError,\tline\tnumber\t\",\tGScriba.index(line),\t\"\treads:\t\",\tline\tSince we know how many charters there should be. At the end of our loop, the value of n should be the same as the number of charters. And, in any iteration of the loop, if the value of n does not correspond to the next successive charter number, then we know we've got a problem somewhere, and the print statements should help us find it. Here's a sample of the output our script will give us: 23\tthere's\ta\tcharter\troman\tnumeral\tmissing?,\tbecause\tline\tnumber\t\t156\t\treads:\t\tXXIV.\t25\tthere's\ta\tcharter\troman\tnumeral\tmissing?,\tbecause\tline\tnumber\t\t186\t\treads:\t\tXXVIII.\t36\tKeyError,\tline\tnumber\t\t235\t\treads:\t\tXXXV1.\t37\tKeyError,\tline\tnumber\t\t239\t\treads:\t\tXXXV\tII.\t38\tthere's\ta\tcharter\troman\tnumeral\tmissing?,\tbecause\tline\tnumber\t\t252\t\treads:\t\tXL.\t\t41\tthere's\ta\tcharter\troman\tnumeral\tmissing?,\tbecause\tline\tnumber\t\t262\t\treads:\t\tXLII.\t43\tKeyError,\tline\tnumber\t\t265\t\treads:\t\tXL:III.\tNOTA BENE: Our regex will report an error for the single digit Roman numerals ('I','V','X' etc.). You could test for these in the code, but sometimes leaving a known and regular error is a help to check on the efficacy of what you're doing. Our aim is to satisfy ourselves that any inconsistencies on the charter number line are understood and accounted for. Once we've found, and fixed, all the roman numeral charter headings, then we can write out a new file with an easy-to-find-by-regex string, a 'slug,' for each charter in place of the bare roman numeral. Comment out the for loop above, and replace it with this one: for\tline\tin\tGScriba:\t\t\t\t\tif\tromstr.match(line):\t\t\t\t\t\t\t\t\trnum\t=\tline.strip().strip('.')\t\t\t\t\t\t\t\t\tnum\t=\trom2ar(rnum)\t\t\t\t\t\t\t\t\tfout.write(\"[~~~~\tGScriba_%s\t::::\t%d\t~~~~]\\n\"\t%\t(rnum,\tnum))\t\t\t\t\telse:\t\t\t\t\t\t\t\t\tfout.write(line)\tWhile it's important in itself for us to have our OCR output reliably divided up by page and by charter, the most important thing about these initial operations is that you know how many pages there are, and how many charters there are, and you can use that knowledge to check on subsequent operations. If you want to do something to every charter, you can reliably  \n 267 test whether or not it worked because you can count the number of charters that it worked on.  Find and normalize folio markers Our OCR'd text is from the 1935 published edition of Giovanni Scriba. This is a transcription of a manuscript cartulary which was in the form of a bound book. The published edition preserves the pagination of that original by noting where the original pages change: [fo. 16 r.] the face side of the 16th leaf in the book, followed by its reverse [fo. 16 v.]. This is metadata that we want to preserve for each of the charters so that they can be referenced with respect to the original, as well as with respect to the published edition by page number. Many of the folio markers (e.g. \"[fo. 16 v.]\") appear on the same line as the roman numeral for the charter heading. To normalize those charter headings for the operation above, we had to put a line break between the folio marker and the charter number, so many of the folio markers are on their own line already. However, sometimes the folio changes in the middle of the charter text somewhere. We want these markers to stay where they are; we will have to treat those two cases differently. For either case, we need to make sure all the folio markers are free of errors so that we can reliably find them by means of a regular expression. Again, since we know how many folios there are, we can know if we've found them all. Note that because we used .readlines(), GScriba is a list, so the script below will print the line number from the source file as well as the line itself. This will report all the correctly formated folio markers, so that you can find and fix the ones that are broken. #\tnote\tthe\toptional\tquantifiers\t'\\s?'.\tWe\twant\tto\tfind\tas\tmany\tas\twe\tcan,\tand\t#\tthe\tOCR\tis\terratic\tabout\twhitespace,\tso\tour\tregex\tis\tpermissive.\tBut\tas\t#\tyou\tfind\tand\tcorrect\tthese\tstrings,\tyou\twill\twant\tto\tmake\tthem\tconsistent.\tfol\t=\tre.compile(\"\\[fo\\.\\s?\\d+\\s?[rv]\\.\\s?\\]\")\t\tfor\tline\tin\tGScriba:\t\t\t\t\tif\tfol.match(line):\t\t\t\t\t\t\t\t\t#\tsince\tGScriba\tis\ta\tlist,\twe\tcan\tget\tthe\tindex\tof\tany\tof\tits\tmembers\tto\tfind\tthe\tline\tnumber\tin\tour\tinput\tfile.\t\t\t\t\t\t\t\t\tprint\tGScriba.index(line),\tline\tWe would also like to ensure that no line has more than one folio marker. We can test that like this: for\tline\tin\tGScriba:\t\t\t\t\tall\t=\tfol.findall(line)\t\t\t\t\tif\tlen(all)\t>\t1:\t\t\t\t\t\t\t\t\tprint\tGScriba.index(line),\tline\tAgain, as before, once you've found and corrected all the folio markers in your input file, save it with a new name and use it as the input to the next section.  \n 268  Find and normalize the Italian summary lines. This important line is invariably the first one after the charter heading. \titalian summary line Since those roman numeral headings are now reliably findable with our 'slug' regex, we can now isolate the line that appears immediately after it. We also know that the summaries always end with some kind of parenthesized date expression. So, we can compose a regular expression to find the slug and the line following: slug_and_firstline\t=\tre.compile(\"(\\[~~~~\\sGScriba_)(.*)\\s::::\\s(\\d+)\\s~~~~\\]\\n(.*)(\\(\\d?.*\\d+\\))\")\tLet's break down that regex using the verbose mode (again, see O'Hara's tutorial in this book). Our 'slug' for each charter takes the form \"[~~~~ GScriba_CCVII :::: 207 ~~]\" for example. The compiled pattern above is exactly equivalent to the folowing (note the re.VERBOSE switch at the end): slug_and_firstline\t=\tre.compile(r\"\"\"\t\t\t\t\t(\\[~~~~\\sGScriba_)\t\t#\tmatches\tthe\t\"[~~~~\tGScriba_\"\tbit\t\t\t\t\t(.*)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmatches\tthe\tcharter's\troman\tnumeral\t\t\t\t\t\\s::::\\s\t\t\t\t\t\t\t\t\t\t\t\t#\tmatches\tthe\t\"\t::::\t\"\tbit\t\t\t\t\t(\\d+)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmatches\tthe\tarabic\tcharter\tnumber\t\t\t\t\t\\s~~~~\\]\\n\t\t\t\t\t\t\t\t\t\t#\tmatches\tthe\tlast\t\"\t~~~~\t\"\tbit\tand\tthe\tline\tending\t\t\t\t\t(.*)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tmatches\tall\tof\tthe\tnext\tline\tup\tto:\t\t\t\t\t(\\(\\d?.*\\d+\\))\t\t\t\t\t\t#\tthe\tparanthetical\texpression\tat\tthe\tend\t\t\t\t\t\"\"\",\tre.VERBOSE)\tthe parentheses mark match groups, so each time our regex finds a match, we can refer in our code to specific bits of the match it found: match.group(0) is the whole match, both our slug and the line that follows it. match.group(1) = \"[~~ GScriba_\" match.group(2) = the charter's roman numeral match.group(3) = the arabic charter number match.group(4) = the whole of the Italian summary line up to the parenthesized date expression match.group(5) = the parenthesized date expression. Note the escaped parentheses. Because our OCR has a lot of mysterious whitespace (OCR software is not good at parsing whitespace and you're likely to get newlines, tabs, spaces, all mixed up without rhyme or reason), we want to hunt for this regex as substrings of a great big string, so this time we're going to use .read() \n \n 269 instead of .readlines(). And we'll also need a counter to keep track of the lines we find. This script will report the charter numbers where the first line does not conform to our regex model. This will usually happen if there's no line break after our charter header, or if the Italian summary line has been broken up into multiple lines. num_firstlines\t=\t0\tfin\t=\topen(\"your_current_source_file.txt\",\t'r')\t\t#\tNB:\tGScriba\tis\tnot\ta\tlist\tof\tlines\tthis\ttime,\tbut\ta\tsingle\tbig\tstring.\tGScriba\t=\tfin.read()\t\t#\tfinditer()\tcreates\tan\titerator\t'i'\tthat\twe\tcan\tdo\ta\t'for'\tloop\tover.\ti\t=\tslug_and_firstline.finditer(GScriba)\t\t#\teach\telement\t'x'\tin\tthat\titerator\tis\ta\tregex\tmatch\tobject.\tfor\tx\tin\ti:\t\t\t\t\t#\tcount\tthe\tsummary\tlines\twe\tfind.\tRemember,\twe\tknow\thow\tmany\t\t\t\t\t\t#\tthere\tshould\tbe,\tbecause\twe\tknow\thow\tmany\tcharters\tthere\tare.\t\t\t\t\tnum_firstlines\t+=\t1\t\t\t\t\t\t\t\t\t\tchno\t=\tint(x.group(3))\t#\tour\tcharter\tnumber\tis\ta\tstring,\twe\tneed\tan\tinteger\t\t\t\t\t\t\t\t\t\t#\tchno\tshould\tequal\tn\t+\t1,\tif\tit\tdoesn't,\treport\tto\tus\t\t\t\t\tif\tchno\t!=\tn\t+\t1:\t\t\t\t\t\t\t\t\tprint\t\"problem\tin\tcharter:\t%d\"\t%\t(n\t+\t1)\t\t#NB:\tthis\twill\tmiss\tconsecutive\tproblems.\t\t\t\t\t\t#\tthen\tset\tn\tto\tthe\tright\tcharter\tnumber\t\t\t\t\tn\t=\tchno\t\t#\tprint\tout\tthe\tnumber\tof\tsummary\tlines\twe\tfound\tprint\t\"number\tof\titalian\tsummaries:\t\",\tnum_firstlines\tAgain, run the script repeatedly until all the Italian Summary lines are present and correct, then save your input file with a new name and use it the input file for the next bit:  Find and normalize footnote markers and texts One of the trickiest bits to untangle, is the infuriating editorial convention of restarting the footnote numbering with each new page. This makes it hard to associate a footnote text (page-bound data), with a footnote marker (charter-bound data). Before we can do that we have to ensure that each footnote text that appears at the bottom of the page, appears in our source file on its own separate line with no leading white-space. And that none of the footnote markers within the text appears at the beginning of a line. And we must ensure that every footnote string, \"(1)\" for example, appears exactly twice on a page: once as an in-text marker, and once at the bottom for the footnote text. The following script reports the page number of any page that fails that test, along with a list of the footnote strings it found on that page.    \n 270 #\tDon't\tforget\tto\timport\tthe\tCounter\tmodule:\t\tfrom\tcollections\timport\tCounter\tfin\t=\topen(\"your_current_source_file.txt\",\t'r')\tGScriba\t=\tfin.readlines()\t#\tGScriba\tis\ta\tlist\tagain\tr\t=\tre.compile(\"\\(\\d{1,2}\\)\")\t\t#\tthere's\tlots\tof\tways\tfor\tOCR\tto\tscrew\tthis\tup,\tso\tbe\talert.\t\tpg\t=\tre.compile(\"~~~~~\tPAGE\t\\d+\t~~~~~\")\tpgno\t=\t0\t\tpgfnlist\t=\t[]\t#\tremember,\twe're\tprocessing\tlines\tin\tdocument\torder.\tSo\tfor\teach\tpage\t#\twe'll\tpopulate\ta\ttemporary\tcontainer,\t'pgfnlist',\twith\tvalues.\tThen\t#\twhen\twe\tcome\tto\ta\tnew\tpage,\twe'll\treport\twhat\tthose\tvalues\tare\tand\t#\tthen\treset\tour\tcontainer\tto\tthe\tempty\tlist.\t\tfor\tline\tin\tGScriba:\t\t\t\t\tif\tpg.match(line):\t\t\t\t\t\t\t\t\t#\tif\tthis\ttest\tis\tTrue,\tthen\twe're\tstarting\ta\tnew\tpage,\t\t\t\t\t\t\t\t\t\t#so\tincrement\tpgno\t\t\t\t\t\t\t\t\t\tpgno\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tif\twe've\tstarted\ta\tnew\tpage,\t\t\t#then\ttest\tour\tlist\tof\tfootnote\tmarkers\t\t\t\t\t\t\t\t\tif\tpgfnlist:\t\t\t\t\t\t\t\t\t\t\t\t\tc\t=\tCounter(pgfnlist)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tif\tthere\tare\tfn\tmarkers\tthat\tdo\tnot\tappear\texactly\ttwice,\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthen\treport\tthe\tpage\tnumber\tto\tus\t\t\t\t\t\t\t\t\t\t\t\t\tif\t1\tin\tc.values():\tprint\tpgno,\tpgfnlist\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthen\treset\tour\tlist\tto\tempty\t\t\t\t\t\t\t\t\t\t\t\t\tpgfnlist\t=\t[]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfor\teach\tline,\tlook\tfor\tALL\toccurences\tof\tour\tfootnote\tmarker\tregex\t\t\t\t\ti\t=\tr.finditer(line)\t\t\t\t\t\tfor\tmark\tin\t[eval(x.group(0))\tfor\tx\tin\ti]:\t\t\t\t\t\t\t\t\t#\tand\tadd\tthem\tto\tour\tlist\tfor\tthis\tpage\t\t\t\t\t\t\t\t\tpgfnlist.append(mark)\tNote: the elements in the iterator 'i' are string matches. We want the strings that were matched, group(0). e.g. \"(1)\". And if we do eval(\"(1)\") we get an integer that we can add to our list. Our Counter is a very handy special data structure. We know that we want each value in our pgfnlist to appear twice. Our Counter will give us a hash where the keys are the elements that appear, and the values are how many times each element appears. Like this: >>>\tl\t=\t[1,2,3,1,3]\t>>>\tc\t=\tCounter(l)\t>>>\tprint\tc\tCounter({1:\t2,\t3:\t2,\t2:\t1})\t \n 271 So if for a given page we get a list of footnote markers like this [1,2,3,1,3], then the test if\t1\tin\tc.values() will indicate a problem because we know each element must appear exactly twice: >>>\tl\t=\t[1,2,3,1,3]\t>>>\tc\t=\tCounter(l)\t>>>\tprint\tc.values()\t[2,\t1,\t2]\twhereas, if our footnote marker list for the page is complete [1,2,3,1,2,3], then: >>>\tl\t=\t[1,2,3,1,2,3]\t>>>\tc\t=\tCounter(l)\t>>>\tprint\tc.values()\t[2,\t2,\t2]\t#\ti.e.\t1\tis\tnot\tin\tc.values()\tAs before, run this script repeatedly, correcting your input file manually as you discover errors, until you are satisfied that all footnotes are present and correct for each page. Then save your corrected input file with a new name. Our text file still has lots of OCR errors in it, but we have now gone through it and found and corrected all the specific metadata bits that we want in our ordered data set. Now we can use our corrected text file to build a Python dictionary. Creating the Dictionary Now that we've cleaned up enough of the OCR that we can successfully differentiate the component parts of the page from each other, we can now sort the various bits of the meta-data, and the charter text itself, into their own separate fields of a Python dictionary. We have a number of things to do: correctly number each charter as to charter number, folio, and page; separate out the Italian summary and the marginal notation lines; and associate the footnote texts with their appropriate charter. To do all this, sometimes it is convenient to make more than one pass.  Create a skeleton dictionary. We'll start by generating a python dictionary whose keys are the charter numbers, and whose values are a nested dictionary that has fields for some of the metadata we want to store for each charter. So it will have the form:      \n 272 charters\t=\t{\t\t\t\t\t.\t\t\t\t\t.\t\t\t\t\t.\t\t\t\t\t300:\t{\t\t\t\t\t\t\t\t\t\t\t\t\t'chid':\t\"our\tcharter\tID\",\t\t\t\t\t\t\t\t\t\t\t\t\t'chno':\t300,\t\t\t\t\t\t\t\t\t\t\t\t\t'footnotes':\t[],\t#\tan\tempty\tlist\tfor\tnow\t\t\t\t\t\t\t\t\t\t\t\t\t'folio':\t\"the\tfolio\tmarker\tfor\tthis\tcharter\",\t\t\t\t\t\t\t\t\t\t\t\t\t'pgno':\t\"the\tpage\tnumber\tin\tthe\tprinted\tedition\tfor\tthis\tcharter,\t\t\t\t\t\t\t\t\t\t\t\t\t'text':\t[]\t#\tan\tempty\tlist\tfor\tnow\t\t\t\t\t\t\t\t\t\t\t},\t\t\t\t\t301:\t{\t\t\t\t\t\t\t\t\t\t\t\t\t'chid':\t\"our\tcharter\tID\",\t\t\t\t\t\t\t\t\t\t\t\t\t'chno':\t301,\t\t\t\t\t\t\t\t\t\t\t\t\t'footnotes':\t[],\t#\tan\tempty\tlist\tfor\tnow\t\t\t\t\t\t\t\t\t\t\t\t\t'folio':\t\"the\tfolio\tmarker\tfor\tthis\tcharter\",\t\t\t\t\t\t\t\t\t\t\t\t\t'pgno':\t\"the\tpage\tnumber\tin\tthe\tprinted\tedition\tfor\tthis\tcharter,\t\t\t\t\t\t\t\t\t\t\t\t\t'text':\t[]\t#\tan\tempty\tlist\tfor\tnow\t\t\t\t\t\t\t\t\t\t\t},\t\t\t\t\t.\t\t\t\t\t.\t\t\t\t\t.\tetc.\t}\tFor this first pass, we'll just create this basic structure and then in subsequent loops we will add to and modify this dictionary until we get a dictionary for each charter, and fields for all the metadata for each charter. Once this loop disposes of the easily searched lines (folio, page, and charter headers) and creates an empty container for footnotes, the fall-through default will be to append the remaining lines to the text field, which is a python list. slug\t=\tre.compile(\"(\\[~~~~sGScriba_)(.*)\\s::::\\s(\\d+)\\s~~~~\\]\")\tfol\t=\tre.compile(\"\\[fo\\.\\s?\\d+\\s?[rv]\\.\\s?\\]\")\tpgbrk\t=\tre.compile(\"~~~~~\tPAGE\t(\\d+)\t~~~~~\")\t\tfin\t=\topen('your_current_source_file.txt',\t'r')\tGScriba\t=\tfin.readlines()\t\t#\twe\twill\talso\tneed\tthese\tglobal\tvariables\twith\tstarting\tvalues\tn\t=\t0\tthis_folio\t=\t'[fo.\tl\tr.]'\tthis_page\t=\t1\t\t#\t‘charters’\tis\talso\tdefined\tas\ta\tglobal\tvariable.\tThe\t‘for’\tloop\tbelow\t#\tand\tin\tthe\tfollowing\tsections,\twill\tbuild\ton\tand\tmodify\tthis\tdictionary\tcharters\t=\tdict()\t\tfor\tline\tin\tGScriba:\t\tif\tfol.match(line):\t\t\t#\tuse\tthis\tglobal\tvariable\tto\tkeep\ttrack\tof\tthe\tfolio\tnumber.\t\t\t#\twe’ll\tcreate\tthe\t‘folio’\tfield\tusing\tthe\tvalue\tof\tthis\tvariable\t\t\tthis_folio\t=\tfol.match(line).group(0)\t\t\tcontinue\t#update\tthe\tvariable\tbut\totherwise\tdo\tnothing\t\tif\tslug.match(line):\t\t\t#\tif\tour\t‘slug’\tregex\tmatches,\twe\tknow\twe\thave\ta\tnew\tcharter\t\t\t#\tso\tget\tthe\tdata\tfrom\tthe\tmatch\tgroups\t \n 273 \t\tm\t=\tslug.match(line)\t\t\tchild\t=\t'GScroba_'\t+\tm.group(2)\t\t\tchnm\t=\tint(m.group(3))\t\t\t\t#\tthen\tcreate\tan\tempty\tnested\tdictionary\t\t\tcharters[chno]\t=\t{}\t\t\t\t#\tand\tan\tempty\tcontainer\tfor\tthe\tlines\twe\twon’t\tuse\tthis\tpass\t\t\ttemplist\t=\t[]\t\t\t#\tthis\tworks\tbecause\twe\tare\tproceeding\tin\ta\tdocument\torder\t\t#\ttemplist\tcontinues\tto\texist\tas\twe\titerate\tthrough\teach\tline\t\t\t#\tin\tthe\tcharter,\tthen\tis\treset\tto\tthe\tempty\tlist\twhen\twe\t\t#\tstart\ta\tnew\tcharter(slug.match(line)).\t\tcontinue\t#\twe\tgenerate\tthe\tentry,\tbut\tdo\tnothing\twith\tthe\ttext\t\tif\tchno:\t\t#\tif\ta\tcharter\tdictionary\thas\tbeen\tcreated\t\t#\tthen\twe\tcan\tnow\tpopulate\tit\twith\tdata\tfrom\tour\tslug.match\tabove\t\t\td\t=\tcharters[chno]\t\t\td['footnotes']\t=\t[]\t\t\td['child']\t=\tchild\t\t\td['chno']\t=\tchno\t\t\td['folio']\t=\tthis_folio\t\t\td['pgno']\t=\tthis_page\t\t\t\t\tif\tre.match('^\\(\\d+\\)’,\tline:\t\t\t\t#\tthis\tline\tis\tfootnote\ttext,\tbecause\tit\thas\ta\tfootnote\tmarker\t\t\t\t#\tline\t'(1)'\tat\tthe\tbeginning.\tSo\twe’ll\tdeal\twith\tit\tlater\t\t\t\tcontinue\t\t\telif\tfol.search(line):\t\t\t\t#\tif\tfolio\tchanges\twith\tthe\tcharter\ttext,\tupdate\tthe\tvariable\t\t\t\tthis_folio\t=\tfol.search(line).group(0)\t\t\t\ttemplist.append(line)\t\t\telse:\t\t\t\t#\tany\tline\tnot\totherwise\taccounted\tfor,\t\t\t\t\t#add\tto\ttemporary\tcontainer\t\t\t\ttemplist.append(line)\t\t\t#\tadd\tthe\ttemporary\tcontainer\tto\tthe\tdictionary\tafter\tusing\t\t\t#\ta\tlist\tcomprehension\tto\tstrip\t\tout\tany\tempty\tlines.\t\t\td['text']\t=\t[x\tfor\tx\tin\ttemplist\tif\tnot\tx\t==\t'\\n']\t Add the 'marginal notation' and Italian summary lines to the dictionary When we generated the dictionary of dictionaries above, we assigned fields for footnotes (just an empty list for now), charterID, charter number, the folio, and the page number. All remaining lines were appended to a list and assigned to the field 'text'. In all cases, the first line of each charter's text field should be the Italian summary as we have insured above. The second line in MOST cases, represents a kind of marginal notation usually ended by the ']' character (which OCR misreads a lot). We have to find the cases that do not meet this criterion, supply or correct the missing ']', and in the cases where there is no marginal notation I've supplied \"no marginal]\" in my working text. The following diagnostic script will print the charter number and first two lines of the text field for those charters that do not meet these criteria. Run this script separately against the charters dictionary, and correct and update your canonical text accordingly.  \n 274 n\t=\t0\tfor\tch\tin\tcharters:\t\t\t\t\ttxt\t=\tcharters[ch]['text']\t#\tremember:\tthe\ttext\tfield\tis\ta\tpython\tlist\tof\tstrings\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\tline1\t=\ttxt[0]\t\t\t\t\t\t\t\t\tline2\t=\ttxt[1]\t\t\t\t\t\t\t\t\tif\tline2\tand\t']'\tnot\tin\tline2:\t\t\t\t\t\t\t\t\t\t\t\t\tn\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"charter:\t%d\\ntext,\tline\t1:\t%s\\ntext,\tline\t2:\t%s\"\t%\t(ch,\tline1,\tline2)\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\tprint\tch,\t\"oops\"\t#\tto\tpass\tthe\tcharters\tfrom\tthe\tmissing\tpage\t214\tNote: The try: except: blocks are made necessary by the fact that in my OCR output, the data for pg 214 somehow got missed out. This often happens. Scanning or photographing each page of a 600 page book is tedious in the extreme. It's very easy to skip a page. You will inevitably have anomalies like this in your text that you will have to isolate and work around. The Python try: except: pattern makes this easy. Python is also very helpful here in that you can do a lot more in the except: clause beyond just printing \"oops\". You could call a function that performs a whole separate operation on those anomalous bits. Once we're satisfied that line 1 and line 2 in the 'text' field for each charter in the charters dictionary are the Italian summary and the marginal notation respectively, we can make another iteration of the charters dictionary, removing those lines from the text field and creating new fields in the charter entry for them. NOTA BENE: we are now modifying a data structure in memory rather than editing successive text files. So this script should be added to the one above that created your skeleton dictionary. That script creates the charters dictionary in memory, and this one modifies it for\tch\tin\tcharters:\t\t\t\t\td\t=\tcharters[ch]\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\td['summary']\t=\td['text'].pop(0).strip()\t\t\t\t\t\t\t\t\td['marginal']\t=\td['text'].pop(0).strip()\t\t\t\t\texcept\tIndexError:\t#\tthis\twill\treport\tthat\tthe\tcharters\ton\tp\t214\tare\tmissing\t\t\t\t\t\t\t\t\tprint\t\"missing\tcharter\t\",\tch\t Assign footnotes to their respective charters and add to dictionary The trickiest part is to get the footnote texts appearing at the bottom of the page associated with their appropriate charters. Since we are, perforce, analyzing our text line by line, we're faced with the problem of associating a given footnote reference with its appropriate footnote text when there are perhaps many lines intervening. For this we go back to the same list of lines that we built the dictionary from. We're depending on all the footnote markers appearing within the  \n 275 charter text, i.e. none of them are at the beginning of a line. And, each of the footnote texts is on a separate line beginning with '(1)' etc. We design regexes that can distinguish between the two and construct a container to hold them as we iterate over the lines. As we iterate over the lines of the text file, we find and assign markers and texts to our temporary container, and then, each time we reach a page break, we assign them to their appropriate fields in our existing Python dictionary charters and reset our temporary container to the empty dict. Note how we construct that temporary container. fndict starts out as an empty dictionary. As we iterate through the lines of our input text, if we find footnote markers within the line, we create an entry in fndict whose key is the footnote number, and whose value is another dictionary. In that dictionary we record the id of the charter that the footnote belongs to, and we create an empty field for the footnote text. When we find the footnote texts (ntexts) at the bottom of the page, we look up the footnote number in our container fndict and write the text of the line to the empty field we made. So when we come to the end of the page, we have a dictionary of footnotes that looks like this: {1:\t{'chid':\t158,\t'fntext':\t'Nel\tms.\tde\tdue\tvolte\te\tripa\tcancellato.'},\t\t2:\t{'chid':\t158,\t'fntext':\t'Sic\tnel\tms.'},\t\t3:\t{'chid':\t159,\t'fntext':\t'genero\tcancellato\tnel\tms.'}}\tNow we have all the necessary information to assign the footnotes to the empty 'footnotes' list in the charters dictionary: the number of the footnote (the key), the charter it belongs to (chid), and the text of the footnote (fntext). This is a common pattern in programming, and very useful: in an iterative process of some kind, you use an accumulator (our fndict) to gather bits of data, then when your sentinel encounters a specified condition (the pagebreak) it does something with the data. fin\t=\topen(\"your_current_source_file.txt\",\t'r')\tGScriba\t=\tfin.readlines()\t\t#\tin\tnotemark,\tnote\tthe\t'lookbehind'\texpression\t'?<!'\tto\tinsure\tthat\t#\tthe\tmarker\t'(1)'\tdoes\tnot\tbegin\tthe\tstring\tnotemark\t=\tre.compile(r\"\\(\\d+\\)(?<!^\\(\\d+\\))\")\t\tnotetext\t=\tre.compile(r\"^\\(\\d+\\)\")\tthis_charter\t=\t1\tpg\t=\tre.compile(\"~~~~~\tPAGE\t\\d+\t~~~~~\")\tpgno\t=\t1\tfndict\t=\t{}\t\tfor\tline\tin\tGScriba:\t\t\t\t\tnmarkers\t=\tnotemark.findall(line)\t\t\t\t\tntexts\t=\tnotetext.findall(line)\t\t\t\t\tif\tpg.match(line):\t\t\t\t\t\t\t\t\t#\tThis\tis\tour\t'sentinel'.\tWe've\tcome\tto\tthe\tend\tof\ta\tpage,\t\t\t\t\t\t\t\t\t#\tso\twe\trecord\tour\taccumulated\tfootnote\tdata\tin\tthe\t'charters'\tdict.\t\t\t\t\t\t\t\t\tfor\tfn\tin\tfndict:\t \n 276 \t\t\t\t\t\t\t\t\t\t\t\tchid\t=\tfndict[fn]['chid']\t\t\t\t\t\t\t\t\t\t\t\t\tfntext\t=\tfndict[fn]['fntext']\t\t\t\t\t\t\t\t\t\t\t\t\tcharters[int(chid)]['footnotes'].append((fn,\tfntext))\t\t\t\t\t\t\t\t\t\t\tpgno\t+=\t1\t\t\t\t\t\t\t\t\tfndict\t=\t{}\t\t#\tand\tthen\tre-initialize\tour\ttemporary\tcontainer\t\t\t\t\t\t\t\tif\tslug.match(line):\t#\there's\tthe\tbeginning\tof\ta\tcharter,\tso\tupdate\tthe\tvariable.\t\t\t\t\t\t\t\t\tthis_charter\t=\tint(slug.match(line).group(3))\t\t\t\t\tif\tnmarkers:\t\t\t\t\t\t\t\t\tfor\tmarker\tin\t[eval(x)\tfor\tx\tin\tnmarkers]:\t\t\t\t\t\t\t\t\t\t\t\t\t#\tcreate\tan\tentry\twith\tthe\tcharter's\tid\tand\tan\tempty\ttext\tfield\t\t\t\t\t\t\t\t\t\t\t\t\tfndict[marker]\t=\t{'chid':this_charter,\t'fntext':\t''}\t\t\t\t\tif\tntexts:\t\t\t\t\t\t\t\t\tfor\ttext\tin\t[eval(x)\tfor\tx\tin\tntexts]:\t\t\t\t\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tfill\tin\tthe\tappropriate\tempty\tfield.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfndict[text]['fntext']\t=\tre.sub('\\(\\d+\\)',\t'',\tline).strip()\t\t\t\t\t\t\t\t\t\t\t\t\texcept\tKeyError:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"printer's\terror?\t\",\t\"pgno:\",\tpgno,\tline\tNote that the try:\texcept: blocks come to the rescue again here. The loop above kept breaking because in 3 instances it emerged that there existed footnotes at the bottom of a page for which there were no markers within the text. This was an editorial oversight in the published edition, not an OCR error. The result was that when I tried to address the non-existent entry in fndict, I got a KeyError. My except: clause allowed me to find and look at the error, and determine that the error was in the original and nothing I could do anything about, so when generating the final version of charters I replaced the print statement with pass. Texts made by humans are messy; no getting around it. try:\texcept: exists to deal with that reality. NOTA BENE: Again, bear in mind that we are modifying a data structure in memory rather than editing successive text files. So this loop should be added to your script below the summary and marginal loop, which is below the loop that created your skeleton dictionary.  Parse Dates and add to the dictionary Dates are hard. Students of British history cling to Cheyney296 as to a spar on a troubled ocean. And, given the way the Gregorian calendar was adopted so gradually, and innumerable other local variations, correct date reckoning for medieval sources will always require care and local knowledge. Nevertheless, here too Python can be of some help. Our Italian summary line invariably contains a date drawn from the text, and it's conveniently set off from the rest of the line by parentheses. So we can parse them and create Python date objects. Then, if we want, we can do some simple calendar arithmetic.                                                         296\tC.R.\tCheney,\tMichael\tJones,\tA\thandbook\tof\tdates\tfor\tstudents\tof\tBritish\tHistory\t(2000).\t \n 277 First we have to find and correct all the dates in the same way as we have done for the other metadata elements. Devise a diagnostic script that will iterate over your charters dictionary, report the location of errors in your canonical text, and then fix them in your canonical text manually. Something like this: summary_date\t=\tre.compile('\\((\\d{1,2})?(.*?)(\\d{1,4})?\\)')\t#\twe\twant\tto\tcatch\tthem\tall,\tand\tsome\thave\tno\tday\tor\tmonth,\thence\tthe\toptional\tquantifiers:\t`?`.\t\t#\tAnd\twe\twant\tto\tmake\tPython\tspeak\tItalian:\tital2int\t=\t{'gennaio':\t1,\t'febbraio':\t2,\t'marzo':\t3,\t'aprile':\t4,\t'maggio':\t5,\t'giugno':\t6,\t'luglio':\t7,\t'agosto':\t8,\t'settembre':\t9,\t'ottobre':\t10,\t'novembre':\t11,\t'dicembre':\t12}\t\timport\tsys\tfor\tch\tin\tcharters:\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\td\t=\tcharters[ch]\t\t\t\t\t\t\t\t\ti\t=\tsummary_date.finditer(d['summary'])\t\t\t\t\t\t\t\t\tdt\t=\tlist(i)[-1]\t\t\t\t#\tAlways\tthe\tlast\tparenthetical\texpression,\tin\tcase\tthere\tis\tmore\tthan\tone.\t\t\t\t\t\t\t\t\t\tif\tdt.group(2).strip()\tnot\tin\tital2int.keys():\t\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"chno.\t%d\tfix\tthe\tmonth\t%s\"\t%\t(d['chno'],\tdt.group(2))\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\tprint\td['chno'],\t\"The\tusual\tsuspects\t\",\tsys.exc_info()[:2]\tNote: When using try/except blocks, you should usually trap specific errors in the except clause, like ValueError and the like; however, in ad hoc scripts like this, using sys.exc_info is a quick and dirty way to get information about any exception that may be raised. (The sys module297 is full of such stuff, useful for debugging) Once you're satisfied that all the parenthetical date expressions are present and correct, and conform to your regular expression, you can parse them and add them to your data structure as dates rather than just strings. For this you can use the datetime module. This module is part of the standard library, is a deep subject, and ought to be the subject of its own tutorial, given the importance of dates for historians. As with a lot of other python modules, a good introduction is Doug Hellmann's PyMOTW(module of the week).298 An even more able extension library is mxDateTime.299 Suffice it here to say that the datetime.date module expects parameters like this: >>>\tfrom\tdatetime\timport\tdate\t>>>\tdt\t=\tdate(1160,\t12,\t25)\t>>>\tdt.isoformat()\t'1160-12-25'\t                                                        297\t‘sys\t–\tSystem-specific\tConfiguration’:\thttps://pymotw.com/2/sys/index.html#module-sys\t298\t‘datetime\t–\tDate/time\tvalue\tmanipulation’:\thttps://pymotw.com/2/datetime/\t299\t‘mxDateTime\t–\tDate/Time\tLibrary\tfor\tPython’,\tEGenix.com:\thttp://www.egenix.com/products/python/mxBase/mxDateTime/\t \n 278 So here's our loop to parse the dates at the end of the Italian summary lines and store them in our charters dictionary (remembering again that we want to modify our in-memory data structure charters created above): summary_date\t=\tre.compile('\\((\\d{1,2})?(.*?)(\\d{1,4})?\\)')\tfrom\tdatetime\timport\tdate\tfor\tch\tin\tcharters:\t\t\t\t\tc\t=\tcharters[ch]\t\t\t\t\ti\t=\tsummary_date.finditer(c['summary'])\t\t\t\t\tfor\tm\tin\ti:\t\t\t\t\t\t\t\t\t#\tremember\t'i'\tis\tan\titerator\tso\teven\tif\tthere\tis\tmore\tthan\tone\t\t\t\t\t\t\t\t\t#\tparenthetical\texpression\tin\tc['summary'],\tthe\ttry\tclause\twill\t\t\t\t\t\t\t\t\t#\tsucceed\ton\tthe\tlast\tone,\tor\tfail\ton\tall\tof\tthem.\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\t\t\t\t\tyr\t=\tint(m.group(3))\t\t\t\t\t\t\t\t\t\t\t\t\tmo\t=\tital2int[m.group(2).strip()]\t\t\t\t\t\t\t\t\t\t\t\t\tday\t=\tint(m.group(1))\t\t\t\t\t\t\t\t\t\t\t\t\tc['date']\t=\tdate(yr,\tmo,\tday)\t\t\t\t\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\t\t\t\t\tc['date']\t=\t\"date\twon't\tparse,\tsee\tsummary\tline\"\tOut of 803 charters, 29 wouldn't parse, mostly because the date included only month and year. You can store these as strings, but then you have two data types claiming to be dates. Or you could supply a 01 as the default day and thus store a Python date object, but Jan. 1, 1160 isn't the same thing as Jan. 1160 and thus distorts your metadata. Or you could just do as I have done and refer to the relevant source text: the Italian summary line in the printed edition. Once you've got date objects, you can do date arithmetic. Supposing we wanted to find all the charters dated to within 3 weeks of Christmas, 1160. #\tLet's\timport\tthe\twhole\tthing\tand\tuse\tdot\tnotation:\tdatetime.date()\tetc.\timport\tdatetime\t\t\t#\ta\ttimedelta\tis\ta\tspan\tof\ttime\tweek\t=\tdatetime.timedelta(weeks=1)\t\tfor\tch\tin\tcharters:\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\tdt\t=\tcharters[ch]['date']\t\t\t\t\t\t\t\t\tchristmas\t=\tdatetime.date(1160,12,25)\t\t\t\t\t\t\t\t\tif\tabs(dt\t-\tchristmas)\t<\tweek\t*\t3:\t\t\t\t\t\t\t\t\t\t\t\t\tprint\t\"chno:\t%s,\tdate:\t%s\"\t%\t(charters[ch]['chno'],\tdt)\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\tpass\t#\tavoid\tthis\tidiom\tin\tproduction\tcode\tWhich will give us this result: chno:\t790,\tdate:\t1160-12-14\tchno:\t791,\tdate:\t1160-12-15\tchno:\t792,\tdate:\t1161-01-01\tchno:\t793,\tdate:\t1161-01-04\tchno:\t794,\tdate:\t1161-01-05\tchno:\t795,\tdate:\t1161-01-05\tchno:\t796,\tdate:\t1161-01-10\t \n 279 chno:\t797,\tdate:\t1161-01-10\tchno:\t798,\tdate:\t1161-01-06\tCool, huh?  Our completed data structure Now we've corrected our canonical text as much as we need to to differentiate between the various bits of meta-data that we want to capture, and we've created a data structure in memory, our charters dictionary, by making 4 passes, each one extending and modifying the dictionary in memory. create the skeleton separate out the summary and marginal lines and create dictionary fields for them. collect and assign footnotes to their respective charters parse the dates in the summary field, and add them to their respective charters Print out our resulting dictionary using pprint(charters) and you'll see something like this: {\t.\t.\t.\t\t\t52:\t{'chid':\t'GScriba_LII',\t\t\t\t\t\t\t'chno':\t52,\t\t\t\t\t\t\t'date':\tdatetime.date(1156,\t3,\t27),\t\t\t\t\t\t\t'folio':\t'[fo.\t6\tr.]',\t\t\t\t\t\t\t'footnotes':\t[(1,\t'Cancellato:\tm.')],\t\t\t\t\t\t\t'marginal':\t'no\tmarginal]',\t\t\t\t\t\t\t'pgno':\t29,\t\t\t\t\t\t\t'summary':\t'I\tconsoli\tdi\tGenova\triconoscono\tcon\tsentenza\til\tdiritto\tdi\tRomano\tdi\tCasella\tdi\tpagarsi\tsui\tbeni\tdi\tGerardo\tConfector\tper\tun\tcredito\tche\taveva\tverso\til\tmedesimo\t(27\tmarzo\t1156).',\t\t\t\t\t\t\t'text':\t['\t\t\t\tIn\tpontili\tcapituli\tconsules\tE.\tAurie,\tW.\tBuronus,\tOgerius\tVentus\tlaudaverunt\tquod\tRomanus\tde\tCasella\thaberet\tin\tbonis\tGerardi\tConfectoris\ts.\t.xxvi.\tdenariorum\tet\tpossit\teos\taccipere\tsine\tcontradicione\teius\tet\tomnium\tpro\teo.\tHoc\tideo\tquia,\tcum;\tRomanus\tante\tipsos\tinde\tconquereretur,\tipso\tGerardo\tdebitum\tnon\tnegante,\tsed\tquod\tde\tusura\tesset\tobiiciendo,\tiuravit\tnominatus\tRomanus\tquod\tcapitalis\terat\t(1)\tet\tnon\tde\tusura,\tunde\tut\tsupra\tlaudaverunt\t,\t.MCLVI.,\tsexto\tkalendas\taprilis,\tindicione\ttercia.\\n']},\t\t53:\t{'chid':\t'GScriba_LIII',\t\t\t\t\t\t\t'chno':\t53,\t\t\t\t\t\t\t'date':\tdatetime.date(1156,\t3,\t27),\t\t\t\t\t\t\t'folio':\t'[fo.\t6\tr.]',\t\t\t\t\t\t\t'footnotes':\t[],\t\t\t\t\t\t\t'marginal':\t'Belmusti]',\t\t\t\t\t\t\t'pgno':\t29,\t\t\t\t\t\t\t'summary':\t\"Maestro\tArnaldo\te\tGiordan\tnipote\tdel\tfu\tGiovanni\tdi\tPiacenza\tsi\tobbligano\tdi\tpagare\tuna\tsomma\tnell'ottava\tdella\tprossima\tPasqua,\tper\tmerce\tricevuta\t(27\tmarzo\t1156).\",\t \n 280 \t\t\t\t\t\t'text':\t['\t\tTestes\tConradus\tPorcellus,\tAlbericus,\tVassallus\tGambalixa,\tPetrus\tArtodi.\tNos\tArnaldus\tmagister\tet\tIordan\tnepos\tquondam\tIohannis\tPlacentie\taccepimus\ta\tte\tBelmusto\ttantum\tbracile\tunde\tpromittimus\tdare\ttibi\tvel\ttuo\tcerto\tmisso\tlb.\t.xLIII.\tdenariorum\tusque\toctavam\tproximi\tpasce,\tquod\tsi\tnon\tfecerimus\tpenam\tdupli\ttibi\tstipulanti\tpromittimus,\tbona\tpignori,\tpossis\tunumquemque\tconvenire\tde\ttoto.\tActum\tprope\tcampanile\tSancti\tLaurentii,\tmillesimo\tcentesimo\t.Lv.,\tsexto\tkalendas\taprilis,\tindictione\ttercia.\\n']},\t.\t.\t.\tetc.\t}\tPrinting out your Python dictionary as a literal string is not a bad thing to do. For a text this size, the resulting file is perfectly manageable, can be mailed around usefully and read into a python repl session very simply using eval(), or pasted directly into a Python module file. On the other hand, if you want an even more reliable way to serialize it in an exclusively Python context, look into Pickle.300 If you need to move it to some other context, JavaScript for example, or some RDF triple stores, Python's json module301 will translate effectively. If you have to get some kind of XML output, I will be very sorry for you, but the lxml python module302 may ease the pain a little.  Order from disorder, huzzah. Now that we have an ordered data structure, we can do many things with it. As a very simple example, let's append some code that just prints charters out as html for display on a web-site: fout\t=\topen(\"your_page.html\",\t'w')\t#\tcreate\ta\ttext\tfile\tto\twrite\tthe\thtml\tto\t\t#\twrite\tto\tthe\tfile\tyour\thtml\theader\twith\tsome\tCSS\tformatting\tdeclarations\tfout.write(\"\"\"\t<!DOCTYPE\thtml\tPUBLIC\t\"-//W3C//DTD\tHTML\t4.01//EN\">\t\t<html>\t<head>\t\t\t<title>Giovanni\tScriba\tVol.\tI</title>\t\t\t<style>\t\t\t\t\th1\t{text-align:\tcenter;\tcolor:\t#800;\tfont-size:\t16pt;\tmargin-bottom:\t0px;\tmargin-top:\t16px;}\t\t\t\t\tul\t{list-style-type:\tnone;}\t\t\t\t\t.sep\t{color:\t#800;\ttext-align:\tcenter}\t\t\t\t\t.charter\t{width:\t650px;\tmargin-left:\tauto;\tmargin-right:\tauto;\tmargin-top:\t60px;\tborder-top:\tdouble\t#800;}\t\t\t\t\t.folio\t{color:\t#777;}\t\t\t\t\t.summary\t{color:\t#777;\tmargin:\t12px\t0px\t12px\t12px;}\t\t\t\t\t.marginal\t{color:\tred}\t\t\t\t\t.charter-text\t{margin-left:\t16px}\t\t\t\t\t.footnotes\t\t\t\t\t.page-number\t{font-size:\t60%}\t\t\t</style></head>\t\t<body>\t                                                        300\t‘Pickle\t-\t\tPython\tobject\tserialization’:\thttps://docs.python.org/2/library/pickle.html\t301\t‘json\t–\tJSON\tencoder\tand\tdecoder’:\thttps://docs.python.org/2/library/json.html#module-json\t302\t‘lxml\t–\tXML\tand\tHTML\twith\tPython’,\thttp://lxml.de/\t \n 281 \"\"\")\t\t#\ta\tloop\tthat\twill\twrite\tout\ta\tblob\tof\thtml\tcode\tfor\teach\tcharter\tin\tour\tdictionary:\tfor\tx\tin\tcharters:\t\t\t\t\t\t\t\t\t\t#\tuse\ta\tshallow\tcopy\tso\tcharters[x]\tis\tnot\tmodified\tfor\tthis\tspecialized\tpurpose\t\t\t\t\td\t=\tcharters[x].copy()\t\t\t\t\t\t\t\t\t\ttry:\t\t\t\t\t\t\t\t\tif\td['footnotes']:\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tremember,\tthis\tis\ta\tlist\tof\ttuples.\tSo\tyou\tcan\tfeed\tthem\tdirectly\t\t\t\t\t\t\t\t\t\t\t\t\t#\tto\tthe\tstring\tinterpolation\toperator\tin\tthe\tlist\tcomprehension.\t\t\t\t\t\t\t\t\t\t\t\t\tfnlist\t=\t[\"<li>(%s)\t%s</li>\"\t%\tt\tfor\tt\tin\td['footnotes']]\t\t\t\t\t\t\t\t\t\t\t\t\td['footnotes']\t=\t\"<ul>\"\t+\t''.join(fnlist)\t+\t\"</ul>\"\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\td['footnotes']\t=\t\"\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\td['text']\t=\t'\t'.join(d['text'])\t#\td['text']\tis\ta\tlist\tof\tstrings\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tblob\t=\t\"\"\"\t\t\t\t\t\t\t\t\t\t\t\t\t<div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"charter\">\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1>%(chid)s</h1>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"folio\">%(folio)s\t(pg.\t%(pgno)d)</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"summary\">%(summary)s</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"marginal\">%(marginal)s</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"text\">%(text)s</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"footnotes\">%(footnotes)s</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div>\t\t\t\t\t\t\t\t\t\t\t\t\t</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfout.write(blob\t%\td)\t\t\t\t\t\t\t\t\t\t#\t`string\t%\tdictionary`\tis\ta\tneat\ttrick\tfor\thtml\ttemplating\t\t\t\t\t\t\t\t\t#\tthat\tmakes\tuse\tof\tpython's\tstring\tinterpolation\tsyntax\t\t\t\t\t\t\t\t\t#\tsee:\thttp://www.diveintopython.net/html_processing/dictionary_based_string_formatting.html\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfout.write(\"\\n\\n\")\t\t\t\t\texcept:\t\t\t\t\t\t\t\t\t#\tinsert\tentries\tnoting\tthe\tabsence\tof\tcharters\ton\tthe\tmissing\tpg.\t214\t\t\t\t\t\t\t\t\terratum\t=\t\"\"\"\t\t\t\t\t\t\t\t\t\t\t\t\t<div>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div\tclass=\"charter\">\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1>Charter\tno.\t%d\tis\tmissing\tbecause\tthe\tscan\tfor\tPg.\t214\twas\tommited</h1>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div>\t\t\t\t\t\t\t\t\t\t\t\t\t</div>\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"\t\t%\td['chno']\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfout.write(erratum)\t\t\t\t\t\t\t\t\t\tfout.write(\"\"\"</body></html>\"\"\")\tDrop the resulting file on a web browser, and you've got a nicely formated electronic edition.  \n 282 \n\thtml formatted charter example Being able to do this with your, still mostly uncorrected, OCR output is not a trivial advantage. If you're serious about creating a clean, error free, electronic edition of anything, you've got to do some serious proofreading. Having a source text formatted for reading is crucial; moreover, if your proofreader can change the font, spacing, color, layout, and so forth at will, you can increase their accuracy and productivity substantially. With this example in a modern web browser, tweaking those parameters with some simple CSS declarations is easy. Also, with some ordered HTML to work with, you might crowd-source the OCR error correction, instead of hiring that army of illiterate street urchins. And, our original problem, OCR cleanup, is now much more tractable because we can target regular expressions for the specific sorts of metadata we have: errors in the Italian summary or in the Latin text? Or we could design search-and-replace routines just for specific charters, or groups of charters. Beyond this though, there's lots you can do with an ordered data set, including feeding it back through a markup tool like the brat303 as we did for the ChartEx project. Domain experts can then start adding layers of semantic tagging even if you don't do any further OCR error correction. Moreover, with an ordered dataset we can get all sorts of output, some other flavor of XML (if you must) for example: TEI (Text Encoding Initiative), or EAD (Encoded Archival Description). Or you could read your dataset directly into a relational database, or some kind of key/value store. All of these things are essentially impossible if you're working simply with a plain text file. The bits of code above are in no way a turn-key solution for cleaning arbitrary OCR output. There is no such magic wand. The Google approach to scanning the contents of research libraries threatens to drown us in an                                                         303\t‘brat\trapid\tannotation\ttool’,\thttp://brat.nlplab.org/\t\n \n 283 ocean of bad data. Worse, it elides a fundamental fact of digital scholarship: digital sources are hard to get. Reliable, flexible, and useful digital texts require careful redaction and persistent curation. Google, Amazon, Facebook, et alia do not have to concern themselves with the quality of their data, just its quantity. Historians, on the other hand, must care first for the integrity of their sources. The vast 18th and 19th century publishing projects, the Rolls Series, the Monumenta Germaniae Historica, and many others, bequeathed a treasure trove of source material to us by dint of a huge amount of very painstaking and detailed work by armies of dedicated and knowledgeable scholars. Their task was the same as ours: to faithfully transmit history's legacy from its earlier forms into a more modern form, thereby making it more widely accessible. We can do no less. We have powerful tools at our disposal, but while that may change the scale of the task, it does not change its nature. About the Author Jon Crump is an independent scholar and freelance digital humanist based in Seattle, Washington.  \n 284 29. Transliterating non-ASCII characters with Python Seth Bernstein – 2013     Lesson Goals: This lesson shows how to use Python to transliterate automatically a list of words from a language with a non-Latin alphabet to a standardized format using the American Standard Code for Information Interchange (ASCII)304 characters. It builds on readers’ understanding of Python from the lessons “Understanding Web Pages and HTML,” “Downloading Web Pages with Python,” “From HTML to List of Words (part 1)” and “Intro to Beautiful Soup.”305 At the end of the lesson, we will use the transliteration dictionary to convert the names from a database of the Russian organization Memorial306 from Cyrillic307 into Latin characters.308 Although the example uses Cyrillic characters, the technique can be reproduced with other alphabets using Unicode.309 What Is Transliteration and for Whom Is It Useful? Transliteration is something that most people do every day, knowingly or not. Many English speakers would have trouble recognizing the name Владимир Путин but know that Vladimir Putin is Russia’s current president. Transliteration is especially useful with names, because a standardized transliterated name is often the same as a translated name. (Exceptions are when someone’s name is translated in a non-uniform way. Leon Trotsky’s Russian name would be transliterated in a standardized form as Lev Trotskii.) But transliteration has other uses too, especially for scholars. In many fields, the publishing convention is to transliterate any evidence used in the original. Moreover, citations from scholarly works need to be transliterated carefully so that readers can find and verify evidence used in                                                         304\t‘ASCII’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Ascii\t305\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Understanding\tWeb\tPages\tand\tHTML’\t(2012);\t‘Downloading\tWeb\tPages\twith\tPython’\t(2012);\t‘From\tHTML\tto\tList\tof\tWords\t(part\t1)’\t(2012);\tJeri\tWieringa,\t‘Intro\tto\tBeautiful\tSoup’,\tThe\tProgramming\tHistorian\t(2012).\t306\t‘Жертвы\tполитического\tтеррора\tв\tСССР’:\thttp://lists.memo.ru/\t307\t‘Cyrillic\tscript’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Cyrillic_script\t308\t‘Latin\tscript’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Latin_script\t309\t‘Unicode’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Unicode\t \n 285 texts. Finally, transliteration can be more practical for authors who can type more fluently with Latin letters than in the native alphabet of a language that does not use Latin characters. Programming languages like Python also benefit from transliteration. Python handles Cyrillic relatively well in certain environments, like Terminal for MacOS or Linux,310 or in Windows, IDLE, the official Python integrated development environment.311 However, even in these Python converts non-ASCII characters into code. Other environments, like the Python shell for Windows (command line) or Komodo Edit,312 know Unicode but will not print the Cyrillic characters that Unicode represents without tricky additional configuration. In environments that do support Cyrillic, switching between a Latin character set to write code and a non-Latin character set to handle inputs can be tedious. Thus, creating a program to transliterate evidence automatically eliminates the step of transliteration for researchers and it converts the text into a format that Python can handle more readily. This lesson was built and tested using IDLE for Windows and Terminal for MacOS. The author strongly recommends that you follow along using the program tested on your operating system rather Windows Command Prompt or Komodo Edit. This lesson will be particularly useful for research in fields that use a standardized transliteration format, such as Russian history field, where the convention is to use a simplified version of the American Library Association-Library of Congress (ALA-LC) transliteration table.313 (All tables currently available can be accessed here.) Researchers dealing with large databases of names can benefit considerably. However, this lesson will also allow practice with Unicode, character translation and using the parser Beautiful Soup in Python.314 Converting a Webpage to Unicode The goal of this lesson is to take a list of names from a Russian database and convert them from Cyrillic into ASCII characters. The page we will use is from the site of the Russian human rights organization Memorial. During Glasnost315 professional and amateur historians in the Soviet Union gained the ability to conduct research on previously taboo subjects, such as repression under Stalin. Banding together, they founded Memorial316 to collect and publicize their findings. Today, the NGO conducts research on a                                                         310\t‘Terminal\t(OS\tX)’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Terminal_%28OS_X%29\t311\t‘IDLE\t(Python)’,\tWikipedia:\thttps://en.wikipedia.org/wiki/IDLE_%28Python%29\t312\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/\t313\t‘ALA-LC\tRomanization\tfor\tRussia’,\tWikipedia:\thttps://en.wikipedia.org/wiki/ALA-LC_romanization_for_Russian\t314\t‘Beautiful\tSoup’:\thttp://www.crummy.com/software/BeautifulSoup/\t315\t‘Glasnost’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Glasnost\t316\t‘Жертвы\tполитического\tтеррора\tв\tСССР’:\thttp://lists.memo.ru/\t \n 286 range of civil rights abuses in Russia, but collecting data about the victims of Stalinism remains one of its main functions. On the Memorial website researchers can find a database with some three million entries of people who were arrested or executed by Stalin’s regime. It is an important resource on a dark topic. However, because the database has many, many names, it lends itself nicely to automated transliteration. This lesson will use just the first page of the database317, but using the lesson on “Automated Downloading with Wget,”318 it would be possible to go through the entire database as fast as your computer would allow. We need to start by modifying the process found in the lesson “Downloading Web Pages with Python”.319 There we learned how to open and copy the HTML from a web page in Python. But what if we want to open a page in a language that does not use Latin characters? Python can do this but we need to tell it how to read these letters using a codec, a library of codes that allows Python to represent non-ASCII characters. Working with web pages makes this easy because almost all web pages specify what kind of encoding they use, in the page’s headers. In Python, opening a web page does not just give you the HTML, but it creates an object with several useful characteristics. One is that we can access the headers by calling the header() method. This method returns something a lot like a Python dictionary with information that is important to web programmers. For our purposes, what is important is that the encoding is stored under the ‘content-type’ key. #transliterator.py\timport\turllib2\t\tpage\t=\turllib2.urlopen('http://lists.memo.ru/d1/f1.htm')\t\t#what\tis\tthe\tencoding?\tprint\tpage.headers['content-type']\tUnder the ‘content-type’ key we find this information: text/html;\tcharset=windows-1251\tThe ‘content-type’ is telling us that the file stored at the url we accessed is in HTML and that its encoding (after ‘charset=’, meaning character set) is ‘windows-1251′, a common encoding for Cyrillic characters. You can visit the webpage and view the Page Source and see for yourself that the first line does in fact contain a ‘content-type’ variable with the value text/html; charset=windows-1251. It would not be so hard to work with the ‘windows-1251′ encoding. However, ‘windows-1251′ is specifically for Cyrillic and will not handle all languages. For the sake of learning a standard method, what we want is Unicode, a coding set that handles not just Cyrillic but                                                         317\tAvailable\tat:\thttp://lists.memo.ru/d1/f1.htm\t318\tIan\tMilligan,\t‘Automated\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2012).\t319\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Downloading\tWeb\tPages\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 287 characters and symbols from virtually any language. (For more on Unicode, see the What is Unicode page.)320 Converting into Unicode gives us the potential to create a transliteration table that could cover multiple languages and special characters in a way that region-specific character sets do not allow. How do you convert the characters to Unicode? First, Python needs to know the original encoding of the source, ‘windows-1251.’ We could just assign ‘windows-1251′ to a variable by typing it manually but the encoding may not always be ‘windows-1251.’ There are other character sets for Cyrillic, not to mention other languages. Let’s find a way to make the process more automatic for those cases. It helps that the encoding is the very last part of the string, so we can isolate it from everything that came before in the string. By using the .split() method, the string containing whatever encoding it is can be assigned to a variable. The .split(separator) method in Python returns a list of sections in the string that are split by some user-defined separator. Assigning no separator to .split() separates a string at the spaces. Another use of the .split() method is to separate by commas, which can help to work with comma separated value (csv) files.321 In this case, though, by splitting the ‘content-type’ string at ‘charset=’, we get a list with two strings where the second will be the character set. encoding\t=\tpage.headers['content-type'].split('charset=')[1]\tThe encoding is assigned to the variable called ‘encoding’. You can check to see if this worked by printing the ‘encoding’ variable. Now we can tell Python how to read the page as Unicode. Using the unicode(object\t[,\tencoding]) method turns a string of characters into a Unicode object. A Unicode object is similar to a string but it can contain special characters. If they are in a non-ASCII character set, like here with ‘windows-1251’, we have to use the optional encoding parameter. #read\tthe\tHTML\tas\ta\tstring\tinto\ta\tvariable\tcontent\t=\tpage.read()\t\t#\tthe\tunicode\tmethod\ttries\tto\tuse\tASCII\tso\twe\tneed\tto\ttell\tit\tthe\tencoding\tcontent\t=\tunicode(content,\tencoding)\tcontent[200:300]\t\tu'\"list-right\">\\r\\n\t<ul>\t\t\t\t\t<li>\t<p\tclass=\"name\"><a\tname=\"n1\"></a>\\u0410-\\u0410\\u043a\\u0443\t\\u0422\\u0443\\u043b\\u0438\\u043a\\u043e\\u0432\\u0438\\u0447</p>\t<p\tclass=\"cont\">\\r\\n\\u0420\\u043e\\u0434\\u0438\\u043b\\u0441\\u044f\\xa0\\u0432\t'</p>\t                                                        320\t‘What\tis\tUnicode?’:\thttp://lists.memo.ru/d1/f1.htm\t321\t‘Comma-separated\tvalues’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Comma-separated_values\t \n 288 In some editors like Komodo, printing even Unicode will raise an error. Indeed, the inability of some Python environments to print Unicode out of the box is one big advantage of transliterating it into ASCII. In IDLE, though, we can print this content to see it in Cyrillic rather than Unicode: #\tsee\twhat\thappens\twhen\tPython\tprints\tUnicode\tprint\tcontent[200:300]\t\"list-right\">\t<ul>\t\t\t\t\t<li>\t<p\tclass=\"name\"><a\tname=\"n1\"></a>А-Аку\tТуликович</p>\tРодился\tв\tExcellent - the web page is now converted to Unicode. All the ‘\\u0420’-type marks are Unicode and Python knows that they code to Cyrillic characters. The forward slash is called an ‘escape character’ and allows Python to do things like use special characters in Unicode or signify a line break (‘\\n’) in a document. Each counts as just one character. Now we can create a Python dictionary that will act as the transliteration table. Unicode Transliteration Dictionary A dictionary is an unordered collection of key-object pairs. What this means is that under each key, the dictionary stores some number or string or other object – even another dictionary. (See also the lesson “Counting Word Frequencies with Python.”)322 A dictionary has the following syntax: my_dictionary\t=\t{'Vladimir':\t'Putin',\t'Boris':\t'Yeltsin'}\tprint\tmy_dictionary['Vladimir']\t\t>\tPutin\tHow can we turn this into a transliteration table? Just make each Unicode character a key in the dictionary. Its value will be whatever character(s) it transliterates to. The table for Romanization of Russian is available from the Library of Congress.323 This table needs to be simplified slightly. The ALA-LC suggests using characters with umlauts or ligatures to represent Cyrillic letters but those characters are no more ASCII than Cyrillic characters. So instead no umlauts or ligatures will be used. Each Cyrillic letter has a different Unicode value. It would take time to find each one of them but fortunately Wikipedia has a table.324 If the script were very rare, we could find it at the Unicode website.325                                                         322\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t323\t‘Russian’,\tLibrary\tof\tCongress:\thttp://www.lcweb.loc.gov/catdir/cpso/romanization/russian.pdf\t324\t‘Cyrillic\tscript\tin\tUnicode’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Cyrillic_script_in_Unicode\t325\t‘Unicode\t8.0\tCharacter\tCode\tCharts’,\thttp://www.unicode.org/charts/\t \n 289 We just need to combine the transliteration table with the Unicode table. The Unicode value for the Russian letter “Ж” is 0416 and it transliterates to the Latin characters “Zh.” Python needs more than just the Unicode identifier. It also needs to know to look out for a Unicode character. Therefore all the Unicode characters used in the dictionary should be in the format u’\\uXXXX’. In this case, the letter Ж is u’\\u0416’. We can create a transliteration dictionary and assign ‘Zh’ as the value for the key u’\\u0416’ in it. cyrillic_translit\t=\t{\tu'\\u0416':\t'Zh'}\tAs it turns out, lowercase Cyrillic letters in Unicode have the same value as their uppercase counterparts except the value of the second number is two greater. Thus, ‘ж’ codes to 0436. Now that we have a transliteration dictionary created, we just add a dictionary key-value pair. cyrillic_translit[u'\\u0436']\t=\t'zh'\tOf course, rather than do each pair one by one, it would probably be easier to write the dictionary in a Python module or paste it in from a word processor. The full Cyrillic transliteration dictionary is here: cyrillic_translit={u'\\u0410':\t'A',\tu'\\u0430':\t'a',\tu'\\u0411':\t'B',\tu'\\u0431':\t'b',\tu'\\u0412':\t'V',\tu'\\u0432':\t'v',\tu'\\u0413':\t'G',\tu'\\u0433':\t'g',\tu'\\u0414':\t'D',\tu'\\u0434':\t'd',\tu'\\u0415':\t'E',\tu'\\u0435':\t'e',\tu'\\u0416':\t'Zh',\tu'\\u0436':\t'zh',\tu'\\u0417':\t'Z',\tu'\\u0437':\t'z',\tu'\\u0418':\t'I',\tu'\\u0438':\t'i',\tu'\\u0419':\t'I',\tu'\\u0439':\t'i',\tu'\\u041a':\t'K',\tu'\\u043a':\t'k',\tu'\\u041b':\t'L',\tu'\\u043b':\t'l',\tu'\\u041c':\t'M',\tu'\\u043c':\t'm',\tu'\\u041d':\t'N',\tu'\\u043d':\t'n',\tu'\\u041e':\t'O',\tu'\\u043e':\t'o',\tu'\\u041f':\t'P',\tu'\\u043f':\t'p',\tu'\\u0420':\t'R',\tu'\\u0440':\t'r',\tu'\\u0421':\t'S',\tu'\\u0441':\t's',\tu'\\u0422':\t'T',\tu'\\u0442':\t't',\tu'\\u0423':\t'U',\tu'\\u0443':\t'u',\tu'\\u0424':\t'F',\tu'\\u0444':\t'f',\tu'\\u0425':\t'Kh',\tu'\\u0445':\t'kh',\tu'\\u0426':\t'Ts',\tu'\\u0446':\t'ts',\tu'\\u0427':\t'Ch',\tu'\\u0447':\t'ch',\tu'\\u0428':\t'Sh',\tu'\\u0448':\t'sh',\tu'\\u0429':\t'Shch',\tu'\\u0449':\t'shch',\tu'\\u042a':\t'\"',\tu'\\u044a':\t'\"',\tu'\\u042b':\t'Y',\tu'\\u044b':\t'y',\tu'\\u042c':\t\"'\",\tu'\\u044c':\t\"'\",\tu'\\u042d':\t'E',\tu'\\u044d':\t'e',\tu'\\u042e':\t'Iu',\tu'\\u044e':\t'iu',\tu'\\u042f':\t'Ia',\tu'\\u044f':\t'ia'}\t \n 290 Now that we have the transliteration dictionary, we can simply loop through every character in the source page and convert those Unicode characters in the dictionary. If we turn it into a procedure, then we can reuse it for other webpages. def\ttransliterate(word,\ttranslit_table):\t\t\t\t\tconverted_word\t=\t''\t\t\t\t\tfor\tchar\tin\tword:\t\t\t\t\t\t\t\t\ttranschar\t=\t''\t\t\t\t\t\t\t\t\tif\tchar\tin\ttranslit_table:\t\t\t\t\t\t\t\t\t\t\t\t\ttranschar\t=\ttranslit_table[char]\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\ttranschar\t=\tchar\t\t\t\t\t\t\t\t\tconverted_word\t+=\ttranschar\t\t\t\t\treturn\tconverted_word\tWe can then call this function using the newly created dictionary and the webpage downloaded earlier. #we\twill\trun\tit\twith\tthe\tcyrillic_translit\tdictionary\tand\tthe\twebpage\tconverted_content\t=\ttransliterate(content,\tcyrillic_translit)\tconverted_content[200:310]\tHere is what we end up with: u'=\"list-right\">\\r\\n</li>\t\t\t\t\t<li>\t<p\tclass=\"name\"><a\tname=\"n1\"></a>A-Aku\tTulikovich</p>\t<p\tclass=\"cont\">\\r\\nRodilsia\\xa0v\t1913\tg.'</p>\tStill not perfect. Python did not convert the special character ‘\\xa0′ that signifies a non-breaking space. But with the transliteration dictionary, any characters that pop up can just be added to the dictionary and they will be converted. First we need to find out what that character is. We could search for it on the Internet or we can just print it: #let's\tfind\tout\twhat\tu'\\xa0'\tis\tprint\tu'\\xa0'\t\t#it's\tnot\tnothing\tbut\ta\tnon-breaking\tspace\t#it\twould\tbe\tbetter\tif\tour\ttransliteration\tdictionary\tcould\tchange\tit\tinto\ta\tspace\t\tcyrillic_translit[u'\\xa0']\t=\t'\t'\tWith this fix, all the Cyrillic and special characters are gone, making it much easier to read the file and deal with it. For the last part of the lesson, we will modify methods used in the lesson “Intro to Beautiful Soup”326 to get a list of transliterated names from the webpage.                                                         326\tJeri\tWieringa,\t‘Intro\tto\tBeautiful\tSoup’,\tThe\tProgramming\tHistorian\t(2012).\t \n 291 Transliterated List of Names There may be cases where it is best to transliterate the entire file but if the goal is to transliterate and extract just a part of the data in the file, it would be best to extract first and transliterate later. That way Python will only transliterate a small part of the file rather than having to loop through the whole of the HTML. Speed is not a huge issue when dealing with a handful of web pages but Memorial’s site has thousands of pages. The difference between looping through thousands of whole pages and just looping through a small part of each of those pages can add up. But, of course, it would have been anti-climactic to have all the names before the transliteration dictionary and also more difficult for non-Cyrillic readers to understand the rest of the lesson. So now we need to find a way to get just the names from the page. Here is the first bit of HTML from the converted_content string, containing parts of two database entries: converted_content[200:1000]\tThis code prints out characters 200 to 1000 of the HTML, which happens to include the entire first entry and the beginning of the second: u'=\"list-right\">\\r\\n</li>\t\t\t\t\t<li>\t<p\tclass=\"name\"><a\tname=\"n1\"></a>A-Aku\tTulikovich</p>\t<p</li>\t\t\t\t\t<li>class=\"cont\">\\r\\nRodilsia\tv\t1913\tg.,\tKamchatskaia\tgub.,\tTigil\\'skii\tr-n,\tstoibishsha\tUtkholok;\tkoriak-kochevnik;\tmalogramotnyi;\tb/p;\t\\r\\n\\r\\n\tArestovan12\tnoiabria\t1938\tg.\\r\\n\tPrigovoren:\tKoriakskii\tokrsud\t8\taprelia\t1939\tg.,\tob</li>\t</ul>\t\t\t\tv.:\tpo\tst.\t58-2-8-9-10-11\tUK\tRSFSR.\\r\\n\tPrigovor:\t20\tlet.\tOpredeleniem\tVoen\t\tnoi\tkollegii\tVS\tSSSR\tot\t17\toktiabria\t1939\tg.\tmera\tsnizhena\tdo\t10\tlet.\\r\\nReabili\t\ttirovan\t15\tmarta\t1958\tg.\tReabilitirovan\topredeleniem\tVoennoi\tkollegii\tVS\tSSSR\\r\\\t\tn\t<p\tclass=\"author\">Istochnik:\tBaza\tdannykh\to\tzhertvakh\trepressii\tKamchatskoi</p>\tobl.\t<ul>\t\t\t\t\t<li>\\r\\n</li>\t\t\t\t\t<li>\t<p\tclass=\"name\"><a\tname=\"n2\"></a>Aab\tAvgust\tMikhailovich</p>\tp>\t<p\tclass=\"cont\">\\r\\nRodilsia\tv\t1899\tg.,\tSaratovskaia\tobl.,\tGrimm\ts.;\tnemets;</p>\tobrazovanie\tnachal\\'noe;'\t \n 292 Each entry includes lots of information: name (last, first and patronymic), date of birth, place of birth, profession, date of arrest, date of sentencing and so on. If we wanted the detailed information about each person, we would have to parse the page ourselves and extract that information using the string manipulation techniques from the lesson “Manipulating Strings in Python.”327 However, for just the names it will be quicker to use the HTML parsing module Beautiful Soup. If you have not installed Beautiful Soup, see “Installing Python Modules with pip”328 and read “Intro to Beautiful Soup”329 for an overview of how this tool works. In the transliterator module, we will load Beautiful Soup and then turn our converted page into a Beautiful Soup object. #load\tBeautiful\tSoup\tfrom\tbs4\timport\tBeautifulSoup\t\t#convert\tthe\tpage\tconverted_soup\t=\tBeautifulSoup(converted_content)\tThe lesson “Intro to Beautiful Soup” teaches how to grab sections of a web page by their tags. But we can also select sections of the page by attributes, HTML code that modifies elements. Looking at the HTML from this page, notice that the text of our names are enclosed in the tag <p class=“name”>. The class attribute allows the page’s Cascading Style Sheets (CSS)330 settings to change the look of all elements that share the “name” class at once. CSS itself is an important tool for web designers. For those interested in learning more on this aspect of CSS, I recommend Code Academy’s331 interactive lessons in its web fundamentals track. In mining data from the web, though, attributes like class give us a pattern to separate out certain values. What we want is to get the elements where the class attribute’s value is “name”. When dealing with most types of attributes, Beautiful Soup can select parts of the page using the same syntax as HTML. The class attribute makes things a little tricky because Python uses “class” to define new types of objects. Beautiful Soup gets around this by making us search for class followed by an underscore: class_=“value”. Beautiful Soup objects’ .find_all() method will generate a Python list of Beautiful Soup objects that match the HTML tags or attributes set as parameters. The method .get_text() extracts just the text from Beautiful Soup objects, so “\t<p\tclass=“name”><a\tname=“n1”></a>A-Aku\tTulikovich</p>\t“.get_text() will become “A-Aku Tulikovich”. We need to use                                                         327\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Manipulating\tStrings\tin\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t328\tFred\tGibbs\t‘Installing\tPython\tModules\twith\tpip’,\tThe\tProgramming\tHistorian\t(2013).\t329\tJeri\tWieringa,\t‘Intro\tto\tBeautiful\tSoup’,\tThe\tProgramming\tHistorian\t(2012).\t330\t‘CSS\tTutorial’,\tW3Schools:\thttp://www.w3schools.com/css/\t331\t‘CSS:\tCoding\twith\tStyle’,\tCode\tAcademy:\thttps://www.codecademy.com/courses/css-coding-with-style/0/1\t \n 293 .get_text() on each item in the list, then append it to a new list containing just the names: #creating\tthe\tfinal\tnames\tlist\tnames\t=\t[]\t\t#creating\tthe\tlist\twith\t.find_all()\tand\tlooping\tthrough\tit\tfor\tentry\tin\tconverted_soup.find_all(class_=\"name\"):\t\t\t\t\tnames.append(entry.get_text())\tTo make sure it worked, let’s check the number of names and then see if they look like we expect: #check\tthe\tnumber\tof\tnames\tlen(names)\t\t>\t190\t\t#see\tthe\tfirst\ttwenty\tnames\tin\tthe\tlist\tnames[:20]\t\t>\t[u'A-Aku\tTulikovich\t',\tu'Aab\tAvgust\tMikhailovich',\tu'Aab\tAvgust\tKhristianovich',\tu'Aab\tAleksandr\tAleksandrovich',\tu\"Aab\tAleksandr\tKhrist'ianovich\",\tu\"Aab\tAl'bert\tViktorovich\",\tu\"Aab\tAl'brekht\tAleksandrovich\",\tu'Aab\tAmaliia\tAndreevna',\tu'Aab\tAmaliia\tIvanovna',\tu'Aab\tAngelina\tAndreevna',\tu'Aab\tAndrei\tAndreevich',\tu'Aab\tAndrei\tFilippovich',\tu'Aab\tArvid\tKarlovich',\tu\"Aab\tArnol'd\tAleksandrovich\",\tu'Aab\tArtur\tAvgustovich',\tu\"Aab\tArtur\tVil'gel'movich\",\tu\"Aab\tAelita\tArnol'dovna\",\tu'Aab\tViktor\tAleksandrovich',\tu'Aab\tViktor\tAleksandrovich',\tu\"Aab\tViktor\tVil'gel'movich\"]\tThe ‘u’ in front of each of the names indicates that they are unicode objects in Python, not strings. But when Python needs a string, it will automatically change any unicode to be a string if it only uses ASCII characters or else throw a “unicodedecode error”. Fortunately, because we have transliterated all the Cyrillic characters, this list fits Python’s needs. If we had not parsed the transliterated page, that would be easy to handle with the transliterate function from earlier. All it would take is to use the transliterate function on the text from each item in the list before appending it to the final list. Transliteration can only do so much. Except for proper names, it can tell you little about the content of the source being transliterated. Yet the ability to transliterate automatically is of great use when dealing with lots of names or for people who prefer or need to use ASCII characters. It is a simple tool but one that can be an enormous time saver. About the Author Seth Bernstein is a postdoctoral fellow at the Higher School of Economics in Moscow. He defended his doctoral dissertation, ‘Communist Upbringing under Stalin: The Political Socialization and Militarization of Soviet Youths, 1934-1941’ at the University of Toronto, in 2013.  \n 294 Part Four: Analyzing Data You have your materials, you’ve cleaned them up. Now you want to see what they can tell you. The lessons in this section deal with different approaches to analyzing historical data. This is where research meets computation. What comes out the other side is up to you.  \n 295 30. Counting Word Frequencies with Python William J. Turkel and Adam Crymble – 2012    Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Normalizing Textual Data with Python’.332  Lesson Goals Your list is now clean enough that you can begin analyzing its contents in meaningful ways. Counting the frequency of specific words in the list can provide illustrative data. Python has an easy way to count frequencies, but it requires the use of a new type of variable: the dictionary. Before you begin working with a dictionary, consider the processes used to calculate frequencies in a list. Files Needed For This Lesson obo.py If you do not have these files, you can download a (zip - http://programminghistorian.org/assets/programming-historian3.zip) file from the previous lesson. Frequencies Now we want to count the frequency of each word in our list. You’ve already seen that it is easy to process a list by using a for loop. Try saving and executing the following example. Recall that += tells the program to append something to the end of an existing variable.                                                             332\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Normalizing\tTextual\tData\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 296 #\tcount-list-items-1.py\t\twordstring\t=\t'it\twas\tthe\tbest\tof\ttimes\tit\twas\tthe\tworst\tof\ttimes\t'\twordstring\t+=\t'it\twas\tthe\tage\tof\twisdom\tit\twas\tthe\tage\tof\tfoolishness'\t\twordlist\t=\twordstring.split()\t\twordfreq\t=\t[]\tfor\tw\tin\twordlist:\t\t\t\t\twordfreq.append(wordlist.count(w))\t\tprint\t\"String\\n\"\t+\twordstring\t+\"\\n\"\tprint\t\"List\\n\"\t+\tstr(wordlist)\t+\t\"\\n\"\tprint\t\"Frequencies\\n\"\t+\tstr(wordfreq)\t+\t\"\\n\"\tprint\t\"Pairs\\n\"\t+\tstr(zip(wordlist,\twordfreq))\tHere, we start with a string and split it into a list, as we’ve done before. We then create an (initially empty) list called wordfreq, go through each word in the wordlist, and count the number of times that word appears in the whole list. We then add each word's count to our wordfreq list. Using the zip operation, we are able to match the first word of the word list with the first number in the frequency list, the second word and second frequency, and so on. We end up with a list of word and frequency pairs. The str function converts any object to a string so that it can be printed. You should get something like this: String\tit\twas\tthe\tbest\tof\ttimes\tit\twas\tthe\tworst\tof\ttimes\tit\twas\tthe\tage\tof\twisdom\tit\twas\tthe\tage\tof\tfoolishness\t\tList\t['it',\t'was',\t'the',\t'best',\t'of',\t'times',\t'it',\t'was',\t'the',\t'worst',\t'of',\t'times',\t'it',\t'was',\t'the',\t'age',\t'of',\t'wisdom',\t'it',\t'was',\t'the',\t'age',\t'of',\t'foolishness']\t\tFrequencies\t[4,\t4,\t4,\t1,\t4,\t2,\t4,\t4,\t4,\t1,\t4,\t2,\t4,\t4,\t4,\t2,\t4,\t1,\t4,\t4,\t4,\t2,\t4,\t1]\t\tPairs\t[('it',\t4),\t('was',\t4),\t('the',\t4),\t('best',\t1),\t('of',\t4),\t('times',\t2),\t('it',\t4),\t('was',\t4),\t('the',\t4),\t('worst',\t1),\t('of',\t4),\t('times',\t2),\t('it',\t4),\t('was',\t4),\t('the',\t4),\t('age',\t2),\t('of',\t4),\t('wisdom',\t1),\t('it',\t4),\t('was',\t4),\t('the',\t4),\t('age',\t2),\t('of',\t4),\t('foolishness',\t1)]\tIt will pay to study the above code until you understand it before moving on. Python also includes a very convenient tool called a list\t \n 297 comprehension,333 which can be used to do the same thing as the for loop more economically. #\tcount-list-items-1.py\t\twordstring\t=\t'it\twas\tthe\tbest\tof\ttimes\tit\twas\tthe\tworst\tof\ttimes\t'\twordstring\t+=\t'it\twas\tthe\tage\tof\twisdom\tit\twas\tthe\tage\tof\tfoolishness'\twordlist\t=\twordstring.split()\t\twordfreq\t=\t[wordlist.count(w)\tfor\tw\tin\twordlist]\t#\ta\tlist\tcomprehension\t\tprint\t\"String\\n\"\t+\twordstring\t+\"\\n\"\tprint\t\"List\\n\"\t+\tstr(wordlist)\t+\t\"\\n\"\tprint\t\"Frequencies\\n\"\t+\tstr(wordfreq)\t+\t\"\\n\"\tprint\t\"Pairs\\n\"\t+\tstr(zip(wordlist,\twordfreq))\tIf you study this list comprehension carefully, you will discover that it does exactly the same thing as the for loop in the previous example, but in a condensed manner. Either method will work fine, so use the version that you are most comfortable with. At this point we have a list of pairs, where each pair contains a word and its frequency. This list is a bit redundant. If 'the' occurs 500 times, then this list contains five hundred copies of the pair ('the', 500). The list is also ordered by the words in the original text, rather than listing the words in order from most to least frequent. We can solve both problems by converting it into a dictionary, then printing out the dictionary in order from the most to the least commonly occurring item. Python Dictionaries Both strings and lists are sequentially ordered, which means that you can access their contents by using an index, a number that starts at 0. If you have a list containing strings, you can use a pair of indexes to access first a particular string in the list, and then a particular character within that string. Study the examples below. s\t=\t'hello\tworld'\tprint\ts[0]\t->\th\t\tprint\ts[1]\t->\te\t\tm\t=\t['hello',\t'world']\tprint\tm[0]\t->\thello\t\tprint\tm[1]\t->\tworld\t\t                                                        333\t‘List\tComprehensions’,\tPython:\thttps://docs.python.org/2/tutorial/datastructures.html#list-comprehensions\t \n 298 print\tm[0][1]\t->\te\t\tprint\tm[1][0]\t->\tw\tTo keep track of frequencies, we’re going to use another type of Python object, a dictionary. The dictionary is an unordered collection of objects. That means that you can't use an index to retrieve elements from it. You can, however, look them up by using a key (hence the name \"dictionary\"). Study the following example. d\t=\t{'world':\t1,\t'hello':\t0}\tprint\td['hello']\t->\t0\t\tprint\td['world']\t->\t1\t\tprint\td.keys()\t->\t['world',\t'hello']\tDictionaries might be a bit confusing to a new programmer. Try to think of it like a language dictionary. If you don’t know (or remember) exactly how \"bijection\" differs from \"surjection\" you can look the two terms up in the Oxford English Dictionary. The same principle applies when you print\td['hello']; except, rather than print a literary definition it prints the value associated with the keyword 'hello', as defined by you when you created the dictionary named d. In this case, that value is \"0\". Note that you use curly braces to define a dictionary, but square brackets to access things within it. The keys operation returns a list of keys that are defined in the dictionary. Word-Frequency Pairs Building on what we have so far, we want a function that can convert a list of words into a dictionary of word-frequency pairs. The only new command that we will need is dict, which makes a dictionary from a list of pairs. Copy the following and add it to the obo.py module. #\tGiven\ta\tlist\tof\twords,\treturn\ta\tdictionary\tof\t#\tword-frequency\tpairs.\t\tdef\twordListToFreqDict(wordlist):\t\t\t\t\twordfreq\t=\t[wordlist.count(p)\tfor\tp\tin\twordlist]\t\t\t\t\treturn\tdict(zip(wordlist,wordfreq))\tWe are also going to want a function that can sort a dictionary of word-frequency pairs by descending frequency. Copy this and add it to the obo.py module, too.   \n 299 #\tSort\ta\tdictionary\tof\tword-frequency\tpairs\tin\t#\torder\tof\tdescending\tfrequency.\t\tdef\tsortFreqDict(freqdict):\t\t\t\t\taux\t=\t[(freqdict[key],\tkey)\tfor\tkey\tin\tfreqdict]\t\t\t\t\taux.sort()\t\t\t\t\taux.reverse()\t\t\t\t\treturn\taux\tWe can now write a program which takes a URL and returns word-frequency pairs for the web page, sorted in order of descending frequency. Copy the following program into Komodo Edit, save it as html-to-freq.py and execute it. Study the program and its output carefully before continuing. #html-to-freq.py\t\timport\turllib2,\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\thtml\t=\tresponse.read()\ttext\t=\tobo.stripTags(html).lower()\twordlist\t=\tobo.stripNonAlphaNum(text)\tdictionary\t=\tobo.wordListToFreqDict(wordlist)\tsorteddict\t=\tobo.sortFreqDict(dictionary)\t\tfor\ts\tin\tsorteddict:\tprint\tstr(s)\tRemoving Stop Words When we look at the output of our html-to-freq.py program, we see that a lot of the most frequent words in the text are function words like \"the\", \"of\", \"to\" and \"and\". (192,\t'the')\t(105,\t'i')\t(74,\t'to')\t(71,\t'was')\t(67,\t'of')\t(62,\t'in')\t(53,\t'a')\t(52,\t'and')\t(50,\t'you')\t(50,\t'he')\t(40,\t'that')\t(39,\t'his')\t(36,\t'it')\tThese words are usually the most common in any English language text, so they don't tell us much that is distinctive about Bowsey's trial. In general, we are more interested in finding the words that will help us differentiate this text from texts that are about different subjects. So we're going to filter out the common function words. Words that are ignored like this are known as stop words. We’re going to use the following list, adapted from one  \n 300 posted online by computer scientists at Glasgow.334 Copy it and put it at the beginning of the obo.py library that you are building. stopwords\t=\t['a',\t'about',\t'above',\t'across',\t'after',\t'afterwards']\tstopwords\t+=\t['again',\t'against',\t'all',\t'almost',\t'alone',\t'along']\tstopwords\t+=\t['already',\t'also',\t'although',\t'always',\t'am',\t'among']\tstopwords\t+=\t['amongst',\t'amoungst',\t'amount',\t'an',\t'and',\t'another']\tstopwords\t+=\t['any',\t'anyhow',\t'anyone',\t'anything',\t'anyway',\t'anywhere']\tstopwords\t+=\t['are',\t'around',\t'as',\t'at',\t'back',\t'be',\t'became']\tstopwords\t+=\t['because',\t'become',\t'becomes',\t'becoming',\t'been']\tstopwords\t+=\t['before',\t'beforehand',\t'behind',\t'being',\t'below']\tstopwords\t+=\t['beside',\t'besides',\t'between',\t'beyond',\t'bill',\t'both']\tstopwords\t+=\t['bottom',\t'but',\t'by',\t'call',\t'can',\t'cannot',\t'cant']\tstopwords\t+=\t['co',\t'computer',\t'con',\t'could',\t'couldnt',\t'cry',\t'de']\tstopwords\t+=\t['describe',\t'detail',\t'did',\t'do',\t'done',\t'down',\t'due']\tstopwords\t+=\t['during',\t'each',\t'eg',\t'eight',\t'either',\t'eleven',\t'else']\tstopwords\t+=\t['elsewhere',\t'empty',\t'enough',\t'etc',\t'even',\t'ever']\tstopwords\t+=\t['every',\t'everyone',\t'everything',\t'everywhere',\t'except']\tstopwords\t+=\t['few',\t'fifteen',\t'fifty',\t'fill',\t'find',\t'fire',\t'first']\tstopwords\t+=\t['five',\t'for',\t'former',\t'formerly',\t'forty',\t'found']\tstopwords\t+=\t['four',\t'from',\t'front',\t'full',\t'further',\t'get',\t'give']\tstopwords\t+=\t['go',\t'had',\t'has',\t'hasnt',\t'have',\t'he',\t'hence',\t'her']\tstopwords\t+=\t['here',\t'hereafter',\t'hereby',\t'herein',\t'hereupon',\t'hers']\tstopwords\t+=\t['herself',\t'him',\t'himself',\t'his',\t'how',\t'however']\tstopwords\t+=\t['hundred',\t'i',\t'ie',\t'if',\t'in',\t'inc',\t'indeed']\tstopwords\t+=\t['interest',\t'into',\t'is',\t'it',\t'its',\t'itself',\t'keep']\tstopwords\t+=\t['last',\t'latter',\t'latterly',\t'least',\t'less',\t'ltd',\t'made']\tstopwords\t+=\t['many',\t'may',\t'me',\t'meanwhile',\t'might',\t'mill',\t'mine']\tstopwords\t+=\t['more',\t'moreover',\t'most',\t'mostly',\t'move',\t'much']\tstopwords\t+=\t['must',\t'my',\t'myself',\t'name',\t'namely',\t'neither',\t'never']\tstopwords\t+=\t['nevertheless',\t'next',\t'nine',\t'no',\t'nobody',\t'none']\tstopwords\t+=\t['noone',\t'nor',\t'not',\t'nothing',\t'now',\t'nowhere',\t'of']\tstopwords\t+=\t['off',\t'often',\t'on','once',\t'one',\t'only',\t'onto',\t'or']\tstopwords\t+=\t['other',\t'others',\t'otherwise',\t'our',\t'ours',\t'ourselves']\tstopwords\t+=\t['out',\t'over',\t'own',\t'part',\t'per',\t'perhaps',\t'please']\tstopwords\t+=\t['put',\t'rather',\t're',\t's',\t'same',\t'see',\t'seem',\t'seemed']\tstopwords\t+=\t['seeming',\t'seems',\t'serious',\t'several',\t'she',\t'should']\tstopwords\t+=\t['show',\t'side',\t'since',\t'sincere',\t'six',\t'sixty',\t'so']\tstopwords\t+=\t['some',\t'somehow',\t'someone',\t'something',\t'sometime']\tstopwords\t+=\t['sometimes',\t'somewhere',\t'still',\t'such',\t'system',\t'take']\tstopwords\t+=\t['ten',\t'than',\t'that',\t'the',\t'their',\t'them',\t'themselves']\tstopwords\t+=\t['then',\t'thence',\t'there',\t'thereafter',\t'thereby']\tstopwords\t+=\t['therefore',\t'therein',\t'thereupon',\t'these',\t'they']\tstopwords\t+=\t['thick',\t'thin',\t'third',\t'this',\t'those',\t'though',\t'three']\tstopwords\t+=\t['three',\t'through',\t'throughout',\t'thru',\t'thus',\t'to']\tstopwords\t+=\t['together',\t'too',\t'top',\t'toward',\t'towards',\t'twelve']\tstopwords\t+=\t['twenty',\t'two',\t'un',\t'under',\t'until',\t'up',\t'upon']\tstopwords\t+=\t['us',\t'very',\t'via',\t'was',\t'we',\t'well',\t'were',\t'what']\tstopwords\t+=\t['whatever',\t'when',\t'whence',\t'whenever',\t'where']\tstopwords\t+=\t['whereafter',\t'whereas',\t'whereby',\t'wherein',\t'whereupon']\tstopwords\t+=\t['wherever',\t'whether',\t'which',\t'while',\t'whither',\t'who']\tstopwords\t+=\t['whoever',\t'whole',\t'whom',\t'whose',\t'why',\t'will',\t'with']\tstopwords\t+=\t['within',\t'without',\t'would',\t'yet',\t'you',\t'your']\tstopwords\t+=\t['yours',\t'yourself',\t'yourselves']\tNow getting rid of the stop words in a list is as easy as using another list comprehension. Add this function to the obo.py module, too.                                                           334\t‘stop\twords’,\thttp://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\t \n 301 #\tGiven\ta\tlist\tof\twords,\tremove\tany\tthat\tare\t#\tin\ta\tlist\tof\tstop\twords.\t\tdef\tremoveStopwords(wordlist,\tstopwords):\t\t\t\t\treturn\t[w\tfor\tw\tin\twordlist\tif\tw\tnot\tin\tstopwords]\tPutting it All Together Now we have everything we need to determine word frequencies for web pages. Copy the following to Komodo Edit, save it as html-to-freq-2.py and execute it. #\thtml-to-freq-2.py\t\timport\turllib2\timport\tobo\t\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\tresponse\t=\turllib2.urlopen(url)\thtml\t=\tresponse.read()\ttext\t=\tobo.stripTags(html).lower()\tfullwordlist\t=\tobo.stripNonAlphaNum(text)\twordlist\t=\tobo.removeStopwords(fullwordlist,\tobo.stopwords)\tdictionary\t=\tobo.wordListToFreqDict(wordlist)\tsorteddict\t=\tobo.sortFreqDict(dictionary)\t\tfor\ts\tin\tsorteddict:\tprint\tstr(s)\tIf all went well, your output should look like this: (25,\t'house')\t(20,\t'yes')\t(20,\t'prisoner')\t(19,\t'mr')\t(17,\t'man')\t(15,\t'akerman')\t(14,\t'mob')\t(13,\t'black')\t(12,\t'night')\t(11,\t'saw')\t(9,\t'went')\t(9,\t'sworn')\t(9,\t'room')\t(9,\t'pair')\t(9,\t'know')\t(9,\t'face')\t(8,\t'time')\t(8,\t'thing')\t(8,\t'june')\t(8,\t'believe')\t...\t \n 302 Suggested Readings Lutz, Learning Python335 Ch. 9: Tuples, Files, and Everything Else Ch. 11: Assignment, Expressions, and print Ch. 12: if Tests Ch. 13: while and for Loops Pilgrim, Diving into Python336 Ch. 7: Regular Expressions Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each chapter you can download the \"programming-historian\" zip file to make sure you have the correct code. programming-historian-3   zip: http://programminghistorian.org/assets/programming-historian3.zip  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Creating and Viewing HTML Files with Python’.337 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        335\tMark\tLutz,\tLearning\tPython\t(5th\tedition),\tO’Reilly:\t2013.\t336\tPilgrim,\t‘Regular\tExpressions’,\tDiving\tinto\tPython:\thttp://www.diveintopython.net/regular_expressions/index.html\t337\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Creating\tand\tViewing\tHTML\tFiles\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 303 31. Counting Frequencies from Zotero Items Spencer Roberts – 2013  Editor’s Note: This is the third of three lessons on the ‘Zotero API’. You may find it easier to complete this tutorial if you have already completed the previous one: ‘Adding New Items to Zotero.338  Lesson Goals In ‘Counting Word Frequencies with Python’339 you learned how to count the frequency of specific words in a list using python. In this lesson, we will expand on that topic by showing you how to get information from Zotero HTML items, save the content from those items, and count the frequencies of words. It may be beneficial to look over the previous lesson before we begin. Files Needed For This Lesson obo.py If you do not have these files, you can download a zip file [http://programminghistorian.org/assets/programming-historian3.zip]. Modifying the obo.py Module Before we begin, we need to adjust obo.py in order to use this module to interact with different html files. The stripTags function in the obo.py module must be updated to the following, because it was previously designed for Old Bailey Online340 content only. First, we need to remove the line that instructs the program to begin at the end of the header, then we will tell it where to begin. Open the obo.py file in your text editor and follow the instructions below:                                                          338\tAmanda\tMorton,\t‘Adding\tNew\tItems\tto\tZotero’,\tthe\tProgramming\tHistorian\t(2013).\t339\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t340\tTim\tHitchcock,\tRobert\tShoemaker,\tClive\tEmsley,\tSharon\tHoward\tand\tJamie\tMcLaughlin,\tet\tal.,\tThe\tOld\tBailey\tProceedings\tOnline,\t1674-1913\t(www.oldbaileyonline.org,\tversion\t7.0,\t24\tMarch\t2012).\t \n 304 \tdef\tstripTags(pageContents):\t\t\t\t\t#remove\tthe\tfollowing\tline\t\t\t\t\t#startLoc\t=\tpageContents.find(\"<hr/><h2>\")\t\t\t\t\t\t#modify\tthe\tfollowing\tline\t\t\t\t\t#pageContents\t=\tpageContents[startLoc:]\t\t\t\t\t#so\tthat\tit\tlooks\tlike\tthis\t\t\t\t\tpageContents\t=\tpageContents[0:]\t\t\t\t\t\tinside\t=\t0\t\t\t\t\ttext\t=\t'\t'\t\t\t\t\tfor\tchar\tin\tpageContents:\t\t\t\t\t\t\t\t\tif\tchar\t==\t'<':\t\t\t\t\t\t\t\t\t\t\t\t\tinside\t=\t1\t\t\t\t\t\t\t\t\telif\t(inside\t==\t1\tand\tchar\t=='>'):\t\t\t\t\t\t\t\t\t\t\t\t\tinside\t=\t0\t\t\t\t\t\t\t\t\telif\tinside\t==\t1:\t\t\t\t\t\t\t\t\t\t\t\t\tcontinue\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\ttext\t+=\tchar\t\t\t\t\t\treturn\ttext\t\tRemember to save your changes before we continue. Get Items from Zotero and Save Local Copy After we have modified the obo.py file, we can create a program designed to request the top two items from a collection within a Zotero library, retrieve their associated URLs, read the web pages, and save the content to a local copy. This particular program will only work on webpage-type items with html content (for instance, entering the URLs of JSTOR or Google Books pages will not result in an analysis of the actual content). First, create a new .py file and save it in your programming historian directory. Make sure your copy of the obo.py file is in the same location. Once you have saved your file, we can begin by importing the libraries and program data we will need to run this program: #Get\turls\tfrom\tZotero\titems,\tcreate\tlocal\tcopy,\tcount\tfrequencies\timport\tobo\tfrom\tlibZotero\timport\tzotero\timport\turllib2\t\tNext, we need to tell our program where to find the items we will be using in our analysis. Using the sample Zotero library from which we retrieved items in Amanda Morton’s lesson ‘Intro to the Zotero API’,341 or using your personal library, we will pull the first two top-level items from either the library or from a specific collection within the library. (To find your collection key, mouseover the RSS button on that collection’s page and use the second alpha-numeric sequence in the URL. If you are trying to connect                                                         341\tAmanda\tMorton,\t‘Intro\tto\tthe\tZotero\tAPI’,\tThe\tProgramming\tHistorian\t(2013).\t \n 305 to an individual user library, you must change the word group to the word user, replace the six-digit number with your user ID, and insert your own API key.) #links\tto\tZotero\tlibrary\tzlib\t=\tzotero.Library('group',\t'155975',\t'<null>',\t'f4Bfk3OTYb7bukNwfcKXKNLG')\t\t#specifies\tsubcollection\t-\tleave\tblank\tto\tuse\twhole\tlibrary\tcollectionKey\t=\t'I253KRDT'\t\t#retrieves\ttop\ttwo\titems\tfrom\tlibrary\titems\t=\tzlib.fetchItemsTop({'limit':\t2,\t'collectionKey':\tcollectionKey,\t'content':\t'json,bib,coins'})\t\tNow we can instruct our program to retrieve the URL from each of our items, create a filename using that URL, and save a copy of the html on the page. #retrieves\turl\tfrom\teach\titem,\tcreates\ta\tfilename\tfrom\tthe\turl,\t\t#saves\ta\tlocal\tcopy\tfor\titem\tin\titems:\t\t\t\t\turl\t=\titem.get('url')\t\t\t\t\tfilename\t=\turl.split('/')[-1]\t+\t'.html'\t\t\t\t\t\t\t\t\t\t\t\t#splits\turl\tat\tlast\t/\t\t\t\t\tfilename\t=\tfilename.split('=')[-1]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#splits\turl\tat\tlast\t=\t\t\t\t\tfilename\t=\tfilename.replace('.html.html',\t'.html')\t#removes\tdouble\t.html\t\t\t\t\tprint\t'Saving\tlocal\tcopy\tof\t'\t+\tfilename\t\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\twebContent\t=\tresponse.read()\t\t\t\t\tf\t=\topen(filename,'w')\t\t\t\t\tf.write(webContent)\t\t\t\t\tf.close()\tRunning this portion of the program will result in the following: Saving\tlocal\tcopy\tof\tPastsFutures.html\tSaving\tlocal\tcopy\tof\t29.html\t\tGet Item URLs from Zotero and Count Frequencies Now that we've retrieved our items and created local html files, we can use the next portion of our program to retrieve the URLs, read the web pages, create a list of words, count their frequencies, and display them. Most of this should be familiar to you from the ‘Counting Word Frequencies with Python’ lesson.342 \n                                                        342\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 306 \t#retrieves\turl\tfrom\teach\titem,\tcreates\ta\tfilename\tfrom\tthe\turl\tfor\titem\tin\titems:\t\t\t\t\titemTitle\t=\titem.get('title')\t\t\t\t\turl\t=\titem.get('url')\t\t\t\t\tfilename\t=\turl.split('/')[-1]\t+\t'.html'\t\t\t\t\t\t\t\t\t\t\t\t#splits\turl\tat\tlast\t/\t\t\t\t\tfilename\t=\tfilename.split('=')[-1]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#splits\turl\tat\tlast\t=\t\t\t\t\tfilename\t=\tfilename.replace('.html.html',\t'.html')\t#removes\tdouble\t.html\t\t\t\t\tprint\t'\\n'\t+\titemTitle\t+'\\nFilename:\t'\t+\tfilename\t+\t'\\nWord\tFrequencies\\n'\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\thtml\t=\tresponse.read()\t\t\t\t\t\tThis section of code grabs the URL from our items, removes the unnecessary portions, and creates and prints a filename. For the items in our sample collection, the output looks something like this: \tThe\tPasts\tand\tFutures\tof\tDigital\tHistory\tFilename:\tPastsFutures.html\tWord\tFrequencies\t\tHistory\tand\tthe\tWeb,\tFrom\tthe\tIllustrated\tNewspaper\tto\tCyberspace:\tVisual\tTechnologies\tand\tInteraction\tin\tthe\tNineteenth\tand\tTwenty-First\tCenturies\tFilename:\t29.html\tWord\tFrequencies\t\tNow we can go ahead and create our list of words and their frequencies. Enter the following: #strips\tHTML\ttags,\tstrips\tnonAlpha\tcharacters,\tremoves\tstopwords\t\t\t\t\ttext\t=\tobo.stripTags(html).lower()\t\t\t\t\tfullwordlist\t=\tobo.stripNonAlphaNum(text)\t\t\t\t\twordlist\t=\tobo.removeStopwords(fullwordlist,\tobo.stopwords)\t\t#counts\tfrequencies\t\t\t\t\tdictionary\t=\tobo.wordListToFreqDict(wordlist)\t\t\t\t\tsorteddict\t=\tobo.sortFreqDict(dictionary)\t\t#displays\tlist\tof\twords\tand\tfrequencies\t\t\t\t\tfor\ts\tin\tsorteddict:\tprint\tstr(s)\tYour final output will include a long list of words accompanied by their frequency within the html file:  \n 307 \tSaving\tlocal\tcopy\tof\tPastsFutures.html\tSaving\tlocal\tcopy\tof\t29.html\t\tThe\tPasts\tand\tFutures\tof\tDigital\tHistory\tFilename:\tPastsFutures.html\tWord\tFrequencies\t\t(51,\t'history')\t(43,\t'new')\t(31,\t'9')\t(27,\t'historians')\t(24,\t'digital')\t(23,\t'social')\t(21,\t'narrative')\t(16,\t'media')\t(15,\t'time')\t(13,\t'possibilities')\t(13,\t'past')\t(12,\t'science')\t...\t\tHistory\tand\tthe\tWeb,\tFrom\tthe\tIllustrated\tNewspaper\tto\tCyberspace:\tVisual\tTechnologies\tand\tInteraction\tin\tthe\tNineteenth\tand\tTwenty-First\tCenturies\tFilename:\t29.html\tWord\tFrequencies\t\t(52,\t'new')\t(49,\t'history')\t(46,\t'media')\t(44,\t'ndash')\t(34,\t'figure')\t(34,\t'digital')\t(24,\t'visual')\t(24,\t'museum')\t(24,\t'http')\t(23,\t'edu')\t(22,\t'web')\t(22,\t'text')\t(22,\t'barnum')\t(21,\t'users')\t(21,\t'information')\t...\tAbout the Author Spencer Roberts is a Research Assistant and former Digital History Research Fellow at the Roy Rosenzweig Center for History and New Media, and a PhD graduate student at George Mason University in the Department of History.   \n 308 32. Getting Started with Topic Modeling and MALLET Shawn Graham, Scott Weingart, and Ian Milligan – 2012 Editor's Note This lesson requires you to use the command line. If you have no previous experience using the command line you may find it helpful to work through the Programming Historian Bash Command Line lesson.343 Lesson Goals In this lesson you will first learn what topic modeling is and why you might want to employ it in your research. You will then learn how to install and work with the MALLET natural language processing toolkit to do so. MALLET involves modifying an environment variable (essentially, setting up a short-cut so that your computer always knows where to find the MALLET program) and working with the command line (ie, by typing in commands manually, rather than clicking on icons or menus). We will run the topic modeller on some example files, and look at the kinds of outputs that MALLET installed. This will give us a good idea of how it can be used on a corpus of texts to identify topics found in the documents without reading them individually. Please see the MALLET users' discussion list344 for the full range of things one can do with the software. (We would like to thank Robert Nelson and Elijah Meeks for hints and tips in getting MALLET to run for us the first time, and for their examples of what can be done with this tool.) What is Topic Modeling And For Whom is this Useful? A topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary. Before you begin with topic modeling, you should ask yourself whether or not it is likely to be useful for your project. Matthew Kirschenbaum's Distant Reading345 (a talk given at the 2009 National Science Foundation Symposium on the Next Generation of Data Mining and Cyber-Enabled Discovery for Innovation) and Stephen Ramsay's                                                         343\tIan\tMilligan\tand\tJames\tBaker,\t‘Introduction\tto\tthe\tBash\tCommand\tLine’,\tThe\tProgramming\tHistorain\t(2014).\t344\t‘The\tMallet\tDevelopment\tMailling\tList’:\thttp://mallet.cs.umass.edu/mailinglist.php\t345\tMatthew\tG.\tKirschenbaum,\t‘The\tRemaking\tof\tReading:\tData\tMining\tand\tthe\tDigital\tHumanities’:\thttp://www.csee.umbc.edu/~hillol/NGDM07/abstracts/talks/MKirschenbaum.pdf\t \n 309 Reading Machines346 are good places for beginning to understand in which circumstances a technique such as this could be most effective. As with all tools, just because you can use it, doesn't necessarily mean that you should. If you are working with a small number of documents (or even a single document) it may well be that simple frequency counts are sufficient, in which case something like Voyant Tools347 might be appropriate. However, if you have hundreds of documents from an archive and you wish to understand something of what the archive contains without necessarily reading every document, then topic modeling might be a good approach. Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry – that is, any kind of unstructured text. By unstructured we mean that there are no computer-readable annotations that tell the computer the semantic meaning of the words in the text. Topic modeling programs do not know anything about the meaning of the words in a text. Instead, they assume that any piece of text is composed (by an author) by selecting words from possible baskets of words where each basket corresponds to a topic. If that is true, then it becomes possible to mathematically decompose a text into the probable baskets from whence the words first came. The tool goes through this process over and over again until it settles on the most likely distribution of words into baskets, which we call topics. There are many different topic modeling programs available; this tutorial uses one called MALLET. If one used it on a series of political speeches for example, the program would return a list of topics and the keywords composing those topics. Each of these lists is a topic according to the algorithm. Using the example of political speeches, the list might look like: Job Jobs Loss Unemployment Growth Economy Sector Economics Stock Banks Afghanistan War Troops Middle-East Taliban Terror Election Opponent Upcoming President et cetera By examining the keywords we can discern that the politician who gave the speeches was concerned with the economy, jobs, the Middle East, the upcoming election, and so on.                                                         346\tStephen\tRamsay,\tReading\tMachines:\tTowards\tan\tAlgorithmic\tCriticism\t(University\tof\tIllinois,\t2011).\t347\t‘Voyant\tTools’:\thttp://voyant-tools.org/\t \n 310 As Scott Weingart warns, there are many dangers that face those who use topic modeling without fully understanding it.348 For instance, we might be interested in word use as a proxy for placement along a political spectrum. Topic modeling could certainly help with that, but we have to remember that the proxy is not in itself the thing we seek to understand – as Andrew Gelman demonstrates in his mock study of zombies using Google Trends.349 Ted Underwood and Lisa Rhody (see Further Reading) argue that we as historians would be better to think of these categories as discourses; however for our purposes here we will continue to use the word: topic. Note: You will sometimes come across the term \"LDA\" when looking into the bibliography of topic modeling. LDA and Topic Model are often used synonymously, but the LDA technique is actually a special case of topic modeling created by David Blei and friends in 2002.350 It was not the first technique now considered topic modeling, but it is by far the most popular. The myriad variations of topic modeling have resulted in an alphabet soup of techniques and programs to implement them that might be confusing or overwhelming to the uninitiated; ignore them for now. They all work in much the same way. MALLET uses LDA. Examples of topic models employed by historians: Rob Nelson, Mining the Dispatch.351 Cameron Blevins, \"Topic Modeling Martha Ballard's Diary\" Historying, April 1, 2010.352 David J Newman and Sharon Block, \"Probabilistic topic decomposition of an eighteenth century American newspaper,\" Journal of the American Society for Information Science and Technology vol. 57, no. 6 (April 1, 2006): 753-767. Installing MALLET There are many tools one could use to create topic models, but at the time of this writing (summer 2012) the simplest tool to run your text through is called MALLET.353 MALLET uses an implementation of Gibbs sampling,354 a statistical technique meant to quickly construct a sample distribution, to create its topic models. MALLET requires using the command line – we'll                                                         348\tScott\tWeingart,\t‘The\tMyth\tof\tText\tAnalytics\tand\tUnobtusive\tMeasurement’\t(6\tMay\t2012):\thttp://www.scottbot.net/HIAL/?p=16713\t349\tAndrew\tGelman,\t‘How\tmany\tzombies\tdo\tyou\tknow?\t\tUsing\tindirect\tsurvey\tmethods\tto\tmeasure\talien\tattacks\tand\toutbreaks\tof\tthe\tundead’,\tarXiv:1003.6087:\thttp://arxiv.org/abs/1003.6087\t350\t‘Latent\tDirichlet\tallocation’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\t351\tRob\tNelson,\t‘Mining\tthe\tDispatch’:\thttp://dsl.richmond.edu/dispatch/\t352\tCameron\tBlevins,\t‘Topic\tModeling\tMartha\tBallard’s\tDiary’,\tHistorying\t(1\tApril\t2010):\thttp://www.cameronblevins.org/posts/topic-modeling-martha-ballards-diary/\t353\t‘Machine\tLearning\tfor\tLanguagE\tToolkit’:\thttp://mallet.cs.umass.edu/index.php\t354\t‘Gibbs\tsampling’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Gibbs_sampling\t \n 311 talk about that more in a moment, although you typically use the same few commands over and over. The installation instructions are different for Windows and Mac. Follow the instructions appropriate for you below: Windows Instructions Go to the MALLET project page, and download MALLET.355 (As of this writing, we are working with version 2.0.7.) You will also need the Java developer's kit356 – that is, not the regular Java that's on every computer, but the one that lets you program things. Install this on your computer. Unzip MALLET into your C: directory . This is important: it cannot be anywhere else. You will then have a directory called C:\\mallet-2.0.7 or similar. For simplicity's sake, rename this directory just mallet. MALLET uses an environment variable to tell the computer where to find all the various components of its processes when it is running. It's rather like a shortcut for the program. A programmer cannot know exactly where every user will install a program, so the programmer creates a variable in the code that will always stand in for that location. We tell the computer, once, where that location is by setting the environment variable. If you moved the program to a new location, you'd have to change the variable. To create an environment variable in Windows 7, click on your Start\tMenu\t->\tControl\tPanel\t->\tSystem\t->\tAdvanced\tSystem\tSettings (Figures 1,2,3). Click new and type MALLET_HOME in the variable name box. It must be like this – all caps, with an underscore – since that is the shortcut that the programmer built into the program and all of its subroutines. Then type the exact path (location) of where you unzipped MALLET in the variable value, e.g., c:\\mallet. To see if you have been successful, please read on to the next section. \n                                                         355\t‘Machine\tLearning\tfor\tLanguagE\tToolkit’:\thttp://mallet.cs.umass.edu/index.php\t356\t‘Java\tDeveloper’s\tKit’:\thttp://www.oracle.com/technetwork/java/javase/downloads/index.html\t\n \n 312 Advanced System Settings on Windows \n Environment Variables Location  \n Environment Variable  Running MALLET using the Command Line MALLET is run from the command line, also known as Command Prompt (Figure 4). If you remember MS-DOS, or have ever played with a Unix computer Terminal, this will be familiar. The command line is where you can type commands directly, rather than clicking on icons and menus. \n \n 313 \n Command Prompt on Windows Click on your Start\tMenu\t->\tAll\tPrograms\t->\tAccessories\t->\tCommand\tPrompt. You'll get the command prompt window, which will have a cursor at c:\\user\\user> (or similar; see Figure 4). Type cd\t.. (That is: cd-space-period-period) to change directory. Keep doing this until you're at the C:\\ . (as in Figure 5) \n Navigating to the C: Directory in Command Prompt  Then type cd\tmalletand you are in the MALLETdirectory. Anything you type in the command prompt window is a command. There are commands like cd (change directory) and dir (list directory contents) that the computer understands. You have to tell the computer explicitly that 'this is a MALLET command' when you want to use MALLET. You do this by telling the computer to grab its instructions from the MALLET bin, a subfolder in MALLET that contains the core operating routines. Type bin\\mallet as in Figure 6. If all has gone well, you should be presented with a list of MALLET commands – congratulations! If you get an error message, check your typing. Did you use the wrong slash? Did you set up the environment variable correctly? Is MALLET located at C:\\mallet ?  \n \n 314 \n Command Prompt MALLET Installed  You are now ready to skip ahead to the next section. Mac Instructions Many of the instructions for OS X installation are similar to Windows, with a few differences. In fact, it is a bit easier. Download and install MALLET (mallet-2.0.7.tar.gazas of Summer 2012).357 Download the Java Development Kit.358 Unzip MALLET into a directory on your system (for ease of following along with this tutorial, your /user/ directory works but anywhere is okay). Once it is unzipped, open up your Terminal window (in the Applications directory in your Finder. Navigate to the directory where you unzipped MALLET using the Terminal (it will be mallet-2.0.7 . If you unzipped it into your /user/ directory as was suggested in this lesson, you can navigate to the correct directory by typing cd\tmallet-2.0.7). cd is short for \"change directory\" when working in the Terminal. The same command will suffice to run commands from this directory, except you need to append ./ (period-slash) before each command. This needs to be done before all MALLET commands when working on a Mac. Going forward, the commands for MALLET on a Mac will be nearly identical to those on Windows, except for the direction of slashes (there are a few other minor differences that will be noted when they arise). If on Windows a command would be \\bin\\mallet, on a Mac you would instead type: ./bin/mallet\t                                                        357\t‘MAchine\tLearning\tfor\tLanguagE\tToolkit’:\thttp://mallet.cs.umass.edu/download.php\t358\t‘Java\tDevelopment\tKit’:\thttp://www.oracle.com/technetwork/java/javase/downloads/index.html\t\n \n 315 A list of commands should appear. If it does, congratulations – you've installed it correctly! Typing in MALLET Commands Now that you have MALLET installed, it is time to learn what commands are available to use with the program. There are nine MALLET commands you can use (see Figure 6 above). Sometimes you can combine multiple instructions. At the Command Prompt or Terminal (depending on your operating system), try typing: import-dir\t--help\tYou are presented with the error message that import-dir is not recognized as an internal or external command, operable program, or batch file. This is because we forgot to tell the computer to look in the MALLET bin for it. Try again, with bin\\mallet\timport-dir\t--help\tRemember, the direction of the slash matters (See Figure 7, which provides an entire transcript of what we have done so far in the tutorial). We checked to see that we had installed MALLET by typing in bin\\mallet. We then made the mistake with import-dir a few lines further down. After that, we successfully called up the help file, which told us what import-dir does, and it listed all of the potential parameters you can set for this tool. \n The help menu in MALLET Note: there is a difference in MALLET commands between a single hyphen and a double hyphen. A single hyphen is simply part of the name; it replaces a space (e.g., import-dir rather than import dir), since spaces offset multiple commands or parameters. These parameters let us tweak \n \n 316 the file that is created when we import our texts into MALLET. A double hyphen (as with –help above) modifies, adds a sub-command, or specifies some sort of parameter to the command. For Windows users, if you got the error 'exception in thread \"main\" java.lang.NoClassDefFoundError:' it might be because you installed MALLET somewhere other than in the C:\\ directory. For instance, installing MALLET at C:\\Program\tFiles\\mallet will produce this error message. The second thing to check is that your environment variable is set correctly. In either of these cases, check the Windows installation instructions and double check that you followed them properly. Working with data MALLET comes pre-packaged with sample .txt files with which you can practice. Type dir at the C:\\mallet>\tprompt, and you are given the listing of the MALLET directory contents. One of those directories is called sample-data. You know it is a directory because it has the word <dir> beside it. Type cd\tsample-data. Type dir again. Using what you know, navigate to first the web then the en directories. You can look inside these .txt files by typing the full name of the file (with extension). Note that you cannot now run any MALLET commands from this directory. Try it: bin\\mallet\timport-dir\t--help\tYou get the error message. You will have to navigate back to the main MALLET folder to run the commands. This is because of the way MALLET and its components are structured. Importing data In the sample\tdata directory, there are a number of .txt files. Each one of these files is a single document, the text of a number of different web pages. The entire folder can be considered to be a corpus of data. To work with this corpus and find out what the topics are that compose these individual documents, we need to transform them from several individual text files into a single MALLET format file. MALLET can import more than one file at a time. We can import the entire directory of text files using the import command. The commands below import the directory, turn it into a MALLET file, keep the original texts in the order in which they were listed, and strip out the stop words (words such as and, the, but, and if that occur in such frequencies that they obstruct analysis) using the default English stop-words dictionary. Try the following (swapping in the correct pathway to the sample data). bin\\mallet\timport-dir\t--input\tpathway\\to\\the\\directory\\with\\the\\files\t--output\ttutorial.mallet\t--keep-sequence\t--remove-stopwords\t \n 317 If you type dir now (or ls for Mac), you will find a file called tutorial.mallet. (If you get an error message, you can hit the cursor up key on your keyboard to recall the last command you typed, and look carefully for typos). This file now contains all of your data, in a format that MALLET can work with. For Mac Mac instructions are similar to those above for Windows, but keep in mind that Unix file paths (which are used by Mac) are different: for example, if the directory was in one's home directory, one would type ./bin/mallet\timport-dir\t--input\t/users/username/database/\t--output\ttutorial.mallet\t--keep-sequence\t--remove-stopwords\tIssues with Big Data If you're working with extremely large file collections – or indeed, very large files – you may run into issues with your heap space, your computer's working memory. This issue will initially arise during the import sequence, if it is relevant. By default, MALLET allows for 1GB of memory to be used. If you run into the following error message, you've run into your limit: Exception\tin\tthread\t\"main\"\tjava.lang.OutOfMemoryError:\tJava\theap\tspace\tIf your system has more memory, you can try increasing the memory allocated to your Java virtual machine. To do so, you need to edit the code in the mallet file found in the bin subdirectory of your MALLET folder. Using Komodo Edit, (See Mac, Windows, Linux for installation instructions),359 open the Mallet.bat file (C:\\Mallet\\bin\\mallet.bat) if you are using Windows, or the mallet file (~/Mallet/bin/mallet) if you are using Linux or OS X. Find the following line: MEMORY=1g You can then change the 1g value upwards – to 2g, 4g, or even higher depending on your system's RAM, which you can find out by looking up the machine's system information. Save your changes. You should now be able to avoid the error. If not, increase the value again. Your first topic model At the command prompt in the MALLET directory, type: bin\\mallet\ttrain-topics\t\t--input\ttutorial.mallet\t                                                        359\tSee:\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Setting\tUp\tan\tIntegrated\tDevelopment\tEnvironment\tfor\tPython'\t(Mac,\tLinux,\tor\tWindows),\tThe\tProgramming\tHistorian\t(2012).\t \n 318 This command opens your tutorial.mallet file, and runs the topic model routine on it using only the default settings. As it iterates through the routine, trying to find the best division of words into topics, your command prompt window will fill with output from each run. When it is done, you can scroll up to see what it was outputting (as in Figure 8). \n Basic Topic Model Output The computer is printing out the key words, the words that help define a statistically significant topic, per the routine. In Figure 8, the first topic it prints out might look like this (your key words might look a bit different): 0\t\t\t\t5\t\t\t\ttest\tcricket\tAustralian\thill\tacting\tEngland\tnorthern\tleading\tended\tinnings\trecord\truns\tscored\trun\tteam\tbatsman\tplayed\tsociety\tEnglish\tIf you are a fan of cricket, you will recognize that all of these words could be used to describe a cricket match. What we are dealing with here is a topic related to Australian cricket. If you go to C:\\mallet\\sample-data\\web\\en\\hill.txt, you will see that this file is a brief biography of the noted Australian cricketer Clem Hill. The 0 and the 5 we will talk about later in the lesson. Note that MALLET includes an element of \n \n 319 randomness, so the keyword lists will look different every time the program is run, even if on the same set of data. Go back to the main MALLET directory, and type dir. You will see that there is no output file. While we successfully created a topic model, we did not save the output! At the command prompt, type bin\\mallet\ttrain-topics\t\t--input\ttutorial.mallet\t--num-topics\t20\t--output-state\ttopic-state.gz\t--output-topic-keys\ttutorial_keys.txt\t--output-doc-topics\ttutorial_compostion.txt\t\tHere, we have told MALLET to create a topic model (train-topics) and everything with a double hyphen afterwards sets different parameters This command opens your tutorial.mallet file trains MALLET to find 20 topics outputs every word in your corpus of materials and the topic it belongs to into a compressed file (.gz; see www.gzip.org on how to unzip this) outputs a text document showing you what the top key words are for each topic (tutorial_keys.txt) and outputs a text file indicating the breakdown, by percentage, of each topic within each original text file you imported (tutorial_composition.txt). (To see the full range of possible parameters that you may wish to tweak, type bin\\mallet\ttrain-topics\t–help at the prompt.) Type dir. Your outputted files will be at the bottom of the list of files and directories in C:\\Mallet. Open tutorial_keys.txt in a word processor (Figure 9). You are presented with a series of paragraphs. The first paragraph is topic 0; the second paragraph is topic 1; the third paragraph is topic 2; etc. (The output begins counting at 0 rather than 1; so if you ask it to determine 20 topics, your list will run from 0 to 19). The second number in each paragraph is the Dirichlet parameter for the topic. This is related to an option which we did not run, and so its default value was used (this is why every topic in this file has the number 2.5). \n \n \n 320 Keywords shown in a Word Processor If when you ran the topic model routine you had included --optimize-interval\t20\tas below bin\\mallet\ttrain-topics\t\t--input\ttutorial.mallet\t\t--num-topics\t20\t--optimize-interval\t20\t--output-state\ttopic-state.gz\t\t--output-topic-keys\ttutorial_keys.txt\t--output-doc-topics\ttutorial_composition.txt\tthe output might look like this: 0\t0.02995\txi\tness\tregular\tasia\tonline\tcinema\testablished\talvida\tacclaim\tveenr\tcommercial\t\tThat is, the first number is the topic (topic 0), and the second number gives an indication of the weight of that topic. In general, including –optimize-interval leads to better topics. The composition of your documents What topics compose your documents? The answer is in the tutorial_composition.txt file. To stay organized, import the tutorial_composition.txt file into a spreadsheet (Excel, Open Office, etc). You will have a spreadsheet with a #doc, source, topic, proportion columns. All subsequent columns run topic, proportion, topic, proportion, etc., as in figure 10. \n Topic Composition You can see that doc# 0 (ie, the first document loaded into MALLET), elizabeth_needham.txt has topic 2 as its principal topic, at about 15%; topic 8 at 11%, and topic 1 at 8%. As we read along that first column of topics, we see that zinta.txt also has topic 2 as its largest topic, at 23%. \n \n 321 The topic model suggests a connection between these two documents that you might not at first have suspected. If you have a corpus of text files that are arranged in chronological order (e.g., 1.txt is earlier than 2.txt), then you can graph this output in your spreadsheet program, and begin to see changes over time, as Robert Nelson has done in Mining the Dispatch.360 How do you know the number of topics to search for? Is there a natural number of topics? What we have found is that one has to run the train-topics with varying numbers of topics to see how the composition file breaks down. If we end up with the majority of our original texts all in a very limited number of topics, then we take that as a signal that we need to increase the number of topics; the settings were too coarse. There are computational ways of searching for this, including using MALLETs hlda\tcommand, but for the reader of this tutorial, it is probably just quicker to cycle through a number of iterations (but for more see Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National Academy of Science, 101, 5228-5235). Getting your own texts into MALLET The sample\tdata folder in MALLET is your guide to how you should arrange your texts. You want to put everything you wish to topic model into a single folder within c:\\mallet, ie c:\\mallet\\mydata. Your texts should be in .txt format (that is, you create them with Notepad, or in Word choose Save\tAs\t->\tMS\tDos\ttext). You have to make some decisions. Do you want to explore topics at a paragraph by paragraph level? Then each txt file should contain one paragraph. Things like page numbers or other identifiers can be indicated in the name you give the file, e.g., pg32_paragraph1.txt. If you are working with a diary, each text file might be a single entry, e.g., april_25_1887.txt. (Note that when naming folders or files, do not leave spaces in the name. Instead use underscores to represent spaces). If the texts that you are interested in are on the web, you might be able to automate this process.361 Further Reading about Topic Modeling To see a fully worked out example of topic modeling with a body of materials culled from webpages, see Mining the Open Web with Looted Heritage Draft.362                                                         360\tRobert\tNelson,\t‘Mining\tthe\tDispatch’,\thttp://dsl.richmond.edu/dispatch/\t361\tSee\tShawn\tGraham,\t‘Mining\ta\tDay\tof\tArchaeology’,\tElectric\tArchaeology\t(9\tJuly\t2012):\thttp://electricarchaeology.ca/2012/07/09/mining-a-day-of-archaeology/\t362\tShawn\tGraham,\t‘Mining\tthe\tOpen\tWeb\twith\t‘Looted\tHeritage’\t–\tDraft’,\tElectric\tArchaeology\t(8\tJune\t2012),\thttp://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/\t \n 322 You can grab the data for yourself at Figshare.com,363 which includes a number of .txt files. Each individual .txt file is a single news report. For extensive background and bibliography on topic modeling you may wish to begin with Scott Weingart's Guided Tour to Topic Modeling364 Ted Underwood's 'Topic modeling made just simple enough' is an important discussion on interpreting the meaning of topics.365 Lisa Rhody's post on interpreting topics is also illuminating. 'Some Assembly Required' Lisa @ Work August 22, 2012.366 Clay Templeton, 'Topic Modeling in the Humanities: An Overview | Maryland Institute for Technology in the Humanities', n.d.367 David Blei, Andrew Ng, and Michael Jordan, 'Latent dirichlet allocation,' The Journal of Machine Learning Research 3 (2003).368 Finally, also consult David Mimno's bibliography of topic modeling articles.369 They're tagged by topic to make finding the right one for a particular application that much easier. Also take a look at his recent article on Computational Historiography from ACM Transactions on Computational Logic370 which goes through a hundred years of Classics journals to learn something about the field. While the article should be read as a good example of topic modeling, his 'Methods' section is especially important, in that it discusses preparing text for this sort of analysis. About the Author Shawn Graham is associate professor of digital humanities and history at Carleton University.   Scott Weingart is a historian of science and doctoral candidate at Indiana University.   Ian Milligan is an assistant professor of history at the University of Waterloo.                                                         363\tShawn\tGraham,\t‘Full\tText\tof\t207\tReports\tfrom\t1st\tQuarter\tof\t2012\ton\tLooted\tHeritage’,\tFigshare:\thttps://figshare.com/articles/looted_heritage_reports_txt.zip/91828\t364\tScott\tWeingart,\t‘Topic\tModelilng\tfor\tHumanists:\tA\tGuided\tTour’\t(25\tJuly\t2012):\thttp://www.scottbot.net/HIAL/?p=19113\t365\tTed\tUnderwood,\t‘Topic\tmodeling\tmade\tjust\tsimple\tenough’,\tThe\tStone\tand\tthe\tShell\t(7\tApril\t2012):\thttp://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\t366\tLisa\tMarie\tRhody,\t‘Some\tAssembly\tRequired:\tUnderstanding\tand\tInterpreting\tTopics\tin\tLDA\tModels\tof\tFigurative\tLanguage’\t(22\tAugust\t2012):\thttp://www.lisarhody.com/some-assembly-required/\t367\tClay\tTempleton,\t‘Topic\tModeling\tin\tthe\tHumanities:\tAn\tOverview’:\thttp://mith.umd.edu/topic-modeling-in-the-humanities-an-overview/\t368\tDavid\tBlei,\tAndrew\tNg,\tand\tMichael\tJordan,\t‘Latent\tdirichlet\tallocation’,\tThe\tJournal\tof\tMachine\tLearning\tResearch,\tvol.\t3\t(2003):\thttp://dl.acm.org/citation.cfm?id=944937\t369\tDavid\tMimno,\t‘Topic\tModeling\tBibliography’:\thttp://mimno.infosci.cornell.edu/topics.html\t370\tDavid\tMimno,\t‘Computational\tHistoriography:\tData\tMining\tin\ta\tCentury\tof\tClassics\tJournals’:\thttp://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf\t \n 323 33. Corpus Analysis with Antconc Heather Froehlich – 2015    Introduction Corpus analysis is a form of text analysis which allows you to make comparisons between textual objects at a large scale (so-called 'distant reading'). It allows us to see things that we don't necessarily see when reading as humans. If you’ve got a collection of documents, you may want to find patterns of grammatical use, or frequently recurring phrases in your corpus. You also may want to find statistically likely and/or unlikely phrases for a particular author or kind of text, particular kinds of grammatical structures or a lot of examples of a particular concept across a large number of documents in context. Corpus analysis is especially useful for testing intuitions about texts and/or triangulating results from other digital methods. By the end of this tutorial, you will be able to: create/download a corpus of texts conduct a keyword-in-context search identify patterns surrounding a particular word use more specific search queries look at statistically significant differences between corpora make multi-modal comparisons using corpus lingiustic methods You have done this sort of thing before, if you have ever... searched in a PDF or a word doc for all examples a specific term Used Voyant Tools371 for looking at patterns in one text Followed Programming Historian’s Introduction to Python tutorials In many ways Voyant is a gateway into conducting more sophisticated, replicable analysis, as the DIY aesthetic of Python or R scripting may not appeal to everyone. AntConc372 fills this void by being a standalone software package for linguistic analysis of texts, freely available for Windows, Mac OS, and Linux and is highly maintained by its creator, Laurence Anthony. There are other concordance software packages                                                         371\t‘Voyant\tTools’:\thttp://voyant-tools.org/\t372\t‘AntConc’:\thttp://www.laurenceanthony.net/software/antconc/\t \n 324 available, but it is freely available across platforms and very well maintained. See the concordance bibliography for other resources.373 This tutorial explores several different ways to approach a corpus of texts. It's important to note that corpus linguistic approaches are rarely, if ever, a one-size-fits all affair. So, as you go through each step, it's worth thinking about what you're doing and how it can help you answer a specific question with your data. Although I present this tutorial in a building-block approach of 'do this then that to achieve x', it's not always necessary to follow the exact order outlined here. This lessons provides an outline of some of the methods available, rather than a recipe for success. Tutorial downloads 1. Software: AntConc.374 Unzip the download if necessary, and launch the application. Screen shots below may vary slightly from the version you have (and by operationg system, of course), but the procedures are more or less the same across platforms and recent versions of AntConc. This tutorial is written with a (much older) version of AntConc in mind, as I find it easier to use in an introductory context. You are welcome to use the most recent version, but if you wish to follow along with the screenshots provided, you can download the version used here, version 3.2.4. 2. Sample Corpus: Download the zip file of movie reviews:  (https://db.tt/2PsC23px). A broad outline of this tutorial: Working with plain text files The AntConc user interface, loading corpora Keyword-in-context searching Advanced keyword-in-context searching Collocates and word lists Comparing corpora Discussion: Making meaningful comparisons Further resources Working with Plain Text Files Antconc works only with plain-text files with the file appendix .txt (eg Hamlet.txt). Antconc will not read .doc, .docx, .pdf, files. You will need to convert these into .txt files. It will read XML files that are saved as .txt files (it's OK if you don't know what an XML file is).                                                         373\tHeather\tFroehlich,\t‘An\tIntroductory\tBibliography\tto\tCorpus\tLinguistics’:\thttp://hfroehli.ch/2014/05/11/intro-bibliography-corpus-linguistics/\t374\t‘AntConc’:\thttp://www.laurenceanthony.net/software/antconc/\t \n 325 Visit your favorite website for news, and navigate to a news article (doesn't matter which one, as long as it is primarily text). Highlight all text in the article (header, byline, etc), and right-click \"copy\". Open a text editor such as Notepad (on Windows) or TextEdit (on Mac) and paste in your text. Other free options for text editors include Notepad++ (Windows)375 or TextWrangler (Mac),376 which offer more advanced features, and are especially good for doing a lot of text clean-up. By text clean-up, I mean removing extratextual information such as \"boilerplate\", which appears regularly throughout. If you keep this information, it's going to throw your data off; text analysis software will address these words in word counts, statistical analyses, and lexical relationships. For example, you might want to remove standard headers and footers which will appear on every page. Please see “Cleaning Data with OpenRefine\"377 for more on how to automate this task. On smaller corpora it may be more feasible to do this yourself, plus you'll get a much better sense of your corpus this way. Save the article as a .txt file to the desktop. You may want to do some follow-up text cleanup on other information, such as author by-line or title (remove them, then save the file again.) Remember that anything you leave in the text file can and will be addressed by text analysis software. Go to your desktop and check to see you can find your text file. Repeating this a lot is how you would build a corpus of plain text files; this process is called corpus construction, which very often involves addressing questions of sampling, representativeness and organization. Remember, each file you want to use in your corpus must be a plain text file for Antconc to use it. It is customary to name files with the .txt suffix so that you know what kind of file it is. As you might imagine, it can be rather tedious to build up a substantial corpus one file at a time, especially if you intend to process a large set of documents. It is very common, therefore, to use webscraping (using a small program to automatically grab files from the web for you) to construct your corpus. To learn more about the concepts and techniques for webscraping, see the Programming Historian tutorials scraping with Beautiful Soup and automatic downloading with wget.378 Rather than build a corpus one document at a time, we're going to use a prepared corpus of positive and negative movie reviews, borrowed from the Natural Language Processing Toolkit.379 The NLTK movie review corpus has 2000 reviews, organized by                                                         375\t‘Notepad++’:\thttps://notepad-plus-plus.org/\t376\t‘TextWrangler’:\thttp://www.barebones.com/products/textwrangler/\t377\tSeth\tvan\tHooland,\tRuben\tVerborgh,\tand\tMax\tde\tWilde,\t‘Cleaning\tData\twith\tOpenRefine’,\tThe\tProgramming\tHistorian\t(2013).\t378\tJeri\tWieringa,\t‘Intro\tto\tBeautiful\tSoup’\t(2012);\tIan\tMilligan\t‘Automated\tDownloading\twith\tWget’,\tThe\tProgramming\tHistorian\t(2012).\t379\t‘Natural\tLanguage\tToolkit’:\thttp://www.nltk.org/\t \n 326 positive and negative outcomes; today we will be addressing a small subset of them (200 positive, 200 negative). Corpus construction is a subfield in its own right. Please see Representativeness in Corpus Design,\" Literary and Linguistic Computing, 8 (4): 243-257 and Developing Linguistic Corpora: a Guide to Good Practice for more information.380 Getting Started with AntConc: The AntConc user interface, loading corpora When AntConc launches, it will look like this. \n On the left-hand side, there is a window to see all corpus files loaded (which we'll use momentarily). There are 7 tabs across the top: 1. Concordance: This will show you what's known as a Keyword in  Context view (abbreviated KWIC, more on this in a minute), using  the search bar below it. 2. Concordance Plot: This will show you a very simple visualization of  your KWIC search, where each instance will be represented as a  little black line from beginning to end of each file containing the  search term. 3. File View: This will show you a full file view for larger context of a  result. 4. Clusters: This view shows you words which very frequently appear  together. 5. Collocates: Clusters show us words which _definitely _appear together  in a corpus; collocates show words which are statistically likely to  appear together. 6. Word list: All the words in your corpus. 7. Keyword List: This will show comparisons between two corpora.                                                         380\tDouglas\tBiber,\t‘Representativeness\tin\tCorpus\tDesign’,\tLiterary\t&\tLinguistic\tComputing,\tvol.\t8,\tno.\t4\t(1993),\t243-257;\t\tMartin\tWynne,\t‘Developing\tLinguistic\tCorpora:\tA\tGuide\tto\tGood\tPractice’.\t\n \n 327 As an introduction, this tutortial barely scratches the surface of what you can do with AntConc. We will focus on the Concordance, Collocates, Keywords, and Word List functions. Loading Corpora Like opening a file elsewhere, we're going to start with File  > Open, but instead of opening just ONE file we want to open the directory of all our files. AntConc allows you to open entire directories, so if you're comfortable with this concept, you can just open the folder 'all reviews' and jump to Basic Analysis, below  \n Remember we've put our files on the desktop, so navigate there in the dropdown menu.  \n From the Desktop you want to navigate to our folder \"movie reviews from nltk\": \n \n 328  \n First you will select \"Negative Reviews\" and hit OK. 200 texts should load in the lefthand column Corpus Files – watch the Total No. box!  \n Then you're going to repeat the process to load the folder \"Positive Reviews\". You should now have 400 texts in the Corpus Files column. \n \n 329  \n \n\tall reviews loaded Searching Keywords in Context Start with a basic search One of the things corpus tools like Antconc are very good at are finding patterns in language which we have a hard time identifying as readers. Small boring words like the, I, he, she, a, an, is, have, will are especially difficult to keep track of as readers, because they're so common, but computers happen to be very good at them. These words are called function words, though they commonly known as 'stopwords' in digital humanities; they are often very distinct measures of authorial and generic style. As a result, they can be quite powerful search terms on their own or when combined with more content-driven terms, helping the researcher identify patterns they may not have been aware of previously. \n \n 330 In the search box at the bottom, type the and click \"start\". The Concordance view will show you every time the word the appears in our corpus of movie reviews, and some context for it. This is called a \"Key Words in Context\" viewer.  \n (14618 times, according to the Concordance Hits box in the bottom centre.) As above, the KWIC list is a good way to start looking for patterns. Even though it's still a lot of information, what kinds of words appear near \"the\"? Try a similar search for \"a\". Both \"a\" and \"the\" are articles, but one is a definite article and one an indefinite article - and the results you get will be illustrative of that. Now that you're comfortable with looking at a KWIC line, try doing it again with \"shot\": this will produce examples of both shot the noun ('line up the shot') and the verb 'this scene was shot carefully') What do you see? I understand this can be a difficult to read way of identifiying patterns. Try pressing the yellow \"sort\" button. What happens now? \n  \n \n 331 (This might be easier to read!) You can adjust the way AntConc sorts information by changing the parameters in the red circle: L corresponds with 'left' and R corresponds with 'right'; you can extend these up to ±5 in either direction. The default is 1 left, 2 right, 3 right, but you can change that to search 3 left, 2 left, 1 right (to get phrases and/or trigrams that end in the search term in question, for example) by clicking the arrow buttons up or down. If you don't want to include a sorting option you can skip it (as in the default: 1L, 2R, 3R) or include it as a 0. Less linear sorting practices are available, such as 4 left, 3 right, 5 right, which includes a lot of other contextual information. These parameters can be slow to respond, but be patient. If you're not sure what the resulting search is, just press 'sort' to see what's happened and adjust accordingly. ### Search Operators The * operator (wildcard) The * operator (which finds zero or more characters) can help, for instance, find both the singular and the plural forms of nouns. Task: Search for qualit*, then sort this search. What tends to precede and follow quality & qualities? (Hint: they're different words, and have different contexts. Again- look for patterns in usage using the KWIC!) For a full list of available wildcard operators and what they mean, go to Global Settings > Wildcard Settings.  \n To find out the difference between * and ?, search for th*n and th?n. These two search queries look very similiar, but show very different results. The ? operator is more specific than the * operator: wom?n – both women and woman m?n – man and men, but also min contrast to m*n: not helpful, because you'll get mean, melon, etc. Task: Compare these two searches: wom?n and m?n 1. sort each search in a meaningful way (eg. by search term then 1L then  2L) 2. File > Save output to text file (& append with .txt. \n \n 332 HINT:\tDuring\tthe\tcourse\tof\texploring\tin\tyour\tresearch,\tyou\tmay\tgenerate\tmany\tsuch\tfiles\tfor\treference;\tit's\thelpful\tto\tuse\tdescriptive\tfilenames\tthat\tdescribe\twhat's\tin\tthem\t(such\tas\t\"wom?n-results.text\",\tnot\t\"antconc_results.txt\").\t\n  \n And now you can open the plain text file in your text editor; you might have to widen the application window to make it readable: \n \n \n 333 Do this for each of the two searches and then look at the two text files side by side. What do you notice? The | operator (\"or\") Task: Search on she|he. Now search for these separately: how many instances of she vs he? There are many fewer instances of she – why? That's a research question! A good follow-up questions might be to sort the she|he search for patterns, and look to see if particular verbs follow each. Task: Practice searching a word of your choice, sorting in different ways, using wildcard(s), and finally exporting. Guiding focus question here: what kinds of patterns do you see? Can you explain them? Collocates and word lists Having looked at the KWIC lines for patterns, don't you wish there was a way for the computer to give you a list of words which appear most frequently in company with your keyword? Good news - there is a way to get this information, and it's available from the Collocates tab. Click that, and AntConc will tell you it needs to create a word list. Hit OK; it will do it automatically. NOTE:\tYou\twill\tonly\tget\tthis\tnotice\twhen\tyou\thaven't\tcreated\ta\tword\tlist\tyet.\t\t\tTry\tgenerating\tcollocates\tfor\tshe.\t\n \n 334 \n\tshe with collocates The unsorted results will seem to start with function words (words that build phrases) then go down to content words (words that build meaning)– these small boring words are the most frequent words in English,381 which are largely phrase builders. Later versions of AntConc often include the search term as the first hit, presumably because the search term you are looking for shows up in the text and we are looking for words which are likely to appear with this word. Some people might want to remove these small words by using a stopword list; this is a common step in topic modelling. Personally I don't encourage this practice because addressing highly-frequent words is where computers shine! As readers we tend not to notice them very much. Computers, especially software like Antconc, can show us where these words do and do not appear and that can be quite interesting, especially in very large collections of text - as explored earlier in the tutorial, with the, a, she and he. Additionally you may have a single letter 's' appear, quite high as well - that represents the possessive 's (the apostrophe won't be counted), but AntConc considers that s indicative of another word. Another example of this is 't appearing with do, as they contract as don't. Because these so commonly appear together, this makes them highly likely collocates. Task: Generate collocates for m?n and wom?n. Now sort them by frequency to 1L.                                                         381\t‘Word\tfrequency\tdata:\tCorpus\tof\tContemporary\tAmerican\tEnglish’:\thttp://www.wordfrequency.info/free.asp\t\n \n 335 This tells us about what makes a man or woman 'movie-worthy': – women have to be 'beautiful' or 'pregnant' or 'sophisticated' – men have to be somehow outside the norm – 'holy' or 'black' or 'old' This is not necessarily telling us about the movies but about the way those movies are written about in reviews, and can lead us to ask more nuanced questions, like \"How are women in romantic comedies described in reviews written by men compared to those written by women?\" Comparing corpora One of the most powerful types of analysis is comparing your corpus to a larger reference corpus. I've pulled out reviews of movies with which Steven Spielberg is associated (as director or producer). We can compare them to a reference corpus of movies by a range of directors. Be sure to think carefully about what a reference corpus for your own research might look like (eg. a study of Agatha Christie's language in her later years would work nicely as an analysis corpus for comparison to a reference corpus of all her novels). Remember, again, that corpus construction is a subfield in its own right. Settings > Tool preferences > Keyword List Under 'Reference Corpus' make sure \"Use raw files\" is checked Add Directory > open the folder containing the files that make up the reference corpus  Ensure you have a whole list of files \n  Hit Load (& wait …) then once the 'Loaded' box is checked, hit Apply. You can also opt to swap reference corpus & main files (SWAP REF/MAIN FILES). It is worth looking at what both results show. > If you're using a later version of AntConc, the Swap Ref/Main files option may be marked as \n \n 336 'swap with target files', and you will need to ensure the target and reference corpora have been loaded (press the load button each time you upload, or swap, a corpus). In Keyword List, just hit Start (with nothing typed in the search box). If you've just swapped the reference corpus and the target files, you may be prompted to create a new word list before AntConc will calculate the keywords. We see a list of Keywords that have words that are much more \"unusual\" – more statistically unexpected – in the corpus we are looking at when compared to the reference corpus. > Keyness: this is the frequency of a word in the text when compared with its frequency in a reference corpus, \"such that the statistical probability as computed by an appropriate procedure is smaller than or equal to a p value specified by the user.\"382  What are our keywords? \n\tspielberg vs movie reviews Discussion: Making meaningful comparisons Keep in mind that the way your organize your text files makes a difference to the kinds of questions you can ask and the kinds of results you will get. Remember that we are comparing 'negative' and 'positive' reviews quite flatly here. You could, for instance, make other comparisons with different subsets of reviews, which yield very different kinds of questions. Of course, the files you put in your corpus will shape your results. Again, the question of representativeness and sampling are highly relevant here – it's not always necessary or even ideal to use all of a dataset at once, even if you do have it. At this juncture, it's really worth interrogating how these methods help produce research questions.                                                         382\t‘Definition\tof\tKey-ness’:\thttp://www.lexically.net/downloads/version6/HTML/index.html?keyness_definition.htm\t\n \n 337 When thinking about how movie reviews work as a genre, you could consider, for example... Movie reviews vs music reviews Movie reviews vs book reviews Movie reviews vs news articles about sport Movie reviews vs news articles in general Each of these comparisons will tell you something different, and can produce different research questions, such as: How are movie reviews different than other kinds of media reviews? How are movie reviews different than other kinds of published writing? How do movie reviews compare to other specific kinds of writing, such as sport writing? How do movie reviews have in common with music reviews? And of course you could flip those questions to make further research questions: How are book reviews different to movie reviews? How are music reviews different than movie reviews? What do published newspaper articles have in common? How are movie reviews similar to other kinds of published writing? In summary: it's worth thinking about: Why you might want to compare two corpora What kinds of queries make meaningful research questions Principles of corpora construction: sampling & ensuring you can get something representative  Further resources for this tutorial A short bibliography on corpus linguistics.383 A more step-by-step version of this tutorial, assuming no computer knowledge384 About the Author Heather Froehlich is a PhD student at the University of Strathclyde (Glasgow, UK), where she studies gender in Early Modern London plays using computers. Her thesis draws heavily from sociohistoric linguistics and corpus stylistics, though she sustains an interest in digital methods for literary and linguistic inquiry.                                                          383\tHeather\tFroehlich,\t‘An\tIntroductory\tBibliography\tto\tCorpus\tLinguistics’:\thttp://hfroehli.ch/2014/05/11/intro-bibliography-corpus-linguistics/\t384\tHeather\tFroehlich,\t‘Getting\tStarted\twith\tAntconc’:\thttp://hfroehli.ch/workshops/getting-started-with-antconc/\t \n 338 34. From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources Marten Düring – 2015    Table of contents: Introduction About the case study Developing a coding scheme Visualize network data in Palladio The added value of network visualizations Other network visualization tools to consider Introduction Network visualizations can help humanities scholars reveal hidden and complex patterns and structures in textual sources. This tutorial explains how to extract network data (people, institutions, places, etc) from historical sources through the use of non-technical methods developed in Qualitative Data Analysis (QDA) and Social Network Analysis (SNA), and how to visualize this data with the platform-independent and particularly easy-to-use Palladio.385 \n A network visualization in Palladio and what you will be able to create by the end of this tutorial  The graph above shows an excerpt from the network of Ralph Neumann, particularly his connections to people who helped him and his sister during                                                         385\t‘Palladio’:\thttp://palladio.designhumanities.org/#/\t\n \n 339 their life in the underground in Berlin 1943-1945. You could easily modify the graph and ask: Who helped in which way? Who helped when? Who is connected to whom? Generally, network analysis provides the tools to explore highly complex constellations of relations between entities. Think of your friends: You will find it very easy to map out who are close and who don't get along well. Now imagine you had to explain these various relationships to somebody who does not know any of your friends. Or you wanted to include the relationships between your friends’ friends. In situations like this language and our capacity to comprehend social structures quickly reach their limits. Graph visualizations can be means to effectively communicate and explore such complex constellations. Generally you can think of Social Network Analysis as a means to transform complexity from a problem to an object of research. Often, nodes in a network represent humans connected to other humans by all imaginable types of social relations. But pretty much anything can be understood as a node: A film, a place, a job title, a point in time, a venue. Similarly, the concept of a tie (also called edge) between nodes is just as flexible: two theaters could be connected by a film shown in both of them, or by co-ownership, geographical proximity, or being in business in the same year. All this depends on your research interests and how you express them in form of nodes and relations in a network. This tutorial can not replace any of the many existing generic network analysis handbooks, such as John Scott's Social Network Analysis.386 For a great general introduction to the field and all its pitfalls for humanists I recommendScott Weingart’s blog post series “Networks Demystified”387 as well asClaire Lemercier’s paper “Formal network methods in history: why and how?\".388 You may also want to explore the bibliography and event calendar over at Historical Network Research389 to get a sense of how historians have made use of networks in their research. This tutorial will focus on data extraction from unstructured text and shows one way to visualize it using Palladio. It is purposefully designed to be as simple and robust as possible. For the limited scope of this tutorial it will suffice to say that an actor refers to the persons, institutions, etc. which are the object of study and which are connected by relations. Within the context of a network visualization or computation (also called graph), we call them nodes and we call the connections ties. In all cases it is important to remember that nodes and ties are drastically simplified models used to represent the complexities of past events, and in themselves do not always suffice to generate insight. But it is likely that the graph will highlight interesting aspects, challenge your hypothesis and/or lead you to                                                         386\tJohn\tScott,\tSocial\tNetwork\tAnalysis,\t2013.\t387\tScott\tWeingart,\t‘Demystifying\tNetworks’\t(14\tDecember\t2011):\thttp://www.scottbot.net/HIAL/?p=6279\t388\tClaire\tLemercier,\t‘Formal\tnetwork\tmethod\tin\thistory:\twhy\tand\thow?’\t(2011):\thttps://hal.archives-ouvertes.fr/file/index/docid/649316/filename/lemercier_A_zg.pdf\t389\t‘Historical\tNetwork\tResearch’:\thttp://historicalnetworkresearch.org/\t \n 340 generate new ones. Network diagrams become meaningful when they are part of a dialogue with data and other sources of information. Many network analysis projects in the social sciences rely on pre-existing data sources or data that was created for the purpose of network analysis. Examples include email logs, questionnaires or trade relations which make it relatively easy to identify who is connected to whom and how. It is considerably more difficult to extract network data from unstructured text. This forces us to somehow marry the complexities of hermeneutics with the rigor of formal data analysis. The term “friend” might serve as an example: Depending on the context it can signify anything from an insult to an expression of love. Context knowledge and analysis of the text will help you identify what it stands for in any given case. A formal category system should represent the different meanings inasmuch detail as necessary for your purposes. In other words, the challenge is to systematize text interpretation. Networks created from pre-existing data sets need to be considered within the context in which they were created (e.g. wording of questions in a questionnaire and selected target groups). Networks created from unstructured text pose challenges on top of this: interpretations are highly individual and depend on viewpoints and context knowledge. About the case study The case study I use for this tutorial is a first-person narrative of Ralph Neumann, a Jewish survivor of the Holocaust. You can find the text online.390 The coding scheme which I will introduce below is a simplified version of the one I developed during my PhD project on covert support networks during the Second World War. My research was driven by three questions: To what extent can social relationships help explain why ordinary people took the risks associated with helping? How did such relationships enable people to provide these acts of help given that only very limited resources were available to them? How did social relationships help Jewish refugees to survive in the underground? In this project network visualisations helped me to discover hitherto forgotten yet highly important contact brokers, highlight the overall significance of Jewish refugees as contact brokers and generally to navigate through a total of some 5,000 acts of help which connected some 1,400 people between 1942 and 1945. Developing a coding scheme In visualizing network relationships, one of the first and most difficult challenges is to decide who should be part of the network and which relations between the selected actors are to be coded. It will probably take                                                         390\tRalph\tNeuman,\t‘Memories\tfrom\tMy\tEarly\tLife\tin\tGermany\t1926-1946’:\thttp://www.gdw-berlin.de/fileadmin/bilder/publ/publikationen_in_englischer_sprache/2006_Neuman_eng.pdf\t \n 341 some time to figure this out and will likely be an iterative process since you will need to balance your research interests and hypotheses with the availability of information in your texts and represent both in a rigid and necessarily simplifying coding scheme. The main questions during this process are: Which aspects of relationships between two actors are relevant? Who is part of the network? Who is not? Which attributes matter? What do you aim to find? I found the following answers to these: What defines a relationship between two actors? Any action which directly contributed to the survival of persecuted persons in hiding. This included e.g. non-Jewish communists but excluded bystanders who chose not to denunciate refugees or mere acquaintances between actors (for lack of sufficient coverage in the sources). Actors were coded as either providers or recipients of an act of help independently of their status as refugees. There is no simple and robust way to handle ambiguities and doubt at the moment. I therefore chose to collect verifiable data only. Who is part of the network? Who is not? Anyone who is mentioned as a helper, involved in helping activities, involved in activities which aimed to suppress helping behaviour. In fact, some helping activities turned out to be unconnected to my case studies but in other cases this approach revealed hitherto unexpected cross-connections between networks. Which types of relationships do you observe? Rough categorizations of: Form of help, intensity of relationships, duration of help, time of help, time of first meeting (both coded in 6-months steps). Which attributes are relevant? Mainly racial status according to National Socialist legislation. What do you aim to find? A deeper understanding of who helps whom how, and discovery of patterns in the data that correspond to network theory. A highly productive interaction between my sources and the visualized data made me stick with this. Note that coding schemes in general are not able to represent the full complexity of sources in all their subtleties and ambivalence. The purpose of the coding scheme is to develop a model of the relationships you are interested in. As such, the types of relations and the attributes are abstracted and categorized renditions of the complexities conveyed in the text(s). This also means that in many cases network data and  \n 342 visualizations will only make sense once reunited with their original context, in my case the primary sources from which I extracted it. The translation of text interpretation into data collection has its roots in sociological Qualitative Data Analysis. It is important that you and others can retrace your steps and understand how you define your relations. It is very helpful to define them abstractly and to provide examples from your sources to further illustrate your choices. Any data you produce can only be as clear and coherent as your coding practices. Clarity and coherence increase during the iterative process of creating coding schemes and by testing it on a variety of different sources until it fits. \n A first stab at the coding scheme  The figure above shows a snapshot with sample data of the coding scheme I used during my project. In this case Alice helps Paul. We can express this as a relation between the actors “Alice” and “Paul” which share a relation of the category “Form of Help”. Within this category we find the subcategory “4. Food, Commodities” which further describes their relation. All major network visualization tools let you specify whether a network is directed like this one or undirected. In directed networks, relations describe an exchange from one actor to another, in our case this is “help”. By convention, the active nodes are mentioned first (in this case Alice) in the dataset. In a visualization of a directed network, you will see arrows going from one actor to another. Relations can also be reciprocal, for example when Alice helps Bob and Bob helps Alice. Quite often, however, it doesn’t make sense to work with directionality, for example when two actors are simply part of the same organization. In this case the network should be undirected and would be represented by a simple line between the two actors. I wanted to know how often actors gave help and how often they received it. I was particularly interested in the degree of Jewish self-help, which is why \n \n 343 a directed network approach and the role of “Giver” and “Recipient” make sense. The third column in the coding scheme is optional and further describes the kind of relationship between Alice and Paul. As a category I chose “Form of Help” which reflects the most common ways in which support was given. The categories and subcategories emerged during a long process of coding different types of texts and different types of support networks. During this process I learned, for example, which relevant forms of help are rarely described and therefore not traceable, such as the provision of support-related information. Expect having to adapt your coding scheme frequently in the beginning and brace yourself for re-coding your data a few times until it consistenly corresponds with your sources and interests. As it stands, the coding scheme conveys the information that Alice provided food or other commodities for Paul, as indicated by the value 4 which corresponds to the subcategory “4. Food, Commodities” in the category “Form of Help”. Human relationships are however significantly more complex than this and characterized by different and ever-changing layers of relations. To an extent, we can represent some of this complexity by collecting multiplex relationships. Consider this sample sentence: “In September 1944 Paul stayed at his friend Alice’s place; they had met around Easter the year before.” \n A representation of the sample sentence.  The coding scheme in the figure above describes the relationships between helpers and recipients of help in greater detail. “Relation” for example gives a rough categorization of how well two actors knew each other, “Duration” captures how long an act of help lasted, “Date of Activity” indicates when an act of help occurred and “Date of first Meeting” should be self explanatory. The value “99” here specifies “unknown” since the sample sentence does not describe the intensity of the relationship between Alice and Paul in greater detail. Note that this scheme focuses exclusively on collecting acts of help, not on capturing the development of relationships \n \n 344 between people (which were not covered in my sources). Explicit choices like this define the value of the data during analysis. It is also possible to collect information on the actors in the network; so-called attribute data uses pretty much the same format. The figure below shows sample data for Alice and Paul. \n Sample attribute data If we read the information now stored in the coding scheme we learn that Alice provided accommodation for Paul (“Form of Help”: 4), that we do not know how close they were (“Relation”: 99) or how long he stayed (“Duration”: 99). We do know however that this took place some time in the second half of 1944 (“Date of Activity”: 14) and that they had met for the first time in the first half of 1943 (“Date of first Meeting”: 11). The date of first meeting can be inferred from the words “around Easter the year before”. If in doubt, I always chose to enter “99” representing “unknown”. But what if Alice had also helped Paul with emotional support (another subcategory of “Form of Help”) while he was staying with her? To acknowledge this, I coded one row which describes the provision of accommodation and a second below which describes the provision of emotional support. Note that not all network visualization tools will allow you to represent parallel edges and will either ignore the second act of help which occurred or try to merge the two relations. Both NodeXL and Palladio can handle this however and it is rumoured that a future release of Gephi will as well. If you encounter this problem and if none of the two tools are an option for you, I would recommend to set up a relational database and work with specific queries for each visualization. The process of designing such a coding scheme forces you to become explicit about your assumptions, interests and the materials at your disposal, something valuable beyond data analysis. Another side effect of extracting network data from text is that you will get to know your sources very well: Sentences following the model of “Person A is connected to Persons B, C and D through relation type X at time Y” will probably be rare. Instead it will take close reading, deep context knowledge and interpretation to find out who is connected to whom in which way. This means that coding data in this way, will raise many questions and will force you to study your \n \n 345 sources more deeply and more rigorously than if you had worked through them the “traditional” way. Visualize network data in Palladio Once you have come up with a coding scheme and encoded your sources you are ready to visualize the network relationships. First make sure that all empty cells are filled with either a number representing a type of tie or with \"99\" for “unknown”. Create a new copy of your file (Save as..) and delete the codes for the different categories so that your sheet looks something like the image below.  Sample attribute data ready to be exported for visualization or computation All spreadsheet editors let you export tables as either .csv (comma-separated values) or as .txt files. These files can be imported into all of the commonly used network visualization tools (see the list at the end of the tutorial). For your first steps however I suggest that you try out Palladio, a very easy-to-use data visualization tool in active development by Stanford University. It runs in browsers and is therefore platform-independent. Please note that Palladio, although quite versatile, is designed more for quick visualizations than sophisticated network analysis. The following steps will explain how to visualize network data in Palladio but I also recommend that you take a look at their own training materials and explore their sample data. Here however I use a slightly modified sample dataset based on the coding scheme presented earlier (you can also download it and use it to explore other tools).391 Step by Step: 1. Palladio. Go to http://palladio.designhumanities.org/. 2. Start. On their website click the “Start” button. 3. Load attribute data. From your data sheet, copy the attribute data (Sample dataset, Sheet 2) and paste it in the white section of the page, now click “Load”.                                                         391\tReferred\tto\thereafter\tas\t‘Sample\tdataset’.\tAvailable\tfrom:\thttps://docs.google.com/spreadsheets/d/1LzbWsG73m74t3p6xE7lutfVWuOdzOIfN55FbhCCRZvk/edit#gid=0.\tLink\talso\tavailable\tin\tthe\tonline\tversion\tof\tthe\ttutorial.\t\n \n 346 \n Loading attribute data into Palladio 4. Edit attributes. Change the title of the table to something more meaningful, such as “People”. Now you see the columns “Person”, “Race Status” and “Sex” which correspond to the columns in the sample data. Next you need to make sure that Palladio understands that there are actions associated with the people you just entered in the database. \n View of attribute data in Palladio 5. Load relational data. To do this, click on “Person” and “Add a new table”. Now paste all the relational data (Sample data, Sheet 1) in the appropriate field. Palladio expects unique identifiers to link the relational information to the actor attribute information. Make sure this lines up well and that you avoid any irritating characters such as “/”. Palladio will prompt you with error messages if you do. Click “Load data”, close the overlay window and go back to the main data overview. You should see something like this: \n \n 347 \n Loading relational data 6. Link attributes and relations. Next, we need to explicitly link the two tables we created. In our case, peoples’ first- and last names work as IDs so we need to connect them. To do this click on the corresponding occurrences in the new table. In the sample files these are “Giver” and “Recipient”. Click on “Extension” (at the bottom) and select “People”, the table which contains all the people attribute information. Do the same for “Recipient”. \n Linking People to Relations \n \n 348  7. Identify temporal data. Palladio has nice time visualization features. You can use it if you have start and end points for each relation. The sample data contains two columns with suitable data. Click on “Time Step Start” and select the data type “Year or Date”. Do the same for “Time Step End”. The Palladio team recommends that your data is in the YYYY-MM-DD format, but my more abstract time steps worked well. If you were to load geographical coordinates (not covered by this tutorial but here: Palladio Simple Map Scenario)392 you would select the “Coordinates” data type. \n Changing the data type to 'Year or Date' 8. Open the Graph tool. You are now done with loading the data. Click “Graph” to load the visualization interface.  Load the Graph tool 9. Specify source and target nodes. First off Palladio asks you to specify the “Source” and “Target” nodes in the network. Let’s start with “Givers” and “Recipients”. You will now see the graph and can begin to study it in greater detail.                                                         392\t‘Scenario\t#1:\tSimple\tMap’:\thttp://hdlab.stanford.edu/doc/scenario-simple-map.pdf\t\n \n 349 \n Select 'Giver' as source and 'Recipient' as target. 10. Highlight nodes. Continue by ticking the “Highlight” boxes. This will give you an immediate sense of who acted as a provider of help, who merely received help and which actors were both givers and recipients of help. 11. Facet filter. Next up, try the faceted filter (Figure 13). You will recognize the columns which describe the different acts of help. Start by selecting “3” in the “Form of Help” column. This will reduce the graph to only provisions of accommodation. Next, select values from the “Date of Activity” column to further narrow down your query. This will show you who provided accommodation and how this changes over time. Re-select all values in a column by clicking on the check box next to the column name. Take your time to explore the dataset – how does it change over time? When you are done, make sure to delete the Facet filter using the small red trashcan. Network visualizations can be incredibly suggestive. Remember that whatever you see is a different representation of your data coding (and the choices you made along the way) and that there will be errors you might have to fix. Either of the graphs I worked with would have looked differently had I chosen different time steps or included people who merely knew each other but did not engage in helping behavior. \n \n 350 \n The Facet filter in Palladio 12. Bipartite network visualization. Now this is nice. But there is something else which makes Palladio a great tool to start out with network visualization: It makes it very easy to produce bipartite, or 2-mode networks.393 What you have seen until now is a so-called unipartite or 1-mode network: It represents relations between source and target nodes of one type (for example “people”) through one or more types of relations, Figures 13 and 14 are examples of this type of graph. Network analysis however gives you a lot of freedom to rethink what source and targets are. Bipartite networks have two different types of nodes, an example could be to select “people” as the first node type and “point in time” as the second. Figure 15 shows a bipartite network and reveals which recipients of help were present in the network at the same time. Compare this graph to Figure 16 which shows which givers of help were present at the same time. This points at a high rate of fluctuation among helpers, an observation which holds true for all of the networks I studied. While humans are very good at processing people-to-people networks, we find it harder to process these more abstract networks. Give it a try and experiment with different bipartite networks: Click again on “Target” but this time select “Form of Help” or “Sex” or any other category. Note that if you wanted to see \"Giver\" and \"Recipients\" as one node type and \"Date of Activity\" as the second, you would need to create one column with all the persons and a second with the points in time during which they were present in your spreadsheet editor and import this data into Palladio. Also, at this stage Palladio does not yet let you represent attribute data for example by coloring the nodes, but all other tools have this functionality.                                                         393\t‘Bipartite\tgraphs’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Bipartite_graph#Examples\t\n \n 351 \n Visualization of a unipartite network: Givers and Recipients of help \n Visualization of a bipartite network: Recipients and Date of Activity  \n Visualization of a bipartite network: Givers and Date of Activity  \n \n 352 13. Timeline. The Timeline feature provides a relatively easy way to visualize changes in your network. Figure 17 shows the distribution of men and women in the network over time. The first column on the y-axis corresponds to the “Dates” field and represents the different time steps. The bars represent the “Sex” attribute: Unknown, numbers of women and men are represented by the height of the segments in a bar (ranging from light grey to black). Hover over them to see what is what. The lower bar segment corresponds to the “Height shows” field and here represents the total number of persons which changes between time step 13 and 14.  Gender distribution in the network over time 14. Time Span. Even more interesting is the Time Span view which updates the network visualization dynamically. Click on “Time Span”. Figure 17 illustrates what you should see now. Use the mouse to highlight a section between the time steps which will then be highlighted in grey. You can now drag the highlighted section across the timeline and see how the graph changes from time step to time step. \n Timeline. Visualization of Time Steps  \n \n 353 15. Node size. Palladio lets you size your nodes based on actor attributes. Note that this does not make sense for the sample data given that numerical values represent categories. Node sizes can however be useful if you were to represent the sum of a person’s acts of help, which in this case would correspond to his or her Out-Degree,394 the number of outgoing relations for a node. 16. Export your visualizations. Palladio lets you export your network as .svg files, a vector-based image format. Use your browser of choice to open them. 17. Lists, Maps and Galleries. You will have noticed that Palladio has a variety of additional visualization formats: Lists, Maps, and Galleries. All of which are as intuitive and well-designed as the Graph section. Galleries let you specify certain attributes of your actors and present them in a card-view. By adding latitude/longitude values to your actor attributes you will get an instant sense of where your network happens. Take a look at their own sample files to explore this. The added value of network visualizations Careful extraction of network data from text is time consuming and exhausting since it requires full concentration at every step along the way. I regularly asked myself whether it was worth it--and in the end whether or not I could have made the same observations without the support of network visualizations. The answer is yes, I might have come to the same main conclusions without coding all this data and yes, it was worth it. Entering the relational data soon becomes fast and painless in the process of close reading. In my experience, question-driven close reading and interpretation on one side and data coding and visualization on the other are not at all separate processes but intertwined and they can complement each other very effectively. Play is not generally considered to be something very academic, but especially with this type of data it is a valuable investment of your time: You don’t just play with your data, you rearrange and thereby constantly rethink what you know about your topic and what you can know about your topic. Each tie I coded represents a story of how somebody helped somebody else. Network visualizations helped me understand how these ca. 5,000 stories and 1,400 individuals relate to each other. They often confirmed what I knew but regularly also surprised me and raised interesting questions. For example, it led me to identify Walter Heymann as the person whose contact brokerage started off two major support networks and subsequently enabled them to save hundreds of people. Descriptions of his contacts to leading actors in both networks were scattered across different documents                                                         394\t‘Indegree\tand\toutdegree’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Directed_graph#Indegree_and_outdegree\t \n 354 which I had worked on during different phases of the project. The visualization aggregated all these relations and revealed these connections. Further investigation then showed that it was in fact him who brought all of them together. \n Walter Heymann brokered contacts which led to the emergence of two major support networks  On other occasions, visualizations revealed the existence of long reaching contact chains across different social classes which helped refugees create trusted ties with strangers, they also showed unexpected gaps between actors I expected to be connected, led me to identify clusters in overlapping lists of names, observe phases of activity and inactivity, helped me spot people who bridged different groups and overall led me to emphasize the contact brokerage of Jewish victims of persecution as a major, hitherto overlooked factor in the emergence of covert networks. Visualisations are of course not “proof” of anything but tools to help understand complex relations; their interpretation is based on a good understanding of the underlying data and how it was visualized. Selected network visualizations can also accompany text and help your readers better understand the complex relationships you discuss, much like the maps you sometimes find on the inside covers of old books. A few practical points: Collect and store data in one spreadsheet and use a copy for visualizations \n \n 355 Make sure you understand the basic rationale behind any centrality and layout algorithms you choose as they will affect your view on your data. Wikipedia is usually a good source for comprehensive information on them. Don't hesitate to revise and start over if you sense that your coding scheme does not work out as expected. It will definitely be worth it. Finally, any of the visualizations you can create with the small sample dataset I provide for this tutorial requires context knowledge to be really meaningful. The only way for you to find out whether this method makes sense for your research is to start coding your own data and to use your own context knowledge to make sense of your visualizations. Good luck! Other network visualization tools to consider Nodegoat395 – similar to Palladio in that it makes data collection, mapping and graph visualizations easy. Allows easy setup of relational databases and lets users store data on their servers. Tutorial available.396 NodeXL397 – capable to perform many tasks common in SNA, easy-to-use, open source but requires Windows and MS Office 2007 or newer. Tutorial 1,398 Tutorial 2.399 Gephi – open source, platform independent.400 The best known and most versatile visualization tool available but expect a steep learning curve. The developers announce support for parallel edges in version 1.0. Tutorials: by Clement Levallois401 and Sebastien Heymann.402 VennMaker – is platform-independent and can be tested for free.403 VennMaker inverts the process of data collection: Users start with a customizable canvas and draw self-defined nodes and relations on it. The tool collects the corresponding data in the background. The most commonly used tools for more mathematical analyses are UCINET 404(licensed, tutorials available on their website) and Pajek405                                                         395\t‘Nodegoat’:\thttp://nodegoat.net/\t396\tYanan\tSun,\t‘Geo-Layout\tfor\tDynamic\tNetwork\tin\tNodegoat’:\thttp://nodegoat.net/cms/UPLOAD/AsmallguidebyYanan11082014.pdf\t397\t‘NodeXL’:\thttp://nodexl.codeplex.com/\t398\t‘NodeXL\tTutorial\t(part\t1\tof\t3)’,\tYouTube:\thttps://www.youtube.com/watch?v=pwsImFyc0lE\t399\t‘Introduction\tto\tNodeXL\t–\ta’,\tYouTube:\thttps://www.youtube.com/watch?v=xKhYGRpbwOc\t400\t‘Gephi’:\thttps://gephi.org/\t401\tClement\tLevallois,\t‘Teaching\tand\tTraining\tMaterial’:\thttp://www.clementlevallois.net/training.html\t402\tSebastien\tHeymann,\t‘Sebastian\tHeymann\tExploratory\tNetwork\tAnalysis\twith\tGephi\tPart\t2’:\thttps://www.youtube.com/watch?v=L6hHv6y5GsQ\t403\t‘Venn\tMaker’:\thttp://www.vennmaker.de/\t404\t‘UCINET\tSoftware’:\thttps://sites.google.com/site/ucinetsoftware/home\t405\t‘Pajek’:\thttp://mrvar.fdv.uni-lj.si/pajek/\t \n 356 (free) for which a great handbook exists.406 Both were developed for Windows but run well elsewhere using Wine. For Python users the very well documented package Networkx407 is a great starting point; other packages exist for other programming languages. About the Author Marten Düring is a historian, works as researcher in the Digital Humanities Lab at CVCE Luxembourg, runs http://historicalnetworkresearch.org and regularly teaches workshops on network analysis.  \n                                                        406\tWoultor\tde\tNooy,\tAndrej\tMrvar,\tVladimir\tBatagelj,\tExploratory\tSocial\tNetwork\tAnalysis\twith\tPajek\t(2nd\tedition),\t2011.\t407\t‘NetworkX’:\thttps://networkx.github.io/\t \n 357 35. Supervised Classification: The Naive Bayesian Returns to the Old Bailey Vilja Hulden – 2014 Introduction A few years back, William Turkel wrote a series of blog posts called A Naive Bayesian in the Old Bailey,408 which showed how one could use machine learning to extract interesting documents out of a digital archive. This tutorial is a kind of an update on that blog essay, with roughly the same data but a slightly different version of the machine learner. The idea is to show why machine learning methods are of interest to historians, as well as to present a step-by-step implementation of a supervised machine learner. This learner is then applied to the Old Bailey digital archive,409 which contains several centuries' worth of transcripts of trials held at the Old Bailey in London. We will be using Python for the implementation. One obvious use of machine learning for a historian is document selection. If we can get the computer to \"learn\" what kinds of documents we want to see, we can enlist its help in the always-time-consuming task of finding relevant documents in a digital archive (or any other digital collection of documents). We'll still be the ones reading and interpreting the documents; the computer is just acting as a fetch dog of sorts, running to the archive, nosing through documents, and bringing us those that it thinks we'll find interesting. What we will do in this tutorial, then, is to apply a machine learner called Naive Bayesian to data from the Old Bailey digital archive. Our goals are to learn how a Naive Bayesian works and to evaluate how effectively it classifies documents into different categories - in this case, trials into offense categories (theft, assault, etc.). This will help us determine how useful a machine learner might be to us as historians: if it does well at this classification task, it might also do well at finding us documents that belong to a \"class\" we, given our particular research interests, want to see. Step by step, we'll do the following: learn what machine learners do, and look more closely at a popular learner called Naive Bayesian. download a set of trial records from the Old Bailey archive.                                                         408\tWilliam\tJ.\tTurkel,\t‘A\tNaïve\tBayesian\tin\tthe\tOld\tBailey,\tPart\t1’,\tDigital\tHistory\tHacks\t(2005-08),\t(24\tMay\t2008):\thttp://digitalhistoryhacks.blogspot.co.uk/2008/05/naive-bayesian-in-old-bailey-part-1.html\t409\tTim\tHitchcock,\tRobert\tShoemaker,\tClive\tEmsley,\tSharon\tHoward\tand\tJamie\tMcLaughlin,\tet\tal.,\tThe\tOld\tBailey\tProceedings\tOnline,\t1674-1913\t(www.oldbaileyonline.org,\tversion\t7.0,\t24\tMarch\t2012).\t \n 358 write a script that saves the trials as text (removing the XML markup) and does a couple of other useful things. write a couple of helper scripts to assist in testing the learners. write a script that tests the performance of the learner. Files you will need save-trialtxts-by-category.py tenfold-crossvalidation.py count-offense-instances.py pretestprocessing.py test-nb-learner.py naivebayes.py english-stopwords.txt A zip file of the scripts is available [http://programminghistorian.org/assets/baileycode.zip]. You can also download another zip file containing the scripts, the data that we are using and the files that result from the scripts [http://dx.doi.org/10.5281/zenodo.13284]. (The second option is probably easiest if you want to follow along with the lesson, since it gives you everything you need in the correct folder structure.) More information about where to put the files is in the \"Preliminaries\" section of the part where we actually begin to code. Note: You will not need any Python modules that don't come with standard installations, except for BeautifulSoup410 (used in the data creation step, not in the learner code itself). The Old Bailey Digital Archive The Old Bailey digital archive contains 197,745 criminal trials held at the Old Bailey, aka the Central Criminal Court in London. The trials were held between 1674 and 1913, and since the archive provides the full transcript of each trial, many of which include testimony by defendants, victims, and witnesses, it's a great resource for all kinds of historians interested in the lives of ordinary people in London. What makes the collection particularly useful for our purposes is that the text of each trial is richly annotated with such information as what type of an offense was involved (pocketpicking, assault, robbery, conspiracy...), the name and gender of each witness, the verdict, etc. What's more, this information has been added to the document in XML markup, which allows us to extract it easily and reliably. That, in turn, lets us train a machine learner to recognize the things we are interested in, and then test the learner's performance.                                                         410\t‘Beautiful\tSoup’:\thttp://www.crummy.com/software/BeautifulSoup/\t \n 359 Of course, in the case of the Old Bailey archive, we might not need this computer-assisted sorting all that badly, since the archive's curators, making use of the XML markup, offer us a ready-made search interface411 that lets us look for documents by offense type, verdict, punishment, etc. But that's exactly what makes the Old Bailey such a good resource for testing a machine learner: we can check how well the learner performs by checking its judgments against the human-annotated information in the Old Bailey documents. That, in turn, helps us decide how (or whether) a learner could help us explore other digital document collections, most of which are not as richly annotated. Machine learning Machine learning can mean a lot of different things, but the most common tasks are classification and clustering.412 Classification is performed by supervised learners — \"supervised\" meaning that a human assistant helps them learn, and only then sends them out to classify by themselves. The basic training procedure is to give the learner labeled data: that is, we give it a stack of things (documents, for example) where each of those things is labeled as belonging to a group. This is called training data. The learner then looks at each item in the training data, looks at its label, and learns what distinguishes the groups from each other. To see how well the learner learned, we then test it by giving it data that is similar to the training data but that the learner hasn't seen before and that is not labeled. This is called (you guessed it!) test data. How well the learner performs on classifying this previously-unseen data is a measure of how well it has learned. The classic case of a supervised classifier is a program that separates junk email (spam) from regular email (ham). Such a program is \"trained\" by giving it a lot of spam and ham to look at, along with the information of which is which. It then builds a statistical model of what a spam message looks like versus what a regular email message looks like. So it learns that a message is more likely to be spam if it contains sexual terms, or words like \"offer\" and \"deal\", or, as things turn out, \"ff0000,\" the HTML code for red.413 It can then apply that statistical model to incoming messages and discard the ones it identifies as spam. Clustering is usually a task for unsupervised learners. An unsupervised learner doesn't get any tips on how the data \"ought\" to be sorted, but rather is expected to discover patterns in the data automatically, grouping the data by the patterns it has discovered. Unlike in supervised classification, in unsupervised clustering we don't tell the learner what the \"right\" groups are, or give it any hints on what items in the data set should go together.                                                         411\t‘Search\tHome’,\tThe\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/forms/formMain.jsp\t412\t‘Statistical\tClassification’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Statistical_classification;\t‘Clustering\tAlgorithms’:\thttp://home.deib.polimi.it/matteucc/Clustering/tutorial_html/\t413\t‘A\tPlan\tfor\tSpam’\t(August\t2002):\thttp://www.paulgraham.com/spam.html\t \n 360 Rather, we give it data with a bunch of features, and (often, but not always) we tell it how many groups we want it to create. The features could be anything: in document clustering, they are normally words. But clustering isn't limited to grouping documents: it could also be used in, say, trying to improve diagnoses by clustering patient records. In that task, the features would be various attributes of the patient (age, weight, blood pressure, presence and quality of various symptoms etc.) and the clustering algorithm would attempt to create groups that share as many features as closely as possible. A side note: Some of you may have come to think of an objection to this supervised/unsupervised distinction: namely, that the clustering method is not entirely \"unsupervised\" either. After all, we tell it what features it should look at, whether it is words (rather than sentences, or two-word sequences, or something else) in a document, or a list of numeric values in a patient record. The learner never encounters the data entirely unprepared. Quite true. But no matter - the distinction between unsupervised and supervised is useful nevertheless, in that in one we tell the learner what the right answer is, and in the other it comes to us with some pattern it has figured out without an answer key. Each is useful for different kind of tasks, or sometimes for different approaches to the same task. In this tutorial, we are dealing with a supervised learner that we train to perform document classification. We give our learner a set of documents along with their correct classes, and then test it on a set of documents they haven't seen, with the hope that it will succeed in guessing the document's correct classification. A Naive Bayesian learner A Naive Bayesian is a supervised learner: we give it things marked with group labels, and its job is basically to learn the probability that a thing that looks a particular way belongs in a particular group. But why \"naive\"? And what \"Bayesian\"? \"Naive\" simply means that the learner makes the assumption that all the \"features\" that make up a document are independent of each other. In our case, the features are words, and so the learner assumes that the occurrence of a particular word is completely independent of the occurrence of another word. This, of course, is often not true, which is why we call it \"naive.\" For example, when we put \"new\" and \"york\" together to form \"New York,\" the result has a very different meaning than the \"new\" and \"york\" in \"New clothes for the Prince of York.\" If we were to distinguish \"New York\" from \"New\" and \"York\" occurring separately, we might find that each tends to occur in very different types of documents, and thus not identifying the expression \"New York\" might throw our classifier off course.  \n 361 Despite their simplistic assumption that the occurrence of any particular feature is independent of the occurrence of other features, Naive Bayesian classifiers do a good enough job to be very useful in many contexts (much of the real-world junk mail detection is performed by Naive Bayesian classifiers, for example). Meanwhile, the assumption of independence means that processing documents is much less computationally intensive, so a Naive Bayesian classifier can handle far more documents in a much shorter time than many other, more complex methods. That in itself is useful. For example, it wouldn’t take too long retrain a Naive Bayesian learner if we accumulated more data. Or we could give it a bigger set of data to begin with; a pile of data that a Naive Bayesian could burrow through in a day might take a more complex method weeks or even months to process. Especially when it comes to classification, more data is often as significant as a better method — as Bob Mercer of IBM famously quipped in 1985, “there is no data like more data.” As for the \"Bayesian\" part, that refers to the 18th-century English minister, statistician, and philosopher Thomas Bayes. When you google for \"Naive Bayesian,\" you will turn up a lot of references to \"Bayes' theorem\" or \"Bayes' rule,\" which is a formula for applying conditional probabilities (the probability of some thing X, given some other thing Y). Bayes' theorem is related to Naive Bayesian classifiers, in that we can formulate the classification question as \"what is the probability of document X, given class Y?\" However, unless you've done enough math and probability to be comfortable with that kind of thinking, it may not provide the easiest avenue to grasping how a Naive Bayesian classifier works. Instead, let's look at the classifier in a more procedural manner. (Meanwhile, if you prefer, you can check out an explanation of Bayes' rule and conditional probabilities414 that does a very nice job and is also a good read.) Understanding Naive Bayesian classification using a generative story To understand Naive Bayesian classification, we will start by telling a story about how documents come into being. Telling such a story — called a \"generative story\" in the business — often simplifies the probabilistic analysis and helps us understand the assumptions we're making. Telling the story takes a while, so bear with me. There is a payoff at the end: the story directly informs us how to build a classifier under the assumptions that the particular generative story makes. The fundamental assumption we will make in our generative story is that documents come into being not as a result of intellectual cogitation but as a result of a process whereby words are picked at random out of a bag and then put into a document (known as a bag-of-words model).                                                         414\tEliezer\tS.\tYudkowsky,\t‘An\tIntuitive\tExplanation\tof\tBayes’\tTheorem’:\thttp://www.yudkowsky.net/rational/bayes\t \n 362 So we pretend that historical works, for example, are written in something like the following manner. Each historian has his or her own bag of words with a vocabulary specific to that bag. So when Ann the Historian writes a book, what she does is this: She goes to the bag that is her store of words. She puts her hand in and pulls out a piece of paper. She reads the word on the piece of paper, writes it down in her book, and puts the paper back in the bag. Then she again puts her hand in the bag and pulls out a piece of paper. She writes down that word in the book, and puts the piece of paper back in the bag. Ann the Historian keeps going until she decides her book (or article, or blog post, or whatever) is finished. The next time she wants to write something, she goes back to her bag of words and does the same thing. If her friend John the Historian were to write a book, he would go to his own bag, which has a different set of words, and then he would follow the same procedure of taking out a word, writing it down, putting it back in. It's just one damn word after another. \n Bags of Words  (If this procedure sounds familiar, that may be because it sounds a bit like the generative story told in explaining how topic modeling works.415 However, the story in topic modeling is a bit different in that, for instance, each document contains words from more than one class. Also, you should note that topic modeling is unsupervised — you don't tell the modeler what the \"right\" topics are, it comes up with them all by itself.) So let's say you are a curator of a library of historical works, and one day you discover a huge forgotten trunk in the basement of the library. It turns out that the trunk contains dozens and dozens of typed book manuscripts. After some digging, you find a document that explains that these are transcripts of unpublished book drafts by three historians: Edward Gibbon, Carl Becker, and Mercy Otis Warren.                                                         415\tShawn\tGraham,\tScott\tWeingart,\tand\tIan\tMilligan,\t‘Getting\tStarted\twith\tTopic\tModeling\tand\tMALLET’,\tThe\tProgramming\tHistorian\t(2012).\t\n \n 363 What a find! But unfortunately, as you begin sorting through the drafts, you realize that they are not marked with the author's name. What can you do? How can you classify them correctly? Well, you do have other writings by these authors. And if historians write their documents in the manner described above — if each historian has his or her own bag of words with a particular vocabulary and a particular distribution of words — then we can figure out who wrote each document by looking at the words it contains and comparing the distribution of those words to the distribution of words in documents we know were written by Gibbon, Becker, and Warren, respectively. So you go to your library stacks and get out all the books by Gibbon, Becker, and Warren. Then you start counting. You start with Edward Gibbon's oeuvre. For each word in a work by Gibbon, you add the word to a list marked \"Gibbon.\" If the word is already in the list, you add to its count. Then you do the same with the works of Mercy Otis Warren and Carl Becker. Finally, for each author, you add up the total number of words you've seen. You also add up the total number of monographs you have examined so you'll have a metric for how much work each author has published. So what you end up with is something like this: Edward Gibbon (5) Carl Becker (18) Mercy Otis Warren (2) empire, 985 everyman, 756 revolution, 989 rome, 897 revolution, 699 constitution, 920 fall, 887 philosopher, 613 principles, 899 … … … (total), 352,003 (total), 745,532 (total), 300,487 What you have done, in essence, is to reconstruct each historian's \"bag of words\" — now you know (at least approximately) what words each historian uses and in what proportions. Armed with this representation of the word distributions in the works of Gibbons, Becker, and Warren, you're ready to tackle the task of figuring out who wrote which manuscripts. You're going to work manuscript by manuscript and author by author, first pretending that the manuscript you're currently considering was written by Gibbons, then that it was written by Becker, and so on. For each author, you calculate how likely it is that the manuscript really was written by that author. So with your first manuscript in hand, you start by assuming that the manuscript was written by Gibbons. First you figure out the overall probability of any monograph being written by Gibbons rather than either of the two others — that is, of the Gibbons bag rather than the Becker bag or the Warren bag being used to produce a monograph. You do this by taking the number of books written by Gibbons and dividing it by the total  \n 364 number of books written by all these authors. That comes out to 5/25, or 0.2 (20 percent). Then, you start looking at the words in the manuscript. Let's say the first word is \"fall.\" You check how often that word occurred in Gibbons' published oeuvre, and you find that the answer is 887. Then you check how many words, overall, there were in Gibbons' total works, and you note that the answer is 352,003. You divide 887 by 352,003 to get the proportional frequency (call it p) of \"fall\" in Gibbons' work (0.0025). For the next word, you do the same procedure, and then multiply the probabilities together (you multiply since each action — picking an author, or picking a word — represents an independent choice). In the end you end with a tally like this: p_bag\t*\tp_word_1\t*\tp_word_2\t*\t...\t*\tp_word_n\tNote that including the probability of picking the bag (p_bag) is an important step: if you only go by the words in the manuscript and ignore how many manuscripts (or rather, published works) each author has written, you can easily go wrong. If Becker has written ten times the number of books that Warren has, it should reasonably require much firmer evidence in the form of an ample presence of \"Warrenesque\" words to assume that a manuscript was written by Warren than that it was written by Becker. \"Extraordinary claims require extraordinary evidence,\" as Carl Sagan once said. OK, so now you have a total probability of the manuscript having been written by Gibbons. Next, you repeat the whole procedure with the assumption that maybe it was instead written by Becker (that is, that it came out of the bag of words that Becker used when writing). That done, you move on to considering the probability that the author was Warren (and if you had more authors, you'd keep going until you had covered each of them). When you're done, you have three total probabilities — one probability per author. Then you just pick out the largest one, and, as they say, Bob's your uncle! That's the author who most probably wrote this manuscript. (Minor technical note: when calculating p_bag\t*\tp_word1\t*\t...\t*\tp_word_n\tin a software implementation we actually work with the logarithms416 of the probabilities since the numbers easily become very small. When doing this, we actually calculate log(p_bag)\t+\tlog(p_word1)\t+\t...\t+\tlog(p_word_n)\t                                                        416\tKalid\tAzad,\t‘Using\tLogarithms\tin\tthe\tReal\tWorld’,\tBetter\tExplained:\thttp://betterexplained.com/articles/using-logs-in-the-real-world/\t \n 365 That is, our multiplications turn into additions in line with the rules of logarithms. But it all works out right: the class with the highest number at the end wins.) But wait! What if a manuscript contains a word that we've never seen Gibbons use before, but also lots of words he used all the time? Won't that throw off our calculations? Indeed. We shouldn't let outliers throw us off the scent. So we do something very \"Bayesian\": we put a \"prior\" on each word and each class — we pretend we've seen all imaginable words at least (say) once in each bag, and that each bag has produced at least (say) one document. Then we add those fake pretend counts — called priors,417 or pseudocounts — to our real counts. Now, no word or bag gets a count of zero. In fact, we can play around with the priors as much as we like: they're simply a way of modeling our \"prior belief\" in the probability of one thing over another. They could model our assumptions about a particular author being more likely than others, or a particular word being more likely to have come from the bag of a specific author, and so on. Such beliefs are \"prior\" in the sense that we hold the belief before we've seen the evidence we are considering in the actual calculation we are making. So above, for example, we could add a little bit to Mercy Otis Warren's p_bag number if we thought it likely that as a woman, she might well have had a harder time getting published, and so there might reasonably be more manuscript material from her than one might infer from a count of her published monographs. In some cases, priors can make a Naive Bayesian classifier much more usable. Often when we're classifying, after all, we're not after some abstract \"truth\" — rather, we simply want a useful categorization. In some cases, it's much more desirable to be mistaken one way than another, and we can model that with proper class priors. The classic example is, again, sorting email into junk mail and regular mail piles. Obviously, you really don't want legitimate messages to be deleted as spam; that could do much more damage than letting a few junk messages slip through. So you set a big prior on the \"legitimate\" class that causes your classifier to only throw out a message as junk when faced with some hefty evidence. By the same token, if you're sorting the results of a medical test into \"positive\" and \"negative\" piles, you may want to weight the positive more heavily: you can always do a second test, but if you send the patient home telling them they're healthy when they're not, that might not turn out so well. So there you have it, step by step. You have applied a Naive Bayesian to the unattributed manuscripts, and you now have three neat piles. Of course, you should keep in mind that Naive Bayesian classifiers are not                                                         417\t‘Prior\tDistributions’:\thttp://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introbayes_sect004.htm\t \n 366 perfect, so you may want to do some further research before entering the newfound materials into the library catalog as works by Gibbons, Becker, and Warren, respectively. OK, so let's code already! So, our aim is to apply a Naive Bayesian learner to data from the Old Bailey. First we get the data; then we clean it up and write some routines to extract information from it; then we write the code that trains and tests the learner. Before we get into the nitty-gritty of downloading the files and examining the training/testing script, let's just summarize what our aim is and what the basic procedure looks like. We want to have our Naive Bayesian read in trial records from the Old Bailey and do with them the same thing as we did above in the examples about the works of Gibbons, Becker, and Warren. In that example, we used the published works of these authors to reconstruct each historian's bag of words, and then used that knowledge to decide which historian had written which unattributed manuscripts. In classifying the Old Bailey trials, we will give the learner a set of trials labeled with the offense for which the defendant was indicted so it can figure out the \"bag of words\" that is associated with that offense. Then the learner will use that knowledge to classify another set of trials where we have not given it any information about the offense involved. The goal is to see how well the learner can do this: how often does it label an unmarked trial with the right offense? The procedure used in the scripts we employ to train the learner is no more complicated than the one in the historians-and-manuscripts example. Basically, each trial is represented as a list of words, like so: michael,\tcarney,\twas,\tindicted,\tfor,\tstealing,\ton,\tthe,\t22nd,\tof,\tdecember,\t26lb,\tweight,\tof,\tnails,\tvalue,\t7s,\t18,\tdozen,\tof,\tscrews,\t...\t...\t,\tthe,\tprisoners,\tcame,\tto,\tmy,\tshop,\ton,\tthe,\tnight,\tin,\tquestion,\tand,\tbrought,\tin,\tsome,\tragged,\tpieces,\tof,\tbeef,\t...\t...,\ti,\thad,\tleft,\tmy,\tdoor,\topen,\tand,\twhen,\ti,\treturned,\ti,\tmissed,\tall,\tthis,\tproperty,\ti,\tfound,\tit,\tat,\tthe,\tpawnbroker,\t...\tWhen we train the learner, we give it a series of such word lists, along with their correct bag labels (correct offenses). The learner then creates word lists for each bag (offense), so that it ends up with a set of counts similar to the counts we created for Gibbons, Becker, and Warren, one count for each offense type (theft, deception, etc.) When we test the learner, we feed it the same sort of word lists representing other trials. But this time we don't give it the information about what offense was involved. Instead, the learner does what we did above: when it gets a list of words, it compares that list to the word counts for each offense type, calculating which offense type has a bag of words most similar to this list of words. It works offense by offense, just like we  \n 367 worked author by author. So first it assumes that the trial involved, say, the offense \"theft\". It looks at the first word in the trial's word list, checks how often that word occurred in the \"theft\" bag, performs its probability calculations, moves on to the next word, and so on. Then it checks the trial's word list against the next category, and the next, until it has gone through each offense. Finally it tallies up the probabilities and labels the trial with the offense category that has the highest probability. Finally, the testing script evaluates the performance of the learner and lets us know how good it was at guessing the offense associated with each trial. Preliminaries Many of the tools we are using to deal with the preliminaries have been discussed at Programming Historian before. You may find it helpful to check out (or revisit) the following tutorials: Milligan & Baker, ‘Introduction to the Bash Command Line’ Milligan, ‘Automated Downloading with wget’ Knox, ‘Understanding Regular Expressions’ Wieringa, ‘Intro to Beautiful Soup’ A few words about the file structure the scripts assume/create: I have a \"top-level\" directory, which I'm calling bailey (you could call it something else, it's not referenced in the code). Under that I have two directories: baileycode and baileyfiles. The first contains all the scripts; the second contains the files that are either downloaded or created by the scripts. That in turn has subdirectories; all except one (for the downloaded XML files — see below) are created by the scripts. If you downloaded the complete zip package with all the files and scripts, you automatically get the right structure; just unpack it in its own directory. The only files that are omitted from that are the zip files of trials downloaded below (if you got the complete package, you already have the unpacked contents of those files, and the zips would just take up unnecessary space). If you only downloaded the scripts, you should do the following: Create a directory and name it something sensible (say, bailey). In that directory, create another directory called baileycode and unpack the contents of the script zip file into that directory (make sure you don't end up with two baileycode directories inside one another). In the same directory (bailey), create another directory called baileyfiles. On my Mac, the structure looks like this:  \n 368 \n Bailey Folders Downloading trials The Old Bailey lets you download trials in zip files of 10 trials each, so that's what we'll do. This is how we do it: we first look at how the Old Bailey system requests files, and then we write a script that creates a file with a bunch of those requests. Then we feed that file to wget, so we don't have to sit by our computers all day downloading each set of 10 trials that we want. As explained on the Old Bailey ‘documentation for developers’ page,418 this is what the http request for a set of trials looks like: http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&term1=todate_18391216&count=10&start=211&return=zip\t\t\t\t\t\tAs you see, we can request all trials that took place between two specified dates (fromdate and todate). The count specifies how many trials we want, and the start variable says where in the results to start (in the above, we start with result number 211 and get the ten following trials). Ten seems to be the highest number allowed for count, so we need to work around that. We get around the restriction for how many trials can be in a zip file with a little script that builds as many of the above type of requests as we need to get all trials from the 1830s. We can find out how many trials that is by going to the Old Bailey ‘search page’419 and entering January 1830 as the start date, December 1839 as the end date, and choosing \"Old Bailey Proceedings > trial accounts\" in the \"Search In\" field. Turns out there were 22,711 trials in the 1830s. Here's the whole script (wgetxmls.py) that creates the list of http requests we need:                                                         418\t‘Documentation\tfor\tDevelopers’,\tThe\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/static/DocAPI.jsp\t419\t‘Search\tHome’,\tThe\t\tOld\tBailey\tOnline:\thttp://www.oldbaileyonline.org/forms/formMain.jsp\t\n \n 369 \t\t\t\tmainoutdirname\t=\t'../baileyfiles/'\t\t\t\t\twgets\t=\t''\t\t\t\t\tfor\tx\tin\trange(0,22711,10):\t\t\t\t\t\t\t\t\tgetline\t=\t'http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&term1=todate_18391216&count=10&start='\t+\tstr(x+1)\t+\t'&return=zip\\n'\t\t\t\t\t\t\t\t\twgets\t+=\tgetline\t\t\t\t\tfilename\t=\tmainoutdirname\t+\t'wget1830s.txt'\t\t\t\t\twith\topen(filename,'w')\tas\tf:\t\t\t\t\t\t\t\t\tf.write(wgets)\tAs you see, we accept the limitation of 10 trials at a time, but manipulate the start point until we have covered all the trials from the 1830s. Assuming you're in the baileycode directory, you can run the script from the command line like this: python\twgetxmls.py\tWhat that gets us is a file that looks like this: http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&term1=todate_18391216&count=10&start=1&return=zip\thttp://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&term1=todate_18391216&count=10&start=11&return=zip\thttp://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&term1=todate_18391216&count=10&start=21&return=zip\t...\tThis file is saved in the baileyfiles directory; it is called wget1830s.txt. To download the trials, create a new directory under baileyfiles; call it trialzips. Then go into that directory and call wget with the file we just created. So, assuming you are still in the baileycode directory, you would write the following commands on the command line: cd\t../baileyfiles\tmkdir\ttrialzips\tcd\ttrialzips\twget\t-w\t2\t-i\t../wget1830s.txt\tThe \"-w 2\" is just to be polite and not overload the server; it tells wget to wait 2 seconds between each request. The \"-i\" flag tells wget that it should request the URLs found in wget1830s.txt. What wget returns is a lot of zip files that have unwieldy names and no extension. You should rename these so that the extension is \".zip\". Then, in the directory baileyfiles, create a subdirectory called 1830s-trialxmls and then unpack the zips into that so that it contains 22,170 XML files that each look like t18391216-388.xml. Assuming you are still in the trialzips directory, you would write: for\tf\tin\t*\t;\tdo\tmv\t$f\t$f.zip;\tdone;\tmkdir\t../1830s-trialxmls\tunzip\t\"*.zip\"\t-d\t../1830s-trialxmls/\t \n 370 If you open one of the trial XMLs in a browser, you can see that it contains all kinds of useful information: name and gender of defendant, name and gender of witnesses, type of offense, and so on. Here's a snippet from one trial: <persname\tid=\"t18300114-2-defend110\"\ttype=\"defendantName\">\tTHOMAS\tTAYLOR\t\t\t\t\t<interp\tinst=\"t18300114-2-defend110\"\ttype=\"surname\"\tvalue=\"TAYLOR\">\t\t\t\t\t<interp\tinst=\"t18300114-2-defend110\"\ttype=\"given\"\tvalue=\"THOMAS\">\t\t\t\t\t<interp\tinst=\"t18300114-2-defend110\"\ttype=\"gender\"\tvalue=\"male\">\t\t\t\t\t<interp\tinst=\"t18300114-2-defend110\"\ttype=\"age\"\tvalue=\"25\">\t</interp></interp></interp></interp></persname>\twas\tindicted\tfor\t\t\t\t\t\t<rs\tid=\"t18300114-2-off7\"\ttype=\"offenceDescription\">\t\t\t\t\t\t\t\t\t<interp\tinst=\"t18300114-2-off7\"\ttype=\"offenceCategory\"\tvalue=\"violentTheft\">\t\t\t\t\t\t\t\t\t<interp\tinst=\"t18300114-2-off7\"\ttype=\"offenceSubcategory\"\tvalue=\"robbery\">\t\t\t\t\t\t\t\t\t\t\t\t\tfeloniously\tassaulting\t\t\t\t\t\t\t\t\t\t<persname\tid=\"t18300114-2-victim112\"\ttype=\"victimName\">\t\t\t\t\t\t\t\t\t\t\t\t\tDavid\tGrant\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<interp\tinst=\"t18300114-2-victim112\"\ttype=\"surname\"\tvalue=\"Grant\">\t\t\t\t\t\t\t\t\t\t\t\t\t<interp\tinst=\"t18300114-2-victim112\"\ttype=\"given\"\tvalue=\"David\">\t\t\t\t\t\t\t\t\t\t\t\t\t<interp\tinst=\"t18300114-2-victim112\"\ttype=\"gender\"\tvalue=\"male\">\t\t\t\t\t\t\t\t\t\t\t\t\t<join\tresult=\"offenceVictim\"\ttargorder=\"Y\"\ttargets=\"t18300114-2-off7\tt18300114-2-victim112\">\t\t\t\t\t\t\t\t\t</join></interp></interp></interp></persname>\t\t\t\t</interp></interp></rs>\tThe structured information in the XML lets us reliably extract the \"classes\" we want to sort our documents into. We are going to classify the trials by offense category (and subcategory), so that's the information we're going to extract before converting the XML into a text file that we can then feed to our learner. Saving the trials into text files Now that we have the XML files, we can start extracting information and plain text from them to feed to our learner. We want to sort the trials into text files, so that each text file contains all the trials in a particular offense category (theft-simplelarceny, breakingpeace-riot, etc.). We also want to create a text file that contains all the trial IDs (marked in the XML), so we can use that to easily create cross-validation samples. The reasons for doing this are discussed below in the section \"Creating the cross-validation samples\". The script that does these things is called save-txttrials-by-category.py and it's pretty extensively commented, so I'll just note a few things here. We strip the trial text of all punctuation, including quote marks and parentheses, and we equalize all spaces (newlines, tabs, multiple spaces)  \n 371 into a single space. This helps us simplify the coding of the training process (and, incidentally, keep the code that trains the learner general enough that as long as you have text files saved in the same format as we use here, you should be able to apply it more or less directly to your data). That of course makes the text hard to read for a human. Therefore, we also save the text of each trial into a file named after the trial id, so that we can easily examine a particular trial if we want to (which we will). The script creates the following directories and files under baileyfiles: Directory 1830s-trialtxts: this will contain the text file versions of the trials after they have been stripped of all XML formatting. Each file is named after the trial's ID. Directory 1830s-trialsbycategory: this will contain the text files that represent all the text in all the trials belonging to a particular category. These are named after the category, e.g., theft-simplelarceny.txt. Each category file contains all the trials in that category, with one trial per line. File trialids.txt. This contains the sorted list of trial IDs, one ID per line; we will use it later in creating cross-validation samples for training the learner (this is the next step). Files offensedict.json and trialdict.json. These json files will come into use in training the learner. So if you're still in the trialxmls directory, you would write the following commands to run this script: cd\t../../baileycode/\tpython\tsave-trialtxts-by-category.py\t\tThis will take a while. After it's done, you should have the directories and files described above. Creating the cross-validation samples Now that we have all our trials saved where we want them, all we need to do is to create the cross-validation samples and we're ready to test our learners. Cross-validation simply means repeatedly splitting our data into chunks, some of which we use for training and others for testing. Since the idea is to get a learner to extract information from one set of documents that it can then use to determine the class of documents it has never seen, we obviously have to reserve a set of documents that are unknown to the learner if we want to test its performance. Otherwise it's a bit like letting your students first read an exam and its answers and then have them take that same exam. That would only tell you how closely they read the actual exam, not whether they've learned something more general. So what you want to do is to test the learner on data it hasn't seen before, so that you can tell whether it has learned some general principles from the training data. You could just split your data into two sets, using, say, 80  \n 372 percent for training and 20 percent for testing. But a common practice is to split your data repeatedly into different test and train sets, so that you can ensure that your test results aren't the consequence of some oddball quirk in the portion of data you left for testing. Two scripts are involved in creating the cross-validation sets. The script tenfold-crossvalidation.py creates the samples. It reads in the list of trial ids we created in the previous step, shuffles that list to make it random, and divides it into 10 chunks of roughly equal length (that is, a roughly equal number of trial ids). Then it writes those 10 chunks each into its own text file, so we can read them into our learner code later. Next, to be meticulous, we can run the count-offense-instances.py to confirm that if we are interested in a particular trial category, that category is reasonably evenly distributed across the samples. Before you run the count-offense-instances.py script, you should edit it to set the category to the one you're interested in and let the script know whether we're looking at a broad or a specific category. This is what the relevant part of the code looks like: indirname\t=\t'../baileyfiles/'\toffensedict_fn\t=\tindirname\t+\t'offensedict.json'\toffensecat\t=\t'breakingpeace'\t#change\tto\ttarget\tcategory\tbroadcat\t=\tTrue\t#set\ttrue\tif\tcategory\tis\te.g.\t\"theft\"\tinstead\tof\t\"theft-simplelarceny\"\tAnd here are the commands to run the cross-validation scripts (assuming you are still in the baileycode directory). python\ttenfold-crossvalidation.py\t\tpython\tcount-offense-instances.py\t\tAlternatively, you can run them using pypy,420 which is quite a bit faster. pypy\ttenfold-crossvalidation.py\t\tpypy\tcount-offense-instances.py\t\tThe output of the count-offense-instances.py script looks like this: Offense\tcategory\tchecked\tfor:\tbreakingpeace\tsample0.txt:\t31\tsample1.txt:\t25\tsample2.txt:\t32\tsample3.txt:\t25\tsample4.txt:\t36\tsample5.txt:\t33\tsample6.txt:\t29\tsample7.txt:\t35\tsample8.txt:\t27\tsample9.txt:\t31\t                                                        420\t‘pypy’:\thttp://pypy.org/\t \n 373 From the output, we can conclude that the distribution of instances of \"breakingpeace\" is more or less even. If it isn't, we can re-run the tenfold-crossvalidation.py script, and then check the distribution again. Testing the learner All right, we are ready train and test our Naive Bayesian! The script that does this is called test-nb-learner.py. It starts by defining a few variables: categoriesdir\t=\t'../baileyfiles/1830s-trialsbycategory/'\tsampledirname\t=\t'../baileyfiles/Samples_1830s/'\t#location\tof\t10-fold\tcross-validation\tstopwordfilename\t=\t'../baileyfiles/english-stopwords.txt'\t#\tthe\tones\tbelow\tshould\tbe\tset\tto\tNone\tif\tnot\tusing\tcattocheck\t=\t'breakingpeace'\t#if\tevaluating\trecognition\tone\tcategory\tagainst\trest\tpattern\t=\t'[^-]+'\t#regex\tpattern\tto\tuse\tif\tcategory\tis\tnot\tcomplete\tfilename\tMost of these are pretty self-explanatory, but note the two last ones. The variable \"cattocheck\" determines whether we are looking to identify a specific category or to sort each trial into its proper category (the latter is done if the variable is set to None). The variable \"pattern\" tells us whether we are using the whole file name as the category designation, or only a part of it, and if the latter, how to identify the part. In the example above, we are focusing on the broad category \"breakingpeace\", and so we are not using the whole file name, which would be e.g. \"breakingpeace-riot\" but only the part before the dash. Before you run the code, you should set these variables to what you want them to be. Note that \"cattocheck\" here should match the \"offensecat\" that you checked for with the count-offense-instances.py script. No error is produced if it does not match, and it's fairly unlikely that it will have any real impact, but if the categories don't match, then of course you have no assurance that the category you're actually interested in is more or less evenly distributed across the ten cross-validation samples. Note also that you can of course set \"cattocheck\" to \"None\" and leave the pattern as it is, in which case you will be sorting into the broader categories. So, with the basic switches set and knobs turned, we begin by reading in all the trials that we have saved. We do this with the function called process_data that can be found in the pretestprocessing.py file. (That file contains functions that are called from the scripts you will run, so it isn't something you'll run directly at any point.) print\t'Reading\tin\tthe\tdata...'\ttrialdata\t=\tptp.process_data(categoriesdir,stopwordfilename,cattocheck,pattern)\t \n 374 The process_data function reads in all the files in the directory that contains our trial category files, and processes them so that we get a list containing all the categories and the trials belonging to them, with the trial text lowercased and tokenized (split into a list of words), minus stopwords (common words like a, the, me, which, etc.) Each trial begins with its id number, so that's one of our words (though we ignore it in training and testing). Like this: \t\t\t\t[\t\t\t\t\t\t[breakingpeace,\t\t\t\t\t\t\t\t\t['trialid','victim','peace','disturbed','man','tree',...]\t\t\t\t\t\t\t\t['trialid','dress','blood','head','incited',...]\t\t\t\t\t\t\t...]\t\t\t\t\t\t[theft,\t\t\t\t\t\t\t\t\t['trialid','apples','orchard','basket','screamed','guilty',....]\t\t\t\t\t\t\t\t['trialid','rotten','fish']\t\t\t\t\t\t\t...]\t\t\t\t\t]\tNext, making use of the results of the ten-fold cross-validation routine we created, we loop through the files that define the samples, each time making one sample the test set and the rest the train set. Then we split 'trialdata', the list of trials-by-category that we just created, into train and test sets accordingly. The functions that do these two steps are create_sets and splittraintest, both in the pretestprocessing.py file. Now we train our Naive Bayesian classifier on the train set. The classifier we are using (which is included in the scripts zip file) is one written by Mans Hulden, and it does pretty much exactly what the \"identify the author of the manuscript\" example above describes. \t\t\t\t#\tsplit\ttrain\tand\ttest\t\t\t\t\t\tprint\t'Creating\ttrain\tand\ttest\tsets,\trun\t{0}'.format(run)\t\t\t\t\ttrainsetids,\ttestsetids\t=\tptp.create_sets(sampledirname,run)\t\t\t\t\ttraindata,\ttestdata\t=\tptp.splittraintest(trainsetids,testsetids,trialdata)\t\t\t\t\t\t\t\t\t\t#\ttrain\tlearner\t\t\t\t\tprint\t'Training\tlearner,\trun\t{0}...'.format(run)\t\t\t\t\tmynb\t=\tnb.naivebayes()\t\t\t\t\tmynb.train(traindata)\tAfter the learner is trained, we are ready to test how well the it performs. Here's the code: \t\t\t\tprint\t'Testing\tlearner,\trun\t{0}...'.format(run)\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\ttrialset\tin\ttestdata:\t\t\t\t\t\t\t\t\tcorrectclass\t=\ttrialset[0]\t\t\t\t\t\t\t\t\tfor\ttrial\tin\ttrialset[1:]:\t\t\t\t\t\t\t\t\t\t\t\t\tresult\t=\tmynb.classify(trial)\t\t\t\t\t\t\t\t\t\t\t\t\tguessedclass\t=\t\tmax(result,\tkey=result.get)\t\t\t\t\t\t\t\t\t\t\t\t\t#\tthen\trecord\tcorrectness\tof\tclassification\tresult\t\t\t\t\t\t\t\t\t\t\t\t\t#\tnote\tthat\tfirst\tversion\tdoes\ta\tmore\tcomplex\tevaluation\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t...\tfor\ttwo-way\t(one\tclass\tagainst\trest)\tclassification\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tcattocheck:\t\t\t\t\t\t\t \n 375 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tcorrectclass\t==\tcattocheck:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcatinsample\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tguessedclass\t==\tcattocheck:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tguesses\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tguessedclass\t==\tcorrectclass:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thits\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\tif\tguessedclass\t==\tcorrectclass:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcorrectguesses\t+=\t1\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttotal\t+=1\tSo we loop through the categories in the \"testdata\" list (which is of the same format as the \"trialdata\" list). For each category, we loop through the trials in that category, classifying each trial with our Naive Bayesian classifier, and comparing the result to the correct class (saved in the first element of each category list within the testdata list.) Then we add to various counts to be able to evaluate the results of the whole classification exercise. To run the code that trains and tests the learner, first make sure you have edited it to set the \"cattocheck\" and \"pattern\" switches, and then call it on the command line (assuming you're still in the directory baileycode): \t\t\t\tpython\ttest-nb-learner.py\t\t\t\tAgain, for greater speed, you can also use pypy: \t\t\t\tpypy\ttest-nb-learner.py\t\t\t\t\t\tThe code will print out some accuracy measures for the classification task you have chosen. The output should look something like this: Reading\tin\tthe\tdata...\tCreating\ttrain\tand\ttest\tsets,\trun\t0\tTraining\tlearner,\trun\t0...\tTesting\tlearner,\trun\t0...\tCreating\ttrain\tand\ttest\tsets,\trun\t1\tTraining\tlearner,\trun\t1...\t...\tTraining\tlearner,\trun\t9...\tTesting\tlearner,\trun\t9...\tSaving\tcorrectly\tclassified\ttrials\tand\tclose\tmatches...\tCalculating\taccuracy\tof\tclassification...\tTwo-way\tclassification,\ttarget\tcategory\tbreakingpeace.\tAnd\tthe\tresults\tare:\tAccuracy\t99.00%\tPrecision:\t61.59%\tRecall:\t66.45%\tAverage\tnumber\tof\ttarget\tcategory\ttrials\tin\ttest\tsample\tper\trun:\t30.4\tAverage\tnumber\tof\ttrials\tin\ttest\tsample\tper\trun:\t2271.0\tObtained\tin\t162.74\tseconds\tNext, let's take a look at what these measures of accuracy mean.  \n 376 Measures of classification The basic measure of classification prowess is accuracy: how often did classifier guess the class of a document correctly? This is calculated by simply dividing the number of correct guesses by the total number of documents considered. If we're interested in a specific category, we can extract a bit more data. So if we set, for example, cattocheck = 'breakingpeace', like above, we can then examine how well the classifier did with respect to the \"breakingpeace\" category in particular. So, in the testlearner code, if we're doing multiway classification, we only record how many trials we've seen (\"total\") and how many of our guesses were correct (\"correctguesses\"). But if we're considering a single category, say \"breakingpeace,\" we record a few more numbers. First, we keep track of how many trials belonging to the category \"breakingpeace\" there are in our test sample (this tally is in \"catinsample\"). We also keep track of how many times we've guessed that a trial belongs to the \"breakingpeace\" category (\"guesses\"). And finally we record how many times we have guessed correctly that a trial belongs to \"breakingpeace\" (\"hits\"). Now that we have this information, we can use it to calculate a couple of standard measures of classification efficiency: precision and recall. Precision tells us how often we correctly guessed that a trial was in the \"breakingpeace\" category. Recall lets us know what proportion of the \"breakingpeace\" trials we caught. Let's take another example to clarify precision and recall. Imagine you want all the books on a particular topic — World War I, say — from your university library. You send out one of your many minions (all historians possess droves of them, as you know) to get the books. The minion dutifully returns with a big pile. Now, suppose you were in possession of a list that contained of every single book in the library on WWI and no books that weren't related to the WWI. You could then check the precision and recall of your minion with regard to the category of \"books on WWI.\" Recall is the term for the proportion of books on WWI in the library that your minion managed to grab. That is, the more books on WWI remaining in the library after your minion's visit, the lower your minion's recall. Precision, in turn, is the term for the proportion of books in the pile brought by your minion that actually had to do with WWI. The more irrelevant (off-topic) books in the pile, the lower the precision. So, say the library has 1,000 books on WWI, and your minion lugs you a pile containing 400 books, of which 300 have nothing to do with WWI. The minion's recall would be (400-300)/1,000, or 10 percent. The minion's precision, in turn, would be (400-300)/400, or 25 percent.  \n 377 (Should have gone yourself, eh?) A side note: the minion's overall accuracy — correct guesses divided by actual number of examples — would be: (the\tnumber\tof\tbooks\ton\tWWI\tin\tyour\tpile\t-\tthe\tnumber\tof\tbooks\t*not*\ton\tWWI\tin\tyour\tpile\t+\tthe\tnumber\tof\tbooks\tin\tthe\tlibrary\t*not*\ton\tWWI)\t------------------------------------------------------------------------\tthe\ttotal\tnumber\tof\tbooks\tin\tthe\tlibrary\tSo if the library held 100,000 volumes, this would be (100 - 300 + 99,000) / 100,000 — or 98.8 percent. That seems like a great number, but since it merely means that your minion was smart enough to leave most of the library books in the library, it's not very helpful in this case (except inasmuch as it is nice not to be buried under 100,000 volumes.) How well does our Naive Bayesian do? Our tests on the Naive Bayesian use the data set consisting of all the trials from the 1830s. It contains 17,549 different trials in 50 different offense categories (which can be grouped into 9 broad categories). If we run the Naive Bayesian so that it attempts to sort all trials into their correct broad categories, its accuracy is pretty good: 94.3 percent. So 94 percent of the time, when it considers how it should sort trials in the test sample into \"breakingpeace,\" \"deception,\" \"theft,\" and so on, it chooses correctly. For the more specific categories (\"theft-simplelarceny,\" \"breakingpeace-riot,\" and so on) the same exercise is much less accurate: then, the classifier gets it right only 72 percent of the time. That's no wonder, really, given that some of the categories are so small that we barely have any examples. We might do a bit better with more data (say, all the trials from the whole 19th century, instead of only all the trials from the 1830s). The first, overall category results are pretty impressive. They give us quite a bit of confidence that if what we needed to do was to sort documents into piles that weren't all too fine-grained, and we had a nice bunch of training data, a Naive Bayesian could do the job for us. But the problem for a historian is often rather different. A historian using a Naive Bayesian learner is more likely to want to separate documents that are \"interesting\" from documents that are \"not interesting\" — usually meaning documents dealing with a particular issue or not dealing with it. So the question is really more one where we have a mass of \"uncategorized\" or \"other\" documents and a much smaller set of \"interesting\" documents, and we try to find more of the latter among the former. In our current exercise, that situation is fairly well represented by trying to identify documents from a single category in the mass of the rest of the documents, set to category \"other.\" So how well are we able to do that? In other words, if we set cattocheck = 'breakingpeace' (or another category) so  \n 378 that all trials get marked as either that category or as \"other,\" and then run the classifier, what kinds of results do we get? Well, our overall accuracy is still high: over 95 percent in all cases for the broad categories, and usually about that for the detailed ones as well. But just like the minion going off to the library to get books on WWI had a pretty high accuracy because he/she didn't bring back half the library, in this case, too, our accuracy is mostly just due to the fact that we manage to not misidentify too many \"other\" trials as being in the category we're interested in. Because there are so many \"other\" trials, those correct assessments far outweigh the minus points we may have gotten from missing interesting trials. Precision and recall, therefore, are more in this case more interesting measures than overall accuracy. Here's a table showing precision and recall for each of the \"broad\" categories in our trial sample, and for a few sample detailed categories. The last column shows how many target category trials there were in the test set on average (remember, we did ten runs with different train/test splits, so all our results are averages of that). Naive Bayesian classifier, two-way classification, 10-fold cross-validation Broad categories Category Precision (%) Recall (%) Avg # trials in cat in TeS breakingpeace 63.52 64.05 24.2 damage 0.00 0.00 1.2 deception 53.47 61.43 47.7 kill 62.5 89.39 17.9 miscellaneous 47.83 4.44 24.8 royaloffenses 85.56 91.02 42.3 sexual 93.65 49.17 24.0 theft 96.26 98.75 1551.8 violenttheft 68.32 33.01 20.9 Sample detailed categories Category Precision (%) Recall (%) Avg # trials in cat in TeS theft-simplelarceny 64.37 89.03 805.9 theft-receiving 92.21 61.53 198.1 deception-forgery 74.29 11.87 21.9 violenttheft-robbery 68.42 31.86 20.4 theft-extortion 0.00 0.00 1.3 There are a few generalizations we can make from these numbers.  \n 379 First, it's obvious that if the category is too small, we are out of luck. So for \"damage,\" a small enough broad category that our test samples only held a little over one instance of it on average, we get no results. Similarly, in the detailed categories, when the occurrence of cases per test sample drops into the single digits, we fail miserably. This is no wonder: if the test sample contains about one case on average, there can't be much more than ten cases total in the whole data set. That's not much to go on. Second, size isn't everything. Although we do best for the biggest category, theft (which in fact accounts for over half our sample), there are some smaller categories we do very well for. We have very high recall and precision for \"royaloffenses,\" a mid-sized category, and very high recall plus decent precision for \"kill,\" our smallest reasonable-sized category. A reasonable guess would be that the language that occurs in the trials is distinctive and, in the case of \"royaloffenses,\" doesn't occur much anywhere else. Meanwhile, unsurprisingly, we get low scores for the \"miscellaneous\" category. We also have high precision for the \"sexual\" category, indicating that it has some language that doesn't tend to appear anywhere else — though we miss about half the instances of it, which would lead us to suspect that many of the trials in that category omit some of the language that most distinguishes it from others. Third, in this sample at least, there seems to be no clear pattern regarding whether the learner has better recall or better precision. Sometimes it casts a wide net that drags in both a good portion of the category and some driftwood, and sometimes it handpicks the trials for good precision but misses a lot that don't look right enough for its taste. So in half the cases here, our learner has better precision than recall, and in half better recall than precision. The differences between precision and recall are, however, bigger for the cases where precision is better than recall. That isn't necessarily a good thing for us, since as historians we might be happier to see more of the \"interesting\" documents and do the additional culling ourselves than to have our learner miss a lot of good documents. We'll return to the question of the meaning of classification errors below. Extracting the most indicative features The naivebayes.py script has a feature that allows you to extract the most (and least) indicative features of your classification exercise. This allows you to see what weighs a lot in the learner's mind — what it has, in effect, learned. The command to issue is: mynb.topn_print(10) (for the 10 most indicative; you can put in any number you like). Here are the results for a multi-way classification of the broad categories in our data: deception\t['norrington',\t'election',\t'flaum',\t'polish',\t'caton',\t'spicer',\t'saltzmaun',\t'newcastle',\t'stamps',\t'rotherham']\troyaloffences\t['mould',\t'coster',\t'coin',\t'caleb',\t'counterfeit',\t'obverse',\t'mint',\t'moulds',\t'plaster-of-paris',\t'metal']\tviolenttheft\t['turfrey',\t'stannard',\t'millward',\t'falcon',\t'crawfurd',\t'weat \n 380 herly',\t'keith',\t'farr',\t'ventom',\t'shurety']\tdamage\t['cow-house',\t'ewins',\t'filtering-room',\t'fisk',\t'calf',\t'skirting',\t'girder',\t'clipping',\t'saturated',\t'firemen']\tbreakingpeace\t['calthorpe-street',\t'grievous',\t'disable',\t'mellish',\t'flag',\t'bodily',\t'banner',\t'aforethought',\t'fursey',\t'emerson']\tmiscellaneous\t['trevett',\t'teuten',\t'reitterhoffer',\t'quantock',\t'feaks',\t'boone',\t'bray',\t'downshire',\t'fagnoit',\t'ely']\tkill\t['vault',\t'external',\t'appearances',\t'slaying',\t'deceased',\t'marchell',\t'disease',\t'pedley',\t'healthy',\t'killing']\ttheft\t['sheep',\t'embezzlement',\t'stealing',\t'table-cloth',\t'fowls',\t'dwelling-house',\t'missed',\t'pairs',\t'breaking',\t'blankets']\tsexual\t['bigamy',\t'marriage',\t'violate',\t'ravish',\t'marriages',\t'busher',\t'register',\t'spinster',\t'bachelor',\t'married']\tSome of these make sense instantly. In \"breakingpeace\" (which includes assaults, riots and woundings) you can see the makings of phrases like \"grievous bodily harm\" and \"malice aforethought,\" along with other indications of wreaking havoc like \"disable\" and \"harm.\" In royaloffenses, the presence of \"mint,\" \"mould\" and \"plaster-of-paris\" make sense since the largest subcategory is coining offenses. In \"theft,\" one might infer that sheep, fowl, and table-cloths seem to have been popular objects for stealing (though table-cloth may of course have been a wrapping for stolen objects; one would have to examine the trials to know). Others are more puzzling. Why is violenttheft almost exclusively composed of what seem to be person or place names? Why is \"election\" indicative of deception? Is there a lot of election fraud going on, or abuse of elected office? Looking at the documents, one finds that 9 of the words indicative of violent theft are person names, and one is a pub; why person and pub names should be more indicative here than for other categories is mildly intriguing and might bear further analysis (or might just be a quirk of our data set — remember that \"violenttheft\" is a fairly small category). As for \"election,\" it's hard to distinguish a clear pattern, though it seems to be linked to fraud attempts on and by various officials at different levels of government. The indicative features, then, may be intriguing in themselves (though obviously, one should not draw any conclusions about them without closely examining the data first). They are also useful in that they can help us determine whether something is skewing our results in a way we don't wish, something we may be able to correct for with different weighting or different selection of features (see the section on Tuning below). The meanings of misclassification Again, it's good to keep in mind that in classifying documents we are not always after an abstract \"true\" classification, but simply a useful or interesting one. Thus, it is a good idea to look a bit more closely at the \"errors\" in classification. We'll focus on two-way classification, and look at the cases where the Naive Bayesian incorrectly includes a trial in a category (false positives) as well  \n 381 as take a look at trials it narrowly excludes from the category (let's call them close relatives). In the script for testing the learner (test-nb-learner.py), we saved the trial ids for false positives and close relatives so we could examine them later. Here's the relevant code bit: result\t=\tmynb.classify(trial)\tguessedclass\t=\t\tmax(result,\tkey=result.get)\t\t\t\t\t\tif\tcattocheck:\t\t\t\t\tdiff\t=\tabs(result[cattocheck]\t-\tresult['other'])\t\t\t\t\tif\tdiff\t<\t10\tand\tguessedclass\t!=\tcattocheck:\t\t\t\t\t\t\t\t\tclosetrials.append(trial[0])\t\t\t\t\t\t\t\t\tdifflist.append(diff)\t\t\t\t\tif\tcorrectclass\t==\tcattocheck:\t\t\t\t\t\t\t\t\tcatinsample\t+=\t1\t\t\t\t\tif\tguessedclass\t==\tcattocheck:\t\t\t\t\t\t\t\t\t\tguesses\t+=\t1\t\t\t\t\t\t\t\t\t\tif\tguessedclass\t==\tcorrectclass:\t\t\t\t\t\t\t\t\t\t\t\t\t\thits\t+=\t1\t\t\t\t\t\t\t\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfalsepositives.append(trial[0])\tif\tguessedclass\t==\tcorrectclass:\t\t\t\t\tcorrectguesses\t+=\t1\tFalse positives are easy to catch: we simply save the cases where we guessed that a trial belonged to the category but it really did not. For close relatives, we first check how confident we were that the trial did not belong in our category. When we issue the call to classify the trial mynb.classify(trial), it returns us a dictionary that looks like this: {\t\t\t\t\t'other':\t-2358.522248351527,\t\t\t\t\t\t'violenttheft-robbery':\t-2326.2878233211086\t}\tSo to find the close relatives, we compare these two values, and if the difference between them is small, we save the id of the trial we are currently classifying into a list of close relatives. (In the code chunk above, we have rather arbitrarily defined a \"small\" difference as being under 10). At the end of the script, we write the results of these operations into two text files: falsepositives.txt and closerelatives.txt. Let's look more closely at misclassifications for the category \"violenttheft-robbery.\" Here are the first 10 rows of the close relatives file and the first 20 rows of the false positives file, sorted by offense: Close relatives    \n 382 breakingpeace-wounding,\tt18350105-458,\t1.899530878\ttheft-pocketpicking,\tt18310407-90,\t0.282424548\ttheft-pocketpicking,\tt18380514-1168,\t0.784184742\ttheft-pocketpicking,\tt18301028-208,\t0.797341405\ttheft-pocketpicking,\tt18341016-85,\t1.296811989\tviolenttheft-robbery,\tt18370102-317,\t1.075548985\tviolenttheft-robbery,\tt18350921-2011,\t1.105672712\tviolenttheft-robbery,\tt18310407-204,\t1.521788666\tviolenttheft-robbery,\tt18370102-425,\t1.840718222\tviolenttheft-robbery,\tt18330214-13,\t2.150018805\tFalse positives breakingpeace-assault,\tt18391021-2933\tbreakingpeace-wounding,\tt18350615-1577\tbreakingpeace-wounding,\tt18331017-159\tbreakingpeace-wounding,\tt18350615-1578\tbreakingpeace-wounding,\tt18330704-5\tkill-manslaughter,\tt18350706-1682\tkill-manslaughter,\tt18360919-2161\tkill-manslaughter,\tt18380618-1461\tkill-murder,\tt18330103-7\tkill-murder,\tt18391021-2937\tmiscellaneous-pervertingjustice,\tt18340904-144\ttheft-pocketpicking,\tt18300114-128\ttheft-pocketpicking,\tt18310407-66\ttheft-pocketpicking,\tt18330905-92\ttheft-pocketpicking,\tt18370703-1639\ttheft-pocketpicking,\tt18301028-127\ttheft-pocketpicking,\tt18310106-87\ttheft-pocketpicking,\tt18331017-109\ttheft-pocketpicking,\tt18320216-108\ttheft-pocketpicking,\tt18331128-116\tThe first thing we notice is that many of the close relatives are in fact from our target category — they are cases that our classifier has narrowly missed. So saving these separately could compensate nicely for an otherwise low recall number. The second thing we notice is that more of the false positives seem to have to do with violence, whereas more of the close relatives seem to have to do with stealing; it seems our classifier has pegged the violence aspect of robberies as more significant in distinguishing them than the filching aspect. The third thing we notice is that theft-pocketpicking is a very common category among both the close relatives and the false positives. And indeed, if we look at a sample trial from violenttheft-robbery and another from among the close pocketpicking relatives, we notice that there are definitely close similarities. For example, trial t18310407-90, the closest close relative, involved a certain Eliza Williams indicted for pocketpicking. Williams was accused of stealing a watch and some other items from a certain Thomas Turk; according to Turk and his friend, they had been pub-crawling, Eliza  \n 383 Williams (whom they did not know from before) had tagged along with them, and at one point in the evening had pocketed Turk's watch (Turk, by this time, was quite tipsy). Williams was found guilty and sentenced to be confined for one year. Meanwhile, in trial t18300708-14, correctly classed as violenttheft-robbery, a man called Edward Overton was accused of feloniously assaulting a fellow by the name of John Quinlan. Quinlan explained that he had been out with friends, and when he parted from them he realized it was too late to get into the hotel where he worked as a waiter and (apparently) also had lodgings. Having nowhere to go, he decided to visit a few more public-houses. Along the way, he met Overton, whom he did not know from before, and treated him to a few drinks. But then, according to Quinlan, Overton attacked him as they were walking from one pub to another, and stole his watch as well as other possessions of his. According to Overton, however, Quinlan had given him the watch as a guarantee that he would repay Overton if Overton paid for his lodging for the night. Both men, it seems, were thoroughly drunk by the end of the evening. Overton was found not guilty. Both trials, then, are stories of groups out drinking and losing their possessions; what made the latter a trial for robbery rather than for pocketpicking was simply Quinlan's accusation that Overton had \"struck him down.\" For a historian interested in either robberies or pocketpickings (or pub-crawling in 1830s London), both would probably be equally interesting. In fact, the misclassification patterns of the learner indicate that even when data is richly annotated, such as in the case of the Old Bailey, using a machine learner to extract documents may be useful for a historian: in this case, it would help you extract trials from different offense categories that share features of interest to you, regardless of the offense label. Tuning The possibilities for tuning are practically endless. For example, you might consider tweaking your data. For instance, instead of giving your classifier all the words in the document, you might present it with a reduced set. One way of reducing the number of words is to collapse different words together through stemming. So the verb forms \"killed,\" \"kills,\" \"killing\" would all become \"kill\" (as would the plural noun \"kills\"). A popular stemmer is the Snowball Stemmer,421 and you could add that to the tokenization step. (I ran a couple of tests with this, and while it made the process much slower, it didn't much improve the results. But that would probably depend a bit on the kind of data you have.)                                                         421\t‘Snoball’:\thttp://snowball.tartarus.org/\t \n 384 Another way is to select the words you give to the classifier according to some principle. One common solution is to pick only the words with a high TF-IDF score. TF-IDF is short for \"term frequency - inverse document frequency,\" and a high score means that the term occurs quite frequently in the document under consideration but rarely in documents in general. (You can also check out a more detailed explanation of TF-IDF,422 along with some Python code for calculating it.) Other options include simply playing with the size of the priors: now, the Naive Bayesian has a class prior as well as a feature prior of 0.5, meaning that it pretends to have seen all classes and all words at least one-half times. Doing test runs with different priors might get you different results. In addition to simply changing the general prior sizes, you might consider having the classifier set a higher prior on the target category than on the \"other\" category, in effect requiring less evidence to include a trial in the target category. It might be worth a try particularly since we noted above when examining the close relatives (under Meanings of Misclassification) that many of them were in fact members of our target category. Setting a larger prior on the target class would probably catch those cases, boosting the recall. At the same time, it probably would also lower the precision. (To change the priors, you need to edit the naivebayes.py script.) As you can see, there is quite a lot of fuzziness here: how you pick the features, how you pick the priors, and how you weight various priors all affect the results you get, and how to pick and weight is not governed by hard logic but is rather a process of trial and error. Still, like we noted noted in the section on the meaning of classification error above, if your goal is to get some interesting data to do historical analysis on, some fuzziness may not be such a big problem. About the Author Vilja Hulden is a history instructor and research associate in the Departments of History and Linguistics and the University of Colorado Boulder.  \n                                                        422\tSteven\tLoria\t‘Tutorial:\tFinding\tImportant\tWords\tin\tText\tUsing\tTF-IDF’\t\t(1\tSeptember\t2013):\thttp://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\t \n 385 Part Five: Presenting History The digital environment offers many opportunities for displaying and presenting historical materials in a number of ways. The lessons in this part explore some of those opportunities by sharing modes of putting history online. From basic web pages, to web exhibits, to digital maps. This is about putting the products of your efforts out there.        \n 386 36. Understanding Web Pages and HTML William J. Turkel and Adam Crymble – 2012   Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Python Introduction and Installation’.423  \"Hello World\" in HTML Viewing HTML files When you are working with online sources, much of the time you will be using files that have been marked up with HTML (Hyper Text Markup Language). Your browser already knows how to interpret HTML, which is handy for human readers. Most browsers also let you see the HTML source code for any page that you visit. The two images below show a typical web page (from the Old Bailey Online)424 and the HTML source used to generate that page, which you can see with the Tools\t->\tWeb\tDeveloper\t->\tPage\tSource command in Firefox. When you're working in the browser, you typically don't want or need to see the source for a web page. If you are writing a page of your own, however, it can be very useful to see how other people accomplished a particular effect. You will also want to study HTML source as you write programs to manipulate web pages or automatically extract information from them. \n                                                        423\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Python\tIntroduction\tand\tInstallation’,\tThe\tProgramming\tHistorian\t(2012).\t424\tTim\tHitchcock,\tRobert\tShoemaker,\tClive\tEmsley,\tSharon\tHoward\tand\tJamie\tMcLaughlin,\tet\tal.,\tThe\tOld\tBailey\tProceedings\tOnline,\t1674-1913\t(www.oldbaileyonline.org,\tversion\t7.0,\t24\tMarch\t2012).\t \n 387 \n Old Bailey Online Screenshot \n HTML source for Old Bailey Online web page (To learn more about HTML, you may find it useful at this point to work through the W3 Schools HTML tutorial.425 Detailed knowledge of HTML isn't immediately necessary to continue reading, but any time that you spend learning HTML will be amply rewarded in your work as a digital historian or digital humanist.) \"Hello World\" in HTML HTML is what is known as a markup language. In other words, HTML is text that has been \"marked up\" with tags that provide information for the interpreter (which is often a web browser). Suppose you are formatting a bibliographic entry and you want to indicate the title of a work by italicizing it. In HTML you use em tags (\"em\" stands for emphasis). So part of your HTML file might look like this                                                         425\t‘HTML\t(5)\tTutorial’,\tW3\tSchools:\thttp://www.w3schools.com/html/default.asp\t\n \n 388 ...\tin\tCohen\tand\tRosenzweig's\t<em>Digital\tHistory</em>,\tfor\texample\t...\tThe simplest HTML file consists of tags which indicate the beginning and end of the whole document, and tags which identify a head and a body within that document. Information about the file usually goes into the head, whereas information that will be displayed on the screen usually goes into the body. <html>\t<head></head>\t<body>Hello\tWorld!</body>\t</html>\tYou can try creating some HTML code. Go to your text editor, and create a new file. Copy the code below into the editor. The first line tells the browser what kind of file it is. The html tag has the text direction set to ltr (left to right) and the lang (language) set to US English. The title tag in the head of the HTML document contains material that is usually displayed in the top bar of a window when the page is being viewed, and in Firefox tabs. <!doctype\thtml>\t<html\tdir=\"ltr\"\tlang=\"en-US\">\t\t<head>\t\t\t\t\t<title><!--\tInsert\tyour\ttitle\there\t--></title>\t</head>\t\t<body>\t\t\t\t\t<!--\tInsert\tyour\tcontent\there\t-->\t</body>\t</html>\tChange both <!--\tInsert\tyour\ttitle\there\t-->\tand <!--\tInsert\tyour\tcontent\there\t-->\tto Hello\tWorld!\tSave the file to your programming-historian directory as hello-world.html. Now go to Firefox and choose File\t->\tNew\tTab and then File\t->\tOpen\tFile. Choose hello-world.html. Depending on your text editor you may have a 'view page in browser' or 'open in browser' option. Once you have opened the file, your message should appear in the browser. Note the difference between opening an HTML file with a browser like Firefox (which interprets it) and opening the same file with your text editor (which does not).  \n 389 Suggested readings for learning HTML  W3 Schools HTML Tutorial426    If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Working with Text Files in Python’.427  About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        426\t‘HTML\t(5)\tTutorial’,\tW3\tSchools:\thttp://www.w3schools.com/html/default.asp\t427\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Working\twith\tText\tFiles\tin\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 390 37. Output Data as an HTML File with Python William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Creating and Viewing HTML Files with Python’.428 Lesson Goals This lesson takes the frequency pairs created in ‘Counting Word Frequencies with Python’429 and outputs them to an HTML file. Here you will learn how to output data as an HTML file using Python. You will also learn about string formatting. The final result is an HTML file that shows the keywords found in the original source in order of descending frequency, along with the number of times that each keyword appears. Files Needed For This Lesson obo.py If you do not have these files from the previous lesson, you can download programming-historian-3, a zip file from the previous lesson: http://programminghistorian.org/assets/programming-historian3.zip Building an HTML wrapper In the previous lesson, you learned how to embed the message \"Hello World!\" in HTML tags, write the result to a file and open it automatically in the browser. A program that puts formatting codes around something so that it can be used by another program is sometimes called a wrapper. What we're going to do now is develop an HTML wrapper for the output of our code that computes word frequencies. We're also going to add some helpful, dynamic metadata to supplement the frequency data collected in ‘Counting Word Frequencies with Python’. Metadata The distinction between data and metadata is crucial to information science. Metadata are data about data. This concept should already be very familiar to you, even if you haven't heard the term before. Consider a traditional book. If we take the text of the book to be the data, there are a                                                         428\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Creating\tand\tViewing\tHTML\tFiles\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t429\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 391 number of other characteristics which are associated with that text, but which may or may not be explicitly printed in the book. The title of the work, the author, the publisher, and the place and date of publication are metadata that are typically printed in the work. The place and date of writing, the name of the copy editor, Library of Congress cataloging data, and the name of the font used to typeset the book are sometimes printed in it. The person who purchased a particular copy may or may not write their name in the book. If the book belongs in the collection of a library, that library will keep additional metadata, only some of which will be physically attached to the book. The record of borrowing, for example, is usually kept in some kind of database and linked to the book by a unique identifier. Libraries, archives and museums all have elaborate systems to generate and keep track of metadata. When you're working with digital data, it is a good idea to incorporate metadata into your own files whenever possible. We will now develop a few basic strategies for making our data files self-documenting. In our wrapper, we want to include dynamic information about the file, such as the time and date it was created, as well as an HTML title that is relevant to the file. In this case we could just give it a name ourselves, but when we start working with multiple files, automatically creating self-documenting files will save a lot of time, so we’ll practice now. And for that, we'll have to learn to take advantage of a few more powerful string formatting options. Python string formatting Python includes a special formatting operator that allows you to insert one string into another one. It is represented by a percent sign followed by an \"s\". Open a Python shell and try the following examples. frame\t=\t'This\tfruit\tis\ta\t%s'\tprint\tframe\t->\tThis\tfruit\tis\ta\t%s\t\tprint\tframe\t%\t'banana'\t->\tThis\tfruit\tis\ta\tbanana\t\tprint\tframe\t%\t'pear'\t->\tThis\tfruit\tis\ta\tpear\tThere is also a form which allows you to interpolate a list of strings into another one. frame2\t=\t'These\tare\t%s,\tthose\tare\t%s'\tprint\tframe2\t->\tThese\tare\t%s,\tthose\tare\t%s\t\tprint\tframe2\t%\t('bananas',\t'pears')\t->\tThese\tare\tbananas,\tthose\tare\tpears\tIn these examples, a %s in one string indicates that another string is going to be embedded at that point. There are a range of other string formatting  \n 392 codes, most of which allow you to embed numbers in strings in various formats, like %i for integer (eg. 1, 2, 3), %f for floating-point decimal (eg. 3.023, 4.59, 1.0), and so on. Using this method we can input information that is unique to the file. Self-documenting data file Let's bundle some of the code that we've already written into functions. One of these will take a URL and return a string of lowercase text from the web page. Copy this code into the obo.py module. #\tGiven\ta\tURL,\treturn\tstring\tof\tlowercase\ttext\tfrom\tpage.\t\tdef\twebPageToText(url):\t\t\t\t\timport\turllib2\t\t\t\t\tresponse\t=\turllib2.urlopen(url)\t\t\t\t\thtml\t=\tresponse.read()\t\t\t\t\ttext\t=\tstripTags(html).lower()\t\t\t\t\treturn\ttext\tWe're also going to want a function that takes a string of any sort and makes it the body of an HTML file which is opened automatically in Firefox. This function should include some basic metadata, like the time and date that it was created and the name of the program that created it. Study the following code carefully, then copy it into the obo.py module. Mac Instructions If you are using a Mac, make sure you include the proper file path in the filename variable on the 2nd last line to reflect where you're saving your files. #\tGiven\tname\tof\tcalling\tprogram,\ta\turl\tand\ta\tstring\tto\twrap,\t#\toutput\tstring\tin\thtml\tbody\twith\tbasic\tmetadata\tand\topen\tin\tFirefox\ttab.\tdef\twrapStringInHTML(program,\turl,\tbody):\t\t\t\t\timport\tdatetime\t\t\t\t\tfrom\twebbrowser\timport\topen_new_tab\t\t\t\t\t\tnow\t=\tdatetime.datetime.today().strftime(\"%Y%m%d-%H%M%S\")\t\t\t\t\tfilename\t=\tprogram\t+\t'.html'\t\t\t\t\tf\t=\topen(filename,'w')\t\t\t\t\twrapper\t=\t\"\"\"<html>\t\t\t\t\t<head>\t\t\t\t\t<title>%s\toutput\t-\t%s</title>\t\t\t\t\t</head>\t\t\t\t\t<body><p>URL:\t<a\thref=\\\"%s\\\">%s</a></p><p>%s</p></body>\t\t\t\t\t</html>\"\"\"\t\t\t\t\t\twhole\t=\twrapper\t%\t(program,\tnow,\turl,\turl,\tbody)\t\t\t\t\tf.write(whole)\t\t\t\t\tf.close()\t\t\t\t\t#Change\tthe\tfilepath\tvariable\tbelow\tto\tmatch\tthe\tlocation\tof\tyour\tdirectory\t\t\t\t\tfilename\t=\t'file:///Users/username/Desktop/programming-historian/'\t+\tfilename\t\t\t\t\t\topen_new_tab(filename)\t \n 393 Windows Instructions #\tGiven\tname\tof\tcalling\tprogram,\ta\turl\tand\ta\tstring\tto\twrap,\t#\toutput\tstring\tin\thtml\tbody\twith\tbasic\tmetadata\t#\tand\topen\tin\tFirefox\ttab.\t\tdef\twrapStringInHTML(program,\turl,\tbody):\t\t\t\t\timport\tdatetime\t\t\t\t\tfrom\twebbrowser\timport\topen_new_tab\t\t\t\t\t\tnow\t=\tdatetime.datetime.today().strftime(\"%Y%m%d-%H%M%S\")\t\t\t\t\t\tfilename\t=\tprogram\t+\t'.html'\t\t\t\t\tf\t=\topen(filename,'w')\t\t\t\t\t\twrapper\t=\t\"\"\"<html>\t\t\t\t\t<head>\t\t\t\t\t<title>%s\toutput\t-\t%s</title>\t\t\t\t\t</head>\t\t\t\t\t<body><p>URL:\t<a\thref=\\\"%s\\\">%s</a></p><p>%s</p></body>\t\t\t\t\t</html>\"\"\"\t\t\t\t\t\twhole\t=\twrapper\t%\t(program,\tnow,\turl,\turl,\tbody)\t\t\t\t\tf.write(whole)\t\t\t\t\tf.close()\t\t\t\t\t\topen_new_tab(filename)\tNote that this function makes use of the string formatting operator about which we just learned. If you are still having trouble with this idea, take a look at the HTML file that opened in your new Firefox tab and you should see how this worked. If you're still stuck, take a look at the URL:\thttp://www.oldbaileyonline.org/print.jsp?div=t17800628-33\tin the HTML file and trace back how the program knew to put the URL value there. The function also calls the Python datetime library to determine the current time and date. Like the string formatting operator %s, this library uses the % as replacements for values. In this case, the %Y\t%m\t%d\t%H\t%M\t%S represents year, month, date, hour, minute and second respectively. Unlike the %s, the program will determine the value of these variables for you using your computer's clock. It is important to recognize this difference. This date metadata, along with the name of the program that called the function, is stored in the HTML title tag. The HTML file that is created has the same name as the Python program that creates it, but with a .html extension rather than a .py one. Putting it all together Now we can create another version of our program to compute frequencies. Instead of sending its output to a text file or an output window, it sends the output to an HTML file which is opened in a new Firefox tab. From there, the program's output can be added easily as bibliographic entries to Zotero.  \n 394 Type or copy the following code into your text editor, save it as html-to-freq-3.py and execute it, to confirm that it works as expected. #\thtml-to-freq-3.py\timport\tobo\t\t#\tcreate\tsorted\tdictionary\tof\tword-frequency\tpairs\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\ttext\t=\tobo.webPageToText(url)\tfullwordlist\t=\tobo.stripNonAlphaNum(text)\twordlist\t=\tobo.removeStopwords(fullwordlist,\tobo.stopwords)\tdictionary\t=\tobo.wordListToFreqDict(wordlist)\tsorteddict\t=\tobo.sortFreqDict(dictionary)\t\t#\tcompile\tdictionary\tinto\tstring\tand\twrap\twith\tHTML\toutstring\t=\t\"\"\tfor\ts\tin\tsorteddict:\t\t\t\t\toutstring\t+=\tstr(s)\t\t\t\t\toutstring\t+=\t\"<br\t/>\"\tobo.wrapStringInHTML(\"html-to-freq-3\",\turl,\toutstring)\tNote that we interspersed our word-frequency pairs with the HTML break tag <br\\>, which acts as a newline. If all went well, you should see the same word frequencies that you computed in the last section, this time in your browser window. Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each chapter you can download the \"programming-historian\" zip file to make sure you have the correct code. If you are following along with the Mac / Linux version you may have to open the obo.py file and change \"file:///Users/username/Desktop/programming-historian/\" to the path to the directory on your own computer. Mac/Linux: http://programminghistorian.org/assets/programming-historian-mac-linux.zip Windows: http://programminghistorian.org/assets/programming-historian-windows.zip  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Keywords in Context (Using n-grams) with Python’.430 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.                                                        430\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Keywords\tin\tContext\t(Using\tn-grams)\twith\tPython)’,\tThe\tProgramming\tHistorian,\t2012.\t \n 395 38. Creating and Viewing HTML Files with Python William J. Turkel and Adam Crymble – 2012  Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Counting Word Frequencies with Python’.431 Lesson Goals This lesson uses Python to create and view an HTML file. If you write programs that output HTML, you can use any browser to look at your results. This is especially convenient if your program is automatically creating hyperlinks or graphic entities like charts and diagrams. Here you will learn how to create HTML files with Python scripts, and how to use Python to automatically open an HTML file in Firefox. Files Needed For This Lesson obo.py If you do not have these files from the previous lesson, you can download programming-historian-3, a zip file from the previous lesson: http://programminghistorian.org/assets/programming-historian3.zip Creating HTML with Python At this point, we’ve started to learn how to use Python to download online sources and extract information from them automatically. Remember that our ultimate goal is to incorporate programming seamlessly into our research practice. In keeping with this goal, in this lesson and the next, we will learn how to output data back as HTML. This has a few advantages. First, by storing the information on our hard drive as an HTML file we can open it with Firefox and use Zotero432 to index and annotate it later. Second, there are a wide range of visualization options for HTML which we can draw on later. If you have not done the W3 Schools HTML tutorial yet,433 take a few minutes to do it before continuing. We’re going to be creating an HTML document using Python, so you will have to know what an HTML document is!                                                         431\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Counting\tWord\tFrequencies\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t432\t‘Zotero’:\thttp://zotero.org/\t433\t‘HTML\t(5)\tTutorial’,\tW3\tSchools:\thttp://www.w3schools.com/html/default.asp\t \n 396 \"Hello World\" in HTML using Python One of the more powerful ideas in computer science is that a file that seems to contain code from one perspective can be seen as data from another. It is possible, in other words, to write programs that manipulate other programs. What we're going to do next is create an HTML file that says \"Hello World!\" using Python. We will do this by storing HTML tags in a multiline Python string and saving the contents to a new file. This file will be saved with an .html extension rather than a .txt extension. Typically an HTML file begins with a doctype declaration.434 You saw this when you wrote an HTML \"Hello World\" program in an earlier lesson. To make reading our code easier, we will omit the doctype in this example. Recall a multi-line string is created by enclosing the text in three quotation marks (see below). #\twrite-html.py\t\tf\t=\topen('helloworld.html','w')\t\tmessage\t=\t\"\"\"<html>\t<head></head>\t<body><p>Hello\tWorld!</p></body>\t</html>\"\"\"\t\tf.write(message)\tf.close()\tSave the above program as write-html.py and execute it. Use File -> Open in your chosen text editor to open helloworld.html to verify that your program actually created the file. The content should look like this: \n HTML Source Generated by Python Program  Now go to your Firefox browser and choose File -> New Tab, go to the tab, and choose File -> Open File. Select helloworld.html. You should now be able to see your message in the browser. Take a moment to think about                                                         434\t‘HTML\t<!DOCTYPE>\tDeclaration’:\thttp://www.w3schools.com/tags/tag_doctype.asp\t\n \n 397 this: you now have the ability to write a program which can automatically create a webpage. There is no reason why you could not write a program to automatically create a whole website if you wanted to. Using Python to Control Firefox We automatically created an HTML file, but then we had to leave our editor and go to Firefox to open the file in a new tab. Wouldn't it be cool to have our Python program include that final step? Type or copy the code below and save it as write-html-2.py. When you execute it, it should create your HTML file and then automatically open it in a new tab in Firefox. Sweet! Mac Instructions Mac users will have to specify to the precise location of the .html file on their computer. To do this, locate the programming-historian folder you created to do these tutorials, right-click it and select \"Get Info\". You can then cut and paste the file location listed after \"Where:\" and make sure you include a trailing slash (/) to let the computer know you want something inside the directory (rather than the directory itself). #\twrite-html-2.py\timport\twebbrowser\t\tf\t=\topen('helloworld.html','w')\t\tmessage\t=\t\"\"\"<html>\t<head></head>\t<body><p>Hello\tWorld!</p></body>\t</html>\"\"\"\t\tf.write(message)\tf.close()\t\t#Change\tpath\tto\treflect\tfile\tlocation\tfilename\t=\t'file:///Users/username/Desktop/programming-historian/'+'helloworld.html'\twebbrowser.open_new_tab(filename)\tIf you're getting a \"File not found\" error you haven't changed the filename path correctly. \n 398 Windows Instructions #\twrite-html-2.py\t\timport\twebbrowser\t\tf\t=\topen('helloworld.html','w')\t\tmessage\t=\t\"\"\"<html>\t<head></head>\t<body><p>Hello\tWorld!</p></body>\t</html>\"\"\"\t\tf.write(message)\tf.close()\t\twebbrowser.open_new_tab('helloworld.html')\tNot only have you written a Python program that can write simple HTML, but you've now controlled your Firefox browser using Python. In the next lesson, we turn to outputting the data that we have collected as an HTML file. Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each chapter you can download the \"programming-historian\" zip file to make sure you have the correct code. If you are following along with the Mac / Linux version you may have to open the obo.py file and change \"file:///Users/username/Desktop/programming-historian/\" to the path to the directory on your own computer. programming-historian [Mac / Linux] (zip)  http://programminghistorian.org/assets/programming-historian-mac-linux.zip programming-historian [Windows] (zip)  http://programminghistorian.org/assets/programming-historian-windows.zip  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Output Data as an HTML File with Python’.435 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.                                                        435\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Output\tData\tas\tan\tHTML\tFile\twith\tPython’,\tThe\tProgramming\tHistorian,\t2012.\t \n 399 39. Output Keywords in Context in an HTML File with Python William J. Turkel and Adam Crymble – 2012   Editor’s Note: This lesson was originally written as part of a series of ‘Intro to Python’ lessons. You may find it easier to complete if you have already completed the previous lesson in this series: ‘Keywords in Context (Using n-grams) with Python’.436  Lesson Goals This lesson builds on ‘Keywords in Context (Using N-grams)’, where n-grams were extracted from a text. Here, you will learn how to output all of the n-grams of a given keyword in a document downloaded from the Internet, and display them clearly in your browser window. Files Needed For This Lesson obo.py If you do not have these files from the previous lesson, you can download a zip file from the previous lesson: http://programminghistorian.org/assets/programming-historian3.zip Making an N-Gram Dictionary Our n-grams have an odd number of words in them for a reason. At this point, our n-grams don\"t actually have a keyword; they're just a list of words. However, if we have an odd numbered n-gram the middle word will always have an equal number of words to the left and to the right. We can then use that middle word as our keyword. For instance, [\"it\", \"was\", \"the\", \"best\", \"of\", \"times\", \"it\"] is a 7-gram of the keyword \"best\". Since we have a long text, we want to be able to output all n-grams for our keyword. To do this we will put each n-gram into a dictionary, using the middle word as the key. To figure out the keyword for each n-gram we can use the index positions of the list. If we are working with 5-grams, for example, the left context will consist of terms indexed by 0, 1, the keyword will be indexed by 2, and the right context terms indexed by 3, 4. Since Python indexes start at 0, a 5-gram's keyword will always be at index position 2.                                                         436\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Keywords\tin\tContext\t(Using\tn-grams)\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 400 That's fine for 5-grams, but to make the code a bit more robust, we want to make sure it will work for any length n-gram, assuming its length is an odd number. To do this we'll take the length of the n-gram, divide it by 2 and drop the remainder. We can achieve this using Python's floor\tdivision operator, represented by two slashes, which divides and then returns an answer to the nearest whole number, always rounding down – hence the term \"floor\". print\t(7\t//\t2)\tprint\t(5\t//\t2)\tprint\t(3\t//\t2)\tLet's build a function that can identify the index position of the keyword when given an n-gram with an odd number of words. Save the following to obo.py. #\tGiven\ta\tlist\tof\tn-grams\tidentify\tthe\tindex\tof\tthe\tkeyword.\t\tdef\tnGramsToKWICDict(ngrams):\t\t\t\t\tkeyindex\t=\tlen(ngrams[0])\t//\t2\t\t\t\t\t\treturn\tkeyindex\tTo determine the index of the keyword, we have used the len property to tell us how many items are in the first n-gram, then used floor division to isolate the middle index position. You can see if this worked by creating a new program, get-keyword.py and running it. If all goes well, since we are dealing with a 5-gram, you should get 2 as the index position of the keyword as we determined above. import\tobo\t\ttest\t=\t'this\ttest\tsentence\thas\teight\twords\tin\tit'\tngrams\t=\tobo.getNGrams(test.split(),\t5)\t\tprint\tobo.nGramsToKWICDict(ngrams)\tNow that we know the location of the keywords, let's add everything to a dictionary that can be used to output all KWIC n-grams of a particular keyword. Study this code and then replace your nGramsToKWICDict with the following in your obo.py module. #\tGiven\ta\tlist\tof\tn-grams,\treturn\ta\tdictionary\tof\tKWICs,\t#\tindexed\tby\tkeyword.\t\tdef\tnGramsToKWICDict(ngrams):\t\t\t\t\tkeyindex\t=\tlen(ngrams[0])\t//\t2\t\t\t\t\t\tkwicdict\t=\t{}\t\t\t\t\t\tfor\tk\tin\tngrams:\t\t\t\t\t\t\t\t\tif\tk[keyindex]\tnot\tin\tkwicdict:\t\t\t\t\t\t\t\t\t\t\t\t\tkwicdict[k[keyindex]]\t=\t[k]\t\t\t\t\t\t\t\t\telse:\t \n 401 \t\t\t\t\t\t\t\t\t\t\t\tkwicdict[k[keyindex]].append(k)\t\t\t\t\treturn\tkwicdict\tA for loop and if statement checks each n-gram to see if its keyword is already stored in the dictionary. If it isn't, it's added as a new entry. If it is, it's appended to the previous entry. We now have a dictionary named kwicdict that contains all the n-grams, sortable by keyword and we can turn to the task of outputting the information in a more useful format as we did in ‘Output Data as HTML File’.437 Try rerunning the get-keyword.py program and you should now see what's in your KWIC dictionary. Outputting to HTML Pretty Printing a KWIC \"Pretty printing\" is the process of formatting output so that it can be easily read by human beings. In the case of our keywords in context, we want to have the keywords lined up in a column, with the terms in the left-hand context right justified, and the terms in the right-hand context left justified. In other words, we want our KWIC display to look something like this: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tamongst\tthem\ta\tblack\tthere\twas\tone\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfirst\tsaw\tthe\tblack\ti\tturned\tto\t\t\t\t\t\t\t\t\t\t\t\t\t\thad\tobserved\tthe\tblack\tin\tthe\tmob\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsay\twho\tthat\tblack\twas\tno\tseeing\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ti\tsaw\ta\tblack\tat\tfirst\tbut\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tswear\tto\tany\tblack\tyes\tthere\tis\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tswear\tto\ta\tblack\tthan\tto\ta\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t...\tThis technique is not the best way to format text from a web designer's perspective. If you have some experience with HTML we encourage you to use another method that will create a standards compliant HTML file, but for new learners, we just can't resist the ease of the technique we're about to describe. After all, the point is to integrate programming principles quickly into your research. To get this effect, we are going to need to do a number of list and string manipulations. Let's start by figuring out what our dictionary output will look like as it currently stands. Then we can work on refining it into what we want.                                                            437\tWilliam\tJ.\tTurkel\tand\tAdam\tCrymble,\t‘Output\tData\tas\tan\tHTML\tFile\twith\tPython’,\tThe\tProgramming\tHistorian\t(2012).\t \n 402 #\thtml-to-pretty-print.py\timport\tobo\t\t#\tcreate\tdictionary\tof\tn-grams\tn\t=\t7\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\ttext\t=\tobo.webPageToText(url)\tfullwordlist\t=\tobo.stripNonAlphaNum(text)\tngrams\t=\tobo.getNGrams(fullwordlist,\tn)\tworddict\t=\tobo.nGramsToKWICDict(ngrams)\t\tprint\tworddict[\"black\"]\tAs you can see when you run the above program, the output is not very readable yet. What we need to do is split the n-gram into three parts: before the keyword, the keyword, and after the keyword. We can then use the techniques learned in the previous chapters to wrap everything in HTML so that it is easy to read. Using the same slice method as above, we will create our three parts. Open a Python shell and try the following examples. Pay close attention to what appears before and after the colon in each case. Knowing how to manipulate the slice method is a powerful skill for a new programming historian. #\tcalculate\tthe\tlength\tof\tthe\tn-gram\tkwic\t=\t'amongst\tthem\ta\tblack\tthere\twas\tone'.split()\tn\t=\tlen(kwic)\tprint\tn\t->\t7\t\t#\tcalculate\tthe\tindex\tposition\tof\tthe\tkeyword\tkeyindex\t=\tn\t//\t2\tprint\tkeyindex\t->\t3\t\t#\tdisplay\tthe\titems\tbefore\tthe\tkeyword\tprint\tkwic[:keyindex]\t->\t['amongst',\t'them',\t'a']\t\t#\tdisplay\tthe\tkeyword\tonly\tprint\tkwic[keyindex]\t->\tblack\t\t#\tdisplay\tthe\titems\tafter\tthe\tkeyword\tprint\tkwic[(keyindex+1):]\t->\t['there',\t'was',\t'one']\tNow that we know how to find each of the three segments, we need to format each to one of three columns in our display. The right-hand context is simply going to consist of a string of terms separated by blank spaces. We’ll use the join method to turn the list entries into a string.  \n 403 print\t'\t'.join(kwic[(keyindex+1):])\t->\tthere\twas\tone\tWe want the keywords to have a bit of whitespace padding around them. We can achieve this by using a string method called center, which will align the text to the middle of the screen. We can add padding by making the overall string be longer than the keyword itself. The expression below adds three blank spaces (6/2) to either side of the keyword. We've added hash marks at the beginning and end of the expression so you can see the leading and trailing blanks. print\t'#'\t+\tstr(kwic[keyindex]).center(len(kwic[keyindex])+6)\t+\t'#'\t->\t#\t\t\tblack\t\t\t#\tFinally, we want the left-hand context to be right justified. Depending on how large n is, we are going to need the overall length of this column to increase. We do this by defining a variable called width and then making the column length a multiple of this variable (we used a width of 10 characters, but you can make it larger or smaller as desired). The rjust method handles right justification. Once again, we've added hash marks so you can see the leading blanks. width\t=\t10\tprint\t'#'\t+\t'\t'.join(kwic[:keyindex]).rjust(width*keyindex)\t+\t'#'\t->\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tamongst\tthem\ta#\tWe can now combine these into a function that takes a KWIC and returns a pretty-printed string. Add this to the obo.py module. Study the code to make sure you understand it before moving on. #\tGiven\ta\tKWIC,\treturn\ta\tstring\tthat\tis\tformatted\tfor\t#\tpretty\tprinting.\t\tdef\tprettyPrintKWIC(kwic):\t\t\t\t\tn\t=\tlen(kwic)\t\t\t\t\tkeyindex\t=\tn\t//\t2\t\t\t\t\twidth\t=\t10\t\t\t\t\t\toutstring\t=\t'\t'.join(kwic[:keyindex]).rjust(width*keyindex)\t\t\t\t\toutstring\t+=\tstr(kwic[keyindex]).center(len(kwic[keyindex])+6)\t\t\t\t\toutstring\t+=\t'\t'.join(kwic[(keyindex+1):])\t\t\t\t\t\treturn\toutstring\tPutting it All Together We can now create a program that, given a URL and a keyword, wraps a KWIC display in HTML and outputs it in Firefox. This program begins and ends in a similar fashion as the program that computed word frequencies. Type or copy the code into your text editor, save it as html-to-kwic.py, and execute it.   \n 404 #\thtml-to-kwic.py\t\timport\tobo\t\t#\tcreate\tdictionary\tof\tn-grams\tn\t=\t7\turl\t=\t'http://www.oldbaileyonline.org/print.jsp?div=t17800628-33'\t\ttext\t=\tobo.webPageToText(url)\tfullwordlist\t=\t('#\t'\t*\t(n//2)).split()\tfullwordlist\t+=\tobo.stripNonAlphaNum(text)\tfullwordlist\t+=\t('#\t'\t*\t(n//2)).split()\tngrams\t=\tobo.getNGrams(fullwordlist,\tn)\tworddict\t=\tobo.nGramsToKWICDict(ngrams)\t\t#\toutput\tKWIC\tand\twrap\twith\thtml\ttarget\t=\t'black'\toutstr\t=\t'<pre>'\tif\tworddict.has_key(target):\t\t\t\t\tfor\tk\tin\tworddict[target]:\t\t\t\t\t\t\t\t\toutstr\t+=\tobo.prettyPrintKWIC(k)\t\t\t\t\t\t\t\t\toutstr\t+=\t'<br\t/>'\telse:\t\t\t\t\toutstr\t+=\t'Keyword\tnot\tfound\tin\tsource'\t\toutstr\t+=\t'</pre>'\tobo.wrapStringInHTML('html-to-kwic',\turl,\toutstr)\tThe first part is the same as above. In the second half of the program, we've wrapped everything in the HTML pre tag (pre-formatted), which tells the browser not to monkey with any of the spacing we've added. Also, notice that we use the has_key dictionary method to make sure that the keyword actually occurs in our text. If it doesn't, we can print a message for the user before sending the output to Firefox. Try changing the target variable to a few other keywords. Try one you know isn't there to make sure your program doesn't output something when it shouldn't. We have now created a program that looks for a keyword in a dictionary created from an HTML page on the web, and then outputs the n-grams of that keyword to a new HTML file for display on the web. All of the lessons up to this point have included parts of Python vocabulary and methods needed to create this final program. By referring to those lessons, you can now experiment with Python to create programs that accomplish specific tasks that will help in your research process. Code Syncing To follow along with future lessons it is important that you have the right files and programs in your \"programming-historian\" directory. At the end of each chapter you can download the \"programming-historian\" zip file to make sure you have the correct code. If you are following along with the Mac / Linux version you may have to open the obo.py file and change  \n 405 \"file:///Users/username/Desktop/programming-historian/\" to the path to the directory on your own computer. programming-historian [Mac / Linux] (zip)  http://programminghistorian.org/assets/programming-historian-mac-linux.zip programming-historian [Windows] (zip)  http://programminghistorian.org/assets/programming-historian-windows.zip  If you are following along the ‘Intro to Python’ lessons in order, the next lesson in this sequence is ‘Downloading Multiple Records Using Query Strings’.438 About the Authors William J. Turkel is a professor of history at Western University. Adam Crymble is a lecturer of digital history at the University of Hertfordshire.\n                                                        438\tAdam\tCrymble,\t‘Downloading\tMultiple\tRecords\tUsing\tQuery\tStrings’,\tThe\tProgramming\tHistorian,\t2012.\t \n 406 40. Up and Running with Omeka.net Miriam Posner – 2013    ‘Omeka’439 is a free content management system440 that makes it easy to create websites that show off collections of items. As you will learn below, there are actually two versions of Omeka: Omeka.net and Omeka.org. In this lesson you willl be using the former. If you would rather learn how to install Omeka yourself, read the Jonathan Reeve’s lesson on, ‘Installing Omeka’.441 Omeka is an ideal solution for historians who want to display collections of documents, archivists who want to organize artifacts into categories, and teachers who want students to learn about the choices involved in assembling historical collections. It is not difficult, but it is helpful to start off with some basic terms and concepts. In this lesson, you will sign up for an account at Omeka.net and start adding digital objects to your site. When might Omeka.net be the right choice for your website? You have a set of items you want to display on the web. Omeka is designed to display collections. The content of the collection can be anything from physical objects, to photographs, to people, or even ideas. To make the most of Omeka you should have lots of items that you want to show off. You want to tell stories with those items. With Omeka, you can create exhibits: narrative walk-throughs of items. You want to preserve complete information about each object. Omeka excels at metadata;442 that is, information about the items in your collection. With Omeka you can fill out a form to describe the attributes of each item in your collection, helping you to keep track of this information in the future. \n                                                        439\t‘Omeka’:\thttp://www.omeka.net/\t440\t‘Content\tManagement\tSystem’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Content_management_system\t441\tJonathan\tReeve,\t‘Installing\tOmeka’,\tThe\tProgramming\tHistorian\t(2015).\t442\t‘Metadata’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Metadata\t \n 407 When might Omeka.net not be the right choice for your website? You want a simple website. If you just want a website with a few pages, some text, images, and other media, Omeka might be more tool than you need. Instead, consider WordPress443 or some basic HTML. You want a lot of control over the way things look. Omeka.net sites come with a number of built-in themes444 which define how the website looks (the colours, fonts, layouts, etc), but you cannot control every element of your site’s appearance. If you want to fine-tune the appearance of your site, consider using Omeka.org and customizing a theme. You will need some experience with CSS to do this effectively.445 You want sophisticated, dynamic queries of your database. A user can search your Omeka collection, but you cannot easily customize the home page so that it, say, always shows the most-viewed spoon in your spoon collection. That is, Omeka does not allow you to create custom queries. If this is important to you, consider Drupal.446 You want to create very complex paths through your collection. Omeka exhibits, which tell the story of your items, are pretty linear and straightforward. If you find this constraining, you might consider Scalar, which allows you to set up and visualize multiple paths through a database.447 An Omeka vocabulary lesson Item: The basic unit of an Omeka site. An item can be anything: a photograph, a work of art, a person, an idea. You will describe each item, and you can upload files to represent it. You will build your Omeka site by assembling items. Collection: A set of items that you have grouped together. Your Omeka site can have multiple collections, but an individual item can only belong to one collection at a time. Exhibit: A thematic tour of your items. Each exhibit has sections and pages. You might think of these as akin to book chapters and book pages. A section is a group of pages, and a page is a group of items (along with descriptions). You can have multiple exhibits, and items can belong to multiple exhibits. Dublin Core: Dublin Core is the name for a kind of metadata. Metadata is sort of what it sounds like; that is, information about information. You will use metadata to describe attributes of your items, like their sizes, dates of creation, etc. In order to keep these descriptions consistent,                                                         443\t‘Wordpress’:\thttps://wordpress.com/\t444\t‘Themes’:\thttp://omeka.org/add-ons/themes/\t445\t‘CSS\tTutorial’,\tW3\tSchools:\thttp://www.w3schools.com/css/\t446\t‘Drupal’:\thttps://www.drupal.org/\t447\t‘Scalar’:\thttp://scalar.usc.edu/\t \n 408 information professionals have defined various metadata standards. Dublin Core448 is the name of the standard that Omeka uses. Item Type: An item, as we learned, can be many different things, like a photograph, a website, a book, or a person. An “item type” is just the kind of thing the item is. You can choose from a built-in list of item types, or you can create your own. Simple Pages: A page on your Omeka site that is not part of an exhibit or item. For example, you can add an “About” page using Simple Pages. Omeka.org versus Omeka.net: There are two kinds of Omeka sites. The kind you are using is hosted at Omeka.net, meaning that you do not have to install anything and you do not need to have a web server of your own. You just sign up for an account using a web form. If you would like to customize your Omeka site more heavily than Omeka.net allows, you might consider Omeka.org. With an Omeka.org site, you download a free software package and install it on your own server. This means that Omeka.org sites can be more customized, but you have to be comfortable installing Omeka on a server. … And one more thing! You might think it’s pronounced oh-mee-ka, but it’s actually oh-meh-ka. Confusing, I know! Now that we have got that out of the way, let’s get started! Sign up for an Omeka account \n\tSign up for a new account screen on Omeka.net  Go to www.omeka.net and click on Sign Up. Choose the Basic plan. Fill in the sign-up form. Check your email for the link to activate your account.                                                         448\t‘Dublin\tCore’:\thttp://dublincore.org/documents/dcmi-terms/\t\n \n 409 Create your new Omeka site \n\tThe Omeka Dashboard, add your site  After you have clicked on the link in your email, click on Add a Site. Fill in information about your site’s URL, the title you want to use, and a description if you would like. Click on Add Your Site. You have a new Omeka site! \n\tThe Omeka Dashboard, view your site  To see what the website looks like, click on View Site. An empty Omeka site \n\tThe public view of the website  This is the public-facing element of your empty Omeka site. It is currently empty, waiting for you to fill it in. You will need to return to the dashboard to begin filling in the website. To get back to your dashboard, click the \n \n 410 Back button or enter http://www.nameofyoursite.omeka.net/admin. This time, click on Manage Site. Switch themes \n Switching Omeka Themes Omeka allows you to change the look of your public-facing site by switching themes. To do this, click on Settings (at the top right of your dashboard), then select Themes on the left side of the page. Switch themes by selecting one of the options on the page. Press the green Switch Theme button to activate your new theme. Then visit your public site by clicking on View Public Site at the top right. If you do not immediately see the new theme, try doing a hard refresh449 on your browser. You have a new theme! \n\tYour site with a new Omeka theme  Once you have checked out your new theme, head back to your dashboard. You can switch back to your old theme, keep this one, or select one of the other options.                                                         449\t‘Bypass\tyour\tcache’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Wikipedia:Bypass_your_cache\t\n \n 411 Install plugins \n\tInstalling Omeka plugins  Your Omeka site comes with plugins, which are snippets of pre-written code that offer some extra functionality. These plugins are deactivated by default. If you want to use this extra functionality you need to enable the desired plugin. To do that, click on the red Settings button at the top right of the dashboard screen. On the following page, click the Install button next to Exhibit Builder and Simple Pages. On the following page you will be given additional options, but leave these as they are for now. Add an item to your archive \n\tAdd an item to your Omeka archive  Click on Add a new item to your archive. \n \n 412  Describe your new item \n\tDescribe an Omeka item  Remember, Dublin Core refers to the descriptive information you will enter about your item. All of this information is optional, and you cannot really do it wrong. But try to be consistent. (If you are interested in learning about each of the Dublin Core fields and how to use them consistently, read more about them in the Dublin Core documentation.)450 Be sure to click the Public checkbox so that your item is viewable by the general public. If you do not click that box, only people who are logged into your site will be able to see the item. To add multiple fields — for example, if you want to add multiple subjects for your item — use the green Add input button to the left of the text boxes. \n                                                        450\t‘DCMI\tMetadata\tTerms’:\thttp://dublincore.org/documents/dcmi-terms/\t\n \n 413 To what does the metadata really refer? \n\tIs the metadata referring to Bertie, my dog, or this photograph of Bertie?  I am creating an item record for my dog, Bertie. But am I describing Bertie himself or a photograph of Bertie? If it is the former, the Creator would be — well, I guess that depends on your religious outlook. If it is the latter, the creator would be Brad Wallace, who took the photo. The decision about whether you are describing the object or the representation of the object is up to you. But once you have decided, be consistent. Attach a file to your item record \tAttach a file to an Omeka item  Once you have finished adding Dublin Core metadata, you can attach a file to your item record by clicking Files to the left of the Dublin Core form. (You do not have to click Add Item before you do this; Omeka will automatically save your information.) You can add multiple files, but be aware that the Basic plan only comes with 500 MB of storage space. Once you have added a file or files, you can add Tags by clicking on the button. You can also click on Item Type Metadata to choose the category — person, place, animal, vegetable, mineral — your item is. If you do not see the appropriate item type for your item, do not worry. You can add a new item type later. \n \n 414 When you are finished, click the green Add Item button. Your completed item \n\tA completed Omeka item  This list contains all the items you have added, which so far numbers only one. Notice the green checkmark that appears in the Public column. To see what the page for your new item looks like, click on the name of the item. This is not the public page for your item \n\tThe private view of your item page  It may look like it, but this page is not what a non-logged-in user will see when she navigates to the page for your item. To see what a user would see, click on View Public Page. (Or you can continue to edit the item by clicking on Edit this item at the top right.) \n \n 415 The public page for your item \n\tThe public page of an Omeka item  This is what a general user will see if she navigates to your page. Create a collection \tCreate an Omeka collection  Once you have several items, you can begin to bring order to those items by grouping them together into collections. To do this, return to your dashboard, click on the Collections tab, and click on Add a Collection. \n \n 416 Enter information about your collection \n\tEnter information about your Omeka collection  In Omeka, metadata is key. Enter some information about your new collection, and remember to click on the Public button near the bottom of the page. Then save your collection. You now have an empty collection. Add items to your collection \n\tAdd items to an Omeka collection  To add items to the collection you have just created, click on the Items tab. From your Browse Items list, click the boxes of the items that belong in your new collection. Then click on the green Edit Selected Items button. \n \n 417 Choose the collection \n\tChoose the Omeka collection to which you wish to add your item  On the Batch Edit Items page, select the Collection you would like to add your items to. (Also, take note of all the other options you have on this page.) View your new collection \n\tView the Omeka collection  To view the new collection, return to the public site. If you click on the Browse Collections tab on the public-facing site, you should now have a new collection containing the items you identified. Now that you have added some items and grouped them into a collection, take some time to play with your site. It is beginning to take shape now that you have both individual items and thematic units. But Omeka can do even more. We will talk about that in the next lesson. Further Resources The Omeka team has put together great resources on the software’s help pages451 About the Author Miriam Posner is the digital humanities program coordinator at the University of California, Los Angeles.                                                        451\t‘Help\tfor\tOmeka.net’:\thttp://info.omeka.net/\t\n \n 418 41. Creating an Omeka.net Exhibit Miriam Posner – 2013 In the previous lesson, ‘Up and Running with Omeka.net’, you added items to your Omeka.net site and grouped them into collections. Now you are ready for the next step: taking your users on a guided tour through the items you have collected. Before you begin: Plan your exhibit It pays to do some thinking before you launch into creating an exhibit. You will be creating both sections and pages, and you will need to give some thought to the argument you want to make and how you intend to make it. In this lesson that follows, I use the silly example of my dogs. But what if I were discussing, say, silent film? My sections might be thematic (comedies, romances, dramas), chronological (early silent film, the transitional period, classical era), or stylistic (modernist, impressionist, narrative). It all depends on the message I want to convey to the site’s visitors. You might draw out a map of your exhibit, showing where you want to put each digital asset. Add an exhibit \n\tAdd an exhibit in Omeka  A collection is just a list of objects. An exhibit, on the other hand, is a guided tour through your items, complete with descriptive text and customized layouts. To create one, click on the Exhibits tab and then Add \n \n 419 an exhibit. Fill out the form on the top half of the page. A slug is a machine-readable name for your exhibit and will become part of your URL. The slug of this lesson is “creating-an-omeka-exhibit,” which you can see in the URL at the top of your browser. Add a section \n\tAdd a section in Omeka  Every exhibit has sections and pages — like the chapters and individual pages in a book. Add a new section by clicking on the green Add Section button and then filling out the information on the following page. Add a page \n\tAdd a page in Omeka  Pages are where you will stick the actual items in your exhibit. Click on the green Add Page button. On the following page, you will enter some information and pick a layout for your exhibit page. The small blue squares \n \n 420 indicate item thumbnails, the large blue squares indicate full-sized images, and the lined areas indicate descriptive text. Pick a layout; you can change it later. Then click on Save Changes. Add items to your page \n\tAdd items to your page  On the page that follows, you will see a numbered grid. You will fill in that grid by attaching items (in the places indicated by blue boxes) and typing in descriptive information about your item. Remember, an exhibit is a kind of guided tour through your items, so try to write descriptions that guide the reader from one item to the next. When you are finished adding items, you can add another page, or another section, or both. When you are done, return to your public site to see how your Omeka site looks. You have an Omeka site! \n\tThe completed Omeka Exhibit  \n \n 421 Now your site has items, collections, and an exhibit — all the basic units of an Omeka site. Further Resources The Omeka team has put together great resources on the software’s help pages.452 About the Author Miriam Posner is the digital humanities program coordinator at the University of California, Los Angeles.\n                                                        452\t‘Help\tfor\tOmeka.net’:\thttp://info.omeka.net/\t \n 422 42. Intro to Google Maps and Google Earth Jim Clifford, Joshua MacFadyen, Daniel Macfarlane – 2013     Google Maps Google Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places. In the new Google Maps interface, click on the gear menu [icon] at the upper right of the menu bar, and select My Places. The new (as of summer 2013) interface provides a new way of creating custom maps: ‘Google Maps Engine Lite’ allows users to import and add data onto the map to visualize trends. In Maps Engine Lite you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google's search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS. Getting Started Open your favorite browser Go to Google's Maps Engine Lite: https://mapsengine.google.com/map/?gmp=mpp Log in to your Google Account if you aren't already logged in (follow the basic instructions to create an account if necessary)  \n 423 \n  Select Take a Tour at the bottom right for an introduction to how Maps Engine Lite works The Tour will first prompt you to Click New Map  \n  At the upper left corner, a menu box appears, titled 'Untitled Map'. By clicking on the title you can rename as 'My test map' or a title of your choice. The Tour next prompts you to search for a place in the search bar. Try searching the location of your current research project. You can then click on the location and add it to your map by clicking 'add to map'. This is the simplest method of adding points to your new map. Try searching for some historical place names that no longer exist (Ontario's Berlin or Constantinople). You will find mixed results, where Google often identifies the correct location, but also offers up incorrect alternatives. This is important to keep in mind when creating spreadsheet, as it is normally \n \n 424 better to use the modern place names and avoid risking that Google with choose the wrong Constantinople. CLICK NEXT on the Google Maps Tour.  \n  \n  The Tour next prompts you to Import a Dataset. Click the ADD Layer button. Then click the Import.    A new window will pop up and give you the option of importing a CSV (comma separated value) or XLXS (Microsoft Excel) file. These are two common spreadsheet formats; CSV is simple and universal, XLXS is the MS Excel format. You can also work with a Google spreadsheet from your Drive account. \n \n 425 \n Download this sample data and located it on your computer: UK Global Fat Supply CSV file. If you open the file in Excel or another spreadsheet program, you'll find a simple two column dataset with a list of different kinds of fats and the associated list of places. This data was created using British import tables from 1896.  \n  Drag the file into the box provided by Google Maps. \n \n 426 You will then be promoted to choose which column Google should use to identify a the location. Choose Place.  \n You will then be promoted again to choose which column should be used for the label. Choose 'Commodity'. You should now have a global map of the major exporters of fat to Britain during the mid-1890s. \n You can now explore the data in more detail and change the Style to distinguish between the different types of fats. Click on the UK Global Fats Layer, then click on Style and finally click on Uniform Style and change it to Style by Data Column: Commodities. On the left hand side, the legend will show the amount of occurrences of each style in brackets, e.g. 'Flax Seeds (4)'. \n \n \n 427 \n Continue to play with the options. • This feature provides a powerful tool to display historical datasets. It does have limitations, however, as Google Maps will only import the first 100 rows of a spreadsheet. At this point it only allows you to include three datasets in a map, so a maximum of 300 features. • When you are done exploring this feature click Next on the Google Tour. (Note: The Google tour crashed a few times while we created this lesson. It is possible to go back to the beginning to start a new Tour. Feel free to skip through the Import Data section if the Tour crashes at this point.) \n  Creating Vector Layers At this stage of the tutorial, we create map layers (known more formally as vector layers). Vector layers are one of the main components of digital mapping (including GIS). They are simply points, lines, or polygons used to represent geographic features. Points can be used to identify and label key locations, lines are often used for streets or railroads, and polygons allow you to represent area (fields, buildings, city wards, etc). They work the same in Google Maps as they do in GIS. The big limitation is that you can only add limited information into the database tables associated with the \n \n 428 points, lines, or polygons. This is a problem as you scale up your digital mapping research, but it is not a problem when you are starting out. In Google Maps you can add a label, a text description, and links to a website or photo. More information about creating historical vectors in a full GIS is available in ‘Creating New Vector Layers in QGIS 2.0’.453 To add a layer, you can either click on the layer that has been created for you in the menu box, with the name 'Untitled Layer'. Click on 'Untitled Layer' and rename it 'Layer 1′. Or you can create another layer: click on the 'Add layer' button. This will add a new 'Untitled Layer' which you can name as 'Layer 2′. It should look like this: \n Note that to the right of Layer there is a checkbox – unchecking this box turns off (i.e. it doesn't appear on the map) a layer and its information. Uncheck the UK Global Fats layer and click on Layer Before adding vector layers we should consider which base map to use. At the bottom of the menu window, there is a line that says 'base map'. A base map is a map** depicting background reference information such as roads, borders, landforms, etc. on top of which layers containing different types of spatial information can be placed. Google's Maps Engine allows you to choose from a variety of base maps, depending on the kind of map you want to create. Satellite imagery is becoming a standard form of base map, but it is information-rich and may detract from the other map features you are trying to highlight. Some simple alternatives include 'light landmass', or even 'light political' if you require political boundaries. • Click on the arrow to the right of 'Base map' in the window; a submenu appears allowing you to choose different types of base maps. Choose 'Satellite'. • Start by adding some Placemarks (the Google equivalent of a point). Click on the add Markers button underneath the search bar near the top of the window. Click on the spot on the map where you want the Placemark to appear.                                                          453\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDan\tMacfarlane\t‘Creating\tNew\tVector\tLayers\tin\tQGIS\t2.0’,\tThe\tProgramming\tHistorian\t(2013).\t\n \n 429 A box will pop up and give you the opportunity to label the Placemark and add a description into the text box. We added Charlottetown and included that it was founded in 1765 in the description box. \n Add a few more points, including labels and descriptions. • You will notice that your Placemark now appears under Layer 1 on the left of the screen in your menu window. There is a place to change icon shape and icon colour if you click on the symbol just to the right of the Placemark name. Also, directly under the title Layer 1 there are menus titled Style, Data, and Labels. The Style menu controls different aspects of the Layer's appearance, while Data shows you the data you added in the description box for your Placemark. Labels menu allows you to control whether the name or description of your Placemark appears besides it on the actual map. \n Now we will add some lines and shapes (called polygons in GIS software). Adding lines and polygons is a very similar process. We will draw some lines in a new layer (different types of points, lines, and shapes should be in separate layers). \n \n 430 Select Layer 2 in your menu box (you will know which layer you have selected because of the blue outline on the left of the box). Click the 'add line or shape' icon box directly to the right of the Markers symbol:  • Pick a road and click with your mouse along it, tracing the route for a while. Hit \"enter\" when you want to finish the line. Again you can add a label (i.e. name a road) and description information. • You can also change the colour and width of the line. To do this, find the road you have drawn in Layer 2 in the menu box, and click to the right of the name of the road. \n To create a polygon (a shape) you can connect the dots of the line to create an enclosed formation. To do this, start drawing and finish by clicking on the first point in your line. You can create simple shapes, such as a farmer's field, or much more complex shapes, such as the outline of a city (see examples below). Feel free to experiment with creating lines and polygons. \n \n \n 431 \n Like placemarks and lines, you can change the name and description of a polygon. You can also change the colour and line width by clicking on the icon to the right of your polygon name in the menu box. Here you can also change the transparency, which is discussed immediately below. Note that the area bounded (i.e. inside) a polygon is shaded the same colour as the polygon outline. You can change the opaqueness of this shading by changing the 'transparency' which alters the extent to which you can clearly see the background image (your base map). Now you can click Next on the Google Tour The tour will again show you how to Change the colour and style of your points, lines and polygons Click Next on the Google Tour. The Tour will remind you how to click on layers to hid them or add them back on the map. Try adding the UK Global Fats layer again and zoom between your global and local data points. Click Next on the Google Tour. Share your custom map The best way to share the map online is by using the green Share button in the top right corner. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter. Another way to share a dynamic version of your map is to embed it in a blog or website using the \"embed on my website\" option under the Save menu. Selecting this option provides an inline frame or <iframe> tag that you can then insert into an HTML site. You can modify the height and width of the frame by changing the numbers in quotation marks. Note: there is currently (as of summer 2013) no way to set the default scale or legend options of the embedded map, but if you need to eliminate the legend from the map that appears on your HTML site you can do so by reducing the width if the <iframe> to 580 or less. You can also export the data as a KML file. It will give you the option to export the whole map or to select one layer in particular. Try exporting the UK Global Fats layer as a KML layer. You'll be able to import this data into other programs, including Google Earth and Quantum GIS. This is an important feature, as it means you can start working with digital maps \n \n 432 using Google Map and still export your work into a GIS database in the future. You can stop the lesson here if you think this free Google Map service provides all the tools you need for your research topic. Or you can keep going and learn about Google Earth and in lesson 2, Quantum GIS. \n  Google Earth Google Earth works in much the same way as Google Maps Engine Lite, but has additional features. For example, it provides 3-D maps and access to data from numerous third party sources, including collections of historical maps. Google Maps doesn't require you to install software and your maps are saved in the cloud. Google Earth requires software installation and is not cloud-based, though maps you create can be exported. Install Google Earth: http://www.google.com/earth/index.html Open the program and familiarize yourself with the digital globe. Use the menu to add and remove layers of information. This is very similar to how more advanced GIS programs work. You can add and remove different kinds of geographical information including Political Boundaries (polygons), Roads (lines), and Places (points). See the red arrows in the following image for the location of these layers. \n \n 433 \n Note that under the 'Layer' heading on the lower left side of the window margin, Google provides a number of ready-to-go layers that can be turned on by selecting the corresponding checkbox. \n Google Earth also contains some scanned historical maps and aerial photographs (in GIS these types of maps, which are made up of pixels, are known as raster data). Under Gallery you can find and click Rumsey Historical Maps. This will add icons all over the globe (with a concentration in the United States) of scanned maps that have been georeferenced (stretched and pinned to match a location) onto the digital globe. This previews a key methodology in historical GIS. (You can also find historical map layers and other HGIS layers in the Earth Gallery). Take some time to explore a number of historical maps. See if there are any maps included in the Rumsey Collection that might be useful for your research or teaching. (You can find many more digitized, but not georeferenced maps at www.davidrumsey.com.) \n \n 434 \n You might need to zoom in to see all of the Map icons. Can you find the World Globe from 1812? \n Once you click on an icon an information panel pops up. Click on the map thumbnail to see the map tacked onto the digital globe. We will learn to properly georeference maps in ‘Georeferencing in QGIS 2.0’.454                                                         454\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDan\tMacfarlane\t‘Georeferencing\tin\tQGIS\t2.0’,\tThe\tProgramming\tHistorian\t(2013).\t\n \n 435 \n \n KML: Keyhole Markup Language files Google developed a file format to save and share map data: KML. This stands for Keyhole Markup Language, and it is a highly portable type of file (i.e. a KML can be used with different types of GIS software) that can store many different types of GIS data, including vector data. Maps and images you create in Google Maps and Google Earth can be saved as KML files This means you can save work done in Google Maps or Google Earth. With KML files you can transfer data between these two platforms and bring your map data into Quantum GIS or ArcGIS. For example, you can import the data you created in Google Maps Engine Lite. If you created a map in the exercise above, it can be found by clicking \"Open Map\" on the ‘Maps Engine Lite’ home page.455 Click on the folder icon on the left hand side of the legend beneath the map title and click                                                         455\t‘Welcome\tto\tMy\tMaps’:\thttps://www.google.com/maps/d/\t\n \n 436 \"export to KML\". (You can also download and explore Dan Macfarlane's ‘Seaway map’456 for this part of the exercise). Bringing your KML file into Google Earth Download the KML file from Google Maps Engine Lite (as described above). Double click on the KML file in your Download folder. Find the data in the Temporary Folder in Google Earth. \n You can now explore these map features in 3D, or you can add new lines, points and polygons using the various icons along the top left of your Google Earth window (see image below). This works in essentially the same way as it did for Google Maps, although there is more functionality and options. In Dan's Seaway map, the old canals and current Seaway route were traced in different line colours and widths using the line feature (this was made possible by overlaying historical maps, which is described below), while various features were marked off with appropriate Placemarks. For those so inclined, there is also the option of recording a tour that could be useful for presentations or teaching purposes (when the \"record a tour\" icon is selected, recording options will show up on the bottom left of the window).   Try adding a new feature to Dan's Seaway data. We've created a polygon (in GIS terminology a polygon is a closed shape of any type – a circle, hexagon, and square are all examples) of Lake St. Clair in the next image. Find Lake St. Clair (east of Detroit) and try adding a polygon.                                                         456\t‘Seaway\tMap’:\thttps://www.google.com/maps/d/kml?mid=zLGDOilNtMgo.kTGA5fuMJTds&ie=UTF8&hl=en&t=h&source=embed&authuser=0&msa=0&output=kml\t\n \n 437 \n \n  Label the new feature Lake St. Clair. You can then drag the new feature onto Dan's Seaway data and add it to the collection. You can then save this expanded version of the Seaway map as a KML to share via email, upload into Google Maps, or to export this data into QGIS. Find the save option by right-clicking on the Seaway collection and choose Save Place As or Email. \n  \n \n 438 \n \n Adding Scanned Historical Maps Within Google Earth, you can upload a digital copy of a historical map. This could be a map that has been scanned, or an image obtained that is already in a digital format (for tips on finding historical maps online see: Mobile Mapping and Historical GIS in the Field). The main purpose for uploading a digital map, from a historical perspective, is to place it over top of a Google Earth image in the browser. This is known as an overlay. Performing an overlay allows for useful comparisons of change over time. Start by identifying the images you want to use: the image within Google Earth, and the map you want to overlay with. For the map you want to overlay, the file can be in JPEG or TIFF format, but not PDF. Within Google Earth, identify the area of the map you want to overlay. Note that you can go back in time (i.e. look at older satellite photos) by clicking on the 'Show historical imagery' icon on the top toolbar. and then adjusting the time-scale slider that will appear. \n \n 439  \n Once you have identified the images you plan to use, click on the 'Add Image Overlay' icon on the top toolbar.  A new window will appear. Begin by giving it a different title if you wish (the default is 'Untitled Image Overlay'). \n To the right of the Link field, click the Browse button to select from your files the map you wish to be the overlaying image. Move the New Image Overlay window out of the way (don't close it or click \"Cancel\" or \"OK\") so that you can see the Google Earth browser. The map you uploaded will now appear over top of the Google Earth satellite image in the Google Earth browser. There are fluorescent green markers in the middle and at the edges of the uploaded map. These can be used to stretch, shrink, and move the map so that it aligns properly with the satellite image. This is a simple form of georeferencing (see ‘Georeferencing in QGIS 2.0’ lesson). The image below shows the above steps using an old map of the town of Aultsville overlaid on top of Google satellite imagery from 2008 in which the remains of the town's roads and building foundations in the St. Lawrence River are visible (Aultsville was one of the Lost Villages flooded out by the St. Lawrence Seaway and Power Project). \n \n 440 \n Back in the New Image Overlay window, note that there are a range of options (Description, View, Altitude, Refresh, Location) that you can select. At this point, you likely don't need to worry about these, although you may wish to add information under the Description tab. Once you are satisfied with your overlay, in the New Image Overlay window click on OK in the bottom right corner. You will want to save your work. Under File on your computer's menu bar, you have two options. You can save a copy of the image (File -> Save -> Save Image…) you have created to your computer in jpg format, and you can also save the overlay within Google Earth so that it can be accessed in the future (File -> Save -> Save My Places). The latter is saved as a KML file. To share KML files simply locate the file you saved to your computer and upload it to your website, social media site, or send it as an email attachment. You have learned how to use Google Maps and Earth. Make sure you save your work! About the Authors Jim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University.\n \n 441 43. Installing QGIS 2.0 and Adding Layers Jim Clifford, Joshua MacFadyen, Daniel MacFarlane – 2013    Lesson Goals In this lesson you will install QGIS software, download geospatial files like shapefiles and GeoTIFFs, and create a map out of a number of vector and raster layers. Quantum or QGIS is an open source alternative to the industry leader, ArcGIS from ESRI. QGIS is multiplatform, which means it runs on Windows, Macs, and Linux and it has many of the functions most commonly used by historians. ArcGIS is prohibitively expensive and only runs on Windows (though software can be purchased to allow it to run on Mac). However, many universities have site licenses, meaning students and employees have access to free copies of the software (try contacting your map librarian, computer services, or the geography department). QGIS is ideal for those without access to a free copy of Arc and it is also a good option for learning basic GIS skills and deciding if you want to install a copy of ArcGIS on your machine. Moreover, any work you do in QGIS can be exported to ArcGIS at a later date if you decide to upgrade. The authors tend to use both and are happy to run QGIS on Mac and Linux computers for basic tasks, but still return to ArcGIS for more advanced work. In many cases it is not lack of functions, but stability issues that bring us back to ArcGIS. For those who are learning Python with the Programming Historian, you will be glad to know that both QGIS and ArcGIS use Python as their main scripting language. Installing QGIS Navigate to the QGIS Download page.457 The procedure is a little different depending on your operating system. Click on the appropriate Operating System. Follow the instructions below. Mac Instructions For most people it will be best to choose Master release (the one that has a single installer package). You will still need to install other software packages before installing QGIS. Under 4.2, click on the link (KyngChaos Qgis download page)458 and download the following two files (see screen shot below): 1) GDAL complete 1.10 framework package (under Requirements) and 2) QGIS 2.0.1 (under Download) for your respective Mac OS (this works with Lion, Mountain Lion, and Snow Leopard – no                                                         457\t‘QGIS’:\thttp://hub.qgis.org/projects/quantum-gis/wiki/Download\t458\t‘QGIS’,\tKychChaos\tWiki:\thttp://www.kyngchaos.com/software/qgis\t \n 442 word yet on using it with the forthcoming Mavericks). Install these like any other Mac programs. \n once the frameworks are installed, download and install QGIS. as with any other Mac application you are using for the first time, you will have to go find the QGIS application in Applications Windows Instructions under Standalone Installer, click on the link to Download QGIS  double-click on the .exe file to execute QGIS is very simple to install in most versions of Linux. Follow the instructions on the download page. Prince Edward Island Data We will be using some government data from the Canadian province of Prince Edward Island. PEI is a great example because there is a lot of data for free online and because it is Canada's smallest province, making the downloads quick! Download PEI shapefiles: Navigate to the links below in your web browser, read/accept the license agreement, and then download the following (they will ask for your name and email with each download). We created the final two shapefiles, so they should download directly:  http://www.gov.pe.ca/gis/license_agreement.php3?name=coastline&file_format=SHP http://www.gov.pe.ca/gis/license_agreement.php3?name=lot_town&file_format=SHP \n \n 443 http://www.gov.pe.ca/gis/license_agreement.php3?name=hydronetwork&file_format=SHP http://www.gov.pe.ca/gis/license_agreement.php3?name=forest_35&file_format=SHP http://www.gov.pe.ca/gis/license_agreement.php3?name=nat_parks&file_format=SHP PEI Highways: http://programminghistorian.org/assets/PEI_highway.zip PEI Places: http://programminghistorian.org/assets/PEI_placenames.zip  After downloading all seven files, move them into a folder and unzip the zipped files. Take a look at the contents of the folders. You will notice four files with the same name, but different file types. When you navigate to these folders from GIS software, you will find that you only need to click on the .shp and that the other three formats support this file in the background. It is important when moving files on your computer to always move and keep all four files together. This is one reason Shapefiles are normally shared using zip compression. Remember the folder in which you save uncompressed shapefile folders, as you will need to find them from within QGIS in a few minutes. Starting Your GIS Project Open QGIS. The first thing we need to do is set up the Coordinate Reference System (CRS) correctly.459 The CRS is the map projection and projections are the various ways to represent real world places on two-dimensional maps. The default is WGS84 (it is increasingly common to use WGS 84 which is compatible with Google Earth type software), but since most of our data and examples are created by Canadian governments we recommend using NAD 83 (North American Datum, 1983). For more on NAD 83 and the Federal Government's datum, see NRCan's website.460 PEI has its own NAD 83 coordinate reference system which uses a Double Stereographic projection.461 Managing the CRS of different layers of information and making sure they are working correctly is one of the most complicated aspects of GIS for beginners. Nonetheless, if the software is setup correctly, it should convert the CRS and allow you to work with data imported from different sources. Select Project Properties Mac: Project–>Project Properties \n                                                        459\t‘Spatial\treference\tsystem’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Spatial_reference_system\t460\t‘Map\tDatum’:\thttp://www.nrcan.gc.ca/earth-sciences/geography/topographic-information/maps/9791\t461\t‘PEI\tCoordinate\tSystem’:\thttp://www.gov.pe.ca/gis/index.php3?number=77865&lang=E\t \n 444 \n Windows: Settings-> Project Properties \n In the left window pane select CRS (second from the top) • click Enable 'on the fly' CRS transformation button on top left in the Filter box enter '2291′ – this quickly navigates to the best Coordinate reference system for Prince Edward Island. • under the box titled Coordinate reference systems of the world, select 'NAD83(CSRS98) / Prince Edward Isl. (Stereographic)' and hit OK \n notice the projection has changed in the bottom right corner of the QGIS window. Next to that you will see the geographic location of your mouse pointer in metres \n \n 445 under Project menu, select Save Project (you should save your project after each step) You are now set up to work on the tutorial project, but might have a few questions about what CRS to use for your own project. WGS83 might work in the short term, particularly if you are working on a fairly large scale, but it will be difficult to accurately work on local maps. One hint is to learn what CRS or Projections is used for paper maps of the region. If you are scanning a good quality paper map to use as the base layer it might be a good idea to use the same projection. You can also try searching the internet for the more common CRS for a particular region. For those of you working on North American projects identifying the correct NAD83 for your region will often be the best CRS. Here are a few links to other resources that will help you choose a CRS for your own project: Tutorial: Working with Projections in QGIS.462 Building a Base Map Now that your computer is driving with the right directions, it's time to add some information that makes sense to humans. Your project should start with a base map, or a selection of geospatial information that lets your readers recognize real world features on the map. For most users this will be comprised of several 'layers' of vector and raster data, which can be rearranged, coloured, and labeled in such a way that they make sense to your readers and your project's objectives. A relatively new feature on many GIS programs is the availability of pre-fab base maps, but since this technology is under development for open source platforms like QGIS we will walk through the process of creating our own base map by adding vector and raster layers in this module. For those who would like to add pre-fab base maps to QGIS, you can try installing the 'OpenLayers' Plugin under Plugins->Manage and Install Plugins. Select \"Get More\" on the left. Click OpenLayers and then click Install plugin. Click OK and then click close. Once installed, you'll find OpenLayers in the Plugins Menu. Try installing some of the different Google and OpenStreetMaps layers. At the time of writing this module, the OpenLayers plugin (v. 1.1.1) installs but fails to work properly on any Mac using OSX. It appears to work more consistently on QGIS running on Windows 7. Give it a try, as we expect it will only get better in the months ahead. Note, however, that the projection for some of these global maps do not correct on the fly, so the satellite images might not alway sync up with data projected in a different CRS. Opening Vectors Vectors defined:463 GIS uses points, lines, and polygons, also known as vector data. Its first order of work is to arrange these points, lines, and                                                         462\t‘Tutorial:\tWorking\twith\tProjections\tin\tQGIS’:\thttp://qgis.spatialthoughts.com/2012/04/tutorial-working-with-projections-in.html\t463\t‘GIS\tData\tExplored\t–\tVector\tand\tRaster\tData’:\thttps://www.gislounge.com/geodatabases-explored-vector-and-raster-data/\t \n 446 polygons and project them accurately on maps. Points may be towns or telephone poles; lines could represent rivers, roads, or railroads; and polygons could encompass a farmer's lot or larger political boundaries. However, it is also possible to attach historical data to these geographical places and study how people interacted with and changed their physical environments. The population of towns changed, rivers moved their courses, lots were subdivided, and land was planted with various crops. under Layer on toolbar, choose Add Vector Layer (alternatively the same icon you see next to 'Add Vector Layer' can also be selected from the tool bar on the upper left side)  click Browse, find your downloaded Prince Edward Island shapefiles in the folder open the coastline_polygon folder \n select coastline_polygon.shp, then select 'OK', and you should see the island's coastline on your screen. Sometimes QGIS adds a coloured background (see the image above). If you have a coloured background, follow the steps below. If not, skip down the page to the ***. right click the layer (coastline_polygon) in the Layers menu and choose Properties. \n  \n \n 447 • In the ensuing window, click Style in the left pane • There are a range of options, but we want to get rid of the background all together. Click Simple fill. \n Then choose 'No Brush' in the Fill style drop down menu. Click OK. \n Choose Add Vector Layer again. click Browse, find your downloaded Prince Edward Island shapefiles in the folder select 'PEI_HYDRONETWORK' click on 'PEI_HYDRONETWORK.shp' and then hit 'Open' right click the layer in the Layers menu and choose Properties. select Style tab, and choose an appropriate blue to color the hydronetwork and select 'OK' at the bottom right of the window \n \n 448 \n Your map should now look like this: \n Choose Add Vector Layer again. click Browse, find your downloaded Prince Edward Island shapefiles in the folder double-click on '1935 inventory_region.shp' and then hit 'Open' This will add a dense map showing the different forest cover in 1935. However, to see the different categories, you will need to change the symbology to represent the different categories of forest with different colours. We will need to know which column of the database tables includes the forest category information, so the first step is to open and inspect the attribute table. right click on the 1935_inventory_region layer in the Layers window on the left and click on Open Attribute Table \n \n 449 \n An Attribute Table will open. It has a number of categories and identifiers. Of particular interest is the LANDUSE category which provides information on the forest cover in 1935. We will now show you how to display these categories on the map. \n Close the Attribute Table, and right click on the 1935_inventory_region layer again and this time choose Properties (alternatively, the shortcut is to double click on the 1935_inventory_region layer). click Style along the left  \n \n 450  on the menu bar that reads 'Single Symbol' select 'Categorized' \n beside Column choose 'Landuse' under Color ramp choose Greens click 'Classify' below and to the left in Symbol Column, choose the furthest down dark green square (with no value beside it) and hit the 'Delete' button (to the right of Classify); also delete the Developed category, as we want to highlight forested areas. Click 'OK' \n • in Layers sidebar menu, click on the little arrow beside 1935_inventory_region to view the legend. • You can now see the extent of the forests in 1935. Try using the magnifying glass tool to zoom in and inspect the different landuses. \n \n 451 \n  To get back to the full island, right click on any of the layers and choose 'Zoom to Layer Extent.'  Next, we will add a layer of roads. under Layer on toolbar, choose Add Vector Layer click Browse, find your downloaded Prince Edward Island shapefiles in the folder select 'PEI_highway.shp' in the Layers menu on the left, double-click 'PEI_highway_ship' and select Style from the menu on the left (if it isn't already selected) click on 'Single Symbol' on top left and select 'Categorized' beside Column choose 'TYPE' click Classify \n  \n \n 452 • in the Symbol column, double-click beside 'primary' – in the ensuing window, there is a box with different symbols. Scroll down and find 'primary road'. • You are back in the Style window. Repeat for the item that called 'primary_link' in the Label column.  click Symbol beside secondary and change color to black and width to 0.7 repeat for secondary link click OK. You will now have the highways and other major roads represented on the map \n under Layer on toolbar, choose Add Vector Layer click Browse, find your downloaded Prince Edward Island shapefiles in the folder select 'PEI_placenames_shp' double click on 'PEI_placenames' and select 'Open' in the Layers window, double-click on the PEI_placenames layer. Choose Labels tab along the left (under Style). At the top, select the box beside 'Label this layer with' and in the dropdown box beside that select 'Placename' \n \n 453 \n  Change Font size to '18′ Click 'OK' and examine the results on the map \n  Labelling is where QGIS falls well short of real cartography – it will take tinkering to adjust settings to display the detail desired for a presentation. Try going back to the Labels tab and changing the different settings to see how symbols and displays change. Note that in the Layers menu you can add and remove the various layers we've added to the map much the same way you did in Google Earth. Click on the check boxes to remove and add the various layers. Drag and drop layers to change the the order they appear. Dragging a layer to the top will place it above the rest of the layers and make it the most prominent. For example, if you drag 'coastline_polygon' to the top, you have a simplified outline of the province along with place names. \n \n 454 \n  Along the toolbar on the top left of the main window are icons that allow you to explore the map. The hand symbol, for example, allows you to click on the map and move it around, while the magnifying glass symbols with plus and minus on them allow you to zoom in and out. Play with these and familiarize yourself with the various functions  having created a map using vector layers, we will now add or use our first raster layer. This is a good time to save your work. Opening Rasters: Raster data are digital images made up of grids. All remote sensing data such as satellite images or aerial photos464 are rasters, but usually you can't see the grids in these images because they are made up of tiny pixels. Each pixel has its own value and when those values are symbolized in colour or greyscale they make up an image that is useful for display or topographical analysis. A scanned historical map is also brought into GIS in raster format. download: 'http://programminghistorian.org/ /assets/PEI_CumminsMap1927_compLZW.tif' to your project folder. under Layer on toolbar, choose Add Raster Layer (alternatively the same icon you see next to 'Add Raster Layer' can also be selected from the tool bar along the left side of the window)  find the file you have downloaded titled 'PEI_CumminsMap1927.tif' you will be prompted to define this layer's coordinate system. In the Filter box search for '2291′, then in the box below select 'NAD83(CSRS98) / Prince Edward Isl. (Stereographic)…'                                                         464\t‘Orthophoto’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Orthophoto\t\n \n 455 \n If the program does not prompt you for the CRS you need to change it yourself. Double click the PEI_CummingMap1927_compLZW layer and choose 'General' from the menu on the left. Click 'Specify…' beside the box showing the incorrect Coordinate reference system. Then follow the instructions above (choose 2291). \n In the Layers window, the map should appear below the vector data. Move it to the bottom of the menu if necessary: \n \n 456 \n Now we would like to make the coastline more visible, so double-click on 'coastline_polygon' and select 'Style' on the left. In the box under Symbol layers, select 'Simple fill' and options appear in the box to the right. Click on the menu next to 'Border' and make it red, and then beside Border width change it to 0.5, and click OK. \n You are now able to see the background raster map through the 'coastline_polygon' layer. Zoom in for closer inspection, and you should be able to see the coastline layer clearly. Notice that the alignment is relatively good, but not perfect. We will learn more in lesson 4 about the challenges of georeferencing historical maps to give them real world coordinates. \n \n \n 457 You have learned how to install QGIS and add layers. Make sure you save your work! About the Authors Jim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University. \n 458 44. Creating New Vector Layers in QGIS 2.0 Jim Clifford, Joshua MacFadyen, Daniel MacFarlane – 2013 Lesson Goals In this lesson you will learn how to create vector layers based on scanned historical maps. In ‘Intro to Google Maps and Google Earth’ you used vector layers and created attributes in Google Earth. We will be doing the same thing in this lesson, albeit at a more advanced level, using QGIS software. Vector layers are, along with raster layers, one of the two basic types of data structures that store data. Vector layers use the three basic GIS features – lines, points, and polygons – to represent real-world features in digital format. Points can be used to represent specific locations, such as towns, buildings, events, etc. (the scale of your map will determine what you represent as a point – in a map of a province, a town would be a point, whereas in a map of a town, a building might be a point). Lines can effectively represent features such as roads, canals, railways, and so on. Polygons (effectively enclosed shapes with more than a few sides) are used to represent more complex objects such as the boundaries of a lake, country, or electoral riding (again, scale will affect your choice – large buildings in a close-up map of a city might be better represented as polygons than as points). In this lesson you will be creating shapefiles (which are a type of vector data) to represent the historical development of communities and roads in Prince Edward Island. Each shapefile can be created as one of the three types of features: line, point, polygon (though these features can't be mixed within a shapefile) . Each feature you create in a shapefile has a corresponding set of attributes, which are stored in an attribute table. You will create features and learn how to modify them, which involves not only the visual creation of the three types of features, but also the modification of their attributes. To do so, we will use the files from ‘Installing QGIS 2.0 and Adding Layers’ concerning Prince Edward Island. Getting Started Start by downloading the ‘PEI_Holland map’ to the project folder: Open the file you saved at the end of ‘Installing QGIS 2.0 and Adding Layers’. You should have the following layers in your Layers window: PEI_placenames PEI_highway PEI HYDRONETWORK 1935 inventory_region coastline_polygon  \n 459 PEI-CumminsMap1927 Uncheck all of these layer except for PEI_placenames, coastline_polygon and PEI_CumminsMap1927 \n We are now going to add a second historical map as a raster layer.  under Layer on toolbar, choose Add Raster Layer (alternatively the same icon you see next to 'Add Raster Layer' can also be selected from tool bar) find the file you have downloaded titled 'PEI_HollandMap1798' you will be prompted to define this layer's coordinate system. In the Filter box search for '2291' then in the box below select 'NAD83(CSRS98) / Prince Edward Isl. Stereographic' If you are not prompted to define the layer's coordinate system, you need to change a setting. Click Settings and then Options. Click CRS on the right hand menu and then choose 'Prompt for CRS' from the options below 'When a new layer is created, or when a layer is loaded that has no CRS'. Click OK. Remove the Holland Map (right click on it and click Remove) and try adding it again. This time you should be prompted for a CRS and you can select the NAD83 option (see above). \n \n 460 \n In previous steps you have selected and unselected layers in the Layers window by checking and unchecking the boxes next to them. These layers are organized in descending order of visibility – i.e. the layer at the top is the top layer in your viewer window (provided it is selected). You can drag the layers up and down in the Layer window to change the order in which they will be visible on your viewing window. The coastline_polygon raster layer is currently not visible because it is below the PEI_HollandMap1798 and PEI_Cummins1927 layers. In general it is best to keep vector layers above the raster layers. Uncheck PEI_Cummins1927 so that the only layer you have remaining is PEI_HollandMap1798. Note that the map appears crooked on the screen; this is because it has already been georeferenced by the lesson writers to match the GIS vector layers. Learn more about georeferencing in ‘Georeferencing in QGIS 2.0’, the next lesson. \n \n \n 461 We will now create a point shapefile, which is a vector layer. Click Layer -> New -> New Shapefile Layer alternatively you can select the New Shapefile Layer icon on the top of the QGIS toolbar window \n After selecting New Shapefile Layer, a window titled New Vector Layer appears In the Type category, Point is already selected for you. Click the Specify CRS button, and select NAD83(CSRS98) / Prince Edward Isl. Stereographic (EPSG: 2291), and then click OK (for information on understanding and selecting UTM zone: http://www.lib.uwaterloo.ca/locations/umd/digital/clump_classes.html) \n Returning to the New Vector Layer window, we are going to make some attributes. To create the first attribute: under New attribute, in the field beside Name, type in 'Settlement_Name' (note that when working in databases you cannot use empty spaces in names so the convention is to use underscores in their place) \n \n 462 click Add to attributes list Now we are going to create a second attribute: under New attribute, in the field beside Name, type in 'Year' this time, we are going to change the Type to Whole Number click Add to attribute list For the third attribute: under New attribute, in the field beside Name, type in \"End_Year\" (GIS is not always optimal for dealing with change over time, so in some cases it is important to have a field to identify approximately when something ceased to exist) change the Type again to Whole Number click Add to attribute list \n When you complete these three steps, finish creating this shapefile by clicking OK on the bottom right of the New Vector Layer window. A pops up – name it 'settlements' and save it with your other GIS files. Note that a layer called 'settlements' now appears in your Layers window. Relocate it above the raster layers. \n \n \n 463 Uncheck all layers except settlements. You will notice that your viewing window is now blank as we have not created any data. We will now create new data from both the PEI_HollandMap\t1798 and the PEI_CumminsMap1927 to show the increase in settlement between the late 18th and early 20th centuries. we will begin with the more recent, and thus usually more accurate, map. Reselect (i.e. check the boxes beside) coastline_polygon and PEI_CumminsMap1927 in your viewing window, zoom in to Charlottetown (hint: Charlottetown is near the middle of the island on the south side, at the confluence of three rivers) select settlements layer in Layers window on the menu bar, select Toggle Editing  After selecting Toggle Editing, editing buttons will become available to the right along the menu bar. Select the 3 dot feature button.  Your cursor now appears as a crosshair – point the crosshair at Charlottetown (if you don't happen to know PEI's geography, you can cheat by adding on the PEI_placenames layer) keeping it within the modern day coastline, and click (digitization is always a compromise between accuracy and functionality; depending on the quality of the original map and the digitization, for most historical applications extreme accuracy is not necessary). An Attributes window will appear. Leave id field blank (at time of writing, QGIS appears to be making two id fields and this one is unnecessary). In Settlement field, type in 'Charlottetown'. In the Year field type in 1764. Click OK We will now repeat the steps we took with Charlottetown for Montague, Summerside, and Cavendish (again, you can find these locations by adding the PEI_placenames layers). Find Montague on the map, select the 3 dot feature button and click on Montague on the map. When the Attributes window appears, input Montague and 1732 in the appropriate fields. Repeat for Summerside (1876) and Cavendish (1790). \n \n 464 \n In the Layers window, unselect the PEI_CumminsMap1927 and select PEI_HollandMap1798. We are now going to identify two settlements (Princetown & Havre-St-Pierre) that no longer exist. • To locate Princetown, look for Richmond Bay and Cape Aylebsury (on the north coast to the west of Cavendish), here you will find Princetown (shaded-in) near the boundary between the yellow and the blue • If you look at the Wikipedia entry for the city465 you will notice that because of a shallow harbor, Princetown did not become a major settlement. It was renamed in 1947 and later downgraded to a hamlet. For this reason we will include 1947 as the end date for this settlement. • With the crosshair click on Princetown. In the Attribute table that appears, put Princetown in the Settlement field, put 1764 into the Year field, and put 1947 into the End_Year. Click OK \n   • Click on Save Edits icon on the menu bar (it is between Toggle and Add Feature)                                                         465\t‘Prince\tRoyalty,\tPrince\tEdward\tIsland’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Prince_Royalty,_Prince_Edward_Island\t\n \n 465 • Double-click on settlements layer in the Layers window, choose Labels tab at the top of the ensuing window. Click on the box beside Display labels. In Field containing label select Year (if necessary), change font size to 18.0, change Placement to Above Left, and then click OK On the northern coast of Lot 39 between Britain's Pond and St. Peters Bay, we will now put a dot for the location of a long lost village called Havre-St-Pierre. • Havre-St-Pierre was the island's first Acadian settlement but has been uninhabited since the Acadian deportation of 1758. • With the crosshair click on Havre-St. Pierre. In the Attribute table that appears, put Havre-St-Pierre in the Settlement field, put 1720 into the Year field, and put 1758 into the End_Year. Click OK \n We will now now create another vector layer – this layer will be a line vector. Click Layer -> New -> New Shapefile Layer. The New Vector Layer window will appear (in the Type category at the top, select Line) Click the Specify CRS button, and select NAD83(CSRS98) / Prince Edward Isl. Stereographic (EPSG: 2291), and then click OK under New attribute, in the field beside Name, type in 'Road_Name' click Add to attributes list Create a second attribute under New attribute, in the field beside Name, type in Year change the Type to Whole Number click Add to attribute list To finish creating this shapefile, click OK on the bottom right of the New Vector Layer window. A 'save' screen pops up – name it 'roads' and save it with your other GIS files. We are now going to trace the roads from the 1798 map so that we can compare them to the modern roads. Make that you have the PEI_Holland1798 and settlements layers checked in the Layers window. \n \n 466 Select road layer in the layers window, select Toggle Editing on the top toolbar, and then select Add Feature  First trace the road from Charlottetown to Princetown. Click on Charlottetown and then click repeatedly at points along the road to Princetown and you will see the line being created. Repeat until you arrive at Princetown, then right-click. In the resulting Attributes – road window, in the Name field enter \"to Princetown\" and in the Year field enter 1798. Click OK \n repeat this step for 3 to 4 more roads found on the PEI_HollandMap1798. click Save Edits and then click Toggle Editing to turn it off Deselect the PEI_HollandMap1798 in the Layers window and select the PEI_highway map. Compare the roads represented in the PEI_highway map (the red dotted lines) to the roads you have just traced. \n We can see that some of these roads correspond closely to modern roads, while others do not at all correspond. It would take further historical research to determine whether this is simply because the Holland map did not sufficiently survey roads at the time, or if roads have changed considerably since then. \n \n 467 Now create a third type of vector layer: a polygon vector. Click Layer -> New -> New Shapefile Layer. The New Vector Layer window will appear – in the Type category at the top, select Polygon Click the Specify CRS button, and select NAD83(CSRS98) / Prince Edward Isl. Stereographic (EPSG: 2291), and then click OK under New attribute, in the field beside Name, type in 'lot_name' in the field beside Year click Add to attributes list Create a second attribute under New attribute, in the field beside Name, type in Year change the Type to Whole Number click Add to attribute list \n Start by creating a polygon for lot 66, which is the only rectangular lot on the island Click on Toggle Editing on top tool bar, and then click on Add Feature click on all four corners of lot 66 and you will see a polygon created right-click on the final corner and an Attributes window will appear. Add 66 to lot_names field and add 1764 (the year these lots were surveyed) to the Year field \n \n 468 \n We are now going to trace lot 38, which is just west of Havre-St-Pierre. Make sure that there is a check mark in the box beside PEI_HollandMap1798 layer in the Layers window Click on Toggle Editing on top tool bar, and then click on Add Feature Trace the outline of Lot 38, which is more difficult because of the coastline, as accurately as possible. In order to show you the Snap feature, we want you to trace along the modern coastline (snapping is an automatic editing operation that adjusts the feature you have drawn to coincide or lineup exactly with the coordinates and shape of another nearby feature) select Settings-> Snapping Options  a Snapping options window will open: click on the box beside coastal_polygon, for the Mode category select \"to vertex and segment\", for Tolerance select 10.0, and for Units select 'pixels'. Click OK \n Make sure that the lots layer is selected in Layers window, and select Add Feature from the tool bar \n \n 469 with your cursor click on the two bottom corners of your polygon just as you did with Lot 38. At the coastline you will notice that you have a collection of lines to trace around Savage Harbour. This is where the Snapping features becomes helpful. As you work to trace along the modern coastline it will significantly improve your accuracy by snapping your clicks directly on top of the existing line. The more clicks you make the more accurate it will be, but keep in mind that for many HGIS purposes obtaining extreme accuracy sometimes produces diminishing returns. \n When you finish tracing and creating the polygon, select and deselect the various layers you have created, comparing and seeing what relationships you can deduce. In Google Earth there were limitations on the types of features, attributes, and data provided by Google, and Google Earth did much of the work for you. That is fine when you are learning or want to quickly create maps. The advantage of using QGIS software to create new vector layers is that you have a great deal of freedom and control over the types of data you can use and the features and attributes that you can create. This in turn means that you can create custom maps far beyond what can be achieved in Google Earth or Google Maps Engine Lite. You have seen this firsthand with the points, lines, and polygons vector layers you learned how to create in this lesson. If you found data on, for example, public health records in the 18th century, you could create a new layer to work with what you already created showing the distribution of typhoid outbreaks and see if there are correlations with major roads and settlements. Moreover, GIS software allows you to not only spatially represent and present data in much more sophisticated ways, but to analyze and create new data in ways that aren't possible otherwise. You have learned how to create vector layers. Make sure you save your work! About the Authors Jim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University.\n \n 470 45. Georeferencing in QGIS 2.0 Jim Clifford, Joshua MacFadyen, Daniel MacFarlane – 2013 Lesson Goals In this lesson, you will learn how to georeference historical maps so that they may be added to a GIS as a raster layer. Georeferencing is required for anyone who wants to accurately digitize data found on a paper map, and since historians work mostly in the realm of paper, georeferencing is one of our most commonly used tools. The technique uses a series of control points to give a two-dimensional object like a paper map the real world coordinates it needs to align with the three-dimensional features of the earth in GIS software (in the ‘Intro to Google Maps and Google Earth’ lesson we saw an 'overlay' which is a Google Earth shortcut version of georeferencing). Georeferencing a historical map requires a knowledge of both the geography and the history of the place you are studying to ensure accuracy. The built and natural landscapes change over time, and it is important to confirm that the location of your control points — whether they be houses, intersections, or even towns — have remained constant. Entering control points in a GIS is easy, but behind the scenes, georeferencing uses complex transformation and compression processes. These are used to correct the distortions and inaccuracies found in many historical maps and stretch the maps so that they fit geographic coordinates. In cartography this is known as rubber-sheeting466 because it treats the map as if it were made of rubber and the control points as if they were tacks 'pinning' the historical document to a three dimensional surface like the globe. To offer some examples of georeferenced historical maps, we prepared some National Topographic Series maps hosted on the University of Toronto Map Library website courtesy of Marcel Fortin, and we overlaid them on a Google web map. Viewers can adjust the transparency with the slider bar on the top right, view the historical map as an overlay on terrain or satellite images, or click 'Earth' to switch into Google Earth mode and see 3D elevation and modern buildings (in Halifax and Dartmouth). Note: these historical images are large and will appear on the screen slowly, especially as you zoom into the Google map. National Topographic System Maps – Halifax, 1920s467 National Topographic System Maps – Western PEI, 1939-1944 National Topographic System Maps – Eastern PEI 1939-1944                                                         466\t‘Rubbersheeting’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Rubbersheeting\t467\t‘National\tTopographic\tSystems\tMap\t–\tHalifax,\t1920x’:\thttp://maps.library.utoronto.ca/datapub/digital/3400s_63_1929/maptile/Halifax/googlemaps.html\t \n 471 Getting Started Before proceeding with georeferencing in Quantum GIS, we need to activate the appropriate Plugins. On the toolbar go to Plugins -> Manage and Install Plugins  A window titled Plugin Manager will open. Scroll down to Georeference GDAL and check the box beside it, and click OK. \n At this point, you need to shut down and relaunch QGIS. For the purposes of this example, and to keep things as simple as possible, don't reload your existing project but instead start a new project. Set up the Coordinate Reference System (CRS)468 correctly (see ‘Installing QGIS 2.0 and adding Layers’469 for a reminder) Save this new project (under File menu, select Save Project) and call it 'georeferencing.' Add the 'coastline_polygon' layer (see ‘Installing QGIS 2.0 and adding Layers’ for a reminder) Open the Necessary GIS Layers For the Prince Edward Island case study, we are going to use the township boundaries as control points because they were established in 1764 by                                                         468\t‘Spatial\treference\tsystem’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Spatial_reference_system\t469\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDaniel\tMacfarlane,\t‘Installing\tQGIS\t2.0\tand\tAdding\tLayers’,\tThe\tProgramming\tHistorian,\t(2013).\t\n \n 472 Samuel Holland, they are identified on most maps of PEI, and they have changed very minimally since then. Download lot_township_polygon: This is the shapefile containing the modern vector layer we are going to use to georeference the historical map. Note that townships were not given names but rather a lot number in 1764, so they are usually referred to as 'Lots' in PEI. Hence the file name 'lot_township_polygon'. Navigate to the link below in your web browser, read/accept the license agreement, and then download the following (they will ask for your name and email before you can download the file). http://www.gov.pe.ca/gis/license_agreement.php3?name=lot\\_town&file_format=SHP After downloading the file called 'lot_township_polygon', move it into a folder that you can find later and unzip the file. (Remember to keep the files together as they are all required to open this layer in your GIS) \n Add lot_township_polygon to QGIS: under Layer on the toolbar, choose Add Vector Layer (alternatively the same icon you see next to 'Add Vector Layer' can also be selected from the tool bar) Click Browse. Navigate to your unzipped file and select the file titled 'lot_township_polygon.shp' Click Open  \n \n 473 For more information on adding and visualizing layers see ‘Installing QGIS 2.0 and adding Layers’.470 \n Open the Georeferencer Tool Georeferencer is now available under the Raster menu on the toolbar – select it. \n Add your historical map: In the resulting window, click on the Open Raster button on the top left (which looks identical to the Add Raster layer).  Find the file titled 'PEI_LakeMap1863.jpg' on your computer and select Open (the file can be downloaded from the Geospatian Historian website,471 or in its original location at the Island Imagined online map repository)472                                                         470\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDaniel\tMacfarlane,\t‘Installing\tQGIS\t2.0\tand\tAdding\tLayers’,\tThe\tProgramming\tHistorian,\t(2013).\t471\tAvailable\tat:\thttps://geospatialhistorian.files.wordpress.com/2013/02/pei_lakemap1863.jpg\t472\t‘Island\tImagined’:\thttp://www.islandimagined.ca/fedora/repository/imagined%3A208687\t\n \n 474 You will be prompted to define this layer's coordinate system. In the Filter box search for '2291′, then in the box below select 'NAD83(CSRS98) / Prince Edward …' The result will look like this: \n Adding control points: Plan the locations you are going to use as control points in advance of the steps that follow. It is much easier to navigate around the historical map first, so get a good idea of the best points to use and keep them in mind. Some tips for choosing control points: How many points do you need? Usually the more points you assign the more accurate your georeferenced map will be. Two control points will tell the GIS to scale and rotate the map to those two points, but in order to truly rubber-sheet the historical document you need to add more points. Where should you put control points? Select areas as close as possible to the four corners of your map so that these outer areas do not get omitted in the rubber-sheeting. Select additional control points close to your area of interest. Everything in between the four corner control points should georeference evenly, but if you are concerned about the accuracy of one place in particular, make sure to select additional control points in that area. Select the middle of intersections and roads, because the edges of roads changed a certain amount over time as road improvements were made. Check that your control points did not change location over time. Roads were often re-routed, and even houses and other buildings were moved, especially in Atlantic Canada!473 Add your first control point:                                                         473\tRobert\tMelin,\t‘Tilting:\tHouse\tLaunching,\tSide\tHauling,\tPotato\tTrenching,\tand\tOther\tTales\tfrom\ta\tNewfoundland\tFishing\tVillage’\t(2008).\t\n \n 475 First, navigate to the location of your first control point on the historical map. click on Zoom In Magnifying Glass on the window tool bar or use the mouse roller wheel to zoom in \n • zoom in to a point which you can recognize on both your printed map and your GIS • Click on Add Point on toolbar \n Click on the place in the printed map that you can locate in your GIS (i.e. the control point). The Georeferencer window will now minimize automatically. If it does not (some versions have a bug in this plugin) manually minimize the window Click on the place in the GIS which matches the control point \n At this stage we identified a problem in lot boundaries. We planned to use the location where the southern border of Lot 1 at the West end of the \n \n 476 Province contains a \"dog leg\" near the middle of the land mass. However, it was clear that not all the dog legs on these lots matched the historical map. It is possible that lot boundaries have changed somewhat in the 250 years since they were established, so it is best to choose the point you are most sure of. In this case the dog leg between Lot 2 and Lot 3 was fine (see arrow). It was the border of Lots 3 and 4 that has changed. The discrepancy at the border of 1 and 2 shows that more control points are needed to properly rubber-sheet this somewhat distorted 1863 map to the Provincial GIS layer \n Add at least one more control point: return to the Georeferencer window and repeat the steps under 'Add your first control point' above, to add additional control points. Add a point close to the opposite side of your printed map (the further apart your control points are placed the more accurate the georeferencing process) and another one near Charlottetown return to the Georeferencer window. You should see three red dots on the printed map, and three records in the GCP table at the bottom of your window (outlined in red on the following image) \n \n 477 \n  Determine the transformation settings: Before you click Play and start the automated georeferencing process you need to tell QGIS where to save the new file (this will be a raster file), how it should interpret your control points, and how it should compress the image. Click on the Transformation Settings button  Most of these settings can be left as default: linear transformation type, nearest neighbour resampling method, and LZW compression. (The world file474 is not necessary, unless you want to georeference the same image again in another GIS or if someone else needs to georeference the image and does not have access to your GIS data, coordinate reference system, etc.) The target SRS is not important, but you could use this feature to give the new raster a different reference system. Assign a folder for your new georeferenced raster file. Tif475 is the default format for rasters georeferenced in QGIS. Be aware that a Tif file is going to be much larger than your original map, even with LZW compression, so make sure you have adequate space if you are using a jump drive. (warning: the Tif file produced from this 6.8 Mb .jpg will be over 1GB once georeferenced. One way to manage the size of the georeferenced raster file while maintaining a high enough resolution for legibility is to crop out only the area needed for the map project. In this                                                         474\t‘World\tFile’,\tWikipedia:\thttps://en.wikipedia.org/wiki/World_file\t475\t‘Tagged\tImage\tFile\tFormat’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Tagged_Image_File_Format\t\n \n 478 case, a lower resolution option is also available from the Island Imagined476 online map repository.) Leave the target resolution at the default You can select 'Use 0 transparency when needed' to eliminate black spaces around the edges of the map, but this is not necessary and you can experiment as needed Make sure 'Load in QGIS' is selected to save a step. This will automatically add the new file to your GIS's Table of Contents so that you don't have to go looking for the Tif file later \n Georeference! Click on the Play button on the toolbar (beside Add Raster) – this begins the georeferencing process  \n A window will appear titled Define CRS: select 2291, click OK                                                         476\t‘Island\tImagined’:\thttp://www.islandimagined.ca/fedora/repository/imagined%3A208687\t\n \n 479 \n Explore your map: Drag the new layer 'PEI_LakeMap1863_modified' down to the bottom of your Table of Contents (i.e. below the 'lot_township_polygon' layer \n Change the fill of the lot_township_polygon layer to 'no brush' by Selecting the layer, clicking on Layer -> Properties, and clicking on Symbol Properties. Click OK \n \n 480 \n Now you should see the modern GIS layer with the historical map in behind \n Now that you have a newly georeferenced map in your GIS you can explore the layer, adjust the transparency, contrast and brightness, and go back through ‘Creating New Vector Layers in QGIS’477 to digitize some of the historical information that you have created. For instance, this georeferenced map of PEI shows the locations of all homes in 1863, including the name of the head of household. By assigning points on the map you can enter home locations and owner names and then analyze or share that new geospatial layer as a shapefile.                                                         477\tJim\tClifford,\tJosh\tMacFadyen,\tand\tDaniel\tMacfarlane,\t‘Creating\tNew\tVector\tLayers\tin\tQGIS’,\tThe\tProgramming\tHistorian\t(2013).\t\n \n 481 By digitizing line vectors such as roads or coastlines you can compare the location of these features with other historical data, or simply compare them visually with the lot_township_polygon layer in this GIS. In more advanced processes you can even drape this georeferenced image over a DEM (digital elevation model) to give it a hillshade terrain or 3D effect and perform a 'fly-over' of PEI homes in the nineteenth century. About the Authors Jim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University. \n 482 Part Six: Sustaining Data We spend a lot of time and energy (as well as money) building digital history resources. The lessons in this section provide tips and methods for helping to ensure that energy is preserved for the future. This is about good practice.  \n 483 46. Preserving Your Research Data James Baker – 2014. Background In his 2003 essay 'Scarcity or Abundance' Roy Rosenzweig sought to alert historians to what he called 'the fragility of evidence in the digital era' (Rosenzweig, 736).478 And whilst his concerns were focused on sources available on the open web, they can easily be extended to the born-digital materials – or data – historians create during their research. It is this research data that the present guide will focus upon. But why? Well, historians are moving toward using computers as the default means of storing all of their research data, their stuff. Their manuscripts have been digital objects for some time and their research is moving accordingly – be that in the form of typed notes, photographs of archives, or tabulated data. Moreover research data held in a digital form has clear advantages over its physical antecedents: it can be browsed and searched, hosted in ways that enable access in many places, and merged with or queried against other research data. Merely putting research data into digital form does not guarantee it will survive. Here by survival I neither mean survive in a literal sense nor in a survival as readable by the next version of Microsoft Word sense, but rather in a usable by people sense. For if not a problem solved, the nuts and bolts of how to preserve research data for the future is a problem whose potential solutions have already been addressed at length, both with and without historians in mind. So too have data management experts, services and the like talked about scholarly best practice with regards to documenting, structuring and organising research data. In spite of all this, research data generated by an individual historian is at risk of loss if that historian is not able to generate and preserve it in a form they can understand and find meaningful years or decades after the fact, let alone someone else wading through the idiosyncrasies of their research process. In short, there is a risk of loss as a consequence of data being detached from the context of its creation, from the tacit knowledge that made it useful at the time of preparing talk X or manuscript Y. As William Stafford Noble puts it: \tThe\tcore\tguiding\tprinciple\tis\tsimple:\tSomeone\tunfamiliar\twith\tyour\tproject\tshould\tbe\t\table\tto\t\tlook\tat\tyour\tcomputer\tfiles\tand\tunderstand\tin\tdetail\twhat\tyou\tdid\tand\twhy\t\t[…]Most\tcommonly,\thowever,\tthat\t“someone”\tis\tyou.\tA\tfew\tmonths\tfrom\tnow,\tyou\t\tmay\tnot\tremember\twhat\tyou\twere\tup\tto\twhen\tyou\tcreated\ta\tparticular\tset\tof\tfiles,\tor\t\tyou\tmay\tnot\tremember\t\twhat\t                                                        478\tRoy\tRosenzweig,\t‘Scarcity\tor\tAbundance?\tPreserving\tthe\tPast\tin\ta\tDigital\tEra’:\thttp://www.islandimagined.ca/fedora/repository/imagined%3A208687\t \n 484 conclusions\tyou\tdrew.\tYou\twill\teither\thave\tto\tthen\tspend\t\ttime\treconstructing\tyour\tprevious\texperiments\tor\tlose\twhatever\tinsights\tyou\tgained\t\tfrom\tthose\texperiments.479\tDrawing on the lessons and expertise of research data experts, the present guide will suggest ways in which historians can document and structure their research data so as to ensure it remains useful in the future. The guide is not intended to be prescriptive, rather it is assumed readers will iterate, change, and adapt the ideas presented to best fit their own research.  Documenting research data Birkwood,\tKatie\t(girlinthe).\t“Victory\tis\tmine:\twhile\tago\tI\tworked\tout\tsome\tClever\tStuff\t™\tin\tExcel.\tAnd\tI\tMADE\tNOTES\tON\tIT.\tAnd\tthose\tnotes\tENABLED\tME\tTO\tDO\tIT\tAGAIN.”\t7\tOctober\t2013,\t3:46\ta.m..\tTweet.480\tThe purpose of documentation is to capture the process of data creation, changes made to data, and tacit knowledge associated with data. Project management methodologies, such as PRINCE2,481 place great emphasis on precise, structured, and verbose documentation. Whilst there are benefits to this approach, especially for large, complex, multi-partner projects, the average working historian is more likely to benefit from a flexible, bespoke approach to documentation that draws on, but is not yoked to, project management principles. In the case of historical research, the sort of documentation that might be produced to preserve the usefulness of research data includes: documentation describing notes taken whilst examining a document in an archive, such as the archival reference for the original document, how representative the notes are (e.g. full transcriptions, partial transcriptions, or summaries), how much of the document was examined, or decisions taken to exclude sections of the document from the research process. documentation describing tabulated data, such as how it was generated (e.g. by hand or in an automated manner), archival references for the original sources some data came from, or what attributes of the original sources were retained (and why). documentation describing a directory of digital images, such as how each image was created, where those images were downloaded from, or research notes that refer to them. As the last example suggests, one of the key purposes of documentation is to describe the meaningful links that exist between research data, links that may not remain obvious over time.                                                         479\tWilliam\tStafford\tNoble\t(2009)\tA\tQuick\tGuide\tto\tOrganizing\tComputational\tBiology\tProjects.\tPLoSComputBiol\t5(7):\te1000424.\tdoi:10.1371/journal.pcbi.1000424\t480\tTweet\tby\tKatie\tBirkwood,\thttps://twitter.com/Girlinthe/status/387166944094199809\t481\t‘PRINCE2’,\tWikipedia:\thttps://en.wikipedia.org/wiki/PRINCE2\t \n 485 When to document is very much up to the individual and the rhythm of their research. The main rule is to get into a habit of writing and updating documentation at regular intervals, ideally every time a batch of work is finished for the morning, afternoon, or day. At the same time it is important not to worry about perfection, rather to aim to write consistent and efficient documentation that will be useful to you, and hopefully someone else using your research data, years after the fact.  File formats Research data and documentation should ideally be saved in platform agnostic formats482 such as .txt for notes and .csv (comma-separated values) or .tsv (tab-seperated values) for tabulated data. These plain text formats are preferable to the proprietary formats used as defaults by Microsoft Office or iWork because they can be opened by many software packages and have a strong chance of remaining viewable and editable in the future. Most standard office suites include the option to save files in .txt, .csv and .tsv formats, meaning you can continue to work with familiar software and still take appropriate action to make your work accessible. Compared to .doc or .xls these formats have the additional benefit, from a preservation perspective, of containing only machine-readable elements. Whilst using bold, italics, and colouring to signify headings or to make a visual connection between data elements is common practice, these display-orientated annotations are not machine-readable and hence can neither be queried and searched nor are appropriate for large quantities of information. Preferable are simple notation schemes such as using a double-asterisk or three hashes to represent a data feature: in my own notes, for example, three question marks indicate something I need to follow up on, chosen because '???' can easily be found with a CTRL+F search. It is likely that on many occasions these notation schemes will emerge from existing individual practice (and as a consequence will need to be documented), though existing schema such as Markdown483 are available (Markdown files are saved as .md). An excellent Markdown cheat sheet is available on GitHub https://github.com/adam-p/markdown-here) for those who wish to follow – or adapt – this existing schema. ‘Notepad++’ http://notepad-plus-plus.org/ is recommended for Windows users, though by no means essential, for working with .md files. Mac or Unix users may find ‘Komodo Edit’ or ‘Text Wrangler’ helpful.484                                                          482\t‘Cross-platform’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Cross-platform\t483\t‘Markdown’,\tWikipedia:\thttps://en.wikipedia.org/wiki/Markdown\t484\t‘Komodo\tEdit’:\thttp://komodoide.com/komodo-edit/;\t‘TextWrangler’:\thttps://itunes.apple.com/gb/app/id404010395?mt=12\t \n 486 Recap 1 To recap, the key points about documentation and file formats are: Aim for documentation to capture in a precise and consistent manner the tacit knowledge surrounding a research process, be that with relation to note taking, generating tabulated data, or accumulating visual evidence. Keep documentation simple by using file formats and notation practices that are platform agnostic and machine-readable. Build time for updating and creating documentation into your workflow without allowing documentation work to become a burden. Make an investment in leaving a paper trail now to save yourself time attempting to reconstruct it in the future.  Structuring research data Documenting your research is made easier by structuring your research data in a consistent and predictable manner. Why? Well, every time we use a library or archive catalogue, we rely upon structured information to help us navigate data (both physical and digital) the library or archive contains. Without that structured information, our research would be much poorer. Examining URLs is a good way of thinking about why structuring research data in a consistent and predictable manner might be useful in your research. Bad URLs are not reproducible and hence, in a scholarly context, not citable. On the contrary, good URLs represent with clarity the content of the page they identify, either by containing semantic elements or by using a single data element found across a set or majority of pages. A typical example of the former are the URLs used by news websites or blogging services. WordPress URLs follow the format: website name/year(4 digits)/month (2 digits)/day (2 digits)/words-of-title-separated-by-hyphens http://cradledincaricature.com/2014/02/06/comic-art-beyond-the-print-shop/ A similar style is used by news agencies such as a The Guardian newspaper: website name/section subdivision/year (4 digits)/month (3 characters)/day (2 digits)/words-describing-content-separated-by-hyphens http://www.theguardian.com/uk-news/2014/feb/20/rebekah-brooks-rupert-murdoch-phone-hacking-trial .  \n 487 In archival catalogues, URLs structured by a single data element are often used. The British Cartoon Archive structures its online archive using the format: website name/record/reference number http://www.cartoons.ac.uk/record/SBD0931 And the Old Bailey Online uses the format: website name/browse.jsp?ref=reference number http://www.oldbaileyonline.org/browse.jsp?ref=OA16780417 What we learn from these examples is that a combination of semantic description and data elements make consistent and predictable data structures readable both by humans and machines. Transferring this to digital data accumulated during the course of historical research makes research data easier to browse, to search and to query using the standard tools provided by our operating systems (and, as we shall see in a future lesson, by more advanced tools). In practice (for OS X and Linux users, replace all backslashes hereafter with forward slash), the structure of a good research data archive might look something like this: A base or root directory, perhaps called 'work'. \\work\\\tA series of sub-directories. \t\t\t\t\t\\work\\events\\\t\t\t\t\t\t\\research\\\t\t\t\t\t\t\\teaching\\\t\t\t\t\t\t\\writing\\\tWithin these directories are series of directories for each event, research project, module, or piece of writing. Introducing a naming convention that includes a date elements keeps the information organised without the need for subdirectories by, say, year or month. \\work\\research\\2014-01_Journal_Articles\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\2014-02_Infrastructure\tFinally, further sub-directories can be used to separate out information as the project grows. \\work\\research\\2014_Journal_Articles\\analysis\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\data\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\notes\tObviously not all information will fit neatly within any given structure and as new projects arise taxonomies will need to be revisited. Either way, idiosyncrasy is fine so long as the overall directory structure is consistent and predictable, and so long as anything that isn’t is clearly documented:  \n 488 for example, the 'writing' sub-directory in the above structure might include a .txt file stating what it contained (drafts and final version of written work) and what it didn't contain (research pertaining to that written work). The name of this .txt file, indeed any documentation and research data, is important to ensuring it and its contents are easy to identify. 'Notes about this folder.docx' is not a name that fulfils this purpose, whilst '2014-01-31_Writing_readme.txt' is as it replicates the title of the directory and included some date information (North American readers should note that I've chosen the structure year_month_date). A readme file I made for a recent project https://www.dropbox.com/s/i12cv5rdnfbdoz3/network_analysis_of_Isaac_Cruikshank_and_his_publishers_readme.txt contains the sort of information that you and other users of your data might find useful. An cautionary tale should be sufficient to confirm the value of this approach. During the course of a previous research project, I collected some 2,000 digital images of Georgian satirical prints from a number of online sources, retaining the file names upon download. Had I applied a naming convention to these from the outset (say 'PUBLICATION YEAR_ARTIST SURNAME_TITLE OF WORK.FORMAT') I would be able to search and query these images. Indeed starting each filename with some version of YYYYMMDD would have meant that the files could be sorted in chronological order on Windows, OS X and Linux. And ensuring that all spaces or punctuation (except dash, dot and underscore) were removed from the filenames in the process of making them consistent and predictable, would have made command line work with the files possible. But I did not, and as it stands I would need to set aside a large amount of time to amend every filename individually so as to make the data usable in this way. Further, applying such naming conventions to all research data in a consistent and predictable manner assists with the readability and comprehension of the data structure. For example for a project on journal articles we might choose the directory… \\work\\research\\2014-01_Journal_Articles\\\t…where the year-month elements captures when the project started. Within this directory we include a \\data\\ directory where the original data used in the project is kept. 2014-01-31_Journal_Articles.tsv\tAlongside this data is documentation that describes 2014-01-31_Journal_Articles.tsv. 2014-01-31_Journal_Articles_notes.txt\t \n 489 Going back a directory level to \\2014-01_Journal_Articles\\ we create the \\analysis\\ directory in which we place: 2014-02-02_Journal_Articles_analysis.txt\t2014-02-15_Journal_Articles_analysis.txt\tNote the different month and date attributes here. These reflect the dates on which data analysis took place, a convention described briefly in 2014-02-02_Journal_Articles_analysis_readme.txt. Finally, a directory within \\data\\ called \\derived_data\\ contains data derived from the original 2014-01-31_Journal_Articles.tsv. In this case, each derived .tsv file contains lines including the keywords, 'africa', 'america', 'art' et cetera, and are named accordingly. 2014-01-31_Journal_Articles_KW_africa.tsv\t\t2014-01-31_Journal_Articles_KW_america.tsv\t\t2014-02-01_Journal_Articles_KW_art\t.tsv\t\t2014-02-02_Journal_Articles_KW_britain.tsv\t Recap 2 To recap, the key points about structuring research data are: Data structures should be consistent and predictable. Consider using semantic elements or data identifiers to structure research data directories. Fit and adapt your research data structure to your research. Apply naming conventions to directories and file names to identify them, to create associations between data elements, and to assist with the long term readability and comprehension of your data structure.  Summary This lesson has suggested ways for documenting and structuring research data, the purpose of which is to ensure that data is preserved by capturing tacit knowledge gained during the research process and thus making the information easy to use in the future. It has recommended the use of platform agnostic and machine-readable formats for documentation and research data. It has suggested that URLs offer a practice example of both good and bad data structures that can be replicated for the purposes of a historian's research data. These suggestions are intended merely as guides; it is expected that researchers will adapt them to suit their purposes. In doing so, it is recommended that researchers keep digital preservation strategies and  \n 490 project management best practice in mind, whilst ensuring that time spent documenting and structuring research does not become a burden. After all, the purpose of this guide is to make more not less efficient historical research that generates data. That is, your research.  Further Reading Ashton, Neil, 'Seven deadly sins of data publication', School of Data blog (17 October 2013) http://schoolofdata.org/2013/10/17/seven-deadly-sins-of-data-publication/ Hitchcock, Tim, 'Judging a book by its URLs', Historyonics blog (3 January 2014) http://historyonics.blogspot.co.uk/2014/01/judging-book-by-its-url.html Howard, Sharon, 'Unclean, unclean! What historians can do about sharing our messy research data', Early Modern Notes blog (18 May 2013) http://earlymodernnotes.wordpress.com/2013/05/18/unclean-unclean-what-historians-can-do-about-sharing-our-messy-research-data/ Noble, William Stafford, A Quick Guide to Organizing Computational Biology Projects.PLoSComputBiol 5(7): e1000424 (2009) http://dx.doi.org/10.1371/journal.pcbi.1000424 Oxford University Computing Services, 'Sudamih Project. Research Information Management: Organising Humanities Material' (2011) http://dspace.jorum.ac.uk/xmlui/handle/10949/14725 Pennock, Maureen, 'The Twelve Principles of Digital Preservation (and a cartridge in a repository…)', British Library Collection Care blog (3 September 2013) http://britishlibrary.typepad.co.uk/collectioncare/2013/09/the-twelve-principles-of-digital-preservation.html Pritchard, Adam, 'Markdown Cheatsheet' (2013) https://github.com/adam-p/markdown-here Rosenzweig, Roy, 'Scarcity or Abundance? Preserving the Past in a Digital Era', The American Historical Review 108:3 (2003), 735-762. UK Data Archive, 'Documenting your Data' http://data-archive.ac.uk/create-manage/document About the Author James Baker is a lecturer of digital history at the University of Sussex.  \n 491 47. Getting Started with Markdown Sarah Simpkin – 2015 Lesson goals In this lesson, you will be introduced to Markdown, a plain text-based syntax for formatting documents. You will find out why it is used, how to format Markdown files, and how to preview Markdown-formatted documents on the web. Since Programming Historian lessons are submitted as Markdown files, I have included PH-specific examples whenever possible. It is my hope that this guide will be useful to you if you are considering authoring a lesson for this site. What is Markdown? Developed in 2004 by John Gruber,485 Markdown refers to both (1) a way of formatting text files, as well as (2) a Perl utility to convert Markdown files into HTML. In this lesson, we'll focus on the first part and learn to write files using the Markdown syntax. Plain text files have many advantages over other formats. For one, they are readable on virtually all devices. They have also withstood the test of time better than other file types -- if you've ever tried to open a document saved in a legacy word processor format, you'll be familiar with the compatibility challenges involved. By following Markdown syntax, you'll be able to produce files that are both legible in plain text and ready to be styled on other platforms. Many blogging engines, static site generators, and sites like GitHub486 also support Markdown, and will render these files into HTML for display on the web. Additionally, tools like Pandoc can convert files into and out of Markdown. For more on Pandoc, visit the lesson on ‘Sustainable authorship in plain text using Pandoc and Markdown’ by Dennis Tenen and Grant Wythoff.487 Markdown Syntax Markdown files are saved with the extension .md, and can be opened in a text editor such as TextEdit, Notepad, Sublime Text, or Vim. Many websites and publishing platforms also offer web-based editors and/or extensions for entering text using Markdown syntax.                                                         485\tJohn\tGruber,\t‘Markdown’,\tDaring\tFireball:\thttp://daringfireball.net/projects/markdown/\t486\t‘Github’:\thttps://github.com/\t487\tDennis\tTenen\tand\tGrant\tWythoff,\t‘Sustainable\tAuthorship\tin\tPlain\tText\tusing\tPandoc\tand\tMarkdown’,\tThe\tProgramming\tHistorian,\t(2014).\t \n 492 In this tutorial, we'll be practicing Markdown syntax in the browser using StackEdit.488 You'll be able to enter Markdown-formatted text on the left and immediately see the rendered version alongside it on the right. Since all Programming Historian lessons are written in Markdown, we can examine these files in StackEdit too. From the StackEdit editor,489 click on the # in the upper left corner for a menu. Choose Import\tfrom\tURL, then paste the following URL to display the \"Intro to Bash\" lesson in the editor: https://raw.githubusercontent.com/programminghistorian/jekyll/gh-pages/lessons/intro-to-bash.md\tYou'll notice that while the right panel features a more elegant rendering of the text, the original Markdown file on the left is still fairly readable. Now, let's dive into the lesson by writing our own Markdown syntax. Create a new document in StackEdit by clicking the folder icon in the upper right and choosing New\tdocument. You may enter a title for the document in the textbox on the top of the page. Headings Four levels of headings are available in Markdown, and are indicated by the number of # preceding the heading text. Paste the following examples into the textbox on your left: #\tFirst\tlevel\theading\t##\tSecond\tlevel\theading\t###\tThird\tlevel\theading\t####\tFourth\tlevel\theading\tFirst and second level headings may also be entered as follows: First\tlevel\theading\t=======\t\tSecond\tlevel\theading\t----------\tThese will render as: First level heading  Second level heading  Third level heading  Fourth level heading                                                          488\t‘Stackedit’:\thttps://stackedit.io/\t489\t‘Stackedit\tEditor’:\thttps://stackedit.io/editor\t \n 493 First level heading Second level heading Notice how the Markdown syntax remains understandable even in the plain text version. Paragraphs & Line Breaks Try typing the following sentence into the textbox: Welcome\tto\tthe\tProgramming\tHistorian.\t\tToday\twe'll\tbe\tlearning\tabout\tMarkdown\tsyntax.\tThis\tsentence\tis\tseparated\tby\ta\tsingle\tline\tbreak\tfrom\tthe\tpreceding\tone.\tThis renders as: Welcome to the Programming Historian. Today we'll be learning about Markdown syntax. This sentence is separated by a single line break from the preceding one. Paragraphs must be separated by an empty line. Leave an empty line between syntax. and This to see how this works. In some implementations of Markdown, single line breaks must also be indicated with two empty spaces at the end of each line. This is unnecessary in the GitHub Flavored Markdown variant that StackEdit uses by default.490 Adding Emphasis Text can be italicized by wrapping the word in * or _ symbols. Likewise, bold text is written by wrapping the word in ** or __. Try adding emphasis to a sentence using these methods: I\tam\t**very**\texcited\tabout\tthe\t_Programming\tHistorian_\ttutorials.\tThis renders as: I am very excited about the Programming Historian tutorials. Making Lists Markdown includes support for ordered and unordered lists. Try typing the following list into the textbox:                                                            490\t‘Categories/Writing\ton\tGithub’:\thttps://help.github.com/categories/writing-on-github/\t \n 494 Shopping\tList\t----------\t*\tFruits\t\t\t\t\t*\tApples\t\t\t\t\t*\tOranges\t\t\t\t\t*\tGrapes\t*\tDairy\t\t\t\t\t*\tMilk\t\t\t\t\t*\tCheese\tIndenting the * by four spaces will allow you to created nested items. This renders as:   Shopping List Fruits  Apples Oranges Grapes Dairy  Milk Cheese Ordered lists are written by numbering each line. Once again, the goal of Markdown is to produce documents that are both legible as plain text and able to be transformed into other formats. To-do\tlist\t----------\t1.\tFinish\tMarkdown\ttutorial\t2.\tGo\tto\tgrocery\tstore\t3.\tPrepare\tlunch\tThis renders as: To-do\tlist\tFinish Markdown tutorial Go to grocery store Prepare lunch Code Snippets Representing code snippets differently from the rest of a document is a good practice that improves readability. Typically, code is represented in monospaced type. Since Markdown does not distinguish between fonts, we represent code by wrapping snippets in back-tick characters like `. For  \n 495 example, `<br\t/>`. Whole blocks of code are written by typing three back-tick characters before and after each block. In the StackEdit preview window, this will render a shaded box with text in a monospaced font. Try typing the following text into the textbox: ```html\t<html>\t\t\t\t\t<head>\t\t\t\t\t\t\t\t\t<title>Website\tTitle</title>\t\t\t\t\t</head>\t\t\t\t\t<body>\t\t\t\t\t</body>\t</html>\t```\tThis renders as: \t\t\t\t<html>\t\t\t\t\t\t\t\t\t<head>\t\t\t\t\t\t\t\t\t\t\t\t\t<title>Website\tTitle</title>\t\t\t\t\t\t\t\t\t</head>\t\t\t\t\t\t\t\t\t<body>\t\t\t\t\t\t\t\t\t</body>\t\t\t\t\t</html>\tNotice how the code block renders in a monospaced font. Blockquotes Adding a > before any paragraph will render it as a blockquote element. Try typing the following text into the textbox: >\tHello,\tI\tam\ta\tparagraph\tof\ttext\tenclosed\tin\ta\tblockquote.\tNote\tI\tam\toffset\tfrom\tthe\tleft\tmargin.\t\tThis renders as: \t\t\t\t\t\t\t\tHello,\tI\tam\ta\tparagraph\tof\ttext\tenclosed\tin\ta\tblockquote.\tNote\tI\tam\t\t\toffset\tfrom\tthe\tleft\tmargin.\tLinks Links can be written in two styles. Inline links are written by enclosing the link text in square brackets first, then including the URL and optional alt-text in round brackets. For\tmore\ttutorials,\tplease\tvisit\tthe\t[Programming\tHistorian](http://programminghistorian.org/\t\"Programming\tHistorian\tmain\tpage\"). This renders as: For more tutorials, please visit the Programming Historian.  \n 496 Reference-style links are handy for footnotes and may keep your plain text document neater. These are written with an additional set of square brackets to establish a link ID label. One\texample\tis\tthe\t[Programming\tHistorian][1]\twebsite. You may then add the URL to another part of the document: [1]:\thttp://programminghistorian.org/\t\"The\tProgramming\tHistorian\" This renders as: One example is the Programming Historian website. Images Images can be referenced using !, followed by some alt-text in square brackets, followed by the image URL and an optional title. These will not be displayed in your plain text document, but would be embedded into a rendered HTML page. ![Wikipedia\tlogo](http://upload.wikimedia.org/wikipedia/en/8/80/Wikipedia-logo-v2.svg\t\"Wikipedia\tlogo\") This renders as: \tWikipedia logo Horizontal Rules Horizontal rules are produced when three or more -, * or _ are included on a line by themselves, regardless of the number of spaces between them. All of the following combinations will render horizontal rules: ___\t*\t*\t*\t-\t-\t-\t-\t-\t-\tThis renders as:    \n \n 497  Tables The core Markdown spec does not include tables; however, some sites and applications use variants of Markdown that may include tables and other special features. GitHub Flavored Markdown491 is one of these variants, and is used to render .md files in the browser on the GitHub site. To create a table within GitHub, use pipes | to separate columns and hyphens - between your headings and the rest of the table content. While pipes are only strictly necessary between columns, you may use them on either side of your table for a more polished look. Cells can contain any length of content, and it is not necessary for pipes to be vertically aligned with each other. |\tHeading\t1\t|\tHeading\t2\t|\tHeading\t3\t|\t|\t---------\t|\t---------\t|\t---------\t|\t|\tRow\t1,\tcolumn\t1\t|\tRow\t1,\tcolumn\t2\t|\tRow\t1,\tcolumn\t3|\t|\tRow\t2,\tcolumn\t1\t|\tRow\t2,\tcolumn\t2\t|\tRow\t2,\tcolumn\t3|\t|\tRow\t3,\tcolumn\t1\t|\tRow\t3,\tcolumn\t2\t|\tRow\t3,\tcolumn\t3|\tThis renders as: Heading 1 Heading 2 Heading 3 Row 1, column 1 Row 1, column 2 Row 1, column 3 Row 2, column 1 Row 2, column 2 Row 2, column 3 Row 3, column 1 Row 3, column 2 Row 3, column 3 To specify the alignment of each column, colons : can be added to the header row as follows: |\tLeft-aligned\t|\tCentered\t|\tRight-aligned\t|\t|\t:--------\t|\t:-------:\t|\t--------:\t|\t|\tApples\t|\tRed\t|\t5000\t|\t|\tBananas\t|\tYellow\t|\t75\t|\tThis renders as: Left-aligned Centered Right-aligned Apples Red 5000 Bananas Yellow 75 Markdown Limitations While Markdown is becoming increasingly popular, particularly for styling documents that are viewable on the web, many people and publishers still expect traditional Word documents, PDFs, and other file formats. This can be mitigated somewhat with command line conversion tools such as                                                         491\t‘Categories/\tWriting\ton\tGithub’:\thttps://help.github.com/categories/writing-on-github/\t \n 498 Pandoc;492 however, certain word processor features like track changes are not supported yet. Please visit the Programming Historian lesson on ‘Sustainable authorship in plain text using Pandoc and Markdown’493 for more information about Pandoc. Conclusion Markdown is a useful middle ground between unstyled plain text files and legacy word processor documents. Its simple syntax is quick to learn and legible both by itself and when rendered into HTML and other document types. Finally, choosing to write your own documents in Markdown should mean that they will be usable and readable in the long-term. About the Author Sarah Simpkin is a GIS, Geography, and Computer Science librarian at the University of Ottawa. \n                                                        492\t‘Pandoc’:\thttp://pandoc.org/\t493\tDennis\tTenen\tand\tGrant\tWythoff,\t‘Sustainable\tAuthorship\tin\tplaint\ttext\tusing\tPandoc\tand\tMarkdown’,\tThe\tProgramming\tHistorian\t(2014).\t \n 499  48. Sustainable Authorship in Plain Text using Pandoc and Markdown Dennis Tenen and Grant Wythoff – 2014     \n Objectives In this tutorial, you will first learn the basics of Markdown—an easy to read and write markup syntax for plain text—as well as Pandoc,494 a command line tool that converts plain text into a number of beautifully formatted file types: PDF, .docx, HTML, LaTeX, slide decks, and more. With Pandoc as your digital typesetting tool, you can use Markdown syntax to add figures, a bibliography, formatting, and easily change citation styles from Chicago to MLA (for instance), all using plain text. The tutorial assumes no prior technical knowledge, but it scales with experience, as we often suggest more advanced techniques towards the end of each section. These are clearly marked and can be revisited after some practice and experimentation. Instead of following this tutorial in a mechanical way, we recommend you strive to understand the solutions offered here as a methodology, which may need to be tailored further to fit your environment and workflow. The installation of the necessary tools presents perhaps the biggest barrier to participation. Allot yourself enough time and patience to install everything                                                         494\t‘Pandoc’:\thttp://pandoc.org/\t\n \n 500 properly, or do it with a colleague who has a similar set-up and help each other out. Consult the ‘Useful Resources’ section below if you get stuck. Philosophy Writing, storing, and retrieving documents are activities central to the humanities research workflow. And yet, many authors base their practice on proprietary tools and formats that sometimes fall short of even the most basic requirements of scholarly writing. Perhaps you can relate to being frustrated by the fragility of footnotes, bibliographies, figures, and book drafts authored in Microsoft Word or Google Docs. Nevertheless, most journals still insist on submissions in .docx format. More than causing personal frustration, this reliance on proprietary tools and formats has long-term negative implications for the academic community. In such an environment, journals must outsource typesetting, alienating authors from the material contexts of publication and adding further unnecessary barriers to the unfettered circulation of knowledge.  When you use MS Word, Google Docs, or Open Office to write documents, what you see is not what you get. Beneath the visible layer of words, sentences, and paragraphs lies a complicated layer of code understandable only to machines. Because of that hidden layer, your .docx and .pdf files depend on proprietary tools to be rendered correctly. Such documents are difficult to search, to print, and to convert into other file formats. Moreover, time spent formatting your document in MS Word or Open Office is wasted, because all that formatting is removed by the publisher during submission. Both authors and publishers would benefit from exchanging files with minimal formatting, leaving the typesetting to the final typesetting stage of the publishing process. This is where Markdown shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\" Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents. For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general. Popular general purpose plain text editors include ‘TextWrangler’495 and ‘Sublime’ for Mac,496 ‘Notepad++’ for Windows,497 as well as ‘Gedit’ and                                                         495\t‘TextWrangler’:\thttp://www.barebones.com/products/textwrangler/\t496\t‘Sublime’:\thttp://www.sublimetext.com/\t497\t‘Notepad++’:\thttps://notepad-plus-plus.org/\t \n 501 ‘Kate’ for Linux. However, there are also editors that specialize in displaying and editing Markdown. It is important to understand that Markdown is merely a convention. Markdown files are stored as plain text, further adding to the flexibility of the format. Plain text files have been around since the electronic typewriter. The longevity of this standard inherently makes plain text more sustainable and stable than proprietary formats. While files produced even ten years ago in Microsoft Word and Apple's Pages can cause significant problems when opened with the latest version, it is still possible to open a file written in any number of \"dead\" plain text editors from the past several decades: AlphaPlus, Perfect Writer, Text Wizard, Spellbinder, WordStar, or Isaac Asimov's favorite SCRIPSIT 2.0, made by Radio Shack. Writing in plain text guarantees that your files will remain readable ten, fifteen, twenty years from now. In this tutorial, we outline a workflow that frees the researcher from proprietary word processing software and fragile file formats. It is now possible to write a wide range of documents in one format—articles, blog posts, wikis, syllabi, and recommendation letters—using the same set of tools and techniques to search, discover, backup, and distribute our materials. Your notes, blog entries, code documentation, and wikis can all be authored in Markdown. Increasingly, many platforms like WordPress, Reddit, and GitHub support Markdown authorship natively. In the long term, your research will benefit from such unified workflows, making it easier to save, search, share, and organize your materials. Principles Inspired by best practices in a variety of disciplines, we were guided by the following principles: 1. Sustainability. Plain text both ensures transparency and answers the standards of long-term preservation. MS Word may go the way of Word Perfect in the future, but plain text will always remain easy to read, catalog, mine, and transform. Furthermore, plain text enables easy and powerful versioning of the document, which is useful in collaboration and organizing drafts. Your plain text files will be accessible on cell phones, tablets, or, perhaps, on a low-powered terminal in some remote library. Plain text is backwards compatible and future-proof. Whatever software or hardware comes along next, it will be able to understand your plain text files. 2. Preference for human-readable formats. When writing in Word or Google Docs, what you see is not what you get. The .doc file contains hidden, automatically-generated formatting characters, creating an obfuscated typesetting layer that is difficult for the user to troubleshoot. Something as simple as pasting an image or text from the browser can have unpredictable effects on your document's formatting.  \n 502 3. Separation of form and content. Writing and formatting at the same time is distracting. The idea is to write first, and format later, as close as possible to the time of publication. A task like switching from Chicago to MLA formatting should be painless. Journal editors who want to save time on needless formatting and copy editing should be able to provide their authors with a formatting template which takes care of the typesetting minutia. 4. Support for the academic apparatus. The workflow needs to handle footnotes, figures, international characters, and bibliographies gracefully. 5. Platform independence. As the vectors of publication multiply, we need to be able to generate a multiplicity of formats including for slide projection, print, web, and mobile. Ideally, we would like to be able to generate the most common formats without breaking bibliographic dependencies. Our workflow needs to be portable as well–it would be nice to be able to copy a folder to a thumbdrive and know that it contains everything needed for publication. Writing in plain text means you can easily share, edit, and archive your documents in virtually any environment. For example, a syllabus written in Markdown can be saved as a PDF, printed as a handout, and converted into HTML for the web, all from the same file. Both web and print documents should be published from the same source and look similar, preserving the logical layout of the material. Markdown and LaTeX answer all of these requirements. We chose Markdown (and not LaTeX) because it offers the most light-weight and clutter free syntax (hence, mark down) and because when coupled with Pandoc it allows for the greatest flexibility in outputs (including .docx and .tex files).  Software Requirements We purposefully omit some of the granular, platform- or operating system-bound details of installing the software listed below. For example, it makes no sense to provide installation instructions for LaTeX, when the canonical online instructions for your operating system will always remain more current and more complete. Similarly, the mechanics of Pandoc installation are best explored by searching for \"installing Pandoc\" on Google, with the likely first result being Pandoc's homepage. • Plain text editor. Entering the world of plain-text editing expands your choice of innovative authoring tools dramatically. Search online for \"markdown text editor\" and experiment with your options. It does not matter what you use as long as it is explicitly a plain text editor. Notepad++ on Windows or TextWrangler on Macs are easy, free choices. Remember, since we are not tied to the tool, you can change editors at any time.  \n 503 • Command line terminal. Working \"in the command line\" is equivalent to typing commands into the terminal. On a Mac you simply need to use your finder for \"Terminal\". On Windows, use PowerShell. Linux users are likely to be familiar with their terminals already. We will cover the basics of how to find and use the command line below. • Pandoc. Detailed, platform-specific installation instructions are available at the Pandoc website.498 Installation of Pandoc on your machine is crucial for this tutorial, so be sure to take your time and click through the instructions. Pandoc was created and is maintained by John MacFarlane, Professor of Philosophy at the University of California, Berkeley. This is humanities computing at its best and will serve as the engine of our workflow. With Pandoc, you will be able to compile text and bibliography into beautifully formatted and flexible documents. Once you've followed the installation instructions, verify that Pandoc is installed by entering pandoc\t--version into the command line. We assume that you have at least version 1.12.3, released in January 2014. The following two pieces of software are recommended, but not required to complete this tutorial. • Zotero or Endnote. Bibliographic reference software like Zotero and Endnote are indispensable tools for organizing and formatting citations in a research paper. These programs can export your libraries as a BibTeX file (which you will learn more about in Case 2 below). This file, itself a formatted plain text document of all your citations, will allow you to quickly and easily cite references using @tags. It should be noted that it's also possible to type all of your bibliographic references by hand, using our bibliography as a template.499 • LaTeX. Detailed, platform-specific installation instructions available at the Pandoc website.500 Although LaTeX is not covered in this tutorial, it is used by Pandoc for .pdf creation. Advanced users will often convert into LaTeX directly to have more granular control over the typesetting of the .pdf. Beginners may want to consider skipping this step. Otherwise, type latex\t-v to see if LaTeX was installed correctly (you will get an error if it was not and some information on the version if it was). Markdown Basics Markdown is a convention for structuring your plain-text documents semantically. The idea is to identify logical structures in your document (a title, sections, subsections, footnotes, etc.), mark them with some                                                         498\t‘Installing\tPandoc’:\thttp://pandoc.org/installing.html\t499\t‘pandoc-workflow\t/\tpandoctut.bib’:\thttps://github.com/dhcolumbia/pandoc-workflow/blob/master/pandoctut.bib\t500\t‘Installing\tPandoc’:\thttp://pandoc.org/installing.html\t \n 504 unobtrusive characters, and then \"compile\" the resulting text with a typesetting interpreter which will format the document consistently, according to a specified style. Markdown conventions come in several \"flavors\" designed for use in particular contexts, such as blogs, wikis, or code repositories. The flavor of Markdown used by Pandoc is geared for academic use. Its conventions are described one the Pandoc's Markdown page. One of its conventions include the \"YAML\" block,501 which contains some useful metadata. Let's now create a simple document in Markdown. Open a plain-text editor of your choice and begin typing. It should look like this: ---\t\t\ttitle:\tPlain\tText\tWorkflow\t\t\tauthor:\tDennis\tTenen,\tGrant\tWythoff\t\t\tdate:\tJanuary\t20,\t2014\t\t\t---\t\t\tPandoc-flavored Markdown stores each of the above values, and \"prints\" them in the appropriate location of your outputted document once you are ready to typeset. We will later learn to add other, more powerful fields to the YAML block. For now, let's pretend we are writing a paper that contains three sections, each subdivided into two subsections. Leave a blank line after last three dashes in the YAML block and paste the following: #\tSection\t1\t\t\t\t##\tSubsection\t1.1\t\t\tLorem\tipsum\tdolor\tsit\tamet,\tconsectetur\tadipisicing\telit,\tsed\tdo\teiusmod\ttempor\tincididunt\tut\tlabore\tet\tdolore\tmagna\taliqua.\tUt\tenim\tad\tminim\tveniam,\tquis\tnostrud\texercitation\tullamco\tlaboris\tnisi\tut\taliquip\tex\tea\tcommodo\tconsequat.\t\tNext\tparagraph\tshould\tstart\tlike\tthis.\tDo\tnot\tindent.\t\t##\tSubsection\t1.2\tSed\tut\tperspiciatis\tunde\tomnis\tiste\tnatus\terror\tsit\tvoluptatem\taccusantium\tdoloremque\tlaudantium,\ttotam\trem\taperiam,\teaque\t\tipsa\tquae\tab\tillo\tinventore\tveritatis\tet\tquasi\tarchitecto\tbeatae\tvitae\tdicta\tsunt\texplicabo.\t\t#\tSection\t2\t\t##\tSubsection\t2.1\tGo ahead and enter some dummy text as well. Empty space is meaningful in Markdown: do not indent your paragraphs. Instead, separate paragraphs by using an blank line. Blank lines must also precede section headers.                                                         501\t‘Pandoc\tUser’s\tGuide’:\thttp://pandoc.org/README.html#yaml-metadata-block\t \n 505 You can use asterisks to add bold or italicized emphasis to your words, like this: *italics* and **bold**. We should also add a link and a footnote to our text to cover the basic components of an average paper. Type: A\tsentence\tthat\tneeds\ta\tnote.[^1]\t\t\t[^1]:\tmy\tfirst\tfootnote!\tAnd\ta\t[link](https://www.eff.org/).\tWhen the text of the link and the address are the same it is faster to write <www.eff.org> instead of [www.eff.org](www.eff.org). Let's save our file before advancing any further. Create a new folder that will house this project. You are likely to have some system of organizing your documents, projects, illustrations, and bibliographies. But often, your document, its illustrations, and bibliography live in different folders, which makes them hard to track. Our goal is to create a single folder for each project, with all relevant materials included. The general rule of thumb is one project, one paper, one folder. Name your file something like main.md, where \"md\" stands for markdown. Once your file is saved, let's add an illustration. Copy an image (any small image) to your folder, and add the following somewhere in the body of the text: ![image\tcaption](your_image.jpg). At this point, your main.md should look something like the following. You can download this sample .md file: http://programminghistorian.org/assets/sample.md. ---\t\t\ttitle:\tPlain\tText\tWorkflow\t\t\tauthor:\tDennis\tTenen,\tGrant\tWythoff\t\t\tdate:\tJanuary\t20,\t2014\t\t\t---\t\t\t\t#\tSection\t1\t\t##\tSubsection\t1.1\tLorem\t*ipsum*\tdolor\tsit\tamet,\t**consectetur**\tadipisicing\telit,\tsed\tdo\teiusmod\ttempor\tincididunt\tut\tlabore\tet\tdolore\tmagna\taliqua.\tUt\tenim\tad\tminim\tveniam,\tquis\tnostrud\texercitation\tullamco\tlaboris\tnisi\tut\taliquip\tex\tea\tcommodo\tconsequat.\t\t##\tSubsection\t1.2\tSed\tut\tperspiciatis\tunde\tomnis\tiste\tnatus\terror\tsit\tvoluptatem\taccusantium\tdoloremque\tlaudantium,\ttotam\trem\taperiam,\teaque\t\tipsa\tquae\tab\tillo\tinventore\tveritatis\tet\tquasi\tarchitecto\tbeatae\tvitae\tdicta\tsunt\texplicabo.\t\tNext\tparagraph\tshould\tstart\tlike\tthis.\tDo\tnot\tindent.\t\t#\tSection\t2\t\t##\tSubsection\t2.1\t![image\tcaption](your_image.jpg)\t\t##\tSubsection\t2.2\t \n 506 A\tsentence\tthat\tneeds\ta\tnote.[^1]\t\t[^1]:\tmy\tfirst\tfootnote!\tAnd\ta\t[link](https://www.eff.org/)\tAs we shall do shortly, this plain text file can be rendered as a very nice PDF: \n Screenshot of a PDF rendered by Pandoc If you'd like to get an idea of how this kind of markup will be interpreted as HTML formatting, try an online sandbox502 and play around with various kinds of syntax. Remember that certain elements of Pandoc-flavored Markdown (such as the title block and footnotes) will not work in this web form, which only accepts the basics. At this point, you should spend some time exploring some of other features of Markdown like quotations (referenced by > symbol), bullet lists which start with * or -, verbatim line breaks which start with | (useful for poetry), tables, and a few of the other functions listed on Pandoc's markdown page. Pay particular attention to empty space and the flow of paragraphs. The documentation puts it succinctly when it defines a paragraph to be \"one or more lines of text followed by one or more blank line.\" Note that \"newlines are treated as spaces\" and that \"if you need a hard line break, put two or more spaces at the end of a line.\" The best way to understand what that means is to experiment freely. Use your editor's preview mode or just run Pandoc to see the results of your experiments.                                                         502\tAvailable\tat:\thttp://daringfireball.net/projects/markdown/dingus\t\n \n 507 Above all, avoid the urge to format. Remember that you are identifying semantic units: sections, subsections, emphasis, footnotes, and figures. Even *italics* and **bold** in Markdown are not really formatting marks, but indicate different level of emphasis. The formatting will happen later, once you know the venue and the requirements of publication. There are programs that allow you to watch a live preview of Markdown output as you edit your plain text file, which we detail below in the Useful Resources section. Few of them support footnotes, figures, and bibliographies however. To take full advantage of Pandoc, we recommend that you stick with simple, plain text files stored locally, on your computer. Getting in touch with your inner terminal Before we can start publishing our main.md file into other formats, we need to get oriented with working on the command line using your computer's terminal program, which is the only (and best) way to use Pandoc. The command line is a friendly place, once you get used to it. If you are already familiar with using the command line, feel free to skip this section. For others, it is important to understand that being able to use your terminal program directly will all you to use a broad range of powerful research tools that you couldn't use otherwise, and can serve as a basis for more advanced work. For the purposes of this tutorial, you need to learn only a few, very simple commands. First, open a command line window. If you are using a Mac, open Terminal in the 'Applications/Utilities' directory. On Windows, you'll use PowerShell. On Windows 7 or later, click Start, type \"powershell\" in \"Search programs and files,\" and hit enter. For a detailed introduction to using the command line, see Zed A. Shaw's excellent Command Line Crash Course.503 Once opened, you should see a text window and a prompt that looks something like this: computer-name:~username$. The tilde indicates your \"home\" directory, and in fact you can type $\tcd\t~ at any point to return to your home directory. Don't type the dollar sign, it just symbolizes the command prompt of your terminal, promting you to type something into your terminal (as opposed to typing it into your document); remember to hit enter after every command. It is very likely that your \"Documents\" folder is located here. Type $\tpwd (= print working directory) and press enter to display the name of the current directory). Use $\tpwd whenever you feel lost. The command $\tls (= list), which simply lists the files in the current directory. Finally, you can use $\tcd> (= change directory) like $\tcd\tDIRECTORY_NAME (where DIRECTORY_NAME is the name of the directory you'd like to navigate to). You can use $\tcd\t.. to automatically move up one                                                         503\t‘The\tCommand\tLine\tCrash\tCourse’:\thttp://cli.learncodethehardway.org/book/\t \n 508 level in the directory structure (the parent directory of the directory you are currently in). Once you start typing the directory name, use the Tab key to auto complete the text—particularly useful for long directory names, or directories names that contain spaces.  These three terminal commands: pwd, ls, and cd are all you need for this tutorial. Practice them for a few minutes to navigate your documents folder and think about they way you have organized your files. If you'd like, follow along with your regular graphical file manager to keep your bearings. Using Pandoc to convert Markdown to an MS Word document We are now ready to typeset! Open your terminal window, use $\tpwd and $\tcd to navigate to the correct folder for your project. Once you are there, type $\tls in the terminal to list the files. If you see your .md file and your images, you are in the right place. To convert .md into .docx type: $\tpandoc\t-o\tmain.docx\tmain.md\tOpen the file with MS Word to check your results. Alternatively, if you use Open or Libre Office you can run: $\tpandoc\t-o\tproject.odt\tmain.md\tIf you are new to the command line, imagine reading the above command as saying something like: \"Pandoc, create an MS Word file out of my Markdown file.\" The -o part is a \"flag,\" which in this case says something like \"instead of me explicitly telling you the source and the target file formats, just guess by looking at the file extension.\" Many options are available through such flags in Pandoc. You can see the complete list on Pandoc's website504 or by typing $\tman\tpandoc\tin the terminal. Try running the command $\tpandoc\t-o\tproject.html\tmain.md\tNow navigate back to your project directory. Can you tell what happened? More advanced users who have LaTeX installed may want to experiment by converting Markdown into .tex or specially formatted .pdf files. Once LaTeX is installed, a beautifully formatted PDF file can be created using the same command structure: $\tpandoc\t-o\tmain.pdf\tmain.md\t                                                        504\t‘Pandoc\tUser’s\tGuide’:\thttp://pandoc.org/README.html\t \n 509 With time, you will be able to fine tune the formatting of PDF documents by specifying a LaTeX style file (saved to the same directory), and running something like: $\tpandoc\t-H\tformat.sty\t-o\tproject.pdf\t--number-sections\t--toc\tproject.tex\tWorking with Bibliographies In this section, we will add a bibliography to our document and then convert from Chicago to MLA formats. If you are not using a reference manger like Endnote or Zotero, you should. We prefer Zotero, because, like Pandoc, it was created by the academic community and like other open-source projects it is released under the GNU General Public License. Most importantly for us, your reference manager must have the ability to generate bibliographies in plain text format, to keep in line with our \"everything in plain text\" principle. Go ahead and open a reference manager of your choice and add some sample entries. When you are ready, find the option to export your bibliography in BibTeX (.bib) format. Save your .bib file in your project directory, and give it a reasonable title like \"project.bib\". The general idea is to keep your sources organized under one centralized bibliographic database, while generating specific and much smaller .bib files that will live in the same directory as your project. Go ahead and open your .bib file with the plain-text editor of your choice.  Your .bib file should contain multiple entries that look something like this: @article{fyfe_digital_2011,\t\t\t\t\ttitle\t=\t{Digital\tPedagogy\tUnplugged},\t\t\t\t\tvolume\t=\t{5},\t\t\t\t\turl\t=\t{http://digitalhumanities.org/dhq/vol/5/3/000106/000106.html},\t\t\t\t\tnumber\t=\t{3},\t\t\t\t\turldate\t=\t{2013-09-28},\t\t\t\t\tauthor\t=\t{Fyfe,\tPaul},\t\t\t\t\tyear\t=\t{2011},\t\t\t\t\tfile\t=\t{fyfe_digital_pedagogy_unplugged_2011.pdf}\t}\tYou will rarely have to edit these by hand (although you can). In most cases, you will simply \"export\" the .bib file from Zotero or from a similar reference manager. Take a moment to orient yourself here. Each entry consists of a document type, \"article\" in our case, a unique identifier (fyfe_digital_2011), and the relevant meta-data on title, volume, author, and so on. The thing we care most about is the unique ID which immediately follows the curly bracket in the first line of each entry. The unique ID is what allows us to connect the bibliography with the main document. Leave this file open for now and go back to your main.md file.  \n 510 Edit the footnote in the first line of your main.md file to look something like the following examples, where @name_title_date can be replaced with one of the unique IDs from your project.bib file. A\treference\tformatted\tlike\tthis\twill\trender\tproperly\tas\tinline-\tor\tfootnote-\tstyle\tcitation\t[@name_title_date,\t67].[^7](#fn:7)  \"For\tcitations\twithin\tquotes,\tput\tthe\tcomma\toutside\tthe\tquotation\tmark\"\t[@name_title_2011,\t67]. Once we run the markdown through Pandoc, \"@fyfe_digital_2011\" will be expanded to a full citation in the style of your choice. You can use the @citation syntax in any way you see fit: in-line with your text or in the footnotes. To generate a bibliography simply include a section called #\tBibliography at the end of document. Now, go back to your metadata header at the top of your .md document, and specify the bibliography file to be used, like so: ---\ttitle:\tPlain\tText\tWorkflow\tauthor:\tDennis\tTenen,\tGrant\tWythoff\tdate:\tJanuary\t20,\t2014\tbibliography:\tproject.bib\t---\tThis tells Pandoc to look for your bibliography in the project.bib file, under the same directory as your main.md. Let's see if this works. Save your file, switch to the terminal window and run: $\tpandoc\t-S\t-o\tmain.docx\t--filter\tpandoc-citeproc\tmain.md\tThe upper case S flag stands for \"smart\", a mode which produces \"typographically correct output, converting straight quotes to curly quotes, — to em-dashes, — to en-dashes and … to ellipses.\" The \"pandoc-citeproc\" filter parses all of your citation tags. The result should be a decently formatted MS Word file. If you have LaTeX installed, convert into .pdf using the same syntax for prettier results. Do not worry if things are not exactly the way you like them—remember, you are going to fine-tune the formatting all at once and at later time, as close as possible to the time of publication. For now we are just creating drafts based on reasonable defaults. Changing citation styles The default citation style in Pandoc is Chicago author-date. We can specify a different style by using stylesheet, written in the \"Citation Style Language\" (yet another plain-text convention, in this case for describing citation styles) and denoted by the .csl file extension. Luckily, the CSL project maintains a repository of common citation styles, some even tailored for specific journals. Visit http://editor.citationstyles.org/about/ to find the .csl file for Modern Language Association, download modern- \n 511 language-association.csl, and save to your project directory as mla.csl. Now we need to tell Pandoc to use the MLA stylesheet instead of the default Chicago. We do this by updating the YAML header: ---\ttitle:\tPlain\tText\tWorkflow\tauthor:\tDennis\tTenen,\tGrant\tWythoff\tdate:\tJanuary\t20,\t2014\tbibliography:\tproject.bib\tcsl:\tmla.csl\t---\tYou then simply use the same command: $\tpandoc\t-S\t-o\tmain.docx\t--filter\tpandoc-citeproc\tmain.md\tParse the command into English as you are typing. In my head, I translate the above into something like: \"Pandoc, be smart about formatting, and output a Word Doc using the citation filter on my Markdown file (as you can guess from the extension).\" As you get more familiar with citation stylesheets, consider adding your custom-tailored .csl files for journals in your field to the archive as a service to the community. Summary You should now be able to write papers in Markdown, to create drafts in multiple formats, to add bibliographies, and to easily change citation styles. A final look at the project directory will reveal a number of \"source\" files: your main.md file, project.bib file, your mla.csl file, and some images. Besides the source files you should see some some \"target\" files that we created during the tutorial: main.docx or main.pdf. Your folder should look something like this: Pandoc-tutorial/\t\t\t\t\tmain.md\t\t\t\t\tproject.bib\t\t\t\t\tmla.csl\t\t\t\t\timage.jpg\t\t\t\t\tmain.docx\tTreat you source files as an authoritative version of your text, and you target files as disposable \"print outs\" that you can easily generate with Pandoc on the fly. All revisions should go into main.md. The main.docx file is there for final-stage clean up and formatting. For example, if the journal requires double-spaced manuscripts, you can quickly double-space in Open Office or Microsoft Word. But don't spend too much time formatting. Remember, it all gets stripped out when your manuscript goes to print. The time spent on needless formatting can be put to better use in polishing the prose of your draft.  \n 512 Useful Resources Should you run into trouble, there is no better place to start looking for support than John MacFarlane's Pandoc site505 and the affiliated mailing list.506 At least two \"Question and Answer\" type sites can field questions on ‘Pandoc: Stack Overflow’507 and ‘Digital Humanities Q&A’.508 Questions may also be asked live, on ‘Freenode IRC’, #Pandoc channel, frequented by a friendly group of regulars. As you learn more about Pandoc, you can also explore one of its most powerful features: filters.509 Although we suggest starting out with a simple editor, many (70+, according to Grace Smith’s blog post)510 other, Markdown-specific alternatives to MS Word are available online, and often free of cost. From the standalone ones, we liked ‘Mou’, ‘Write Monkey’, and ‘Sublime Text’.511 Several web-based platforms have recently emerged that provide slick, graphic interfaces for collaborative writing and version tracking using Markdown. These include: ‘prose.io’, ‘Authorea’, ‘Penflip’, ‘Draft’, and ‘StackEdit’.512 But the ecosystem is not limited to editors. ‘Gitit’ and ‘Ikiwiki’513 support authoring in Markdown with Pandoc as parser. To this list we may a range of tools that generate fast, static webpages, ‘Yst’, ‘Jekyll’, ‘Hakyll’, and bash shell script by the historian Caleb McDaniel.514 Finally, whole publishing platforms are forming around the use of Markdown. Markdown to marketplace platform ‘Leanpub’515 could be an interesting alternative to the traditional publishing model. And we ourselves are experimenting with academic journal design based on GitHub and readthedocs.org (tools usually used for technical documentation). Don't worry if you don't understand some of of this terminology yet!                                                         505\t‘About\tPandoc’:\thttp://pandoc.org/\t506\t‘pandoc-discuss’:\thttps://groups.google.com/forum/#!forum/pandoc-discuss\t507\t‘Stack\tOverflow:\tPandoc’:\thttp://stackoverflow.com/questions/tagged/pandoc\t508\t‘Digital\tHumanities\tQuestions\tand\tAnswers’:\thttp://digitalhumanities.org/answers/\t509\t‘Pandoc\tFilters’:\thttps://github.com/jgm/pandoc/wiki/Pandoc-Filters\t510\tGrace\tSmith,\t’78\tTools\tfor\tWriting\tand\tPreviewing\tMarkdown’,\tMashable\t(24\tJune\t2013):\thttp://web.archive.org/web/20140120195538/http://mashable.com/2013/06/24/markdown-tools/.\t511\t‘Mou’:\thttp://25.io/mou/;\t‘WriteMonkey’:\thttp://writemonkey.com/;\t‘Sublime\tText’:\thttp://www.sublimetext.com/.\t512\t‘Prose.io’:\thttp://prose.io/;\t‘Authorea’:\thttps://www.authorea.com/;\t‘Penflip’:\thttps://www.penflip.com/;\t‘Draft’:\thttps://draftin.com/;\t‘StackEdit’:\thttps://stackedit.io/\t513\t‘Gitit’:\thttp://gitit.net/;\t‘Ikiwiki’:\thttps://github.com/dubiousjim/pandoc-iki\t514\t‘Yst’:\thttps://github.com/jgm/yst;\t‘Jekyll’:\thttps://github.com/fauno/jekyll-pandoc-multiple-formats;\t‘Hakyll’:\thttps://jaspervdj.be/hakyll/;\t‘wcaleb\t/\twebsite’:\thttps://github.com/wcaleb/website\t515\t‘Leanpub’:\thttps://leanpub.com/\t \n 513 The source files for this document can be downloaded from GitHub.516 Use the \"raw\" option when viewing in GitHub to see the source Markdown. The authors would like to thank Alex Gil and his colleagues from Columbia's Digital Humanities Center, and the participants of openLab at the Studio in the Butler library for testing the code in this tutorial on a variety of platforms.  See Charlie Stross's excellent discussion of this topic in ‘Why Microsoft Word Must Die’.517  Note that the .bib extension may be \"registered\" to Zotero in your operating system. That means when you click on a .bib file it is likely that Zotero will be called to open it, whereas we want to open it within a text editor. Eventually, you may want to associate the .bib extension with your text editor.  There are no good solutions for directly arriving at MS Word from LaTeX.  It is a good idea to get into the habit of not using spaces in folder or file names. Dashes-or_underscores instead of spaces in your filenames ensure lasting cross-platform compatibility.  Thanks to @nickbart80 (https://github.com/nickbart1980) for the correction. In response to our original suggestion, Some\tsentence\tthat\tneeds\tcitation.^[@fyfe_digital_2011\targues\tthat\ttoo.] he writes: \"This is not recommended since it keeps you from switching easily between footnote and author-date styles. Better use the [corrected] (no circumflex, no final period inside the square braces, and the final punctuation of the text sentence after the square braces; with footnote styles, pandoc automatically adjusts the position of the final punctuation).\"518  About the Authors Dennis Tenen is an assistant professor of English and Comparative Literature at Columbia University. Grant Wythoff is a lecturer in the Department of English and Comparative Literature at Columbia University. \n                                                        516\t‘dhcolumbia\t/\tpandoc-workflow’:\thttps://github.com/dhcolumbia/pandoc-workflow\t517\tCharlie\tStross,\t‘Why\tMicrosoft\tWord\tmust\tDie’,\tCharlie’s\tDiary\t(12\tOctober\t2013):\thttp://www.antipope.org/charlie/blog-static/2013/10/why-microsoft-word-must-die.html\t518\thttps://github.com/programminghistorian/jekyll/issues/46#issuecomment-59219906\t \n 514 Epilogue This book represents a snapshot of the Programming Historian project as it was in early February 2016. The very process of putting it together has given the editorial team a chance to check-over lessons to find problems that have inevitably crept into the site over the years. Links break, technologies become unsupported, screenshots cease to be representative. It’s also given us a chance to reflect on ways that we can improve the learning experience for our readers. We’re constantly changing, always trying to find better ways to be the go-to place for digital history pedagogy.  With that in mind, we’re always looking to hear from potential contributors. That might be authors, reviewers, or people who want to chip in and help us make The Programming Historian a little bit better. We’re building this for all of us, and we’d encourage you to look us up and lend a hand.  http://programminghistorian.org     February 2016 London  Adam Crymble, on behalf of the Editorial Board  \n 515  Editorial Board   • Adam Crymble is a lecturer of digital history at the University of Hertfordshire • Fred Gibbs is an assistant professor of history at the University of New Mexico • Allison Hegel is a PhD candidate at UCLA • Caleb McDaniel is an associate professor of history at Rice University • Ian Milligan is an assistant professor of history at the University of Waterloo • Miriam Posner is the digital humanities program coordinator at UCLA • Evan Taparata is a PhD candidate in history at the University of Minnesota"
    },
    {
        "title": "An Emotion Model for Music Using Brain Waves.",
        "author": [
            "Rafael Cabredo",
            "Roberto Sebastian Legaspi",
            "Paul Salvador Inventado",
            "Masayuki Numao"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416398",
        "url": "https://doi.org/10.5281/zenodo.1416398",
        "ee": "https://zenodo.org/records/1416398/files/CabredoLIN12.pdf",
        "abstract": "Every person reacts differently to music. The task then is to identify a specific set of music features that have a signifi- cant effect on emotion for an individual. Previous research have used self-reported emotions or tags to annotate short segments of music using discrete labels. Our approach uses an electroencephalograph to record the subject’s reac- tion to music. Emotion spectrum analysis method is used to analyse the electric potentials and provide continuous- valued annotations of four emotional states for different segments of the music. Music features are obtained by pro- cessing music information from the MIDI files which are separated into several segments using a windowing tech- nique. The music features extracted are used in two sepa- rate supervised classification algorithms to build the emo- tion models. Classifiers have a minimum error rate of 5% predicting the emotion labels.",
        "zenodo_id": 1416398,
        "dblp_key": "conf/ismir/CabredoLIN12",
        "keywords": [
            "music",
            "emotion",
            "electroencephalograph",
            "emotion spectrum analysis",
            "music features",
            "supervised classification",
            "emotion models",
            "error rate",
            "emotion labels",
            "music information"
        ],
        "content": "AN EMOTION MODEL FOR MUSIC USING BRAIN WA VES\nRafael Cabredo1,2, Roberto Legaspi1, Paul Salvador Inventado1,2, and Masayuki Numao1\n1Institute of Scientiﬁc and Industrial Research, Osaka University, Japan,\n2Center for Empathic Human-Computer Interactions, De La Salle University, Philippines,\nfcabredo,roberto,inventado,numaog@ai.sanken.osaka-u.ac.jp\nABSTRACT\nEvery person reacts differently to music. The task then is to\nidentify a speciﬁc set of music features that have a signiﬁ-\ncant effect on emotion for an individual. Previous research\nhave used self-reported emotions or tags to annotate short\nsegments of music using discrete labels. Our approach\nuses an electroencephalograph to record the subject’s reac-\ntion to music. Emotion spectrum analysis method is used\nto analyse the electric potentials and provide continuous-\nvalued annotations of four emotional states for different\nsegments of the music. Music features are obtained by pro-\ncessing music information from the MIDI ﬁles which are\nseparated into several segments using a windowing tech-\nnique. The music features extracted are used in two sepa-\nrate supervised classiﬁcation algorithms to build the emo-\ntion models. Classiﬁers have a minimum error rate of 5%\npredicting the emotion labels.\n1. INTRODUCTION\nListening to music brings out different kinds of emotions.\nIt can be involuntary and different for every person and\nprimarily caused by musical content. A lot of research has\nbeen done identifying music features that are associated\nwith affecting emotion or mood [3, 5, 17]. The work of [9]\nalso investigates music features and discusses how chang-\ning these features can affect the emotions the music elicits.\nWith a good background of how different music fea-\ntures affect emotions, it is possible to automatically clas-\nsify and predict what kind of emotions a person will ex-\nperience. A survey of music emotion research by Kim et\nal. [6] report that the typical approach for classifying mu-\nsic using emotion is to build a database of ground truth of\nemotion labels by subjective tests. Afterwards, a machine\nlearning technique is used to train a classiﬁer to automati-\ncally recognize high-level or low-level music features.\nA common problem encountered by previous work is\nthe limitation of the annotation for emotion. It takes a lot\nof time and resources to annotate music. Lin, et al. [8] re-\nviews various work on music emotion classiﬁcation and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.utilize the vast amount of online social tags to improve\nemotion classiﬁcation. However, a personalized emotion\nmodel for labelling music would still be desirable. Music\nthat is relaxing for some people may be stressful for others.\nSongs are also usually annotated with the most promi-\nnent emotion (i.e. only one emotion label per song). Multi-\nlabel classiﬁcation [18] can be used to have richer emotion\nannotations. These annotations however are still discrete-\nvalued emotion labels.\nIn our work, we are interested in learning how emo-\ntion changes throughout the song and identify music fea-\ntures that could have caused these changes. Because of\nthis, continuous-valued emotion annotations are preferred.\nOne method to do this is to use an electroencephalograph\n(EEG) in recognizing emotions similar to the work used\nto develop Constructive Adaptive User Interface (CAUI),\nwhich can arrange [7, 13] and compose [14] music based\non one’s impressions of music. In addition to collecting\ncontinuous-valued annotations for full-length music, we\nfocus our work on considering individual emotion reac-\ntions to music as opposed to building a generalized emo-\ntion model.\n2. DATA COLLECTION METHODOLOGY\nWe construct a user speciﬁc model by using supervised\nmachine learning techniques to classify songs using music\nfeatures. As mentioned earlier, this task requires songs that\ncan elicit emotions from a listener and the music features\nof these songs.\nFor this research, we had a 29-year old female partic-\nipant who selected and annotated songs. The music col-\nlection is a set of MIDI ﬁles comprised of 121 Japanese\nand Western songs having 33 Folk, 20 Jazz, 44 Pop, and\n24 Rock music. By using MIDI ﬁles, the music informa-\ntion can be easily extracted to produce high-level features\nfor the classiﬁer. MIDI ﬁles also eliminate any additional\nemotions contributed by lyrics.\n2.1 Emotion annotation\nMusic emotion annotation is performed in 3 stages. First,\nthe subject listened to all songs and manually annotated\neach one. The subject was instructed to listen to the entire\nsong and was given full control on which parts of the song\nshe wanted to listen to.\nAfter listening to each song, the subject gives a general\nimpression on how joyful, sad, relaxing, and stressful eachFigure 1. The EEG has 23 electrodes used to record elec-\ntrical changes on the scalp. Each node is identiﬁed by a\nletter to indicate lobe position: F-Frontal lobe, T-Temporal\nlobe, C-Central lobe, P-Parietal lobe, O-Occipital lobe. ’Z’\nrefers to an electrode placed on the mid-line\nsong was using a ﬁve-point Likert scale. Aside from the\nemotions felt, the subject was also asked to rate whether\nshe was familiar with the song or not using the same scale.\nWith this feedback, we chose the 10 most relaxing songs\nand 10 most stressful songs with varying levels of familiar-\nity to the subject. The manual annotation was done in one\nsession for approximately one and a half hours.\nSince collection of the emotion annotations takes a lot\nof time and effort from the subject, it was decided to con-\ncentrate time and resources on a certain type of emotion.\nWe opted to concentrate on relaxing music because these\nare normally the kind of music people would want to listen\nto on stressful days. The stressful songs are meant to serve\nas negative examples for the classiﬁer.\nIn the second stage an EEG was used to measure brain\nactivity while the subject listened to the 20 songs previ-\nously selected. The EEG device is a helmet with electrodes\nthat can be placed on all scalp positions according to the In-\nternational 10–20 Standard. Figure 1 shows the location of\nthe different electrodes. Using the EEG, electric potential\ndifferences were recorded with a reference electrode on the\nright earlobe.\nWork on EEG to recognize emotions ﬁnd that different\nmental state produces a distinct pattern of electrical activ-\nity [1, 2]. The right hemisphere is responsible for negative\nemotions (i.e. stress, disgust, sadness) while the left hemi-\nsphere is responsible for positive emotions (i.e. happiness,\ngratitude, amusement).\nThe EEG device is very sensitive. As such, the subject\nwas instructed to close her eyes and remain still while data\nwas being collected. Listening sessions had to be limited\nto a maximum of 30 minutes or upto the moment that the\nsubject begins to feel uncomfortable wearing the helmet.\nWe had to ensure that the subject was comfortable and\neliminate external factors that may contribute to changes\nin emotion. On average, EEG readings for 7 songs were\nrecorded per session.\nPrior to playing each music, we introduce a 10 second\nwhite noise to help the subject focus on the task at hand\nwithout stimulating a strong emotional response. After lis-\ntening to one song, a short interview is conducted to de-termine if the subject particularly liked or disliked speciﬁc\nparts of the song. The interview also helped conﬁrm the\ninitial manual annotations of the subject.\nIn the ﬁnal stage, continuous emotion annotations were\nobtained using EMonSys. This software1uses the emo-\ntion spectrum analysis method (ESAM) [12] to convert\nbrain wave readings to emotion readings. Using data from\n10 scalp positions at Fp1, Fp2, F3, F4, T3, T4, P3, P4, O1,\nO2, electric potentials were separated into their \u0012(5–8 Hz),\n\u000b(8–13 Hz) and \f(13–20 Hz) frequency components by\nmeans of fast Fourier transforms (FFT). Cross-correlation\ncoefﬁcients for each pair of channels are computed (i.e.,\n10 channels * 9 channels/2) and these are evaluated for ev-\nery time step together with the 3 bands to obtain an input\nvectorYhaving 135 variables at each time step. EMonSys\ncan evaluate the EEG readings at different time steps. We\nused the smallest available: 0.64 seconds.\nUsing an emotion matrix C, this 135-dimensional vec-\ntor is linearly transformed into a 4-D emotion vector E=\n(e1;e2;e3;e4), whereeicorresponds to the 4 emotional\nstates, namely: stress, joy, sadness, and relaxation. For-\nmally, the emotion vector is obtained by\nC\u0001Y+d=E; (1)\nwheredis a constant vector. The emotion vector is used\nto provide a continuous annotation to the music every 0.64\nseconds. For example, if one feels joy, the emotion vector\nwould have a value of E= (0;e2;0;0).\n2.2 Extracting Music Features\nA song having length mis split into several segments us-\ning a sliding window technique. Each segment, or now re-\nferred to as a window whas a length n, where one unit of\nlength corresponds to one sample of emotion annotation.\nMIDI information for each window is read using a mod-\nule adapted from jSymbolic [10] to extract 109 high-level\nmusic features. These features can be loosely grouped into\nthe following categories: Instrumentation, Texture, Dy-\nnamics, Rhythm, Pitch Statistics, and Melody. The fea-\nture set includes one-dimensional and multi-dimensional\nfeatures. For example, Amount of Arpeggiation is a one-\ndimensional Melody feature, Beat Histogram is a\n161-dimensional Rhythm feature, etc. All features avail-\nable in jSymbolic were used to build a 1023-dimension\nfeature vector. The category distribution of the feature vec-\ntor is shown in Table 1. The Others category refers to\nthe features Duration andMusic Position. Duration is a\nfeature from jSymbolic, which describes the length of the\nsong in seconds. Music Position refers to the position of\nthe window relative to duration of the song. Although it\nwas known that not all of the features will be used, this ap-\nproach allows utilization of feature selection techniques to\ndetermine which features were the most important in clas-\nsiﬁcation.\nAfter extracting the features for one window, the win-\ndow goes through the data using a step size suntil the end\n1software developed by Brain Functions Laboratory, Inc.Category Amount Percentage\nDynamics 4 0.39%\nInstrumentation 493 48.19%\nMelody 145 14.17%\nPitch 174 17.01%\nRhythm 191 18.67%\nTexture 14 1.37%\nOthers 2 0.20%\nTable 1. Distribution of features used for the instances\nof the\nsong is reached. Each window was labelled using\nthe average emotion values within the length of the win-\ndow. Formally, the label for wiis the emotion vector\nEi=1\nni+nX\nj=iEj=1\nni+nX\nj=i\u0010\nej\n1;ej\n2;ej\n3;ej\n4\u0011\n, (2)\nwhere 1\u0014j\u0014m\u0000n.\n3. EMOTION MODEL\nWeka’s [4] implementation of linear regression and C4.5\nwere used to build the emotion models for each emotion.\nThe training examples were derived from the window given\none emotion label, which results to four datasets. Each\ndataset has a maximum of 6156 instances using the small-\nest values for the sliding window (i.e. n= 1 ands= 1).\nThe number of instances depends on the parameters used\nfor windowing. During preliminary experiments we ob-\nserved that the decrease of training data due to larger step\nsizes had too much of a negative inﬂuence on performance.\nAs such, all features were extracted using the smallest size\nofs= 1for all experiments.\nPrior to training, all features that do not change at all\nor vary too frequently (i.e. varies 99% of the time) are\nremoved. Afterwards, normalization is performed to have\nall feature values within [0;1].\n3.1 Using Linear Regression\nThe linear regression used for building the emotion mod-\nels uses the Akaike criterion for model selection and M5\nmethod [15] to select features. The M5 method steps\nthrough the features and removes features with the smallest\nstandardized coefﬁcient until no improvement is observed\nin the estimate of the error given by the Akaike information\ncriterion.\n3.2 Using C4.5\nC4.5 [16] is a learning technique that builds a decision\ntree from the set of training data using the concept of in-\nformation entropy. Since this technique requires nominal\nclass values, the emotion labels are ﬁrst discretized into\nﬁve bins. Initial work used larger bin sizes but we observed\npoorer performance using these.\n3.3 Testing and Evaluation\nWe used 10-fold cross-validation to assess the models gen-\nerated by the two methods using different values for the\nFigure 2. Relative absolute error using linear regression\nFigure 3. Relative absolute error using C4.5\nwindow length. We use the relative absolute error for eval-\nuating performance of the classiﬁers. Weka computes this\nerror measure by normalizing with respect to the perfor-\nmance obtained by predicting the classes’ prior probabili-\nties as estimated from the training data with a simple\nLaplace estimator. Figures 2 and 3 show the change in\nrelative absolute error using linear regression and C4.5, re-\nspectively. Window length values were varied from 1 to 30\nsamples (i.e. 0.64 seconds to 19.2 seconds of music).\n4. RESULTS AND ANALYSIS\nIncreasing the window size increases accuracy of the clas-\nsiﬁers. Further experiments were done to include window\nsizes upto 240 samples. Results of these are shown in Fig-\nures 4 and 5. From these results, we ﬁnd the value of n\nwhich minimizes the average relative absolute error over\nn= [1::20]. For linear regression, using n= 90 gives\nthe minimum average relative absolute error of 7.6% with\na correlation coefﬁcient of 0.8532 and root mean squared\nerror of 0.1233. The average is taken from values for the\nfour emotion model results.\nUsing C4.5, a smaller window length is necessary to\nobtain similar results. Using n= 60, the average relative\nabsolute error is 5.1%, average root mean squared error is\n0.0871, and average Kappa statistic is 0.9530. The Kappa\nstatistic describes the chance-corrected measure of agree-\nment between the classiﬁcations and the true classes.\nWhenn\u0015120, we notice that some songs are no longer\nincluded in the training data as the window length becomes\ngreater than the song length. As such, results using these\nwindow lengths may not be accurate.n= 1 n= 30 n= 60 n= 90 n= 120\nClass No. S R S R S R S R S R\n1 84.0% 95.3% 56.5% 82.2% 52.3% 80.5% 51.0% 81.5% 49.1% 80.5%\n2 13.3% 3.8% 31.6% 9.9% 28.6% 6.0% 26.1% 3.7% 25.7% 3.2%\n3 1.9% 0.7% 8.7% 6.5% 15.4% 10.3% 18.4% 11.1% 20.7% 9.4%\n4 0.5% 0.2% 1.8% 1.0% 1.8% 2.2% 2.3% 2.7% 1.6% 5.7%\n5 0.3% 0.0% 1.4% 0.4% 1.9% 1.0% 2.1% 1.1% 2.9% 1.1%\nTable 2. Class sizes for Stress (S) and Relaxation (R) data after discretization\nFigure 4. Relative absolute error using linear regression\nFigure 5. Relative absolute error using C4.5\n4.1 Inﬂuence of window length\nModel accuracy is highly dependent on the parameters of\nthe windowing technique. Increasing the window length\nallows more music information to be included in the in-\nstances making each more distinguishable from instances\nof other classes.\nIncreasing the window length also affects the emotion\nannotations. ESAM was conﬁgured to produce emotion\nvectors having positive values. Since most of the emotion\nvalues are near zero, the average emotion values for the\nwindows are also low. Figure 6 shows the steady increase\nof the values for the class labels as the window length is in-\ncreased. The standard deviation also follows a linear trend\nand steadily increases from 0.091 to 0.272 for the same\nwindow lengths. Using larger window lengths diversiﬁes\nthe emotion labels as well which, in turn, contributes to\nbetter accuracy.\nThe low average values also affected the discretization\nof the emotion labels for C4.5. It resulted to having a ma-\njority class. Table 2 shows that class 1 is consistently the\nmajority class for the data set. With a small window length,\nmore instances are labelled with emotion value close to 0.\nWe note, however that as window length is increased, the\nnumber of classes steadily balances out. For example, at\nFigure 6. Average of emotion value for different window\nlengths\nCategory Stress Relaxation Sadness Joy\nRhythm 40.4% 32.4% 32.8% 34.0%\nPitch 21.3% 29.7% 28.4% 32.0%\nMelody 10.6% 16.2% 19.4% 20.0%\nInstrumentation\n17.0% 10.8% 10.4% 8.0%\nTexture 8.5% 5.4% 4.5% 2.0%\nDynamics 0.0% 2.7% 1.5% 0.0%\nOthers 2.1% 2.7% 3.0% 4.0%\nTable 3. Distribution of features used in C4.5\nn= 1,\n84% of the data is labelled as class 1, but when\nn= 90, it is only 51%. This is the general trend for all\nthe emotion models. At n= 90, the instances labelled as\nclass 1 for the other emotion labels are as follows: 62.2%\nfor Joy, 78.8% for Sadness, and 81.5% for Relaxation.\n4.2 Important features used in C4.5\nC4.5 builds a decision tree by ﬁnding features in the data\nthat most effectively splits the data into subsets enriched in\none class or the other. This causes a side effect of identify-\ning music features that are most beneﬁcial for classifying\nemotions.\nTable 3 summarizes the features included in the trees\ngenerated by the algorithm using n= 60. The items are\nordered according to the number of features present in the\ndecision trees. A big portion of the features included are\nrhythmic features averaging 34.9% of the feature set. Fea-\ntures related to instrumentation also play a big part in iden-\ntifying Stress unlike the other emotions. On the other hand,\nmelody features are more important for Relaxation, Stress\nand Joy.\nA closer inspection of the decision tree reveals that each\nemotion can be classiﬁed faster using a different ordering\nof music features. Table 4 shows the distribution of fea-\ntures found in the ﬁrst 5 levels of the different decisionCategory Stress Relaxation Sadness Joy\nRhythm 23.4% 13.5% 6.0% 14.0%\nPitch 0.0% 10.8% 9.0% 10.0%\nMelody 4.3% 2.7% 1.5% 6.0%\nInstrumentation\n4.3% 2.7% 4.5% 4.0%\nTexture 0.0% 0.0% 0.0% 0.0%\nDynamics 2.1% 2.7% 1.5% 0.0%\nOthers 0.0% 2.7% 0.0% 0.0%\nTable 4. Distribution of features found in the ﬁrst 5\nlevels\nof the decision trees of C4.5\ntrees. The Stress model mostly uses rhythmic features and\n2 melodic features for the ﬁrst 4 levels and uses Instru-\nmentation for the 5th level. During the interview with the\nsubject, when asked which parts of the songs are stressful,\nshe explains that songs with electric guitar and rock songs\nin general are very stressful for her. Rock songs used in\nthe dataset had a fast tempo and may be a factor as to the\nconstruction of the decision tree.\nFor relaxing music, the subject mentioned that there\nare speciﬁc parts of the songs that made her feel relaxed.\nThese include introductory parts, transitions between cho-\nrus and verses, piano and harp instrumentals, and climactic\nparts of the song (i.e. last verse-chorus or bridge). Exam-\nining the decision tree for relaxation, we ﬁnd that Melodic\nInterval Histogram, Basic Pitch Histogram, and Music Po-\nsition are used for the ﬁrst 3 levels, which are features that\nsupport the statements of the subject. Although emotion\nmodels for Joy and Sadness are available, a complete anal-\nysis of these cannot be done since the dataset was primarily\nfocused on relaxing and stressful music.\n4.3 Accuracy of Emotion labels\nThe manual emotion labels were also compared to the emo-\ntion values from ESAM. The average emotion value for\neach song was calculated and transformed into a 5-point\nscale. Comparing the manual annotations with the dis-\ncretized continuous annotations, we ﬁnd that only 25%\nof the emotion labels from EEG were the same with the\nmanual annotations, 62% of the emotion labels from EEG\nslightly differed from the manual annotations, and 13%\nwere completely opposite from what was originally\nreported. It is difﬁcult to attribute error for the discrep-\nancy. One possible cause could be the methodology for\nmanual annotations. While the subject was doing the man-\nual annotations, we observed that usually, she would only\nlisten to the ﬁrst 30 seconds of the song and in some cases\nskip to the middle of the song. It is possible that the man-\nual annotation incompletely represents the emotion of the\nentire song.\nIt is also possible that the subject experienced a differ-\nent kind of emotion unconsciously while listening to the\nmusic. For example some songs that were reported to be\nstressful turned out not stressful at all. We examined the\nemotion annotations and checked if there was any depen-\ndency between the values.\nIn Table 5 we can see that the subject treated the emo-\ntion Stress to be the bipolar opposite of Relaxation due toJoy Sadness Relaxation Stress\nSadness -0.5638\nRelaxation 0.5870 0.0733\nStress -0.6221 -0.0555 -0.9791\nFamiliarity 0.7190 -0.2501 0.5644 -0.6252\nTable 5. Correlation of manual annotations\nJoy Sadness Relaxation Stress\nSadness -0.1187\nRelaxation 0.4598 -0.2338\nStress -0.4450 0.3100 -0.4223\nFamiliarity -0.0579 0.2956 -0.2343 0.5731\nTable 6. Correlation of annotations using ESAM\nthe high negati\nve correlation value. Using ESAM, we ﬁnd\na similar situation but there is only a moderate negative\ncorrelation between the two as shown in Table 6. If we\nexamine the other emotions, we ﬁnd that Joy has a correla-\ntion with Relaxation and a negative correlation with Stress.\nThis is consistently reported for both manual annotations\nand annotations using ESAM.\nFinally, we compared the amount of discrepancy be-\ntween manual and automated annotations against the sub-\nject’s familiarity with the song. We found that the discrep-\nancy values for joyful and relaxing songs have a high corre-\nlation with familiarity : 0.6061 for Joy and 0.69551 for Re-\nlaxation. This implies that measurements of ESAM for Joy\nand Relaxation become more accurate when the subject is\nnot familiar with the songs. It is possible that unfamiliar\nsongs will help induce stronger emotions as compared to\nfamiliar music. This may be an important factor when us-\ning psychophysiological devices in measuring emotion.\n5. CONCLUSION\nThis research focuses on building an emotion model for\nrelaxing and stressful music. The model was built by ex-\ntracting high-level music features from MIDI ﬁles using\na windowing technique. The features were labelled using\nemotion values generated using EEG and ESAM. These\nvalues were also compared against manual emotion anno-\ntations. With the help of interviews conducted with the\nsubject, we observe that EEG and ESAM can be used for\nannotating emotion in music especially when the subject\nexperiences a strong intensity of that emotion. Familiarity\nof the subject with the song can affect genuine emotions.\nLinear regression and C4.5 were used to build the differ-\nent emotion models. Using a 10-fold cross-validation for\nevaluating the models, high accuracy with low relative ab-\nsolute errors was obtained by using large window lengths\nencompassing between 38.4 seconds (n = 60) to 57.6 sec-\nonds (n = 90) of music.\n6. FUTURE WORK\nThe current work involves one subject and it would be in-\nteresting to see if the model can be generalized using moresubjects or, at the least, to verify if the current methodology\nwill yield similar results when used with another subject.\nInstead of using the average value for the emotion la-\nbel, we intend to explore other metrics to summarize the\nemotion values for each window.\nFurther study on the music features is also needed. The\ncurrent model uses both one-dimensional and multidimen-\nsional features. Experiments using only one set of the fea-\ntures will be performed. We also wish to explore the ac-\ncuracy of the classiﬁcation if low-level features were used\ninstead of high-level features.\nThe window length greatly affects model accuracy. We\nhave yet to investigate if there is a relationship between the\naverage tempo of the song with window length. We hy-\npothesize that slower songs would require longer window\nlengths to capture the same amount of information needed\nfor fast songs. On the other hand, songs with fast tempo\nwould need shorter window lengths.\nFinally, this model will be integrated to a music recom-\nmendation system that can recommend songs which can\ninduce similar emotions to the songs the user is currently\nlistening to.\n7. ACKNOWLEDGEMENTS\nThis research is supported in part by the Management\nExpenses Grants for National Universities Corporations\nthrough the Ministry of Education, Culture, Sports, Sci-\nence and Technology (MEXT) of Japan, by the Global\nCOE (Centers of Excellence) Program of MEXT, and by\nKAKENHI 23300059.\n8. REFERENCES\n[1] K. Ansari-Asl, G. Chanel, T. Pun, “A channel se-\nlection method for EEG classiﬁcation in emotion as-\nsessment based on synchronization likelihood,” EU-\nSIPCO 2007,15th Eur. Signal Proc. Conf., p. 1241–\n1245, 2007.\n[2] G. Chanel, J. Kronegg, D. Grandjean, T. Pun: “Emo-\ntion assessment: Arousal evaluation using EEGs and\nperipheral physiological signals, Lecture Notes in\nComputer Science, V ol. 4105, p. 530, 2006.\n[3] A. Gabrielsson, P.N. Juslin,: “Emotional expression\nin music.” In R. J. Davidson, K. R. Scherer, and H.\nH. Goldsmith, editors, Handbook of affective sciences,\nNew York: Oxford University Press, pp. 503-534,\n2003.\n[4] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-\nmann, I.H. Witten: “The WEKA Data Mining Soft-\nware: An Update, SIGKDD Explorations,” V ol. 11,\nNo. 1, pp. 10–18, 2009.\n[5] P.N. Juslin, J.A. Sloboda: “Handbook of music and\nemotion: theory, research, applications,” Oxford Uni-\nversity Press, 2010.[6] Y .E. Kim, E.M. Schmidt, R. Migneco, B.G. Morton,\nP. Richardson, J. Scott, J.A. Speck, D. Turnbull: “Mu-\nsic Emotion Recognition: A State of the Art Review,”\nProc. of the 11th ISMIR Conf., pp. 255–266, 2010.\n[7] R. Legaspi, Y . Hashimoto, K. Moriyama, S. Kurihara,\nM. Numao: “Music Compositional Intelligence with\nan Affective Flavor,” Proc. of the 12th International\nConference on Intelligent User Interfaces, pp. 216–\n224, 2007.\n[8] Y .-C. Lin, Y .-H. Yang, and H. H. Chen: “Exploiting on-\nline music tags for music emotion classiﬁcation, ACM\nTransactions on Multimedia Computing, Communica-\ntions, and Applications, V ol. 7S, No. 1, pp. 1–16, 2011.\n[9] S.R. Livingstone, R. Muhlberger, A.R. Brown, and\nW.F. Thompson: “Changing musical emotion: A com-\nputational rule system for modifying score and per-\nformance,” Computer Music Journal, V ol. 34, No. 1,\npp. 41–64, 2010.\n[10] C. McKay, and I. Fujinaga: “jSymbolic: A feature ex-\ntractor for MIDI ﬁles,” Proc. of the International Com-\nputer Music Conference, pp. 302–305, 2006.\n[11] E.R. Miranda, and A. Brouse: “Toward direct brain-\ncomputer musical interfaces,” New Interfaces for Mu-\nsical Expression, 2005.\n[12] T. Musha, Y . Terasaki, H.A. Haque, and G.A. Ivanit-\nsky: “Feature extraction from EEGs associated with\nemotions,” Journal of Artiﬁcial Life and Robotics,\nV ol. 1, No. 1, pp. 15–19,1997.\n[13] M. Numao, M. Kobayashi, and K. Sakaniwa: “Aquisi-\ntion of human feelings in music arrangement,” Proc. of\nIJCAI ’97, pp. 268–273, 1997.\n[14] M. Numao, S. Takagi, and K. Nakamura: Construc-\ntive adaptive user interfaces - Composing music based\non human feelings,” Proc. of AAAI ’02, pp. 193–198,\n2002.\n[15] J.R. Quinlan: “Learning with continuous classes, Proc.\nAI92, 5th Australian Joint Conference on Artiﬁcial In-\ntelligence, Adams & Sterling (eds.), World Scientiﬁc,\nSingapore, pp. 343–348, 1992.\n[16] J.R. Quinlan: “C4.5: Programs for Machine Learning,”\nMorgan Kaufmann Publishers, 1993.\n[17] E. Schubert: “Affective, Evaluative, and Collative Re-\nsponses to Hated and Loved Music,” Psychology of\nAesthetics Creativity and the Arts, V ol. 4, No. 1,\npp. 36–46, 2010.\n[18] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-\nhavas: “Multilabel classiﬁcation of music into emo-\ntions, in Proc. of the 9th International Conference on\nMusic Information Retrieval, pp. 325–330, 2008."
    },
    {
        "title": "Chord Recognition Using Duration-explicit Hidden Markov Models.",
        "author": [
            "Ruofeng Chen",
            "Weibin Shen",
            "Ajay Srinivasamurthy",
            "Parag Chordia"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417077",
        "url": "https://doi.org/10.5281/zenodo.1417077",
        "ee": "https://zenodo.org/records/1417077/files/ChenSSC12.pdf",
        "abstract": "We present an audio chord recognition system based on a generalization of the Hidden Markov Model (HMM) in which the duration of chords is explicitly considered - a type of HMM referred to as a hidden semi-Markov model, or duration-explicit HMM (DHMM). We find that such a system recognizes chords at a level consistent with the state-of-the-art systems – 84.23% on Uspop dataset at the major/minor level. The duration distribution is estimated from chord duration histograms on the training data. It is found that the state-of-the-art recognition result can be im- proved upon by using several duration distributions, which are found automatically by clustering song-level duration histograms. The paper further describes experiments which shed light on the extent to which context information, in the sense of transition matrices, is useful for the audio chord recognition task. We present evidence that the con- text provides surprisingly little improvement in performance, compared to isolated frame-wise recognition with simple smoothing. We discuss possible reasons for this, such as the inherent entropy of chord sequences in our training database.",
        "zenodo_id": 1417077,
        "dblp_key": "conf/ismir/ChenSSC12",
        "keywords": [
            "audio chord recognition system",
            "generalization of HMM",
            "duration of chords",
            "hidden semi-Markov model",
            "DHMM",
            "state-of-the-art systems",
            "Uspop dataset",
            "clustering song-level duration histograms",
            "context information",
            "transition matrices"
        ],
        "content": "CHORD RECOGNITION USING DURATION-EXPLICIT\nHIDDEN MARKOV MODELS\nRuofeng Chen Weibin Shen Ajay Srinivasamurthy\nGeorgia Tech Center for Music Technology\nfruofengchen, weibin shen, ajaysg@gatech.eduParag Chordia\nSmule Inc.\nparag@smule.com\nABSTRACT\nWe present an audio chord recognition system based on\na generalization of the Hidden Markov Model (HMM) in\nwhich the duration of chords is explicitly considered - a\ntype of HMM referred to as a hidden semi-Markov model,\nor duration-explicit HMM (DHMM). We ﬁnd that such\na system recognizes chords at a level consistent with the\nstate-of-the-art systems – 84.23% on Uspop dataset at the\nmajor/minor level. The duration distribution is estimated\nfrom chord duration histograms on the training data. It is\nfound that the state-of-the-art recognition result can be im-\nproved upon by using several duration distributions, which\nare found automatically by clustering song-level duration\nhistograms. The paper further describes experiments which\nshed light on the extent to which context information, in\nthe sense of transition matrices, is useful for the audio\nchord recognition task. We present evidence that the con-\ntext provides surprisingly little improvement in performance,\ncompared to isolated frame-wise recognition with simple\nsmoothing. We discuss possible reasons for this, such as\nthe inherent entropy of chord sequences in our training\ndatabase.\n1. INTRODUCTION AND BACKGROUND\nThe problem of audio chord recognition has been explored\nfor over a decade and thus there exists an established ba-\nsic framework that is widely used. First, a chroma feature,\nor its variation is computed, followed by classiﬁers or se-\nquential decoders to recognize the chord sequence. Certain\nenhancements to the basic feature computation and classi-\nﬁcation algorithms have been found to be useful.\nThe chromagram, a sequence of 12-dimensional vectors\nthat attempts to represent the strength of each pitch class,\nis computed using a log-frequency spectrogram, estimated\nwith a constant-Q transform (CQT). Several methods have\nbeen proposed to reﬁne the basic features. Non-Negative\nLeast Square estimation [1] and Harmonic Product Spec-\ntrum [2] reduce the power of non-octave overtones in the\nspectrum. Harmonic-Percussive Source Separation [3] de-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.creases the power of percussive sounds, which typically do\nnot contain chord information [4], [5]. The CRP chroma\nalgorithm [6] attempts to produce a timbre-invariant fea-\nture and has been applied in [5] and [7]. Background Sub-\ntraction [1] removes the running mean of a given spec-\ntrum, which is based on the same principle as the CRP\nchroma algorithm in that they both conduct long-pass ﬁl-\ntering on audio signal, damping timbral information in fea-\ntures. Loudness-based chroma performs A-weighting [5]\non log-frequency spectrogram. Besides chroma, a 6-di-\nmensional feature called “tonnetz”, based on Neo-Rieman-\nnian theory, is also commonly used and has proven to be\nhelpful [8], [9]. Finally, a machine-learned transformation\nmatrix that converts log-frequency spectrogram to chroma-\ngram is shown to outperform an arbitrary transformation\nmatrix in [10].\nStructural information has also been utilized to help au-\ndio chord recognition. Many systems use beat-synchronous\nchromagrams that are computed over a beat or half-beat,\nrather than short frames [1], [9], [11], [12]. In [7], the au-\nthors smoothed the chromagram based on a regressive plot.\nIn [11], the authors demonstrate that an attempt to ﬁnd ex-\nplicit repetition in a piece can improve performance.\nIn the domain of classiﬁers or decoders, many published\nworks use Hidden Markov Models (HMMs). Recent pa-\npers have used Dynamic Bayesian Network (DBN) in con-\njunction with separate bass and treble chromas for recog-\nnition [1], [5]. In the past, two methods implementing key\ndetection to assist chord recognition have been proposed.\nThe ﬁrst method builds a group of key-speciﬁc models [4],\n[9], while the other treats key information as one of the\nlayers in its graphical model [1], [5]. In some cases, tran-\nsition matrices were based on, or initialized, using princi-\nples from music theory rather than learned from the train-\ning set [12]. Apart from HMMs, a Pitman-Yor Language\nModel [13] has also been used to build a vocabulary-free\nmodel for chord recognition. Finally, another popular ap-\nproach is the use of chroma templates of chords [14], [15].\nIn this paper, we present our approach which proposes\na novel method to compute the chroma, and uses duration-\nexplicit HMMs (DHMMs) for chord recognition. DHMMs\nare discussed in [16], but have rarely been used in MIR\nresearch. We also try to answer an important question:\nhow much can transitional context knowledge (i.e. chord\nprogressions) contribute to increasing the accuracy of the\nmodel?\nThis paper is organized as follows: Section 2 describesthe chroma feature that we use, emphasizing on a novel\nway of computing chromagram; Section 3 presents the DHMM\nand its implementation; Section 4 evaluates our models and\nanalyzes the contribution of duration constraints and tran-\nsitional context knowledge; Section 5 presents the conclu-\nsions and sheds light on future research.\n2. CHROMA FEATURE\nOur chroma feature is based on the 60 dimensional log-\nfrequency spectrogram computation proposed in [5], which\nuses perceptual loudness scaling of the spectrogram. We\nset our frame length to 512 ms with a hop size 64 ms. We\nonly consider the energy within 5 octaves between 55 Hz\nand 1760 Hz. We propose a new method to compute the\nchromagram from the spectrogram.\n2.1 Chroma Based On Linear Regression\nChroma is typically calculated by “folding” the spectrum\ninto one octave and summing over frequencies correspond-\ning to a quarter-tone around a given pitch-class. In [10], the\nauthors show that a machine-learned transformation matrix\noutperforms this basic method. We developed a method\nwith similar motivation. The ground truth chord label is\nconverted into a 24 dimensional chroma template logical\nvector, where the ﬁrst 12 dimensions represent whether\none of the 12 notes exists in the bass label, and the last\n12 dimensions represent whether one of the 12 notes exists\nin the chord label. For an example, a “C:maj/5” is con-\nverted to\n[0;0;0;0;0;0;0;1;0;0;0;0;1;0;0;0;1;0;0;1;0;0;0;0].\nThe target vectors are the chroma templates and we ﬁt\na transformation matrix which converts the log-frequency\nspectrum to a chroma vector that is as close to the chroma\ntemplate as possible. Similar to [10], we explored the use\nof neural networks, experimenting with various activation\nfunctions including sigmoid, logistic, linear and quadratic.\nWe found that sigmoid and logistic functions compress the\noutputs, leading to additional confusion between 0 and 1.\nLinear and quadratic regressions return nearly the same re-\nsults without compressing the output. Consequently, we\nchose linear regression to ﬁt the transformation matrix. The\nregressed matrix shown in Figure 1 transforms a 60 di-\nmensional log-frequency spectrum into a 24 dimensional\nchroma vector, which is a concatenation of the bass and\ntreble chroma. It is worth noticing that the transformation\nmatrix has weights for both base frequency and harmon-\nics, leading to a certain degree of inhibition of harmonics\nin the ﬁnal chroma. We have additionally tried some other\nproposed methods to inhibit harmonics (e.g. NNLS [1],\nHPS [2]), but none of them returned an improvement on\nthe ﬁnal results. Since the matrix relies on ground truth\nchord labels in the training set, the linear regression is per-\nformed every time we train the model.\n2.2 Tonnetz Feature for Bass\nThe chromagram we have obtained through the aforemen-\ntioned method is not full rank. This is because the infor-\n  \n10 20 30 40 50 605\n10\n15\n20\n−0.100.10.20.30.4Figure 1. The transformation matrix for calculating\nchroma from CQTed-spectrum.\nmation about the root note occurs in both bass and treble\nchroma templates. Given the common knowledge that bass\nchroma contains less chord related information than treble\nchroma, it might be more suitable for a lower-dimensional\nrepresentation. So, we convert a bass chroma (i.e. the ﬁrst\n12 dimensions) into a 6-dimensional tonnetz feature, de-\nscribed in detail in [8]. We explored converting the treble\nchroma into tonnetz, but we did not observe any improve-\nment on accuracy, which is consistent with [5]. Our fea-\nture vector is thus 18 dimensional, consisting of the treble\nchroma and the tonnetz features from bass chroma.\n2.3 Beat Level Feature\nIn popular music, chord changes usually occur at beat and\nbar boundaries and thus, beat information can be useful\nin determining likely boundaries. We beat-tracked the au-\ndio using the dynamic programming approach of Ellis [17].\nHowever, we extended the algorithms to allow for tempo\nvariation within a song. A dynamic programming process\nwas used to estimate the tempo before ﬁnding beat posi-\ntions, similar to a method described by [18]. This resulted\nin a slight improvement in chord recognition accuracy.\nWe explored three approaches to calculate beat level\nfeatures: (1) calculate chroma on the entire beat (large win-\ndow for CQT); (2) calculate chroma on the frame level,\nthen average over the frames in the beat; (3) calculate chroma\non frame level, then take the median of each dimension\nwithin a beat. We found that approach (2) worked the best.\nIn our experiments, we explore the use of both the frame\nlevel and beat level features in the HMMs.\n3. DURATION-EXPLICIT HIDDEN MARKOV\nMODEL (DHMM)\nIn this section, we present a detailed discussion of the DHMM\nand its implementation, at the beat level. DHMMs estimate\nthe chord sequence by simultaneously estimating chord la-\nbels and positions, which can be thought of as estimating\non the chord level (See Figure 2). Initially, we applied\nDHMM hoping to reveal transitional context knowledge\nsince at the frame and the beat level, self-transition is dom-\ninant. However, as we show in section 4.2, transitional\ncontext knowledge is not as important as we had hypothe-\nsized. Yet, modeling the duration of chords contributes to\nthe majority of our improvement.|\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \nbeat level \n|\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r   |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r   |\t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r  \t\r  |\t\r  \t\r  \t\r   |\t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \nchord level frame level Figure 2. Frame level, beat level and chord level. Hori-\nzontal axis is frame level feature vector index; vertical axis\nis feature dimension index.\nWe adopt the notation used in [16]. To better under-\nstand the following expressions, readers are encouraged to\nbrieﬂy review III.A and III.B of [16]. In chord recogni-\ntion,Tis the number of beats in a song; observation se-\nquenceO=fO1O2:::O Tgis the time sequence of fea-\nture vectors; state sequence Q=fq1q2:::qTgis the hid-\nden chord label sequence; Nis the number of chords being\nconsidered (i.e. the total number of states of the HMM);\nS1S2:::S Nare possible states; qt2fS1S2:::S Ng;\u0019is\nthe vector of initial probabilities; A=faijgis the chord\ntransition matrix, which denotes the probabilities of transi-\ntions fromSitoSj;B=fbi(Ot)gis the emission matrix,\nwhich denotes the probabilities of emitting OtfromSi;\np=fpi(d)gis the duration distribution, which denotes the\nprobabilities of Sispanningdbeats.\nThe model\u0015comprises of \u0019,A,Bandp. In our ex-\nperiments, we found that \u0019is unimportant so we set it to\nuniform distribution. Ais trained by counting all the chord\nchanges. A small value (0.05) is added to the diagonal of\nAbefore normalization, in order to bring the Viterbi algo-\nrithm back to sync when the actual duration has zero prob-\nability inp. A multivariate normal distribution is trained\nfor each chord in order to calculate B.pis computed by\ncounting the durations (i.e. number of beats) of each chord.\nThe same duration distribution is used for all chords. How-\never, the notation pi(d)is retained for better generalization.\nWe limit the maximum duration Dto 20 beats.\n3.1 Viterbi Algorithm\nViterbi algorithm is a dynamic programming algorithm that\nﬁnds the globally optimal state sequence Q\u0003=fq\u0003\n1q\u0003\n2:::q\u0003\nTg\nexplaining an observation sequence, given the model \u0015.\nWe denote\n\u000et(i) = max\niP(S1S2:::S iends attj\u0015)\nA. Initialization (t \u0014D):\n\u000e\u0003=t\u00001max\nd=1Nmax\nj=1;j6=i\u000et\u0000d(j)ajipi(d)tY\ns=t\u0000d+1bi(Os)\n\u000et(i) = maxf\u0019ipi(t)tY\ns=1bi(Os);\u000e\u0003g\nB. Recursion (D<t \u0014T):\u000et(i) =Dmax\nd=1Nmax\nj=1;j6=i\u000et\u0000d(j)ajipi(d)tY\ns=t\u0000d+1bi(Os)\nIn addition to \u000et(i), we need two other variables:  t(i)\nto track the last optimal state, and \u001et(i)to track optimal\nduration. If Siends att, the optimal duration of Siwould\nbe\u001et(i), and the optimal last state would be  t(i).\nIn initialization and recursion, we can get the index ^j\nand^dthat produce \u000et(i), then\n t(i) =^j\n\u001et(i) =^d\nIf\u000et(i)equals\u0019ipi(t)Qt\ns=1bi(Os)in initialization, then\n t(i) =i\n\u001et(i) =t\nC. Termination:\nq\u0003\nT= arg max\n1\u0014i\u0014N\u000eT(i)\nD. Backtracking:\nd=\u001eT(q\u0003\nT)\nt=T\nwhilet>d do\nqt\u0000d+1:::qt\u00001=qt\nqt\u0000d= t(qt)\nt=t\u0000d\nd=\u001et(qt)\nend while\nq1:::qt\u00001=qt\n3.2 Probability of Observation Sequence\nIn some cases, it is necessary to know P(Oj\u0015), the proba-\nbility that a model \u0015generates an observation sequence O.\nThe computation of this probability is detailed in [16]. We\napplied the scaling method to prevent the probability from\ngoing below the machine precision.\nThe forward variable is deﬁned as\n\u000bt(i) =P(O1O2:::O t;Siends attj\u0015)\nA. Initialization (t \u0014D):\n\u000b\u0003=t\u00001X\nd=1NX\nj=1;j6=i\u000bt\u0000d(j)ajipi(d)tY\ns=t\u0000d+1bi(Os)\n\u000bt(i) =\u0019ipi(t)tY\ns=1bi(Os) +\u000b\u0003\nct=1\ntP\ns=1NP\ni=1\u000bs(i)\n\u000bs(i) =\u000bs(i)ct;s= 1:::t\nB. Recursion (D<t \u0014T):\n\u000bt(i) =DX\nd=1NX\nj=1;j6=i\u000bt\u0000d(j)ajipi(d)tY\ns=t\u0000d+1bi(Os)\nct=1\ntP\ns=t\u0000D +1NP\ni=1\u000bs(i)\n\u000bs(i) =\u000bs(i)ct;s=t\u0000D+ 1:::twithout\t\r  clustering\t\r  \nclustering\t\r  1\t\r  \nclustering\t\r  2\t\r  \nclustering\t\r  3\t\r  \nclustering\t\r  4\t\r  Figure 3. Top panel: Global duration distribution trained\nusing the whole training set. Panel 2-5: Clustered duration\ndistributions.\nC. Termination:\nlogP(Oj\u0015) =\u0000TX\nt=1logct\nAnother precision problem (which does not exist in an\nordinary HMM) is caused byQbi(Os). Our solution is to\ndivide allbi(Os)’s by the maximum value of B.\n3.3 Time Signature Clustering\nAs will be shown in section 4.2, a global duration model\nhas a limited contribution towards the accuracy improve-\nment, because we train one single duration distribution us-\ning the whole training set. In fact, popular music is com-\nposed using a limited number of time signatures (e.g. 4/4,\n6/8), and usually keeps its time signature unchanged for\nthe whole length.\nIn other words, we train multiple duration distributions\nso that we have multiple models \u00151;\u00152:::\u0015 m(where only\ntheirp’s are different), and we calculate P(Oj\u00151),P(Oj\u00152)\n. . .P(Oj\u0015m)and choose the model which maximizes like-\nlihood, before running the Viterbi algorithm.\nIn order to train multiple duration distributions, we cal-\nculate a duration distribution for each song in the training\nset, and then cluster all the duration distributions into c\ncategories using k-means algorithm (in our experiments,\nc= 4). We don’t manually annotate time signatures be-\ncause beat tracking algorithm is very likely to double or\ntriple the tempo. Through clustering, we don’t need to ac-\ntually know the true time signature, and can account for\npotential errors caused by beat tracking. Figure 3 gives\nan example of clustered duration distributions compared\nto the global duration distribution.\n5\n10\n15\n200\n5\n10\n15\n20\n2500.10.2 \n 5\n10\n15\n200\n5\n10\n15\n20\n2500.51 \n Figure 4. Left: Putting prior distribution to all rows of\ntransition matrix. Right: Adding 3 to the diagonal of the\nmatrix on the left and normalize each row.\n4. EVALUATION\n4.1 Experiments\nWe evaluate our models on two datasets: Beatles set by\nHarte [8] and Uspop set by Cho [7]. 44 songs in Uspop\nwere excluded because we couldn’t ﬁnd audio of a length\nthat matched the corresponding annotation. 12 songs were\nexcluded from Beatles for reasons such as audio being off\ntune, or inconsistent time offsets of annotations. (See http://\nwww.weibinshen.com/ISMIR2012Table.html for a full list\nof songs that were not used in this study).\nWe perform a 4-fold cross-validation experiment and\nreport the average chord recognition accuracy on 24 ma-\njor/minor chords. During training, all chord labels are re-\nmapped to 24 major/minor chords as in [5]. Each chord is\ntrained with a single Gaussian distribution and corresponds\nto one state in HMM. During testing, each frame or beat is\nrecognized as one of the major/minor chords. During eval-\nuation, recognized labels on beat level are transformed to\nthe frame level, and only frames with major/minor labels\nin the ground-truth annotations are counted for evaluation.\nThe recognition accuracy metric is the frame-based recall\nrate - the number of frames that are recognized correctly,\ndivided by the total number of frames, same as the evalua-\ntion metric used in MIREX evaluations.\nIn order to determine the contribution of chord progres-\nsion information to the improved performance, we also\nbaseline with Bayesian-type classiﬁers, where a chord pre-\ndiction is determined by a MAP classiﬁer independently at\neach frame or beat. We implement Bayesian-type classi-\nﬁers by simply replacing every row of a transition matrix\nwith the prior chord distribution, obtained by counting the\nunconditional occurrence of each chord (Figure 4).\n4.2 Results\nWe compare the accuracy of different models in Table 1.\nIn “Bayes”, we train the transition matrix by applying the\nprior distribution to all the rows. In “Bayes+smooth”, we\napply a “majority” ﬁlter on the Bayesian classiﬁer’s out-\nput, in order to remove short-term deviations. In “Mod.\nBayes”, we add a relatively large number (arbitrarily, 3)\nto the diagonal elements of the “Bayes” transition matrix,\nand then normalize each row (see Figure 4-Right). In or-\nder to compare it with the state of the art, we also run theHarmony Progression Analyser (HPA) proposed by Ni et.\nal [5] on the same datasets. It is a state of the art model\nusing a three-layer HMM with key, bass note and chord\nestimation.\nLevel Model Uspop Beatles\nFrame levelBayes 0.7518 0.7206\nBayes+smooth 0.8285 0.8204\nHMM 0.8096 0.7966\nBeat levelBayes 0.7867 0.7733\nMod. Bayes 0.8340 0.8331\nHMM 0.8365 0.8361\nDuration modelBayes 0.8371 0.8398\nDHMM 0.8377 0.8352\nTime signature clustered Bayes 0.8410 0.8413\nduration model DHMM 0.8423 0.8419\nHPA [5] 0.8401 0.8278\nTable 1. A comparison of Accuracy\n4.3 Analysis\nWe see that we achieve a performance comparable to the\nstate of the art [5]. At the frame level, Bayesian classiﬁer\nhas a fairly low accuracy (75.18% on Uspop and 72.06%\non Beatles). With smoothing (82.85% and 82.04%), it out-\nperforms the basic frame-level HMM (80.96% and 79.66%).\nAt the beat level, the Bayesian classiﬁer attains a recogni-\ntion rate of 78.67% for Uspop. However, when the self-\ntransitions are emphasized in the “Modiﬁed Bayes” method,\naccuracy is on par with the beat-level HMM. Smoothing,\nas well as emphasizing self-transitions, essentially incor-\nporate the knowledge that most chords last for more than a\nfew frames or beats.\nFor DHMMs, duration information is decoupled from\nthe transition matrix and results for the Bayesian classi-\nﬁer (83.71% and 83.98%) and the DHMM (83.77% and\n83.52%) are similar. Using multiple duration models after\nclustering raises accuracy to 84.23% on Uspop, which is\ncomparable the current state of the art.\nThe results suggest that the primary reason why HMMs\nare more effective than Bayesian classiﬁer is that the strong\nself-transition probabilities emphasize continuity, rather than\nthe knowledge of the chord progression represented in the\ntransition matrix. In other words, when continuity is enforc-\ned by smoothing, or modeling durations separately, HMMs\nperform no better than a Bayesian classiﬁer. Although\nthere have been past works stating an improvement by us-\ning smoothing [22], we did not ﬁnd any previous work dis-\ncussing if the reason for HMMs outperforming Bayesian\nClassiﬁers is because of the smoothing effect of its transi-\ntion matrix, or if the context information was really useful.\nTo further understand the contribution of chord progres-\nsion knowledge we constructed an “oracle” condition in\nwhich the true transition matrix for a song was revealed\n(i.e. the transition matrix was computed using the ground\ntruth labels for a particular song). This transition matrix\nwas then used by the DHMM. The results are summa-\nrized in Table 2 and can be interpreted as an upper bound\nfor chord recognition accuracy using a ﬁrst-order DHMM.These results suggest that even in the case where the chord\ntransitions are exactly known for a song, accuracy improves\nno more than 2%.\nLevel Model Uspop Beatles\nDuration modelBayes 0.8735 0.8726\nDHMM 0.8863 0.8919\nTable 2. Upper bound on performance\nWhy doesn’t knowledge of the chord progression give\ngreater improvements? In most cases, it seems the evi-\ndence provided in the local chromagram feature is quite\nstrong, minimizing the need for top-down information. On\nthe other hand, when the feature is noisy or ambiguous, it\nseems that the the prior imposed by the transition matrix\nis not very strong. In other words, chords progressions are\nless predictable than they seem.\nWe tested this hypothesis by estimating the entropy of\nchord progressions in the training set using a Variable Leng-\nth Markov Model (VLMM) [19], [20]. In other words,\ngiven the context, we tested how sure we can be of the\nnext symbol, on an average. A VLMM is an ensemble of\nMarkov models which effectively captures variable length\npatterns in the chord sequence. A VLMM was used, as op-\nposed to a ﬁrst-order Markov model, because we wanted\nto ensure that long patterns were captured in addition to\nlocal chord transitions. Given the true symbol sequence\ntill the current time frame, we obtain a predictive distribu-\ntion over the chord labels for the next time frame, which is\nused to obtain the cross entropy of the test sequence. Using\na VLMM, the minimum cross-entropy obtained was 3.74\non the Uspop dataset and 3.67 on the Beatles dataset (at a\nmaximum VLMM order 2), in a leave-one-out cross vali-\ndation experiment. It was found that the cross-entropy in-\ncreased beyond order 2 in both datasets. An entropy value\nof 3.74 corresponds to a perplexity of 13.4, which can be\ninterpreted as the average number of symbols the system\nwas confused between. Thus, knowing the chord history\ndoes not, in general, narrow the possibilities greatly, and is\nunlikely to overcome a noisy or ambiguous feature vector.\n5. CONCLUSIONS\nIn this paper, we presented an implementation of DHMMs\nand applied them to the chord recognition task. This model\ndecouples the duration constraints from the transition ma-\ntrix. We then build separate models for duration distribu-\ntions that indicate different time signatures to improve the\nduration constraint in each model. Using this method, a\ncomparable performance to the state of the art is demon-\nstrated.\nThough duration-explicit HMMs don’t produce ground-\nbreaking results, we believe that the proposed model may\nbeneﬁt other MIR tasks in the future, e.g. melody esti-\nmation and structural segmentation. Perhaps most impor-\ntantly we show that state of the art results can be obtained\nusing simple classiﬁers that do not use transition informa-\ntion. Further attempts to fully incorporate key and chord-\nprogression knowledge (at least for popular songs of thistype) using techniques such as high-order HMMs, are un-\nlikely to yield signiﬁcant improvements.\nThis material is based upon work supported by the Na-\ntional Science Foundation under Grant No. 0855758.\n6. REFERENCES\n[1] M. Mauch, S. Dixon: “Approximate Note Tran-\nscription For The Improved Identiﬁcation Of Difﬁ-\ncult Chords,” In Proceedings of the 10th International\nConference on Music Information Retrieval, Utrecht,\nNetherlands, 2010.\n[2] K. Lee, “Automatic Chord Recognition Using En-\nhanced Pitch Class Proﬁle,” In Proceedings of Inter-\nnational Computer Music Conference, New Orleans,\nUSA, 2006.\n[3] N. Ono, K. Miyamoto, H. Kameoka, S. Sagayama: “A\nReal-time Equalizer of Harmonic and Percussive Com-\nponents in Music Signals,” In Proceedings of the 9th\nInternational Conference on Music Information Re-\ntrieval, Philadelphia, USA, 2008.\n[4] Y . Ueda, Y . Uchiyama, T. Nishimoto, N. Ono,\nS. Sagayama, “HMM-based Approach for Automatic\nChord Detection Using Reﬁned Acoustic Features,”\nInThe 35th International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), Dallas,\nTexas, USA, 2010.\n[5] Y . Ni, M. McVicar, R. Rodriguez and T. Bie: “An end-\nto-end machine learning system for harmonic analysis\nof music,” In IEEE Transactions on Audio, Speech and\nLanguage Processing, In Press, 2012.\n[6] M. Muller, S. Ewert, “Towards timbre-invariant audio\nfeatures for harmony-based music,” In IEEE Trans-\nactions on Audio, Speech, and Language Processing\n(TASLP), 18(3):649–662, 2010.\n[7] T. Cho and J. Bello, “A Feature Smoothing Method For\nChord Recognition Using Recurrence Plots,” In Pro-\nceedings of the 12th International Conference on Mu-\nsic Information Retrieval, Miami, Florida, USA, 2011.\n[8] C. Harte, M. Sandler, M. Gasser: “Detecting Harmonic\nChange In Musical Audio,” In Proceedings of the 1st\nACM workshop on Audio and music computing multi-\nmedia, V olume: C, Issue: 06,Santa Barbara, Califor-\nnia, USA, 2006.\n[9] K. Lee, M. Slaney, “A Uniﬁed System for Chord Tran-\nscription and Key Extraction Using Hidden Markov\nModels,” In Proceedings of the 8th International Con-\nference on Music Information Retrieval , Vienna, Aus-\ntria, 2007.\n[10] O. Izmirli, R. Dannenberg: “Understanding Features\nand Distance Functions for Music Sequence Align-\nment,” In Proceedings of the International Conference\non Music Information Retrieval, Utrecht, Netherlands,\n2010.[11] M. Mauch, K. Noland, S. Dixon: “Using musical struc-\nture to enhance automatic chord transcription,” In Pro-\nceedings of the 10th International Conference on Mu-\nsic Information Retrieval, Kobe, Japan, 2009.\n[12] J. Bello, J. Pickens, S. Pauws “A Robust Mid-Level\nRerpresentation For Harmonic Content In Music Sig-\nnals,” In Proceedings of the 6th International Confer-\nence on Music Information Retrieval, London, UK,\n2005.\n[13] K. Yoshi, M. Goto, “A V ocabulary-Free Inﬁnity-gram\nModel For Nonparametric Bayesian Chord Progression\nAnalysis,” In Proceedings of the 12th International\nConference on Music Information Retrieval, Miami,\nFlorida, USA, 2011.\n[14] L. Oudre, C. Fevotte, Y . Grenier, “Probabilistic\nTemplate-Based Chord Recognition,” In TELECOM\nParisTech, Paris, France, 2010\n[15] T. Fujishima, “Realtime Chord Recognition of Musi-\ncal Sound: a System Using Common Lisp Music,” In\nThe 1999 International Computer Music Conference,\nBeijing, China, 1999.\n[16] L. Rabiner: “A Tutorial On Hidden Markov Models\nAnd Selected Applications In Speech Recognition,” In\nProceeding of the IEEE, V ol.77, No.2, February 1989.\n[17] D. Ellis, “Beat Tracking by Dynamic Programming,”\nInJournal of New Music Research V ol.36(1), 51-60,\n2007.\n[18] F. Wu, T. Lee, J. Jang, K. Chang, C. Lu, W. Wang, “A\nTwo-Fold Dynamic Programming Approach to Beat\nTracking for Audio Music With Time-Varying Tempo,”\nInProceedings of the 12th International Conference\non Music Information Retrieval, Miami, Florida, USA,\n2011.\n[19] D. Conklin, I. H. Witten, “Multiple viewpoint systems\nfor music prediction,” In Journal of New Music Re-\nsearch, 24:51-73, 1995.\n[20] M. Pearce, D. Conklin, and G. Wiggins, “Methods for\nCombining Statistical Models of Music,” In U. K. Wiil\n(Ed.), Computer Music Modelling and Retrieval (pp.\n295-312). Heidelberg: Springer.\n[21] J. A. Burgoyne, J. Wild, I. Fujinaga, “An Expert\nGround-Truth Set For Audio Chord Recognition And\nMusic Analysis,” In Proceedings of the 12th Interna-\ntional Conference on Music Information Retrieval, Mi-\nami, Florida, USA, 2011.\n[22] T. Cho, R. J. Weiss, J. P. Bello, “Exploring Common\nVariations in State of the Art Chord Recognition Sys-\ntems,” In 7th Sound and Music Computing Conference,\nBarcelona, Spain, 2010."
    },
    {
        "title": "Creating Ground Truth for Audio Key Finding: When the Title Key May Not Be the Key.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414972",
        "url": "https://doi.org/10.5281/zenodo.1414972",
        "ee": "https://zenodo.org/records/1414972/files/ChuanC12.pdf",
        "abstract": "In this paper, we present an effective and efficient way to create an accurately labeled dataset to advance audio key finding research. The MIREX audio key finding contest has been held twice using classical compositions for which the key is designated in the title. The problem with this accepted practice is that the title key may not be the perceived key in the audio excerpt. To reduce manual an- notation, which is costly, we use a confusion index gen- erated by existing audio key finding algorithms to deter- mine if an audio excerpt requires manual annotation. We collected 3224 excerpts and identified 727 excerpts re- quiring manual annotation. We evaluate the algorithms’ performance on these challenging cases using the title keys, and the re-labeled keys. The musicians who aurally identify the key also provide comments on the reasons for their choice. The relabeling process reveals the mismatch between title and perceived keys to be caused by tuning practices (in 471 of the 727 excerpts, 64.79%), and other factors (188 excerpts, 25.86%) including key modulation and intonation choices. The remaining 68 challenging cases provide useful information for algorithm design.",
        "zenodo_id": 1414972,
        "dblp_key": "conf/ismir/ChuanC12",
        "keywords": [
            "audio key finding",
            "MIREX audio key finding contest",
            "confusion index",
            "manual annotation",
            "title key",
            "perceived key",
            "musicians",
            "key modulation",
            "intonation choices",
            "algorithm design"
        ],
        "content": "CREATING GROUND TRUTH FOR  AUDIO KEY FINDING: \nWHEN THE TITLE KEY MAY NOT BE THE KEY  \nChing -Hua Chuan Elaine Chew \nUniversity of North Florida \nSchool of Computing \nc.chuan@unf.edu  Queen Mary, University of London \nCentre for Digital Music \nelaine.chew@eecs.qmul.ac.uk  \nABSTRACT  \nIn this paper , we present an effective and efficient way to \ncreate an accurately labeled dataset to advance audio key \nfinding research . The MIREX audio key finding contest \nhas been held twice using classical compositions for \nwhich the key is designated in the title. The  problem with \nthis accepted practice is that the title key may not be the \nperceived key in the audio excerpt. To reduce manual an-notation, which is costly, we use a confusion index gen-\nerated by existing audio key finding algorithms to dete r-\nmine if an audio  excerpt requires manual annotation. We \ncollected 3224 excerpts and identified 727 excerpts re-\nquiring  manual annotation. We evaluate the algorithms’ \nperformance on these challenging cases using the title \nkeys, and the re- labeled keys. The musicians who aur ally \nidentify the key also provide comments on the reasons for \ntheir choice. The relabeling process reveals the mismatch \nbetween title and perceived keys to be caused by tuning \npractices  (in 471 of the 727 excerpts, 64.79%) , and other \nfactors (188 excerpts, 25.86%) including key modulation \nand intonation choices . The remaining 68 challenging \ncases provide useful  information for algorithm design.  \n1. INTRODUCTION  \nThe typical trend in technology development is for s ys-\ntems proposed later to  outperform earlier ones, but this \ndoes not seem to have be en the case for audio key fin d-\ning, judging by the results of the MIREX audio key fin d-\ning contest . The first MIREX audio key finding contest \nwas held in 2005, and the second contest took place  six \nyears later. The same  dataset was used in the two con-\ntests, and based on the numbers, the systems in MIREX \n2011 seem to have performed worse than the ones in \n2005 on average.  This points to the fact that the statistics \nof the contest results alone have not provided sufficient \ninformation for future researchers to develop better sys-\ntems. If the goal of the contest is to move the research \ncommunity forward, a detailed examination of the results is required . \nAn effective way t o improve audio key finding is to \nexamine the cases in the dataset for which most systems have difficulties. For this paper, we constructed a dataset \nwith 33 24 music audio recording  excerpts, 2.6 times the \nnumber in the MIREX dataset . It is worth noting that this \ndataset created from actual music recordings is  distinct \nfrom the MIREX dataset synthesized from MIDI files. We implemented five existing audio key finding systems \nand tested them on the dataset. Using the title key as \nground truth, let the confusion index, I, be the number of \nsystems that disagree wit h this ground truth. We then ex-\ntracted a sub set of the  data consisting of excerpts for \nwhich no more than two system s reported correct an-\nswers, i.e. I\n≥3. This subset, called the challenging set, \nwas re-examined by three professional  musicians  and \ntheir key s manual ly label ed. By c omparing the relabeled  \nkeys with the title keys, we observe reasons why  the ex-\ncerpt’s perceived key might be different from the title \nkey. We also present the musician s’ comments about \ntheir annotations to show  the factors that impact audio \nkey finding. Finally, we describe some controversial a u-\ndio key finding cases for which we received three co n-\nflicting answers .  \n The paper is organized as follows. Section 2 provides \nthe background of MIREX audio key finding contests. Section 3  presents the five audio key finding systems i m-\nplemented for the study.  In Section 4, we describe the e x-\nperiment design, followed by a detailed examination of \nthe experiment results. We state our conclusion s and su g-\ngestions for future work in Section 5.  \n2. BACKGROUND  \nIn MIREX 2005, Chew, Mardirossian and Chuan pro-\nposed contests for symbolic and audio key finding [ 7]. \nFor audio key finding, a wav e file is given as in put and \none answer  including a key name (for example, “C”) and \na mode (such as “major ”) is expected as output. The \nground truth used in the contest is the key defined by the \ncomposer in the title of the piece , as is the practice in key \nfinding research . Each output key is compared to the \nground truth and assigned a score as follows: 1 point if \nthe output is  identical  to the ground truth , 0.5 if they are \na perfect fifth apart , 0.3 if one is the relative major/minor  \nof the other , and 0.2 if one is the  parallel major/minor  of \nthe other . In th \ne contest, 1252 audio files synthesized \nfrom MIDI were used as the test dataset , consisting of  \nsymphonies from  various time periods . The best system \nachieved an 87% correct rate, with a composite score of  \n89.55% [8]. The second audio key finding/detection co n-\ntest was held in 2011  [9]. The same dataset was used and \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012  International Society for Music Information Retrieval    \n \nsame evaluation method was employed. The best system \nachieved a 75% correct rate and weighted score of 82%.   \n3. AUDIO KEY FINDING SYSTEMS  \nIn this section we  first describe the general structure of an \naudio key finding system, then the five systems imple-\nmented in this study. Systems that rely on training  data \nare not implemented  because the title key (ground truth) \nmay not be the key of the excer pt. \n3.1 A General Structure  \nFigure 1  illustrates a general structure of an audio key \nfinding system. The system can be divided to two major \nparts as shown in the dashed boxes. The components in \nthe left dashed box are designed to transform low -level \nacoustic features such a s frequency magnitude into high -\nlevel music information such as pitches or pitch classes.  Some audio key finding systems start with some prepro-\ncessing  of the data , such as the removal of noise and si-\nlence. The next two steps, windowing and Fast Fourier \nTransform  (FFT), indicate a basic approach for spectral \nanalysis. After spectral analysis, the step labeled Pitch \nClass Profile  (PCP) generation converts the frequency \nspectrum  information  into a pitch class distribution called \na pitch class profile . This is often the step where most  au-\ndio key finding systems differ . \n \nFigure 1. General structure of audio key finding systems . \nAfter a pitch class profile is generated, it is then co m-\npared with 24 predefined key templates  or profiles , 12 \nmajor and 12 minor, to determine the key in the audio \nexcerpt . This step is shown in the right dashed box in \nFigure 1, consisting of two components: a key finding \nalgorithm and a representation model  of keys . A repr e-\nsentation model provides a typical pitch class distribution \nfor a spe cific key . The key profiles produced by the re p-\nresentation model are then used to determine the key in a \nkey finding algorithm. For example, a  simple key finding \nalgorithm calculate s the correlation between the pitch \nclass profile of the audio excerpt and the 24 key profiles, and the key profile that reports the highest  correlation  \nvalue is selected as the key.  \nIn the following sub sections, w e describe the audio \nkey finding systems i mplemented in this study in detail , \nemphasizing the uniqueness of each system . \n3.2 Krumhansl’s and Temperley’s Key Profile s \nTonality has been studied extensively not only in music theory, but also from many other perspectives. In 1986, \nKrumhansl and Schmuckler  [6] developed a widely ac-\ncepted model called the probe tone profile method  (re-ferred to as the K -S model for convenience) , which co n-\nstructs pitch class profiles for major and minor keys by using user ratings from probe tone experiments. In 1999, \nTemperley  [10] improved upon the K-S model by mod i-\nfying the key profiles to emphasize the differences b e-\ntween diatonic and chromatic scales. Temperley also a d-\njusted the weights of the forth and seventh pitches so as \nto differentiate between keys with highly similar  pitch \nclass signatures. The key profiles generated by the K -S \nmodel and Temperley are shown in Figure 2.  \nThese key profiles can be directly used to build a sy m-\nbolic key finding system. In this study, we added an au-\ndio signal processing module to these key  profiles,  and \ncreated  two base audio key finding systems for compari-\nson. The audio signal processing module consists of a si-\nlence removal preprocessing  step, rectangular windowing \nwith non -overlapped frames, FFT, and generation of a \nPCP using a uniform we ighting function across the fr e-\nquency range of a pitch and also for all pitches folded in-to 12 pitch classes.  \n \nFigure 2. C major and C minor key profiles proposed by \nKrumhansl, Temperley, Izmirli and G\nómez. \n3.3 Izmirli’s  System  \nIzmirli proposed a template -based correlation model for \naudio key finding in [ 5]. During the  PCP generation step \nshown  in Figure 1, called chroma template calculation in \nIzmirli’s system, pitches are weighted using a decreasing \nfunction that gives low -frequency pitches more weight.  In \nIzmirli’s system, he construct ed key templates from \nmonophonic instruments samples, weighted by a comb i-\nnation of the K -S and Temperley’s modified pitch class \nprofiles , as shown in Figure 2. Izmirli’s system also \ntracks the confidence value for e ach key answer, and the \nglobal key is then selected as the one having the highest sum of confidence values over the length of the piece . \n  \n \n3.4 Gómez’s HPCP  \nIn [5 ], G ómez detected pitches using three times the \nstandard resolution of the pitch frequency spectrum of the \nFFT method, and distributed the frequency values among \nthe adjacent frequency bins using a triangular weighting \nfunction to reduce boundary errors . A Harmonic Pitch \nClass Profile (HPCP) is generated as input to the key \nfinding algorithm, using a modified version of \nKrumhansl’s key templates  as shown in Figure 2 .  \n3.5 Chuan and Chew’s FACEG  \nIn [3 ], Chuan and Chew proposed an audio key finding \nsystem called Fuzzy Analysis Center of Effect Generator (FACEG) . A fuzzy analysis technique is used for PCP \ngeneration using the harmonic series to reduce the errors \nin noisy low frequency pitches. The PCP is further re-\nfined periodically  using the current key information.  The \nrepresentation model used in the system is Ch ew’s Spiral \nArray model [ 1, 2], a represent ation of  pitches, chords, \nand keys in the same 3-dimensional space with distances \nreflecting their musical relations. The Center of Effect \nGenerator (CEG) key finding algorithm determines key in \nreal-time: a n inst antaneous key answer is generated in \neach window  based on past information . It is the only \nmodel amongst the ones considered with pitch spelling.   \n4. EXPERIMENT DESIGN  \nIn this section we describe the manner in which the d a-\ntaset is prepared , and the experiment design for exploring \nthe reasons for the  challenging cases. \n4.1 Data Collection and Selection \nThe dataset used in this study is provided by Classical KUSC, a classical  public radio station . The entire dataset \nconsists of over 40,000 audio recordings of classical pe r-\nformances.  For this study, we selected compositions  by \nBach, Mozart and Schubert. We chose the se three co m-\nposers ’ work  because (1) tonality is generally more clea r-\nly defined in these pieces than in more recent  composi-\ntions; and (2) the co mposers represent three different \nstyles with  distinguishable  levels of tonal complexity . We \nfurther refined the dataset by filtering out the recordings that do not have key information in the title . For multi -\nmovement works, we used only the first  and last move-\nment, because they are generally in the title key.  \nAs a result, the dataset we used in this study consists of \n1662 recordings  of varying length s. Similar to the evalua-\ntion procedure in the MIREX 2005 and 2011 audio key \nfinding contests, we extract ed two excerpts from each re-\ncording: one contain ing the first 15 seconds and the other \nrepresent ing the last 15 seconds  of the recording . We o n-\nly reserved the beginning and end sections of a recording because these two sections are more likely to be in the \nkey shown in the title. As a result , we ended up with a \ntotal of 3324 different excerpts in the experiment.   4.2 Evaluation Method  \nTo improve the performance of existing audio key fin d-\ning systems, some more detailed examination of the chal-\nlenging cases is neces sary. An excerpt is considered cha l-\nlenging if no more than two systems out of  the five im-\nplement ed reported  the key identical to the one in the t i-\ntle. A challenging set was thus built from such cases . \nThe excerpts from the challenging cases were given to \ntwo professional musicians for key annotations. During the process, the two musicians were provided with the \n15-second long excerpts instead of the entire pieces, to \nensure that they have the same acoustic information as \nthe systems. No key labels or title s were revealed to the \nmusicians. The only information other than the audio e x-\ncerpt provided is the name of the composer.  \nThe musicians were asked to write down one answer  \nas the global key for each excerpt, based on the 15 se-\nconds they heard. They were also asked to comment on \nthe reasons behind their answers, particularly for excerpts \nthat they felt  were  difficult to annotate. When the key an-\nnotations by the two musicians differed, the excerpt was  \ngiven to a third professional  musician for relabeling. The \nfinal relabeled key was determined by majority vote from \nthe three annotations.  \n5. EXPERIMENT RESULTS \n5.1 Results Using  Title  Keys as Ground Truth  \nOut of the 3324 excerpts, there were 727 excerpts \n(21.87%) for which no more than two systems reported  \nanswer s identical to the  title keys, i.e. I\n≥3. We focused  \non these 727 excerpts, the challenging  cases  in this study, \nto examine the d i\nfficulties  most key finding systems e n-\ncounter.  Table 1 shows the distribution of the challenging  \nset, in absolute numbers and as a percentage of the num-\nber of excerpts we considered by each composer. \nComposer  Total # of  \nrecordings  Challenging set  \n(first 15 sec)  Challenging set  \n(last 15 sec)  \nBach  553 245 (44.30%)  244 (44. 12%)  \nMozart  873 75 (8.5 9%) 98 (11.23%)  \nSchubert  236 24 (10.1 7%) 41 (17.3 7%) \nTable 1. Details of the entire data set and the challenging  \nset by Bach, Mozart and Schubert . \nWe divided the reported key into 9 categorie s based on \nits relation to the title  key: correct, dominant (Dom), sub-\ndominant (SubD), parallel major/minor (Par), relative \nmajor/minor (Rel), same mode with the root one half -step \nhigher (Mode +1), same mode with the root one half -step \nlower (Mode – 1), same mode but not in the previous ca t-\negories (Mode Others), and the rest of relations not i n-\ncluded in any of the previous categories (Others).   \nFigure 3 shows the results for the Bach challenging set \nin (a) first 15 seconds and (b) last 15 seconds respectively. It can be observed that most of  the incorrect answers r e-\nported by the systems fall into the last three categories,   \n \nespecially in the category (Mode – 1), indicat ing that tu n-\ning may be an issue in key finding for Bach’s pieces.   \n \nFigure 3. Key finding results  for the challenging  Bach \ndataset using the title key as ground truth . \nFigure 4 shows the results for the Mozart challenging \nset in the (a) first and (b) last 15 seconds, respectively. In \nFigure 4 (a), similar to the results in Figure 3 (a), the last \nthree categories account for the majority of the results in the first 15 seconds . However,  unlike Figure 3 (b),  the \nparallel major/minor (Par) category accounts for a signif-\nicant proportion of the results in Figure 4 (b). The report-\ned keys are also more evenly distributed than in Figu re 3. \n \nFigure 4. Key finding results for the challenging Mozart \ndataset using the title key as ground truth . \nFigure 5 shows the results for the Schubert challenging \nset in the (a) first and (b) last 15 seconds, respectively. \nThe results of Schubert challe nging set are more similar \nto Mozart’s than Bach’s, as the results are more evenly \ndistributed in the first 15 seconds and the parallel m a-\njor/minor dominates in the last 15 seconds. One distinct \nfeature observed in the Schubert results is that the (Mode \n– 1) category is much less significant.  \nFigure 5. Key finding results for the challenging  Schu-\nbert dataset using the title key as ground truth . \n5.2 Results Using Re -labeled Keys as Ground Truth  \nTable 2 shows the statistics of re- labeled keys in relation \nto title keys. The tuning category consists of  re-labeled \nkeys one half step away from title keys, while the other  \ncategory includes relabeled key s that are neither  identical \nto title key s nor in the tuning category.     \nCompo s-\ner First 15 seconds Last 15 seco nds \ntuning other  tuning other  \nBach  183 \n(74.7%)  36 (14.7%)  182 \n(74.6%)  54 (22.1%)  \nMozart  48 (64%)  16 (21. 3%) 55 (56.1%)  38 (38. 8%) \nSchubert  1 (4.2%) 8 (33. 3%) 2 (4.9%) 36 (87.8%)  \nTable 2. Relations between title keys and re -labele d keys . \n \nFigure 6. Key finding results for the challenging Bach \ndataset using the re-labeled key as ground truth . \nFigure 6 shows the results for the Bach challenging set in \nthe (a) first 15 seconds and (b) last 15 seconds using the \nrelabeled keys as ground truth. Comparing  the results in \nFigure 3 with those in Figure 6, it is clear that many of the recordings of Bach’s compositions are not tuned to \n  \n \nmodern definitions of the title key. Pitches ranged from \none quarter to one half -step lower than what one might \nexpect in moder n tuning. Therefore, the cases labeled as \n(Mode – 1) in Figure 3 could be considered correct. This \nalso points to the importance of verifying the title keys manually for the audio key finding . However, it is deba t-\nable whether the key should be relabeled ba sed on mo d-\nern tuning. For example, a recording may be recognized as being in the key of B major according to modern tu n-\ning, but B major is a very uncommon key in Baroque m u-\nsic and some musicians still prefer to call it C major d e-\nspite the flattened tuning.  \n \nFigure 7. Key finding results for the challenging Mozart \ndataset using the re-labeled key as ground truth . \nFigure 7 shows the results for the Mozart challenging \nset using the relabeled keys as ground truth in the (a) first \n15 and (b) last 15 seconds. Ob serve that the number of \ncorrect answers is increased, indicating that the first and last 15 seconds of these pieces are actually in a key other \nthan the title key. By comparing Figure 7 (a) and Figure 4 \n(a), we observe that the increase in correct answers  in \nFigure 7 (a) mainly results from decreasing numbers in \nthe four categories: Mode – 1, Others, Dominant (Dom) \nand Parallel (Par). This shows that for Mozart, a piece \nmay start in a related parallel major/minor or even a fo r-\neign key. For the last 15 seco nd excerpts, a piece may end \nin a parallel major/minor key. The tuning problem, indi-cated by the (Mode – 1) category, can still be observed in \nMozart’s recordings in both the first and last 15 seconds.  \nFigure 8 shows the results on the Schubert challenging  \nset in the (a) first 15 and (b) last 15 seconds. The number \nof correct answer s does not increase much in Figure 8 (a ) \ncomparing  to Figure 5 (a), and the increment in Figure 8 \n(a) is the result of decrement in the Parallel major/minor \n(Par) category . When the relabeled keys are used as the \nground truth, almost all the systems recognize the key s \ncorrectly in the last 15 -second excerpts as shown in Fi g-\nure 8 (b). This result shows that Schubert’s piece s may \nend in the  parallel major/minor key, or even some more \ndistant keys in the same mode.   \nFigure 8. Key finding results for the challenging Schu-\nbert dataset using the re-labeled key as ground truth . \n5.3 Musician s’ Comments and Case Studies  \nIn this sectio n we present the musicians’ comments \nalong side their answers, and excerpts where they  disa-\ngreed with each other.  \nThe musician s were encouraged to write down co m-\nments with their answers but were not restricted in terms \nof the words they can use. Table 3 shows the most fr e-\nquently used keywords and thei r meanings.  \nKeywords  Meanings  Num. of \noccurrence \nsharp/flat Notes sound sharp/flat compared to  \nmodern tuning s 79 \neasy Clear V -I chord progression  66 \npicardy 3rd A piece in a minor key that ends in \nthe parallel major  28 \nmodulation  A piece changes from one key to an-\nother  16 \ntough/tricky Difficult to determine the key, mostly \ndue to missing cadence  15 \ncadence Cadence cut off; cadence spotted in \nthe middle of the piece; misleading \ncadence 4 \nTable 3. Keywords with meanings and number  of occur-\nrence s in musician s’ comments . \nAmong 727 excerpts, there are only 8 excerpts in \nwhich all three musicians  disagreed on the key. Table 4 \ngives the details of the 8 excerpts, the musicians’ anno-\ntated keys, and our notes on why these excerpts might have confused the annotators.  \n6. CONCLUSION S AND FUTURE WORK \nIn this paper we presented an approach to effectively and efficiently develop a well -annotated dataset for audio key \nfinding . Having a well -annotated dataset is essential for \nany kind of algorithm testing and development, but it is very time- consuming to create one with numerous exa m-\nples. In this paper we implemented five audio key finding systems, and used them to select the examples that r e-\n  \n \nquire manual examination. Three professional musicians \nre-labeled the keys for these difficult cases.  \nComposer  Recording  (excerpt )/ \nPerformer Title  key Relabeled \nkeys  \nBach  Cello Suite #3 BWV \n1009 (last 15 secs) /Yo-\nYo Ma  C major  C major,  \nB major,  \nC minor  \n Cello. Briefly in minor mode at the beginning, but ends  \nunequivocally  in C major; pitches flat.  \nSchubert  Moments Musicaux: #6 \nOp 94  (last 15 \nsecs)/David Fray  Ab major  Eb major,  \nG# major,  \nA major  \n Piano. In Ab major; tuning sharp. Annotator 1 misled \nby Bb’s in beginning. \nMozart  String Quartet #16 K428 \n(first 15 secs) /Quartetto \nItaliano  Eb major  F minor,  \nC minor,  \nEb major  \n String Quartet. Chromatic start and notes following led \nto ambiguity in mode ; ends clearly in Eb major.  \nMozart  String Quartet #16 K428 \n(first 15 secs)/Quatuor \nMosaiques  Eb major  B minor,  \nD major,  \nEb minor  \n Same piece as above; annotations completely different.  \nMozart  String Quartet #19 K465 \n(first 15 secs)/  Quatuor \nMosaiques  C major  C minor,  \nB minor,  \nunsure  \n String Quartet.  In C but Eb in vln 2 and flat A in vl n 1 \n(an intonation choice) led to perceived minor mode.  \nMozart  Gran Partita Serenade \nK361 (last 15 \nsecs)/Octophorus  B major  Bb major,  \nD major,  \nA major  \n Strings. Tuning flat , which explains the Bb and A. \nMozart  Symphony #22 K162 \n(last 15 secs)/Amsterdam \nBaroque O rchestra C major  B major,  \nE major,  \nD major  \n Orchestra. Tuning flat , which explains the B. \nMozart  Requiem K626 (last 15 \nsecs)/Vienna Philharmo n-\nic D minor  C major,  \nunsure,      \nF major  \n Voices/Str/Winds/ Perc. Flat; insufficent information.  \nTable 4. Information of the excerpts where three musi-\ncians disagree on the key . \nBy examining the relabeled keys, we discovered potential \ncauses for the difficulties  and make the following obse r-\nvations and recommendations:  \n(a) tuning:  In recorded performances , different tuning or \nintonation choices can  cause confusion. E valuations \ncould either account for all possible categorical key name \ninterpretations (e.g. flat C might be interpreted as B), or \nallow for tuning  (and letter name)  independent key fin d-\ning, for example by requiring systems to locate the most \nstable tone. \n(b) modulations:  some excerpts may not be entirely in \none key, modulating midstream or ending with a Picardy 3\nrd. These excerpts  could either be removed, or more n u-\nanced ground truth created with key change annotations . \n(c) missing cadences:  a key is theoretically established \nwhen a complete cadence confirms its identity. Many e x-\ncerpts from the first 15 seconds of pieces may not have \nthese cadences. C are could either be taken to make sure  these cadences exist in the evaluation samples , or the \nscoring system could account for levels of difficulty of assessing key based on annotators’ notes.  \nThe established dataset and annotations , and the pr o-\ncess of collecting them,  will benefi t the audio key finding \ncommunity and MIREX contests. While we cannot pu b-\nlicly share the music files, we will post  the results of the \nannotations online. Future plans include augmenting the \ndataset with automatically generated key labels. \n7. ACKNOWLEDGEMENTS  \nWe like to thank the annotators, Fred Bertino, Victor Hormilla, and Jaime Kaufman, for their time and effort . \n8. REFERENCES  \n[1] E. Chew : Towards a Mathematical Model of \nTonality , Doct oral dissertation, Department of \nOperations Research, Massachusetts Institute of Technology, Cambridge, Mass, USA, 2000.  \n[2] E. Chew : “Modeling Tonality: Applications to \nMusic Cognition ,” in Proc. of the 23\nrd Annual \nMeeting of the Cognitive Science Society, pp. 206–\n211, Edinburgh, Scotland, UK, 2001. \n[3] C.-H. Chuan  and E. Chew : “Fuzzy Analysis in \nPitch -Class Determination for Polyphonic Audio \nKey Finding,” in  Proc. of the 6th International \nConference on Music Information Retrieval , pp. \n296– 303, London, UK, 2005. \n[4] E. Gómez, “Tonal Description of Polyphonic Audio \nfor Music Content Processing,” INFORMS Journal \non Computing, summer 2006, Vol. 18, No. 3, pp. \n294– 304, 2006.  \n[5] Ö. İzmirli, “Template Based Key Finding from \nAudio,” in Proc.  of the International Computer \nMusic  Conference , Barcelona, Spain, 2005. \n[6] C. L. Krumhansl, “Quantifying Tonal Hierarchies and Key Distances,” in Cognitive Foundations of \nMusical Pitch , chapter 2, pp. 16 –49, Oxford \nUniversity Press, New York, USA, 1990.  \n[7] MIREX 2005 Audio Key Finding Contest,  www.music -\nir.org/mirex/wiki/2005:Audio_and_Symbolic_Key  \n[8] MIREX 2005 Audio Key Finding Contest Results, www.music -\nir.org/mirex/wiki/2005:Audio_Key_Finding_Results  \n[9] MIREX 2011 Audio Key Detection Contest, nema.lis.illinois.edu/nema_out/mirex2011/results/ak\nd/index.html  \n[10] D. Temperley, “What’s Key for Key? The Krumhansl -Schmuckler Key -Finding Algorithm \nReconsidered,” Music Perception , Vol. 17, No. 1, \npp. 65– 100, 1999."
    },
    {
        "title": "Influence in Early Electronic Dance Music: An Audio Content Analysis Investigation.",
        "author": [
            "Nick Collins"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417621",
        "url": "https://doi.org/10.5281/zenodo.1417621",
        "ee": "https://zenodo.org/records/1417621/files/Collins12.pdf",
        "abstract": "Audio content analysis can assist investigation of musi- cal influence, given a corpus of date-annotated works. We study a number of techniques which illuminate musicolog- ical questions on genre and creative influence. By applying machine learning tests and statistical analysis to a database of early EDM tracks, we examine how distinct putatively different musical genres really are, the retrospectively la- belled Detroit techno and Chicago house being the core case study. Further, by building predictive models based on works from earlier years, both by a priori assumed genre groups and by individual tracks, we examine questions of influence, and whether Detroit techno really is a sort of electronic future funk, and Chicago house an electronic ex- tension of disco. We discuss the implications and prospects for modeling musical influence.",
        "zenodo_id": 1417621,
        "dblp_key": "conf/ismir/Collins12",
        "keywords": [
            "audio content analysis",
            "musical influence",
            "corpus of annotated works",
            "genre",
            "creative influence",
            "machine learning tests",
            "statistical analysis",
            "database of early EDM tracks",
            "Detroit techno",
            "Chicago house"
        ],
        "content": "INFLUENCE IN EARLY ELECTRONIC DANCE MUSIC: AN AUDIO\nCONTENT ANALYSIS INVESTIGATION\nNick Collins\nUniversity of Sussex\nN.Collins@sussex.ac.uk\nABSTRACT\nAudio content analysis can assist investigation of musi-\ncal inﬂuence, given a corpus of date-annotated works. We\nstudy a number of techniques which illuminate musicolog-\nical questions on genre and creative inﬂuence. By applying\nmachine learning tests and statistical analysis to a database\nof early EDM tracks, we examine how distinct putatively\ndifferent musical genres really are, the retrospectively la-\nbelled Detroit techno and Chicago house being the core\ncase study. Further, by building predictive models based on\nworks from earlier years, both by a priori assumed genre\ngroups and by individual tracks, we examine questions of\ninﬂuence, and whether Detroit techno really is a sort of\nelectronic future funk, and Chicago house an electronic ex-\ntension of disco. We discuss the implications and prospects\nfor modeling musical inﬂuence.\n1. INTRODUCTION\nGenre is a contentious area at the best of times [1], but\nan especial mineﬁeld in electronic dance music, where\nproducers, journalists and consumers are always eager\nto promote new micro-genres [12]. As Brewster and\nBroughton have written of one highly strained genre term\n‘if you name a genre of music after a club which was\nopen for ten whole years and which was known for its\neclecticism, you’re going to run into problems of deﬁnition\npretty quickly. The word ‘garage’ is by far the most\nmangled term in the whole history of music’ [4, p. 307].1\nElectronic dance music’s origins range across African-\nAmerican music and European synth pop, against a back-\ndrop of increasingly affordable synthesis and sampling tech-\nnology [6, 11, 15, 17, 18]. The important role through the\n1980s of the US cities Chicago and Detroit as crucibles\nof new club music is unassailable, though they were not\nthe only centres of activity (New Yorks’ frenetic hip hop\ndevelopments into electro, or the post-Moroder italo-disco\nmovement in Europe are also worth mentioning, as indeed\nare general trends to danceable and synthesizer-laden pop\n1‘hardcore’ is another example of a heavily overloaded genre term.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.throughout the 1980s mainstream). The cities are enshrined\nin the genre names Chicago house and Detroit techno as\ntwo foundational pillars of later electronic dance music:\nthey form the core of the study in this paper, though we\ndo not assume without investigation that they are really as\ndistinct as their names imply.\nEven before audio content analysis investigation, there\nare good reasons for a musicologist to be wary of\ntreating house and techno too individually in their 1980s\ngrowth. Chicago is around a ﬁve hour drive from Detroit,\nand Detroit artists often went to Chicago to sell their\nrecords in the larger market there; Derrick May sold\nFrankie Knuckles his TR-909 drum machine! The term\n‘techno’ has many precedents, including track titles from\nBuggles, Yellow Magic Orchestra and Kraftwerk, although\nmost famously used in Techno City, a 1985 Cybotron\ntrack co-produced by Juan Atkins (the elder of Techno’s\n‘Belleville Three’). The genre term was ﬁnally applied\nas a differentiating stamp in 1988 for the Techno! The\nNew Dance Sound of Detroit compilation curated by Neil\nRushton, at Juan Atkins’ insistence on techno over Derrick\nMay’s ‘Detroit house.’ Nevertheless, the compilation itself\nincludes a ‘megamix’ at its close called Detroit is Jacking\n(jacking being a standard Chicago dance term) and another\ntrack called Share this house by Members of the House!\nDetroit artists have themselves attempted to characterise\nmusical differences with Chicago. In the liner notes to the\nTechno! compilation Derrick May writes ‘House still has\nits heart in 70s disco; we dont have any of that respect for\nthe past, its strictly future music. We have a much greater\naptitude for experiment’ (sic) [9], and most famously, that\n‘It’s like George Clinton and Kraftwerk are stuck in an el-\nevator with only a sequencer to keep them company’ [9].\nThe hypothesis of Chicago house as an extension of disco,\nand Detroit techno as a combination of electronic funk and\nsynth pop, will be examined herein.\nTwo previous studies of musical inﬂuence [5, 8] pub-\nlished in ISMIR, on synth pop and sampling, have indi-\ncated the beneﬁts of data-annotated corpora in new musi-\ncological investigations. Through musical similarity mea-\nsures, this paper examines the use of automatic audio con-\ntent analysis to establish the strength of links between his-\ntoric tracks and putative genre groupings. Where Bryan\nand Wang [5] worked on an existing database of annota-\ntions over sample-based music concerning ‘WhoSampled\nwho’ (http://www.whosampled.com/), Collins [8] exam-\nined audio similarity between date-annotated synth pop asGenre Dates Num\nTracksDuration\n(mins)Notes\nChicago House 1986-1989 31 197.7 Sourced in particular from Chicago Trax andThe Original Chicago House Classics\nas well as compilations including Warp10+1:Inﬂuences\nDetroit Techno 1986-1989 31 186.5 Including Derrick May, early Model 500, and the Techno! compilation. No second\nwave, nor Cybotron\n1980s Pop 1985-1989 31 127.7 Including Michael Jackson, Madonna, Prince\nFunk 1965-1978 31 118.6 Selected tracks from Parliament’s Mothership Connection, The Godfather, James\nBrown, The Very Best Of... andFunk Soul Classics\nDisco 1973-1980 31 112.5 Selected tracks from Anthems Disco andDisco Fever\nSynth Pop 1977-1981 31 145.6 Including Kraftwerk, Human League, Gary Numan, Ultravox, Depeche Mode\nElectro and Hip Hop 1980-1984 31 180.6 Some early rap, with an emphasis on the transition into electro. Includes Grandmaster\nFlash and the Furious Five The Message (1982) and excerpts from The Tommy Boy\nStory\nPunk/Post-Punk 1977-1979 31 89.7 UK artists including Sex Pistols, UK Subs, Wire, The Cure, Gang of Four\nTable 1. Overview of music corpus\npart of the process of identifying inﬂuence. The latter might\nbe justiﬁed as the more general case and is followed here: it\nis of particular import when scaling up to larger databases\nof audio where annotations are impractical for musicolo-\ngists. Network techniques introduced in [5] are still valu-\nable in providing applicable metrics for later analysis once\nsimilarity scores are established. However, this paper will\nlook at direct ﬁrst generation inﬂuence rather than longer-\nterm networks spanning chains of multiple nodes.\nThe paper proceeds through section 2 detailing the\nset of 248 source audio ﬁles split over eight genre\ngroups, and section 3 which discusses the technicalities\nof the predictive models used. Section 4 explores the\nseparability of genre groups suggested, using machine\nlearning algorithms, and the Anderson Darling statistical\ntest to look for any rejection of the null hypothesis that\nthey are drawn from the same distribution. In section 5 we\napply the predictive models to examine questions of the\nstrength of inﬂuence of precursor work on Detroit techno,\nChicago house, and a late 80s pop control group. As well\nas working with models trained on whole groups of tracks,\nwe also run tests for some famous individual tracks, such\nas Donna Summer’s I Feel Love (1977). Finally, in section\n6 we explore the implications of the experimental ﬁndings,\nand broach larger questions for studies of musical inﬂuence\nusing MIR techniques.\n2. SOURCES\nTable 1 is an overview of the materials used in this study.\nEight genre groups are presented, with 31 tracks per group.\nFive of these are precursor genres, movements in popular\nmusic from the mid 1960s to the early 1980s. The three top\ngroupings are Chicago house, Detroit techno, and a control\ngroup of mid- to late-1980s pop including Madonna\nand Michael Jackson, coincident with an explosion in\npopularity of electronic dance music in the UK. The earlier\ngenre groups include four important to the origins of\nelectronic dance music: funk, disco, synth pop and hip-\nhop (particularly in its electro form). Some UK punk and\npost-punk records are included as a further control.2\n2Joy Division were speciﬁcally excluded, since their New Order\nmanifestation intersects with electro circa 1983.Although the total duration of the genre groups differ,\nthe critical thing is the equal number of examples in\neach, since tests are based on equal length excerpts from\nindividual tracks, or otherwise involve a normalization\nfor duration, such as the average log loss of a predictive\nmodel. 1980s examples of electronic dance music tend to\ninvolve longer tracks, where many 1960s and 1970s singles\nare much closer to the three minute (or less) pop song\n(short songs were also revived with punk’s throwaway\nnumbers); creating groups of an equal number of tracks all\nbalanced in duration and the number of years associated is\nan unsolvable dynamic programming challenge.\nThere are many overlaps between these ostensibly\nseparate groups, such as the shift to disco via Philly soul,\nthe use of synthesizers by new wave acts as well as more\nexplicitly by synth pop groups, or the appropriation of\nfunk and disco backings in early rap records.3The a\npriori use of genre groups is justiﬁed on the grounds of the\nmusicians’ statements themselves, such as Derrick May\ncited above, who treat ‘funk’ and ‘disco’ as known areas\nof musical endeavour. The groups have been constructed\nfrom well known examples of the genres in question,\nand one confound in particular avoided in construction;\nsynthesized electronic instrumentation in disco is not\nrepresented in the disco group, but a few examples from\nthe Moroder camp are included under synth pop. Part\nof our analysis shall be to consider the well-deﬁnition\nof the groups, in terms of their internal consistency; as\nwell as considering genre based inﬂuence, we shall also\ntake a look at inﬂuential individual tracks later in the\npaper, to avoid any claims of resting too heavily on genre\nconstructions.\nNo categorisation can be perfect, and there are some\nmissing early 1980s genre groups, such as European elec-\ntronic body music and industrial (e.g. Liaisons Dangereuses,\nFront 242) and italo-house (e.g. Klein and MBO, Alexan-\nder Robotnick), and mid 1980s New York production de-\nvelopments (freestyle, Mantronix etc.). Manageability of\nthe overall study, and the greater overlap with the forma-\ntion dates in Chicago and Detroit, made these categories\nout of the scope of the current investigation; however, again,\n3The term ‘house’ itself ﬂoats around in 1983, for example on Rock\nthe House by Pressure Drop, a 1983 release on Tommy Boy.we return to a few individual tracks rather than whole genre\ngroups below. Although there are some earlier prototype\nChicago house tracks, such as Jesse Saunders’ On and On\n(1984), we have avoided these for some separation in date\nfrom the precursor genres in this study; most commercially\navailable international Trax releases, for example, tend to\nbe available from 1986 at the earliest.4\nComplete track lists can be made available on researcher\nrequest; all music was purchased.\n3. PREDICTIVE MODELING\nBag of features assumptions [7] are avoided in favour of\nusing time series modeling to construct predictive models;\nin particular, Prediction by Partial-Match (PPM) variable\norder Markov models [2, 13]. The strength of prediction\nof one piece or group of pieces by another is measured\nby average log loss in information theoretic terms [2], as\nfurther detailed below.\nVarious musical attributes of the pieces under consid-\neration are modelled, such as timbral, rhythmic and har-\nmonic change components. The ﬁnal model combines three\ncore elements:\n1. A model \u001cof timbre based on 11 features, with fea-\nture vectors accumulated by beats, vector quantised\nby a k-Means classiﬁer into symbols, and used to\ntrain a PPM model\n2. A model \u0013of inter-onset intervals after onset detec-\ntion on polyphonic audio, using a classiﬁer for IOI\nsizes into symbols, and subsequent PPM\n3. A model \u0011of harmonic change, based on extracting\nbeat-wise 12TET chroma, forming the sum of differ-\nences between beats, a classiﬁcation into symbols,\nand PPM\nFor\u001c, a more general set of timbral descriptors was se-\nlected than MFCCs, to try to reﬂect the character of an-\nalyzed audio, without high dimensionality (which would\nimpact on the k-Means step). The timbral features were\nperceptual loudness, sensory dissonance (using a Sethares\nmodel [16]), two transient detection measures using the\nwavelet method of [10], spectral centroid, spectral percentile\nat 0.8% and 0.95% energy, zero crossing rate, spectral crest\nmeasure, spectral slope, and a raw onset detection function\n(the preprocessed signal for an onset detector). The onset\ndetector for the raw detection function, and for the isola-\ntion of onsets for IOI detection in \u0013, is based on work by\nStowell [19], and is applied to polyphonic audio tracks; the\nrectiﬁed complex deviation (RCD) onset detection func-\ntion used here has proven reasonable for such applications.\nAll features were subject to normalization with respect to\ncorpus derived minimum and maximum values, and were\ngathered in beats by averaging feature vectors. Chroma for\n\u0011were also accumulated in beats, the difference between\n4One example of a famous and inﬂuential track which was recorded\nearlier but released much later is Phuture’s Acid Tracks, recorded late\n1985, released 1987.beatwise chroma vectors taken, and summed over the dif-\nference vector. This created a one-dimensional measure of\nharmonic change, where the summation process avoided\nissues with different absolute pitch centres in the music.\nFeature vectors, IOIs and delta chroma sums were\nsubject to vector quantisation into 20 tokens before PPM\nmodeling. In order to symbolize the multidimensional\ntimbral feature vectors in \u001c, vectors extracted from the\ntraining corpus were clustered with the unsupervised\nk-Means algorithm, with k=20. As one dimensional\nquantities, the IOIs in \u0013and harmonic change sums\nin\u0011were classiﬁed into twenty bands by histogram\nequalisation [3, p.188]. A histogram for categorisation\nwas constructed by sorting the values into order, splitting\nthem by twenty equal size bands, and taking histogram bin\npositions by the maximum in each band. Twenty bands\nwas a good compromise for a reasonable alphabet size for\nthe PPM, without invoking too high a dimensionality. PPM\nmodels were then trained on the sequences in the 20 token\nalphabet, using consecutive subsequences of ﬁve values at\na time. The particular model variant used here is what\nPearce and Wiggins [13] denote the PPM-AX variant.\nScoring for a given PPM model \ron novel data set X\nwas then calculated by\naveragelogloss\r(X) =\u0000P\nx2Xlog2P(xj\r)\njXj(1)\nwhere thexare all the sequence contexts of the data to be\ntested [2] Minimal scores correspond to high probability\nsequences, that is, highly expected with respect to the\nmodel. The log is critical to avoid ﬂoating point underﬂow\non multiplication of small probabilities. This average log\nloss measure is robust to different durations of sequences\nconsidered; in any case, we use equal length excerpts from\npieces.\nIn applying this to a corpus, a predictive model is\ntrained on a subset of pieces. The model can then be\nused to examine one or more target pieces, via equation\n1. In this work, the models are applied to equal size\ngroups of pieces, summing the model predictions within\nthe group to get a total score for the predictability of that\ngroup with respect to the probabilistic model.5The ﬁnal\nscores are actually the sum of those from the three models\n\u001cfor timbre, \u0013for rhythm and \u0011for harmonic change;\nthese three components are individually normalized before\nthe ﬁnal sum. Whilst a given model’s predictions are\ninternally comparable, care must be taken in comparing\nthe absolute value of scores between different models; the\nnormalization reﬂects that only relative degree and order is\ncomparable.\nAll implementations used the open source SuperCol-\nlider Music Information Retrieval library by the author,6\nwhich includes an example with the three component model\npresented here. Speciﬁc client source code for the work is\navailable on researcher request.\n5For a common group size, we can divide by the group size without\ncompromising comparability, rather than taking an average over a\ndifferent number of contributing members.\n6http://www.sussex.ac.uk/Users/nc81/code.htmlProbability of rejecting null hypothesis\nModel Chicago Detroit Pop Funk Disco Synth Pop Electro Punk\nChicago 0.4297 0.0033 0.3879 0.2131 0.0776 0.0832 0.3986 0.2532\nDetroit 0.0033 0.0023 0.0013 0.0005 0.0001 0.0004 0.0074 0.0003\nPop 0.3879 0.0013 0.4402 0.1113 0.2218 0.0283 0.2739 0.2109\nFunk 0.2131 0.0005 0.1113 0.3554 0.0040 0.1224 0.1629 0.2456\nDisco 0.0776 0.0001 0.2218 0.0040 0.3057 0.0003 0.0625 0.0741\nSynth Pop 0.0832 0.0004 0.0283 0.1224 0.0003 0.0982 0.0671 0.0474\nElectro 0.3986 0.0074 0.2739 0.1629 0.0625 0.0671 0.3366 0.1992\nPunk 0.2532 0.0003 0.2109 0.2456 0.0741 0.0474 0.1992 0.4389\nTable 2. Application of Anderson-Darling tests within and between genre groups. Each cell entry is the probability of rejecting the null\nhypothesis that the tracks being tested together are a homogenous entity. Signiﬁcant table entries are in bold, with respect to a Bonferroni\nsigniﬁcance level for multiple comparisons.\n4. MACHINE LEARNING TESTS AND\nSTATISTICAL TESTS OF SEPARABILITY FOR\nREPRESENTATIVE FEATURE VECTORS\nAn initial examination was carried out on the genre groups\nthemselves, to see how “separable” the genre groups were\nfrom one another with respect to the ability of machine\nlearning to differentiate them, and in terms of statistical\ntests for their internal and paired consistency. All tests\nwere repeated twice, ﬁrst for the timbral feature vector\ndetailed in section 3 for model \u001c, and secondly for a vector\nof 11 MFCCs. Single vectors summarised single tracks;\n45 seconds of feature vectors were extracted from a point\n25% of the way into a given sound ﬁle, and averaged\n(normalization factors had already been calculated across\nthe entire corpus of 248 tracks in an earlier sweep). ARFF\nﬁles were exported for tests in Weka, and arrays of data\ninto MATLAB for statistical tests.\nFor machine learning, we tested the discriminatory power\nof supervised classiﬁers to learn the training sets (given\nthe genre labels 0-7), and of unsupervised clustering algo-\nrithms to match these labels (the ‘classes to clusters’ evalu-\nation setting in Weka). Over 8 genres, the best scores came\nfrom the 11 different features rather than the MFCCs,7\nbut were still of low classiﬁcation success. The best results\nwere for a k-Means clusterer (correctly classiﬁed 69 of 248\ninstances, 27.8%) and naive Bayes (68 of 248, 27.4%); a\nrange of other algorithms were investigated including neu-\nral nets and J48 decision trees. The best MFCC results\nwere for k-Means (correct 47, 18.95%), and naive Bayes\n(correct 53, 21.4%). Examination of 2-dimensional sub-\nsets of features revealed a lot of overlap between genres.\nThis result motivated using a more sophisticated time se-\nries modeling approach rather than average feature vectors,\nand using the mixed feature vector for timbre rather than\nMFCCs. With just the house and techno groups, and the\nheterogenous feature vector, classiﬁcation accuracy was\naround 50% (at chance given two groups), with best perfor-\nmance from k-Means (correctly classiﬁed 37 of 62, 60%)\nand naive Bayes (34 of 62, 54.8%). For the 11 MFCCs,\nnothing better than 33 out of 62 (53.2%) accuracy was ob-\nserved, with most algorithms performing worse than chance.\nStatistical tests were also applied to the model \u001cfeature\nvector data, to look for overlap between genre groups,\n7Vectors of 40 MFCCs were also tested without any improvement in\nclassiﬁcation scores.and internal consistency. An Anderson-Darling test was\nutilized [20], which tests the null hypothesis that all\nfeature data arose from the same distribution (without\nassuming normality of that distribution); a signiﬁcant p-\nvalue indicates that the data is from multiple distributions.\nTable 2 presents the symmetric matrix of values for\nall pairwise (62 tracks at a time) and within-group (31\ntracks) tests. The Detroit techno group is seen as more\nheterogenous, and the null hypothesis would be rejected\nif the threshold was set at 0.05% p-value. Because there\nare 28 pairs and 8 individual genres = 36 tests, under the\nBonferroni correction the p-value of 0:05=36 = 0 :0013889\nhas the statistical power to cover everything up to 0.05 and\nmay give a better sense of whether the Detroit techno result\nis aberrant; we may keep the null hypothesis of Detroit\ntechno as consistent at this level. To the extent that the\nprobabilities point to degree of homogeneity, the Detroit\ncorpus is more heterogenous. The make-up of the Detroit\ncorpus unbalances things when paired with other groups,\nwhilst there seems to be a strong overlap of Chicago house\nand late 80s pop (given UK No.1s by Chicago house\nproducers, this may not be so unexpected) as well as with\nelectro and funk. Nonetheless, the average feature vector\napproach is quite coarse, and the predictive models are now\ndeployed for a ﬁner-grained examination.\n5. INVESTIGATION OF INFLUENCE THROUGH\nPREDICTIVE MODELS\nIn this section, results are reported for the predictive scores\ngiven to particular genre groups, and to individual tracks,\nfrom models constructed from ﬁrst genre groups, and then\nsome interesting precursor tracks. Section 3 describes the\nmodel construction and algorithm for prediction scores.\nScores are normalized for a particular run of a particular\nmodel to fall from 0 to 1, where 0 would be totally\npredicted by the model at probability 1, and 1 is the least\nexpected observed situation. Models can be constructed\nin two directions; we favour constructing a model from\nan earlier historical genre or track, to predict later tracks.\nThere is an argument that construction in the opposite\ndirection would also indicate the degree of derivation\nof the later work from earlier, and we report such\nconstructions symmetrically for the genre groups. We do\nnot form the fully symmetric score from a matrix plus itsPrediction Score\nModel Chicago Detroit Pop Funk Disco Synth Pop Electro Punk\nChicago *0.04013* 0.81935 0.83104 0.81382 0.82633 0.7646 0.87111 0.80286\nDetroit 0.7951 *0.04889* 0.8146 0.77913 0.79449 0.76644 0.861 0.75883\nPop 0.8961 0.86525 *0.04548* 0.8945 0.83403 0.85331 0.90055 0.89854\nFunk 0.85148 0.85413 0.90196 *0.02236 * 0.91059 0.84123 0.89624 0.91292\nDisco 0.83158 0.76417 0.82118 0.8417 *0.03575* 0.76346 0.89092 0.813\nSynth Pop 0.8846 0.88163 0.88508 0.87967 0.8056 *0.07829* 0.88813 0.86481\nElectro 0.85575 0.85762 0.89833 0.87927 0.91642 0.82893 *0.02486 * 0.87419\nPunk 0.76547 0.70549 0.89158 0.91145 0.86657 0.81012 0.86613 *0.03303*\nTable 3. Prediction scores of genres from models constructed for each genre. Starred italics on the diagonal correspond to the prediction\nof the data used to construct a model by that model; one bold entry in each row indicates the closest other genre to the model genre.\ntranspose, since model construction itself is not guaranteed\nto produce scores in exactly the same ranges, and the post\ncalculation normalization reported here, whilst helpful\nfor seeing links, is not uniform in comparison (even\nunnormalized, differences in model construction would\nquestion comparability; results are relative to a given\nmodel).\nTable 3 presents the asymmetric matrix of results\nover the predictive models. The diagonal is italicized;\nall models ﬁnd their own source database most highly\npredicted, as we’d expect for any sensible probabilistic\nmodeling. The description of Chicago house as disco with\na drum machine, and Detroit techno as future electro funk,\nis only partially borne out in these ﬁgures. One aberrant\nfactor is the close link of Detroit techno with late 1970s\npunk and post punk guitar tracks; Chicago house tracks\nare also seen as closer to punk than Detroit techno on this\nview. Of the three factors in the scores, the onset detection\ndriven IOI model is the point of similarity here; a related\ndensity of events has an impact, as does timbre to an extent,\nperhaps through some degree of sonic exploration in house\nand techno instrumental tracks. Examining relative values\nwithin rows, the links to synth pop are clear; Detroit is\ncloser to funk than disco, but Chicago also that way round.\nFrom the funk model, Chicago is very marginally ahead of\nDetroit, if within the third decimal place. Electro is closest\nto synth pop, which is musicologically sensible; the synth\npop model ﬁnds Chicago, Detroit and late 80s pop of a\nmuchness in terms of potential inﬂuence.\nThe relative degree of inﬂuence of a seminal piece can\nbe investigated by creating a predictive model from it.\nTable 4 compares 22 interesting tracks from the 1970s\nto the early 1980s; these precursor tracks were selected\nfrom mentions in sources on EDM history such as [11,18].\nComplete individual tracks are used to form predictive\nmodels, which are then deployed to predict the Chicago\nhouse, Detroit techno and late 1980s pop corpuses.\nGiven these mainly synthesizer-ﬂavored precedents,\nDetroit makes the most whole-hearted embrace of the\ntechnologized future, and shows the greater link to James\nBrown to boot (though not Parliament, which links more to\npop, perhaps through the inclusion of Prince in that corpus\nin particular). The sanity checks show some consistency,\nwith two versions of Planet Rock both leading to similar\nresults, and two runs on the same Kano track also coming\nout with a similar ordering. The tests were repeated overthe whole set of songs, using a version of the predictive\nmodel with 10 states rather than 20 per vector quantiser,\nwithout any great divergence from the results presented\nhere, excepting Mothership Connection, Numbers and\nClear being assigned to techno, Magic Fly to Chicago, and\nProbl `emes D’Amour to pop. The greater vocal content\nin Kraftwerk’s The Model may be an explanation of its\nstronger link to certain elements of the Chicago house\ncorpus, or the link of the female vocal of I Feel Love\nthrough to pop.\nIndividual tracks across the corpus of 93 can be exam-\nined, to ﬁnd the most predicted and the most divergent\nfrom a model. For instance, for Kano’s It’s A War, the\nthree closest were Derrick May’s Spaced Out andNude\nPhoto, and Blake Baxter’s Ride Em Boy, all three from the\nDetroit corpus (as we might hope for this track’s reception\nhistory, though there are also aural links to Prince), and\nthe furthest away, in pop, Madonna’s Live to Tell and the\nBangles’ Eternal Flame andHazy Shade of Winter.\n6. DISCUSSION\nMusical inﬂuence is a complex mechanism; the assump-\ntion that strength of prediction is related to degree of inﬂu-\nence seems reasonable, but may hide other factors, such as\nindirect inﬂuence, common equipment and teaching tools\n(such as music technology magazines), social currents, and\neven independent co-creation of the same idea.\nThe audio content analysis used here cannot be claimed\nto be on a par with the musicologist’s ear. On the\nother hand, computer tools can point to useful currents\nof inquiry, and provide an alternative stimulus to musical\nhistorical and analytical investigation. Furthermore, it is\nreally worth exploring the musicological applications at an\nearly stage, to clarify the potential impact of such tools,\nand feedback their effectiveness.\nThe genre groups used in this study make categorical\nassumptions which can hide musical continuity. Whilst\ntheir construction was to answer some questions of inﬂu-\nence and overlap, the most interesting results relate more to\nthe scope of individual tracks. Future work may drop genre\nassumptions entirely, creating a predictive model from ev-\nery individual track, to assess every other; given pairwise\nsimilarity measures and chronological distance, multidi-\nmensional scaling may give insight into structure. It may\nalso be productive to consider rates of change per year, byModel Chicago Detroit Pop\nGiorgio Moroder From Here To\nEternity (1977)0.6551 0.5998 0.6562\nDonna Summer I Feel Love (1977) 0.6461 0.6824 0.6346\nKraftwerk The Model (1978) 0.6659 0.6929 0.7564\nKraftwerk Numbers (1981) 0.6146 0.5761 0.5723\nKraftwerk Trans-Europe Express\n(1977)0.5419 0.4784 0.5648\nCerrone Supernature (1977) 0.9124 0.8361 0.8573\nDee D. Jackson Automatic Lover\n(1978)0.6994 0.6878 0.7077\nSpace Magic Fly (1977) 0.6855 0.6993 0.6753\nSylvester You Make Me Feel\n(Mighty Real) (1978)0.8448 0.7551 0.8348\nLipps Inc. Funkytown (1979) 0.7209 0.6336 0.6734\nKano It’s A War (1980) 0.5009 0.4494 0.5877\nKano It’s A War (1980) 0.5974 0.5235 0.6465\nSoft Cell Tainted Love (1981) 0.8372 0.8085 0.8496\nDepeche Mode Get The Balance\nRight (1983)0.7934 0.727 0.7852\nAfrika Bambaataa et al. Planet\nRock (12” Vocal Version) (1982)0.8073 0.7501 0.7697\nAfrika Bambaataa et al. Planet\nRock (1982)0.7959 0.7562 0.7701\nJames Brown Funky Drummer Pts.\n1 and 2 (1970)0.6383 0.5262 0.6318\nParliament Mothership Connection\n(Star Child) (1975)0.8166 0.798 0.7881\nAlexander Robotnick Probl `emes\nD’Amour (1983)0.6395 0.6128 0.6496\nCybotron Enter (1983) 0.707 0.6833 0.6852\nCybotron Clear (1983) 0.5503 0.5552 0.5601\nCybotron Cosmic Cars (1983) 0.65 0.6387 0.657\nTable 4. Prediction of genres using models constructed from\nindividual tracks. The closest genre from each model is indicated\nin bold.\nconstructing a model on one year (or on other windows of\ntime) and testing how predictable the next is.\nA human study of similarity on this corpus would be\na useful follow-up, though one confound is the factor of\nrecognition; expert musicologists of EDM would recog-\nnise many of the Chicago and Detroit tracks immediately,\nand the corpus used here involves many famous works. Its\nhistorical importance, however, makes it a very interesting\ncorpus to work on, of great musicological relevance.\nIn future work we may consider extending out to a\nlarger-scale investigation of the history of electronic music.\nAlternative time series models, such as Hidden Markov\nModels, could be employed, avoiding vector quantisation\nsimpliﬁcations, and possibly using symmetrised distance\nmeasures such as the cross-likelihood discussed in [21].\nMore sophisticated statistical models of causality may also\nhelp to stretch the machinery for modeling inﬂuence [14].\n7. CONCLUSIONS\nThis paper presented a study of applying MIR techniques\nto probe the borderline of Chicago house and Detroit\ntechno. More generally, we related later 1980s works\nto 1960s to early 1980s precursors through a number of\nmethods. We saw that the house and techno genres overlap,\nand are not necessarily tightly deﬁned. Nonetheless, there\nwas some corroboration of Derrick May’s characterisation\nof Detroit techno as futuristic in its sound world, though\nless support for its separate funkiness; the disco and synthpop heritage is a strong link to the two styles. The study\npresents a template for future work over the same corpus,\nas reﬁned sound analysis models become available, and for\nmore general future audio-content driven examination of\nmusical inﬂuence.\n8. REFERENCES\n[1] J. J. Aucouturier and F. Pachet. Representing musical genre: A state\nof the art. Journal of New Music Research, 32(1):83–93, 2003.\n[2] R. Begleiter, R. El-Yaniv, and G. Yona. On prediction using variable\norder Markov models. Journal of Artiﬁcial Intelligence Research,\n22:385–421, 2004.\n[3] G. Bradski and A. Kaehler. Learning OpenCV: Computer Vision with\nthe OpenCV Library. OReilly Media, Sebastopol, CA, 2008.\n[4] Bill Brewster and Frank Broughton. Last Night a DJ Saved My Life.\nHeadline Book Publishing, London, 2006.\n[5] Nicholas J. Bryan and Ge Wang. Musical inﬂuence network analysis\nand rank of sample-based music. In Proceedings of the International\nSymposium on Music Information Retrieval, Miami, October 2011.\n[6] Mark J. Butler. Unlocking the Groove. Indiana University Press,\nIndiana, 2006.\n[7] Michael Casey, Remco Veltkamp, Masataka Goto, Marc Leman,\nChristophe Rhodes, and Malcolm Slaney. Content-based music\ninformation retrieval: Current directions and future challenges.\nProceedings of the IEEE, 96(4):668–696, April 2008.\n[8] Nick Collins. Computational analysis of musical inﬂuence: A\nmusicological case study using MIR tools. In Proceedings of the\nInternational Symposium on Music Information Retrieval, Utrecht,\nAugust 2010.\n[9] Stuart Cosgrove. Techno! the new dance sound of Detroit (liner\nnotes), 1988.\n[10] Laurent Daudet. Transients modelling by pruned wavelet trees.\nInProceedings of the International Computer Music Conference\n(ICMC), Havana, Cuba, 2001.\n[11] Peter Kirn, editor. Keyboard Presents the Evolution of Electronic\nDance Music. Backbeat Books, Montclair, NJ, 2011.\n[12] Kembrew McLeod. Genres, subgenres, sub-subgenres and more:\nMusical and social differentiation within electronic/dance music\ncommunities. Journal of Popular Music Studies, 13:59–75, 2001.\n[13] Marcus Pearce and Geraint Wiggins. Improved methods for statistical\nmodelling of monophonic music. Journal of New Music Research,\n33(4):367–385, 2004.\n[14] Judea Pearl. Causal inference in statistics: An overview. Statistics\nSurveys, 3:96–146, 2009.\n[15] Simon Reynolds. Generation Ecstasy: Into the World of Techno and\nRave Culture. Routledge, New York, 1999.\n[16] William A. Sethares. Tuning Timbre Spectrum Scale (2nd Edition).\nSpringer Verlag, Berlin, Germany, 2005.\n[17] Peter Shapiro. Turn the Beat Around: The Secret History of Disco.\nFaber and Faber Limited, London, 2005.\n[18] Dan Sicko. Techno Rebels: The Renegades of Electronic Funk (2nd\nEd.). Wayne State University Press, Detroit, MI, 2010.\n[19] D. Stowell and Plumbley. M. D. Adaptive whitening for improved\nreal-time audio onset detection. In Proceedings of the International\nComputer Music Conference (ICMC), Copenhagen, 2007.\n[20] A. Trujillo-Ortiz, R. Hernandez-Walls, K. Barba-Rojo, L. Cupul-\nMagana, and R.C. Zavala-Garcia. Andarksamtest:anderson-darling k-\nsample procedure to test the hypothesis that the populations of the\ndrawn groups are identical., 2007.\n[21] Tuomas Virtanen and Marko Hel ´en. Probabilistic model based\nsimilarity measures for audio query-by-example. In IEEE Workshop\non Applications of Signal Processing to Audio and Acoustics , New\nYork, 2007."
    },
    {
        "title": "Multivariate Autoregressive Mixture Models for Music Auto-Tagging.",
        "author": [
            "Emanuele Coviello",
            "Yonatan Vaizman",
            "Antoni B. Chan",
            "Gert R. G. Lanckriet"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416640",
        "url": "https://doi.org/10.5281/zenodo.1416640",
        "ee": "https://zenodo.org/records/1416640/files/CovielloVCL12.pdf",
        "abstract": "We propose the multivariate autoregressive model for con- tent based music auto-tagging. At the song level our ap- proach leverages the multivariate autoregressive mixture (ARM) model, a generative time-series model for audio, which assumes each feature vector in an audio fragment is a linear function of previous feature vectors. To tackle tag- model estimation, we propose an efficient hierarchical EM algorithm for ARMs (HEM-ARM), which summarizes the acoustic information common to the ARMs modeling the individual songs associated with a tag. We compare the ARM model with the recently proposed dynamic texture mixture (DTM) model. We hence investigate the relative merits of different modeling choices for music time-series: i) the flexibility of selecting higher memory order in ARM, ii) the capability of DTM to learn specific frequency ba- sis for each particular tag and iii) the effect of the hidden layer of the DT versus the time efficiency of learning and inference with fully observable AR components. Finally, we experiment with a support vector machine (SVM) ap- proach that classifies songs based on a kernel calculated on the frequency responses of the corresponding song ARMs. We show that the proposed approach outperforms SVMs trained on a different kernel function, based on a compet- ing generative model.",
        "zenodo_id": 1416640,
        "dblp_key": "conf/ismir/CovielloVCL12",
        "keywords": [
            "multivariate autoregressive model",
            "content based music auto-tagging",
            "generative time-series model",
            "audio feature vectors",
            "linear function",
            "efficient hierarchical EM algorithm",
            "acoustic information",
            "music time-series modeling",
            "support vector machine",
            "kernel function"
        ],
        "content": "MULTIVARIATE AUTOREGRESSIVE MIXTURE MODELS FOR MUSIC\nAUTO-TAGGING\nEmanuele Coviello\nUniversity of California,\nSan Diego\necoviell@ucsd.eduYonatan Vaizman\nUniversity of California,\nSan Diego\nyvaizman@eng.ucsd.eduAntoni B. Chan\nCity University\nof Hong Kong\nabchan@cityu.edu.hkGert R.G. Lanckriet\nUniversity of California,\nSan Diego\ngert@ece.ucsd.edu\nABSTRACT\nWe propose the multivariate autoregressive model for con-\ntent based music auto-tagging. At the song level our ap-\nproach leverages the multivariate autoregressive mixture\n(ARM ) model, a generative time-series model for audio,\nwhich assumes each feature vector in an audio fragment is\na linear function of previous feature vectors. To tackle tag-\nmodel estimation, we propose an efﬁcient hierarchical EM\nalgorithm for ARM s (HEM-ARM), which summarizes the\nacoustic information common to the ARM s modeling the\nindividual songs associated with a tag. We compare the\nARM model with the recently proposed dynamic texture\nmixture (DTM) model. We hence investigate the relative\nmerits of different modeling choices for music time-series:\ni) the ﬂexibility of selecting higher memory order in ARM,\nii) the capability of DTM to learn speciﬁc frequency ba-\nsis for each particular tag and iii) the effect of the hidden\nlayer of the DT versus the time efﬁciency of learning and\ninference with fully observable ARcomponents. Finally,\nwe experiment with a support vector machine (SVM) ap-\nproach that classiﬁes songs based on a kernel calculated on\nthe frequency responses of the corresponding song ARM s.\nWe show that the proposed approach outperforms SVMs\ntrained on a different kernel function, based on a compet-\ning generative model.\n1. INTRODUCTION\nBrowsing and discovery of new music can largely beneﬁt\nfrom semantic search engines for music, which represent\nsongs within a vocabulary of semantic tags, i.e., words or\nshort phrases describing songs’ attributes. By just typing\nthe desired tags as in a standard text search engines (e.g.,\nBing or Google), users can ﬁnd the music they desire.\nE.C. and Y .V . contributed equally to this work. Y .V . was at ICNC,\nHebrew University during this work. E.C., A.B.C. and G.R.G.L. ac-\nknowledge support from Google, Inc. E.C. and G.R.G.L. acknowledge\nsupport from Qualcomm, Inc, Yahoo!, Inc., the Hellman Fellowship Pro-\ngram, the Sloan Foundation, and NSF Grants CCF-0830535 and IIS-\n1054960. A.B.C. was supported by the Research Grants Council of\nthe Hong Kong Special Administrative Region, China [9041552 (CityU\n110610)]. This research was supported in part by the UCSD FWGrid\nProject, NSF Research Infrastructure Grant Number EIA-0303622.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.Historically, attempts to map songs onto a semantic vo-\ncabulary have initially relied on available metadata (e.g.\nartist names, genre annotation, critical reviews), or man-\nual labeling from expert human annotators and social net-\nworks. More recently, distributed human computation gam-\nes, such as TagATune [15] and HerdIt [2], have attempted\nto scale up manual labeling to larger collections by re-\ncruiting non-expert users through engaging or rewarding\ngames. However, these efforts have so far covered only\na small portion of the songs available in modern music\ncollections.1This motivates the development of content-\nbased auto-tagging systems, i.e., intelligent algorithms that,\nby analyzing and understanding the acoustic content of\nsongs, can automatically index them with semantic tags.\n1.1 Related work\nA large number of content-based auto-taggers are trained\non a database of songs annotated with respect to a seman-\ntic vocabulary following a common scheme. First, a time\nseries of low-level spectral features (e.g., Mel Frequency\nCepstral Coefﬁcients (MFCCs)) is extracted from each song\nin the database. Then, for each tag, a representative statis-\ntical model is ﬁne tuned to capture the most predictive pat-\nterns common to the songs annotated with that tag. Once\na new song is available, the auto-tagger uses the learned\ntag-models to process its low-level features and produces a\nvector of tag-afﬁnities. The tag-afﬁnities are then mapped\nonto a semantic multinomial (SMN), which represents the\nsong within the semantic vocabulary.\nA variety of auto-taggers, based on either generative\nmodels [13, 19, 23, 24] or discriminative models [4, 9, 11,\n16, 21, 26], rely on a Bag-of-Features (BoF) representa-\ntion of the spectral content of songs, which ignores tem-\nporal dynamics by treating all feature vectors as indepen-\ndent. While augmenting the spectral features with their\nﬁrst and second instantaneous derivatives has represented a\ncommon choice to enrich the BoF representation with tem-\nporal information (e.g. [3, 23]), more principled solutions\nhave been implemented in recently proposed auto-taggers.\nThe dynamic texture mixture (DTM) treats short fragments\nof audio as the output of a linear dynamical system [7].\nThe multi-scale learning algorithm in [12] leverages sev-\neral pooling functions for summarization of the features\nover time. The Bag-of-Systems (BoS) approach represents\n1Pandora annotated catalog and TagATune labeled clips represent less\nthan5%and0:15% of the iTunes’ collection, respectively.songs with a codebook of time-series models [10]. The\nmultivariate autoregressive (AR) model was used in [17]\nin a semi-parametric approach for genre classiﬁcation of\nshort musical snippets. In [20] various methods for tem-\nporal integration, including the AR model, were examined\nfor musical instrument classiﬁcation.\n1.2 Original contribution\nIn this paper we introduce the autoregressive mixture (ARM)\nmodel [1] for automatic music annotation and retrieval. We\nﬁrst model each song as an ARM, estimated from a collec-\ntion of audio-fragments extracted from the song. Note that\nthis is different from estimating single AR models from\nindividual audio clips as done in [17], since each mixture\ncomponent of a song-level ARM models the music content\nofseveral (perceptually similar) audio fragments.\nIn order to model tag-level distributions as ARMs, we\npropose a novel efﬁcient hierarchical expectation maxi-\nmization algorithm for ARMs (HEM-ARM). Starting from\nall the song-ARMs that are relevant for a speciﬁc tag, the\nproposed algorithm summarizes the common music con-\ntent by clustering similar AR components together, and\nlearning a tag-ARM model with fewer components. We\ncompare our HEM-ARM with previous auto-taggers that\nused GMMs [23] and DTMs [7], to model tag-level dis-\ntribution, in tandem with an efﬁcient HEM algorithm for\nlearning. In particular, we obtain that HEM-DTM gener-\nally performs better than HEM-ARM (e.g., the annotation\nF-scores are 0:264, 0:254, respectively). However, rela-\ntive to HEM-DTM, our HEM-ARM has signiﬁcantly lower\ntime requirements, both for training (two orders of magni-\ntude) and for annotation (one order of magnitude). These\nresults are explained by the differences in the graphical\nstructures of the models. The DT model has an observed\nlayer (which models the spectral content) and a hidden\nlayer (that encodes the temporal dynamics). As a conse-\nquence, using DTMs can learn different frequency bases\nthat better adapt to speciﬁc tags, but requires marginaliza-\ntion over the hidden variables — and hence delays — at\neach training iteration and for inference at annotation. On\nthe opposite, the AR is a fully observable model. Hence,\ntraining and annotation can be implemented efﬁciently by\ncomputing sufﬁcient statistics for each song a single time.\nIn addition, once songs are modeled with ARMs, we\ninvestigate a kernel-SVM method upon these song-ARMs\nfor semantic retrieval, similar to the work done in [3] over\nGMMs and in [17] over single ARs. We test several kernel\nfunctions, some of which represent each song by the quan-\ntized frequency responses (QFR) of its AR components.\nThe remainder of this paper is organized as follows. In\nsection 2 we present the autoregressive (mixture) model,\nand in section 3 we derive the hierarchical EM algorithm\nfor ARMs. In Section 4 we present our kernel-SVM ap-\nproach. In Section 5 we report our experiments.\n2. THE AUTOREGRESSIVE MIXTURE MODEL\nIn this section we present the autoregressive (AR) model\nand the autoregressive mixture (ARM) model for music\ntime series.2.1 The AR model\nA multivariate autoregressive (AR) model is a generative\ntime-series model for audio fragments. Given a time series\nofT d\u0000dimensional feature vectors x1:T2Rd\u0002T, the AR\nmodel assumes each audio feature xtat timetis a linear\ncombination of the previous paudio features. Speciﬁcally,\nthe AR model is described by the equation\nxt=pX\nj=1Ajxt\u0000j+\u0017t (1)\nwherefAjgp\nj=1areptransition matrices of dimension d\u0002\nd.\u0017tis a driving noise process and is i.i.d. zero-mean\nGaussian distributed, i.e., \u0017t\u0018N(0;Q ), whereQ2Rd\u0002d\nis a covariance matrix. The initial condition is speciﬁed by\nx1\u0018N (\u0016;S ), whereS2Rd\u0002dis a covariance matrix.\nWe can express (1) in a vectorial form:\nxt=eAxt\u00001\nt\u0000p+\u0017t (2)\nwhereeA= [A1:::Ap]2Rd\u0002dpandxb\na= [x0\nb:::x0\na]02\nRdp\u00021. Note that, for convenience, we assume xt= 0for\nt2f\u0000p + 1;:::; 0g, and hence assume that x1triggered\nthe generation of the whole time series. An AR model is\nhence parametrized by \u0002 =f\u0016;S;eA;Qg. The likelihood\nof a sequence x1:Tis\np(x1:Tj\u0002) =N(x1j\u0016;S)TY\nt=2N(xtjpX\nj=1Ajxt\u0000j;Q)(3)\nwhereN(\u0001j\u0016;\u0006)is the pdf of a Gaussian distribution with\nmean\u0016and covariance matrix \u0006.\nThe parameters of an AR model can be estimated from a\ntime-seriesx1:Twith various optimization criteria [17,18].\n2.2 The ARM model\nAn ARM model treats a group of audio fragments as sam-\nples fromKAR models. Speciﬁcally, for a given sequence,\nan assignment variable z\u0018categorical(\u0019 1;\u0001\u0001\u0001\u0019K)se-\nlects one of the KAR models, where the ithAR model is\nselected with probability \u0019i. Each mixture component is\nspeciﬁed by the parameters \u0002i=f\u0016i;Si;eAi;Qig, and the\nARM model is speciﬁed by \u0002 =f\u0019i;\u0002igK\ni=1. Whereas a\nsingle AR model sufﬁces to describe an individual audio\nfragment, the ARM model is a more appropriate modeling\nchoice for an entire song. This is motivated by the ob-\nservation that a song usually shows signiﬁcant structural\nvariations within its duration, and hence multiple AR com-\nponents are necessary to model the heterogeneous sections.\nThe likelihood of an audio fragment x1:Tunder an ARM\nmodel is\np(x1:Tj\u0002) =KX\ni=1\u0019ip(x1:Tjz=i;\u0002i); (4)\nwhere the likelihood of x1:Tunder theithAR component\np(x1:Tjz=i;\u0002i)is given by (3).The parameters of an ARM model can be estimated\nfrom a collection of audio-fragments using the expecta-\ntion maximization (EM) algorithm [8], which is an itera-\ntive procedure that alternates between estimating the as-\nsignment variables given the current estimate of the pa-\nrameters, and re-estimating the parameters based on the\nestimated assignment variables.\n3. THE HEM ALGORITHM FOR ARM MODELS\nIn this paper we proposed to model tag distributions as\nARM models. One way to estimate a tag-level ARM model\nis to run the EM algorithm directly on all the audio frag-\nments extracted from the relevant songs. However, this\napproach would require excessive memory and computa-\ntion time, to store all the input audio-sequences in RAM\nand to compute their likelihood at each iteration. In or-\nder to avoid this computational bottleneck, we propose a\nnovel hierarchical EM algorithm for ARM models (HEM-\nARM), which allows to learn ARM models using an efﬁ-\ncient hierarchical estimation procedure. In a ﬁrst stage, in-\ntermediate ARM models are estimated in parallel for each\nsong, using the EM algorithm for ARMs on the song’s au-\ndio fragments. Then, the HEM-ARM algorithm estimates\nthe ﬁnal model by summarizing the common information\nrepresented in the relevant song-ARMs. This is achieved\nby aggregating together all the relevant song-ARMs into a\nsingle big ARM model, and clustering similar AR models\ntogether to form the ﬁnal tag-level ARM model.\nAt a high level, the HEM algorithm consists in max-\nimum likelihood estimation of the ARM tag model from\nvirtual samples distributed according to the song ARM mod-\nels. However, since using the virtual samples can be ap-\nproximated with a marginalization over the song ARM dis-\ntribution (for the law of large numbers, see (8)), the estima-\ntion is carried out in an efﬁcient manner that requires only\nknowledge of the parameters of the song models without\nthe need of generating actual samples. The HEM algorithm\nwas originally proposed by Vasconcelos and Lipmann [25]\nto reduce a GMM with a large number of mixture com-\nponents to a compact GMM with fewer components, and\nextended to DTMs by Chan et al. [5]. The HEM algo-\nrithm has been successfully applied to the estimation of\nGMM tag-distribution [23] and DTM tag-distribution [7].\nWe now derive the HEM algorithm for ARMs.\n3.1 Derivation of the HEM for ARMs\nFormally, let \u0002(s)=f\u0019(s)\ni;\u0002(s)\nigK(s)\ni=1 be an ARM model\nwithK(s)components, which pools together the ARM\nmodels of all the songs relevant for a tag. The goal of the\nHEM-ARM algorithm is to learn a tag-level ARM model\n\u0002(t)=f\u0019(t)\nj;\u0002(t)\njgK(t)\nj=1with fewer components\n(i.e.,K(t)< K(s)), that represents \u0002(s)well. The likeli-\nhood of the tag ARM \u0002(t)is given by (4).\nThe HEM algorithm uses a set of Nvirtual samples\ngenerated from the base model \u0002(s), where theNi=N\u0019(s)\ni\nsamplesXi=fx(i;m)\n1:\u001cgNi\nm=1 are from the ithcomponent,\ni.e.,x(i;m)\n1:\u001c\u0018\u0002(s)\ni. We assume that samples within each\nXiare assigned to the same component of the tag model\u0002(t), and we denote the entire set of virtual samples with\nX=fXigK(s)\ni=1.\nThe log likelihood of the incomplete data under \u0002(t)is\nlogp(Xj\u0002(t)) = logK(s)Y\ni=1p(Xij\u0002(t))\n= logK(s)Y\ni=1K(t)X\nj=1\u0019(t)\njp(Xij\u0002(t)\nj):(5)\nThe HEM algorithm consists of the maximum likelihood\nestimation of the parameters of \u0002(t)from (5). Since (5) in-\nvolves marginalizing over the hidden assignment variables\nz(s)\ni2f1;:::;K(t)g, its maximization can be solved with\nthe EM algorithm. Hence, we introduce an indicator vari-\nable zi;jfor when the virtual audio sample set Xiis as-\nsigned to the jthcomponent of \u0002(t), i.e., whenz(s)\ni=j.\nThe complete data log-likelihood is then:\nlogp(X;Zj\u0002(t)) =\n=K(s)X\ni=1K(t)X\nj=1zi;jlog\u0019(t)\nj+zi;jlogp(Xij\u0002(t)\nj)(6)\nTheQ-function is obtained by taking the conditional\nexpectation of (6) with respect to Z, and the dependency\non the virtual samples is removed by using the law of large\nnumbers, i.e.,\nlogp(Xij\u0002(t)\nj) =Ni1\nNiNiX\nm=1logp(x(i;m)\n1:\u001cj\u0002(t)\nj) (7)\n\u0019NiEx1:\u001cj\u0002(s)\nih\nlogp(x1:\u001cj\u0002(t)\nj)i\n:(8)\nNote that (8) can be computed using the chain rule of the\nexpected log-likelihood and (1) to break the expectation\nEx1:\u001cj\u0002(s)\nih\nlogp(x1:\u001cj\u0002(t)\nj)i\n= (9)\n=\u001cX\nt=1Ex1:tj\u0002(s)\nih\nlogp(xtjx1:t\u00001;\u0002(t)\nj)i\n(10)\n=\u001cX\nt=1Ex1:tj\u0002(s)\nih\nlogp(xtjxt\u0000p:t\u00001;\u0002(t)\nj)i\n(11)\n=\u001cX\nt=1Ex1:t\u00001j\u0002(s)\nih\nExtjxt\u0000p:t\u00001;\u0002(s)\nih\n;\u0002(t)\nj\nlogp(xtjxt\u0000p:t\u00001;\u0002(t)\nj)ii(12)\nThe inner expectation in (12) is the expected log-likelihood\nof a Gaussian, and its closed form solution depends on the\nﬁrst and second order statistics of xt\u0000p;t\u00001\u0018\u0002(s)\ni. The\nouter expectation involves the computation of the expected\nﬁrst and second order statistics of \u0002(s)\ni, which can be car-\nried out with the recursion presented in Algorithm 1. Note\nthat, since the AR model \u0002(t)\njhas no hidden variables, the\ncomputation of the expected sufﬁcient statistics in Algo-\nrithm 1 is independent of \u0002(t)\nj, and hence needs to be exe-\ncuted only once for each input component \u0002(s)\ni.Algorithm 1 Expected sufﬁcient statistics\n1:Input: song-level AR model \u0002(s)\ni=f\u0016;S;eA;Qg, length of\nvirtual samples \u001c.\n2:Compute expected sufﬁcient statistics for t= 1;:::;\u001c\u00001:\n~E(i)\n1=Ex1\u0002(s)\ni[x1x0\n1] =\u0016\u00160+S\n^E(i)\n1=Ex\u0000p+1:1 \u0002(s)\nih\nx1\n\u0000p+1x1\n\u0000p+10i\n=\n=\u0014~E(i)\n1 0d\u0002(d\u00001)p\n0(d\u00001)p\u0002d 0(d\u00001)p\u0002(d\u00001)p\u0015\nFort= 1;:::;\u001c\u00001\n^E(i)\nt=Ex1:tj\u0002(s)\nih\nxt\nt\u0000p+1xt\nt\u0000p+10i\n=\"\neA^E(i)\nt\u00001eA0+Q A ^E(i)\nt\u00001\n^E(i)\nt\u00001eA0 ^E(i)\nt\u00001#\n(1:dp;1:dp)\nEndfor\n3:Compute expected sufﬁcient statistics:\n^E(i)=P\u001c\u00001\nt=1^E(i)\nt(13)\n4:Output: expected sufﬁcient statistics: ^E(i).\nIf hidden variables are present (which is the case for the\nDT components of the DTM model, but not for the AR\nmodel), computing the expected sufﬁcient statistics of a\nsong component \u0002(s)\niinvolves marginalizing over the hid-\nden variables of \u0002(t)\nj, and hence needs to be repeated at\nevery iteration for each j= 1;:::;K(t).\nThe E-step of the HEM consists of computing of the ex-\npected sufﬁcient statistics in Algorithm 1, the assignments\nvariables in (14) and (15), and the cumulative expected\nsufﬁcient statistics in (16). The M-step maximizes the Q-\nfunction with respect to \u0002(t), giving the updates in (17).\nThe full HEM-ARM scheme is presented in Algorithm 2.\n4. KERNEL-SVM APPROACH\nWe then used a semi-parametric approach that leverages\nthe ARM model at the song level, and kernel support vector\nmachine (SVM) for retrieval. In particular, we ﬁrst model\neach song as an ARM using the EM algorithm. Then, for\neach tag, we learn a binary SVM classiﬁer over the train\nset, based on a notion of similarity between ARM models\ndeﬁned in terms on their proximity in parameter space. Fi-\nnally, following [3], we use the SVMs’ decision values as\nthe relevance of a song for a tag, and use it for retrieval of\ntest songs based on one-tag queries.\nSince the AR parameters lie on a non-linear manifold,\nna¨ıvely treating them as Euclidean vectors would not nec-\nessary produce a correct similarity score. Hence, in the\nremainder of this section, we present several kernel func-\ntions based on more appropriate similarity scores between\nautoregressive (mixture) models. In previous work, Meng\nand Shawe-Taylor [17] specialize the Probability Product\nKernel [14] to the AR case, which depends non-linearly on\nthe AR parameters, and is deﬁne as:\nKAR(\u0002a;\u0002b) =R\nx1:p(p(x 1:pj\u0002a)p(x 1:pj\u0002b))\u001a; (18)\nwhere\u001a= 0:5corresponds to the Battaccharyya afﬁnity.\nSince a song-ARM is associated with several AR compo-Algorithm 2 HEM algorithm for ARM\n1:Input: combined song-level ARM f\u0019(s)\ni;\u0002(s)\nigK(s)\ni=1, num-\nber of virtual samples N.\n2:Compute cumulative expected sufﬁcient statistics ^E(i)for\neach\u0002(s)\niusing Algorithm 1\n3:Initialize tag-level ARM, f\u0019(t)\nj;\u0002(t)\njgK(t)\nj=1.\n4:repeat\n5:fE-stepg\n6: Compute expected log-likelihood for each \u0002(s)\niand\u0002(t)\nj:\n`ijj=Ex1:\u001cj\u0002(s)\ni[logp(x1:\u001cj\u0002(t)\nj)]\n=\u0000d\u001c\n2log 2\u0019\u00001\n2logjS(t)\njj\n\u00001\n2traceS(t)\nj\u00001[S(s)\ni+ (\u0016(t)\nj\u0000\u0016(s)\ni)0(\u0016(t)\nj\u0000\u0016(s)\ni)]\n\u0000\u001c\u00001\n2traceQ(t)\nj\u00001Q(s)\ni\u0000\u001c\u00001\n2logjQ(t)\njj\n\u00001\n2trace[Q(t)\nj\u00001(eA(t)\nj\u0000eA(s)\ni)^E(i)(eA(t)\nj\u0000eA(s)\ni)0]\n7: Compute assignment probability and weighting:\n^zi;j=\u0019(t)\njexp\u0000\nNi`ijj\u0001\nPK(t)\nj0=1\u0019(t)\nj0exp\u0000\nNi`ijj0\u0001 (14)\n^wi;j=^zi;jNi=^zi;j\u0019(s)\niN (15)\n8: Computed aggregated expectations for each ^\u0002(t)\nj:\n^Nj=P\ni^zi;j; ^Mj=P\ni^wi;j;\n^Sj=P\ni^wi;j[S(s)\ni+\u0016(s)\ni(\u0016(s)\ni)0] ^mj=P\ni^wi;j\u0016(s)\ni\n^Vj=P\ni^wi;jA(s)\ni^E(i)(eA(s)\ni)0 ^Pj=P\ni^wi;j^E(i)\n^Rj=P\ni^wi;j^E(i)(eA(s)\ni)0 ^Qj=P\ni^wi;jQ(s)\ni(16)\n9:fM-stepg\n10: Recompute parameters for each component ^\u0002(t)\nj:\neA\u0003\nj=^R0\nj^P\u00001\njQ\u0003\nj=1\n(\u001c\u00001)^Mj(^Vi\u0000A\u0003\nj^Rj+^Qj);\n\u0016\u0003\nj=1\n^Mj^mj; S\u0003\nj=1\n^Mj^Sj\u0000\u0016\u0003\nj(\u0016\u0003\nj)0;\n\u0019\u0003\nj=PK(s)\ni=1^zi;j\nK(s):(17)\n11:until convergence\n12:Output: tag-level ARM f\u0019(t)\nj;\u0002(t)\njgK(t)\nj=1.\nnents, for retrieval we collect a decision value for each AR\ncomponent, and then rank the songs according to the aver-\nage of the corresponding decision values (PPK-AR). Note\nthat we compute PPK between individual AR components\nof the song-ARMs. This is different from [17], which uses\nsingle ARs on individual audio snippets.\nIn addition, we experiment SVM classiﬁcation in tan-\ndem with a probability product kernel between auotore-\ngressive mixture models (PPK-ARM). Following an ap-\nproximation by Jebara et al. [14], the PPK-ARM can be\ncomputed from the PPK between individual components\nas\nKARM(\u0002(1);\u0002(2)) =\nPKs\na=1PKs\nb=1(\u0019(1)\na\u0019(2)\nb)\u001aKAR(\u0002(1)\na;\u0002(2)\nb):(19)\nWe ﬁnally propose a novel descriptor of AR models\nbased on their frequency responses, and compute a ker-\nnel between these descriptors. Since an AR is a linear time\ninvariant (LTI) system, its dynamics can be characterizedby a transfer function deﬁned as:\nH(s) = (Id\u0000Pp\nj=1Ajs\u0000j)\u000012Cd\u0002d(20)\nwheres2Cis a complex number and Idis theddimen-\nsional identity matrix. The transfer function describes the\ncross inﬂuences of each pair of components of the audio\nfeature vectors. In particular, we sample the transfer func-\ntion at 200equally spaced points on the unit circle, and\nthen sum the the absolute values of these matrices over\n30linearly spaced frequency bins, to get a representation\nof the system’s frequency response. By concatenating the\nAR’s\u0016parameter and the log values of these 30 frequency\nresponse matrices, we get a descriptor \u00012R(d+30d2)\u00021,\nwhich we call quantized frequency response (QFR). Fi-\nnally, we use a SVM over QFRs based on cosine-similarity\n(CS) kernel and radial basis function (RBF) kernel.2\n5. EXPERIMENTS\n5.1 Data\nWe performed automatic music annotation on the CAL500\ndataset (details in [23] and references therein), which is a\ncollection of 502 popular Western songs by as many differ-\nent artists, and provides binary annotations with respect to\na vocabulary of semantic tags. In our experiments we con-\nsider the 97tags associated to at least 30songs in CAL500\n(11genre, 14instrumentation, 25acoustic quality, 6vocal\ncharacteristics, 35mood and 6usage tags).\nThe acoustic content of a song (resampled at 22;050Hz )\nis represented by computing a time-series of 34-bin Mel-\nfrequency spectral (MFS) features, extracted over half over-\nlapping windows of 92msec of audio signal, i.e., every \u0018\n46msec. Following the insight in recent work of Hamel et\nal. [12], MFS features where further projected on the ﬁrst\nd= 20 principal components, which we estimated over\nthe MFSs collected from the 10;870songs in the CAL10K\ndataset [22].\nSong level ARMs were learned with K= 4 compo-\nnents and memory of p= 5 steps, from a dense sampling\nof audio fragments of length T= 125 (i.e., approximately\n6s), extracted with 80% overlap.\n5.2 Results with HEM-ARM\nFor each tag, all the relevant song ARMs were pooled to-\ngether to form a big ARM, and a tag-level ARM with\nK(t)= 8 components was learned with the HEM-ARM\nalgorithm (with N= 1000 virtual samples of length \u001c=\n10). To reduce the effects of low likelihood in high di-\nmension, for annotation we smooth the likelihood (3) by\nT\u0001d\u0001p. We compare our HEM-ARM with hierarchically\ntrained Gaussian mixture models (HEM-GMM) [23] and\ndynamic texture mixture models (HEM-DTM) [7].\nOn the test set, a novel test song is annotated with the 10\nmost likely tags, corresponding to the peaks in its semantic\nmultinomial. Retrieval given a one tag query involves rank\n2The CS kernel is deﬁned as K(a;b) =a0b=p\njjajj2jjbjj2. The RBF\nkernel is deﬁnes as K(a;b) = expf\u0000ka\u0000bk2\n2=\u001bg. We set the band-\nwidth\u001bof the RBF kernel to the descriptor dimension dim(\u0001).annotation retrieval time\nP R F AROC MAP P@10 train test\nHEM-ARM 0.468 0.203 0.254 0.696 0.421 0.412 198m 41m\nHEM-DTM 0.446 0.217 0.264 0.708 0.446 0.460 424h 482m\nHEM-GMM 0.474 0.205 0.213 0.686 0.417 0.425 41m 38m\nTable 1. Annotation and retrieval on CAL500, for HEM-\nARM,\nHEM-DTM and HEM-GMM.\nordering all songs with respect to the corresponding entry\nin their semantic multinomials. Annotation is measured\nwith average per-tag precision (P), recall (R), and f-score\n(F). Retrieval is measured by per-tag area under the ROC\n(AROC), mean average precision (MAP), and precision at\nthe ﬁrst 10retrieved objects (P@10). Refer to [23] for a\ndetailed deﬁnition of the metrics. All reported metrics are\nthe result of 5 fold-cross validation.\nIn Table 1 we report annotation and retrieval results for\nHEM-ARM, HEM-DTM and HEM-GMM. In addition, we\nregister the total time for the training stage, which consist\nin the estimation of the 97tag models over the 5folds (and\nalso includes the estimation of the 502song-level models),\nas well as for the test stage, i.e., the automatic-annotation\nof the 502songs with the 97tags.\nFrom Table 1 we note that the advantages of the pro-\nposed HEM-ARM relative to HEM-DTM are in terms of\ncomputation efﬁciency. While HEM-DTM performs better\nthan HEM-ARM on each metric (except on annotation pre-\ncision where HEM-ARM is better), HEM-ARM has sig-\nniﬁcantly lower time requirements. Speciﬁcally, the train-\ning time for our HEM-ARM is two orders of magnitude\nlower than that for HEM-DTM. Similarly, our auto-tagger\nrequires approximately 42minutes for the test-stage, while\nthe auto-tagger based on DTMs requires 482 minutes to\naccomplish the same task. These results are explained by\ncomparing the graphical structures of the AR and DT mod-\nels. While the AR is a fully observable model, the DT con-\nsists of an observed layer, which model the spectral con-\ntent, and a hidden layer that encodes the temporal dynam-\nics. Hence, DTMs have the advantage of learning different\nfrequency basis to best represent speciﬁc tags [7]. How-\never, the computation of the (expected) sufﬁcient statistics\nwith respect to each DT component requires marginaliza-\ntion of the hidden variables (see [5]). Hence, during train-\ning, it needs to be executed at each iteration of the learning\nalgorithms for each input datum (i.e., audio-fragments for\nEM, and DTs for HEM) and for each individual compo-\nnent of the learned model; during annotation, it needs to\nbe repeated for each audio-fragment and each DT compo-\nnent of the tag models. On the opposite, the corresponding\nstatistics for ARMs involve no marginalization of hidden\nvariables. Hence, during training, they need to be com-\nputed only a single time for each input datum (i.e., audio-\nfragments for EM, and ARs for HEM). In addition, during\nannotation, the sufﬁcient statistics can be collected a single\ntime for each song.\nFinally, HEM-ARM performs favorably relative to HEM-\nGMM (which does not model temporal dynamics), while\nstill requiring limited computation times. Since learning\nand inference are performed efﬁciently, HEM-ARM can\nleverage higher order memories to model temporal dynam-p 1 2 3 4 5 6 7 8 9\nF-score 0.234 0.247 0.252 0.255 0.254 0.245 0.244 0.242 0.237\nAROC 0.650 0.672 0.686 0.693 0.696 0.695 0.694 0.695 0.694\nTable 2. Annotation (F-score) and retrieval (AROC) per\n-\nformance of HEM-ARM, for different memories p2[1:9].\nkernel AROC MAP P@10 train test\nARM basedPPK-ARM 0.717 0.448 0.459 233m 194m\nPPK-AR 0.727 0.463 0.484 287m 194m\nQFR-CS 0.717 0.461 0.479 125m 65m\nQFR-RBF 0.723 0.469 0.488 137m 74m\nGMM-PPK [3] 0.696 0.436 0.454\nMFCC-PPK-AR [17] 0.706 0.447 0.463\nTable 3. Retrieval for the kernel-SVM approach. Includ-\ning train and test times\nics, without incurring in large delays. In particular, in Table\n2 we plot annotation (F-score) and retrieval (AROC) per-\nformance as a a function of the memory pof the AR mod-\nels. Performance are fairly stable for p= 4; 5;6. Shorter\nmemories (e.g., p= 1; 2;3) do not sufﬁces to capture the\ninteresting dynamics, while too large values deteriorate an-\nnotation performance.\n5.3 Results with kernel-SVM.\nWe implemented the kernel-SVM approach as described\nin Section 4. In particular, we learned song-ARMs with\nK= 4 components and memory p= 5, estimated from\nthed= 20 dimensional PCA-MFS features. We then\ncomputed the QFR-CS and QFR-RBF kernels based on\nthe QFR descriptors, the PPK kernel between ARM (PPK-\nARM), and the PPK kernel between individual AR com-\nponents (PPK-AR). For comparison, we also considered\nPPK similarity between song-GMMs estimated on MFCC\nfeatures [3] (GMM-PPK) and PPK similarity between sin-\ngle AR models estimated on the MFCC features of entire\nsongs as proposed in [17] (MFCC-PPK-AR). We used the\nLibSVM software package [6] for the SVM, with all pa-\nrameters selected using validation on the training set.\nRetrieval scores are reported in Table 3, and are result\nof 5-fold cross validation. We note that these results are\ngenerally superior to those in Table 1, since the SVM is a\ndiscriminative algorithm and hence tends to be more robust\non strongly labeled datasets such as CAL500. In particu-\nlar, the best performance was registered with the QFR-RBF\nand PPK-AR systems (score differences between them are\nnot statistically signiﬁcant). In addition, PPK similarity\non ARMs (PPK-ARM) proves less performant, suggesting\nthat the approximation in (19) may be not enough accurate\nfor our task. Finally, PPK similarity on GMM performs the\nworst, since it does not leverage termporal dynamics, and\nMFCC-AR-PPK, which doesn’t leverage mixtures, is also\nsigniﬁcantly behind.\n6. DISCUSSION\nIn this paper we have proposed the ARM model for mu-\nsic auto-tagging. We have derived a hierarchical EM algo-\nrithm for efﬁciently learning tag ARMs. We have showed\nthat our HEM-ARM can estimate tag models signiﬁcantly\nmore efﬁciently than HEM-DTM, at the price of a small re-duction in performance. We have also successfully tested\na kernel-SVM approach based on several similarity func-\ntions based on the ARM model.\n7. REFERENCES\n[1] A. Agarwal and B. Triggs. Tracking articulated motion using a mixture of au-\ntoregressive models. In ECCV - Lecture notes in computer science, pages 54–\n65, 2004.\n[2] L. Barrington, D. O’Malley, D. Turnbull, and G. Lanckriet. User-centered de-\nsign of a social game to tag music. In Proceedings of the ACM SIGKDD Work-\nshop on Human Computation, pages 7–10. ACM, 2009.\n[3] L. Barrington, M. Yazdani, D. Turnbull, and G. Lanckriet. Combining feature\nkernels for semantic music retrieval. Proc. ISMIR 2008, pages 723–728, 2008.\n[4] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum distances in high-\ndimensional musical spaces. IEEE Transactions on Audio, Speech and Lan-\nguage Processing, 16(5):1015–1028, 2008.\n[5] A.B. Chan, E. Coviello, and G. Lanckriet. Clustering dynamic textures with the\nhierarchical EM algorithm. In Proc. IEEE CVPR, 2010.\n[6] C.C. Chang and C.J. Lin. Libsvm: a library for support vector machines. ACM\nTransactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.\n[7] E. Coviello, A. Chan, and G. Lanckriet. Time Series Models for Semantic Music\nAnnotation. Audio, Speech, and Language Processing, IEEE Transactions on,\n19(5):1343–1359, July 2011.\n[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from in-\ncomplete data via the EM algorithm. Journal of the Royal Statistical Society B,\n39:1–38, 1977.\n[9] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Automatic generation\nof social tags for music recommendation. In Advances in Neural Information\nProcessing Systems, 2007.\n[10] K. Ellis, E. Coviello, and G. Lanckriet. Semantic annotation and retrieval of\nmusic using a bag of systems representation. In ISMIR, 2011.\n[11] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer. Probabilistic combination of\nfeatures for music classiﬁcation. In Proc. ISMIR, pages 111–114, 2006.\n[12] P. Hamel, S. Lemieux, Y . Bengio, and D. Eck. Temporal pooling and multiscale\nlearning for automatic annotation and ranking of music audio. ISMIR, 2011.\n[13] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple probabilistic model\nfor tagging music. In Proc. ISMIR, pages 369–374, 2009.\n[14] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. The Journal\nof Machine Learning Research, 5:819–844, 2004.\n[15] E. Law and L. V on Ahn. Input-agreement: a new mechanism for collecting\ndata using human computation games. In Proceedings of the 27th international\nconference on Human factors in computing systems, pages 1197–1206, 2009.\n[16] M.I. Mandel and D.P.W. Ellis. Multiple-instance learning for music information\nretrieval. In Proc. ISMIR, pages 577–582, 2008.\n[17] A. Meng and J. Shawe-Taylor. An investigation of feature models for music\ngenre classiﬁcation using the support vector classiﬁer. In Proc. ISMIR, pages\n604–609, 2005.\n[18] A. Neumaier and T. Schneider. Estimation of parameters and eigenmodes of\nmultivariate autoregressive models. ACM Transactions on Mathematical Soft-\nware (TOMS), 27(1):27–57, 2001.\n[19] J. Reed and C.H. Lee. A study on music genre classiﬁcation based on universal\nacoustic models. In Proc. ISMIR, pages 89–94, 2006.\n[20] C. Joder S. Essid and G. Richard. Temporal integration for audio classiﬁcation\nwith application to musical instrument classiﬁcation. IEEE Transactions on Au-\ndio, Speech, and Language Processing, 17(1):174–186, 2009.\n[21] M. Slaney, K. Weinberger, and W. White. Learning a metric for music similarity.\nInProc. ISMIR, pages 313–318, 2008.\n[22] Derek Tingle, Youngmoo E. Kim, and Douglas Turnbull. Exploring automatic\nmusic annotation with “acoustically-objectiv” tags. In Proc. MIR, New York,\nNY , USA, 2010.\n[23] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Semantic annotation\nand retrieval of music and sound effects. IEEE Transactions on Audio, Speech\nand Language Processing, 16(2):467–476, February 2008.\n[24] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of audio signals. IEEE\nTransactions on speech and audio processing, 10(5):293–302, 2002.\n[25] N. Vasconcelos and A. Lippman. Learning mixture hierarchies. In Advances in\nNeural Information Processing Systems, pages 606–612, 1998.\n[26] B. Whitman and D. Ellis. Automatic record reviews. In Proc. ISMIR, 2004."
    },
    {
        "title": "The Impact of MIREX on Scholarly Research (2005 - 2010).",
        "author": [
            "Sally Jo Cunningham",
            "David Bainbridge 0001",
            "J. Stephen Downie"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418217",
        "url": "https://doi.org/10.5281/zenodo.1418217",
        "ee": "https://zenodo.org/records/1418217/files/CunninghamBD12.pdf",
        "abstract": "This paper explores the impact of the MIREX (Music In- formation Retrieval Evaluation eXchange) evaluation ini- tiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended ab- stracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication ven- ues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature.",
        "zenodo_id": 1418217,
        "dblp_key": "conf/ismir/CunninghamBD12",
        "keywords": [
            "impact",
            "MIREX",
            "scholarly research",
            "bibliometric evaluation",
            "MIREX extended abstracts",
            "papers citing MIREX results",
            "trial framework and methodology",
            "MIREX datasets",
            "number of publications",
            "citation analysis"
        ],
        "content": "THE IMPACT OF MIREX ON SCHOLARLY RESEARCH  \n(2005 – 2010)  \nSally Jo Cunningham David Bainbridge J. Stephen Downie \nUniversity of Waikato  \nHamilton, New Zealand \nsallyjo@ cs.waikato.ac.nz  University of Waikato \nHamilton, New Zealand \ndavidb@cs.waikato.ac.nz  University of Illinois \nUrbana-Champaign, USA \njdownie @illinois .edu \nABSTRACT  \nThis paper explores the impact of the MIREX ( Music I n-\nformation Retrieval Evaluation eXchange) evaluation in i-\ntiative on scholarly research. Impact  is assessed through a \nbibliometric evaluation of both the MIREX extended a b-\nstracts and the papers citing the MIREX results, the trial \nframework and methodology, or MIREX d atasets. Impact \nis examined through number of public ations and citation \nanalysis. We further explore the primary publicatio n ven-\nues for MIREX results, the geographic distribution of \nboth MIREX contributors and researchers citing MIREX results, and the spread of MIREX -based research beyond \nthe MIREX contributor teams. This analysis indicates that research in this area is highl y collaborative, has \nachieved an international di ssemination, and has grown to \nhave a significant profile in the research literature . \n1. INTRODUCTION  \nIn this paper we report on the results of a study invest i-\ngating the scholarly impact of the Music Information Re-\ntrieval Evaluation eXchange (MIREX), an annual formal \nevaluation of MIR systems and algorithms. A detailed examination of the structure of the MIREX trials and the \nresults of the initial three ye ars of the MIREX program is \npresented in [ 2]. In this pr esent work, we look back on \nthe MIREX publication literature to develop a rich pi c-\nture of patterns of publication, collaboration, and disse m-\nination of MIREX research (Section 3). Our analysis is based on a set of MIREX -related publications gathered \nvia Google  Scholar (Section 2 ).  Issues encountered in \nbuilding our MIREX document set indicate the existence of barriers to the dissemination of MIREX results. These issues are further explored in Section 4, where we also describe proposals to reduce these barriers —specifically, \nby providing a digital library of MIREX extended ab-\nstracts (thereby pulling the scattered abstracts together \ninto a single repository that supports searching and \nbrowsing), and by recommending the development of referencing conventions for MIREX -related documents, \ndatasets, and evaluation frameworks.  2. BIBLIOGRAPHIC DATA G ATHERING \nIn this present paper, the impact of the MIREX trials is \nmeasured through both the number of MIREX -related \npapers published and the number of times that these p a-\npers have been cited. The MIREX publications include \nboth the brief descriptions of the MIREX algorithms submitted to a given trial (referred to in the MIREX trials \nas ‘extended abstracts’) and the papers derived from the \nMIREX extended abstracts and MIREX results. As rely-\ning solely on sheer quantity of papers has obvious dra w-\nbacks, additional analysis focuses on  the citation counts \nto round out the picture by indicating the degree to which \neach publication “has made a difference” [8] [9].   \nThree document sourc es have been commonly used in \nprevious bibliometric studies:  the ISI Web of Science \n(Thomson Reuters), Scopus (Elsevier), and Google \nScholar (Google) . The three have very different colle c-\ntion policies . The differences most significantly impac t-\ning this pre sent study are that ISI restricts its computer \nscience conference proceedings coverage more heavily \nthan the other two; Scopus provides a more comprehe n-\nsive coverage of both publishers and what they term \n‘quality web sources’ than ISI; and Google Scholar i n-\ncludes the majority of the ISI and Scopus offerings as well as books, tech nical reports, and white papers.  \nIn choosing Google Scholar as the source for this pr e-\nsent study, we were influenced by issues of coverage and \nuser preference. MIREX -flavored resear ch is based \nstrongly in computer science and engineering, two fields \nthat place a greater emphasis on conference publications \nand technical reports than other sciences —and both ISI \nand Scopus do not include these publications types to the \nextent of Google Scholar [3] [4]. As the MIREX exten d-\ned abstracts are not formally published, they are not in-cluded in the ISI and Scopus databases, and so their i m-\npact could not be measured through those resources. Fu r-\nther, we are specifically interested in exploring the docu-\nments most readily visible from the viewpoint of r e-\nsearchers interested in MIREX (rather than obtaining comprehensive coverage by hunting down MIREX rela t-\ned publications  through all possible sources). For a given \ntopic, Scopus, ISI, and Google Scholar are each likely to \ncover some content unavailable to the other two. Google \nand Google Scholar are the r esources of preference for \nresearchers in computer sci ence and other science fields \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012  International Society for Music Information Retrieval    \n \n[5]—and so by basing this study on documents drawn \nfrom Google Schola r, we build up a picture of the world \nof MIREX research that more closely resembles the \nviewpoint of MIREX r esearchers.  \nAs citations must build up over time, we restricted the \nscope of this study to the years 2005 – 2010 rather than \ncoming up to date (with  the expectation that the 2009 and \n2010 will show a ‘Groos droop’  [7]— the noticeable \n‘droop’ in the right hand tail of the distribution —as cita-\ntions are still accumulating  for these later years).   Each \nyear was individually searched by using the date r e-\nstriction facility in Google Scholar Advanced Search, and \nthe search crite rion used was “MIREX AND music” \n(‘Mirex’ is also a widely used insecticide, and so a further \nrestriction to the music domain was necessary to filter out agricultural research).  \nEach pa per in these initial results sets was then exam-\nined to gauge its relevance to this study. To be included in the study, the paper had to use / reference the results of a MIREX trial, a MIREX technique, MIREX data, or MIREX software. MIREX extended abstracts  present in \nthe Google Scholar results were also retained (extended abstracts not available through Google Scholar were not included in this study). Papers only tangentially related to \nMIREX were eliminated (for example, papers mentioning the MIREX trials as one example among many of retrie v-\nal evaluation exercises).  Further documents were culled \nbecause they were not formal research papers (for exa m-\nple, undergraduate student assignments ).  Documents that \nwere not publicly availab le were, when possible, dow n-\nloaded for examination through the r esearchers’ univers i-\nty library facilities (for example, the ACM publications). \nSome papers were not readily acces sible, and for these \nthe abstract and search snippet were examined; if these \ndid not indicate a signif icant relatio nship to MIREX then \nthey were also elimina ted. Finally, duplicates were ident i-\nfied and merged (cit ation counts for copies were added \ntogether).  Table 1 shows the document counts for bot h \nthe raw and cleaned datasets.  \n \n 2005 2006 2007 2008 2009 2010 \nRaw 74 154 186 246 281 330 \nCleaned  64 87 131 139 134 196 \n \nTable 1. Number of documents in the initial search r e-\nsults (raw) and final datasets (cleaned).  \n For each document retained, we recorded:  author \nnames, authors’ institutional affiliations, title, abstract, \npublication type (journal article, book chapter, conference paper, thesis, technical report), abstract, source (eg, co n-\nference name), and citation count.   As Google Scholar \nprovides only the raw citation count, we were not able to \nfilter for self -citations. Not all of this metadata was avai l-\nable for every document ; specifically, a small number of \ninstitutional affiliations wer e absent and so the analyses of author geographic distribution and collaboration (Se c-\ntion 3.6) may be slight underestimates.   \n3. ANALYSIS OF MIREX PUBLICATONS  \nThis section examines the impact of MIREX through \npublication and citation counts, the extent of collabor a-\ntion within the MIREX research community, and the geo -\ngraphic distribution of MIREX research efforts .  \n3.1 MIREX Publication Set \nFor 2005  - 2010 we identified a total of 752 publications:  \n236 MIREX extended abstracts, and 516 more formal publications based on the MIREX trials and results (T a-\nble 2).  Theses and disse rtations are treated separately in \nSection 3 .2. Note that this dataset does not provide e x-\nhaustive coverage of either category, and coverage of the MIREX abstracts in particular is patchy when viewed \nthrough the lens of Google Scholar. We return to this point in Section 4  with an explanation of this phenom e-\nnon and a partial solution to the relative invisibility of some MIREX documents.  \nTable 2 shows an overall increase in the number of \nMIREX -derived publications— a ten -fold increase in the \nfirst three years of the trials, and another large increase in \n2010. The MIREX trials are clearly seen by the research  \ncommunity to have value, as expressed through the \ngrowth of literature that builds on MIREX.  \nHowever,  MIREX extended a bstracts can be seen to \nreceive relatively fewer citations than the publications \nderiving from the MIREX trials (and even at that, the c i-\ntation average for MIREX extended abstracts is in most \nyears heavily skewed by one or two abstracts that r e-\nceived large numbers of citations). In contrast, a compa-\nrable  analysis of the TRECVid (video retrieval) [8] [9] \nand ImageClef (image retrieval) [10] evaluations show \nthe papers for those evaluation trials to have a similar c i-\ntation profile to their respective derived literature. Again, \nin Section 4  we ex plore possible reason s for the lower \ncitation counts and offer a  tactic to counter this effect . \nThe h -index is a measure that attempts to encapsulate \nboth the quantity and visibility of  a set of publications \n[1]. It is calculated as the number h that is th e largest \nnumber of papers in the set that  have each received at \nleast h citations. In Table 2  we see further ev idence that \nthe MIREX -derived publications have a far higher profile \nthan the MIREX extended abstracts;  in a given year the \nh-index for the deri ved publications is roughly three to \nfour times higher than that of the exten ded abstracts.  \n3.2 MIREX -derived Publications: Publication Types  \nThe derived papers are published formally as chapters in \nedited books, as conference papers, and in journals, and \nare less formally made available as technical reports. The  \npublication venues follow the profile typical of co mputer \nscience and engineering: there is a greater emphasis on \nconference than on journal publications, with a smaller    \n \n \n   \n MIREX extended abstracts  MIREX derived publications  \nYear  No. Citations  Mean  \n citations h-index  No. Citations  Mean  \ncitations h-index  \n2005 54 302 5.59 10 10 358 35.80 – \n2006 36 226 6.28 6 51 1308 25.65 20 \n2007 33 242 7.33 9 98 1453 14.83 21 \n2008 38 99 2.61 6 101 1754 17.37 22 \n2009 33 34 1.03 3 101 802 7.94 14 \n2010 42 35 0.83 3 155 914 5.90 14 \nTable 2.  Overview of citation data, 2005 – 2010. \n(but not completely negligible) number of book cha pters \nand technical reports (Table 3). \nThe MIREX annual results are reported throug h a spe-\ncial session in the ISMIR conference, and ISMIR is the \nfocal conference for music retrieval research —so it is to \nbe expected that ISMIR would be a significant public a-\ntion venue for the MIREX -derived research. As Table 4  \nillustrates, once past the in augural year over three quar-\nters of the MIREX -derived papers are published outside \nof ISMIR, and that spread to other conferences and jou r-\nnals increased in the final y ear of this present study . \n 2005 2006 2007 2008 2009 2010 \nTechnical  \nreport  0 1 3 3 2 1 \nBook chapter  0 2 1 2 2 7 \nConference  10 37 67 79 83 106 \nJournal article  0 11 17 17 14 41 \n \nTable 3.  Publication type for MIREX -derived papers. \n \n2005 2006 2007 2008 2009 2010 \n5  \n(50%) 14 \n(27%) 29 \n(30%) 26 \n(26%)  25 \n(25%) 28 \n(18%) \n \nTable 4 . Number and pe rcentage of MIREX -derived pa-\npers that are published in ISMIR conferences.  \n \n3.3 MIREX Theses and Dissertations  \nTable 5 shows the number of research theses and dissert a-\ntions that are based to some extent on the MIREX trials —\ntypically by referencing MIREX annual results, by testing \na novel algorithm against published MIREX datasets, or \nreporting more fully on the researcher’s own MIREX e n-\ntry. The uptake of MIREX as a degree focus bodes well \nfor the future of research in this area, as Masters and PhD students move into research p ositions.  \nThe theses and dissertations are cited less than the ot h-\ner MIR EX-derived publications (Table 5 ), but that is to \nbe expected— in the science fields, theses/dissertations \nare commonly re -worked into journal or conference pu b-lications , which are both more visible to other researchers \nand more visibly peer- reviewed (and hence more likely to \nbe noticed and cited).  \nYear  Degrees  No. Total Cit a-\ntions  Mean  \ncitations  \n 2005 Masters: 1  2 7 3.5 \nPhD:       1  \n2006 Masters: 8  13 90 6.92 \nPhD:       5  \n2007 Masters: 10  13 114 8.77 \nPhD:        3  \n2008 Masters: 14  23 90 6.92 \nPhD:        9  \n2009 Masters:  4  14 19 1.36 \nPhD:       10  \n2010 Ugrad:    1   21 46 2.19 \nMasters: 9  \nPhD:       11  \nTable  5. MIREX -related theses an d dissertations.  \n3.4 Collaboration in MIREX Research  \nThe mean number of authors per paper is presented  in \nTable 6 and the distribution of author numbers pe r paper \nis presented in Figure 2 . The research teams submitting to \nthe original MIREX trials were small—t he vast majority \ncomprised one or two  researchers— but over the years the \nnumber of participants in a MIREX submission has \ngrown. The number of co- authors for papers based on \nMIREX has shown steady growth to 2010. Both trends likely reflect the maturing of this area of research, as sta-\nble re search groups develop from the interests of one or \ntwo key researchers.   \nThe size of the collaborative teams for both categories \nof paper are larger than might be expected; typically the mean number of co -authors for a co mputer science or e n-\ngineering paper hovers around two  [6]. \n 2005 2006 2007 2008 2009 2010 \nExtended  \nabstracts  1.75 1.75 2.39 2.47 2.85 2.79 \nDerived  \npapers  2.3 2.31 2.62 2.96 2.95 3 \nTable 6 . Mean number of authors per paper. \n   \n \n \nFigure 2a.  Number of auth ors per paper for MIREX ex-\ntended abstracts.  \n \nFigure 2 b. Number of authors per paper for MIREX d e-\nrived publications (excluding theses and dissertations).  \n3.5 Geographic Distribution of MIREX Researchers  \nThirty- six countries have contributed at least one pub lica-\ntion in the 2005 – 2010 MIREX document set ( Table 7 \npresents  the league table of the top contributors , and Fig-\nure 3 presents a map -based visualization of this geo-\ngraph ic distribution ). Participation in the MIREX evalu a-\ntions is clearly not restricted to  a small inner circle, and \nthe MIREX results are seeing similarly widespread appl i-\ncation.   \nExamining more closely the national affiliations for \nauthors of the papers under study , we see that the r e-\nsearch is surprisingly collaborative across national \nboundaries and between institutions within a single cou n-\ntry (Table 8 ). The percentage of papers involving co -\nauthors from two or more countries seems to have stab i-\nlized at 12% from 2007 – 2009, and then to have in-\ncreased sharply  in 2010 to 18%. The increases in  these \ncross- boundary collaborations may reflect the increasing \nmaturity of the field, as researchers move to new pos i-\ntions while maintaining research ties in their former insti-\ntutions, or perhaps the personal connections made \nthrough ISMIR / MIREX conferences are encouraging \ngreater collaboration outside the researcher’s home insti-tution . A further drill -down into the publications dataset \n(and likely fol low-up survey of MIREX researchers)  is \nnecessary to clarify the factors contributing to  this effect.  Country MIREX  \nabstracts  Derived  \npapers  Theses  Total  \nUSA 33 130 19 182 \nFrance  29 61 5 95 \nSpain 27 48 11 86 \nUK 22 50 11 83 \nCanada  14 32 7 53 \nAustria  16 23 6 45 \nFinland  16 16 2 34 \nGermany  15 19  34 \nChina  14 19 1 34 \nJapan  8 22  30 \nTable 7 . Number of pu blications by country for the top \nten contributors, 2005 -2010.  \n \n Avg no. of  \ncountries per \npaper  % of multi -\nnational  \ncollab orations  Avg no. of  \ninstitutions per \npaper \n2005 1.2 20.0%  1.13 \n2006 1.04 3.9%  1.08 \n2007 1.16 13.3%  1.22 \n2008 1.15 12.9%  1.32 \n2009 1.14 12.0%  1.43 \n2010 1.18 18.3%  1.46 \nTable 7 . Summary of international and cross -institutional \ncollaborations.  \n4. BUILD ING A GREATER PROFIL E FOR MIREX \nEXTENDED ABSTRACTS \nEarly in the data gathering process it became apparent \nthat a substantial proportion of the MIREX extended ab-\nstracts w ere not being harvested by our  Google Scholar \nsearches —for example, a manual count of the 2008 e x-\ntended abstracts on the MIREX  wiki (http://www.music -\nir.org/mirex/wiki/)  yielded 51  submission abstracts, where \nour Google Scholar search identified only 38. Further, several extended abstracts a ppeared as multiple, but not \nidentical, versions of the same  intellectual content (obvi-ously revised versions of a si ngle submission ). We later \ndiscovered that yet other e xtended abstracts were indeed \npresent in the Google Scholar collection, but as they did not include MIREX in the document text or extracted metadata, they were not returned in our searches.  \n Perhaps more troublingly, Google Scholar was unable \nto extract meaningful bibliographic metadata for a nu m-\nber of the extended abstracts that did appear in the MIREX searches.  For these latter extended abstracts, the researchers verified that they were indeed part of the \nMIREX tria ls by traversing backwards through the file \nhierarchy in which the document was stored, until we could determine that it was indeed a legitimate contrib u-\ntion to a MIREX evaluation cycle.  For an extended a b-\nstract lacking metadata, a  researcher unfamiliar w ith \nMIREX, but interested in the intellectual content of the \npaper, would not know the extent to which the results  \npresented in the paper  could be trusted —was this paper\n  \n \n \nFigure 3. Geographic distribution of MIREX researchers.  \npeer reviewed? Was it a  technical report , less forma lly \n‘published’ but still endorsed by the authors’ instit utions? \nOr was it a student assignment  accidentally ha rvested by \nGoogle Scholar ? \nThese issues with identifying both the existence and \nprovenance of MIREX extended abstracts in Google \nScholar are likely explanations for the relatively low cit a-\ntion counts for the extended abstracts identified in this \npresent study (Table 2 ).  To mitigate these issues and, we \nhope, provide a mechanism for the MIREX evaluation documents to gain a higher profile, we  have developed a \ndigital library of the extended abstracts using the open source digital library software Greenstone [ 11].  Figure 4 \nshows a snapshot taken from this resource.  The figure \nshows the result of searching for \"F0\" using th e full -text \nindex of the abstract texts.  Each matching document dis-\nplays the title, year of publication, and the authors, along with a link to the PDF document. Also provided for each document is a \"Locate @ Google Scholar\" link.  Clicking \non this takes the title of the paper and initiates a search for this on Google Scholar.  While not guaranteed to find \na match, we found it worked reliably well in practice, and a convenient way to locate citation information about the extended abstract. Features also inc lude browsing by title, \nauthor and date, as well as search by these metadata fields.  The resource can be accessed through \nhttp://music -ir.org/mirex -dl/library ). \nWhile this digital library provides impro ved access f a-\ncilities to the extended abstracts, it is worth noting that \nthe some of the metadata for each abstract may be pr o-\nvided through the digital library interface and is not ap-\nparent on the document itself.  Searchers may stumble \nacross an extended abstract via any number of mecha-nisms —a Google Scholar search, a general search engine \nquery, a link from another website —and there is no gua r-\nantee that the specific path a particular user takes in  l o-\ncating a given document will provide any cues as to the document’s provenance beyond those included in the text \nof the document itself . For this reason, we recommend that each extended abstract should include a header providing the citation for that abstract. \n \n \nFigure 4. Sample search results display in our prototype \ndigital library of MIREX extended abstracts.  \nClose examination of the MIREX -derived literature a l-\nso uncovered difficulties that some authors had obviously \nexperienced in knowing how to cite the results of the \nMIREX trials (for example, the relative performance of \nspecific algorithms).  While an overview of the year’s \nMIREX evaluations generally appears in the proceedings \nof the annual ISMIR conference, this document does not \nprovide comprehensive results from all ta sks. Exhaustive \n  \n \nsummaries of res ults are available on the MIREX wiki1, \nbut these are not provided in a form that is recognized as \nbeing suitable for indexing by Google Scholar —and no \nguidelines are given on the wiki as to how to cite these \nresults.  A straightforward solution would be to issue th e-\nse results summaries as technical reports and store them \nin repositories indexed by Google Scholar and other \nscholarly indexing systems.  \nSimilar difficulties were apparently experie nced in \nproviding formal acknowledgment  of the MIREX trials, \nexper imental setup, or datasets (the MIREX wiki does not \nprovide a canonical reference form for these). While th e-\nse papers did use the term “MIREX” in describing the \nresults and datasets in the paper body (and so our Google \nSearches did return these papers), these mentions were \nnot tied to entries in the papers’ reference sections— and \nconsequently no MIREX entity receives citation credit.   Contrast this situation with that of the TRECVid evalu a-\ntion series, which suggests standard references f or many \naspects of this programme ( http://www -\nnlpir.nist.gov/projects/t01v/trecvid.citation.html ). We e n-\ncourage the MIREX organizers to develop  similar refe r-\nencing guidelines, and will include them  in the home \npage of our extended abstracts digital library. \n5. CONCLUSION S \nOur examination of the MIREX literature (the extended \nabstracts and papers referring to / referencing MIREX \nresults, datasets, and evaluation trials) portrays a thriving \ninternational  research community, characterized by co l-\nlaboration.  We have identified barriers to the accessibi l-\nity of the MIREX extended abstracts, and present a prot o-\ntype digital library for these documents t hat we believe \ncan improve the MIREX  profile in the larger research \ncommunity. We also provide recommendations for modi-\nfications to the format of extended abstracts and the i n-\nformation pre sented in the MIREX wiki , to increase the \nvisibility of MIREX to search engines and to make it ea s-\nier for researchers to locate  citation information for \nMIREX documents.  \nWe believe that these small changes have th e potential \nfor a large payoff:  MIREX can follow in the steps of the \nsuccessful TRECVid and ImageCLEF series by providing \nthe MIREX extended abstracts and citation information in \nformats that are readily indexed by Google Scholar and other resources, easily located by interested researchers, \nand easil y cited in relevant  publications.   \n6. REFERENCES  \n[1] L. Bornmann and H -D Daniel: “What do we know \nabout the h index?”, Journal of the American Society \n                                                             \n1 eg, “MIREX 2008 Overall Results Poster  , http://www.music -\nir.org/mirex/results/2008/MIREX2008_overview_A 0.pdf  for Information Science and Technology, Vol . 58, \nNo. 9, pp. 1381- 1395, 2007.  \n[2] J. S. Downie: “The music information retrieval evaluation exchange (2005 – 2007): A window into \nmusic information retrieval re search”, Acoustical \nScience and Technology, Vol. 29, No. 4, pp. 247 -\n255, 2008.  \n[3] J. Freyne, L. Coyle, B. Smyth, and P. Cunningham: “Relative status of journal and conference publications in computer science”, Communications \nof the ACM, Vol. 53, No. 11, pp. 124- 132, 2010.  \n[4] A.W. Harzing: “Citation analysis across the \ndisciplines: the impact of different data sources and citation metrics”, \nhttp://www.harzing.com/data_metrics_comparison.h\ntm \n[5] B.M. He mminger, D. Lu, K.T.L. Vaughan, and S.J. \nAdams: “Information seeking behavior of academic scientists”, Journal of the American Society for Information Science and Technology, Vol. 58, No. 14, pp. 2205- 2225, 2007.  \n[6] M.E.J. Newman: “The structure of scientific collaboration networks”, Proceedings of the \nNational Academy of Sciences, Vol. 98, No. 2, pp. \n404-409, 2001.  \n[7] R. Rousseau: “A bibliometric study of \nNieuwenhuysen’s bibliography of microcomputer \nsoftware for online information and documentation \nwork”, Journ al of Information Science, Vol. 16, pp. \n45 – 50, 1990.  \n[8] C.V. Thornley, A.C. Johnson, A.F. Smeaton, and H. \nLee: “The scholarly impact of TRECVid (2003 – \n2009)”, Journal of the Ame4rican Society for \nInformation Science and Technology, Vol. 62, No. \n4, pp. 613- 627, 2011.  \n[9] C.V. Thornley, S.J. McLoughlin, A.C. Johnson, and \nA.F. Smeaton: “A bibliometric study of video retrieval evaluation benchmarking (TRECVid): A \nmethodological analysis,” Journal of Information \nScience, Vol. 37, No. 6, pp. 577 -593, 2011 . \n[10] T, Tsikrik a, A.G. Seco de Herrera, and H. Muller: \n“Assessing the scholarly impact of ImageCLEF”, Multilingual and Multimodal Information Access Evaluation, Lecture Notes in Computer Science, \nVol. 6941, pp. 95- 106, 2011.  \n[11] I.H. Witten, D. Bainbridge, D.M. Nichols: How to \nbuild a digital library (2\nnd edition), Morgan \nKaufmann, 2010."
    },
    {
        "title": "A Study of Intonation in Three-Part Singing using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT).",
        "author": [
            "Johanna Devaney",
            "Michael I. Mandel",
            "Ichiro Fujinaga"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416210",
        "url": "https://doi.org/10.5281/zenodo.1416210",
        "ee": "https://zenodo.org/records/1416210/files/DevaneyMF12.pdf",
        "abstract": "This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles’ performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift.",
        "zenodo_id": 1416210,
        "dblp_key": "conf/ismir/DevaneyMF12",
        "keywords": [
            "Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)",
            "MATLAB toolkit",
            "aligning monophonic audio to MIDI scores",
            "extracting and analyzing performance data",
            "intonation analysis",
            "three-part singing experiment",
            "interval size",
            "pitch drift",
            "Just Intonation tuning",
            "pitch drift"
        ],
        "content": "A STUDY OF INTONATION IN THREE -PART SINGING \nUSING THE AUTOMATIC MU SIC PERFORMANCE \nANALYSIS AND COMPARISON TOOLKIT (AMPACT) \nJohanna Devaney Michael Mandel Ichiro Fujinaga \nCNMAT, UC Berkeley/ \nSchool of Music \nThe Ohio State University \nj@devaney.ca \n Audience Inc./ \nCollege of Engineering \nThe Ohio State University \nmim@mr-pc.org  CIRMMT  \nSchulich School of Music  \nMcGill University \nich@music.mcgill.ca  \nABSTRACT  \nThis paper introduces the Automatic Music Performance \nAnalysis and Comparison Toolkit (AMPACT), is a MAT LAB toolkit for accurately aligning monophonic \naudio to MIDI scores as well as extracting and analyzing \ntiming -, pitch -, and dynamics -related performance data \nfrom the aligned recordings. This paper also pr esents the \nresults of an analysis performed with A MPACT on an \nexperiment studying int onation in three -part sin ging. The \nexperiment examines the interval size and drift in four \nensembles’ perform ances of a short exercise by \nBenedetti, which was designed to highlight the conflict between Just Int onation tun ing and pitch drift.  \n1. INTRODUCTION  \nIn the early 20th century, psychologist Carl Seashore and \nhis colleagues at the University of Iowa undertook exten-\nsive work in performance analysis o f singing, examining \ndynamics, intonation, and vibrato  [22]. Their analyses \nwere based on amplitude and frequency information e x-\ntracted from recordings with phonophotograph ic apparati.  \nThese manual methods were extremely labourious and \nlimited the number of recordings that could be accurately analyzed. Recent developments in digital signal pro-cessing have allowed for many of these manual processes \nto be performed computationally.  \nThe MATLAB -based Automatic Performance Analy-\nsis and Compar ison Toolkit (AMPACT) collects existing \ntools and introduces a new MIDI -audio alignment alg o-\nrithm. The alignm ent algorithm is able to accurately ide n-\ntify onsets and offsets in the difficult cases of the singing voice and instruments with non -percussive o nsets and can \nbe trained to work on recordings of a range of voice types \nand instruments.  The analysis portion of the toolkit in-\ncludes tools for extract ing of various performance para m-\neters related to timing, pitch, and dynamics. AMPACT also includes tools for compa ring data across multiple \nperformances. The purpose of the toolkit is to facilitate empirical anal ysis of musical performance for those \nwithout e xtensive tec hnical training.  This paper also presents an experiment on intonation \nin three -part singing, which used AMPACT to extract \nand analyze the intonation data. The experiment uses a \nshort exercise by a mus ic theorist, Benedetti (1530–\n1590), de signed to result in varying amounts of pitch drift \nwhen different idealized tunings are applied to it. The e x-\nercise was performed numerous times by four different \nensembles and the resultant recordings were a nalyzed in  \nterms of melo dic/vertical interval tuning and pitch drift.  \n2. PREVIOUS WORK  \n2.1 Automatic Performance Data Extraction  \nCurrently, there are no robust automated methods for \nestimating note onsets and offsets in the singing voice.  \nAlthough much work has been done i n the area of note \nonset detection  [1], accurate detection of onsets for the \nsinging voice and other instruments without percu ssive \nonsets is not a solved problem. Collins used a pitch \ndetector for estimating non- percussive onset  detection \n[3]. He im proved on the number of onsets d etected within \na 100 ms tolerance window over the phase deviation approach described in [1] ( 58% versus 45%), with \ncomp arable false positives (36% versus 37%), Friberg,  \nSchoonderwaldt , and Juslin developed an onset and offset \ndetection alg orithm that was evaluated on electric guitar, \npiano, flute, violin, and saxophone  [10]. They reported an \nonset estimation accuracy of 16 ms and an offset estim ation accuracy of 146 ms . Toh, Zhang, and Wang \ndescribe a system for automatic onset detection for solo \nsinging voice that a ccurately predicts 85% of onsets to \nwithin 50 ms of the annotated ground truth  [23]. These \nalgorithms often require a signi ficant amount of manual \ncorrection to obtain sufficient accuracy for performance \nanalysis. Furthermore,  offset detection is required for \nmeasurements r elated to duration, intonation, vibrato, and  \ndyna mics, but most of these algorithms do  not provide it.  \n2.2 Studies of Intonation in Vocal Ensembles  \nIn the absence of robust automated methods for estima t-\ning note onsets an d offsets, studies of the sing ing voice \nhave relied on manual annotation of notes’ onsets and \noffsets. Vurma and Ross studied 13 professional singers’ melodic intonation in their performances of short exe rcis-\nes using PRAAT for F\n0 analysis [2]. They observed that \nascending and descending sem itones were smaller than Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.   \n© 2012 International Society for Music Information Retrieval  \n   \n \nequal temperament and that ascen ding and descending \nfifths were larger than equal temperament [25]. Howard \nstudied two a cappella  SATB quartets and found that \nthey used non -equal temperament with a tendency t o-\nwards, though not full compl iance with, Just -Intonation \n[13]. He also argued that in pieces with mod ulation, that \nsince the ensembles used non -equal tem perament , pitch \ndrift is necessary for choirs to stay in tune  [14]. Howard \nused e lectroglottograph s to obtain F0 estimates in order to \navoid the complication of pol yphonic F 0 estim ation.  \n3. AMPACT \n3.1 Overview  \nAMPACT1 automatically analyzes performance data \nfrom monophonic or quasi -polyphonic recordings. The \nalgorithms included in the toolkit make use of the info r-\nmation available in the score about what notes are e x-\npected in the performance and the order in which they \nwill occur. AMPACT provides estimates of note onsets \nand offsets for tones with non -percussive onsets (e.g., vo-\ncalists) that are more robust than existing blind onset de-\ntection or alignment alg orithms. The analysis portion of \nthe toolkit allows for the extraction of various perfo r-\nmance param eters: inter -onset intervals between notes; \ntempo info rmation; relative d ynamic level between notes; \nmean fr equency for each note and interval sizes in cents; \nand vibrato rate and depth. The statistical tools allow compariso ns of different pe rformances of the same musi-\ncal material or piece. A schematic of the analy sis compo-\nnents of AMPACT is shown in Figure 1.  \n \nFigure 1. Schematic of the Automatic Performance and \nAnalysis Toolkit (AMPACT).  \n                                                             \n1 Available for download at www.ampact.org 3.2 MIDI -Audio Alignment  \nAMPACT uses a MIDI -audio alignment algorithm in or-\nder to identify the beginning and ending of all of the \nnotes in a performance. A MIDI version of the score, \nwhich is a quantized version of all of the pitch and timing \ninformation  in the audio, is adjusted so that its timing i n-\nform ation corresponds to that of the audio. The alg orithm \nin AMPACT refines the results of an existing Dynamic \nTime Warping (DTW) approach, described in [18], with a \nhidden Markov model (HMM). The HMM is trained on \nthe acoustic properties of the  melodic line being aligned \nand, in the case of the singing voice, is guided by the lyr-\nics in the score . The HMM  both increase s the accuracy of \nthe initial alignment and label s transient and steady -state \nsections of each note. Identification of t he steady- state \nsections of notes is important because they correspond to \nthe pitched sections .  \n3.2.1 Hidden Markov Model  \nThis section describes the details of the HMM , with a \nparticular focus on modelling the solo singing voice, \nnamely the observations, states, transition probabil ities, \nand use of a DTW alignment as a prior.  The observ ed \nvariables modelled by  the HMM are the square root of \nperiodicity , power , and F 0 estimates provided by the YIN \nalgorithm [5] for each frame . The F 0 estimates from Y IN \nare a somewhat noisy cue, especially for the silence and \ntransient states , but are  important because they assist \nalignment when the note changes under a sin gle vowel.  \nThe three acoustic e vents are modelled in the HMM : \nsilence, transi ent, and steady state . In the singing voice, \ntransients occur when a consonant starts or ends a syll a-\nble, while vowels produce the steady -state portion of the \nnote.  In instruments, the occurrences of transients are i n-\nfluenced by a rticulation. The transition probabilit ies were \ncalculated from example recordings of the singing voice . \nTwo versions of the state sequences are implemented . \nThe first allows each state to be visited , shown in Figure \n2. The second is modified by the lyrics in the score; tra n-\nsients were only inserted when a n unvoiced consonant \nbegan or ended a syllable and silences were inser ted only \nat the end of phrases , shown in Figure 3.  \nThe initial DTW alignment is used as a prior to g uide \nthe HMM . The use of the DTW alignment obviates the \nneed to encode information about the score in the HMM. \nBy assuming that the DTW alignment is roughly correct, \nit is not necessary to rely excessively on noisy F 0 esti-\nmates in the HMM. This simplifies the design of the \nHMM and allows the same HMM seed to be used for \neach note. One issue with this approach is that it ca nnot \nadjust the initial alignment by more than one note, so the \ninitial alignment has to be relatively a ccurate.  \n   \n \nThe HMM was implemented in M ATLAB  with Kevin \nMurphy’s HMM Toolbox [17] and uses Alain de Che vei-\ngné’s MATLAB implementation of the YIN alg orithm \n[4] as well as Dan Ellis’ MATLAB impl ementation of \nDTW MIDI -audio alignment [9]. An evaluation of the \nalignment algorithm is described in [7]. \n \nFigure 2. Three -state basic state sequence seed  in the \nHMM : steady state (SS), transient (T), silence (S). The \nending transient (ET) and the beginning transient (BT) \nboth have the same observation distribution. \n \nFigure 3. State s equence adapted to sung text.  \n3.2.2 Performance Data Analysis and Comparison  \nThe alignment algorithm provides information about the \nnote onset and offset times , which AMP ACT saves in the \nMIDI toolbox’s note- matrix format [24] from which a \nMIDI file can be saved. The onset and offset loc ations \nalso delineate the starting and ending points for calcula t-\ning pitch - and dynamic s-related parameters of each note.  \nOnset and offset information is also saved in as an A u-\ndacity- readable label file [15], which allows for manual \ncorrection of any alignment errors AMPACT may make.  \nThe YIN algorithm  is used for F0 estimation. One a d-\nvantage of YIN is that it allows for specification of mi n-\nimum and maximum expected F 0s, which AMPACT  sets \naccor ding to the note information in the aligned score . \nThe maximum F 0 is set to one whole tone  above the co r-\nresponding note in the score and the minimum F 0 is set to \none whole tone  below. This is a very useful feature for \nrecordings that are not strictly monophonic, such as re-\ncordings from close  miking in ensemble perfo rmance.  \nPerceived pitch is calculated using a weighted mean \nbased on the F 0’s rate of change [12]. This mean is cal cu-\nlated by assigning a hig her weighting to the frames where \nthe F 0 has a lower rate of change and a lower weighting \nto those with a higher  rate of change. The threshold b e-\ntween high and low rates of change is set at 1.41 oc-\ntaves/second, based on the vibrat o rate and depth values \nreported in  [20; 21]. Vibrato is calculated by finding the dominant frequency of the FFT of the pitch contour.  Dy-\nnamics are calculated using the implementation in Gene-\nsis Acoustics Loudness Tool box for MATLAB [11] of \nGlasberg and Moore’s model for e stimating loudness in \ntime-varying sounds [16]. AMPACT also  includes tools  \nfor statistical comparison of performances through a \nwrapper for var ious t-test, ANOVA, and l inear regression \nfunctions in MATLAB. \n4. INTONATION EXPERIMEN T \nAMPACT was used to extract and analyze intonation d a-\nta in an experiment on four three -part vocal ensembles. \nThe ensembles’ performances were analyzed with regard to melodic whole -tone tunings, a range of vertical interval \ntunings, and overall pitch drift . A pre -release version of \nAMPACT was  used by the authors in larger- scale exper-\niment on solo singing  in [6]. \n4.1 Method  \n4.1.1 Experimental Material  \nThe experimental material is a three -part chord progres-\nsion written by Bendedetti that was d esigned to show that \nsingers would  not tune Justly with the current sustained \nnote since strict adhere nce to Just Intonation would result \nin a significant pitch drift that is not observable in pe r-\nformances of the progression  [19]. The progression is \nbuilt from a seed two -measure progression that is repea t-\ned four times. If the singers were to tune in Just Intona-tion to the su stained note, rather than the bass note, the \nensemble would drift up a s yntonic comma (21.5 cents) \nby the end of each seed, resulting in a total upwards drift of 86 cents by the end of the four repetitions. In contrast, if the singers were to tune to the bass in each vertical s o-\nnority, with D, A, or G in the bass, there should  be no \ndrift. The calculations for both tuning scenarios are shown in Fi gure 4. \n \nFigure 4. Theoretical tuning for Benedetti progression \nused as experimental material. The numbers in the tables \nat the top and bottom of the fig ure indicate the tuning in \nrelation to the starting pitch in the bass .   \n \n4.1.2 Partic ipants  \nFour three -part ensembles participated in this experiment. \nEnsemble 1 served as a pilot study with semi -professional \nalto, tenor, and bass singers who performed without a \nconductor. The ensemble had an average age of 26 years \n(SD = 3.6), with an average of 6.5 years of private voice \nlessons ( SD = 4.5) and 6.5 years of regular practice ( SD = \n2.5). Ensembles 2, 3, and 4 consisted of professional \nsingers who regularly sang toge ther with the conductor \nused in the experiment. These ensembles had an average \nage of 42 years (SD  = 9), an average of 7.75 years of pri-\nvate voice lessons ( SD = 0.5) and 24 years of regular \npractice ( SD = 10). The singers in both ense mbles were \nexperienced  in singing a cap pella Renaissance music  and \nwere asked to sing with their normal tuning.  \nEnsembles 2 and 4 consisted of alto, tenor, and bass \nsingers while Ensemble 3 consisted of soprano, alto, and tenor. Ensembles 1 and 2 were recorded in a 4.85m x \n4.50m x 3.30m lab with low -noise, minimal reflections, \nand short reverberation time. The singers were miked with cardioid headband mics (DPA 4088- F). The micr o-\nphones were run through an RME Micstasy 8- channel \nmicrophone preamplifier and an RME Madi Bridge into a \nMac Pro computer for multi -track recording. Ensembles 3 \nand 4 were recorded on the altar of St. Mathias‘ Church, a church in Montreal dating from 1912 with wooden floors, limestone walls, and seat ing for 350 people. As with the \nlab environment, the singers were miked with cardioid headband mics, although a portable Zaxcom Deva 16 dig-\nital recorder was used for the rest of the recording setup.   \n4.2 Results  \n4.2.1 Interval Size \nThe mean and standard deviation of the interval si zes for \nthe melodic  whole tones are shown  in Table 1. The m a-\njority of the means were within one standard deviation of \nthe equal tempered 200 cent tuning. The main except ion \nto this was Ensemble 1 where the whole tone tended to be smaller. In particular, the middle voice  whole tones, \nwhich  were cl oser to the 182 cent M inor Just Intonation \nwhole tone. Just over half of the singers’ whole tones (12/20) were within one standard deviation of the P y-\nthagorean/Major Just Intonation (204 cents) whole tone.  \nVertical intervals were calculated for each half-\nmeasure between all of the voices: lowest voice to middle \nvoice, lowest voice to up per voice, and middle voice to \nupper voice. The onset and offset times for the vertical \nintervals were determined by the upper voice.  Overall, \nthere were 51 vertical intervals in each rendition: 4 Minor \nThirds (m3), 8 Major Thirds (M3), 9 Perfect Fourths \n(P4), 17 Perfect Fifths (P5), 4 Major Sixths (M6), and 9 Perfect Octaves (P8). The means and standard deviations \nfor each type of vert ical interval across all of the singers in each ensemble are shown in Table 2. There was a wide \nrange in the mean values for both the vertical m3 and M3, \nspecifically 300 –322 cents for the m3 and 375 –413 cents \nfor the M3. When the standard deviations are taken into \naccount, the m3 encompassed the Pythagorean (294 cent), \nequal tempered (300 cents), and Just Intonation (316 \ncents) tunings. Likewise, the M3 range enco mpassed the \nJust Intonation (386 cents), equal tempered (400 cents), and Pythagorean (408 cents) tu nings. The range of the \nmeans for t he M6 encompassed only the equal tempered \ntuning (900 cents) since the means were all larger than the Just Intonation tuning (884 cents) and marginally smaller than the P ythagorean (905 cents). The tunings for \nthe P4 (498 cents), P5 (702 cents), and P8 (12 00 cents) \nare common to both the Pythagorean and Just Intonation systems and are close to the values for equal temperament \n(500, 700, and 1200 cents, respectively); the ranges for \nthese intervals encompassed all of the se tunings.  \n  Voices  \n  Top  Middle  Bottom  \nEnsemble  Up Down  Up Down  Down  \n1 Mean  199 195 185 183 191 \nSD 6 4 6 10 7 \nN 12 12 12 12 12 \n2 Mean  192 191 207 210 189 \nSD 6 16 12 13 6 \nN 16 16 16 16 16 \n3 Mean  199 199 199 196 198 \nSD 9 10 8 8 11 \nN 16 16 16 16 16 \n4 Mean  189 194 196 196 195 \nSD 13 8 10 13 13 \nN 20 20 20 20 20 \nTable 1. Mean , standard deviation , and number of i n-\nstances  of the ascending and descending melodic whole \ntone sizes for all e nsembles, broken down by voice.  \n Vertical Interval  Types  \nEnsembl e m3 M3 P4 P5 M6 P8 \n1 Mean  322 376 509 701 893 1201 \nSD 7 9 10 6 6 7 \nN 12 24 27 51 12 27 \n2 Mean  300 413 497 705 903 1206 \nSD 12 11 17 14 15 12 \nN 16 32 36 68 16 36 \n3 Mean  307 397 507 704 904 1209 \nSD 8 11 12 11 11 9 \nN 16 32 36 68 16 36 \n4 Mean  301 406 493 702 896 1202 \nSD 14 15 13 12 10 12 \nN 20 40 45 85 20 45 \nTable 2. Mean, standard deviation, and number of i n-\nstances of the vertical interval  sizes  between the three \nvoices across all renditions by all of the ense mbles.    \n \nAn ANOVA analysis for each ensemble  was run on \nthe melodic interval data with  whole tone size as the d e-\npendant variable and direction and singer identity as in-\ndependent variables . In Ensemble 1, there was no signif i-\ncant effect for direction , though the middle singer’s \nwhole tones were significantly smaller than the other two \nsingers, F (2, 56)  = 24.59, p < 0.001. Ensemble 2 was \nsimilar, with no effect for direction and a significant e f-\nfect for the middle singer , though in this case the middle \nsinger’s  whole  tones were significantly larger than the \nother two singers, F (2, 75)  = 24.52, p < 0.001. There \nwere no si gnificant effects for direction or singer identity \nin Ense mbles 3 and 4. A separate ANOVA was run with \ndirection and group identity. There was no ove rall effect \nfor direction , but Ensemble 1 ’s whole tones were signif i-\ncantly smaller on average  than E nsembles 2 and 3, F (3, \n311) = 6.96, p < 0.001.  \nSeparate ANOVAs w ere also run on each vertical i n-\nterval to test for group effects. E nsemble 1’s m3 intervals \nwere significantly larger on average  than the other three \nensembles, F (3, 59)  = 11.93, < 0.001, so much so that \ntheir mean  overshot the 316 cent Just Intonation value. In \ncontrast ,, Ensemble 1’s M3 intervals  were significantly \nsmaller on average than the  other ensembles’ , F (3, 127)  \n= 50.31, p < 0.001  and were so small that they  undersho t \nthe 386 cent Just Intonation value. For the P4, Ense mbles \n1 and 3 were significantly larger than Ense mbles 2 and 4, \nF (3, 143)  = 11.75, p < 0.001. There were no si gnificant \neffects between the en sembles for the P5, M6 , or P8.  \n4.2.2 Pitch Drift  \n \nIn order to assess whether the ensembles drifted in the \nmanner predicted by Benedetti, the perceived pitch est i-\nmates in cents were calculated for each note in each ren-\ndition in relation to the rendition’s opening D in the bass. \nThe table at the top of Figure 5 shows the means and \nstandard deviations for each note  across all of the ense m-\nbles. Overall there was a slight drift upwards of 8 cents in \nthe lower voice, 10 cents in the middle v oice, and 13 \ncents in the upper voice. This drift is much smaller than the one  predicted by the calc ulations in the lower chart in \nFigure 5 , suggesting that the singers were tuning to the \nbass note rather than the lowest sustained note.  \nFigure 5  also shows the drift in the bass voice for each \nensemble through plots of the perceived pitch (relative to the starting note) of the D at the start of each seed pr o-\ngression. Ensemble 1 was the most consistent with itself \nacross performances, exhibiting only a small amount of \ndrift from the starting pitch . Ensembles 2 and 3 both \ntended to drift upwards with Ensemble 3 showing a \ngreater amount of variability in the amount of drift. En-\nsemble 4 had little drift overall but showed a large \namount of variation within each p erformance.  \n \n \nFigure 5. Summary of the amount of drift in each e n-\nsemble’ s renditions of the Benedetti chord progression . \nThe lines in the each plot link the perceived pitch est i-\nmates for the notes D1 -D5 in each rend ition. \n4.3 Discussion  \nOverall  the singers tended towards equal temperament. \nThe vast majority of the means of the melodic and vert i-\ncal intervals were within one standard deviation of equal \ntemperament. The melodic intervals that were not within one standard deviation of equal temperament were much \nsmaller, falling instead within one standard deviation of the 182 cent minor Just Intonation semitone. Likewise, \nmost of the outlying vertical intervals fell within one \nstandard deviation of non -equal temperament  idealized \ntunings  (either Just Intonation of Pytheagorean) : Ense m-\nble 1’s m3 mean was within one standard deviation of the \n316 cent Just Intonation tuning; Ense mble 1’s M3 mean \nwas within one sta ndard deviation of the 386 cent Just \nInton ation tuning; Ensemble 2’s M3 mean  was within one \nstandard deviation of the 408 cent M3. The ANOVA \nanalysis revealed some significant effects for singer and group iden tity for some of the ensembles. The lack of a \nsignificant effect for direction in the ANOVA analysis of \nthe whole tone tuni ng mirrors our earlier findings for  pro-\nfessional solo sin gers [8].  \nThe ensembles did drift up slightly on average, but \nnot to the extent they would have if the singers were tun-ing in Just Intonation to the lowest sustained note. This is \nnot surprising , as such a rapid drift, 88 cents over eight \nmeasures, is highly unlikely since it implies that the sin g-\ners were not retaining their starting pitch as a reference \nonly a few tens of seconds after it was sung.   \n5. CONCLUSIONS  \nThis paper presented AMPACT, a MATLAB toolkit for \nautomatically extracting, analyzing, and comparing pe r-  \n \nformance data from monophonic recordings for which a \nscore is available. The alignment algorithm in AMPACT works we ll on sounds without a clearly defined onset, \nmaking it useful for the singing voice and instruments \nwith non -percussive onsets. This paper also demon strates \nthe use of AMPACT in  extracting and analyzing the data \nfor an experiment on vocal intonation . The exper iment \nwith four three -part ensembles found that the singers \ntended toward equal temperament and did not exhibit a \nlarge amount of drift  in an exercise by Renaissance the o-\nrist Benedetti . The detailed analysis of singing intonation \nin this study was fac ilitated  by the automated nature of  \nAMPACT , not only in terms of time saving s but also in \nconsistency of data extraction . \n6. ACKNOWLEDGMENTS  \nThe authors would like to thank Dan Ellis for his assi s-\ntance in developing the alignment algorithm in \nAMPACT . We would  also like to thank Jonathan Wild \nand Peter Schubert for their contributions to the exper i-\nment. We would also to acknowledge the contributions  of \nthe pe rsonal at the Centre for Research in Music Media \nand Technology (CIRMMT) and Center for New Music \nand Au dio Technologies (CNMAT)  to various aspects of \nthis project,  particularly David Wessel . This work was \nsupported by funding from the Fonds de recherche sur la société et la culture (FQRSC) and Social Sciences and Humanities Research Council of Can ada (SSH RC).  \n7. REFERENCES  \n[1] Bello, J. P., L. Daudet, S. Abdallah, C. Duxbury, M. \nDavies, and M. Sandler. 2005. A tutorial on onset detection in music signals. TASLP 13 (5): 1035 –47. \n[2] Boersma, P. 2001. PRAAT, a system for doing \nphonetics by computer. Glot International  5 (9/10): \n341– 5. \n[3] Collins, N. 2005. Using a pitch detector for onset \ndetection. In Proceedings of ISMIR , 100 –6. \n[4] de Cheveigné, A. 2002. YIN MATLAB implementation . \nhttp://audition.ens.fr/adc/sw/yin.zip . \n[5] de Cheveigné, A. , and H. Kawahara. 2002. YIN, a \nfundamental frequency estimator for speech and music. JASA  111 (4): 1917 –30. \n[6] Devaney, J., M. Mandel, I., D. P. W. Ellis, and I. \nFujinaga. 2011. Automatically extracting performance data from recordings of trained singers . \nPyschomusicology 21 (1– 2): 108– 36. \n[7] Devaney, J., M. I. Mandel, and D. P. W. Ellis. 2009. \nImproving MIDI -audio alignment with acoustic \nfeatures. In Proceedings of WASPAA , 45–8. \n[8] Devaney, J., J. Wild, and I. Fujinaga. 2011 Intonation \nin solo vocal pe rformance: A study of semitone and \nwhole tone tuning in undergraduate and professional sopranos. In Proceedings of the International \nSymposium on Performance Science, 219 –24. \n[9] Ellis, D. P. W. 2008. Aligning MIDI scores to music \naudio. http://www.ee.colu mbia.edu/~dpwe/resources/  \nmatlab/alignmidiwav/ . \n[10] Friberg, A., E. Schoonderwaldt, and P. N. Juslin. \n2007. CUEX: An algorithm for extracting \nexpressive tone variables from audio recordings. \nActa Acustica united with Acustica  93: 411– 20. \n[11] Genesis Acou stics Loudness Toolbox for Matlab . \n2010. http://www.genesis- acoustics.com/ .  \n[12] Gockel, H., B. C. J. Moore, and R. P. Carlyon. 2001. \nInfluence of rate of change of frequency on the overall pitch of frequency -modulated tones. JASA \n109 (2): 701 –12. \n[13] Ho ward, D. M. 2007. Equal or non -equal \ntemperament in a cappella SATB singing. Logopedics Phoniatrics Vocology  32: 87– 94. \n[14] Howard, D. M. 2007. Intonation drift in a capella \nsoprano, alto, tenor, bass quartet singing with key modulation. Journal of Voice 21 (3): 300 –15. \n[15] Mazzoni, D., and R. Dannenberg. 2000. Audacity. \nhttp://audacity.sourceforge.net/ . \n[16] Moore, B. C. J., and B. R. Glasberg. 2010. The role \nof temporal fine structure in harmonic segregation through mistuning. JASA 127 (1): 5 –8. \n[17] Murphy, K. 1998. Hidden Markov Model (HMM) \nToolbox for Matlab . http://www.cs.ubc.ca/  \n~murphyk/Software/HMM/hmm.html . \n[18] Orio, N., and D. Schwarz. 2001. Alignment of \nmonophonic and polyphonic music to a score. In \nProceedings of ICMC , 155 –8. \n[19] Palisca, C.  V. 1994. Studies in the History of Italian \nMusic and Music Theory . Oxford, UK: Oxford \nUniversity Press.  \n[20] Prame, E. 1994. Measurements of the vibrato rate of \nten singers. JASA 96 (4): 1979 –84. \n[21] Prame, E. 1997. Vibrato extent and intonation in \nprofe ssional western lyric singing. JASA 102 (1): \n616– 21. \n[22] Seashore, C. 1938. Psychology of Music . Iowa City, \nIA: University of Iowa Press. Original edition, New \nYork, NY: Dover Publications.  \n[23] Toh, C. C., B. Zhang, and Y. Wang. 2008. Multiple -\nfeature fu sion based on onset detection for solo \nsinging voice. In Proceedings of ISMIR , 515 –20. \n[24] Toiviainen, P., and T. Eerola. 2004. MIDI Toolbox: \nMATLAB Tools for Music Research . http://  \nwww.jyu.fi/musica/miditoolbox/  \n[25] Vurma, A., and J. Ross. 2006. Produc tion and \nperception of musical intervals. Music Perception  \n23 (4): 331 –44."
    },
    {
        "title": "A MIREX Meta-analysis of Hubness in Audio Music Similarity.",
        "author": [
            "Arthur Flexer",
            "Dominik Schnitzer",
            "Jan Schlueter"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417865",
        "url": "https://doi.org/10.5281/zenodo.1417865",
        "ee": "https://zenodo.org/records/1417865/files/FlexerSS12.pdf",
        "abstract": "We use results from the 2011 MIREX “Audio Music Sim- ilarity and Retrieval” task for a meta analysis of the hub phenomenon. Hub songs appear similar to an undesirably high number of other songs due to a problem of measuring distances in high dimensional spaces. Comparing 17 algo- rithms we are able to confirm that different algorithms pro- duce very different degrees of hubness. We also show that hub songs exhibit less perceptual similarity to the songs they are close to, according to an audio similarity func- tion, than non-hub songs. Application of the recently intro- duced method of “mutual proximity” is able to decisively improve this situation.",
        "zenodo_id": 1417865,
        "dblp_key": "conf/ismir/FlexerSS12",
        "keywords": [
            "MIREX",
            "Audio Music Similarity",
            "Hub Phenomenon",
            "High Dimensional Spaces",
            "Algorithm Comparison",
            "Perceptual Similarity",
            "Audio Similarity Function",
            "Mutual Proximity",
            "Decisively Improve",
            "Task Results"
        ],
        "content": "A MIREX META-ANALYSIS OF HUBNESS\nIN AUDIO MUSIC SIMILARITY\nArthur Flexer, Dominik Schnitzer, Jan Schl ¨uter\nAustrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria\narthur.flexer|dominik.schnitzer|jan.schlueter@ofai.at\nABSTRACT\nWe use results from the 2011 MIREX “Audio Music Sim-\nilarity and Retrieval” task for a meta analysis of the hub\nphenomenon. Hub songs appear similar to an undesirably\nhigh number of other songs due to a problem of measuring\ndistances in high dimensional spaces. Comparing 17 algo-\nrithms we are able to conﬁrm that different algorithms pro-\nduce very different degrees of hubness. We also show that\nhub songs exhibit less perceptual similarity to the songs\nthey are close to, according to an audio similarity func-\ntion, than non-hub songs. Application of the recently intro-\nduced method of “mutual proximity” is able to decisively\nimprove this situation.\n1. INTRODUCTION\nIn a number of recent publications [21,27,28] the so-called\n“hubness” phenomenon has been described and explored\nas a general problem of machine learning in high dimen-\nsional data spaces. Hubs are data points which keep ap-\npearing unwontedly often in nearest neighbor lists of many\nother data points. This effect is particularly problematic in\nalgorithms for similarity search, as the same “similar” ob-\njects are found over and over again. In Music Information\nRetrieval (MIR), the hub problem has been primarily stud-\nied in the context of music recommendation based on mod-\neling of audio similarity. Songs which act as hubs are re-\nported as being similar to very many other songs and hence\nkeep a signiﬁcant proportion of the audio collection from\nbeing recommended at all. This paper tries to answer the\nfollowing questions concerning hubs in audio music sim-\nilarity which so far have not been solved to a satisfactory\ndegree: (i) Do different parameterizations and algorithms\nproduce different hubs? (ii) Are hub songs perceptually\nmeaningful?\nThis is done by conducting a meta-analysis of 17 algo-\nrithms and utilizing 8500 human gradings of the perceptual\nsimilarity of song pairs. A recently published method [29]\n(“mutual proximity”) is applied to reduce the negative ef-\nfects of hubness.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.2. RELATED WORK\nOne of the central notions of Music Information Retrieval\n(MIR) is that of music similarity. Proper modeling of mu-\nsic similarity is at the heart of every application allowing\nautomatic organization and processing of music data bases.\nA fundamental constituent to computation of music simi-\nlarity is timbre similarity based on parameterization of au-\ndio using Mel Frequency Cepstrum Coefﬁcients (MFCCs)\nplus Gaussian mixtures as statistical modeling [22]. It is\nprecisely for this approach to music similarity where exis-\ntence of hubs has been ﬁrst documented and established in\nMIR by Aucouturier and Pachet in 2004 [3]. Hub songs\nwere deﬁned as songs which are, according to the au-\ndio similarity function, similar to very many other songs\nand therefore keep appearing unwontedly often in recom-\nmendation lists preventing other songs from being recom-\nmended at all. Such songs that do not appear in any recom-\nmendation list have been termed “orphans”. The authors\nfurther stated that hub songs “objectively have nothing to\ndo with the seed song” [3], i.e. they share no perceptual\nsimilarity with the songs they are recommended for ac-\ncording to the audio similarity function. Only anectodic\nevidence in the form of two examples is provided for this\nrather general statement. Since the data set for this study\nwas quite small (350 songs from 37 artists), the authors re-\nmark that “a further study should be done with a larger\ndatabase”. Similar observations about false positives in\nmusic recommendation that are not perceptually meaning-\nful have been made elsewhere [24] using an even smaller\ndata set.\nFollowing this initial report about the hub problem a\nnumber of results concerning hubness in the context of\nMIR have been established. Aucouturier and Pachet [4]\nshowed that hubs are distributed along a scale-free distri-\nbution, i.e. non-hub songs are extremely common and large\nhubs are extremely rare. This is true for MFCCs mod-\nelled with different kinds of Gaussian mixtures as well as\nHidden Markov Models, irrespective whether parametric\nKullback-Leibler divergence or non-parametric histograms\nplus Euclidean distances are used for computation of sim-\nilarity. But is also true that hubness is not the property\nof a song per se since non-parametric and parametric ap-\nproaches produce very different hubs. The hub effect is\nnot an artefact of using small data sets in computer experi-\nments since it also exists in very large databases (> 250000\nsongs) and gets even worse with growing size of databases[14]. Not all parameterizations of audio are equally prone\nto hubness. Fluctuation patterns (FP) [16, 23] have been\nshown to produce almost no hubs and a combination of\nMFCCs and FPs is able to reduce hubness while maintain-\ning an overall high quality of audio similarity [12, 14]. It\nhas also been noted that audio recorded from urban sound-\nscapes, different from polyphonic music, does not produce\nhubs [2] since its spectral content seems to be more homo-\ngeneous and therefore probably easier to model. The same\nhas been observed for monophonic sounds from individ-\nual instruments [17]. Direct interference with the Gaussian\nmodels during or after learning has also been explored (e.g.\nhomogenization of model variances) although with mixed\nresults. Whereas some authors report an increase in hub-\nness [4], others observed the opposite [18]. Using a Hierar-\nchical Dirichlet Process instead of Gaussians for modeling\nMFCCs seems to avoid the hub problem altogether [20].\nThe existence of the hub problem has also been reported\nfor music recommendation based on collaborative ﬁltering\ninstead of on audio content analysis [8]. Similar effects\nexist for image [10, 19] and text retrieval [28] making this\nphenomenon a general problem in multimedia retrieval and\nrecommendation.\nBerenzweig [7] was probably the ﬁrst to suspect a con-\nnection between the hub problem and the high dimension-\nality of the feature space. The hub problem was seen as\na direct result of the curse of dimensionality [5], a term\nwhich refers to a number of challenges due to the high di-\nmensionality of data spaces. Radovanovi ´c et al [27, 28]\nwere able to provide more insight by linking the hub prob-\nlem to the property of concentration [15] which occurs as\na natural consequence of high dimensionality. Concentra-\ntion is the surprising characteristic of all points in a high\ndimensional space to be at almost the same distance to all\nother points in that space. It is usually measured as a ra-\ntio between spread and magnitude, e.g. the ratio between\nthe standard deviation of all distances to an arbitrary refer-\nence point and the mean of these distances. If the standard\ndeviation stays more or less constant with growing dimen-\nsionality while the mean keeps growing the ratio converges\nto zero with dimensionality going to inﬁnity. In such a\ncase it is said that the distances concentrate. This has been\nstudied for Euclidean spaces and other lpnorms [1, 15].\nRadovanovi ´c et al [28] presented the argument that in the\nﬁnite case, some points are expected to be closer to the\ncenter than other points and are at the same time closer, on\naverage, to all other points. Such points closer to the center\nhave a high probability of being hubs, i.e. of appearing in\nnearest neighbor lists of many other points. Points which\nare further away from the center have a high probability of\nbeing “orphans”, i.e. points that never appear in any near-\nest neighbor list. This concentration of distances has also\nbeen reported for audio data [21].\nAlready in the context of concentration of distances it\nhas been noted that the degree of concentration depends on\nthe intrinsic rather than embedding dimension of the fea-\nture space [15]. Whereas the embedding dimension is the\nactual number of dimensions of a feature space the intrin-sic dimension is the, often much smaller, number of dimen-\nsions necessary to represent a feature space without loss of\ninformation. It has also been demonstrated that hubness\ndepends on the intrinsic rather than embedding dimension-\nality [28].\nA direct consequence of the presence of hubs is that a\nlarge number of nearest neighbor relations in the distance\nspace are asymmetric, i.e., a hub yis the nearest neighbor\nofx, but the nearest neighbor of the hub yis another point\na(a6=x). This is because hubs are nearest neighbors to\nvery many data points but only kdata points can be near-\nest neighbors to a hub since the size of a nearest neighbor\nlist is ﬁxed. This behavior is especially problematic if x\nandybelong to the same class but adoes not, violating\nthe pairwise stability of clusters [6]. In a recent publica-\ntion [29] a general unsupervised method to attenuate the\nnegative effects of hubness by repairing asymmetric near-\nest neighbor relations has been presented. It transforms\narbitrary distance matrices to matrices of so-called proba-\nbilistic mutual proximity (MP). On a range of audio data\nsets it has been demonstrated that it is indeed able to de-\ncrease hubness while improving audio similarity as mea-\nsured with genre classiﬁcation accuracy. Since we will use\nthis method to improve results for the MIREX data set it\nwill be described in more detail in section 5.3. Please note\nthat MP can be seen as a reﬁnement of the so-called “P-\nnorm” [26], which has been applied to the hub problem by\nother authors too [9].\n3. DATA AND ALGORITHMS\nFor our meta-analysis of hubness we use the data from\nthe recent 2011 “Audio Music Similarity and Retrieval”\ntask1within the annual MIREX [11] evaluation campaign\nfor MIR algorithms. Each of 18 competing algorithms\nwas given 7000 songs (30 second audio clips). The data\nconsists of 10 almost equally sized genre classes: 700\nsongs from BAROQUE, COUNTRY , EDANCE, JAZZ,\nMETAL, RAPHIPHOP, ROCKROLL, ROMANTIC, 699\nfrom BLUES, 701 from CLASSICAL. Every algorithm\nwas given these 7000 song excerpts and returned either\na full 7000\u00027000 distance matrix (algorithms CTCP1,\nCTCP2, CTCP3, DM2, DM3, ML1, ML2, ML3, SSKS3,\nSSPK2, STBD1, STBD2, STBD3) or a matrix of size\n7000\u0002100(GKC1, HKHLL, PS1, YL1) containing the\nﬁrst 100 nearest neighbors to each song. The resulting dis-\ntance matrix for algorithm ZYC2 is faulty containing the\nsame distance of the same pair of songs over and over again\nand is therefore excluded from our analysis2. Please note\nthat some of the systems are very closely related, some-\ntimes using just different parameters for the same algo-\nrithm (e.g. CTCP1-3, DM2-3, ML1-3, STBD1-3).\nFrom the 7000 songs, “100 songs were randomly se-\nlected from the 10 genre groups (10 per genre) as queries\n1The 2011 results and details can be found at:\nhttp://www.music-ir.org/mirex/wiki/2011:\nAudio Music Similarity andRetrieval Results\n2Please note that ZYC2 did participate in the MIREX task and even\nscored in the mid-ﬁeld of results. This seems to be due to nonrandom\ngenre order during evaluation and not to its real performance.and the ﬁrst 5 most highly ranked songs out of the 7000\nwere extracted for each query (after ﬁltering out the query\nitself, returned results from the same artist were also omit-\nted). Then, for each query, the returned results (candidates)\nfrom all participants were grouped and were evaluated by\nhuman graders”1. For each individual query/candidate\npair, a single human grader provided both a FINE score\n(from 0 (failure) to 100 (perfection)) and a BROAD score\n(not similar NS, somewhat similar SS, very similar VS) in-\ndicating how similar the songs are in their opinion. Since\nwe use FINE scores only, this altogether gives 17\u0002100\u0002\n5 = 8500 human gradings for our analysis.\n4. EV ALUATION\nThe following measures are used to evaluate the hubness\nphenomenon in section 5. Abbreviations correspond to la-\nbels in the result table 1.\nk-occurrence statistics (H25/cov, H50/cov, maxH):\nAs a measure of the hubness of a given song we use the\nso-called k-occurrence Nk[4], i.e. the number of times\nthe song occurs in the ﬁrst knearest neighbors of all the\nother songs in the data base. Please note that the mean\nk-occurrence across all songs in a data base is equal to k.\nAnyk-occurrence signiﬁcantly bigger than ktherefore in-\ndicates existence of a hub. Since human graders evaluated\nthe ﬁve most similar songs we used k= 5. We com-\npute the absolute number of the maximum k-occurrence\nmaxH (i.e. the biggest hub) and the number of songs for\nwhich the k-occurrence is bigger than 25 or 50 (i.e. the\nnumber of small hubs H25and large hubs H50). Ad-\nditionally we give the number of these hubs that appear\nas candidate songs in the human grading evaluation, e.g.\n“H25=cov = 6=3” means that out of six hub songs with k-\noccurrence bigger than 25 three songs have been evaluated\n(covered) by human graders.\nHubness (hub): We compute the hubness for each al-\ngorithm’s distance matrix according to Radovanovi ´c et\nal. [28]. Hubness is deﬁned as the skewness of the dis-\ntribution of k-occurrences Nkof a whole data set. Positive\nskewness indicates high hubness, negative values low hub-\nness.\nReachability (reach): This is the percentage of songs\nfrom the whole data base that are part of at least one of the\nnearest neighbor lists. If a song is not part of any of the\nrecommendation lists of size k= 5 it is an orphan song\nwhich will never be recommended as a candidate song.\nNumber of hub gradings (#H): For every algorithm,\n500 human gradings exist for further analysis. The number\nof hub gradings is the number of gradings where the can-\ndidate song in a query/candidate pair is a hub song. This\nis given using k-occurrences of 25 (H25) and 50 (H50) to\ndistinguish between hubs and non-hubs.\nFine Score (ﬁneH, ﬁneNH, ﬁne): To evaluate the per-\nceptual quality of hubs and non-hubs we compute average\nﬁne scores. For every song in the whole data base we check\nwhether it was the candidate in any of the query/candidate\npairs in the human evaluation experiment. We average\nall respective ﬁne scores for hub songs (ﬁneH) and non-hub songs (ﬁneNH) separately. This is done using k-\noccurrences of 25 (H25) and 50 (H50) to distinguish be-\ntween hubs and non-hubs. We also include the average\nacross all ﬁne scores (ﬁne) irrespective of whether candi-\ndate songs are hubs or not.\nAccuracy (acc): To evaluate the quality of audio sim-\nilarity for the whole data base, and not just for the songs\nwhich have been evaluated by human graders, we com-\nputed the genre classiﬁcation performance. Since songs\nwithin a certain genre will also sound more similar than\nsongs from different genres, high genre classiﬁcation re-\nsults indicate good audio similarity measures. It has also\nbeen demonstrated that algorithms achieving high genre\nclassiﬁcation results are able to produce results that corre-\nlate higher with human music similarity judgements [25, p.\n26]. We compute the k= 5-nearest neighbor classiﬁcation\naccuracy, i.e. the percentage of the ﬁve nearest songs that\nhave the same label as the query song. Songs from the\nsame artist as a query song are omitted from the nearest\nneighbor list by using an artist ﬁlter [13].\n5. RESULTS\nAll results discussed in the following section can be found\nin table 1 listing all evaluation measures (from column hub\ntofineNH , see section 4) for all algorithms (from CTCP1\nto YL1, see section 3) plus improved results using “mutual\nproximity” (rows mp, see section 5.3).\n5.1 Hubness across algorithms\nAs already discussed in section 2 hubness is not a property\nof an individual song but is connected to what features are\ncomputed from the audio, what models are being learned\nfrom the features and how these processing steps are af-\nfected by the concentration of distances in high dimen-\nsional spaces. The MIREX audio similarity results provide\nthe opportunity to compare 17 different algorithms with re-\nspect to their hubness. Looking at the results in table 1 it is\napparent that the different algorithms produce very differ-\nent degrees of hubness. The hubness values (column hub)\nrange from 0.96 (HKHLL1) to 3.98 (STBD3) indicating\nthat all distributions of k-occurrences are skewed to the\nright, i.e. are prone to hubness. Looking at the numbers\nof small ( H25) and large (H 50) hubs it is also clear that\nthe different algorithms produce very different numbers of\nhubs. The number of small hubs range from 0 (SSKS3)\nto 256 (STBD3), those of large hubs from 0 (CTCP1-\n3, GKC1, HKHLL1, ML1, ML3, SSKS3, SSPK2) to 60\n(STBD3). The largest k-occurrence maxH ranges from\n21 (HKHLL1) to 122 (STBD3). The average correlation\nofk-occurrences of all songs across all pairs of algorithms\nis only 0:14 showing that different algorithms produce very\ndifferent hubness for a song. It is interesting to note that\nclosely related algorithms show much higher correlations\n(e.g. an average of 0:76 for CTCP1, CTCP2 and CTCP3).\nThe reachability reach ranges from small 65:5% (STBD3)\nup to 95:9% (SSKS3).H25 H50\nalgo hub acc H25/co vH50/co vmaxH reach ﬁne #H ﬁneH ﬁneNH #H ﬁneH ﬁneNH\nCTCP1 1.28 59.42 6/3 0/0 31 93.8 57.3 3 60.0 57.3 0 - 57.3\nmp 1.32 58.75 9 0 35 92.5\nCTCP2 1.08 59.66 1/1 0/0 26 94.9 58.6 2 41.0 58.7 0 - 58.6\nmp 1.02 59.20 0 0 25 94.4\nCTCP3 1.41 60.07 12/3 0/0 35 92.3 56.2 3 44.3 56.3 0 - 56.2\nmp 1.41 59.38 10 0 36 90.3\nPS1 2.43 59.52 32/15 2/2 81 88.0 57.7 19 55.9 57.8 2 76.5 57.6\nmp 1.02 54.80 1 0 31 99.1\nSSKS3 0.93 60.12 0/0 0/0 25 95.9 58.1 0 - 58.1 0 - 58.1\nmp 1.13 59.61 3 0 31 93.8\nSSPK2 1.19 59.67 5/3 0/0 33 94.5 58.6 3 73.0 58.6 0 - 58.6\nmp 1.33 58.95 5 0 38 93.7\nDM2 2.80 50.62 68/22 4/3 90 84.2 50.5 29 48.2 50.6 5 36.2 50.6\nmp 2.15 51.19 40 1 58 88.9\nDM3 2.83 50.69 76/32 7/3 85 84.4 50.3 43 47.2 50.6 4 42.0 50.4\nmp 2.15 51.03 48 1 58 88.9\nGKC1 1.31 25.83 11/3 0/0 34 90.8 31.8 3 30.0 31.9 0 - 31.8\nmp 0.16 26.52 0 0 15 97.8\nHKHLL1 0.96 38.40 0/0 0/0 23 93.3 42.2 0 - 42.2 0 - 42.2\nmp 0.48 38.76 0 0 24 98.3\nML1 2.19 45.95 61/22 0/0 47 85.9 47.8 26 46.5 47.9 0 - 47.8\nmp 1.06 48.22 2 0 30 93.8\nML2 2.17 44.22 60/19 1/0 54 87.4 47.3 25 36.1 47.9 0 - 47.3\nmp 1.08 45.74 2 1 30 94.4\nML3 1.49 45.17 12/3 0/0 38 90.8 47.8 3 47.7 47.8 0 - 47.8\nmp 0.94 44.74 1 0 26 95.2\nSTBD1 3.46 26.62 161/71 17/10 90 81.0 33.9 86 29.5 34.8 16 22.9 34.3\nmp 1.40 28.81 5 0 45 93.7\nSTBD2 2.88 25.91 135/60 9/5 70 82.0 30.6 72 28.0 31.0 6 21.3 30.7\nmp 1.24 27.37 3 0 34 95.5\nSTBD3 3.98 25.38 256/113 60/35 122 65.5 30.4 149 29.3 30.9 54 23.0 31.3\nmp 2.18 26.88 64 2 57 86.1\nYL1 2.16 41.14 65/21 1/0 54 84.4 42.4 27 33.4 42.9 0 - 42.4\nmp 1.82 41.01 2 1 65 95.6\nTable 1. All results (please see section 5 for details) for all evaluation measures (from column hub tofineNH , see\nsection 4) for all algorithms (from CTCP1 to YL1, see section 3) plus improved results using “mutual proximity” (rows\nmp, see section 5.3). Algorithms CTCP1 to SSPK2 (top six rows) already use “P-norm” and therefore do not show\nimprovements due to mp, see section 5.3.\nTo sum up, different algorithms indeed produce very\ndifferent degrees of hubness.\n5.2 Hubness and perceptual quality\nThe next question we like to clarify is whether hub songs\nexhibit less perceptual similarity to the songs they are close\nto (according to an audio similarity function) than non-hub\nsongs.\nThe correlation between hubness and average ﬁne score\nof all algorithms (columns hub andfine in table 1) is\n\u00000:56. This indicates that algorithms generating large\nhubness show low ﬁne scores, i.e. overall bad perceptual\nsimilarity. Notable exceptions are maybe GKC1 with low\nhubness and low ﬁne scores (hub = 1:31, fine = 31:8)\nand PS1 with rather high hubness and high ﬁne scores\n(hub = 2:43, fine = 57:7).\nAnalyzing the differences in ﬁne scores between hubsand non-hubs, we can see that the average ﬁne scores\nfor small hubs (fineH ,H25) are almost always smaller\nthan those for non-hubs (fineNH ,H25). The only ex-\nceptions are algorithms CTCP1 and SSPK2. The average\ndifference in ﬁne scores is 3:65. This average is taken\nacross all algorithms where grading information for hubs\nis available (#H > 0). The average ﬁne scores for large\nhubs (fineH ,H50) are again almost always smaller than\nthose for non-hubs (fineNH ,H50), with algorithm PS1\nbeing the only exception. The average difference in ﬁne\nscores is 5:49. Again this average is taken across all al-\ngorithms where grading information for hubs is available\n(#H > 0). Please note that e.g. for PS1, only two human\ngradings of large hub songs do exist.\nTo sum up, both small and large hubs seem to be less\nperceptually meaningful than non-hub songs but the aver-\nage difference in human gradings is only 3:65 to5:49 ona scale from 0 to 100. Audio similarity computed with\nalgorithms showing high hubness seems to be less percep-\ntually meaningful than that of algorithms with low hubness\nin general.\n5.3 Reducing hubness\nTo reduce the negative effects of hubness we apply “mu-\ntual proximity” (MP) [29] to the distance matrices from\nall algorithms. MP takes a distance matrix and (i) trans-\nforms distances between points x and y into probabili-\nties that y is closest neighbor to x given the distribution\nof all distances to x in the data base, (ii) combines these\n(asymmetric) probabilistic distances from x to y and y to\nx via the product rule. The ﬁrst step of transformation to\nprobabilities re-scales and normalizes the distances like a\nz-transform. The second step combines the probabilities\nto a mutual measure thereby repairing sometimes contra-\ndicting, asymmetric nearest neighbor information which\nseems to cause hubness in similarity measures. Please\nnote that MP requires knowledge of the full distance ma-\ntrix since it needs to compute means and variances across\nfull rows and columns during the re-scaling step. For al-\ngorithms GKC1, HKHLL, PS1 and YL1 we only have dis-\ntances to the ﬁrst 100 nearest neighbors of each song. In\nthis case we use this limited set of distances instead of full\nrows and columns for computation of MP.\nBefore discussing the improvements due to MP it has\nto be said that six of the competing algorithms (CTCP1-\n3, PS1, SSKS3, SSPK2) already use a transformation of\nthe distance matrix similar to MP. Usage of the so-called\n“P-norm” [26], which can be seen as a predeccesor to MP,\ntogether with application of MP does not seem to improve\nhubness. As a matter of fact, it sometimes even worsens\nthe hubness situation. On the other hand, the six algorithms\nemploying the “P-norm” already are the six best rank-\ning systems according to their average ﬁne score. There-\nfore we now discuss only those algorithms that do not use\nthe “P-norm” already (DM2-3, GKC1, HKHLL1, ML1-3,\nSTBD1-3, YL1).\nComparing hubness indices (column hubin table 1) be-\ntween original algorithms and their improved MP version\n(rows mp) it can be seen that all values improve. The av-\nerage decrease in hubness (hub) is 45:5%. The number\nof small hubs H25also always decrease with an average\nof83% less hubs. The average decrease in the number of\nlarge hubs H50is79:6%. The average decrease of the\nlargest hub maxH is33:2%. Only HKHLL1 and YL1\nshow a slightly larger maxH , with all other indices also di-\nminishing. The reachability reach for all algorithms is en-\nhanced, on average by 8:95 percentage points. This means\nthat audio simialrity re-scaled with MP produces less or-\nphan songs and includes a larger part of the data base in\nthe nearest neighbor lists. All these improvements seem\nto be accompanied with unchanged quality in audio simi-\nlarity. At least all genre classiﬁcation results (column acc)\nremain more or less constant, with some insigniﬁcant in-\ncreases and decreases.\nTo sum up, mutual proximity (MP) is able to decisivelyimprove the hubness situation while not changing the over-\nall performance in audio similarity.\n6. DISCUSSION AND CONCLUSION\nIn this paper we were able to explore two important ques-\ntions concerning hubness in audio music similarity which\nso far have not been answered satisfactory. We have\ncorroborated earlier results indicating that different fea-\ntures computed from the audio in combination with dif-\nferent models being learned produce very different de-\ngrees of hubness. This was done by comparing a yet\nunprecedented number of different approaches (17 algo-\nrithms from the 2011 MIREX “Audio Music Similarity\nand Retrieval” task). We were also able to show that hub\nsongs, when being recommended as being very similar, are\njudged to be less perceptually meaningful than non-hub\nsongs by human evaluators. This was done by conducting\nthe ﬁrst systematic and extensive study on the perceptual\nquality of hub songs based on human evaluations again us-\ning MIREX data. Last but not least we were able to show\nthat it is possible to reduce the many negative effects of\nhubness by applying “mutual proximity” to re-scale audio\nsimilarity distances.\n7. ACKNOWLEDGEMENTS\nMany thanks are due to the spiffy MIREX people (espe-\ncially Stephen Downie and Andreas Ehmann) for conduct-\ning the audio music similarity and retrieval evaluation in\nthe ﬁrst place and for making the data needed for our re-\nsearch available. This research is supported by the Aus-\ntrian Science Fund (FWF, grant P24095) and the Vienna\nScience and Technology Fund (WWTF, project “Audio-\nminer” MA09-024).\n8. REFERENCES\n[1] Aggarwal C.C., Hinneburg A., Keim D.A.: On the\nSurprising Behavior of Distance Metrics in High Di-\nmensional Spaces, Proceedings of the 8th International\nConference on Database Theory, Springer-Verlag Lon-\ndon, UK, pages 420-434, 2001.\n[2] Aucouturier J.-J., Defreville B., Pachet F.: The bag-of-\nframes approach to audio pattern recognition: A sufﬁ-\ncient model for urban soundscapes but not for poly-\nphonic music, Journal of the Acoustical Society of\nAmerica, 122 (2), 881-891, 2007.\n[3] Aucouturier, J.-J., Pachet F.: Improving Timbre Simi-\nlarity: How high is the sky?, Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n[4] Aucouturier J.-J., Pachet F.: A scale-free distribution of\nfalse positives for a large class of audio similarity mea-\nsures, Pattern Recognition, V ol. 41(1), pp. 272-284,\n2007.\n[5] Bellman R.E.: Adaptive Control Processes: A Guided\nTour, Princeton University Press, 1961.[6] Bennett K.P., Fayyad U., Geiger D.: Density-based in-\ndexing for approximate nearest-neighbor queries, Pro-\nceedings of the ﬁfth ACM SIGKDD international con-\nference on Knowledge discovery and data mining,\nKDD ’99, pages 233-243, New York, NY , USA, 1999.\n[7] Berenzweig A.: Anchors and hubs in audio-based mu-\nsic similarity, PhD thesis, Columbia University, New\nYork, USA, 2007.\n[8] Celma, O.: Music Recommendation and Discovery in\nthe Long Tail, PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2008.\n[9] Charbuillet C., Tardieu D., Peeters G.: GMM super-\nvector for content based music similarity, Proceedings\nof the 14th Int. Conference on Digital Audio Effects\n(DAFx-11), Paris, France, 2011.\n[10] Doddington G., Liggett W., Martin A., Przybocki\nM., Reynolds D.A.: SHEEP, GOATS, LAMBS and\nWOLVES: A Statistical Analysis of Speaker Perfor-\nmance in the NIST 1998 Speaker Recognition Eval-\nuation, in Proceedings of the 5th International Con-\nference on Spoken Language Processing (ICSLP’98),\nSydney, Australia, 1998.\n[11] Downie J.S.: The Music Information Retrieval Eval-\nuation eXchange (MIREX), D-Lib Magazine, V olume\n12, Number 12, 2006.\n[12] Flexer A., Gasser M., Schnitzer D.: Limitations of in-\nteractive music recommendation based on audio con-\ntent, Proc. of the 5th Audio Mostly Conference, pp.\n96-102, 2010.\n[13] Flexer A., Schnitzer D.: Effects of Album and Artist\nFilters in Audio Similarity Computed for Very Large\nMusic Databases, Computer Music Journal, V olume\n34, Number 3, pp. 20-28, 2010.\n[14] Flexer A., Schnitzer D., Gasser M., Pohle T.: Combin-\ning features reduces hubness in audio similarity, Proc.\nof the Eleventh International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2010), 2010.\n[15] Francois D., Wertz V ., Verleysen M.: The concen-\ntration of fractional distances, IEEE Transactions on\nKnowledge and Data Engineering, V ol. 19, No. 7, pp.\n873-886, 2007.\n[16] Fruehwirt M., Rauber A.: Self-Organizing Maps for\nContent-Based Music Clustering, Proceedings of the\nTwelth Italian Workshop on Neural Nets, IIAS, 2001.\n[17] Gasser M., Flexer A., Schnitzer D.: Hubs and Orphans\n- an Explorative Approach, Proceedings of the 7th\nSound and Music Computing Conference (SMC’10),\n2010.\n[18] Godfrey M.T., Chordia P.: Hubs and Homogeneity:\nImproving Content-Based Music Modeling, Proceed-\nings of the 9th International Conference on Music In-\nformation Retrieval (ISMIR’08), 2008.[19] Hicklin R.A., Watson C.I., Ulery B.: The Myth of the\nGoats: How Many People Have Fingerprints that are\nHard to Match?, The National Institute of Standards\nand Technology (NIST), NIST Interagency/Internal\nReport (NISTIR) - 7271, 2005.\n[20] Hoffman M., Blei D., Cook P.: Content-Based Musical\nSimilarity Computation Using the Hierarchical Dirich-\nlet Process, Proceedings of the 9th International Con-\nference on Music Information Retrieval (ISMIR’08),\n2008.\n[21] Karydis I., Radovanovi ´c M., Nanopoulos A., Ivanovi ´c\nM.: Looking through the ”glass ceiling”: A conceptual\nframework for the problems of spectral similarity, Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR’10), pages\n267-272, 2010.\n[22] Logan B.: Mel Frequency Cepstral Coefﬁcients for\nMusic Modeling, Proceedings of the International\nSymposium on Music Information Retrieval (IS-\nMIR’00), 2000.\n[23] Pampalk E.: Computational Models of Music Sim-\nilarity and their Application to Music Information\nRetrieval, Vienna University of Technology, Austria,\nDoctoral Thesis, 2006.\n[24] Pampalk E., Dixon S., Widmer G.: On the Evaluation\nof Perceptual Similarity Measures for Music, Proceed-\nings of the 6th International Conference on Digital Au-\ndio Effects (DAFx-03), pp. 7-12, London, U.K., 2003.\n[25] Pohle T.: Automatic Characterization of Music for In-\ntuitive Retrieval, PhD Thesis, Johannes Kepler Univer-\nsity, Linz, Austria, 2010.\n[26] Pohle T., Schnitzer D., Schedl M., Knees P., Widmer\nG.: On rhythm and general music similarity, Proceed-\nings of the 10th International Conference on Music In-\nformation Retrieval (ISMIR’09), 2009.\n[27] Radovanovi ´c M., Nanopoulos A., Ivanovi ´c M.: Near-\nest neighbors in high-dimensional data: The emer-\ngence and inﬂuence of hubs, Proceedings of the\n26th International Conference on Machine Learning\n(ICML’09), ACM International Conference Proceed-\ning Series, volume 382, pages 865-872, 2009.\n[28] Radovanovi ´c M., Nanopoulos A., Ivanovi ´c M.:\nHubs in space: Popular nearest neighbors in high-\ndimensional data, Journal of Machine Learning Re-\nsearch, 11:2487-2531, 2010.\n[29] Schnitzer D., Flexer A., Schedl M., Widmer G.: Us-\ning Mutual Proximity to Improve Content-Based Au-\ndio Similarity, in Proceedings of the 12th International\nSociety for Music Information Retrieval Conference\n(ISMIR’11), Miami, Florida, USA, 2011."
    },
    {
        "title": "A Feature Relevance Study for Guitar Tone Classification.",
        "author": [
            "Wolfgang Fohl",
            "Andreas Meisel",
            "Ivan Turkalj"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414750",
        "url": "https://doi.org/10.5281/zenodo.1414750",
        "ee": "https://zenodo.org/records/1414750/files/FohlMT12.pdf",
        "abstract": "A series of experiments on the automatic classification of classical guitar sounds with support vector machines has been carried out to investigate the relevance of the features and to minimise the feature set for successful classifica- tion. Features used for classification were the time series of the partial tone amplitudes, and of the MFCCs, and the energy distribution of the nontonal percussive sound that is produced in the attack phase of the tone. Furthermore the influence of sound parameters as timbre, player, fret position and string number on the recognition rate is in- vestigated. Finally, several nonlinear kernels are compared in their classification performance. It turns out, that a se- lection of 505 features out of the full feature set of 1155 elements does only reduce the recognition rate of a linear SVM from 82% to 78%. With the use of a polynomial instead of a linear kernel the recognition rate with the re- duced feature set can even be increased to 84%.",
        "zenodo_id": 1414750,
        "dblp_key": "conf/ismir/FohlMT12",
        "keywords": [
            "classical guitar sounds",
            "support vector machines",
            "features",
            "time series",
            "MFCCs",
            "energy distribution",
            "nontonal percussive sound",
            "attack phase",
            "recognition rate",
            "nonlinear kernels"
        ],
        "content": "A FEATURE RELEV ANCE STUDY FOR GUITAR TONE CLASSIFICATION\nWolfgang Fohl Ivan Turkalj Andreas Meisel\nHAW Hamburg University of Applied Sciences\nfwolfgang.fohl | ivan.turkalj | andreas.meiselg@haw-hamburg.de\nABSTRACT\nA series of experiments on the automatic classiﬁcation of\nclassical guitar sounds with support vector machines has\nbeen carried out to investigate the relevance of the features\nand to minimise the feature set for successful classiﬁca-\ntion. Features used for classiﬁcation were the time series\nof the partial tone amplitudes, and of the MFCCs, and the\nenergy distribution of the nontonal percussive sound that\nis produced in the attack phase of the tone. Furthermore\nthe inﬂuence of sound parameters as timbre, player, fret\nposition and string number on the recognition rate is in-\nvestigated. Finally, several nonlinear kernels are compared\nin their classiﬁcation performance. It turns out, that a se-\nlection of 505 features out of the full feature set of 1155\nelements does only reduce the recognition rate of a linear\nSVM from 82% to 78%. With the use of a polynomial\ninstead of a linear kernel the recognition rate with the re-\nduced feature set can even be increased to 84%.\n1. INTRODUCTION\nIn the recent years musical instrument recognition has been\nextensively investigated. Primary research goals were au-\ntomatic indexing of multimedia data bases, automatic mu-\nsical genre classiﬁcation and automatic music transcrip-\ntion systems. A less common topic is the quality assess-\nment of musical instruments, which will be covered in the\npresent paper. Currently the research efforts show sev-\neral trends. One is the attempt to transfer the success-\nful classiﬁcation based on single notes to the analysis and\nclassiﬁcation of solo musical phrases. Joder, Essid, and\nRichard [9] describe a modiﬁcation of Support Vector Ma-\nchines with alignment kernels which they report to perform\nbetter than classiﬁers based on Gaussian Mixture Models\nor Hidden Markov Models. Barbedo and Tsanetakis [2]\npublished their results on the even more challenging task of\ninstrument classiﬁcation in polyphonic recordings. Their\nmethod is the detection of partial tone structures that are\nunique to certain instrument groups, which are fed to a spe-\ncialised decision-tree algorithm.\nA second trend in instrument classiﬁcation research is\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.the systematic comparison of the performance of certain\nstatistical methods and commonly used feature sets in or-\nder to reduce the feature space dimensionality and thus es-\ncaping the “curse of dimensionality” – the exponential in-\ncrease of required training samples with increasing dimen-\nsion of the feature space. A detailed general description\nof algorithms and procedures of feature space reduction is\ngiven by Guyon and Elisseeff [6]. Deng, Simmermacher,\nand Craneﬁeld published a very good and complete sur-\nvey on feature relevance for musical instrument classiﬁca-\ntion [3]. Loughran, Walker, and O’Neill present a genetic\nalgorithm approach to feature selection [11]. Genetic al-\ngorithms can even be employed for feature generation, as\ndescribed by Mierswa and Morik [12], and by Pachet and\nRoy [13]. One outcome of these approaches could be the\ngeneration of meaningful features, that give an insight in\nthe nature of the investigated sounds.\nThere is a paper on the quality assessment of musical\ninstruments by Hsiao and Su [7]. They used an waveform-\nbased feature set in conjunction with a multiclass Mahalanobis-\nTaguchi system to develop an automatic saxophone quality\nassessment system.\nIn this article we present a systematic study on the pa-\nrameters inﬂuencing the classiﬁcation performance of a\nsupport vector machine (SVM) classiﬁer to distinguish sin-\ngle tones of three different classical guitars from each other.\nThis continues an earlier work published on the 2008 DAFx\nconference [4].\nOur research motivation is to pinpoint those acoustical\nfeatures of high-quality instruments that are responsible for\nthe perceived musical quality of the guitar. The classiﬁ-\ncation experiments were conducted to further support our\nworking hypothesis, that the acoustical quality of an instru-\nment reveals itself at least partially already in a single tone.\nThis hypothesis is motivated by the way professional gui-\ntarists assess an instrument: They test the acoustical prop-\nerties of the guitar by carefully listening to single tones\nplayed on all strings and over the whole range of the ﬁn-\ngerboard.\nIn the following section our experimental setup and the\nfeature and sample selection strategy is described, followed\nby the presentation and discussion of classiﬁcation perfor-\nmance results. The article is concluded by a summary and\nan interpretation of the results in terms of musical acous-\ntics.-80-60-40-20020\n0 0.5 1 1.5 2 2.5Amplitude / rel. Units\nt/s\nPartial Tone 6\nP\nartial Tone 7\nPartial Tone 8\nPartial Tone 9\nPartial Tone 10\nPartial Tone 11Figure 1. Time series of partial tones 6 – 11 of a guitar\ntone.\n2. EXPERIMENTS\nSingle guitar tones of three high-quality classical guitars\n(by the luthiers Hense, Marin, and Wichmann) played by\nfour players on three different fret positions with the three\ndifferent sound intentions sonorous, sharp, and warm, were\nrecorded, resulting in \u00194000 tone samples, each with a\nduration of 2–3 seconds. These samples were normalised\nfor equal maximum amplitude and three groups of features\nwere extracted: The time series of the partial tone ampli-\ntudes (PT) (see ﬁgure 1 for an example), the time series of\nthe mel frequency cepstral coefﬁcients (MFCC) as shown\nin ﬁgure 2, and the power distribution of the nontonal spec-\ntrum (NT), see [5]. The partial tone time series was ob-\ntained by ﬁrst taking the magnitude spectrum of the whole\nlength of the sound sample, and f0was identiﬁed by cep-\nstral analysis. Then the sound was split into frames of 4096\nsamples, each frame multiplied with a Blackman window,\nthe FFT was calculated and the amplitudes of the ﬁrst 16\npartial peaks, i.e., the peaks in the vicinity of n\u0001f0, were\nevaluated. The data of the ﬁrst 40 frames was taken as the\npartial tone feature set.\nThe MFCC data was computed using the Matlab Audi-\ntory Toolbox by Slaney [14], with frame size of 1024 sam-\nples, and a frame frequency of 25Hz. The time series of\nthe ﬁrst 10 MFCCs was evaluated, and the ﬁrst 50 frames\nwere taken as the MFCC feature set.\nThe calculation of the nontonal features starts with the\nmagnitude spectrum of the whole sound sample, from which\nthe tonal peaks have been removed as shown in Fig. 3. To\nobtain the nontonal features, the nontonal power spectrum\nPkat frequency index kis computed by squaring the non-\ntonal spectrum Yk, and the accumulated power between\nfstart= 0andfend=fkis calculated to yield the fuction\nCk:\nCk=kX\ni=0Pk (1)\nThe logarithm of this monotonously increasing function\nis taken, and the range of logCkfrom 1 to its ﬁnal value\n05101520253035\n0 0.5 1 1.5 2 2.5MFCC Value / rel.\nUnits\nt/s\nMFCC 1\nMFCC 2\nMFCC3\nMFCC 4\nMFCC 5Figure 2. Time series of the ﬁrst 10 MFCCs of a guitar\ntone. Curves shifted vertically for better overview.\n-100-50050100\n0 500 1000 1500 2000 2500 3000 3500 4000Amplitude / dB\nf/Hz-100-50050100\n0 500 1000 1500 2000 2500 3000 3500 4000Amplitude / dB\nf/Hz\nFigure 3. Magnitude spectrum and nontonal magnitude\nspectrum of a guitar tone.\n/x2D/x33/x2D/x32/x2D/x31/x30/x31/x32/x33/x34\n/x30 /x35/x30 /x31/x30/x30 /x31/x35/x30 /x32/x30/x30 /x32/x35/x30 /x33/x30/x30 /x33/x35/x30 /x34/x30/x30logCk /x2F /x72/x65/x6C/x2E /x55/x6E/x69/x74/x73\n/x66 /x2F /x48/x7A∆ logCk6\n?1\n16∆ logCk?\n6\n\u0000\u0000\nf0\u0000\u0000\nf15\nFigure 4. Calculation of nontonal frequency features.\nPlotted is the function logCkfrom Eq. 1 and the frequen-\ncies, that divide the range \u0001 log Ckfrom logCk= 1to the\nmaximum value of logCkinto 16 equally spaced regions.\nThe corresponding frequencies f1. . .f15are the nontonal\nfeature values.GuitarA\nGuitar\nB\nGuitar C\nAvsB\nAvsC BvsC\nGuitar A Guitar C Guitar BnotB notA\nnotC notA notB notCFigure 5. A directed acyclic graph for multi-class identiﬁ-\ncation\nis split into 16 equally spaced parts. The corresponding 15\nboundary frequencies are taken as the nontonal features, as\nshown in ﬁgure 4.\nThe complete feature set consists of 1155 elements (40 \u0001\n16 PT + 50\u000110 MFCC + 15 NT).\nThese features or subsets of them were used for train-\ning the SVMs and for running the classiﬁcation tests. All\nbut the last group of experiments were performed with lin-\near SVM kernels, the best classiﬁcation performance with\nlinear kernels was 82:0 %, it was achieved using the full\nfeature set of 1155 elements.\nFor each guitar, a SVM for a one-vs-rest classiﬁcation\nwas trained. For the multi-class identiﬁcation, a directed\nacyclic graph (DAG) was constructed according to Fig. 5.\nTheclassiﬁcation performance is determined as the ra-\ntio of correctly classiﬁed test examples and the total num-\nber of examples\nPerformance =ncorrect\nntotal(2)\nA ﬂexible data processing software has been written in\nGNU Octave [1], a free MatlabTMclone, for feature ex-\ntraction and data selection. For the SVMs, the SVMLight\nimplementation of Joachims has been used [8].\nSeveral series of experiments were carried out to inves-\ntigate which features are most important for a correct clas-\nsiﬁcation, how small the feature set can be made without\ndegrading the classiﬁcation performance, and if there are\nnonlinear kernels that perform better than the linear ker-\nnel.\nIn addition, the training and testing was conducted with\ndifferent subsets of the guitar tone samples to investigate\nthe role of the player, and to test, if a preselection of sounds\naccording to several criteria can improve the classiﬁcation\nresult.\nTwo preliminary experiments were performed: A cross-\nvalidation of the classiﬁcation was made by exchanging\nthe sample sets for test and training. In the second prelimi-\nnary experiment an attempt was made to reduce the feature\nspace by performing a Principal Component Analysis on\nthe training data set.3. RESULTS AND DISCUSSION\n3.1 Classiﬁcation Performance With the Full Feature\nSet and a Linear Kernel\nAs a ﬁrst step, the classiﬁcation experiment has been car-\nried out with the full data set of 1155 features, 978 training\nsamples, 972 test samples, with a linear kernel.\nThe classiﬁcation performance of 82:0 % is taken as the\nreference value for all subsequent experiments.\n3.1.1 Cross-validation\nThe experiment was repeated with the test and training data\nexchanged. Table 1 compares the results of the two exper-\niments.\nGuitar ReferenceTest / Train Data\nExchanged\nOverall 82:0 % 84:2 %\nHense 74:4 % 82:9 %\nMarin 81:8 % 77:7 %\nWichmann 88:6 % 92:0 %\nTable 1. Cross-validation of guitar classiﬁcation with ex-\nchanged train and test data\nThe deviations between the two experiments give an\nimpression of the variances of the classiﬁcation measure-\nments.\n3.2 Principal component analysis (PCA)\nAs a supporting study, an attempt was made to reduce the\ndimensionality of the feature space by applying a principal\ncomponent analysis to the feature data. With the ﬁrst 500\nPCA-Eigenfeatures the classiﬁcation rate is only 72:1 %,\nwhich is signiﬁcantly worse than the classiﬁcation result\nof77:7 %, based on a manually selected 505-feature set\nshown in section 3.3.3. An explanation for this poor re-\nsult might be the fact, that most of the features are time\nseries, for which the differences of the neighbouring val-\nues carry important information. The preprocessing in the\nprincomp function of the Octave/Matlab Statistical Tool-\nbox removes these dependencies by calculating the mean\nand variance for each feature in the training set separately,\nand then for each feature value subtracts the mean and\nscales the variance to unity, thus eliminating the informa-\ntion of the relative magnitudes of the feature values.\nTo get a substantial reduction of the feature space, an\nadaption of the PCA method for time series, as described\nin chapter 12 of the book of Jolliffe [10], will have to be\napplied.\n3.3 Relevance of Features\nThe experiment series to determine the relevant features\nwere carried out with the full sample data set: 978 samples\nfor training, and 972 samples for testing.3.3.1 MFCCs\nIn this series of experiments only the time series of MFCC\nfeatures were used for classiﬁcation. In the ﬁrst experi-\nment set the MFCC coefﬁcients were grouped into lower\nand upper half (coefﬁcients 1 – 5 and 6 – 10), in the second\nexperiment set the features were divided into three groups\n(coefﬁcients 1 – 3, 4 – 7, and 8 – 10). All possible com-\nbinations of groups in the series were tested, these are the\nmost important results:\nMFCC\ncoefﬁcients Features Performance\n1 – 10 500 75:5 %\n1 – 5 250 71:2 %\n6 – 10 250 60:2 %\n1 – 7 350 72:0 %\n1 – 3 150 65:9 %\n4 – 7 150 62:6 %\n8 – 10 150 56:8 %\nTable 2. Classiﬁcation performance of subsets of the\nMFCC features\nThe MFCC coefﬁcient group 1 – 3 contains the most\nrelevant third of the MFCC coefﬁcients.\n3.3.2 Partial Tones (PT)\nIn this series of experiments several selections of the ﬁrst\n16 partial tones have been made. Again a grouping in\nhalves and thirds has been performed.\nPartials Features Performance\n1 – 16 640 52:8 %\n1 – 11 440 57:2 %\n1 – 5 200 44:0 %\n6 – 11 240 54:3 % (!)\n12 – 16 200 42:7 %\nTable 3. Classiﬁcation performance of subsets of the PT\nfeatures\nThe medium third of the partial tones gives not only the\nbest result of the one-third-selection, it is noteworthy, that\nthis reduced feature set even performs better than the full\nset of partial tones.\n3.3.3 Feature Combinations\nSeveral combinations of the nontonal, MFCC, and par-\ntial tone features were tested. The best performance was\nachieved by combining nontonal features with MFCCs 1 –\n5 and partial tones 6 – 11 i.e., the best-performing selec-\ntions of the previous experiments:\nComparison of the ﬁrst two lines in Table 4 shows, that\nthe addition of the 15 nontonal features increases the clas-\nsiﬁcation performance by 5 percent points. It has to be\nstressed, that the MFCC and the PT features each repre-\nsent a time series of 50 (MFCC) and 40 (PT) data points\nrespectively, whereas the nontonal energy distribution is aFeature selection Features Performance\nMFCC 1 – 5, PT 6 – 11 NT 1 – 15 505 77:7% (!)\nMFCC 1 – 5, PT 6 – 11 490 71:9%\nMFCC 1 – 5, NT 1 – 15 265 76:0 %\nMFCC 1 – 7, PT 6 – 11 NT 1 – 15 605 77:7 %\nTable 4. Classiﬁcation performance of various feature\ncombinations\nglobal feature set that consists of only 15 single data val-\nues. So it can be concluded, that the nontonal features add\nnew information to the feature set, that is not implicitly\ncontained in the other feature data.\nAnother notable fact is, that the inclusion of the time\nseries of MFCCs 6 and 7 does not at all affect the classiﬁ-\ncation performance, as can be seen by comparing the ﬁrst\nand the last line of Table 4.\n3.4 Selections of Tone Samples\nIn the subsequent experiments certain selections of tone\nsamples were made to further pinpoint the relevance of\nfeatures. Since the number of training samples is reduced,\nalso a reduced parameter set has to be used, so the most\nsuccessful combination of the preceding experiments was\ntaken: nontonal features 1 – 15, MFCCs 1 – 5, and partial\ntones 6 – 11. This is the 505-element feature set of sec-\ntion 3.3.3. In each overview of results the number of audio\nsamples used for test and training is given.\n3.4.1 Player\nIn the ﬁrst set of the experiments with preselected tone\nsamples, the inﬂuence of the player is investigated. In the\nﬁrst part, three out of four players are used for training, the\nremaining player is taken for testing. In the second part,\nthe training is performed with all players, and again one of\nthe players is used for testing.\nPlayer\n(Test)Samples\n(Test)Players\n(Train)Samples\n(Train) Performance\n1 324 2, 3, 4 654 59:6 %\n2 162 1, 3, 4 816 69:1 %\n3 324 1, 2, 4 648 65:4 %\n4 162 1, 2, 3 816 64:2 %\nTable 5. Classiﬁcation performance, player in test set not\nincluded in training set\nPlayer\n(Test)Samples\n(Test)Players\n(Train)Samples\n(Train) Performance\n1 324 1 – 4 978 74:4 %\n2 162 1 – 4 978 71:6 %\n3 324 1 – 4 978 83:5 %\n4 162 1 – 4 978 78:4 %\nTable 6. Classiﬁcation performance, player in test set is\nincluded in training set\nAs was to be expected, the players have quite a large\ninﬂuence on the produced sound, and so the classiﬁcationperformance decreases, when the testing player is not in\nthe group of the training players. The classiﬁcation perfor-\nmance might in this case be improved by a larger pool of\nplayers.\n3.4.2 Timbre\nIn these experiments sound samples of the same timbre are\nused for training and testing. The term timbre here refers to\nthe sound intention of the player. Usually, a warm timbre\nis produced by plucking the string above the sound hole of\nthe guitar with the ﬁnger moving in an angle of approx. 45\u000e\nto the string; a sharp timbre is produced by plucking near\nthe bridge with the plucking ﬁnger moving perpendicular\nto the string.\nTimbreSamples\n(Train / Test) Performance\nsharp 326 / 324 80:6 %\nsonorous 327 / 324 79:0 %\nwarm 325 / 324 82:7 %\nTable 7. Classiﬁcation performance for different timbres\nIt would have been expected, that the preselection of\ntimbre would improve the classiﬁcation performance, but\nthe experiments show, that this is not the case. Obviously\nthe inﬂuence of the different strings and the different posi-\ntions on the ﬁngerboard introduce too much inhomogene-\nity.\n3.4.3 String\nThis series of experiments tests the inﬂuence of the string\non the sound. Only sounds of the same string are taken for\ntraining and testing.\nString\n(Note)Samples\n(Train / Test) Performance\n1 (e’) 165 / 162 96:3 %\n2 (b) 163 / 162 92:6 %\n3 (g) 162 / 162 80:9 %\n4 (d) 163 / 162 79:6 %\n5 (A) 163 / 162 84:0 %\n6 (E) 162 / 162 83:3 %\nTable 8. Classiﬁcation performance for different strings\nObviously the preselection of the string does provide\nsubstantially more homogeneous sample sets. The trained\nSVMs are specialised to the sound of one particular string\nand perform substantially better than with the whole range\nof tone samples.\n3.4.4 Fret\nThe last experiment series in this sections is devoted to the\nfret, i.e., the position on the ﬁngerboard .\nThe observed performances of Table 9 are approximately\nthe same as the overall performance given in Table 1.FretSamples\n(Train / Test) Performance\n1 326 / 324 84:9 %\n5 176 / 174 79:9 %\n6 150 / 150 78:7 %\n10 314 / 312 81:7 %\nTable 9. Classiﬁcation performance for different fret posi-\ntions\n4. NONLINEAR KERNELS\nIn a last series of experiments different nonlinear kernels\nwere used for classiﬁcation. Again the 505-element feature\nset of section 3.3.3 is used.\nThe only nonlinear kernel provided by SVMLight, that\ngave satisfactory results was the polynomial kernel. Ta-\nble 10 shows the classiﬁcation results for several polyno-\nmial degrees:\nPolynomial\nDegreeSamples\n(Train / Test) Performance\n1 978 / 972 77:7 %\n2 978 / 972 82:3 %\n3 978 / 972 84:0 %\nTable 10. Classiﬁcation performance for polynominal ker-\nnels of degree 1–3\nOther available nonlinear kernels (Sigmoid, RBF) and\nhigher degree polynomial kernels performed very poor.\n5. SUMMARY AND OUTLOOK\nIn this paper a detailed feature relevance study about the\nclassiﬁcation performance of SVMs for classical guitar sounds\nis presented. It is shown, that the original feature set of\n1155 features with a classiﬁcation performance of 82:0 %\ncan be reduced to 505 features with an even better perfor-\nmance of 84:0 % when employing a third degree polyno-\nmial kernel.\nSeveral experiments on the preselection of sound sam-\nples for testing and training have been carried out. A ten-\ntative interpretation in musical terms shall be tried in the\nfollowing paragraphs.\nThe group of experiments with a pool of players used\nfor training of the SVMs and one player for testing shows,\nthat there is a large inﬂuence of the player on the sound.\nThis conclusion can be drawn from the fact, that the clas-\nsiﬁcation performance is signiﬁcantly increased, when the\ntesting player is also member of the training players. From\nmusical experience this is plausible. It is the interaction of\nplayer and instrument that produces the sound.\nThe preselection experiments, where the same timbre,\nstring, and fret is used for training and testing can be ex-\nplained in a technical way: the more uniform the samples\nare, the easier is the detection of differences arising from\nthe acoustical properties of the guitars. The very good clas-\nsiﬁcation performance for the highest two strings (96:3 %)\nand92:6 % is in accordance with the experience of luthiersand guitar players: A good guitar reveals its quality on the\ntreble strings (b and e’), whereas even medium quality gui-\ntars may sound good on the lower strings.\nIt would be a promising approach to improve the overall\nclassiﬁcation result by introducing a two-step classiﬁcation\nprocess: in the ﬁrst step the string is determined, and in\nthe second step the guitar is identiﬁed. Currently there are\nexperiments going on to compare the reported results with\nother classiﬁcation methods, in particular neural networks\nand a specialised form of principal component analysis to\nthe classiﬁcation problem presented by Wells and Aldam\nin [15].\nThe classiﬁcation framework is currently being modi-\nﬁed to apply to pieces of polyphonic solo guitar music.\nThe robustness of the method has to be proven, when there\nis a mixture of tones to be analysed, and further features\nmay show up in the musical context, that are not present\nin the single note analysis, especially the range of possible\nvariations in amplitude, attack and decay times and various\nspectral properties.\n6. REFERENCES\n[1] Gnu octave. http://www.octave.org, visited 2011.\n[2] J. G.A Barbedo and G. Tzanetakis. Instrument identi-\nﬁcation in polyphonic music signals based on individ-\nual partials. In Acoustics Speech and Signal Process-\ning (ICASSP), 2010 IEEE International Conference on,\npages 401 – 404, 2010.\n[3] Jeremiah D. Deng, Christian Simmermacher, and\nStephen Craneﬁeld. A Study on Feature Analysis for\nMusical Instrument Classiﬁcation. IEEE Transactions\non Systems, Man, and Cybernetics-Part B: Cybernet-\nics, 38(2):429–438, 2008.\n[4] K. Dosenbach, W. Fohl, and A. Meisel. Identiﬁcation\nof Individual Guitar Sounds by Support Vector Ma-\nchines. In Proc. of the 11th Int. Conference on Digital\nAudio Effects (DAFx-08), Espoo, Finland, 2008.\n[5] Dimitrios Fragoulis, Constantin Papaodysseus, Mihalis\nExarhos, George Roussopoulos, Thanasis Panagopou-\nlos, and Dimitrios Kamarotos. Automated classiﬁca-\ntion of piano-guitar notes. IEEE Trans. on Audio,\nSpeech, and Language Processing, 14(3), 2006.\n[6] I. Guyon and A. Elisseeff. An introduction to variable\nand feature selection. The Journal of Machine Learn-\ning Research, 3:1157–1182, 2003.\n[7] Y . H Hsiao and C. T Su. Multiclass MTS for saxophone\ntimbre quality inspection using waveform-shape-based\nfeatures. IEEE Transactions on Systems, Man, and Cy-\nbernetics, Part B: Cybernetics, 39(3):690 – 704, 2009.\n[8] Thorsten Joachims. Svmlight.\nhttp://svmlight.joachims.org, 2008.[9] Cyril Joder, Slim Essid, and Ga ¨el Richard. Alignment\nkernels for audio classiﬁcation with application to mu-\nsic instrument recognition. In Proceedings of the Euro-\npean Signal Processing Conference, 2008.\n[10] I.T. Jolliffe. Principal Component Analysis, volume 2.\nWiley Online Library, 2002.\n[11] R. Loughran, J. Walker, and M. O’Neill. An explo-\nration of genetic algorithms for efﬁcient musical instru-\nment identiﬁcation. In Signals and Systems Conference\n(ISSC 2009), IET Irish, pages 1–6. IET, 2009.\n[12] Ingo Mierswa and Katharina Morik. Automatic feature\nextraction for classifying audio data. Machine Learn-\ning, 58(2-3):127–149, 2005.\n[13] F. Pachet and P Roy. Analytical features: a knowledge-\nbased approach to audio feature generation. EURASIP\nJournal on Audio, Speech, and Music Processing,\n2009(1), February 2009.\n[14] Malcolm Slaney. Auditory toolbox. A MATLAB\ntoolbox for auditory modeling work. version 2.\nhttp://cobweb.ecn.purdue.edu/ malcolm/interval/1998-\n010/, 1998.\n[15] Jeremy J. Wells and Gregory Aldam. Principal com-\nponent analysis of rasterised audio for crosssynthesis.\nInProc. Of the 12th Int. Conference on Digital Audio\nEffects (DAFx-09), volume 4, Como, Italy, 2009."
    },
    {
        "title": "Folksonomy-based Tag Recommendation for Online Audio Clip Sharing.",
        "author": [
            "Frederic Font",
            "Joan Serrà",
            "Xavier Serra"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415700",
        "url": "https://doi.org/10.5281/zenodo.1415700",
        "ee": "https://zenodo.org/records/1415700/files/FontSS12.pdf",
        "abstract": "Collaborative tagging has emerged as an efficient way to semantically describe online resources shared by a com- munity of users. However, tag descriptions present some drawbacks such as tag scarcity or concept inconsistencies. In these situations, tag recommendation strategies can help users in adding meaningful tags to the resources being de- scribed. Freesound is an online audio clip sharing site that uses collaborative tagging to describe a collection of more than 140,000 sound samples. In this paper we propose four algorithm variants for tag recommendation based on tag co-occurrence in the Freesound folksonomy. On the basis of removing a number of tags that have to be later predicted by the algorithms, we find that using ranks instead of raw tag similarities produces statistically significant improve- ments. Moreover, we show how specific strategies for se- lecting the appropriate number of tags to be recommended can significantly improve algorithms’ performance. These two aspects provide insight into some of the most basic components of tag recommendation systems, and we plan to exploit them in future real-world deployments.",
        "zenodo_id": 1415700,
        "dblp_key": "conf/ismir/FontSS12",
        "keywords": [
            "Collaborative tagging",
            "semantically describe online resources",
            "tag scarcity",
            "concept inconsistencies",
            "tag recommendation strategies",
            "Freesound",
            "online audio clip sharing site",
            "tag co-occurrence",
            "algorithm variants",
            "tag recommendation"
        ],
        "content": "FOLKSONOMY-BASED TAG RECOMMENDATION\nFOR ONLINE AUDIO CLIP SHARING\nFrederic Font1, Joan Serr `a2and Xavier Serra1\n1Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain\n2Artiﬁcial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain\nfrederic.font@upf.edu, jserra@iiia.csic.es, xavier.serra@upf.edu\nABSTRACT\nCollaborative tagging has emerged as an efﬁcient way to\nsemantically describe online resources shared by a com-\nmunity of users. However, tag descriptions present some\ndrawbacks such as tag scarcity or concept inconsistencies.\nIn these situations, tag recommendation strategies can help\nusers in adding meaningful tags to the resources being de-\nscribed. Freesound is an online audio clip sharing site that\nuses collaborative tagging to describe a collection of more\nthan 140,000 sound samples. In this paper we propose four\nalgorithm variants for tag recommendation based on tag\nco-occurrence in the Freesound folksonomy. On the basis\nof removing a number of tags that have to be later predicted\nby the algorithms, we ﬁnd that using ranks instead of raw\ntag similarities produces statistically signiﬁcant improve-\nments. Moreover, we show how speciﬁc strategies for se-\nlecting the appropriate number of tags to be recommended\ncan signiﬁcantly improve algorithms’ performance. These\ntwo aspects provide insight into some of the most basic\ncomponents of tag recommendation systems, and we plan\nto exploit them in future real-world deployments.\n1. INTRODUCTION\nOnline platforms where people share user generated con-\ntent have stressed the need for efﬁcient methods to describe\nand retrieve such content. Freesound [1] is an online audio\nclip sharing site which clearly reﬂects this need. It contains\nmore than two million users and 140,000 user-contributed\nsound samples covering a wide variety of sounds (from\nﬁeld recordings and sound effects to drum loops and in-\nstrument samples), which have to be well described to al-\nlow proper retrieval.\nIn recent years, collaborative tagging has emerged as an\nefﬁcient way to describe online resources shared by a com-\nmunity of users. In collaborative tagging systems, users\ndescribe information items by annotating them with a num-\nber of “free-form” semantically-meaningful textual labels,\ncalled tags, that act as keywords and that can be later used\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.for retrieval purposes. A collection of tags together with\ntheir associations to content resources is commonly known\nas afolksonomy.\nIn the majority of collaborative tagging systems, includ-\ning Freesound, users are not constrained by any particular\nnumber of tags to assign, nor by the use of any speciﬁc\nvocabulary where to pick the tags from. Therefore, de-\nscriptions can be done at many different levels of detail\nand accuracy. Description inconsistencies can then arise\ndue to the ambiguity of tag meanings, tag scarcity, the use\nof personal naming conventions, typographical errors, or\neven the use of different languages [2].\nOne strategy for trying to overcome some of these prob-\nlems, and thus obtain more comprehensive and consistent\ndescriptions, has been the use of tag recommendation sys-\ntems to help users in the tagging process. These systems\nanalyze the ﬁrst (usually few) tags that users introduce when\ndescribing a particular item, and quickly suggest new tags\nthat can also be meaningful or relevant for the item being\ndescribed. The same algorithms for tag recommendation\ncan be used, in an off-line mode, to extend the descriptions\nof information items by analyzing their tags and automati-\ncally adding new ones (what is normally called tag propa-\ngation).\nIn this paper we present and evaluate four variants of an\nalgorithm for tag recommendation in Freesound. Our rec-\nommendation is based on tag semantic similarity derived\nfrom tag co-occurrence in the Freesound folksonomy. A\nnovel aspect of the algorithm is a step focused on automat-\nically selecting the number of tags to recommend given\na sorted list of candidate tags. Tag propagation methods\nfound in related work (see below) do not perform this step,\nand usually evaluate algorithms at different values of N\nrecommended tags. We compare our algorithm with sim-\npler versions which either always recommend a ﬁxed num-\nber of tags, or only recommend tags that are repeated in the\nlist of candidates.\nThe rest of the paper is organized as follows. In Sec. 2\nwe review the related work and in Sec. 3 we brieﬂy de-\nscribe the Freesound folksonomy. Sec. 4 explains the pro-\nposed algorithm for tag recommendation. Secs. 5 and 6\ndescribe the evaluation methodology and present the ob-\ntained results. In Sec. 7 we conclude the paper with a dis-\ncussion about our ﬁndings and future work.2. RELATED WORK\nCollaborative tagging has been widely researched in the\nlast few years. Some studies focus on a general descrip-\ntion of the dynamics of collaborative tagging and user be-\nhavior when tagging [2–5]. Other studies have looked at\nthe motivations that users have at the moment of tagging,\nproposing then automatic tag classiﬁcation methods to or-\nganize types of tags according to these motivations [6, 7].\nMost of the work done in the analysis of collaborative tag-\nging systems takes as case studies well-known sites such as\nDelicious (bookmark sharing), CiteULike (scientiﬁc refer-\nence sharing) and Flickr (photo sharing).\nA variety of methods have been proposed for tag prop-\nagation and tag recommendation, especially for the case of\nimage annotation. In [5] and [8], content analysis of im-\nages is used to obtain similar images and then propagate or\nrecommend tags from these images to the source. Instead\nof using content analysis, Sigurbj ¨ornsson and Zwol [9] pro-\npose a method for image tag recommendation based on tag\nsimilarities derived from a folksonomy. Their approach is\nsimilar to the one we describe in this paper, though they\nuse different strategies for sorting candidate tags and do\nnot perform a ﬁnal selection of the number of tags to rec-\nommend. In [10] and [11], more complex strategies for\ntag recommendation based on folksonomies are described\nand evaluated with data from Delicious, BibSonony and\nLast.fm (using hierarchical tag structures [10] and the Folk-\nRank ranking algorithm [11]). Again, none of these ap-\nproaches performs any selection of the number of tags to\nrecommend.\nIn the ﬁeld of sound and music, most of the work re-\nlated with tag propagation or recommendation is not based\non folksonomy analysis, but on the extraction of content-\nbased audio features that can later be used to annotate songs\nwith labels or tags (which is more commonly known as se-\nmantic annotation). Sordo [12] describes a method based\non audio content similarity for propagating tags (related\nwith style and mood) between acoustically similar songs.\nMart ´ınez et al. [13] use a similar idea for automatically\npropagate tags in scarcely annotated samples of Freesound.\nIn [14] and [15], a different approach for automatic annota-\ntion of music and sound effects is described, which is based\non using machine learning techniques to learn mappings\nbetween tags and audio features. Due to the content-based\nnature of these strategies, they are not directly comparable\nto the approach we propose in this paper.\n3. FREESOUND FOLKSONOMY\nIn Freesound, users can upload sound samples and then\ndescribe them with as many tags as they feel appropri-\nate1. For building the folksonomy we use in our experi-\nments, we considered user annotations between April 2005\nand September 2011. The folksonomy includes a total of\n785,466 annotations that assign 30,985 unique tags (not\n1Since a recent software upgrade, Freesound requires a minimum of\nthree tags to annotate a sound. However, the data we analyze is prior to\nthe introduction of this requirement.\nFigure 1: Distribution of sounds per number of tags. The\nglobal average of tags per sound is 6.16 and the standard\ndeviation is 6.23.\nnecessarily semantically unique, but with different string\nrepresentations) to 118,620 sounds. As opposite to other\nwell studied collaborative tagging systems such as Deli-\ncious or CiteULike, Freesound has what is called a nar-\nrow folksonomy [16], meaning that sound annotations are\nshared among all users and therefore one single tag can\nonly be assigned once to a particular sound (e.g. the tag\nfield-recording cannot be added twice to the same\nsound).\nFig. 1 shows the distribution of the number of tags per\nsound in Freesound. We are particularly interested in rec-\nommending tags for the sounds that fall in the range of\n[3;15]tags (shadowed zone in Fig. 1), which are more than\n80% of the total. The reason for focusing on these sounds\nis that the algorithm variants we present take as input the\ntags that have already been assigned to a sound. We con-\nsider 3 tags as enough input information for our algorithms\nto provide good recommendations. For sounds with less\ntags, content-based strategies such as the ones outlined in\nSec. 2 are probably more suitable. On the other hand,\nsounds with more than 15 tags are, in general, enough well\ndescribed.\nAmong the total number of 30,985 unique tags present\nin the folksonomy, we have applied a threshold to take\nonly into consideration the tags that have been used at least\n10 times, i.e. the tags that appear on at least 10 differ-\nent sounds. By this we assume that tags that have been\nused less than 10 times are irrelevant for our purposes. In\naddition, by discarding less frequent tags, we reduce the\ncomputational complexity of the calculations described in\nSec. 4.1. After applying this threshold, we are left with\n6,232 unique tags, representing 20% of the total. Nonethe-\nless, 93% of all annotations associate one of these 6,232\nunique tags with a sound, thus we still take into account\nthe vast majority of the original information.\n4. PROPOSED ALGORITHM\nThe tag recommendation algorithm described in this paper\nconsists of the three steps depicted in the diagram of Fig. 3.\nVariants are obtained by combining the different strategies\nproposed for the second and third steps. Feeding the algo-\nrithm variants with a number of inputTags , they output a\nset of recommendedTags . In the following subsections\nwe describe each one of the depicted steps.Getting \ncandidate tagsAggregating \ncandidate tags\nSelecting how \nmany tags to \nrecommendrecommended tagsinput tagsFigure 3: Block diagram of the described tag recommen-\ndation algorithm.\n4.1 Getting candidate tags\nThe ﬁrst step in the recommendation processes is getting a\nnumber of candidate tags according to the set of inputTags .\nFor this purpose we build a tag similarity matrix based\non tag co-occurrences in sound descriptions, following the\nActor-Concept-Instance model proposed by Mika [17].\nThis tag similarity matrix gathers information from the\nwhole folksonomy and only needs to be computed once.\nThe Actor-Concept-Instance model proposes to repre-\nsent a folksonomy Fas a tripartite hypergraph H(F) =\nhV; Ei, where vertices are given by three ﬁnite sets of ob-\njects, V=U[T[R(U, T, and R denoting users, tags, and\nresources, respectively), and each edge represents a tag-\nresource association done by a user E=ffu; t; rgj(u; t; r )\n2Fg. We unfold this tripartite hypergraph into the bi-\npartite graph TR, which reﬂects only the associations be-\ntween tags and resources (sounds in our case). We can\nrepresent the bipartite graph TRas a matrix D=fdijg,\nwhere dij= 1 if tag tihas been used to label resource\nrj(otherwise dij= 0). We can then deﬁne the matrix\nS=DD0, which corresponds to a one-mode network con-\nnecting tags on the basis of shared resources. Elements sij\nofSindicate the number of sounds in which tags tiand\ntjappear together. Therefore, the diagonal of Srepresents\nthe total number of different sounds labeled with a tag ti=j.\nWe then normalize Sby dividing each element bypsiipsjj. In this manner, we obtain the cosine similarity\nmeasures between tags tiandtj(the cosine similarity be-tween the i-th and the j-th rows of D). Preliminary experi-\nments using other distances such as Jaccard reported worse\nresults. Furthermore, cosine similarity has been widely\nused in the literature and has been shown to be effective\nas a semantic relatedness measure in folksonomies [18].\nFig. 2 shows a graph visualization of an excerpt of S.\nHaving calculated the tag similarity matrix S, we iterate\noverinputTags and get, for each element i, a set of can-\ndidates CinputTag i. For that we select the Nmost similar\ntags of inputTag i(i.e. the Nmost similar graph neigh-\nbors). We keep these similarity values for further process-\ning in the following steps. In all our experiments we use\nN= 100. Hence, for instance, if our algorithm is feeded\nwith three input tags, it will get a maximum of 300 can-\ndidate tags (provided that all three input tags have at least\n100 neighbors). In preliminary experiments we observed\nthat using values of N > 100did not alter our recommen-\ndation results.\n4.2 Aggregating candidate tags\nThe next step of our algorithm is to aggregate and sort the\nobtained candidate tags into a single list Csorted . For this\npurpose we propose two different strategies which are now\ndescribed.\n4.2.1 Similarity-based strategy\nIn the ﬁrst strategy we construct Csorted by aggregating all\nsets of candidate tags CinputTag iinto a single list Craw,\nand then ordering them by their similarity values (taken\nfrom the previous steps). As we do not want to recom-\nmend tags that are already part of inputTags , we remove\nany occurrences of these tags in Craw. If there are repeated\ncandidate tags in Craw, their similarity values are added as\na way of promoting these tags that appear in CinputTag iof\nmore than one input tag i. We ﬁnally normalize the similar-\nity values by dividing them by the number of inputTags\n(thus maintaining values in the original range).\nFigure 2: Graph visualization of the tag similarity matrix S. Edge widths represent the similarity between two tags. Node\nsize is a logarithmic function of the absolute tag frequency. For the sake of clarity, only edges above a certain threshold and\ntags above a certain level of absolute frequency are shown.Similarity valueFigure 4: Example of the linear regression strategy for se-\nlecting how many tags to recommend. The straight line\nshows the linear regression of the histogram. Recom-\nmended tags are those placed at the right of the point where\nthe linear regression crosses the y-axis.\n4.2.2 Rank-based strategy\nThe second strategy is based on assigning a rank value to\neach candidate tag. For this purpose, we sort each set of\nCinputTag iand then assign rank values as:\nrank(neighbor n) =N\u0000(n\u00001);\nwhere nis the position of the neighbor in CinputTag i(thus\nnranges from 1 to N). This way, the closest tag to every\ninput tag will be assigned with a rank value of N. We then\nperform the aggregation as we would do in the similarity-\nbased strategy, but using the assigned rank values as simi-\nlarities.\n4.3 Selecting how many tags to recommend\nOnce we have computed Csorted (either using similarity-\nbased or rank-based strategies), we select how many of\nthese tags should be outputted as recommendedTags . Our\napproach is based on the hypothesis that given the distribu-\ntion of similarity or rank values in Csorted , it will be pos-\nsible to determine a threshold that separates a set of mean-\ningful tags from the other candidates. That is to say, that\n“good” tags for recommendation will appear as an isolated\ngroup from the rest of the distribution. In order to auto-\nmatically determine this threshold, we again propose two\ndifferent strategies.\n4.3.1 Linear regression strategy\nThe ﬁrst strategy consists in calculating the least-squares\nlinear regression of the histogram of Csorted . The thresh-\nold is set to the point where the linear regression crosses\nthe y-axis. Fig. 4 shows an example of using this strat-\negy with a histogram of tag similarity values. In that case\nthreshold is set at 0.29. Therefore, all candidate tags with\na similarity value higher than 0.29 would be outputted as\nrecommendedTags .\n4.3.2 Statistical test strategy\nThe second strategy has two steps. First, we estimate the\nprobability density function (PDF) of Csorted . For that\npurpose, we use a kernel density estimator [19]. Second,\nRank valueFigure 5: Example of the statistical test strategy for select-\ning how many tags to recommend. The curve represents\nthe estimated PDF of Csorted . Markers on the x-axis show\nthe actual positions of candidate tags. Recommended tags\nare those under the shaded zone in the right.\nwe iteratively take consecutive samples of the PDF (start-\ning from the side where the candidates with highest rank or\nsimilarity lay) and perform a statistical test for normality.\nFor that purpose we use the Anderson-Darling test [20].\nThe threshold is set at the point of the PDF where the test\nfails for the ﬁrst time (i.e.the probability of having an in-\ndependent Gaussian distribution is not statistically signiﬁ-\ncant). The idea behind this process is that there will be a\nset of good tags for the recommendation that will exhibit a\nnormal, independent distribution separated from the rest of\ncandidate tags. The statistical test fails when it detects de-\npartures from normality, and according to our hypothesis\nthis happens when non-meaningful candidate tags start af-\nfecting the PDF. Fig. 5 shows an example of applying this\nstrategy using rank values for Csorted . All tags under the\nshaded zone in the right will be recommended.\n5. EV ALUATION METHODOLOGY\nIn order to compare and evaluate the efﬁcacy of the pro-\nposed variants we followed a systematic approach based\non removing a number of tags from the Freesound sound\ndescriptions and then trying to predict them. The advan-\ntage of this approach is that it allows us to quickly evaluate\ndifferent tag recommendation methods without the need of\nhuman input. The main drawback is that tags that could\nbe subjectively considered as good recommendations for a\nparticular sound description but that are not present in the\nset of removed tags, will not count as positive results (see\nSec. 7).\nWe performed a 10-fold cross validation following the\nmethodology described in [21]. For each fold, we build\na tag similarity matrix using the subset of the folksonomy\ncorresponding to the training set of sounds. Then, for each\none of the sounds in the evaluation set, we remove a ran-\ndom number of their tags (removedTags ) and run tag rec-\nommendation methods using the tag similarity matrix de-\nrived from the training set. We compute standard precision,\nrecall and f-measure for each evaluated sound according\nto:Method name Aggregation step Selection step\nProposed algorithm variants\nRankST Rank-based Statistical test\nSimST Similarity-based Statistical test\nRankLR Rank-based Linear regression\nSimLR Similarity-based Linear regression\nBasic methods\nRankFIX@K Rank-based Fixed number (K2[1;10])\nSimFIX@K Similarity-based Fixed number (K2[1;10])\nRepeated@R Repeated tags in Craw (R2[2;6])\nRandom baselines\nRandom (for\nevery method)Random selection of tags from Craw, with the\nsame length as recommendedTags\nTable 1: Evaluated tag recommendation methods.\nprecision =jrecommendedTags\\removedTagsj\njrecommendedTagsj;\nrecall =jrecommendedTags\\removedTagsj\njremovedTagsj;and\nfmeasure = 2\u0001precision\u0001recall\nprecision+recall:\nTable 1 shows the tag recommendation methods that\nwe compare. The ﬁrst group of methods (Proposed al-\ngorithm variants) are the four possible combinations of\naggregation and selection strategies described in Secs. 4.2\nand4.3. Basic methods correspond to more basic tag rec-\nommendation methods that we used for comparison. On\nthe one hand, we compare with two simpler versions of\nour proposed algorithm (RankFIX@K and SimFIX@K)\nwhich skip the last step of the recommendation process\nand always recommend a ﬁxed number of Ktags. We run\nthese methods for values of Kranging from 1 to 10. On\nthe other hand, we compare with an even simpler method\n(Repeated@R), which only recommends tags that appear\nmore than Rtimes in Craw(independently of any rank or\nsimilarity values). We run these methods for values of R\nranging from 2 to 6. Finally, we also compute a random\nbaseline for each one of the previous methods by replac-\ning the set of recommendedTags with a random selec-\ntion (of the same length) taken from Craw. We choose as\nthe general random baseline the one that gets the highest\nf-measure.\n6. RESULTS\nTable 2 shows the results of our evaluation as described\nin the previous section. The ﬁrst group of results (under\nthe label with input tags range ﬁlter ) has been obtained\nby limiting the number of input tags we used to feed our\nalgorithms to the range of [3;15], thus avoiding scarcely\ntagged sounds. The second group of results does not apply\nany restriction to the number of input tags (provided that\nthere is at least one input tag).\nA ﬁrst observation is that using the input tags range\nﬁlter produces an average increase in f-measure of 0.150\namong our proposed algorithm variants (statistically sig-\nniﬁcant using pairwise Kruskal-Wallis test, p\u00190). Other\nmethods present an average increase of 0.074 (p\u00190). This\nmeans that, as expected, our algorithm works better in theAlgorithm Precision Rrecall F-measure\nWith input tags range ﬁlter (83,010 sounds)\nRankST 0.443 0.537 0.432\nRankLR 0.394 0.564 0.419\nRankFIX@2 0.395 0.466 0.391\nSimLR 0.348 0.397 0.325\nSimST 0.381 0.333 0.317\nRankFIX@5 0.233 0.614 0.308\nSimFIX@2 0.303 0.344 0.294\nSimFIX@5 0.181 0.467 0.237\nRepeated@3 0.177 0.679 0.236\nRankFIX@10 0.136 0.696 0.212\nSimFIX@10 0.111 0.566 0.173\nRandom 0.006 0.033 0.010\nWithout input tags range ﬁlter (118,620 sounds)\nRankST 0.317 0.290 0.258\nRankFIX@2 0.310 0.246 0.244\nRankFIX@5 0.214 0.366 0.238\nRankLR 0.236 0.301 0.221\nSimLR 0.271 0.223 0.212\nSimST 0.294 0.195 0.202\nSimFIX@2 0.256 0.195 0.196\nSimFIX@5 0.176 0.294 0.193\nRankFIX@10 0.142 0.447 0.192\nSimFIX@10 0.120 0.371 0.161\nRepeated@3 0.095 0.262 0.110\nRandom 0.020 0.054 0.026\nTable 2: Average of precision, recall and f-measure results\nfor the evaluated tag recommendation methods. For the\nsake of readability, we only show some representative re-\nsults of FIX and Repeated methods using K= 2;5;10and\nR= 3. Methods are ordered by f-measure.\nrange of [3;15]input tags. This is due to the notable in-\ncrease in recall when using the input tags range ﬁlter (big-\nger than the increase in precision), suggesting that when\nfeeded with at least three tags, our algorithm is able to se-\nlect more relevant candidates. That supports the idea that\nfor sounds with less tags, content-based approaches for tag\nrecommendation would probably be more appropriate.\nWe can also see that all methods using the rank-based\nstrategy for aggregating candidate tags always report higher\nf-measure than their similarity-based counterparts. Among\nthe group with the input tags range ﬁlter, the average in-\ncrease is of 0.104 (p\u00190), while in the group without the\nﬁlter the increase is of 0.033 (p\u00190). That difference be-\ntween both groups suggests that rank-based strategy works\nbetter when aggregating longer sets of candidates.\nRegarding the selection step of our algorithm, both us-\ning the statistical test (ST) or the linear regression (LR)\nstrategy signiﬁcantly improves the performance with re-\nspect to the basic methods. When using the input tags ﬁl-\nter, we observe an average increase in f-measure of 0.114\nand 0.111 for the ST and LR methods, respectively (p\u00190).\nWithout using the ﬁlter the average increase is less impor-\ntant, of 0.045 and 0.036 for ST and LR, respectively (p\u00190).\nIt is surprising that methods recommending a ﬁxed number\nof two tags (FIX@2) perform quite close to their counter-\nparts using ST or LR strategies (and even in some cases\nscoring higher when not using the input tags range ﬁler).\nThis might be due to the fact that the average number of\nremoved tags among all the experiments is 2.5. There-Sound id Input tags Removed tags Recommended tags F-measure\n8780 analog, glitch, warped loﬁ noise, electronic 0.0\n124021 newspaper, reading, paper, page, news read magazine 0.0\n38006 hit, glass, oneshot percussionsinglehit, singlebeat, single, tap,\nhits, house, percussion, place,\nthuds, drum, plock0.17\n54374 spring, nightingale, nature, bird ﬁeld-recording, birdsong, binaural birds, ﬁeld-recording, forest, birdsong 0.5\n78282 metal, medium-loud, interaction impact impact, wood 0.67\nTable 3: Example of tag recommendations using the method RankST. Corresponding sounds can be listened at the follow-\ning url: http://www.freesound.org/search?q=[Sound id].\nfore, precision errors are minimized when recommending\nthat amount of tags. Moreover, the good performance of\nFIX@2 reﬂects the effectiveness of the aggregation strate-\ngies, that successfully promote the most relevant tags on\nthe ﬁrst positions. On the other hand, ST and LR strategies\nperform generally better while at the same time recom-\nmending more tags (average of 3.16 and 4.6 respectively).\nThis suggests that the selection step is able to choose, for\neach sound, the appropriate number of tags to recommend.\nOverall, the method that reports the highest f-measure is\nRankST (both with and without the input tags ﬁlter), and\nall our proposed algorithm variants perform much better\nthan the random baseline.\n7. CONCLUSION AND FUTURE WORK\nIn this paper we have described and evaluated four algo-\nrithm variants for tag recommendation based on the Free-\nsound folksonomy. We have found that using a ranking\ninstead of raw tag similarity values for sorting a list of can-\ndidate tags produces signiﬁcantly better recommendations.\nThe most novel aspect of the described algorithm is a step\nfocused in automatically selecting the number of tags to\nrecommend from a sorted list of candidate tags. The two\nstrategies proposed for this step (statistical test and linear\nregression) have proved to be effective and statistically sig-\nniﬁcantly increase the performance of the algorithm.\nAlthough the systematic evaluation we have conducted\nallowed us to compare the different tag recommendation\nmethods using a lot of sounds, the results in terms of f-\nmeasure are probably much worse than what a user-based\nevaluation could have reported. To exemplify this obser-\nvation, Table 3 shows a few examples of tag recommenda-\ntions performed using the RankST method (the one with\nthe highest f-measure). We have marked in bold the tags\nthat are considered good recommendations under our eval-\nuation framework. Notice that many of the recommended\ntags which are not in italics could also be judged as mean-\ningful recommendations if we listen to the sounds. In fu-\nture work we would like to perform some user-based evalu-\nation. Additionally, we plan to further improve our tag rec-\nommendation algorithm by introducing more tag-speciﬁc\ninformation such as characterizations of tag relevance, se-\nmantic category or usage context. Finally, we also plan to\ninclude our tag recommendation system in future deploy-\nments of Freesound.8. ACKNOWLEDGEMENTS\nThis work is partially supported under BES-2010-037309\nFPI grant from the Spanish Ministry of Science and Inno-\nvation for the TIN2009- 14247-C02-01 DRIMS project. JS\nacknowledges 2009-SGR-1434 from Generalitat de Cata-\nlunya, JAEDOC069/2010 from Consejo Superior de Inves-\ntigaciones Cient ´ıﬁcas, TIN2009-13692-C03-01 from the\nSpanish Government, and EU Feder Funds.\n9. REFERENCES\n[1] V . Akkermans et al.:“Freesound 2: An Improved Platform for Sharing Audio\nClips,” Late-braking demo abstract of the Int. Soc. for Music Information Re-\ntrieval Conf., 2011.\n[2] H. Halpin et al.: “The dynamics and semantics of collaborative tagging,” Proc.\nof the 1st Semantic Authoring and Annotation Workshop, 1-21, 2006.\n[3] S. A. Golder and B. A. Huberman: “Usage patterns of collaborative tagging\nsystems,” Journal of Information Science, 32(2), 198-208, 2011.\n[4] C. Marlow et al.: “HT06, Tagging Paper, Taxonomy, Flickr, Academic Article,\nToRead,” Proc. of the 17th Conf. on Hypertext and Hypermedia, 31-40, 2006.\n[5] U. Farooq et al.: “Evaluating Tagging Behavior in Social Bookmarking Sys-\ntems: Metrics and design heuristics,” Human-Computer Interaction, 1, 351-\n360, 2007.\n[6] K. Bischoff et al.:“Can all tags be used for search?,” Proc. of the 17th ACM\nConf. on Information and Knowledge Management, 32(2), 193202. ACM,\n2008.\n[7] I. Cantador et al.: “Categorising social tags to improve folksonomy-based rec-\nommendations,” Web Semantics: Science, Services and Agents on the World\nWide Web, 9(1), 1-15, 2011.\n[8] I. Ivanov et al.:“Object-based tag propagation for semi-automatic annotation of\nimages,” Proc. of the Int. Conf. on Multimedia Information Retrieval, 497-506,\n2010.\n[9] B. Sigurbj ¨ornsson and R. Van Zwol: “Flickr tag recommendation based on\ncollective knowledge,” Proc. of the 17th Int. Conf. on World Wide Web, 327-\n336, 2008.\n[10] P. De Meo et al.: “Exploitation of semantic relationships and hierarchical data\nstructures to support a user in his annotation and browsing activities in folk-\nsonomies,” Information Systems Journal, 34(6), 511-535, 2009.\n[11] R. J ¨aschke et al.: “Tag Recommendations in Folksonomies,” Knowledge Dis-\ncovery in Databases PKDD, 34(6), 506-514, 2009.\n[12] M. Sordo: “Semantic Annotation of Music Collections: A Computational Ap-\nproach,” PhD thesis, Universitat Pompeu Fabra, 2012.\n[13] E. Mart ´ınez et al.: “Extending the folksonomies of freesound.org using\ncontent-based audio analysis,” Proc. of the Sound and Music Computing Conf.,\n23-25, 2009.\n[14] D. Turnbull et al.: “Semantic Annotation and Retrieval of Music and Sound\nEffects,” IEEE Transactions On Audio Speech And Language Processing, 16,\n467-476 2008.\n[15] L. Barrington et al.: “Audio Information Retrieval using Semantic Similarity,”\nIEEE Int. Conf. on In Acoustics, Speech and Signal Processing, 16, 725-728,\n2007.\n[16] T. Vander Wal: “Explaining and showing broad and narrow folksonomies,”\nhttp://www.personalinfocloud.com/ 2005/02/explaining and.html, 2005.\n[17] P. Mika: “Ontologies are Us: A Uniﬁed Model of Social Networks and Seman-\ntics,” Web Semantics: Science, Services and Agents on the World Wide Web,\n5(1), 5-15, 2007.\n[18] C. Cattuto et al.: “Semantic Analysis of Tag Similarity Measures in Collabo-\nrative Tagging Systems,” Data Engineering, 805, 5-11, 2008.\n[19] B.W. Silverman: “Density Estimation for Statistics and Data Analysis,” Ap-\nplied Statistics, 37(1), 1986.\n[20] F. W. Scholz and M. A. Stephens: “K-Sample Anderson-Darling Tests,” Jour-\nnal of the American Statistical Association, 82, 918-924, 1987.\n[21] S. L. Salzberg: “On Comparing Classiﬁers: Pitfalls to Avoid and a Recom-\nmended Approach ,” Data Mining and Knowledge Discovery, 1(3), 317-328,\n1997."
    },
    {
        "title": "Professionally-produced Music Separation Guided by Covers.",
        "author": [
            "Timothée Gerber",
            "Martin Dutasta",
            "Laurent Girin",
            "Cédric Févotte"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417157",
        "url": "https://doi.org/10.5281/zenodo.1417157",
        "ee": "https://zenodo.org/records/1417157/files/GerberDGF12.pdf",
        "abstract": "This paper addresses the problem of demixing profession- ally produced music, i.e., recovering the musical source signals that compose a (2-channel stereo) commercial mix signal. Inspired by previous studies using MIDI synthe- sized or hummed signals as external references, we pro- pose to use the multitrack signals of a cover interpretation to guide the separation process with a relevant initializa- tion. This process is carried out within the framework of the multichannel convolutive NMF model and associated EM/MU estimation algorithms. Although subject to the limitations of the convolutive assumption, our experiments confirm the potential of using multitrack cover signals for source separation of commercial music.",
        "zenodo_id": 1417157,
        "dblp_key": "conf/ismir/GerberDGF12",
        "keywords": [
            "demixing",
            "professionally produced music",
            "recovering musical source signals",
            "commercial mix signal",
            "MIDI synthesized",
            "hummed signals",
            "multitrack signals",
            "cover interpretation",
            "multichannel convolutive NMF model",
            "EM/MU estimation algorithms"
        ],
        "content": "PROFESSIONALLY-PRODUCED MUSIC SEPARATION\nGUIDED BY COVERS\nTimothée Gerber, Martin Dutasta, Laurent Girin\nGrenoble-INP, GIPSA-lab\nfirstname.lastname@gipsa-lab.grenoble-inp.frCédric Févotte\nTELECOM ParisTech, CNRS LTCI\ncedric.fevotte@telecom-paristech.fr\nABSTRACT\nThis paper addresses the problem of demixing profession-\nally produced music, i.e., recovering the musical source\nsignals that compose a (2-channel stereo) commercial mix\nsignal. Inspired by previous studies using MIDI synthe-\nsized or hummed signals as external references, we pro-\npose to use the multitrack signals of a cover interpretation\nto guide the separation process with a relevant initializa-\ntion. This process is carried out within the framework of\nthe multichannel convolutive NMF model and associated\nEM/MU estimation algorithms. Although subject to the\nlimitations of the convolutive assumption, our experiments\nconﬁrm the potential of using multitrack cover signals for\nsource separation of commercial music.\n1. INTRODUCTION\nIn this paper, we address the problem of source separa-\ntion within the framework of professionally-produced (2-\nchannel stereo) music signals. This task consists of recov-\nering the individual signals produced by the different in-\nstruments and voices that compose the mix signal. This\nwould offer new perspectives for music active listening,\nediting and post-production from usual stereo formats (e.g.,\n5.1 upmixing), whereas those features are currently roughly\nlimited to multitrack formats, in which a very limited num-\nber of original commercial songs are distributed.\nDemixing professionally produced music (PPM) is par-\nticularly difﬁcult for several reasons [11, 12, 17]. Firstly,\nthe mix signals are generally underdetermined, i.e., there\nare more sources than mix channels. Secondly, some sour-\nces do not follow the point source assumption that is often\nimplicit in the (convolutive) source separation models of\nthe signal processing literature. Also, some sources can\nbe panned in the same direction, convolved with large re-\nverberation, or processed with artiﬁcial audio effects that\nare more or less easy to take into account in a separa-\ntion framework. PPM separation is thus an ill-posed prob-\nlem and separation methods have evolved from blind to in-\nformed source separation (ISS), i.e., methods that exploit\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.some “grounded” additional information on the source/mix\nsignals and mix process. For example, the methods in\n[1,4,5,8,20] exploit the musical score of the instrument to\nextract sources, either directly or through MIDI signal syn-\nthesis. In user-guided approaches, the listener can assist\nthe separation process in different ways, e.g., by humming\nthe source to be extracted [16], or by providing informa-\ntion on the sources direction [19] or temporal activity [12].\nAn extreme form of ISS can be found in [6, 9, 10, 14, 15]\nand in the Spatial Audio Object Coding (SAOC) technol-\nogy recently standardized by MPEG [3]: here, the source\nsignals themselves are used for separation, which makes\nsense only in a coder-decoder conﬁguration.\nIn the present paper, we remain in the usual conﬁgura-\ntion where the original multitrack signals are not available,\nalthough we keep the latter spirit of using source signals to\nhelp the demixing process: we propose to use cover mul-\ntitrack signals for this task. This idea is settled on several\nfacts. Firstly, a cover song can be quite different from the\noriginal for the sake of artistic challenge. But very interest-\ningly, for some applications/markets a cover song is on the\ncontrary intended to be as close as possible to the original\nsong: instruments composition and color, song structure\n(chorus, verses, solos), and artists interpretation (includ-\ning the voices) are then closely ﬁtted to the original source\nsignals, hence having a potential for source separation of\noriginal mixes. Remarkably, it happens that multitracks of\nsuch \"mimic\" covers are relatively easy to ﬁnd on the mar-\nket for a large set of famous pop songs. In fact, they are\nmuch easier to obtain than original multitracks. This is be-\ncause the music industry is very reluctant to release orig-\ninal works while it authorizes the licensed production of\nmimic multitracks on a large scale. In the present study, we\nwill use such multitracks provided by iKlax Media which\nis a partner of the DReaM project.1iKlax Media pro-\nduces software solutions for music active listening and has\nlicensed the exploitation of a very large set of cover mul-\ntitracks of popular songs. Therefore, this work involves a\nsizeable artistic and commercial stake. Note that similar\nmaterial can be obtained from several other companies.\nWe set the cover-informed source separation principle\nwithin the currently very popular framework of separation\nmethods based on a local time-frequency (TF) complex\nGaussian model combined with a non-negative matrix fac-\ntorization (NMF) model for the source variances [7,11,13].\n1This research is partly funded by the French National Research\nAgency (ANR) – Grant CONTINT 09-CORD-006.Iterative NMF algorithms for source modeling and separa-\ntion have shown to be very sensitive to initialization. We\nturn this weakness into strength within the following two-\nstep process in the same spirit as the work carried out on\nsignals synthesized from MIDI scores in, e.g., [8] or by\nhumming in [16]. First, source-wise NMF modeling is ap-\nplied on the cover multitrack, and the result is assumed\nto be a suitable initialization of the NMF parameters of the\noriginal sources (that were used to produce the commercial\nmix signal). Starting from those initial values, the NMF\nprocess is then reﬁned by applying to the mix the convo-\nlutive multichannel NMF model of [11]. This latter model\nprovides both reﬁned estimation of the source-within-mix\n(aka source images) NMF parameters and source separa-\ntion using Wiener ﬁlters built from those parameters.\nThe paper is organized as follows. In Sections 2 and 3,\nwe respectively present the models and method employed.\nIn Sections 4 and 5, we present the experiments we con-\nducted to assess the proposed method, and in Section 6,\nwe address some general perspectives.\n2. FRAMEWORK: THE CONVOLUTIVE\nMULTICHANNEL NMF MODEL\n2.1 Mixing Model\nFollowing the framework of [11], the PPM multichannel\nmix signal x(t) is modeled as a convolutive noisy mixture\nofJsource signals sj(t). Using the short-time Fourier\ntransform (STFT), the mix signal is approximated in the\nTF domain as:\nxfn=Afsfn+bfn; (1)\nwhere xfn= [x1;fn; : : : ; x I;fn]Tis the vector of complex-\nvalued STFT coefﬁcients of the mix signal, sfn= [s 1;fn;\n: : : ; s J;fn]Tis the vector of complex-valued STFT coefﬁ-\ncients of the sources, bfn= [b 1;fn; : : : ; b I;fn]Tis a zero-\nmean Gaussian residual noise, Af= [a 1;f; : : : ;aJ;f]is\nthe frequency-dependent mixing matrix of size I\u0002J(aj;f\nis the mixing vector for source j),f2[0; F\u00001]is the fre-\nquency bin index and n2[0; N\u00001]is the time frame in-\ndex. This approach implies standard narrowband assump-\ntion (i.e., the time-domain mixing ﬁlters are shorter than\nthe STFT window size).\n2.2 Source model\nEach source sj;fnis modeled as the sum of Kjlatent com-\nponents ck;fn,k2Kj, i.e.,\nsj;fn=X\nk2K jck;fn; (2)\nwherefKjgjis a non-trivial partition of f1; : : : ; Kg,K\u0015\nJ(Kjis thus the cardinal of Kj). Each component ck;fn is\nassumed to follow a zero-mean proper complex Gaussian\ndistribution of variance wfkhkn, where wfk; hkn2R+,\ni.e.,ck;fn\u0018 N c(0; w fkhkn). The components are as-\nsumed to be mutually independent and individually inde-pendent across frequency and time, so that we have:\nsj;fn\u0018N c(0;X\nk2K jwfkhkn): (3)\nThis source model corresponds to the popular non-negative\nmatrix factorization (NMF) model as applied to the source\npower spectrogram jSjj2=fjsj;fnj2gfn:\njSjj2'WjHj; (4)\nwith non-negative matrices Wj=fwfkgf;k2K jof size\nF\u0002KjandHj=fhkngk2K j;nof size Kj\u0002N. The\ncolumns of Wjare generally referred to as spectral pat-\ntern vectors, and the rows of Hjare referred to as tempo-\nral activation vectors. NMF is largely used in audio source\nseparation since it appropriately models a large range of\nmusical sounds by providing harmonic patterns as well as\nnon-harmonic ones (e.g., subband noise).\n2.3 Parameter estimation and source separation\nIn the source modeling context, the NMF parameters of a\ngiven source signal can be obtained from the observation\nof its power spectrogram using Expectation-Maximization\n(EM) iterative algorithms [7]. In [11], this has been gener-\nalized to the joint estimation of the Jsets of NMF source\nparameters and I\u0002J\u0002Fmixing ﬁlters parameters from\nthe observation of the mix signal power spectrogram. More\nprecisely, two algorithms were proposed in [11]. An EM\nalgorithm consists of maximizing the exact joint likelihood\nof the multichannel data, whereas a multiplicative updates\n(MU) algorithm, maximizes the sum of individual chan-\nnel log-likelihood. If the former better exploits the inter-\nchannel dependencies and gives better separation results,2\nthe latter has a lower computation cost. Those algorithms\nwill not be described in the present paper, the reader is re-\nferred to [11] for technical details.\nOnce all the parameters are estimated, the source sig-\nnals (or their spatial images yj;fn=aj;fsj;fn) are esti-\nmated using spatial Wiener ﬁltering of the mix signal:\n^sfn=\u0006s;fnAH\nf\u0006\u00001\nx;fnxfn; (5)\nwhere \u0006s;fn is the (estimated) covariance matrix of the\nsource signals, and \u0006x;fn=Af\u0006s;fnAH\nf+\u0006b;fis the\n(estimated) covariance matrix of the mix signal.\n3. PROPOSED COVER-INFORMED SEPARATION\nTECHNIQUE\n3.1 Cover-based initialization\nIt is well-known that NMF decomposition algorithms are\nhighly dependent on the initialization. In fact, the NMF\nmodel does not guarantee the convergence to a global min-\nimum but only to a local minimum of the cost function,\nmaking a suitable initialization crucial for the separation\nperformance. In the present study, we have at our disposal\n2When point source and convolutive mixing assumptions are veriﬁed.the 2-channel stereo multitrack cover of each song to sepa-\nrate, and the basic principle is to use the cover source tracks\nto provide relevant initialization for the joint multichannel\ndecomposition. Therefore, the NMF algorithms mentioned\nin Section 2 are applied on PPM within the following con-\nﬁguration. A ﬁrst multichannel NMF decomposition is run\non each stereo source of the cover multitrack (with ran-\ndom initialization). Thus, we obtain a modeled version of\neach cover source signal in the form of three matrices per\nsource: Wcover\nj ,Hcover\nj andAcover\nj =facover\nij;fgi2[1;2];f .\nThe results are ordered according to:\nWmix\ninit= [Wcover\n1: : :Wcover\nJ] (6)\nHmix\ninit=2\n64Hcover\n1\n...\nHcover\nJ3\n75 (7)\nAmix\ninit= [Acover\n1: : :Acover\nJ] (8)\nThen, (6), (7), and (8) are used as an initialization for a sec-\nond convolutive stereo NMF decomposition run on the mix\nsignal as in [11]. During this second phase, the spectral\npattern vectors and time activation vectors learned from\nthe cover source tracks are expected to evolve to match\nthe ones corresponding to the signals used to produce the\ncommercial mix, while the resulting mixing vectors are ex-\npected to fairly model the mix process.\n3.2 Pre-processing: time alignment of the cover tracks\nOne main difference between two versions of the same mu-\nsic piece is often the temporal misalignment due to both\ntempo variation (global misalignment) and musical inter-\npretation (local misalignments). In a general manner, time\nmisalignment can corrupt the separation performances if\nthe spectral pattern vectors used for initialization are not\naligned with the spectral patterns of the sources within the\nmix. In the present framework, this problem is expected to\nbe limited by the intrinsic automatic matching of temporal\nactivity vectors within the multichannel NMF decomposi-\ntion algorithm. However, the better the initial alignment,\nthe better the initialization process and thus expected ﬁnal\nresult. Therefore, we limit this problem by resynchroniz-\ning the cover tracks with the mix signal, in the same spirit\nas the MIDI score-to-audio alignment of [5] or the Dy-\nnamic Time Warping (DTW) applied on synthesized sig-\nnals in [8]. In the present study, this task is performed at\nquarter-note accuracy using the Beat Detective tool from\nthe professional audio editing software Avid ProToolsR\r.\nThis step allows minimizing synchronization error down\nto less than a few TF frames, which is in most cases below\nthe synchronization error limit of 200 ms observed in [5].\nIn-depth study of desynchronization on source separation\nis kept for future works.\n3.3 Exploiting the temporal structure of source signals\nIn order to further improve the results, we follow a user-\nguided approach as in [12]. The coefﬁcients of matrix Hare zeroed when the source is not active in the mix, ex-\nploiting audio markers of silence zones in the cover source\ntracks. As there still may be some residual misalignment\nbetween the commercial song and the cover after the pre-\nprocessing, we relax these constraints to 3frames before\nand after the active zone. When using the MU algorithm,\nthe zeroed coefﬁcients remain at zero. When using the EM\nalgorithm, the update rules do not allow the coefﬁcients of\nHto be strictly null, hence, we set these coefﬁcients to the\nepsvalue in our MatlabR\rimplementation. Observations\nconﬁrm that these coefﬁcients remain small throughout all\nthe decomposition.\n3.4 Summarizing the novelty of the proposed study\nWhile our process is similar in spirit to several existing\nstudies, e.g., [5,8,16], our contribution to the ﬁeld involves:\n\u000fthe use of cover multitrack signals instead of hum-\nmed or MIDI-synthesis source signals. Our cover\nsignals are expected to provide a more faithful image\nof the original source signals in the PPM context.\n\u000fa stereo NMF framework instead of a mono one. The\nmultichannel framework is expected to exploit spa-\ntial information in the demixing process (as far as\nthe convolutive model is a fair approximation of the\nmixing process). It provides optimal spatial Wiener\nﬁlters for the separation, as opposed to the {esti-\nmated magnitude + mix phase} resynthesis of [8] or\nthe (monochannel) soft masks of [16].\n\u000fa synchronization pre-process relying on tempo and\nmusical interpretation instead of, e.g., frame-wise\nDTW. This is completed with the exploitation of the\nsources temporal activity for the initialization of H.\n4. EXPERIMENTS\n4.1 Data and experimental settings\nAssessing the performances of source separation on true\nprofessionally-produced music data is challenging since\nthe original multitrack signals are necessary to perform ob-\njective evaluation but they are seldom available. Therefore,\nwe considered the following data and methodology. The\nproposed separation algorithm was applied on a series of 4\nwell-known pop-music songs for which we have the stereo\ncommercial mix signal and two different stereo multitrack\ncovers (see Table 2). The ﬁrst multitrack cover C1 was\nprovided by iKlax Media, and the second one C2 has been\ndownloaded from the commercial website of another com-\npany. We present two testing conﬁgurations:\n\u000fSetting 1: This setting is used to derive objective\nmeasures (see below). C1 is considered as the “orig-\ninal multitrack”, and used to make a stereo remix of\nthe song which is used as the target mix to be sepa-\nrated. This remix has been processed by a qualiﬁed\nsound engineer with a 10-year background in musicTracks duration 30 s\nNumber of channels I=2\nSampling Rate 32 kHz\nSTFT frame size 2048\nSTFT overlap 50 %\nNumber of iterations 500\nNumber of NMF components 12 or 50\nTable 1: Experimental settings\nproduction, using Avid ProToolsR\r.3C2 is consid-\nered as the cover version and is used to separate the\ntarget mix made with C1.\n\u000fSetting 2: The original commercial mix is separated\nusing C1 as the cover. This setting is used for sub-\njective evaluation in real-world conﬁguration.\nThe covers are usually composed of 8 tracks which are\nquite faithful to the commercial song content as explained\nin the introduction. For simplicity we merged the tracks\nto obtain 4 to 6 source signals.4All signals are resam-\npled at 32kHz, since source separation above 16kHz has\nvery poor inﬂuence on the quality of separated signals and\nthis enables to reduce computations. The experiments are\ncarried out on 30s excerpts of each song.\nIt is difﬁcult to evaluate the proposed method in refer-\nence to existing source separation methods since the cover\ninformation is very speciﬁc. However, in order to have\na reference, we also applied the algorithm with a partial\ninitialization: the spectral patterns Ware here initialized\nwith the cover spectral patterns, whereas the time activa-\ntion vectors Hare randomly initialized (vs. NMF initial-\nization in the full cover-informed conﬁguration). This en-\nables to i) separate the contribution of cover temporal in-\nformation, and ii) simulate a conﬁguration where a dictio-\nnary of spectral bases is provided by an external database\nof instruments and voices. This was performed for both\nEM and MU algorithms. The main technical experimental\nparameters are summarized in Table 1.\n4.2 Separation measures\nTo assess the separation performances in Setting 1, we\ncomputed the signal-to-distortion ratio (SDR), signal-to-\ninterference ratio (SIR), signal-to-artifact ratio (SAR) and\nsource image-to-spatial distortion ratio (ISR) deﬁned in\n[18]. We also calculated the input SIR (SIR in) deﬁned as\nthe ratio between the power of the considered source and\n3The source images are here the processed version of C1 just before\nﬁnal summation, hence we do not consider post-summation (non-linear)\nprocessing. The consideration of such processing in ISS, as in, e.g., [17],\nis part of our current efforts.\n4The gathering was made according to coherent musical sense and\npanning, e.g., grouping two electric guitars with the same panning in a\nsingle track. It is necessary to have the same number of tracks between\nan original version and its cover. Furthermore, original and cover sources\nshould share approximately the same spatial position (e.g., a cover ver-\nsion of a left panned instrument should not be right panned!)Title Tracks Track names\nI Will Survive 6 Bass, Brass, Drums,\nElecGuitar, Strings, V ocal.\nPride and Joy 4 Bass, Drums, ElecGuitar, V ocal.\nRocket Man 6 Bass, Choirs, Drums,\nOthers, Piano, V ocal.\nWalk this Way 5 Bass, Drums, ElecGuitar1,\nElecGuitar2, V ocal.\nTable 2: Experimental dataset\nMethod SDR ISR SIR SAR\nEM W init 0,04 3,51 -1,96 4,82\nEM Cover-based 2.45 6.58 4.00 5.38\nEM Improvement 2,41 3,08 5,97 0,56\nMU W init -0,98 3,58 -1,14 3,40\nMU Cover-based 1.38 6.83 5.04 2.95\nMU Improvement 2,36 3,24 6,18 -0,45\nTable 3: Average source separation performance for 4\nPPM mixtures of 4 to 6 sources (dB).\nthe power of all the other sources in the mix to be sepa-\nrated. We consider this criterion because all sources do not\ncontribute to the mix with the same power. Hence, a source\nwith high SIR inis easier to extract than a source with a low\nSIR in, and SIR inis used to characterize this difﬁculty.\n5. RESULTS\n5.1 Objective evaluation\nLet us ﬁrst consider the results obtained with Setting 1.\nThe results averaged across all sources and songs are pro-\nvided in Table 3. The maximal average separation perfor-\nmance is obtained with the EM cover-informed algorithm\nwith SDR = 2.45dB and SIR = 4.00dB. This corresponds\nto a source enhancement of SDR \u0000SIR in= 10.05dB and\nSIR\u0000SIR in= 11.60dB, with the average global SIR inbeing\nequal to\u00007.60dB. These results show that the overall pro-\ncess leads to fairly good source reconstruction and rejec-\ntion of competing sources. Figure 1a illustrates the separa-\ntion performances in terms of the difference SDR \u0000SIR in\nfor the song “I will survive”. The separation is very satisfy-\ning for tracks with sparse temporal activity such as Brass.\nThe Strings track, for which the point source assumption\nis less relevant, obtains correct results, but tends to spread\nover other sources images such as Bass. Finally, when\ncover tracks musically differ from their original sources,\nthe separation performance decreases. This is illustrated\nwith the Electric Guitar (EGtr) and Bass tracks, which do\nnot fully match the original interpretation.\nLet us now discuss the cover informed EM and MU\nmethods in relation to the initialization of spectral bases\nonly, referred to as W init. The cover-based EM algorithm\nprovides a notable average SDR improvement of 2.41dBover EM with W initinitialization, and a quite large im-\nprovement in terms of SIR (+5.97dB), hence a much better\ninterference rejection. The cover-based MU algorithm also\noutperforms the MU W initconﬁguration to the same extent\n(e.g., +2.36dB SDR and +6.18dB SIR improvement). This\nreveals the ability of the method to exploit not only spectral\nbut also temporal information provided by covers.\nNote that both cover-based and W initEM methods out-\nperform the corresponding MU methods in terms of SDR.\nHowever, it is difﬁcult to claim for clear-cut EM’s better\nuse of the inter-channel mutual information, since EM is\nslightly lower than MU for SIR (approx. 4dB vs. 5dB\nfor the cover-informed method). In fact, the multichannel\nframework can take advantage of both spectral and spatial\ninformation for source extraction, but this depends on the\nsource properties and mixing conﬁguration. In the song\n“Walk this way”, which detailed results are given in Figure\n1b, all sources but the Electric Guitar 1 (Egtr1) are panned\nat the center of the stereo mixture. Thus, the SDR \u0000SIR in\nobtained for Egtr1 reaches 20.32dB, as the algorithm re-\nlies strongly on spatial information to improve the separa-\ntion. On the other hand, the estimated V ocal track in “I\nwill survive” is well separated (+8.57dB SDR \u0000SIR infor\nthe cover-informed EM) despite being centered and coinci-\ndent to other tracks such as Bass, Drums and Electric Gui-\ntar (EGtr). In this case, the proposed multichannel NMF\nframework seems to allow separation of spatially coinci-\ndent sources with distinct spectral patterns. Depending\non the song, some sources obtain better SDR results with\nthe MU algorithm. For example, in “Walk this way”, the\nSDR\u0000SIR infor the Drums track increased from 6.59dB\nwith the EM method to 9.74dB with the MU method. As\npointed out in [11], the point source assumption certainly\ndoes not hold in this case. The different elements of the\ndrums are distributed between both stereo channels and the\nsource image cannot be modeled efﬁciently as a convolu-\ntion of a single point source. By discarding a large part of\nthe inter-channel information, the MU algorithm gives bet-\nter results in this case. Preliminary tests using a monochan-\nnel NMF version of the entire algorithm (monochannel\nseparation using monochannel initialization, as in, e.g., [8,\n16]), even show slightly better results for the Drums track,\nconﬁrming the irrelevancy of the point source convolutive\nmodel in this case.\nFinally, it can be mentioned that the number of NMF\ncomponents per source Kjdoes not inﬂuence signiﬁcantly\nthe SDR and SIR values, although we perceive a slight im-\nprovement during subjective evaluation for Kj= 50.5\n5.2 Discussion\nInformal listening tests on the excerpts from Setting 2 con-\nﬁrm the previous results and show the potential of cover-\ninformed methods for commercial mix signal separation.6\nOur method gives encouraging results on PPM when point\n5Assessing the optimal number of components for each source is a\nchallenging problem left for future work.\n6Examples of original and separated signals are available at\nhttp://www.gipsa-lab.grenoble-inp.fr/\u0018laurent.girin/demo/ismir2012.html.\nBass Brass Drums EGtr Strings Vocal0510152025\nEM – W init\nEM – Cover-informed\n \nMU – W init\nMU – Cover-informedSDR - SIR in (dB)(a) I Will Survive\nBass Drums EGtr1 EGtr2 Vocal0510152025\nEM – W init\nEM – Cover-informed\n \nMU – W init\nMU – Cover-informedSDR - SIR in (dB)\n(b) Walk This Way\nFigure 1: Separation results\nsource and convolutive assumptions are respected. For in-\nstance, the vocals are in most cases suitably separated, with\nonly long reverberation interferences. As expected, the\nquality of the mix separation relies on the quality and faith-\nfulness of the cover. A good point is that when original and\ncover interpretations are well matched, the separated sig-\nnal sounds closer to the original than to the cover, revealing\nthe ability of the adapted Wiener ﬁlters to well preserve the\noriginal information.\nComparative experiments with spectral basis initializa-\ntion only (W init) conﬁrm the importance of the temporal in-\nformation provided by covers, Although this has not been\ntested formally, the cover-to-mix alignment of Section 3.2\nwas shown by informal tests to also contribute to good sep-\naration performances.\n6. CONCLUSION\nThe results obtained by plugging the cover-informed source\nseparation concept in the framework of [11] show that both\nspectral and temporal information provided by cover sig-\nnals can be exploited for source separation. This study in-\ndicates the interest (and necessity) of using high-quality\ncovers. In this case, the separation process may better take\ninto consideration the music production subtleties, com-\npared to MIDI- or hummed-informed techniques.\nPart of the results show the limitations of the convo-\nlutive mixing model in the case of PPM. This is the case\nfor sources that cannot be modeled efﬁciently as a point\nsource convolved on each channel with a linear ﬁlter, such\nas large instruments (e.g., drums and piano). Also, sometracks such as vocals make use of reverberation times much\nhigher than our analysis frame. As a result, most of the vo-\ncals reverberation is not properly separated. The present\nstudy and model also do not consider the possible nonlin-\near processes applied during the mixing process.\nTherefore, further research directions include the use of\nmore general models for both sources and spatial process-\ning. For instance, we plan to test the full-rank spatial co-\nvariance model of [2], within the very recently proposed\ngeneral framework of [13] which also enables more spe-\nciﬁc source modeling, still in the NMF framework (e.g.,\nsource-ﬁlter models). Within such general model, sources\nactually composed of several instruments (e.g., drums) may\nbe spectrally and spatially decomposed more efﬁciently\nand thus better separated.\n7. REFERENCES\n[1] S. Dubnov. Optimal ﬁltering of an instrument sound\nin a mixed recording using harmonic model and score\nalignment. In Int. Computer Music Conf. (ICMC), Mi-\nami, FL, 2004.\n[2] N. Q. K. Duong, E. Vincent, and R. Gribonval. Under-\ndetermined reverberant audio source separation using\na full-rank spatial covariance model. IEEE Trans. on\nAudio, Speech, and Language Proc., 18(7):1830–1840,\n2010.\n[3] J. Engdegård, C. Falch, O. Hellmuth, J. Herre,\nJ. Hilpert, A. Hölzer, J. Koppens, H. Mundt, H. Oh,\nH. Purnhagen, B. Resch, L. Terentiev, M. Valero, and\nL. Villemoes. MPEG spatial audio object coding—the\nISO/MPEG standard for efﬁcient coding of interactive\naudio scenes. In 129th Audio Engineering Society Con-\nvention, San Francisco, CA, 2010.\n[4] S. Ewert and M. Müller. Score-informed voice separa-\ntion for piano recordings. In Proc. of the 12th Int. So-\nciety for Music Information Retrieval Conf. (ISMIR),\nMiami, USA, 2011.\n[5] S. Ewert and M. Müller. Using score-informed con-\nstraints for NMF-based source separation. In Proc. of\nthe IEEE Int. Conf. on Acoustics, Speech, and Signal\nProc. (ICASSP), Kyoto, Japan, 2012.\n[6] C. Faller, A. Favrot, Y-W Jung, and H-O Oh. Enhanc-\ning stereo audio with remix capability. In Proc. of the\n129th Audio Engineering Society Convention, 2010.\n[7] C. Févotte, N. Bertin, and J.-L. Durrieu. Nonnegative\nmatrix factorization with the Itakura-Saito divergence.\nWith application to music analysis. Neural Computa-\ntion, 21(3):793–830, 2009.\n[8] J. Ganseman, P. Scheunders, G. Mysore, and J. Abel.\nSource separation by score synthesis. In Proc. of the\nInt. Computer Music Conf. (ICMC), New-York, 2010.\n[9] S. Gorlow and S. Marchand. Informed source sepa-\nration: Underdetermined source signal recovery froman instantaneous stereo mixture. In Proc. IEEE Work-\nshop on Applications of Signal Processing to Audio\nand Acoustics (WASPAA), New Paltz, NY , 2011.\n[10] A. Liutkus, J. Pinel, R. Badeau, L. Girin, and\nG. Richard. Informed source separation through spec-\ntrogram coding and data embedding. Signal Process-\ning, 92(8):1937–1949, 2012.\n[11] A. Ozerov and C. Févotte. Multichannel nonnegative\nmatrix factorization in convolutive mixtures for audio\nsource separation. IEEE Trans. on Audio, Speech, and\nLanguage Proc., 18(3):550–563, 2010.\n[12] A. Ozerov, C. Févotte, R. Blouet, and J.-L. Durrieu.\nMultichannel nonnegative tensor factorization with\nstructured constraints for user-guided audio source\nseparation. In Proc. of the Int. Conf. on Acoustics,\nSpeech and Signal Proc. (ICASSP), Prague, Czech Re-\npublic, 2011.\n[13] A. Ozerov, E. Vincent, and F. Bimbot. A general ﬂexi-\nble framework for the handling of prior information in\naudio source separation. IEEE Trans. on Audio, Speech\nand Language Proc., 20(4):1118–1133, 2012.\n[14] M. Parvaix and L. Girin. Informed source separation of\nlinear instantaneous under-determined audio mixtures\nby source index embedding. IEEE Trans. on Audio,\nSpeech, and Language Proc., 19(6):1721–1733, 2011.\n[15] M. Parvaix, L. Girin, and J.-M. Brossier. A\nwatermarking-based method for informed source sep-\naration of audio signals with a single sensor. IEEE\nTrans. on Audio, Speech, and Language Proc.,\n18(6):1464–1475, 2010.\n[16] P. Smaragdis and G. Mysore. Separation by \"hum-\nming\": User-guided sound extraction from mono-\nphonic mixtures. In IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics (WASPAA),\nNew Paltz, NY , 2009.\n[17] N. Sturmel, A. Liutkus, J. Pinel, L. Girin, S. Marchand,\nG. Richard, R. Badeau, and L. Daudet. Linear mixing\nmodels for active listening of music productions in re-\nalistic studio condition. In Proc. of the 132th Audio En-\ngineering Society Conv., Budapest, Hungary, 2012.\n[18] E. Vincent, R. Gribonval, and C. Févotte. Perfor-\nmance measurement in blind audio source separation.\nIEEE Trans. on Audio, Speech, and Language Proc.,\n14(4):1462–1469, 2006.\n[19] M. Vinyes, J. Bonada, and A. Loscos. Demixing com-\nmercial music productions via human-assisted time-\nfrequency masking. In Proc. of the 120th Audio En-\ngineering Society Convention, 2006.\n[20] J. Woodruff, B. Pardo, and R. B. Dannenberg. Remix-\ning stereo music with score-informed source separa-\ntion. In Int. Society for Music Information Retrieval\nConference (ISMIR), Victoria, Canada, 2006."
    },
    {
        "title": "Detecting Episodes with Harmonic Sequences for Fugue Analysis.",
        "author": [
            "Mathieu Giraud",
            "Richard Groult",
            "Florence Levé"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416596",
        "url": "https://doi.org/10.5281/zenodo.1416596",
        "ee": "https://zenodo.org/records/1416596/files/GiraudGL12.pdf",
        "abstract": "Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modula- tory sections called episodes. The episodes play an impor- tant role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an al- gorithm to retrieve episodes in the fugues of the first book of Bach’s Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject oc- currences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering “quantized par- tially overlapping intervals” [14] and a strict length match- ing for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue.",
        "zenodo_id": 1416596,
        "dblp_key": "conf/ismir/GiraudGL12",
        "keywords": [
            "fugues",
            "subject",
            "counter-subject",
            "modulatory sections",
            "episodes",
            "fugue design",
            "symbolic score",
            "track-separated",
            "partial harmonic sequences",
            "substitution function"
        ],
        "content": "DETECTING EPISODES WITH HARMONIC SEQUENCES\nFOR FUGUE ANALYSIS\nMathieu Giraud\nLIFL, CNRS, Universit ´e Lille 1\nINRIA Lille, FranceRichard Groult\nMIS, Universit ´e Picardie Jules Verne\nAmiens, FranceFlorence Lev ´e\nMIS, Universit ´e Picardie Jules Verne\nAmiens, France\nABSTRACT\nFugues alternate between instances of the subject and of\nother patterns, such as the counter-subject, and modula-\ntory sections called episodes. The episodes play an impor-\ntant role in the overall design of a fugue: detecting them\nmay help the analysis of the fugue, in complement to a\nsubject and a counter-subject detection. We propose an al-\ngorithm to retrieve episodes in the fugues of the ﬁrst book\nof Bach’s Well-Tempered Clavier, starting from a symbolic\nscore which is already track-separated. The algorithm does\nnot use any information on subject or counter-subject oc-\ncurrences, but tries to detect partial harmonic sequences,\nthat is similar pitch contour in at least two voices. For this,\nit uses a substitution function considering “quantized par-\ntially overlapping intervals” [14] and a strict length match-\ning for all notes, except for the ﬁrst and the last one. On\nhalf of the tested fugues, the algorithm has correct or good\nresults, enabling to sketch the design of the fugue.\n1. INTRODUCTION\nA fugue is a polyphonic piece built in imitation, where all\nvoices appear successively sharing the same initial melodic\nmaterial: a subject and, in most cases, a counter-subject.\nThese patterns are repeated throughout the piece, either in\ntheir initial form or more often altered or transposed, build-\ning a complex harmonic texture. Many composers wrote\nfugues, or included fugal parts in larger pieces. The two\nbooks of Bach’s Well-Tempered Clavier are a particularly\nconsistent corpus, exploring the 24 major and minor tonal-\nities in 48 preludes and fugues.\nFugues are often viewed as one of the pinnacle forms of\nWestern music, and they are also used for pedagogical pur-\nposes, in music analysis as in composition. Their structure\nmay look very formal, but still enable high levels of cre-\nativity. There are many treatises on fugues, or, more gener-\nally, on counterpoint, as for example [13] or [18]. Some of\nthem include a complete musicological analysis of Bach’s\nWell-Tempered Clavier, as the books of S. Bruhn [3, 4].\nThe fugues are thus perfect candidates for Music Informa-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.tion Retrieval (MIR) research, stimulating the development\nof algorithms on symbolic scores.\nA ﬁrst way to analyze fugues can be to use generic tools\ndetecting repeating patterns or themes, possibly with ap-\nproximate occurrences. Similarity between parts of a piece\nmay be computed by the Mongeau-Sankoff algorithm [17]\nand its extensions, or by other methods for approximate\nstring matching [6, 7, 19], allowing a given number of re-\nstricted mismatches. Several studies focus on ﬁnding max-\nimal repeating patterns, limiting the search to non-trivial\nrepeating patterns, that is discarding patterns that are a sub-\npattern of a larger one with the same frequency [10,12,15].\nOther studies try to ﬁnd musically signiﬁcant themes, with\nalgorithms considering the number of occurrences [20],\nbut also the melodic contour or other features [16].\nMore speciﬁcally, some MIR studies already focused\non fugues. The study [21] builds a tool to decide if a piece\nis a fugue or not, with a method to ﬁnd occurrences of\nthematic materials. The bachelor thesis [2] contains meth-\nods to analyze fugues, including voice separation. It pro-\nposes several heuristics to help the selection of repeating\npatterns inside the algorithms of [10] which maximizes the\nnumber of occurrences. The web site [9] also produces an\nanalysis of fugues, extracting sequences of some repeating\npatterns, but without precise formal analysis. Finally, we\nproposed in [8] a method to detect subjects and counter-\nsubjects, based on an analysis of repeating patterns with a\ndiatonic substitution function and a speciﬁc length match-\ning. This method ﬁnds the precise ends of these patterns\nin the majority of the fugues of the ﬁrst book of Bach’s\nWell-Tempered Clavier.\nThe subject and the counter-subject are focus of musi-\ncal cognition, and will often be what is remembered from\na fugue. However, the link between the different exposi-\ntions of these patterns occurs in transitional sections called\nepisodes that modulate from one tonality to another [13,\n18]. The episodes have a part in the development of ten-\nsion during the fugue.\nTo our knowledge, no previous MIR study was devoted\nto analysis of episodes. Episodes can be detected by the\nabsence of subjects and counter-subjects: A perfect detec-\ntion of subjects and counter-subjects should yield a perfect\nepisode detection. In this paper, we try to retrieve episodes\nwithout using any information on subject or counter-subject\noccurrences. We thus look for a positive identiﬁcation ofepisodes. Starting from a symbolic score which is already\ntrack-separated, we propose an algorithm to retrieve episo-\ndes containing partial harmonic sequences, that is similar\npitch contour in at least two voices. Harmonic sequences\nare commonly used to modulate, and are thus an essential\nfeature of many episodes.\nAs in [8], the algorithm uses a strict length matching for\nall notes, except for the ﬁrst and the last one. We tested sev-\neral substitution functions to have a sensible and speciﬁc\napproximate matching. Our best results use the “quantized\npartially overlapping intervals” (QPI), introduced by Lem-\nstr¨om and Laine in [14], that can be also seen as one case\nof the “General Pitch Interval Representation” deﬁned by\nCambouropoulos in [5].\nThe paper is organized as follows. Section 2 gives def-\ninitions and some background on fugues, Section 3 details\nour algorithm for episode detection through partial har-\nmonic sequences, and Section 4 details the results on 21\nfugues of the ﬁrst book of Bach’s Well-Tempered Clavier.\nThese results were evaluated against a reference musico-\nlogical book [4]. On half of the tested fugues, the algo-\nrithm has correct or good results, enabling to sketch the\ndesign of the fugue. The other cases are fugues where\nthe episodes do not show enough harmonic sequences, or\nwhere the sequences are too short or too much altered.\n2. PRELIMINARIES\nAnotexis described by a triplet (p;o;` ), wherepis the\npitch,othe onset, and `the length. The pitches can de-\nscribe diatonic (based on note names) or semitone infor-\nmation. We consider ordered series of notes x1:::x m,\nthat isx1= (p 1;o1;`1);:::;x m= (p m;om;`m), where\n1\u0014o1\u0014o2\u0014:::\u0014om(see Figure 1). The se-\nries is monophonic if there are never two notes sounding\nat the same onset, that is, for every iwith1\u0014i < m ,\noi+`i\u0014oi+1. To be able to match transposed patterns, we\nconsider relative pitches, also called intervals: the interval\nseries is deﬁned as\u0001x2:::\u0001xm, where\u0001xi= (\u0001pi;oi;`i)\nand\u0001pi=pi\u0000pi\u00001.\nFigure 1. A monophonic series of notes (start of Fugue #2,\nsee Figure 4), represented by (p;o;` )or(\u0001p;o;` )triplets.\nIn this example, onsets and lengths are counted in six-\nteenths, and pitches and intervals are counted in semitones\nthrough the MIDI standard.\nFugue. We now introduce some notions about fugue anal-\nysis. These concepts are illustrated by Fugue #2 of the ﬁrst\nbook of Bach’s Well-Tempered Clavier. This fugue has a\nvery regular construction.Afugue is given by a set of voices, where each voice is a\nmonophonic series of notes. In Bach’s Well-Tempered Cla-\nvier, the fugues have between 2and5voices, and Fugue\n#2 is made of 3 voices.\nThe fugue is built on a theme called subject. The ﬁrst\nthree occurrences of the subject in Fugue #2 are detailed\nin Figure 4: the subject is exposed at one voice (the alto),\nbeginning on a C, until the second voice enters (the so-\nprano, measure 3). The subject is then exposed at the sec-\nond voice, but is now transposed to G. Meanwhile, the ﬁrst\nvoice continues with the ﬁrst counter-subject that com-\nbines with the subject. Figure 3 shows a sketch of the en-\ntire fugue. The fugue alternates between other instances\nof the subject together with counter-subjects and develop-\nment and modulatory sections called episodes.\nEpisodes and sequences. The episodes “effect a smooth\ntransition from one key to the next [and] provide variety,\nas well as relief from a constant emphasis on the subject as\na whole” [13]. They are often built on portions of material\nfrom the subjects of counter-subjects. S. Bruhn lists three\nroles for an episode in the design of the fugue: “It can\nlink two subject statements by leading from one towards\nthe next; it can be conclusive by resolving tension that was\nbuilt up by the preceding subject statement; it can represent\na different register, appearing basically independent of its\nsurroundings and serving as a color contrast.” [4].\nThe Figure 3 shows the two ﬁrst episodes of Fugue #2.\nNote that the term “episode” can also be restrained to the\nones after the exposition of all voices, the ﬁrst episode be-\ning called codetta [18].\nThe episodes can include cadential passages for the re-\nlease of tension. However, they are often composed with\nharmonic sequences, which are passages where a pattern\nis consecutively repeated starting on a different pitch. Fig-\nure 2 shows a simple harmonic sequence, outside of a fugue.\nSequences can be diatonic (keeping the same key signa-\nture, possibly modulating to a neighbor tonality) or real\n(possibly gaining or losing some sharps or ﬂats, often mod-\nulating to some other tonality).\nFigure 2. A simple diatonic sequence [1]. The values in-\ndicate the intervals from the preceding note of the same\nvoice, in number of semitones. The occurrences #1 and\n#3 have exactly the same semitone intervals. The occur-\nrence #2 is identical to these occurrences when one con-\nsiders only diatonic information.Figure 3. Analysis of Fugue #2 in C minor in the ﬁrst book of Bach’s Well-Tempered Clavier (BWV 847). Top: ground\ntruth (analysis by S. Bruhn, used with permission [4], [4, p. 80]). Bottom: the two lines named “detected sequences”\nshow the output of the proposed algorithm, detecting partial harmonic sequences in 5 out of the 6 episodes and 68% of\nthe concerned measures. The only false positive is the end of the second episode: at measure 11, it overlaps with the next\noccurrence of the subject (S) and counter-subject (CS).\nFigure 4. Start of Fugue #2 in C minor (BWV 847), showing the ground truth for the ﬁrst two episodes. Non-episodic\nparts are grayed. The notes starting the initial patterns and the occurrences of the sequences are circled.3. EPISODE DETECTION\nWe propose here to detect episodes containing partial har-\nmonic matches in at least two voices. For this, we con-\nsider consecutively repeating patterns under a substitution\nfunction using a relaxed similarity for pitch intervals, and\nenforcing length equalities of all notes but the ﬁrst one and\nthe last one. These are very conservative settings, to have\nas few false positives as possible.\nConsecutively repeating patterns. Formally, in a given\nvoicex, we look for consecutively repeating patterns of\npnotes, starting at note xe. The pattern xe:::xe+p\u00001 has\na candidate second occurrence xe+p:::xe+2p\u00001, and, for\nlarger episodes, we also check for a third (x e+2p:::xe+3p\u00001)\nand fourth (x e+3p:::xe+4p\u00001) occurrences.\nThe scoreI(x;e;p;r )between the pattern xe:::xe+p\u00001\nand its candidate occurrence number r(r=2, 3 or 4) is\ndeﬁned by the number of intervals matched between the\npattern and its candidate occurrence:\nI(x;e;p;r ) =\u000e(\u0001xe+1;\u0001xe+(r\u00001)p+1 )\n+\u000e(\u0001xe+2;\u0001xe+(r\u00001)p+2 )\n+\u0001\u0001\u0001\n+\u000ef(\u0001xe+p;\u0001xe+rp)\nAs in [8], we propose to use a strict length matching for\nall notes, except for the ﬁrst and the last one – the length\nof these notes, at the extremities of the pattern, being more\nfrequently altered. The substitution function \u000echecks thus\npitch intervals and lengths, whereas the substitution func-\ntion,\u000ef, for the last note, only considers pitch intervals:\n\u000e((\u0001p;o;` );(\u0001p0;o0;`0)) =8\n<\n:+1 if\u0001p\u0019\u0001p0and`=`0\n0 if\u0001p6\u0019\u0001p0and`=`0\n\u00001 otherwise (`6=`0)\n\u000ef((\u0001p;o;` );(\u0001p0;o0;`0)) =\u001a\n+1 if\u0001p\u0019\u0001p0\n0 otherwise (\u0001p6\u0019\u0001p0)\nThe actual comparison of lengths (` =`0) also checks\nthe equality of the rests that may be immediately before\nthe compared notes. The length of the ﬁrst note of the pat-\ntern (x eagainstxe+(r\u00001)p) is never checked, as the score\nactually compares the series of intervals\u0001xe+1:::\u0001xe+p\nagainst\u0001xe+(r\u00001)p+1:::\u0001xe+rp.\nThe relation\u0019is a similarity relation on pitch intervals.\nWe use here the “quantized partially overlapping intervals”\n(QPI) [14], that deﬁnes short intervals (from one to three\nsemitones), medium intervals (from three to seven semi-\ntones), and large intervals (starting from six semitones).\nThese classes can be considered for upward or down-\nwards intervals, giving, with the unison intervals, a total\nof 7 different interval classes. Two pitch intervals\u0001pand\n\u0001p0will be considered as similar if there exists one class\ncontaining both of them.There is an exact occurrence of the consecutively re-\npeating pattern if I(x;e;p;r ) =p\u00001. For example, on the\nsequence depicted on Figure 2, for any of the four voices x\nand forr2[2;3], we haveI(x;1;4;r) = 3 , since intervals\nare perfectly similar under the QPI similarity relation. An\napproximate occurrence can be detected if I(x;e;p;r )is\nat least equal to a given threshold \u001c(p).\nHere the score I(x;e;p;r )only considers substitution\noperations, and can be computed in time O(p). The score\ncan be extended to consider other edit operations, with\ncomputation through dynamic programming.\nEpisode detection through partial sequences. On the be-\nginning of the Fugue #2, the consecutively repeating pat-\nterns are as follows:\n\u000fthe second episode ﬁts perfectly into an sequence:\nI(soprano; 58;7;2) = 6 ,I(alto; 76;7;2) = 6 and\nI(tenor; 21;16;2) = 15 .\n\u000fthe ﬁrst episode has two complete occurrences, as\nI(soprano; 21;5;2) = 4 andI(alto; 41;6;2) = 5.\nThere is no complete third occurrence, as the lengths\ndo not match for one voice: I(alto; 41;6;3) =\u00001.\nThe complete algorithm computes I(x;e;p;r )for every\nvoicex, every note xestarting right after a quarter beat,\nseveral periods (1 quarter, and 1/2, 1 and 2 measures) and\nforr2f2; 3;4goccurrences. The algorithm reports an\nepisode every time that at least two different voices contain\na consecutively repeating pattern after the same onset (with\n\u001c(p) = 0:5\u0002p). Overlapping episodes with the same\nperiod are merged into an unique episode. The result on\nthe Fugue #2 is depicted at the bottom of Figure 3.\nFor testing purposes, we used a naive implementation\nrunning inO(n2)worst time, where nis the total number\nof notes in the fugue. Even if similarities between occur-\nrences in a sequence could be computed with tools in exist-\ning frameworks (such as the simil tool in the Humdrum\ntoolkit [11, 19]), we coded our own implementation to be\nable to handle some speciﬁcities (speciﬁc length matching,\npartial detection in two voices).\n4. RESULTS AND DISCUSSION\nResults can be asserted in two different ways:\n\u000fWe can count the sequences that are located com-\npletely or partially in episodes of the ground truth;\n\u000fMore precisely, we can look at the total length of de-\ntected sequences, and compare it to the total length\nof all ground truth episodes, computing a ratio called\nlength sensibility. This sensibility can be seen as a\ncoverage of episodes by harmonic sequences: it will\nnot reach 100%, as some episodes do not have se-\nquences, and as the sequences may not be spanning\nall the episodes. We also compute a length speci-\nﬁcity as the ratio between the lengths of true positive\nmeasures and of detected measures.The result on Fugue #2 is shown at the bottom of Fig-\nure 3. Here 5 episodes out of 6 are detected with partial\nharmonic sequences. This is musically relevant, since the\nlast episode (measures 29-31) is a cadential end, with a last\nexposition on the subject on a bass pedal. The ground truth\nhas 14 1/2 measures of episodes. The algorithm outputs 10\nmeasures (length sensibility of 68%) and falsely marks one\nhalf measure (2 quarters) as an episode (length speciﬁcity\nof 96%).\nThe false negatives are: 1 measure at the codetta (due\nto the shift between the two voices, only 2 occurrences are\ndetected), 2 measures and a half at measure 24 (including\na change of voices, see below), and all the 3 measures of\nthe last episode (discussed above). The only false positive\nis the end of the second episode, which is extended 2 quar-\nters below the next subject occurrence at measure 11, the\nsoprano and the bass voices continuing the sequence (see\nFigure 4, last measure).\nThe complete test contains 21 fugues of the ﬁrst book of\nBach’s Well-Tempered Clavier (fugues #1, #4 and #9 not\nshowing signiﬁcant episodic material). We started from\n.krn Humdrum ﬁles [11], available for academic pur-\nposes at http://kern.humdrum.org/. The output\nof the algorithm on all these 21 fugues is available at http:\n//www.lifl.fr/ ˜giraud/fugues. We checked all\ndetected episodes, and Table 1 summarizes the results. On\nthe 1098 measures of this test set, the algorithm labels\nabout 20% of the measures as episodes, and ﬁnally identi-\nﬁes 43% of all episodes. A subjective quality assessment\non the predictions, looking on the detailed output of each\nrun, gives a good mark on 6 fugues, and a correct mark for\n5 out of the 21 fugues.\nFalse positives. There are very few false positives: less\nthan 5% of the partial harmonic sequences overlap with\nsubject and counter-subject occurrences. As for the mea-\nsure 11 in Fugue #2, this is often because the texture of the\nepisode fades into the next section.\nFalse negatives. The length sensibility, that is the cover-\nage of the episodes (in the ground truth) by the prediction\nof harmonic sequences is, in average, only 36%. These\nfalse negatives can be explained by several facts:\n\u000fAs mentioned above, the sequences often not cover\nall the episodes. Moreover, there are some episodes\nwith no harmonic sequence: It is often the case for\nthe last episode, which is thus almost always missed\nby the proposed method;\n\u000fThere are some episodes with changes of voices (Fig-\nure 5), in which the consecutive occurrences of a pat-\ntern are not in a same voice;\n\u000fFinally, the algorithm fails to detect some partial har-\nmonic sequences that are too much altered to be rec-\nognized with the current threshold, or too short to be\ndiscovered.\nFigure 5. Partial sequence with a change of voices: the\npattern is heard at the soprano, then, transposed, at the alto\n(measure 24 of Fugue #2).\n5. CONCLUSIONS\nWe proposed an algorithm retrieving some episodes in the\nﬁrst book of Bach’s Well-Tempered Clavier, starting from\na symbolic score which is already track-separated. To our\nknowledge, this is the ﬁrst MIR study on episodes in fugues.\nThe algorithm, relying only on partial harmonic sequences\ndetection, gives very few false positives, and already gives\ngood results on some fugues. Enabling voice changes in-\nside harmonic sequences should naturally detect more epi-\nsodes, but may produce more false positives.\nMany improvements are possible to have a better anal-\nysis of episodes. Detection of other positive features of\nthe episodes (such as cadential passages) or, most of all, of\nsome negative features (subject and counter-subject occur-\nrences) could probably lead to a complete fugue analysis\npipeline with better results.\nThe algorithm could also be tested on other corpus of\nfugues. As an example, the web page http://www.\nlifl.fr/ ˜giraud/fugues shows the output of the\nproposed algorithm in the fugue of Mozart’s Adagio and\nFugue in C minor, K 546. Finally, partial or full harmonic\nsequence detection could be used to help the analysis of\nother genres.\nAcknowledgements. The authors would like to thank the\nanonymous reviewers for their valuable comments.\n6. REFERENCES\n[1] Marcel Bitsch. Pr´ecis d’harmonie tonale. Alphonse Leduc,\n1957.\n[2] Lisa Browles. Creating a tool to analyse contrapuntal music.\nBachelor Dissertation, Univ. of Bristol, 2005.\n[3] Siglind Bruhn. J. S. Bach’s Well-Tempered Clavier. In-\ndepth Analysis and Interpretation. 1993. ISBN 962-580-017-\n4, 962-580-018-2, 962-580-019-0, 962-580-020-4. Avail-\nable online at http://www-personal.umich.edu/\n˜siglind/text.htm.\n[4] Siglind Bruhn. J. S. Bachs Wohltemperiertes Klavier, Analyse\nund Gestaltung. Edition Gorz, 2006. ISBN 3-938095-05-9.\n[5] Emilios Cambouropoulos. A general pitch interval represen-\ntation: Theory and applications. Journal of New Music Re-\nsearch, 25(3):231–251, 1996.ground truth found by the proposed algorithm\n# BWV tonality voices` nb` nb`(TP) `(FP) sens spec quality\n2 847 C minor 3 31m 6 14m2q 5 10m 2q 68% 96% +\n3 848 C# major 3 55m 7 31m2q 4 17m 1m 54% 94% +\n5 850 D major 4 27m 5 16m+1q 4 7m – 41% 100% =\n6 851 D minor 3 44m 8 12m 3 7m 1m+1q 57% 84% =\n7 852 Eb major 3 37m 9 22m+2q 7 16m+1q – 72% 100% +\n8 853 D# minor 3 87m 10 22m+2q 1 3m+1q 2q 15% 100% –\n10 855 E minor 2 42m 4 25m 4 18m+2q 1m 75% 95% +\n11 856 F major 3 72m 6 31m+1q 4 10m 1m 31% 87% =\n12 857 F minor 4 58m 8 26m 5 12m+3q 7m 48% 65% –\n13 858 F# major 3 35m 6 17m+2q 4 7m+1q 2m+2q 42% 75% =\n14 859 F# minor 4 40m 5 10m 4 5m+3q 3m 56% 66% =\n15 860 G major 3 86m 9 36m 6 11m 15m+1q 30% 42% –\n16 861 G minor 4 34m 6 12m+1q 2 3m+1q 1m+2q 27% 71% –\n17 862 Ab major 4 35m 7 24m+2q 4 5m+1q 1q 21% 100% –\n18 863 G# minor 4 41m 6 17m 4 9m+3q 1m 57% 92% +\n19 864 A major 3 54m 7 27m 1 2m+3q – 10% 100% –\n20 865 A minor 4 87m 16 19m 3 1m+2q 10m+2q 9% 14% –\n21 866 Bb major 3 48m 3 14m+1q 3 12m 1q 83% 96% +\n22 867 Bb minor 5 75m 6 34m+2q 1 2m+1q 2m 6% 60% –\n23 868 B major 4 34m 7 10m+1q 0 – 0% –\n24 869 B minor 4 76m 12 38m+1q 3 7m+2q 3m 20% 76% –\n1098m 153 457m+20q 72 164m+24q 48m+14q + : 6\n= : 5\n– : 10\nTable 1. Detection of episodes in 21 fugues of the ﬁrst book of Bach’s Well-Tempered Clavier. The ground truth is the\nanalysis of [4]. All lengths (`) are given in number of measures (m) and quarters (q). The sensibility (sens.) and speciﬁcity\n(spec.) are computed on the lengths (see Section 4). The last column, “quality”, is a subjective assessment on the output\nof the algorithm: ﬁnd almost all episodes with \u001550% length sensibility, almost no false positives with \u001590% length\nspeciﬁcity (good, +), ﬁnd most of the episodes, few false positives (correct, =), miss many episodes or produces many false\npositives (bad, –).\n[6] Rapha ¨el Clifford and Costas S. Iliopoulos. Approximate\nstring matching for music analysis. Soft. Comput., 8(9):597–\n603, 2004.\n[7] T. Crawford, C. Iliopoulos, and R. Raman. String matching\ntechniques for musical similarity and melodic recognition.\nComputing in Musicology, 11:71–100, 1998.\n[8] Mathieu Giraud, Richard Groult, and Florence Lev ´e. Subject\nand counter-subject detection for fugue analysis. In Computer\nMusic Modeling and Retrieval (CMMR 2012), 2012.\n[9] J. Hakenberg. The Pirate Fugues. http://www.\nhakenberg.de/music/music.htm.\n[10] J. L. Hsu, C. C. Liu, and A. Chen. Efﬁcient repeating pattern\nﬁnding in music databases. In Int. Conference on Information\nand Knowledge Management (CIKM 1998), 1998.\n[11] David Huron. Music information processing using the hum-\ndrum toolkit: Concepts, examples, and lessons. Computer\nMusic Journal, 26(2):11–26, 2002.\n[12] Ioannis Karydis, Alexandros Nanopoulos, and Yannis\nManolopoulos. Finding maximum-length repeating patterns\nin music databases. Multimedia Tools Appl., 32:49–71, 2007.\n[13] Kent Kennan. Counterpoint. Prentice Hall, 4th ed., 1999.[14] Kjell Lemstr ¨om and Pauli Laine. Musical information re-\ntrieval using musical parameters. In Int. Computer Music\nConference (ICMC ’98), pages 341–348, 1998.\n[15] Chih-Chin Liu, Jia-Lien Hsu, and Arbee L.P. Chen. Efﬁcient\ntheme and non-trivial repeating pattern discovering in mu-\nsic databases. In Int. Conf. on Data Engineering (ICDE 99),\npages 14–21, 1999.\n[16] Colin Meek and William P Birmingham. Automatic the-\nmatic extractor. Journal of Intelligent Information Systems,\n21(1):9–33, 2003.\n[17] Marcel Mongeau and David Sankoff. Comparaison of musi-\ncal sequences. Computer and the Humanities, 24:161–175,\n1990.\n[18] Hugo Norden. Foundation Studies in Fugue. Crescendo Pub-\nlishing, 1977.\n[19] Keith S. Orpen and David Huron. Measurement of similarity\nin music: A quantitative approach for non-parametric repre-\nsentations. Computers in Music Research, 4:1–44, 1992.\n[20] Lloyd Smith and Richard Medina. Discovering themes by ex-\nact pattern matching. In Int. Symp. for Music Information Re-\ntrieval (ISMIR 2001), pages 31–32, 2001.\n[21] Pei-Hsuan Weng and Arbee L. P. Chen. Automatic musical\nform analysis. In Int. Conference on Digital Archive Tech-\nnologies (ICDAT 2005), 2005."
    },
    {
        "title": "Reducing Tempo Octave Errors by Periodicity Vector Coding And SVM Learning.",
        "author": [
            "Aggelos Gkiokas",
            "Vassilis Katsouros",
            "George Carayannis"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417439",
        "url": "https://doi.org/10.5281/zenodo.1417439",
        "ee": "https://zenodo.org/records/1417439/files/GkiokasKC12.pdf",
        "abstract": "In this paper we present a method for learning tempo classes in order to reduce tempo octave errors. There are two main contributions of this paper in the rhythm analy- sis field. Firstly, a novel technique is proposed to code the rhythm periodicity functions of a music signal. Target tempi range is divided into overlapping “tempo bands” and the periodicity function is filtered by triangular masks aligned to those tempo bands, in order to calculate the re- spective saliencies, followed by the application of the DCT transform on band strengths. The second contribution is the adoption of Support Vector Machines to learn broad tempo classes from the coded periodicity vectors. Training instances are assigned a tempo class according to annotated tempo. The classes are assumed to correspond to “music speed”. At classifi- cation phase, each target excerpt is assigned a tempo class label by the SVM. Target periodicity vector is masked by the predicted tempo class range, and tempo is estimated by peak picking in the reduced periodicity vector. The proposed method was evaluated on the benchmark ISMIR 2004 Tempo Induction Evaluation Exchange Dataset for both tempo class and tempo value estimation tasks. Results indicate that the proposed approach pro- vides an efficient framework to tackle the tempo estima- tion task.",
        "zenodo_id": 1417439,
        "dblp_key": "conf/ismir/GkiokasKC12",
        "keywords": [
            "tempo classes",
            "tempo octave errors",
            "novel technique",
            "code rhythm periodicity functions",
            "tempo bands",
            "triangular masks",
            "DCT transform",
            "Support Vector Machines",
            "tempo value estimation",
            "tempo induction"
        ],
        "content": "REDUCING TEMPO OCTAVE ERRORS BY PERIODICITY \nVECTOR CODING A\nND SVM LEARNING \nAggelos Gkiokas1,2 Vassilis Katsour os George Carayannis \n1Institute for Language and Speech Proc-\nessing / R.C. Athena\n \n2National Technical University of Athens \nagkiokas@ils\np.gr Institute for Langua\nge and \nSpeech Processing / R.C. \nAthena \nvsk@ilsp.gr  \n National Technical\n University \nof Athens \ncarayan \n@softlab.ece.ntua.gr  \nABSTRACT \nIn this paper we present a method for learning tempo \nclasses in order to reduce tempo octave errors. There are \ntwo main contributions of this paper in the rhythm analy-\nsis field. Firstly, a novel technique is proposed to code the \nrhythm periodicity functions of a music signal. Target \ntempi range is divided into overlapping “tempo bands” \nand the periodicity function is filtered by triangular masks \naligned to those tempo bands, in order to calculate the re-\nspective saliencies, followed by the application of the \nDCT transform on band strengths. \nThe second contribution is the adoption of Support \nVector Machines to learn broad tempo classes from the \ncoded periodicity vectors. Training instances are assigned \na tempo class according to annotated tempo. The classes \nare assumed to correspond to “music speed”. At classifi-\ncation phase, each target excerpt is assigned a tempo class \nlabel by the SVM. Target periodicity vector is masked by \nthe predicted tempo class range, and tempo is estimated \nby peak picking in the reduced periodicity vector. \nThe proposed method was evaluated on the benchmark \nISMIR 2004 Tempo Induction Evaluation Exchange \nDataset for both tempo class and tempo value estimation \ntasks. Results indicate that the proposed approach pro-\nvides an efficient framework to tackle the tempo estima-\ntion task. \n1. INTRODUCTION \nMost tempo estimation systems suffer from detecting the \ncorrect metrical level, i.e. tend to result in tempi that are \nfractions or multiples of the groundtruth tempo. Such er-\nrors are usually found in the literature as “octave errors”. \nAlthough many methods are reported to achieve accuracy \nover 90% [1-3] when ignoring octave errors, i.e. accuracy \nfor finding the exact, double, treble, half or 1/3 of ground-\ntruth tempo (known as accuracy2 measure), the accuracy \nof these methods decreases to 50~60% for finding the ex-act tempo ( accuracy1).  More details on rhythm analysis \nsystems and evaluation measures can be found in [4,5].  \nTwo certain contemporary aspects arise when consid-\nering the octave error problem. First, when allowing an \nalgorithm to make errors that correspond to the different \nmetrical levels, one can say that such an approach is more \nclose to the notion of perceptual tempo. Different users \nwould tap at different metrical levels for the same song. \nEven a single user might tap at different metrical levels \nfor the same song at different psychosocial states. Thus, it \ncan be claimed that during the evaluation process of a \ntempo estimation system the usage of a single groundtruth \nvalue is not always feasible. On the other hand, not all \nfractions and multiples can be considered as musically \ncorrect. \nOne solution was the P-score evaluation measure in-\ntroduced in MIREX 2005 Audio Tempo Extraction Task1 \nwhere each excerpt was annotated with two dominant \ntempi, and their relative strength. Algorithms should \nsuggest two tempi and the P-score is defined as the mean \nrelative strength of the correct estimated tempi within an \n8% tolerance. In this context, deciding the correct metri-\ncal level is less crucial. \nHowever, consider the following example. The 4th \ntraining instance on McKinley’s dataset excerpt, which \nexhibits a 6/8 measure and 126 bpm tempo, was anno-\ntated by 40 experts. 10 of them tapped at eight note level, \nwhile 30 tapped at dotted quarter notes. Thus tempo value \n42 bpm can be considered more salient than musical \ntempo 126 bpm. If these people were asked to character-\nize this excerpt as “slow” or “fast”, probably they would \njudge it as slow. Although the reliability of annotations \ncan always be questioned, we can conclude that there is a \nstrong relation of the notion of musical “speed” to the \nperceptual tempo (or metrical level). \nChoosing the correct metrical level usually relies on \nincorporating some prior knowledge, mostly in terms of \ncalculating prior tempi distribution [2]. Other methods \nadopt metrical models [1,3], inference from inter-onset \nintervals [6] or by considering the most predominant peak \nin periodicity vector as the correct tempo [7]. Seyerlehner \net al. [8] incorporate instance based learning techniques, \n                                                           \n1http://www.music-ir.org/mirex/wiki/2005:Audio_Tempo \n_Extraction  Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \n \nFigure 1. Overview of the proposed method. \n \nwhere the periodicity vector of the target music piece is \ncompared to periodicity functions of other annotated ex-\ncerpts. The assigned tempo is equal to tempo of the ex-\ncerpt with the most similar periodicity vector. In a similar \nmanner, Peeters adopts spectral templates and a learning \nschema for estimating tempo [9]. \nTwo recent approaches on characterizing the music \nspeed are remarkable. Eronen and Klapuri [10] presented \na tempo estimation system, where the predicted tempo is \nchosen by comparing scaled versions of the periodicity \nvector of the target excerpt with periodicity vectors of \ntempo annotated pieces. In the same paper, results were \nreported for a classification subsystem that classified mu-\nsic excerpts to three categories: slow, medium and fast. In \n[11] Hockman and Fujinaga proposed a system that clas-\nsifies music pieces to fast/slow. Annotations were not ex-\ntracted with the knowledge of any groundtruth tempo but \ndirectly from user tags on YouTube videos. Without any \nrhythmic analysis, but based solely on baseline frame-\nlevel features, their method achieved a classification ac-\ncuracy of 96% by adopting the AdaBoost classifier. In \n[12] Smith proposed a system for identifying octave er-\nrors made by a baseline beat tracker. \nIn this paper, we present a method of learning tempo \noctaves, i.e. classifying a music excerpt to one of the three \ncategories: slow, moderate and fast. The proposed method \nexhibits two key features. Firstly, a coded representation \nof periodicity vector similar to the popular MFCC fea-\ntures is proposed. Secondly, we adopt an SVM learner to \nlearn tempo octaves. SVM’s has been greatly used in \nclassification tasks in the MIR domain such as [13, 14]. \nWe applied the proposed octave learning method to a \nbaseline tempo estimation method [3] in order to limit the \ntarget tempi space and enhance tempo extraction accu-\nracy. Evaluation results indicate that the proposed tech-\nnique enhances greatly the tempo estimation accuracy. \nThe rest of the paper is organized as follows. In Sec-\ntion 2 an overview of the proposed method is described. \nSection 3 is dedicated to present the periodicity function \nextraction procedure. SVM learning formulation is de-\nscribed in Section 4, while in Section 5 the tempo estima-tion method is presented. Evaluation results and discus-\nsion on the proposed method conclude this paper in Sec-\ntions 6 and 7 respectively. \n2. SYSTEM OVERVIEW \nFigure 1 shows an overview of the proposed system. In \ntraining phase, periodicity analysis of the input signal is \nperformed. A set of vectors that is supposed to contain all \nrhythmic information of the signal is extracted. Next, the \nextracted periodicity vectors are rescaled in order to pro-\nduce more training instances. The periodicity vectors are \nthen coded to a more compact representation, and along \nwith the respective tempo class (slow, moderate, fast) \nwhich is inferred from the groundtruth tempo, are used to \ntrain the SVM model.  \nIn classification phase, the unknown input signal is \nprocessed by the periodicity analysis module. Periodicity \nvectors are coded as above and feed the SVM classifier. \nThe output class is then combined with the periodicity \nvectors of the input signal to find the tempo value that is \nconsistent to the metrical level of the SVM classifier. \n3. REPRESENTING RHYTHMIC CONTENT \n3.1 Periodicity Analysis \nPeriodicity analysis is performed by the adoption of the \nmethod presented in [3]. The constant Q transform is ap-\nplied to the signal, and followed by the har-\nmonic/percussive separation algorithm reported in [15]. \nTwo feature multidimensional sequences are extracted by \nthe harmonic/percussive parts of the signal respectively. \nEight band energies from the percussive part, denoted as \n, 1..8ix i=and chroma vectors from the harmonic part de-\nnoted as , 1..12jch j=. Feature sequences are convolved \nwith a bank of resonators with oscillation frequencies set \nto the tempo analysis range. Resonators’ outputs are seg-\nmented by square windows and the maximum values of \nresonators’ outputs are considered as the salient values of \neach feature sequence to each tempo value. We denote as   \n \n[ ]feature\nnp t the periodicity vectors for the input signal \nwhere { , 1..12, 1..8}j ifeature ch x j i∈ ∪ = = denotes the \nfeature type,  n denotes the time index and {30..500}t=  \ndenotes the \ntempo analysis range. Reader should note that \nthis range is larger than target tempi search space. This is \ndue to the fact that periodicity functions contain rhythmic \ninformation in frequency regions beyond the groundtruth \ntempo.  \n3.2 Scaling Training Vectors \nSince there is lack of large amount of annotated tempo \ndata, we could produce artificial data by rescaling a music \nsignal to faster and slower tempi. However, this approach \nwould be computational intensive. To overcome this \nproblem we exploit the following property of the perio-\ndicity vector, i.e., tempo-scaled versions of a signal, say \nby a value of α, produce inversely scaled versions of the \nperiodicity vector by the value of 1/ α. Thus, for a music \nsignal y[i], with periodicity function [ ]feature\nnp t  \n  [ ]   [ / ].periodicity analysis feature\nn y i p t α α  →  (1) \nThis property allows us to rescale directly the periodicity \nvectors, instead of the whole signal, reducing thus the \ncomplexity of the calculations.  \nIn the same manner as in [10], all periodicity vectors \nextracted from each music excerpt are rescaled within a \nrange of values for α around unity. If a music signal [ ]y i \nis assigned a ground-truth tempo groundT, then all α \nscaled versions of the periodicity vectors [ ]feature\nnp t that \nresult from [ ]y i are assigned a tempo value 1\ngroundTα−. \nUnder the ass\numption of almost constant tempo, perio-\ndicity functions are averaged for each feature across all \nsegments n in order to capture better the overall rhythmic \ncontent of the signal as follows \n \n11[ ] [ ]N\nfeature feature\nn\nnp t p tN==∑ɶ  (2) \n3.3 Periodi\ncity Coding \nTo satisfy the necessity to capture broad classes of tempo, \nit seems that it would be more efficient to use a more \ncompact representation for the periodicity vectors. We \nshall exploit the fact that periodicity vectors of similar \ntempo music pieces, will not exhibit the same peaks, but \nmay have a similar shape, or they will exhibit peaks in \nnearby tempi. \nSome recent works deal with the spectral modeling of \nrhythmic information. Holzapfel and Stylianou [16] ap-\nplied the scale transform to the autocorrelation function of \nmusic signals to form a rhythmic representation and ex-\nploit aspects of rhythmic similarity. Peeters [17] com-\nbines rhythm descriptors in a rhythm classification system \n \nFigure 2. Periodicity vector coding process. ⊗ stands \nfor inner product. \nwhile in [9] the DFT of the accent function is sub-\nsampled at frequency bins that correspond to tempo har-\nmonic series of certain meters. \nIn this paper, we introduce a filterbank-like analysis on \nthe periodicity vectors, which is illustrated in Figure 2. \nThe range of target tempi is divided into K equally tempo \nintervals with a 50% overlap between successive inter-\nvals. From each tempo interval, we utilize a symmetric \ntriangular weighting mask. \nThe strength [ ]features kof the periodicity vector \n[]featurep⋅ɶ for each of the K  tempo intervals is calculated \nas the inner product with the respective mask: \n max\nmin[ ] [ ] [ ]T\nfeature feature k\nt Ts k p t mask t\n== ⋅∑ɶ  (3) \nwhere []kmask⋅ denotes the mask of k tempo band.  \nHenceforth two problems arise from this modeling. \nFirstly, there is a strong correlation between features, \ncaused mainly by the overlap of adjacent tempo bands. \nSecondly, different feature type sequences for the same \npiece will result to different periodicity vectors. For ex-\nample energy evolution of lower spectral bands exhibit \nhigher values in lower tempi whereas higher spectral \nbands exhibit faster changes, and thus higher tempi. \nTherefore periodicity vectors calculated from different \nfeatures cannot be compared directly and cannot be \ntreated in the same manner. To suppress the effects of \nband correlation we apply the Discrete Cosine Transform \nto each tempo-band strength vector []features⋅, in order to \nobtain the uncorrelated coefficients []featurem⋅: \n [ ] DCT( [])feature featurem l s= ⋅ (4) \nTo cope with the different feature behaviour, the \nperiodicity representation is finally formed by appending \nall coefficients []featurem⋅ for each segment n to a single \n20K-dimensional vector m: \n \n 1 2 8 1 1 12[ | |...| | | |..| ]x x x ch ch ch=m m m m m m m (5) \n4. LEARNING TEM\nPO CLASSES \nLet {( , ), }l lt l L∈m  denote the vectors extracted from the \nmusic signal\ns using the method described in Section 3,   \n \nwhere tl are the annotated tempi. Depending on the value \nof tl we assign excerpts to one of the following classes:  \n 1,\n( ) 2,\n3,slow l\nl l slow l fast\nl fastT t\nc c t T t T\nt T≥= = < <\n≥ (6) \nThe thresholds slowTand fastTcan either be user specified or \ninferred by data and they divide the target tempi range \ninto the music speed classes of  slow, moderate and fast.  \nWe formulate two SVM problems for inferring the \ntempo classes; a classification SVM  where we learn each \nclass from the training data {( , ), }l lc l L∈m and a regres-\nsion SVM wher\ne we estimate a target tempo function \nfrom the training data {( , ), }l lt l L∈m Then excerpts are \nclassified one of the three classes by applying Eq. 6 on \nthe estimated tempo value.  \nThe conceptual difference between the two formula-\ntions is that while in classification we learn a function \nfrom feature space 20Kℝ to {slow, moderate, fast}, i.e. \ndiscretization takes place directly on the training data (Eq. \n6), in the case of regression discretization is applied to the \nregression estimate of the target tempo ˆit . \nFor the classification SVM the multiclass problem is \nsplit up to binary classification problems by applying the \n“one-vs-one” strategy. There is evidence [18] that the \none-vs-one strategy is more suitable than the more com-\nmon “one-vs-all” strategy, especially when there are \nimbalances between train classes, which is the case of the \nevaluation datasets (see Sec. 6). \nIn the case of the regression SVM the continuous \ntempo estimate ˆit cannot be directly interpreted as an ac-\ncurate tempo value, since in the signal representationim \nmuch of the rhythmic information such as the peaks in the \nperiodicity vectors are suppressed by the periodicity cod-\ning process. However, the value ˆit would give a rough \nestimate of the tempo that will be used in Eq. 6 to infer \nthe tempo class of the excerpt. \n5. ESTIMATING TEMPO \nTo extract the final tempo estimate from the periodicity \nfunction and the tempo class assigned by the SVM, we \ncalculate an overall periodicity function by the superposi-\ntion of the individual periodicity functions. In particular, \nthe periodicity vectors are summed across the two feature \ntypes and the resulting vectors are multiplied to give the \ndecision periodicity vector: \n 8 12\n1 1[ ] [ ] [ ]j ich x\ni jp t p t p t\n= =  =     ∑ ∑   (7) \nAccordingly \nto the estimated class, []p t is reduced to the \ncorresponding tempi range prescribed by Eq. 6. Final tempo estimate is decided as the most predominant peak \nin the reduced periodicity vector. \n6. EVALUATION \nThe proposed method was evaluated on the ISMIR 2004 \nTempo Induction Evaluation Exchange Dataset:  ballroom \nand songs datasets [4]. Periodicity vectors were rescaled \nin the range [0.8, 1.2] with a 0.02 step. We divided the \ntarget tempi to classes by setting 80slowT= bpm and \n130fastT=bpm in Eq. (6). Tempo bands number was set \nto K=20, tempo analysis region was set to [30..500] and \ntarget tempi space to [30..300]. Feature vector values \nwhere normalized to [-1, 1]. We adopted the LIBSVM \nimplementation of SVM [19]. We used an RBF kernel for \nthe SVM and parameter γ of the kernel was set to 1/20 K. \nFor regression, we adopted the ε-support vector regres-\nsion method. Experiments were run for various values of \nthe parameter C (Eqs. 1 and 9 in [19]). Variations of ε \n(Eq. 9 in [19]) did not affect significantly the overall per-\nformance, and was set to 0.1.  \nTo measure the generalization ability of the proposed \nmethod we adopted a three fold cross validation ap-\nproach. Each evaluation set was split randomly to three \nequal subsets. Each subset was used as a test set and the \nremaining two as the training set. Evaluation measures \nwere averaged on every train-test sets combination. \n6.1 Assignment to Tempo Classes \nThe first series of experiments involves the classification \naccuracy to tempo classes. Figure 3 (top) illustrates the \naccuracy for various values of the parameter C on ball-\nroom/songs datasets respectively, for both methods (clas-\nsification / regression). The accuracy is almost constant \nfor a wide range of C values, say for 10<C <500.  It must \nbe noted that SVM classification approach outperforms \nthe regression formulation.  It is clear that although the \ntempo discretization process from the assignment of the \nmusic excerpt to one of the three classes introduces ambi-\nguities for ground-truth tempi that are closer to either slowT \nor fastT, the classification approach is more efficient than \nthe continuous regression approach. This can be ex-\nplained by the fact that learning a continuous function on \na high dimensionality space is much more demanding \nthan separating instances into classes. In addition, the \nsmall number of training instances is probably not suffi-\ncient to learn such a function. However, there is no evi-\ndence that for larger scale experiments classification \nstrategy will be more effective than the regression formu-\nlation. \nTo get a better insight to classification errors Table 1 \npresents the confusion matrix between classes for the \nclassification approach ( C=100), for both datasets. In the \nballroom dataset classification fails for slow excerpts,    \n \n \nFigure 3. Top: Tempo class classification accuracy for \nboth datasets/classifiers. Bottom: Accuracy1 for both \ndatasets/classifiers. \n \n Ballroom Songs \n Slow Mod Fast Slow Mod Fast \nSlow 22 33 44 79 16 5 \nMod 2 86 12 17 82 1 \nFast 1 32 67 46 28 26 \nTable 1. Confusion matrix in tempo category classifica-\ntion percentages for both datasets. Rows correspond to \nground-truth and columns to estimates .  \nsince most of them are classified as fast. This is due to the \nfact that there are very few excerpts with slow tempi. \nThus, during training phase SVM fails to find reliable  \nboundaries for this class. The same effect takes place in \nthe case of fast excerpts in the songs dataset.  \nFigure 4 illustrates the tempo class error with respect \nto ground-truth tempo, along with dataset tempo distribu-\ntion for both datasets. As expected, there are more classi-\nfication errors near the tempo boundaries slowT and fastT \nwith respect to the total test instances with similar tempo. \nFinding the optimal values for slowT and fastT is dataset de-\npended and is out of the scope of this paper. slowT, fastT \nwere chosen arbitrarily based on authors intuition and not \non tempi distribution across data. For example, choosing \n110slowT=and 150fastT=for ballroom dataset would \ngive more separable classes (see Fig. 4). However, the \nerrors ought to the quantization of tempi values demon-\nstrate an inherent limitation of the proposed method. \nFigure 3 (bottom) illustrates the accuracy1 measure of \ntempo estimation for both classifiers (classification, re-\ngression) for various values of C. As expected by the re-\nsults of previous section, the classification approach per-\nforms significantly better than regression. Comparing fig-\nures in Fig. 3, we can see that tempo class and tempo val-\nues estimations are very similar for the ballroom dataset: \naccuracy1 is about 4 percent below tempo class accuracy. \nHowever, this is not the case for songs dataset, where ac.\ncuracy1 is significantly lower (>10%) than classification \naccuracy. To verify this, we estimated tempo for both \n \nFigure 4. Distribution of classification errors (dark bars) \nwith respect to ground-truth tempo compared to the over-\nall dataset tempo distribution (light bars). \n \n Ballroom Songs \nOur Method 75.93 63.87 \nBaseline 59.89 58.49 \nSE1 [6] 78.51 40.86 \nSE2 [6] 73.78 60.43 \nPeeters [9] 75.2 - \nPeeters [1] 65.2 49.5 \nKlapuri [2,4] 63.18 58.49 \nUhle [4] 56.45 41.94 \nScheirer [4] 51.86 37.85 \nTable 2. Accuracy1 of the proposed method compared to \nbest performing methods reported on ballroom/songs \ndatasets. \ndatasets by providing the correct tempo class. Accuracies \nreported are 88% and 76% for ballroom/ songs datasets \nrespectively. Thus, for the songs dataset, even with prior \nknowledge of the tempo class, periodicity analysis and \npeak-picking are not always adequate. \nTable 2 shows the performance of the proposed \nmethod compared to the baseline method adopted and the \nbest performing algorithms reported in the literature for \nboth datasets. It is evident that the proposed method out-\nperforms all other methods. It should be mentioned that \nalthough Seyerlehner’s SE1 [8] performs better in ball.\nroom dataset, results are not directly comparable because \nthey adopt a leave-one-out cross validation. Moreover \nSE1 reports very low accuracy for songs dataset. The sig-\nnificant performance increase of our method is somewhat \nexpected, since it incorporates prior knowledge of the \ndatasets. Although the cross-fold validation strategy splits \ndata to independent subsets, there is still some prior in-\nformation propagated to test sets caused by the uniformity \nof the datasets, i.e. most artists/styles are always present \nin both train/test sets. However the proposed method of-\nfers a promising approach to handle large datasets.   \n \n7. DISCUSSION AND FUTURE WORK \nWe presented a method for learning tempo classes with \nSupport Vector Machines. Tempo class classification ac-\ncuracies of 75% were achieved for both datasets, while \nmost errors were made for excerpts close to class bounda-\nries. The limitation of the target tempi decision space ac-\ncordingly to the tempo class found for a given excerpt, \nreduced octave errors made by a baseline tempo estima-\ntion system significantly. Estimation accuracies where in-\ncreased by a margin of 16% and 5% for ballroom/ songs \ndatasets respectively. \nIt must be noted that classification errors are propa-\ngated to the final tempo decision, especially for excerpts \nthat have tempo close to the tempo class decision bounda-\nries. A softer classification decision may be more sensi-\nble, as for example providing a confidence measure in-\nstead of a hard decision.  Moreover, a different treatment \nof the periodicity function such as analyzing metrical lev-\nels considering knowledge of music speed may be proved \nmore efficient. These two main aspects of the proposed \nmethod would be investigated in future research. \n8. REFERENCES \n[1] Peeters G., “Template-based estimation of time-\nvarying tempo,” in  EURASIP Journal on Applied \nSignal Processing, Volume 2007 Issue 1, 2007 . \n[2] Klapuri A., Eronen A. and Astola J., “Analysis of the \nMeter of Music Acoustic Signals,” IEEE Trans. on \nAudio, Speech and Language Processing , Vol. \n14(1), 2006. \n[3] Gkiokas A., Katsouros V., Carayannis G. and \nStafylakis T., “Music Tempo Estimation and Beat \nTracking by Applying Source Separation and \nMetrical Relations,” in Proc. of the 37th IEEE \nICASSP, Kyoto, Japan, March 25-30, 2012. \n[4] Gouyon F., Klapuri A., Dixon S., Alonso M., \nTzanetakis G., Uhle C., and Cano P., “An \nExperimental Comparison  of Audio Tempo \nInduction Algorithms,” in IEEE Transactions on \nAudio, Speech, and Language Processing, Vol. \n14(5) , September 2006. \n[5] Gouyon F. and Dixon S., “A Review of Automatic \nRhythm Description Systems,” Computer Music \nJournal, 29:1, pp 34.54, Spring 2005. \n[6] Dixon S., “Automatic Extraction of Tempo and Beat \nfrom Expressive Performances,” J. New Music \nResearch, 30(1):39–58, 2001. \n[7] Alonso M., Richard G. and David B., “Accurate \nTempo Estimation Based on Harmonic + Noise \nDecomposition,” EURASIP Journal on Applied Signal Processing,  Volume 2007, Issue 1, January \n2007 \n[8] Seyerlehner K., Widmer G., and Schnitzer D., \n“From Rhythm Patterns to Perceived Tempo,” in \nProc. of ISMIR,  Vienna, Austria, 2007. \n[9] Peeters G., “Template-Based Estimation of Tempo: \nUsing Unsupervised or Supervised Learning to \nCreate Better Spectral Templates,” in Proc. of \nDAFx.10, Graz, Austria, 2010. \n[10] Eronen A. and Klapuri A., “Music Tempo \nEstimation with k-NN Regression,” in IEEE \nTransactions on Audio, Speech, and Language \nProcessing, Vol. 18, No. 1, January 2010. \n[11] Hockman, J.A. and I. Fujinaga. “Fast vs slow: \nLearning tempo octaves from user data,” in Proc. of \nISMIR, Utrecht, Netherlands, 2010. \n[12] Smith L.M., “Beat-Critic: Beat-Tracking Octave \nError Identification by Metrical Profile Analysis,” in \nProc. of ISMIR , Utrecht, Netherlands, 2010. \n[13] Mandel M.I., Poliner G.E. and Ellis D.P.W., \n“Support vector machine active learning for music \nretrieval,” Multimedia systems , 12(1):1-11, August \n2006. \n[14] Wack N., Laurier C., Meyers O., Marxer R., \nBogdanov D., Serrà J., Gómez E. & Herrera P., \n“Music Classification Using High-Level Models,” in \nProc. of ISMIR,  Kobe, Japan, 2009. \n[15] FitzGerald D., “Harmonic/Percussive Separation \nUsing Median Filtering,” in Proc. of DAFx.10 , Graz, \nAustria, 2010. \n[16] Holzapfel A. and Stylianou Y., \"Scale transform in \nrhythmic similarity of music,\" in IEEE Trans. on \nAudio, Speech and Language Processing , Vol. \n19(1), 2011. \n[17] Peeters G., “Spectral and Temporal Periodicity \nRepresentations of Rhythm for the Automatic \nClassification of Music Audio Signal,” in IEEE  \nTrans. on Audio, Speech and Language Processing , \nVol. 19 (5), 2011 \n[18] Hsu C.W. and Lin C.J., “A comparison of methods \nfor multiclass support vector machines,” in IEEE \nTransactions on Neural Networks, Vol. 13(2) , \nMarch 2002 . \n[19] Chang C.-C. and Lin C.-J., “LIBSVM : a Library for \nSupport Vector Machines,” ACM Transactions on \nIntelligent Systems and Technology , 2:27:1--27:27, \n2011."
    },
    {
        "title": "Predominant Fundamental Frequency Estimation vs Singing Voice Separation for the Automatic Transcription of Accompanied Flamenco Singing.",
        "author": [
            "Emilia Gómez",
            "Francisco J. Cañadas-Quesada",
            "Justin Salamon",
            "Jordi Bonada",
            "Pedro Vera-Candeas",
            "Pablo Cabañas Molero"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416990",
        "url": "https://doi.org/10.5281/zenodo.1416990",
        "ee": "https://zenodo.org/records/1416990/files/GomezCSBCM12.pdf",
        "abstract": "This work evaluates two strategies for predominant funda- mental frequency (f0) estimation in the context of melodic transcription from flamenco singing with guitar accompa- niment. The first strategy extracts the f0 from salient pitch contours computed from the mixed spectrum; the second separates the voice from the guitar and then performs mono- phonic f0 estimation. We integrate both approaches with an automatic transcription system, which first estimates the tuning frequency and then implements an iterative strat- egy for note segmentation and labeling. We evaluate them on a flamenco music collection, including a wide range of singers and recording conditions. Both strategies achieve satisfying results. The separation-based approach yields a good overall accuracy (76.81%), although instrumental segments have to be manually located. The predominant f0 estimator yields slightly higher accuracy (79.72%) but does not require any manual annotation. Furthermore, its accuracy increases (84.68%) if we adapt some algorithm parameters to each analyzed excerpt. Most transcription errors are due to incorrect f0 estimations (typically octave and voicing errors in strong presence of guitar) and in- correct note segmentation in highly ornamented sections. Our study confirms the difficulty of transcribing flamenco singing and the need for repertoire-specific and assisted al- gorithms for improving state-of-the-art methods.",
        "zenodo_id": 1416990,
        "dblp_key": "conf/ismir/GomezCSBCM12",
        "keywords": [
            "fundamental frequency estimation",
            "melodic transcription",
            "flamenco singing",
            "guitar accompaniment",
            "pitch contours",
            "voice separation",
            "monophonic estimation",
            "automatic transcription system",
            "tuning frequency estimation",
            "note segmentation"
        ],
        "content": "PREDOMINANT FUNDAMENTAL FREQUENCY ESTIMATION VS\nSINGING VOICE SEPARATION FOR THE AUTOMATIC\nTRANSCRIPTION OF ACCOMPANIED FLAMENCO SINGING\nE. G ´omez1, F. Ca ˜nadas2, J. Salamon1, J. Bonada1, P. Vera2and P. Caba ˜nas2\n1Music Technology Group, Universitat Pompeu Fabra, Spain\n2Telecommunication Engineering Department, University of Jaen, Spain\nemilia.gomez@upf.edu, fcanadas@ujaen.es, justin.salamon@upf.edu, jordi.bonada@upf.edu, pvera@ujaen.es, pcabanas@ujaen.es\nABSTRACT\nThis work evaluates two strategies for predominant funda-\nmental frequency (f 0) estimation in the context of melodic\ntranscription from ﬂamenco singing with guitar accompa-\nniment. The ﬁrst strategy extracts the f0from salient pitch\ncontours computed from the mixed spectrum; the second\nseparates the voice from the guitar and then performs mono-\nphonicf0estimation. We integrate both approaches with\nan automatic transcription system, which ﬁrst estimates the\ntuning frequency and then implements an iterative strat-\negy for note segmentation and labeling. We evaluate them\non a ﬂamenco music collection, including a wide range of\nsingers and recording conditions. Both strategies achieve\nsatisfying results. The separation-based approach yields\na good overall accuracy (76.81%), although instrumental\nsegments have to be manually located. The predominant\nf0estimator yields slightly higher accuracy (79.72%) but\ndoes not require any manual annotation. Furthermore, its\naccuracy increases (84.68%) if we adapt some algorithm\nparameters to each analyzed excerpt. Most transcription\nerrors are due to incorrect f0estimations (typically octave\nand voicing errors in strong presence of guitar) and in-\ncorrect note segmentation in highly ornamented sections.\nOur study conﬁrms the difﬁculty of transcribing ﬂamenco\nsinging and the need for repertoire-speciﬁc and assisted al-\ngorithms for improving state-of-the-art methods.\n1. INTRODUCTION\nFlamenco is a music tradition originating mostly from An-\ndalusia in southern Spain. The singer has a main role and\nis often accompanied by the guitar and other instruments\nsuch as claps, rhythmic feet and percussion. This research\naims to develop a method for computing detailed note trans-\ncriptions of ﬂamenco singing from music recordings, which\ncan then be processed for motive analysis or further sim-\npliﬁed to obtain an overall melodic contour that will char-\nacterize the style. In this study we focus on accompanied\nsinging, and propose a method comprised of two stages:\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.predominant f0estimation and note segmentation. For the\nﬁrst stage, two alternative strategies are evaluated and com-\npared: in the ﬁrst, we use a state-of-the-art predominant f0\nestimation algorithm, which estimates the f0of the pre-\ndominant melody directly from the full audio mix. In the\nsecond, we propose a source separation approach to isolate\nthe singing voice and perform monophonic f0estimation.\n2. SCIENTIFIC BACKGROUND\nAutomatic transcription is a key challenge in the music in-\nformation retrieval (MIR) ﬁeld. It consists of computing\na symbolic musical representation from an audio record-\ning. In polyphonic music material, there is an interest in\ntranscribing the predominant melodic line [1]. Although\nwe ﬁnd some successful approaches for singing transcrip-\ntion [2,3], the singing voice is still one of the most complex\ninstruments to transcribe, given the continuous character of\nthe human voice and the variety of pitch ranges and timbre.\nAdditional challenges in ﬂamenco arise from the quality\nof recordings, the acoustic and expressive particularities of\nﬂamenco singing, its ornamental and improvisational char-\nacter and the yet to be formalized musical structures [4].\nIn [5], we proposed a melodic transcription system from\na cappella ﬂamenco singing and we evaluated it against\nmanual annotations of 72 performances. The obtained over-\nall accuracy was around 70% (50 cents tolerance), which\nwas signiﬁcantly lower than the one obtained for a small\ntest collection of pop/jazz excerpts (\u001885%). The study\nshowed the importance of good monophonic f0estimation,\nand conﬁrmed the difﬁculty of note segmentation for ex-\ncerpts with unstable tuning or highly ornamented sections.\nThe goals of the present study are to apply this tran-\nscription system to accompanied singing and to perform\na comparative evaluation of two alternative strategies for\nsinging voice f0estimation. The ﬁrst is to replace the\nmonophonic f0detector by a predominant f0estimation\nmethod. The task of predominant f0estimation from poly-\nphonic music (sometimes referred to simply as melody ex-\ntraction) has received much attention from the research\ncommunity in recent years, and state-of-the-art approaches\nyield an overall accuracy around 75% [6, 7]. A variety of\ndifferent approaches have been proposed, based on track-\ning agents [8], classiﬁcation [9], streaming rules [10] or\npitch contour characterization [11]. The most common\nset of approaches are “salience based”, i.e. they computea pitch salience representation from the audio signal, and\nthen select the melody out of the peaks of this representa-\ntion over time [8, 9, 11].\nThe second strategy is to separate the singing voice from\nthe guitar accompaniment using source separation and tran-\nscribe the separated track. Recent singing voice separation\nmethods can be classiﬁed into three categories: spectro-\ngram factorization [12–14], pitch-based inference [15, 16]\nand repeating-structure removal [17]. Spectrogram factor-\nization methods decompose a magnitude spectrogram as a\nset of components that represent features such as the spec-\ntral patterns (basis) or the activations (gains) of the active\nsources along the time. Fitzgerald and Gainza [12] propose\na non-negative partial cofactorisation sharing a common\nset of frequency basis functions. In [13], an accompani-\nment model is designed, from the non-vocal segments, to\nﬁt the musical instruments and attempt separation of the\nvocals. In [14], the basis of the vocal track is learned\nfrom the mixture by keeping the accompaniment spectra\nﬁxed. Pitch-based inference methods use information from\nthe pitch contour to determine the harmonic structures of\nsinging voice. In [16], separation of both the voiced and\nthe unvoiced singing voice is presented by means of the\ncombination of detected unvoiced sounds and a spectral\nsubtraction method to enhance voiced singing separation\n[15]. Repeating-structure removal methods [17] use a pat-\ntern recognition approach to identify and extract accompa-\nniment segments, without manual labeling, which can be\nclassiﬁed as repeating musical structures.\n3. TRANSCRIPTION METHOD\nOur method relies on two main stages: low-level feature\nextraction (mainly f0) and note segmentation. We present\nthe two alternatives for f0estimation compared in this study\nfollowed by a summary of the note segmentation approach.\n3.1 Singing voice f0estimation\n3.1.1 Predominant f0estimation\nFor predominant f0estimation, we use [11], which ob-\ntained the highest overall accuracy in MIREX 2011 [6].\nFirst, the audio signal is analyzed and spectral peaks (si-\nnusoids) are extracted. This process is comprised of three\nmain steps: ﬁrst a time-domain equal loudness ﬁlter is ap-\nplied, which has been shown to attenuate spectral com-\nponents belonging primarily to non-melody sources [19].\nNext, the short-time Fourier transform is computed with a\n46 ms Hann window, a hop size of 2.9 ms and a 4 zero\npadding factor. At each frame the local maxima (peaks) of\nthe spectrum are detected. In the third step, the estimation\nof the spectral peaks’ frequency and amplitude is reﬁned\nby calculating each peak’s instantaneous frequency (IF) us-\ning the phase vocoder method and re-estimating its ampli-\ntude based on the IF. The detected spectral peaks are subse-\nquently used to compute a representation of pitch salience\nover time: a salience function. The salience function is\nbased on harmonic summation with magnitude weighting,\nand spans a 5-octave range from 55Hz to 1760Hz. De-\ntails are provided in [11]. In the next stage, the peaks ofthe salience function are grouped over time using heuris-\ntics based on auditory streaming cues. This results in a\nset of pitch contours, out of which the contours belonging\nto the melody need to be selected. The contours are au-\ntomatically analyzed and a set of contour characteristics\nis computed. In the ﬁnal stage of the system, the con-\ntour characteristics and their distributions are used to ﬁl-\nter out non-melody contours. The distribution of contour\nsalience is used to ﬁlter out pitch contours at segments of\nthe song where the melody is not present. Next, we obtain\na rough estimate of the melodic pitch trajectory by comput-\ning a per-frame salience-weighted average of the remain-\ning pitch contours and smoothing it over time using a slid-\ning mean ﬁlter. This rough pitch trajectory is used to min-\nimise octave errors (contours with the correct pitch class\nbut in the wrong octave) and remove pitch outliers (con-\ntours representing highly unlikely jumps in the melody).\nFinally, the melody f0at each frame is selected out of the\nremaining pitch contours based on their salience. For fur-\nther details the reader is referred to [11].\nIn addition to computing the melody f0sequence using\nthe default algorithm parameters (denoted MTG), we also\ncomputed the melody adjusting three parameters of the al-\ngorithm for each musical excerpt: the minimum and maxi-\nmum frequency threshold and the strictness of the voicing\nﬁlter (cf. [11] for details). The results using the per-excerpt\nadjusted parameters are referred to as MTGAdaptedparam.\n3.1.2 Singing voice separation and monophonic f0\nestimation\nStandard Non-negative Matrix Factorization (NMF) [20] is\nnot able to determine if a frequency basis belongs to a per-\ncussive, harmonic or vocal sound. Our proposal attempts\nto overcome this limitation without using any clustering\nprocess. A mixture spectrogram Xis factorized into three\nseparated spectrograms, Xp(percussive), Xh(harmonic)\nandXv(vocal). Using similar spectro-temporal features\n[21, 22], harmonic sounds are modeled by sparseness in\nfrequency and smoothness in time. Percussive sounds are\nmodeled by smoothness in frequency and sparseness in\ntime. V ocal sounds are modeled by sparseness in frequency\nand sparseness in time. Although it is not necessary to dis-\ncriminate between percussive and harmonic sounds in the\naccompaniment, our experimental results showed we ob-\ntain better vocal separation using this discrimination. The\nproposed singing voice separation is composed of three\nstages: segmentation, training and separation.\nIn the segmentation stage, the mixture signal\nX=XnonvocalSXvocal is manually labelled into vo-\ncalXvocal (vocal+instruments) and non-vocal Xnonvocal\n(only instruments) regions. In the training stage, from non-\nvocal regions, the percussive Wpand harmonic Whba-\nsis vectors are learned using an unsupervised NMF per-\ncussive/harmonic separation approach based on spectro-\ntemporal features.\nXnonvocal\u0019Xp+Xh=Wp\u0001Hp+Wh\u0001Hh (1)\nIn the separation stage, the vocal spectrogram Xvis ex-\ntracted from the vocal regions by keeping the percussiveWpand harmonic Whbasis vectors ﬁxed from the previ-\nous stage.\nXvocal\u0019X0\np+X0\nh+Xv=Wp\u0001H0\np+Wh\u0001H0\nh+Wv\u0001Hv\n(2)\nIn this manner, the singing voice signal v(t)is synthesized\nfrom the vocal spectrogram Xv. To obtain an f0sequence\nfrom the synthesized voice signal, the traditional difference\nfunction is computed for each time frame index t:\nd(\u001c;t) =W\u00001X\nn=0(v(t+n)\u0000v(t+n+\u001c))2(3)\nwhereWis the length of the summation window and \u001cis\nthe candidate pitch period. From this function, the cumula-\ntive mean normalized difference function can be computed\nas deﬁned in [23]:\ndn(\u001c;t) =\u001a1; \u001c= 0\nd(\u001c;t)=[1\n\u001cP\u001c\nj=1d(j;t)] otherwise.\n(4)\nObserve that the function dn(\u001c;t) can be viewed as a cost\nmatrix, where each element (\u001c;t) indicates the cost of hav-\ning a pitch period equal to \u001cat time frame t. We esti-\nmate the whole f0sequence by computing the lowest-cost\npath through the matrix dn(\u001c;t). This computation is ac-\ncomplished with dynamic programming. The endpoints of\nthe path are ﬁxed only for the t-axis and the path is con-\nstrained to advance step-by-step along t, under the condi-\ntionj\u001ct\u00001\u0000\u001ctj\u00141. This condition ensures a continuous\nand smoothf0contour. The obtained f0is denoted as UJA.\n3.2 Note segmentation and labeling\nOur approach for note segmentation and labeling is adapted\nfrom a transcription system for mainstream popular mu-\nsic [18]. After consulting a group of ﬂamenco experts\nfrom the COFLA project1, we took the following design\ndecisions. First, we deﬁne an equal-tempered scale with\nrespect to an estimated tuning frequency. Second, we as-\nsume a constant tuning frequency value for each analyzed\nexcerpt. Third, we transcribe all perceptible notes, includ-\ning short ornamentations, in order to cover both expressive\nnuances and the overall melodic contour. We summarize\nbelow the mains steps of the transcription algorithm and\nwe refer to [5] and [18] for further details.\n3.2.1 Tuning frequency estimation\nFrom the obtained f0envelope, we perform an estimation\nof the tuning frequency used by the singer assuming an\nequal-tempered scale. The tuning frequency is assumed to\nbe constant for a given excerpt. We compute the maximum\nof the histogram of f0deviations from an equal-tempered\nscale tuned to 440 Hz. We then map the f0values of all\nframes into a single semitone interval with a one-cent res-\nolution.\nIn our approach, we give more weight to frames where\nthe included f0is stable by assigning higher weights to\n1http://mtg.upf.edu/research/projects/coﬂa\n0 frame index note pitch index node k, j \n0 k j \nk-dmin k-dmax Figure 1. Matrix Mused by the short note segmenta-\ntion process, illustrating how the best path for a node with\nframe index kand note index jis determined. All possible\nnote durations between dminanddmax are considered, as\nwell as all possible jumps to previous notes. The selected\nsegmentation is marked with dark gray.\nframes with low f0derivative. In order to smooth the re-\nsulting histogram and improve its robustness to noisy f0\nestimations, instead of adding a value to a single bin, we\nuse a bell-shaped window that spans several bins. The\nmaximum of this histogram (b max) determines the tuning\nfrequency deviation in cents from 440 Hz. The estimated\ntuning frequency in Hz then becomes\nfref= 440\u00012bmax\n1200 (5)\n3.2.2 Short note transcription\nThe short note transcription step segments a single f0con-\ntour into notes. Using dynamic programming (DP), we\nﬁnd the note segmentation that maximizes a set of prob-\nability functions. The estimated segmentation corresponds\nto the optimal path among all possible paths along a 2-D\nmatrixM(see Figure 1).\nThis matrix Mhas note pitches as rows and analysis\nframes as columns. Note pitches are quantized into semi-\ntones according to the estimated tuning frequency. Possi-\nble note pitches should cover the tessitura of the singer and\ninclude a\u00001 value for the unvoiced sections. Note dura-\ntions are limited to a certain range [dmin;dmax]of frames.\nThe maximum duration dmax should be long enough so\nthat it covers several periods of a vibrato with a low modu-\nlation frequency, e.g. 2:5Hz , but also short enough to have\ngood temporal resolution, e.g. avoid skipping short orna-\nmentations.\nPossible paths considered by the DP algorithm always\nstart from the ﬁrst frame, end at the last audio frame, and\nadvance in time so that notes never overlap. A path pis\ndeﬁned by its sequence of Npnotes,\np=fnp0;np1;:::;npNp\u00001g, where each note npibegins\nat a certain frame kpi, has a duration of dpiframes and a\npitch value of cpi. The optimal path is deﬁned as the path\nwith maximum likelihood among all possible paths.P= arg max\npfL(p)g (6)\nThe likelihood L(p) of a certain path pis determined\nas the product of likelihoods of each note L(npi)times the\nlikelihood of each jump between consecutive notes\nL(npi\u00001;npi):\nL(p) =L(np0)\u0001Np\u00001Y\ni=1L(npi)\u0001L(npi\u00001;npi)(7)\nIn our approach, no particular characteristic is assumed\na priori for the sung melody; therefore all possible note\njumps have the same likelihood L(npi\u00001;npi) = 1. On the\nother hand, the likelihood of a note L(npi)is determined\nas the product of several likelihood functions based on the\nfollowing criteria: duration (L d), pitch (L c), existence of\nvoiced and unvoiced frames (L v), and low-level features\nrelated to stability (L s):\nL(npi) =Ld(npi)\u0001Lc(npi)\u0001Lv(npi)\u0001Ls(npi)(8)\nDuration likelihood Ldis set so that it is small for short\nand long durations. Pitch likelihood Lcis deﬁned so that it\nis higher the closer the frame f0values are to the note nom-\ninal pitchcpi, giving more relevance to frames with low f0\nderivative values. The voicing likelihood Lvis deﬁned so\nthat segments with a high percentage of unvoiced frames\nare unlikely to be a voiced note, while segments with a\nhigh percentage of voiced frames are unlikely to be an un-\nvoiced note. Finally, the stability likelihood Lsconsiders\nthat a voiced note is unlikely to have fast and signiﬁcant\ntimbre or energy changes in the middle. Note that this is\nnot in contradiction with smooth vowel changes, charac-\nteristic of ﬂamenco singing.\n3.2.3 Iterative note consolidation and tuning frequency\nreﬁnement\nThe notes obtained in the previous step have a limited du-\nration between [dmin;dmax]frames, although longer notes\nare likely to have been sung. Therefore, it makes sense to\nconsolidate consecutive voiced notes into longer notes if\nthey have the same pitch. However, signiﬁcant and fast en-\nergy or timbre changes around the note connection bound-\nary may be indicative of phonetic changes unlikely to hap-\npen within a note, and thus may indicate that those con-\nsecutive notes are different ones. Thus, consecutive notes\nwill be consolidated only if they have the same pitch and\nthe stability measure of their connection Lsfalls below a\ncertain threshold.\nOnce notes are consolidated, it may be beneﬁcial to use\nthe note segmentation to reﬁne the tuning frequency esti-\nmation. For this purpose, we compute a pitch deviation\nfor each voiced note, and then estimate a new tuning fre-\nquency value from a one-semitone histogram of weighted\nnote pitch deviations in similar way to that described in\nSection 3.2.1. The difference is that now we add a value for\nFigure 2. Visualization tool for melodic transcription. Au-\ndio waveform (top), estimated f0and pitch in a piano roll\nrepresentation (bottom).\neach voiced note instead of for each voiced frame. Weights\nare determined as a measure of the salience of each note,\ngiving more weight to longer and louder notes. As a ﬁnal\nstep of this process, note nominal pitches are re-computed\nbased on the new tuning frequency. This process is re-\npeated until there are no more consolidations.\nFigure 2 shows an example of a computed transcrip-\ntion. The system outputs both the extracted f0envelope\nand the estimated frame note pitch, according to an equal-\ntempered scale, as requested by ﬂamenco experts for higher-\nlevel analyses.\n4. EV ALUATION STRATEGY\n4.1 Music collection\nWe gathered 26.74 minutes of music, consisting of 30 per-\nformances of singing voice with guitar accompaniment (Fan-\ndango style). This collection has been built in the context\nof the COFLA project. It contains a variety of male and\nfemale singers and recording conditions. The average du-\nration of the analyzed excerpts is 53.48 seconds and they\ncontain a total of 271482 frames and 2392 notes.\n4.2 Ground truth gathering\nWe collected manual note annotations from a musician with\nlimited knowledge of ﬂamenco music, so that there was\nno implicit knowledge applied in the transcription process.\nWe provided him with the user interface shown in Figure\n2. Since transcribing everything from scratch is very time\nconsuming, we also provided the output of our transcrip-\ntion using the MTGAdaptedParam estimation as a guide.\nThe annotator could listen to the original waveform and the\nsynthesized transcription, while editing the melodic data\nuntil he was satisﬁed with the transcription. The criteria\nused to differentiate ornaments and pitch glides were dis-\ncussed with two ﬂamenco experts by collectively annotat-\ning a set of working examples, so that the annotator then\nfollowed a well-deﬁned and consistent strategy.Figure 3. Frame-based accuracy measures (50 cents toler-\nance) for the considered approaches.\n4.3 Evaluation measures\nFor evaluation we compute the measures used in the Audio\nMelody Extraction (AME) MIREX task [6]. The measures\nare based on a frame-by-frame comparison of the ground-\ntruth to the estimated frequency sequence. Note that in our\ncase we compare the ground truth to the frequency of the\nﬁnal note transcription, meaning any observed errors rep-\nresent the combined errors introduced by the two stages of\nour method (f 0estimation and note segmentation). Also,\nsince we do not provide a pitch estimate for frames de-\ntermined as unvoiced, incorrect voicing detection will also\ninﬂuence pitch accuracy (but not overall accuracy). We\nconsider Voicing recall: % of voiced frames in the refer-\nence correctly estimated as voiced; Voicing false alarm: %\nof unvoiced frames in the reference mistakenly estimated\nas voiced; Raw pitch accuracy: % of voiced frames where\nthe pitch estimate is correct within a certain threshold in\ncents (th); Raw chroma accuracy: same as the raw pitch\naccuracy except that octave errors are ignored; and Over-\nall accuracy: total % of correctly estimated frames: correct\npitch for voiced frames and correct labeling of unvoiced\nframes.\n5. RESULTS\n5.1 Frame-based pitch accuracy\nFigure 3 shows the obtained accuracy measures for th=\n50cents. At ﬁrst glance, we see that satisfying results\nare obtained for both strategies. The separation-based ap-\nproach (UJA) yields good results (overall accuracy 76.81%,\npitch accuracy 63.62%), as the guitar timbre can be accu-\nrately estimated from the instrumental segments. Never-\ntheless, these guitar segments have to be manually located.\nThe predominant f0estimator (MTG) yields slightly higher\noverall accuracy (79.72%) and pitch accuracy (71.46%),\nand it does not require manual voicing annotation. More-\nover, the overall accuracy increases to 84.68% (pitch accu-\nracy 77.92%) if we adapt some algorithm parameters for\neach excerpt (MTGAdaptedParam). The observed voicing\nfalse alarm rate (around 10% for MTG and UJA) resultsVx Vx False Raw Raw Overall\nEst. Ref. Recall Alarm pitch chroma accuracy\nMTG UJA 89.24 6.35 74.20 74.82 82.67\nUJA MTG 94.00 12.95 78.29 78.93 82.65\nTable 1. Accuracy measures between f0estimations.\nfrom segments where the guitar is detected as melody.\nThe obtained results are slightly higher than the ones\nobtained for a cappella singing [5] when considering the\nsame note segmentation algorithm together with a mono-\nphonicf0estimator. This is due to two main reasons. Pri-\nmarily, as the singer follows the tuning reference of the\nguitar, there are no tuning errors and the note labeling re-\nsults are improved. Also, as the voice is very predominant\nwith respect to the guitar, the predominant f0estimation\nmethod works very well for this material.\n5.2 Agreement between f0estimations\nWe also estimate the agreement between both f0strategies\nby computing the evaluation measures with one estimator\nas ground truth and the other one as estimation. Results\nare presented in Table 1. We observe that in both cases\nthe overall agreement is around 82.5%. The main differ-\nence between the approaches is in the determination of\nvoiced sections. Whilst in UJA only large non-voiced sec-\ntions were manually annotated, MTGAdaptedParam also\nattempts to automatically detect shorter unvoiced sections\nin the middle of the piece.\n5.3 Error analysis\nWe observe that for the two considered strategies, tran-\nscription errors are introduced in both stages of the tran-\nscription process (f 0estimation and note segmentation).\nRegarding singing voice f0estimation, voicing seems\nto be the main aspect to improve. V oicing false positives\noccasionally appear during melodic guitar segments and\nin short unvoiced phonemes (e.g. fricatives). On the other\nhand, the singing voice f0is sometimes missed in the pres-\nence of strong instrumental accompaniment, resulting in\nvoicing false negatives. Since the subsequent note segmen-\ntation stage relies on the voicing estimation, voicing errors\nduring thef0estimation are bound to introduce errors in\nthe note segmentation stage as well. Another type of error\nis ﬁfth or octave errors at segments with highly predomi-\nnant accompaniment. This occurs especially with the UJA\nmethod, as low harmonics of the singing voice might be\nerased from the spectrum during the separation process.\nRegarding the note segmentation algorithm, most of the\nerrors happen for short notes; either they are consolidated\nwhile the annotation consists of several close notes, or vice\nversa. This especially happens where the energy envelope\nalso accounts for the presence of guitar, so that onset esti-\nmation becomes more difﬁcult. Finally, some of the errors\noccur due to wrong pitch labeling of very short notes, as\nthef0contour is short and unstable. This demonstrates\nthe difﬁculty of obtaining accurate note transcriptions for\nﬂamenco singing, given its ornamental character and thecontinuous variations of f0, easily confused with deep vi-\nbrato or pitch glides. The great variability of the vocal f0\ncontour can be observed in Figure 2.\n6. CONCLUSIONS\nThis paper presents an approach for computer-assisted tran-\nscription of accompanied ﬂamenco singing. It is based\non an iterative note segmentation and labelling technique\nfromf0, energy and timbre. Two different strategies for\nsinging voice f0estimation were evaluated on 30 minutes\nof ﬂamenco music, obtaining promising results which are\ncomparable to (and even better than) previous results for\nmonophonic singing transcription. The main sources of\ntranscription errors were identiﬁed: in the ﬁrst stage (f 0\nestimation) the main issue is voicing detection (e.g. identi-\nﬁcation of the guitar as voice), though we occasionally ob-\nserve pitch errors (e.g. wrong f0in the presence of guitar)\nas well. In the second stage (note segmentation) we ob-\nserved errors in segmenting short notes and labeling notes\nwith an unstable f0contour. There is still much room\nfor improvement. One limitation of this work is the small\namount of manual annotations. This is due to the fact that\nmanual annotation is very time consuming and difﬁcult to\nobtain, and has a degree of subjectivity. We are currently\nexpanding the amount of manual annotations. The second\nlimitation is that we only have manual annotations on a\nnote level (quantized to 12 semitones) and not the contin-\nuousf0ground truth, which would allow us to evaluate\nseparately the accuracy of the two main stages of the algo-\nrithm. We plan to work on this issue. Finally, we plan to\nquantify the uncertainty of the ground truth information by\ncomparing annotations in different contexts, and adapt the\nalgorithm parameters accordingly.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank the COFLA1team for pro-\nviding the data set and expert knowledge in ﬂamenco mu-\nsic. This work has been partially funded by AGAUR (mo-\nbility grant), the COFLA project (P09-TIC-4840 Proyecto\nde Excelencia, Junta de Andaluc ´ıa) and the Programa de\nFormaci ´on del Profesorado Universitario of the Ministerio\nde Educaci ´on de Espa ˜na.\n8. REFERENCES\n[1] A. Klapuri, and M. Davy (Eds): “Signal Processing Methods\nfor Music Transcription,” Springer-Verlag, 2006.\n[2] T. Mulder, J. P. Martens, M. Lesaffre, M. Leman, B. De\nBaets, and H. De Meyer: “An Auditory Model Based Tran-\nscriber of V ocal Queries,” Proc. of ISMIR, 2003.\n[3] M. P. Ryyn ¨anen: “Singing transcription,” in Signal pro-\ncessing methods for music transcription (A. Klapuri and M.\nDavy, eds.), Springer, 2006.\n[4] J. Mora, F. Gomez, E. G ´omez, F. Escobar-Borrego, and J.M.\nDiaz-Ba ˜nez: “Characterization and melodic similarity of a\nCappella ﬂamenco cantes,” Proc. of ISMIR 2010.\n[5] E. G ´omez, J. Bonada, and J. Salamon: “Automatic Transcrip-\ntion of Flamenco Singing from Monophonic and Polyphonic\nMusic Recordings,” Proc. of FMA, 2012.[6] J. Salamon, E. G ´omez: “Melody Extraction from Polyphonic\nMusic: MIREX 2011,” in Music Information Retrieval Eval-\nuation eXchange (MIREX), 2011.\n[7] G. E. Poliner, D. P. W. Ellis, F. Ehmann, E. G ´omez, S. Stre-\nich, and B. Ong: “Melody transcription from music audio:\nApproaches and evaluation,” IEEE Transactions on Audio,\nSpeech and Language Processing, 15(4):1247:1256, 2007.\n[8] M. Goto: “A real-time music-scene-description system:\npredominant-f0 estimation for detecting melody and bass\nlines in real-world audio signals,” Speech Communication,\n43:311:329, 2004.\n[9] G. E. Poliner and D.P.W. Ellis: “A Classiﬁcation Approach\nto Melody Transcription,” Proc. of ISMIR, 2005.\n[10] K. Dressler: “An auditory streaming approach for melody ex-\ntraction from polyphonic music,” Proc. Of ISMIR, pp. 19:24,\nMiami, 2011.\n[11] J. Salamon, and E. G ´omez: “Melody Extraction from Poly-\nphonic Music Signals using Pitch Contours Characteristics,”\nIEEE Transactions on Audio, Speech and Language Process-\ning, 20(6):1759-1770, August 2012.\n[12] D. FitzGerald, and M. Gainza: “Single Channel V ocal Sepa-\nration using Median Filtering and Factorisation Techniques,”\nISAST Transactions on Electronic and Signal Processing,\n4(1):62-73, 2010 (ISSN 1797-2329)\n[13] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval: “Adap-\ntation of Bayesian models for single-channel source separa-\ntion and its application to voice/music separation in popular\nsongs,” IEEE Transactions on Audio, Speech, and Language\nProcessing, 15(5): 1564:1578, July 2007.\n[14] B. Raj, P. Smaragdis, M. V . Shashanka, and R. Singh: “Sepa-\nrating a foreground singer from background music,” Proc. Int\nSymp. Frontiers Res. Speech Music (FRSM), India, 2007.\n[15] Y . Li, and D. Wang: “Separation of singing voice from music\naccompaniment for monaural recordings,” Proc. of ICASSP,\n15(4):1475:1487, May, 2007.\n[16] H. Chao-Ling, and R. Jyh-Shing: “On the improvement of\nsinging voice separation for monaural recordings using the\nMIR-1K dataset,” IEEE Transactions on Audio, Speech, and\nLanguage Processing, 18(2):310:319, February 2010.\n[17] Z. Raﬁi, and B. Pardo: “A Simple Music/V oice Separation\nMethod based on the Extraction of the Repeating Musical\nStructure,” Proc. of ICASSP, Prague, May, 2011.\n[18] J. Janer, J. Bonada, M. de Boer, and A. Loscos: “Au-\ndio Recording Analysis and Rating,” Patent pending\nUS20080026977, Universitat Pompeu Fabra, 06/02/2008.\n[19] J. Salamon, E. G ´omez, and J. Bonada: “Sinusoid Extraction\nand Salience Function Design for Predominant Melody Esti-\nmation,” Proc. of DAFX, Paris, 2011, pp. 73-80.\n[20] D. Lee, and H. Seung: “Algorithms for Non-negative Matrix\nFactorization,” Advances in NIPS, pp. 556-562, 2000\n[21] N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka, and S.\nSagayama: “Separation of a monaural audio signal into har-\nmonic/percussive components by complementary diffusion\non spectrogram,” Proc. of EUSIPCO, 2008\n[22] T. Virtanen: “Monaural Sound Source Separation by Non-\nNegative Matrix Factorization with Temporal Continuity and\nSparseness Criteria,” IEEE Transactions on Audio, Speech,\nand Language Processing, 3(15), March 2007.\n[23] A. de Cheveign ´e, and H. Kawahara: “YIN, a Fundamental\nFrequency Estimator for Speech and Music,” Journal of the\nAcoustic Society of America, 111(4):1971:1930, 2002."
    },
    {
        "title": "Structure-Based Audio Fingerprinting for Music Retrieval.",
        "author": [
            "Peter Grosche",
            "Joan Serrà",
            "Meinard Müller",
            "Josep Lluís Arcos"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416170",
        "url": "https://doi.org/10.5281/zenodo.1416170",
        "ee": "https://zenodo.org/records/1416170/files/GroscheSMA12.pdf",
        "abstract": "Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually gen- erated annotations. In this paper, we introduce the con- cept of structure fingerprints, which are compact descrip- tors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facil- itate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facili- tate robust and efficient content-based music retrieval. Fur- thermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset.",
        "zenodo_id": 1416170,
        "dblp_key": "conf/ismir/GroscheSMA12",
        "keywords": [
            "content-based approaches",
            "music retrieval",
            "structure fingerprints",
            "musical structure",
            "compact descriptors",
            "audio recordings",
            "self-similarity matrix",
            "robust and efficient",
            "experiments",
            "Chopin Mazurkas"
        ],
        "content": "STRUCTURE-BASED AUDIO FINGERPRINTING\nFOR MUSIC RETRIEV AL\nPeter Grosche1,3, Joan Serr `a4,\nMeinard M ¨uller2,3, and Josep Ll. Arcos4\n1Saarland University,2Bonn University,3MPI Informatik\n4Artiﬁcial Intelligence Research Institute (IIIA-CSIC)\n{pgrosche,meinard} @mpi-inf.mpg.de, {jserra,arcos} @iiia.csic.es\nABSTRACT\nContent-based approaches to music retrieval are of great\nrelevance as they do not require any kind of manually gen-\nerated annotations. In this paper, we introduce the con-\ncept of structure ﬁngerprints, which are compact descrip-\ntors of the musical structure of an audio recording. Given\na recorded music performance, structure ﬁngerprints facil-\nitate the retrieval of other performances sharing the same\nunderlying structure. Avoiding any explicit determination\nof musical structure, our ﬁngerprints can be thought of as a\nprobability density function derived from a self-similarity\nmatrix. We show that the proposed ﬁngerprints can be\ncompared by using simple Euclidean distances without\nusing any kind of complex warping operations required\nin previous approaches. Experiments on a collection of\nChopin Mazurkas reveal that structure ﬁngerprints facili-\ntate robust and efﬁcient content-based music retrieval. Fur-\nthermore, we give a musically informed discussion that\nalso deepens the understanding of this popular Mazurka\ndataset.\n1. INTRODUCTION\nThe rapidly growing corpus of digitally available audio\nmaterial requires novel retrieval strategies for exploring\nlarge collections and discovering music. One outstand-\ning instance of content-based music retrieval is query-by-\nexample: Given a query in the form of an audio recording\n(or just a short fragment of it), the goal is to retrieve all doc-\numents from a music collection that are somehow similar\nor related to the query. In this context, the notion of simi-\nlarity used to compare different audio recordings (or frag-\nments) is of crucial importance and largely depends on the\nrespective application. Typical similarity measures assess\ntimbral, melodic, rhythmic, or harmonic properties [2].\nA further key aspect of music is its structure. Indeed,\nthe automatic extraction of structural information from\nmusic recordings constitutes a central research topic within\nthe area of music information retrieval [10]. One goal of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2012 International Society for Music Information Retrieval.structure analysis is to split up a music recording into seg-\nments and to group these segments into musically mean-\ningful categories, such as chorus or verse. The structure\nis a highly characteristic property for many musical styles.\nFolk songs and children songs, for example, typically ex-\nhibit a strophic form, where one tune is repeated over and\nover again with changing lyrics. Popular music typically\nconsists of a number of repeating verses connected by a\nrefrain. In classical music, the structure (or musical form)\nis often more complex and offers more variability.\nBesides being characteristic for a certain musical style,\nthe structure and, in particular, the relative duration of\nits elements is also a good descriptor for a speciﬁc piece\nof music—irrespective of speciﬁc realizations or perfor-\nmances. Furthermore, the structure is invariant to changes\nin instrumentation or key and therefore allows for identi-\nfying different performances of the same piece. So far,\nonly a few approaches exist that exploit structural similar-\nity to facilitate music retrieval [1, 4, 6, 7]. Typically, these\napproaches are based on self-similarity matrices (SSMs)\nwhich in general play an important role for analyzing mu-\nsical structures [10]. For computing an SSM, an audio\nrecording is ﬁrst transformed into a sequence of feature\nvectors and then all elements of the sequence are compared\nin a pairwise fashion using a local similarity measure. Re-\npeating patterns in the feature sequence appear as parallel\npaths in the SSM, see Figure 1a. Revealing structural prop-\nerties, SSMs can in turn be used for analyzing structural\nsimilarities of performances. To this end, one requires a\nsimilarity measure that compares entire SSMs while be-\ning invariant to temporal variations. In [6, 7], the SSMs\nare compared using a similarity measure that is based on\na two dimensional version of dynamic programming. The\napproach proposed by Bello [1] is also based on SSMs,\nbut employs a normalized compression distance (NCD) to\nassess their similarity, without requiring any alignment op-\nerations. Originally proposed for comparing protein struc-\ntures in bioinformatics, the NCD can be regarded as a mea-\nsure of the information distance of two objects where the\nKolmogorov complexity is approximated using a standard\ncompression algorithm, see [1].\nInspired by the work of Bello, we describe in this pa-\nper a simple yet effective approach for measuring struc-\ntural similarities of music recordings. As ﬁrst contribu-\ntion, we introduce the concept of structure ﬁngerprints\nwhich are compact structural descriptors of music record-ings. Analogous to [1, 6, 7], our ﬁngerprints are also de-\nrived from self-similarity matrices while avoiding any\nex-\nplicit determination of structure. Speciﬁcally, we use a bi-\nvariate variant of a Parzen-Rosenblatt kernel density esti-\nmation method for representing a given SSM by a prob-\nability density function (pdf) [13]. This has the desired\neffect of smoothing out temporal variations in the perfor-\nmances. As a result, unlike previous approaches, we do not\nrequire any complex distance measure. Instead, recordings\ncan be compared efﬁciently using, e. g., the Euclidean dis-\ntance between ﬁngerprints. As second contribution, we re-\nport on extensive experiments using a large collection of\nChopin Mazurkas. In particular, we show that structure\nﬁngerprints facilitate content-based music retrieval solely\nbased on structural information and exhibit a high degree\nof robustness against performance variations. This makes\nthe presented approach particularly suited for supporting\ntraditional retrieval systems that assess harmonic similari-\nties [2,5,8,12]. Finally, as third contribution, we provide a\nmusically informed discussion of problematic pieces and\nrecordings which also deepens the understanding of the\nMazurka dataset.\nThe remainder of this paper is organized as follows. In\nSection 2, we introduce our approach to computing struc-\nture ﬁngerprints. Then, in Section 3, we describe our re-\ntrieval experiment and give a quantitative as well as musi-\ncally informed discussion of the results. Conclusions are\ngiven in Section 4.\n2. STRUCTURE FINGERPRINTS\nIn this section, we introduce our strategy for computing\nstructure ﬁngerprints that capture characteristics of a mu-\nsical piece and, at the same time, are invariant to properties\nof a speciﬁc performance. We ﬁrst introduce the underly-\ning feature representation (Section 2.1) and the SSM vari-\nant (Section 2.2). In particular, we introduce various en-\nhancement strategies that absorb a large degree of tempo-\nral and spectral variations. Then, in Section 2.3 we explain\nin detail how the ﬁngerprints are derived from the SSMs.\n2.1 Feature Representation\nWe ﬁrst convert a given music recording into a sequence\nof chroma features, which have turned out to be a pow-\nerful mid-level representation for relating harmony-based\nmusic [1, 2, 5, 8, 10, 12]. The term chroma refers to the\nelements of the set {C,C♯,D,...,B}that consists of the\ntwelve pitch classes as used in Western music notation.\nRepresenting the short-time energy content of the signal\nrelative to the pitch classes, chroma features do not only\naccount for the close octave relationship in harmony, but\nalso introduce a high degree of robustness to variations in\ntimbre and instrumentation [8]. Furthermore, normalizing\nthe features makes them invariant to dynamic variations.\nIn our implementation, we use a variant of chroma\nfeatures referred to as CENS1features [8]. As main\n1Chroma Energy Normalized Statistics features, providedby the\nChroma Toolbox www.mpi-inf.mpg.de/resources/MIR/chromatoolbox\n  \n0 50 100 150 200050100150200\n00.51\n  \n50 100 150 20050100150200\n01\n  \n0 50 100 150 200050100150200\n00.51\n  \n20 40 60 80 10020406080100\n0.51\n  \n20 40 60 80 10020406080100\n01\n  \n20 40 60 80 10020406080100\n00.51\nm mTime (sec) mTime (sec) Time (sec)m\nmTime (sec)\nmTime (sec) Time (sec)(a) (b)\n(c) (d)\n(e) (f)\nFigure 1: Computing structure ﬁngerprints for an Ashkenazy\n(1981) performance of Chopin’s Mazurka Op. 56 No. 1 with the\nmusical form A1A2BA3CA4D.(a)SSM computed from CENS\nfeatures. (b)Thresholded variant of (a) (κ = 10 ).(c)Path-\nstructure enhanced SSM (L = 12 ).(d)Resampled SSM Sﬁx\nM\n(M= 100 ).(e)Thresholded variant of (d) (κ = 10 ).(f)Struc-\nture ﬁngerprints (pdf estimated from (e), ℓ= 10 ).\nadvantage, CENS features involve an additional tempo-\nral smoothing and downsampling step which leads to\nan increased robustness of the features to local tempo\nchanges [8]. This property is crucial for obtaining struc-\nture ﬁngerprints that are invariant to local variations in the\nperformances. In our implementation, the resulting feature\nrepresentation has a resolution of 1 Hz (one feature per\nsecond), where each vector is obtained by averaging over\n4 seconds of the audio.\n2.2 Self-Similarity Matrix\nLetX:= (x1,x2,...,x N)be the feature sequence con-\nsisting of Nnormalized CENS features. Furthermore, let\nsbe a similarity measure that allows for comparing two\nCENS vectors. In the following, we use the inner product\nbetween the normalized CENS vectors (cosine measure,\nwhich yields similarity values between 0and1). Then, a\nself-similarity matrix (SSM) is obtained by comparing all\nelements of Xin a pairwise fashion [10]:\nS(n,m) := s(xn,xm)\nforn,m∈[1 :N] :={1,2,...,N}.\nFigure 1a shows the resulting SSM for an Ashkenazy\n(1981) performance of Chopin’s Mazurka Op. 56 No. 1having the musical form A1A2BA3CA4D. The SSM re-\nveals the repetitive structure (four\nrepeating A-parts) in the\nform of diagonal paths of high similarity (dark colors).\n2.2.1 Path-Structure Enhancement\nMusical variations often lead to fragmented path structures\nofS. To alleviate this problem, various matrix enhance-\nment strategies have been proposed [1, 9, 12] with the idea\nto apply a smoothing ﬁlter along the direction of the main\ndiagonal. This results in an emphasis of diagonal infor-\nmation and a denoising of other structures, see Figure 1c.\nIn the presence of signiﬁcant tempo differences, however,\nsimply smoothing along the main diagonal may smear out\nimportant structural information. To avoid this, we use a\nstrategy that ﬁlters the SSM along multiple gradients as\nproposed in [9]. In our experiments, we compute a simple\nmoving average in windows corresponding to Lseconds\nof audio and use ﬁve gradients covering tempo variations\nof−30to+30 %. In the following, the enhanced SSM is\nagain denoted as S.\n2.2.2 Resampling\nA high degree of local tempo differences is already ab-\nsorbed by the smoothing of the CENS features and the\npath-structure enhancement. Global differences in tempo\nof different performances of a piece of music, however,\nlead to SSMs that have different sizes. For deriving struc-\nture ﬁngerprints that are invariant to such tempo differ-\nences, we apply the idea of [5] and introduce a simple re-\nsampling step that converts the N×Nsimilarity matrix S\ninto anM×Msimilarity matrix Sﬁx\nM, withMﬁxed to a\nsuitable value:\nSﬁx\nM(n,m) := S(⌊nN\nM⌉,⌊mN\nM⌉)\nform,n∈[1 :M], where⌊·⌉denotes rounding to the\nnearest integer.2Figure 1d\nshows an example for Sﬁx\nM.\n2.2.3 Thresholding\nWe ﬁnally process the SSMs by suppressing all values that\nfall below a threshold. Analogous to [1,12], we choose the\nthreshold in a relative fashion by keeping κ%of the cells\nhaving the highest score. The motivation for this thresh-\nolding step is that only a certain amount of the cells of\nthe SSM are expected to encode relevant structural infor-\nmation. The thresholding can then be regarded as some\nkind of denoising, where only relevant paths are retained,\nsee Figure 1e. In the following, the resulting thresholded,\nresampled, and path-structure enhanced SSM is denoted as\nˆSﬁx\nM. Figure 1e also emphasizes the importance of the path-\nstructure enhancement, as directly applying the threshold-\ning operation on the original SSM does not lead to the de-\nsired denoising effect, see Figure 1b.\n2.3 Probability Density Estimation\nThe four repeating A-parts of our Mazurka example are\nclearly revealed by ˆSﬁx\nMin the form of diagonal paths, see\n2In our experiments, using linear or cubic interpolation didnot lead to\nany improvements.\n  \n0 50 100 150 200 250050100150200250\n00.51\n  \n0 50 100 150 200 250050100150200250\n00.51\n  \n20 40 60 80 10020406080100\n00.51\n  \n20 40 60 80 10020406080100\n00.51\nm mTime (sec) Time (sec)m\nmTime (sec) Time (sec)(a) (b)\nFigure 2: Original SSMs (top) and structure ﬁngerprints (bot-\ntom) for two performances of Chopin’s Mazurka Op. 56 No. 1.\n(a)Rubinstein (1966) and (b)Kushner (1989).\nFigure 1e. However, as the structural information is con-\ntained in only a few cells of the thresholded SSM (in other\nwords, the resulting matrix is sparse), small temporal vari-\nations in performances may lead to large distances when\ndirectly comparing these matrices in a pointwise fashion.\nAs a result, some kind of tolerance to temporal variations is\nrequired in the similarity measure, as e. g., introduced by\nthe similarity measures based on dynamic programming\nused in [6, 7] and the NCD used by Bello in [1].\nAvoiding the additional complexity of such techniques,\nwe consider ˆSﬁx\nMas a bivariate random sample of coordi-\nnates(n,m) forn,m∈[1 :M]and our goal is to esti-\nmate the probability density function (pdf) producing this\nSSM.3The underlying assumption is that the pdf corre-\nsponds to the musical structure of the piece and that the\nbivariate random samples we observe are affected by vari-\nations in the realization of a speciﬁc performance. Analo-\ngous to [11], we employ a Parzen-Rosenblatt kernel den-\nsity estimation method [13] that consist in convolving ˆSﬁx\nM\nwith a two-dimensional Gaussian kernel of size ℓ. As a re-\nsult, temporal variations in the performances are smoothed\nout. The choice of the value ℓconstitutes a trade-off be-\ntween ﬁngerprint characteristic (small value) and robust-\nness to temporal variations (large value).\nThe resulting ﬁngerprints (see Figure 1f) are an M×M\nrepresentation4of the musical structure that features a\nhigh degree of robustness against properties of a speciﬁc\nperformance. Figure 2 shows two further examples of ﬁn-\ngerprints for the Mazurka Op. 56 No. 1.\n3. STRUCTURE-BASED RETRIEV AL\nIn this section, we show how the structure ﬁngerprints (SF)\ncan be used to facilitate structure-based music retrieval.\n3In the following, we use the term pdf, although fordiscrete random\nvariables, the term probability mass function would be more appropriate.\n4Note that this matrix is symmetric and only M(M+ 1)/2 entries\nare needed for representing the ﬁngerprints.Method Dist. Dataset P Sync. MAP T[sec]\nBello [1] NCD Bello 2919 No 0.767 >1000\nSF KL ORG 2793 No 0.819 66.45\nSF ED ORG 2793 No 0.816 0.58\nSF ED MOD 2792\nNo 0.828 0.58\nSF ED MOD 2792 Yes 0.958 0.58\nTable 1: Overview of the results obtained for different methods\nand datasets. Dist. denotes the distance measure used, Pthe num-\nber of performances in the dataset,\nandTthe run-time in seconds\nfor computing P×Pdistances.6See Section 3.4 for a descrip-\ntion of the dataset MODand the column Sync. (indicating whether\nsynchronized ﬁngerprints are used).\nWe ﬁrst describe the collection of Chopin Mazurkas (Sec-\ntion 3.1) and the retrieval scenario (Section 3.2). Then, we\ncontinue with a quantitative evaluation (Section 3.3) and\ngive a musically informed discussion (Section 3.4).\n3.1 Mazurka Collection\nIn our experiments, we use an audio collection comprising\nmany recorded performances for each of the 49Mazurka\nby Fr ´ed´eric Chopin. Since different performances of a\nMazurka typically share the same structure, this collection\nis a good choice for evaluating structural similarities. The\ndataset was assembled by the Mazurka Project5and has\nalso been used by Bello in [1]. Note, however, that there\nare differences between our dataset (denoted as ORGin the\nfollowing) and the one used in [1] (denoted as Bello). Ac-\ntually, the datasets constitute a snapshot at different stages\nin the assembly process of the Mazurka Project which also\nresults in a different number of performances (2793 for\nORGand2919 forBello, see Table 1).\n3.2 Retrieval Scenario\nUsing this dataset, we evaluate our structure ﬁngerprints\n(SF) in a document-level retrieval scenario as in [1]. Given\none performance of a Mazurka as query, the goal is to re-\ntrieve all other performances of the same Mazurka from the\ngiven dataset. To this end, we ﬁrst compute the ﬁngerprints\nfor allPperformances of the dataset. Using a suitable dis-\ntance measure, we then derive the P×Pmatrix of pairwise\ndistances between all performances, see Figure 6a. As the\nstructure ﬁngerprints are represented as densities, a natu-\nral choice of distance measure is the Kullback-Leibler di-\nvergence (KL). Additionally, in our experiments, we also\nuse a simple Euclidean distance (ED). Finally, we rank the\nresult with respect to ascending distances and express the\nretrieval accuracy by means of the mean average precision\n(MAP) measure as in [1, 12].\n3.3 Quantitative Evaluation\nFirst, we give a quantitative discussion of the results. Ta-\nble 1 shows overall MAP values for the different meth-\nods and datasets. In [1], Bello reported MAP = 0.767\nusing his approach based on the NCD. Using the param-\netersL= 10,κ = 20,M = 50,ℓ= 5 and the KL di-\nvergence, our approach leads to comparable, if not even\nslightly better results (MAP = 0.819). Note, however,\n5mazurka.org.uk12 4 6 8 10 12 14 16 18 200.60.650.70.750.80.850.9\n  \nκ=5\nκ=10\nκ=15\nκ=20\nκ=25\nκ=40\n1020 35 50 75 100 125 150 175 2000.60.650.70.750.80.850.9\n  \nℓ=1\nℓ=3\nℓ=5\nℓ=10\nℓ=15\nℓ=20L(seconds)MAP\nMMAP(a)\n(b)\nFigure 3: Parameter evaluation using MOD and Euclidean dis-\ntances (ED). MAP values for different values of (a)the smoothing\nparameter Land threshold κ(M= 100 ,ℓ= 10 ) and (b)of the\nﬁngerprint size Mand kernel size ℓ(L= 12 sec ,κ= 20 ).\nthat the results are not directly comparable due to the dif-\nferences in the datasets. The results are insofar surpris-\ning, as our approach is not only conceptually much sim-\npler, but also more explicit and, as it turns out, much more\nefﬁcient. The last column of Table 1 indicates the run-time\nin seconds for computing the matrix of P×Ppairwise dis-\ntances.6Without knowing exact numbers for the NCD, our\napproach using KL seems to be at least one order of mag-\nnitude faster than [1]. Actually, when using the Euclidean\ndistance (ED) instead of KL, the run-time of our approach\ncan be improved signiﬁcantly by two orders of magnitude\n(resulting in a run-time of just 0.58seconds for computing\nallP×Pdistances), without any degradation of retrieval\naccuracy (MAP = 0.816).\nWe now continue with an evaluation of different param-\neter settings using ED (using KL lead to very similar ﬁnd-\nings). Figure 3a shows MAP values obtained on ORGas a\nfunction of the temporal smoothing parameter L(in sec-\nonds) and the relative threshold κ, see Section 2.2. Appro-\npriate values for Lconstitute a trade-off between enhance-\nment capability and level of detail. For the Mazurkas, a\nsmoothing of 6-12 seconds seems to be reasonable, the ac-\ntual choice of the parameter, however, is not crucial. For\nexample, ﬁxing κ= 15, one obtains MAP = 0.815 for\nL= 6 andMAP = 0.816 forL= 10. The threshold\nvalueκconstitutes a trade-off between retaining relevant\nstructural information and denoising the SSMs. For the\nMazurkas, 10%-25% seems to be a good compromise for\ncapturing the repetitive structure. Again, the exact value\nis not crucial. For example, ﬁxing L= 6, one obtains\nMAP = 0.809 forκ= 25 andMAP = 0.814 forκ= 10.\nFigure 3b shows MAP values as a function of the ﬁn-\ngerprint size Mfor different settings of the kernel density\nparameter ℓ. Interestingly, the size of the structure ﬁnger-\n6Using a vectorized MATLAB implementation of ED, a C/C++ im-\nplementation of KL, and an Intel Xeon E3-1225 CPU. Run-times for the\nNCD are estimated from the indicators given in [1] and own experiments.  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\nm mm mm m(a) (b)\nFigure 4: Structure ﬁngerprints (top) and synchronized struc-\nture ﬁngerprints (bottom) for performances of Chopin’s Mazurka\nOp. 24 No. 2. (a)Merzhanov (2004) with applause at start and\nend of recording. (b)Smith (1975) with silence at the end.\nprints can be reduced to M= 50 or evenM= 35, while\nstill retaining a high retrieval accuracy. The ratio of M\nandℓ, however, is of crucial importance as it constitutes\na trade-off between ﬁngerprint characteristic and robust-\nness against temporal variations in the performances. The\nsettingsM= 50,ℓ= 5, and M= 100,ℓ = 10, and\nM= 200,ℓ = 20 yield almost identical retrieval results\n(MAP = 0.816, MAP = 0.818, and MAP = 0.819,\nrespectively). Decreasing the size of the ﬁngerprints, how-\never, has the advantage of reducing the computational load.\nAside from the robustness to actual parameter settings,\nour approach turned out to be rather robust to implemen-\ntation details. For example, very similar results were ob-\ntained by using, e. g., Cosine, Hellinger, and Battacharyya\ndistances between SFs. Even an alternative implementa-\ntion using different chroma features as well as delay co-\nordinates and recurrence plots (instead of the enhanced\nSSMs) similar to [1, 11, 12], lead to almost identical re-\nsults. This also indicates that our concept is generalizable.\n3.4 Musically Informed Discussion\nOur ﬁngerprint-based approach allows for detecting mu-\nsically interesting phenomena and inconsistencies in the\nMazurka collection. A careful investigation of the re-\ntrieval results revealed three phenomena. Firstly, we dis-\ncovered that there are 67recordings in the dataset that are\nincorrectly assigned to one of the Mazurkas, although they\nactually are performances of another Mazurka.7Another\nrecording of the collection did not correspond to any of the\nMazurkas.8We corrected these errors and denote the mod-\niﬁed dataset MOD. Repeating the retrieval experiment using\nthe2792 performances of MOD, the MAP value increases to\n0.828, see Table 1 (fourth row).\n7A majority (51 of the67recordings) affects Op. 41 consistingof four\nMazurkas (No. 1 to No. ˙4), where a permutation of the assigned numbers\noccurs.\n8Labeled as a Rosenthal (1935) performance of Op. 50 No. 2.\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\n  \n1 20 40 60 80 1001  20 40 60 80 100\n00.51\nm mm m(a) (b)\n(c) (d)\nFigure 5: Structure ﬁngerprints for four performances with dif-\nfering structure of Chopin’s Mazurka Op. 68 No. 4 (L = 12 ,\nκ= 20 ,ℓ= 10 ,M= 100 ).(a)Niedzielski (1931). (b)Katin\n(1996). (c)Rubinstein (1952). (d)Rubinstein (1966).\nSecondly, it turned out that many incorrectly retrieved\nperformances exhibit a long passages of applause, silence,\nor spoken moderation at the beginning and/or end. Ac-\ntually, such passages can be regarded as additional struc-\ntural elements. As a result, the structure of these perfor-\nmances does not match to the structure of the other per-\nformances of the same Mazurka, see Figure 4. To quan-\ntify this phenomenon, we use music synchronization tech-\nniques [3, 8] for identifying musically corresponding time\npositions in all versions of a Mazurka and use this infor-\nmation to warp the ﬁngerprints to a common time line. For\nadditional segments appearing in one performance, there\nare no corresponding time positions in the other perfor-\nmances. As a result, such segments are basically not re-\nﬂected in the resulting synchronized ﬁngerprints, see Fig-\nure 4.9Using synchronized ﬁngerprints to exclude the ad-\nditional segments, we repeat our experiment using MODand\nobtainMAP = 0.958, see Table 1 (last row).\nThe third phenomenon detected during our experiments\nare structural differences in the recordings. For instance,\nsome pianists do not strictly stick to the score when per-\nforming a piece but omit (or sometimes even introduce)\nrepetitions. Obviously, these structural differences lead\nto high distances as shown in Figure 6b for the Mazurka\nOp. 56 No. 1, where eight of the 42performances exhibit\na different structure.10The prime example for this effect\nis Mazurka Op. 68 No. 4, where the last bar in the score\ncontains the marking D. C. dal segno senza ﬁne. However,\nthere is no ﬁnemarked in the score that would tell the pi-\nanist where to end. As a result, a performer may repeat\nthe piece as often as he or she wants. This leads to many\nversions of the piece that differ signiﬁcantly in structure as\nalso revealed by the respective pairwise distances shown\nin Figure 6e. Figure 5 shows the ﬁngerprints of four such\n9This strategy has a similar effect as using a distance measure based\non dynamic programming, as proposed in [6, 7].\n10Actually, all eight musicians omit a repetition of the A-part, leading\nto the form A1BA2CA3Dinstead of A1A2BA3CA4D.  \n1 500 1000 1500 2000 2500 2792150010001500200025002792\n00.050.10.150.20.250.30.350.40.450.5\n  \n1835 187618351876\n  \n766 831766831\n  \n2679 2697 2719 27292679269727192729\n  \n2730 279227302792\nPerformance Performance PerformancePerformance\nPerformance Performance(a) (b) (c)\n(d) (e)\n(c)(b)(e)\n(d)\nFigure 6: (a) Matrix of pairwise Euclidean distances for the 2792 performances of MOD. (b)Detail of the42performances of Op. 56\nNo. 1, see also Figure 2. (c)Detail of the 66performances of Op. 24 No. 2, see also Figure 4. (d)Detail of the 51performances of Op. 68\nNo. 3. (e)Detail of the 63performances of Op. 68 No. 4, see also Figure 5.\nversions, which, obviously, cannot be retrieved by a purely\nstructure-based retrieval approach.\nOn the other hand, during our experiments we discov-\nered performances that exhibit a surprisingly low distance,\nsee, e. g., the squares of low distance on the main diagonal\nin Figure 6d. The low distance between the performances\n2697-2699 is actually known as the “Hatto effect”: record-\nings released under the name of the pianist Joyce Hatto in\n1993 (2697) and 2006 (2698) that are actually time-scaled\ncopies of a 1988 recordings of Eugen Indjic (2699). Sim-\nilarly, some performances appear repeatedly in the dataset\nas they were released multiple times. Examples for this ef-\nfect are performances 2719 and2720 (Rubinstein) as well\nas2722 and2723 (Smidowicz).\n4. CONCLUSION\nThe concept of structure ﬁngerprints presented in this pa-\nper allows for retrieving music recordings solely based on\nstructural information. Using a combination of suitable en-\nhancement strategies, our approach is robust as well as ef-\nﬁcient. Furthermore, as our experiments reveal, the results\nobtained by our approach are at least comparable to state-\nof-the-art approaches without relying on complex distance\nmeasures. As further advantage of our approach, just using\nEuclidean distances between ﬁngerprints opens the pos-\nsibility of exploiting efﬁcient index-based methods such\nas locality-sensitive hashing to scale the approach to even\nlarger datasets. We showed that our methods are suited\nfor systematically analyzing structural properties of entire\nmusic collections, thus deepening the musical understand-\ning of the data. Obviously, the limits of structure-based\nretrieval are reached when the assumption of global struc-\ntural correspondence between performances is violated.\nAcknowledgments: The work by P. Grosche und M. M ¨uller has\nbeen supported by the Cluster of Excellence on Multimodal Com-\nputing and Interaction at Saarland University and the German Re-search Foundation (DFG MU 2686/5-1). J. Serr `a and J. L. Arcos\nacknowledge 2009-SGR-1434 from Generalitat de Catalunya,\nTIN2009-13692-C03-01 from the Spanish Government, and EU\nFeder funds. J. Serr `a also acknowledges JAEDOC069/2010 from\nConsejo Superior de Investigaciones Cient ´ıﬁcas.\n5. REFERENCES\n[1] J. P. Bello. Measuring structural similarity in music. IEEE Trans. on\nAudio, Speech and Language Processing, 19(7):2013–2025, 2011.\n[2] M. A. Casey, R. Veltkap, M. Goto, M. Leman, C. Rhodes, and\nM. Slaney. Content-based music information retrieval: Current direc-\ntions and future challenges. Proc. of the IEEE, 96(4):668–696, 2008.\n[3] S. Ewert, M. M ¨uller, and P. Grosche. High resolution audio synchro-\nnization using chroma onset features. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Signal Processing\n(ICASSP), pages 1869–1872, Taipei, Taiwan, 2009.\n[4] J. Foote. ARTHUR: retrieving orchestral music by long-term struc-\nture. In Proceedings of the International Symposium on Music Infor-\nmation Retrieval (ISMIR), Plymouth, USA, 2000.\n[5] P. Grosche and M. M ¨uller. Toward characteristic audio shingles for\nefﬁcient cross-version music retrieval. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Signal Processing\n(ICASSP), pages 473–476, Kyoto, Japan, 2012.\n[6] T. Izumitani and K. Kashino. A robust musical audio search method\nbased on diagonal dynamic programming matching of self-similarity\nmatrices. In Proc. of the International Conference on Music Informa-\ntion Retrieval (ISMIR), pages 609–613, Philadelphia, USA, 2008.\n[7] B. Martin, M. Robine, and P. Hanna. Musical structure retrieval by\naligning self-similarity matrices. In Proc. of the Int. Conf. on Music\nInformation Retrieval (ISMIR), pages 483–488, Kobe, Japan, 2009.\n[8] M. M ¨uller. Information Retrieval for Music and Motion. Springer\nVerlag, 2007.\n[9] M. M ¨uller and F. Kurth. Enhancing similarity matrices for music au-\ndio analysis. In Proc. of the Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 437–440, Toulouse, France, 2006.\n[10] J. Paulus, M. M ¨uller, and A. P. Klapuri. Audio-based music structure\nanalysis. In Proc. of the Int. Conf. on Music Information Retrieval\n(ISMIR), pages 625–636, Utrecht, The Netherlands, 2010.\n[11] J. Serr `a, M. M ¨uller, P. Grosche, and J. L. Arcos. Unsupervised detec-\ntion of music boundaries by time series structure features. In Proc. of\nthe AAAI Int. Conf. on Artiﬁcial Intelligence, 2012. In Press.\n[12] J. Serr `a, X. Serra, and R. G. Andrzejak. Cross recurrence quan-\ntiﬁcation for cover song identiﬁcation. New Journal of Physics ,\n11(9):093017, 2009.\n[13] J. S. Simonoff. Smoothing Methods in Statistics. Springer, 1996."
    },
    {
        "title": "Modeling Piano Interpretation Using Switching Kalman Filter.",
        "author": [
            "Yupeng Gu",
            "Christopher Raphael"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415028",
        "url": "https://doi.org/10.5281/zenodo.1415028",
        "ee": "https://zenodo.org/records/1415028/files/GuR12.pdf",
        "abstract": "An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a “smoothed” version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method.",
        "zenodo_id": 1415028,
        "dblp_key": "conf/ismir/GuR12",
        "keywords": [
            "piano music",
            "piano music interpretation",
            "expressive timing activities",
            "quantifying",
            "discrete variables",
            "continuous variables",
            "tempo discretely",
            "tempo progression",
            "graphical model",
            "musicality"
        ],
        "content": "MODELING PIANO INTERPRETATION USING\nSWITCHING KALMAN FILTER\nYupeng Gu\nIndiana University\nSchool of Informatics and ComputingChristopher Raphael\nIndiana University\nSchool of Informatics and Computing\nABSTRACT\nAn approach of parsing piano music interpretation is\npresented. We focus mainly on quantifying expressive\ntiming activities. A small number of different expressive\ntiming behaviors (constant, slowing down, speeding\nup, accent) are deﬁned in order to explain the tempo\ndiscretely. Given a MIDI performance of a piano music,\nwe simultaneously estimate both discrete variables that\ncorresponds to the behaviors and continuous variables\nthat describe tempo. A graphical model is introduced to\nrepresent the evolution of the discrete behaviors and tempo\nprogression. We demonstrate a computational method\nthat acquires the approximate most likely conﬁguration of\nthe discrete behaviors and the hidden continuous variable\ntempo. This conﬁguration represent a “smoothed” version\nof the performance which greatly reduces parametrization\nwhile retaining most of its musicality. Experiments are\npresented on several MIDI piano music performed on a\ndigital piano. An user study is performed to evaluate our\nmethod.\n1. INTRODUCTION\nThe score of Western classical music is a notation form\nthat contains information such as pitches, durations, and\nwords or symbols that give an abstract reference of how\nmusic should be played. It is rather a cartoon like\ndescription that misses much detail compared to actual\nperformances. Classical musicians are trained to ﬁll the\ndifferences. It is fair to say that the music we hear\nfrom CD or concerts have much more information than\nits corresponding notation. Often people use words such\nas intention, emotion, expression, interpretation, gesture,\nphrasing and articulation to describe this extra information.\nHowever, from a scientiﬁc point of view, these descriptions\nare vague, subjective and hard to quantify.\nIn this work, we propose a mathematical approach that\naims to create a representation for interpretation. We think\nof interpretation as having a categorical component to it.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.This will be the discrete component of our model. We\nconsider interpretation to be performance strategies for\ndifferent groups of music notes, where the ﬁne details\nof notes such as inter-onset intervals (IOI) in each group\nshould be strongly correlated and explicitly constrained,\ninstead of modeled independently. We believe this is\nsimilar to how musicians think of and communicate about\nmusic. Thus our representation will consist of discrete\nstates that describe different performance behaviors and\ncontinuous variables that describe tempo and timing in\ndetail. Where the detail will follow the characteristic of\nthe discrete behaviors.\nIn order to circumvent the difﬁculty of audio recogni-\ntion, we chose to use MIDI data for interpretation parsing\nin this work. The note by note detail and continuous\ncontrollers enable using MIDI to create and preserve\nexpressive performances. For instance, we can ﬁnd many\nMIDI piano performances from internet that demonstrate\nexpressive interpretation. But relating MIDI data directly\nto interpretation is still not straightforward. Because the\nobservable aspects of the performance are consequences\nof hidden interpretive notions. There is a missing layer\nof ideas that one needs in order to interpret the numbers\nin MIDI performance. This hidden layer of interpretive\nconstructs guide the timing and volume data. We believe\nthis hidden layer has a close relationship to interpretation\nand attempt to model it in this work.\nThis approach has many potential applications. In the\narea of creating expressive digital music in symbolic form,\nwe have a long standing interest of trying to systematically\nchange a performance meaningfully. It is often that\nsomeone had a decent performance recorded where some\npart of the performance is not fulﬁlling. If one is not\nwilling or able to repeat the performance until getting\nbetter results, the only thing we can do is to modify\nparameters at the individual note level and hope some\ncombinations might work. This is clearly an unnatural and\nunmusical way to modify an expressive performance. It\nwould be better to operate on a higher-lever representation\nof the interpretation that understands notion of gestures\nand phrases. For instance, when we modify the parameter\nof a single note, some other parameters will compensate to\nretain a musical sense.A musically meaningful representation of interpretation\ncan also be used as a visualization tool. It is often an\ninteresting experience for musicians to listen to a recording\nof themselves. As a listener, one has a different perspective\nand judges the performance more objectively. However,\nlistening to a recording is time consuming, and we can\nonly access a small amount of information at one time.\nOur representation can be used to visualize tempo changes\nin a discrete way, so musicians can take advantages of\ntheir eyes to see and explore an entire performance at\nonce. Furthermore, such visualization can also be used\nto compare different performances, so it will be easier for\nmusicians to discover how they differ from professionals.\nSuch representation could also be applied to the\nexpressive rendering problem. With the development\nof the computer technology, there is a growing interest\nin generating performance that can match the level of\nprofessional musicians. The existing rendering systems\nare mostly rule-based or case-based. Such systems often\ninclude extracting and applying rules with parameters [1]\n[2] [3]. The advantage of our representation is that it is\nmuch lower dimensional than the usual MIDI performance.\nHence it is easier to estimate the parameters rather than to\nestimate all the details for every note. Our representation\nalso has the potential to reduce the unintentional activities\nfrom performers which could cause troubles in applying\nmachine learning techniques to performances.\nAnother possible application of such representation is\nin creating accompaniment system. A traditional accom-\npaniment system seeks to create a ﬂexible accompaniment\nto a live soloist that follows the player [4] [5]. For\nmost existing systems, the main focus is to keep up\nwith the soloist as much as possible. Which could\ninevitably result in overﬁtting the soloists performance\nand failing to understand what the player’s real intentions\nare. Good following requires a deeper understanding\nof the performers intention, thus separating signal from\nnoise. Our representation has the potential to provide\na performance model that maintains a certain level of\nmusicality as well as offer enough ﬂexibility. Also, a more\nadvanced accompaniment system may be able to function\nlike a music partner and even teach the player in the future.\nIt is hard to imagine constructing such system without\nhaving a layer that can represent the interpretation.\nWe present a mathematical model in section 2. There is\na literature on models that combine discrete state variables\nwith Gaussian variables in ﬁelds such as economics,\nmedical science and control engineering [6] [7] [8] [9].\nThese models are known alternately as Markov jump\nprocess, hybrid models, state-space models with switching\nand switching Kalman ﬁlter. We think this type of model\nsuits our purpose of parsing the interpretation of a piano\nperformance. A computation method is introduced in\nsection 3 in order to compute the approximate most likely\nconﬁguration of the variables in our model. Experiments\nare presented in section 4 as well as a brief user study that\nevaluates our model.2. THE MODEL\nWe consider only expressive timing in this section.\nSuppose we have a music performance that contains a\nsequence of note onset times o0;o1;:::oN. Let the score\npositions associated with the notes be p0;p1;:::pNwhich\nare measured in beats. We deﬁne four possible types of\ndifferent behaviors regarding tempo activities. Every event\nwill be labeled as one of the following four behaviors.\n\u000b1. constant speed\nIn much notated music, especially Western classical mu-\nsic, often tempo marks or beats per measure(BPM) are\nused to indicate how fast the music should be played. It\nis clearly impossible for a human being to strictly execute\nthem, but for most of the time these indications are still\nexpected to be respected. There are words such as “rush”\nand “unstable” that sometimes are used by musicians to\ndescribe unintentional tempo change. In our analysis, we\nwant to recognize and ﬁx these unintentional actions.\n\u000b2. slowing down\nAlthough for many sections of music performance con-\nstant speed is intended, it is still very unlikely that such\nspeed will be carried consistently though a music piece.\nAn always strict in-tempo performance is often referred\nto as “mechanical”, which is often undesirable and un-\ncommon for Western classical music. Intentional tempo\nvariation within a short time period is a technique that is\noften used to show expressiveness. Even though certain\nvarying process could be very complicated, it can always\nbe seen as a sequence of basic behaviors. We consider\nslowing down to be one of them.\n\u000b3. speeding up\nWe consider speeding up to be the other basic behav-\nior. Combined with \u000b1and\u000b2, these three “devices” can\ntheoretically represent any kind of tempo behaviors.\n\u000b4. Accent (single note behavior)\nA common technique to make an accent of a certain\nnote is to take a little extra time before playing that note.\nAlthough it can also be seen as a slowing down followed\nby an immediate speeding up, in this discussion, we would\nlike to model this as an individual behavior for two reason-\ns: 1) Such behavior occurs often; 2) the tempi before and\nafter accents are usually the same. We believe this needs\nto be modeled explicitly.\nSo, the possible discrete states for every event are de-\nscribed by the set \u0006 =f\u000b1;\u000b2;\u000b3;\u000b4g. Our goal is\nto label each event onwith a behavior Snfrom \u0006. Let\nS1;S2;:::;SNbe the discrete behavior process, Sn2\u0006;n=\n1;:::;N .\nWe model the sequence of the discrete states as a Markov\nchain. Figure 1 shows The Markov model. The assump-\ntions are: 1) The states can stay in either constant speed\nstate, slowing down state or speeding up state; 2) Before\nspeeding up, there must be a slowing down process; 3)\nbefore slowing down, the performance must be in constant\nspeed; 4) Accent can only happen during constant speed\nmode and will only last for one note. These assumptions\nare not necessarily true, we only make our model this way\nfor simplicity.4 32 1αFigure 1. A Markov model showing possible transitions\nbetween the discrete states.\nThis Markov chain is modeled with initial probabilities:\nI(s1) =P(S1=s1)\ns1could only be \u000b1or\u000b3in our model – meaning we\nonly start a performance with a constant tempo or speeding\nup to a constant tempo.\nThe transition probability matrix is deﬁned as:\nR(sn+1;sn) =P(Sn+1=sn+1jSn=sn)\nNow we model the tempo behavior in different states\nunder a switching Kalman ﬁlter framework. Let t1;t2;:::;tN\nanda1;a2;:::;aNbe the continuous variables that repre-\nsent the tempo and acceleration associated with o1;o2;:::;oN\nrepectively, measured in seconds per quarter note. Denote\nXn;t=tn\nXn;a=an\nXn= (tn;an)T\nln=pn\u0000pn\u00001\nWherelnis the IOI in beats for two consecutive events.\nWe have the initial distribution\nX1\u0018N(\u0012\u0016t\n0\u0013\n;\u0012\u001b2\nt0\n0 0\u0013\n)jS1=\u000b1\nX1\u0018N(\u0012\u0016t\n\u0016a\u0013\n;\u0012\u001b2\nt0\n0\u001b2\na\u0013\n)jS1=\u000b3\nThen we deﬁne the different behaviors\nXn=Xn\u00001 jSn=\u000b1;Sn\u00001=\u000b1 (1)\nXn\u0018N(\u0012\u0016t\n0\u0013\n;\u0012\u001b2\nt0\n0 0\u0013\n)jSn=\u000b1;Sn\u00001=\u000b3 (2)\nXn=Xn\u00001 jSn=\u000b1;Sn\u00001=\u000b4 (3)\nXn;a\u0018N(\u0016a;\u001b2\na)\nXn;t=Xn\u00001;t+lnXn;ajSn=\u000b2;Sn\u00001=\u000b1 (4)\nXn=\u00121ln\n0 1\u0013\nXn\u00001jSn=\u000b2;Sn\u00001=\u000b2 (5)\ns\nx\nyFigure 2. The DAG describing the dependency structure\nof the variables of our model. Circles represent discrete\nvariables while squares represent continuous variables.\nXn;a\u0018N(\u0000\u0016a;\u001b2\na)\nXn;t=Xn\u00001;t+lnXn;ajSn=\u000b3;Sn\u00001=\u000b2 (6)\nXn=\u00121ln\n0 1\u0013\nXn\u00001jSn=\u000b3;Sn\u00001=\u000b3 (7)\nXn=Xn\u00001 jSn=\u000b4 (8)\nThis model forces the tempo to be a constant (but\nunknown) in each section where the discrete states stay in\n\u000b1or\u000b4. It also forces an unknown constant acceleration\nwhen discrete states in \u000b2and\u000b3. Equation (2) means\nwhen the performance comes back from speeding up, the\nperformer will start a new unknown tempo. We denote\nall the unknown tempi fXn;ts.t.Sn=\u000b1;Sn\u00001=\u000b3g\nasf\u001c1;:::;\u001cKg. Equation (4) means every time when\nthe performance gets into slowing down states, a new\nunknown acceleration with a positive mean is introduced.\nEquation (6) means every time when the performance gets\ninto speeding up states, a new unknown acceleration with\na negative mean is introduced. We denote all the unknown\naccelerationsfXn;as.t.Sn=\u000b2;Sn\u00001=\u000b1ORSn=\n\u000b3;Sn\u00001=\u000b2gasf\r1;:::;\rLg.\nNow we relate this tempo and acceleration to the\nobservables. Let the IOI yn=on\u0000on\u00001 forn=\n1;2;:::;N Then the data model is:\nyn=lnXn;t+cn+\u000fn (9)\nwhere\ncn= 0 jSn6=\u000b4 (10)\ncn\u0018N(\u0016c;\u001b2\nc)jSn=\u000b4 (11)\n\u000fn\u0018N(\u0016\u000f;\u001b2\n\u000f) (12)\nEquation (11) means when the performance comes to\nan accent, the performer will stretch the IOI with a\nrandom length. We denote all the unknown variables fCn\ns:t:Sn=\u000b4gasf\u00141;:::;\u0014Mg. Equation (12) represent\nthe observation errors. All the other variables of the model\ndepend deterministically on these variables\n\u001c1;:::;\u001cK;\r1;:::;\rL;\u00141;:::;\u0014M;\u000f1;:::;\u000fNThe directed acyclic graph(DAG) of the graphical\nmodel is represented in ﬁgure 2. The model has both\ndiscrete and continuous variables. For every conﬁguration\nof the discrete variables, the continuous variable have a\nmultivariate Gaussian distribution and is a Kalman ﬁlter.\nThus, theS1;::;SN;X1;:::;XN; y1;:::;yNcollectively\nhave a conditional Gaussian distribution.\n3. COMPUTING THE INTERPRETATION PARSE\nWe want to simultaneously estimate the discrete state vari-\nableS1;S2;:::;SNand the continuous variable tempo X1;X2\n;:::;XNgiven the observed IOI data y1;y2;:::;yn.\nThe joint likelihood function can be expressed as\nL(y;s;x ) =I(s1)P(y1jx1;s1)\n\u0002NY\nn=2(P(snjsn\u00001)P(xnjxn\u00001;sn;sn\u00001)P(ynjxn;sn))\nWe are interested in ﬁnding the best conﬁguration of hid-\nden variable SandXthat has the greatest probability of\ngiving the observation y.\n(^s;^x) = arg max\ns2S;x2XL(y;s;x )\nSince our model has a linear graph structure described\nin Figure 2, the maximization problem can be solved using\ndynamic programming. Using the notation aj\ni=fai;ai+1;\n:::;ajg. LetLn(yn\n1;sn\n1;xn\n1)be the joint likelihood func-\ntion for variables until observation n,yn\n1;Sn\n1;Xn\n1forn=\n1;2;:::;N . Then we deﬁne the density of the optimal con-\nﬁguration for variables until observation n\nHn(sn;xn) = max\nsn\u00001\n1;xn\u00001\n1Ln(yn\n1;sn\n1;xn\n1) (13)\nThenHn(sn;xn)can be computed recursively\nH1(s1;x1) = max\ns1I(s1)P(y1jx1;s1)\nHn(sn;xn) = max\nsn\u00001;xn\u00001Hn\u00001(sn\u00001;xn\u00001)\n\u0002P(snjsn\u00001)\n\u0002P(xnjxn\u00001;sn;sn\u00001)\n\u0002P(ynjxn;sn));n = 2;:::;N\nWe can see that\nmax\nsN;xNHN(sN;xN) = max\nsN\n1;xN\n1Ln(yN\n1;sN\n1;xN\n1))\nA more detailed description of this method can be found\nin [10].\nIt is obvious that the possible state sequences grow ex-\nponentially with the number of event N. In order to make\nthe computation tractable, we need to approximate. In this\nexperiment, we use a simple approach that is to sort the\ncurrent hypotheses on probability densities and leave out\nthe small ones in (13). This method is also known as “beam\nsearch”.\nOnce we compute the approximately optimal conﬁgura-\ntion of discrete states ^s. We can recover ^xfrom the Kalman\nﬁlter deﬁned by ^s.4. EXPERIMENTS\nMIDI is our data format. We use the time stamps directly\nfrom MIDI ﬁles as the onset times of the notes. All data\nare collected from a high quality digital piano made by\nYAMAHA. The reason we choose such an instrument is to\nensure that we can hear exactly the same thing as originally\nrecorded when the music is being reproduced. Also when\nwe evaluate our model by modifying the performances and\ncompare them to the original ones, using this instrument\ncan minimize the effect introduced by difference in sound\ncharacteristic. The piano keyboard is weighed to simulate\nthe feeling of the real piano keys. According to the 5\npianists who helped creating the data, although it is still\nnot the same as playing a real piano, they can adapt to it\nand play expressively.\n3 sets of experiments are performed:\n4.1 Smoothing a Performance\nThe ﬁrst set of experiment demonstrates that our model\nparsimoniously and faithfully represents the original per-\nformance.\nThe data set contains 12 piano excerpts played by\ngraduate level piano major students from the Indiana\nUniversity Jacobs School of Music. In order to show\nthe generality of our model, the excerpts are selected\nfrom composers from different time periods, including\nBach, Haydn, Mozart, Beethoven, Schumann, Brahms and\nBarber. The notated tempi for the excerpts also differ (i.e.\nthere are fast pieces and slow pieces.).\nFor each excerpt, we have a corresponding MIDI score\ncreated from music notation software. Using the method\ndescribed in [11], we can acquire the music times fpkgin\nbeats for the performance. For each note, we use the time\nstamp of the MIDI onset as our observation on. If several\nnotes are struck at the same time (i.e. a chord), we use\nthe onset time of the ﬁrst note. \u0016tis always set to be the\nnotated tempo. \u001bt;\u0016a;\u001ba;\u0016c;\u001bc;\u000fnare manually set to\nsome appropriate value.\nUsing the method described in section 2 and 3, we can\ncompute the approximate optimal state conﬁguration ^sand\ncorresponding tempo process ^x\u000f;t. By reconstructing t,\nand hencey, from our estimated variables ^xn;t, we created\na simpliﬁed or “smoothed” interpretation. Figure 3 4 5\nshows some examples of observed IOIs and “smoothed”\nIOIs. In each ﬁgure, the top plot represents the tempo of\nthe original performance where the bottom plot represents\nthe “smoothed” version. There are many sections of the\n“smoothed” version that are horizontal lines, which is the\nbehavior of \u000b1constant tempo. The “peaks” in the bot-\ntom plot show the \u000b2slowing down and \u000b3speeding up\nexpressive gestures as well as the \u000b4accents. We want\nto see that if the “smoothed” version is approximating the\noriginal one with greatly reduced parametrization as well\nas capturing some of the “important events” and eliminates\n“unintentional variation”.\nWe use the “smoothed” version to render a MIDI per-\nformance with everything else unchanged. Which includes\nMIDI velocities/note length for each notes and pedaling.Figure 3. The original tempi and “smoothed” tempi\nof a performance of Schubert Piano Sonata D959, 1st\nmovement excerpt. (The x-axis represent the music time\nas in beats. The dotted line represents the original\nperformance. The normal line represents the “smoothed”\nversion. Lines with slope = 0represent state \u000b1; lines with\nslope>0represent state \u000b2; lines with slope <0represent\nstate\u000b3; lines with slope = +1represent state \u000b4)\nFigure 4. The original tempi and “smoothed” tempi of a\nperformance of Beethoven Piano Sonata Op.31 No.3, 1st\nmovement excerpt. (The lines have the same meaning as\nin Figure 3)\nFigure 5. The original tempi and “smoothed” tempi of a\nperformance of Chopin Etude Op.10 No.3 excerpt. (The\nlines have the same meaning as in Figure 3)played by worse similar better\nprofessional pianists (4.1) 23 54 31\nother musicians (4.2) 4 6 17\ndynamic experiment (4.3) 1 7 1\nTable 1. Results from the survey of asking 9 subjects\nabout their opinions on “smoothed” version. The total\n16 excerpts of the 3 sets of experiments are presented in\nrandom order to avoid bias.\nThen we perform a simple user study. We present both\nthe original version and the “smoothed” version to 9 par-\nticipants who were graduate level piano major students.\nThe subjects were presented with random ordering of the\ntwo versions of every excerpt. They are asked to choose\none from the following options: 1) version 1 is better; 2)\nversion 2 is better; 3) they are about the same. Although\nwhat we are really interested in is whether the “smoothed”\nversion is similar to the original performance, we design\nthe questionnaire this way to avoid putting bias towards\nchoosing similar in our subjects’ mind.\nFrom the 9\u000212 = 108 results that evaluated in this way.\nThe results are shown in table 1 Which shows many of the\ncases subjects think our “smoothed” version is at least on\npar with the original version.\n4.2 Improving a Performance\nThe second set of experiment demonstrates that our model\ncan provide a performance standard. If someone has a\nsense of musicality but lacks piano skills, our model may\nbe able to improve their performance. This experiment\ndiffer from the previous one because the amateur piano\nplayers play less professionally. So we are testing the\n“correcting” and “improving” abilities of our model rather\nthan “smoothing”. This data set contains several excerpts\nplayed by students majoring string performance who knew\nmusic well but didn’t have serious training in piano\nperformance. We run the exact same procedure as in\nthe ﬁrst set of experiment and ask participants the same\nquestions. From the results in table 1 we can see subjects\nthink the “smoothed” version is better more often than the\nexcerpts in 4.1, though we do not make inference on the\nlarger population.\n4.3 On Dynamics\nThe third set of experiment demonstrates that dynamics\ncan also be modeled with the conditional Gaussian\nframework.\nWe choose Beethoven sonata op.101 1st movement as\nour material. First we manually partition our music into\n3 monophonic voices. For each voice we have a series\nof MIDI velocities v1;v2;:::;vn. Our model for dynamic\nhas two types of behaviors \f1;\f2, in which dynamic can\nchange to a new value or starting a new second order\nsmooth progression.0 10000 20000 30000 40000 5000020 60 100\ndata[range, 3]\n0 10000 20000 30000 40000 5000020406080Figure 6. The original dynamics and “smoothed”\ndynamics of one voice of a performance of Beethoven\nSonata Op.101, 1st movement excerpt.(Red dots represent\nstate\f1; Black dots represent state \f2)\nDenote\nZn= (dn;en;fn)T\nWe deﬁne the dynamic behaviors\nZn=0\n@1 1 0\n0 1 1\n0 0 11\nAZn\u00001jRn\u00001=\f2\nZn\u0018N(0\n@\u0016d\n\u0016e\n\u0016f1\nA0\n@\u001b2\nd0 0\n0\u001b2\ne0\n0 0\u001b2\nf1\nA)jRn=\f2;Rn\u00001=\f1\nZn\u0018N(0\n@\u0016d\n0\n01\nA0\n@\u001b2\nd0 0\n0 0 0\n0 0 01\nA)jRn=Rn\u00001=\f1\nThen we relate the model to observations\nvn=Zn;d+\u000en\n\u000en\u0018N(\u0016\u000e;\u001b2\n\u000e)\nUsing the similar method described in section 3, we\ncompute the “smoothed” dynamics. Figure 6 shows an\nexample of the dynamics before and after the “smoothing”.\nWe use both “smoothed” onset times and dynamics to\nrender a new MIDI performance with everything else un-\nchanged. Then we ask the subjects the same questions.\nAgain, the majority think the “smoothed” version is at least\non par with the original performance.\n5. DISCUSSION\nAlthough there is no clear evidence showing “smoothed”\nperformances computed from our model are better than\noriginal ones, it is still interesting to see that people think\nthey are comparable. It suggests that our model under-\nstands the interpretation in a reasonable way.\nThe direct follow-up of this work is applying the model\nin accompaniment system. It is challenging to deal with\nsoloist’s unintentional activities [11]. We can see from the\nexperiments that our model can reduce the “performancenoise”. Also, learning the discrete parameters from our\nmodel may help with the score following problem since\nwe can model following strategies separately in different\nparts of music.\nIn visualization scenario, our current model is only a\n“toy version” since it only has a limited number of behav-\niors. For future work, we will explore more “devices” (i.e.\nmore discrete states) that match musicians’ intuitive ideas.\nEventually, we also want to use such models for expres-\nsive rendering problem. For ﬁne detail of performance, the\n“devices” may need to be more sophisticated than simple\nlinear models.\nWe look forward to see more generally useful applica-\ntions of this model framework as it develops.\n6. REFERENCES\n[1] Hiraga, R.; Bresin, R.; Hirata, K. & Katayose, H.:\nRencon 2004: “Turing Test for Musical Expression”\nProceedings of International Conference on New\nInterfaces for Musical Expression, in Proc. of NIME\n2004, pp. 120-123, 2004\n[2] G. Widmer, S. Flossmann, and M. Grachten, “YQX\nplays chopin”, AI Magazine , vol. 30, no. 3, pp. 35-48,\n2009.\n[3] T. Suzuki, The Second Phase Development of Case\nBased Performance Rendering System Kagurame, In\nProc. of the IJCAI-03 Rencon Workshop, 2003, pp. 17-\n25.\n[4] C. Raphael, “Music Plus One and Machine Learning\nMachine Learning”, Proceedings of the Twenty-\nSeventh International Conference ICML 2010.\n[5] R. Dannenberg, “An on-line algorithm for real-time\naccompaniment,” in Proceedings of the International\nComputer Music Conference, 1984. Int. Computer\nMusic Assoc., 1984, pp. 193 - 198.\n[6] J.D. Hamilton, “A new approach to the economic\nanalysis of nonstationary time series and the business\ncycle”. Econometrica, 57:357-384, 1989\n[7] R.H. Shumway, D.S. Stoffer, “Dynamic linear models\nwith switching”. J. Amer. Stat. Assoc., 86:763-769,\n1991\n[8] C.J. Kim, “Dynamic linear models with Markov-\nswitching.” J.Econometrics, 60:1-22, 1994\n[9] Z. Ghahramani and G. Hinton. “Variational learning\nfor switching state-space models”. Neural Computa-\ntion, 12(4):963-996, 1998.\n[10] C. Raphael, “A Mixed Graphical Model for Rhythmic\nParsing”, Proceed. of 17th Conf. on Uncertainty in\nArtif. Int.462–471, Morgan Kaufmann, 2001.\n[11] Y . Gu, C. Raphael, “Orchestral Accompaniment for\na Reproducing Piano”, Proceedings of the ICMC09,\nMontreal, Canada 501-504, 2009."
    },
    {
        "title": "Improving Audio Chord Transcription by Exploiting Harmonic and Metric Knowledge.",
        "author": [
            "W. Bas de Haas",
            "José Pedro Magalhães",
            "Frans Wiering"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417541",
        "url": "https://doi.org/10.5281/zenodo.1417541",
        "ee": "https://zenodo.org/records/1417541/files/HaasMW12.pdf",
        "abstract": "We present a new system for chord transcription from polyphonic musical audio that uses domain-specific knowledge about tonal harmony and metrical position to improve chord transcription performance. Low-level pulse and spectral features are extracted from an audio source using the Vamp plugin architecture. Subsequently, for each beat-synchronised chromagram we compute a list of chord candidates matching that chromagram, together with the confidence in each candidate. When one particular chord candidate matches the chromagram significantly bet- ter than all others, this chord is selected to represent the segment. However, when multiple chords match the chro- magram similarly well, we use a formal music theoreti- cal model of tonal harmony to select the chord candidate that best matches the sequence based on the surrounding chords. In an experiment we show that exploiting metri- cal and harmonic knowledge yields statistically significant chord transcription improvements on a corpus of 217 Bea- tles, Queen, and Zweieck songs.",
        "zenodo_id": 1417541,
        "dblp_key": "conf/ismir/HaasMW12",
        "keywords": [
            "domain-specific knowledge",
            "tonal harmony",
            "metrical position",
            "Vamp plugin architecture",
            "low-level pulse",
            "spectral features",
            "chromagram",
            "chord candidates",
            "confidence",
            "formal music theoretical model"
        ],
        "content": "IMPROVING AUDIO CHORD TRANSCRIPTION\nBY EXPLOITING HARMONIC AND METRIC KNOWLEDGE\nW. Bas de Haas\nUtrecht University\nW.B.deHaas@uu.nlJosé Pedro Magalhães\nUniversity of Oxford\njpm@cs.ox.ac.ukFrans Wiering\nUtrecht University\nF.Wiering@uu.nl\nABSTRACT\nWe present a new system for chord transcription\nfrom polyphonic musical audio that uses domain-speciﬁc\nknowledge about tonal harmony and metrical position to\nimprove chord transcription performance. Low-level pulse\nand spectral features are extracted from an audio source\nusing the Vamp plugin architecture. Subsequently, for\neach beat-synchronised chromagram we compute a list of\nchord candidates matching that chromagram, together with\nthe conﬁdence in each candidate. When one particular\nchord candidate matches the chromagram signiﬁcantly bet-\nter than all others, this chord is selected to represent the\nsegment. However, when multiple chords match the chro-\nmagram similarly well, we use a formal music theoreti-\ncal model of tonal harmony to select the chord candidate\nthat best matches the sequence based on the surrounding\nchords. In an experiment we show that exploiting metri-\ncal and harmonic knowledge yields statistically signiﬁcant\nchord transcription improvements on a corpus of 217 Bea-\ntles, Queen, and Zweieck songs.\n1. INTRODUCTION\nChord labels are an indispensable and ubiquitous aid for\nmodern musicians. Although classically trained perform-\ners still rely mainly on printed scores, describing in high\ndetail how a piece of music should be performed, the emer-\ngence of jazz, improvised, and popular music gave rise to\nthe need for more ﬂexible and abstract representations of\nmusical harmony. This led to a notational vehicle often re-\nferred to as a lead sheet. A lead sheet typically contains\nonly the melody of a composition accompanied with the\nessential harmonic changes denoted with chord labels. It\ncan be considered a rather informal map that guides the\nperformers and speciﬁes the boundaries of the musical play-\nground. Also, in music theory, music education, composi-\ntion, and harmony analysis, chord labels have proven to be\na convenient way of abstracting from individual notes in\na score. Hence, these days chord labels are omnipresent:\nthere are publishers that specialise in publishing lead sheets,\nand many lead sheets circulate on the internet.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\nSoundFeature \nextractionKey findingCreate chord \ncandidates Chord \nselectionChord \ntranscriptionFigure 1. A schematic outline of the MPT REEsystem.\nThe many possible applications of chord labels have\nsparked research focusing speciﬁcally on chords. Many\nMusic Information Retrieval (MIR) tasks, like similarity\nestimation, genre detection, or query by humming, can\nbeneﬁt from some reduction of the raw audio signal into\na manageable symbolic representation. Although much\nprogress has been made, multiple fundamental frequency\n(F0) estimation, the holy grail in polyphonic music tran-\nscription, might still be considered too unreliable and im-\nprecise for many MIR tasks. Chord transcription, which\ndeals with transforming polyphonic audio into musically\nfeasible symbolic annotations, has offered a welcome al-\nternative. For example, in automatic harmony analysis [8],\nand similarity estimation [5], chord labels are used as pri-\nmary data representation.\nIn this paper we present a novel system, named\nMPT REE,1that automatically transcribes chord labels from\na polyphonic musical audio source. This system is differ-\nent from most other chord transcription systems, e.g. [12],\nin that it does not rely on statistical learning. Although ma-\nchine learning has brought chord transcription (and MIR\nin general) many merits, we believe that there is a limit\nto what can be learned from musical data alone [6]. Cer-\ntain musical segments can only be annotated correctly\nwhen musical knowledge not exhibited in the data is taken\ninto account as well. Moreover, Hidden Markov Models\n(HMMs), frequently used to model the transitions between\nchords, model only the transition between a small number\nof subsequent chords, and have a bias towards sequences\nthey have been trained on. Our system, on the other hand,\nrelies on a knowledge-based model of tonal harmony. The\nHARM TRACE2harmony model [4] is explicitly designed\nfor modelling the relations between chords, also over a\nlong time span. In this paper we show how this harmony\nmodel can be employed to improve chord transcription.\nA global outline of the system is presented in Figure 1.\nWe start by brieﬂy reviewing some important literature in\nSection 2. Next, we give a complete outline of the MPT REE\n1(Musical) Model Propelled TRanscription of Euphonic Entities\n2Harmony Analysis and Retrieval of Music with Type-level Represen-\ntations of Abstract Chords Entitiessystem in Section 3. In Sections 4 and 5 we discuss the\nexperiments and results. Finally, we conclude the paper\nby recapitulating the main advantages and disadvantages\nof the MPT REEsystem, and highlight some directions for\nfuture research.\nContribution. In this paper we bridge the gap between\ntop-down symbolic music analysis and bottom-up audio\nfeature extraction. We show that exploiting metrical posi-\ntion and a model of tonal harmony yields signiﬁcant chord\ntranscription improvements on 217 songs by the Beatles,\nQueen, and Zweieck.\n2. RELATED WORK\nThe ﬁrst computational approaches to automatic chord tran-\nscription from musical audio emerged at the end of the\n1990s. The ﬁrst audio chord transcription system was de-\nveloped by Fujishima [3]. In general, the outline of Fu-\njishima’s system is not so different from the chord tran-\nscription systems nowadays developed and also no so dif-\nferent from the system presented here. First, chroma fea-\ntures [15] are extracted at every frame, representing the\nintensities of the twelve different pitch classes as found in\nthe spectrum. Next, the chroma vectors are matched with\nchord proﬁles; in Fujishima’s case this is done with an Eu-\nclidean distance. Although the used digital signal process-\ning parameters may vary, most approaches towards auto-\nmatic chord transcription use a chroma feature based rep-\nresentation and differ in other aspects, like chroma tuning,\nnoise reduction, chord transition smoothing, and harmon-\nics removal. For an elaborate review of related work on\nautomatic chord transcription we refer to Mauch [9].\nFrom 2008 on, chord transcription has received a con-\nsiderable amount of attention in the yearly benchmarking\nchallenge MIREX.3Each year, between 7 and 19 differ-\nent chord transcription algorithms were evaluated. In 2008,\nthe system of Bello and Pickens [1], which was the ﬁrst to\nsynchronise chroma vectors at every beat, performed the\nbest. The following year, Mauch et al. [10] presented a\nsystem that gave good results by structurally segmenting\na piece and combining chroma information from multiple\noccurrences of the same segment type. In 2010, Mauch et\nal. [11] improved their previous results by using an approx-\nimate note transcription technique. In 2011, the system of\nNi et al. [12], using only machine learning techniques, gave\ncomparable results.\n3. SYSTEM OUTLINE\nAn outline of the MPT REEsystem is shown in the ﬂowchart\nof Figure 2. First, we extract chroma features and beat lo-\ncations from the audio signal and synchronise the chroma\nfeatures at the beat positions (Section 3.1). The chroma\nfeatures are used to estimate the global key and possible\nmodulations in the musical audio (Section 3.4), and for\ncreating a sequence of chord candidate lists (Section 3.2).\nThese candidate lists contain the chords that match the\nchroma well a particular beat position. If there is a lot\n3http://www.music-ir.org/mirex/wiki/MIREX _HOME\nSound\nNNLS Chroma \nBass/Treble\nChroma ChromaBeat Tracker\nBeats\nBeat-Sync\nB/T Chroma Beat-Sync\nChroma\nKrumhansl\nProfilesBeat-Sync\nKey-strength \nModulation\nDetectionChord \nStructuresChord Probability \nLists\nSegmentation &\nGrouping\nHarmony\nModelChord Selection\nChord \nTranscriptionFeature extraction\nChord candidate \nlists creation\nKey finding\nHarmony based chord selectionFigure 2. A schematic outline of the MPT REEsystem. The\nboxes with dotted lines denote the high level modules as\noutlined in Figure 1.\nof uncertainty in the data, these lists might contain mul-\ntiple chords; however, if there is a strong match between\nthe spectrum and one particular chord candidate, the lists\nwill contain a single candidate. Subsequently, the sequence\nof chord candidate lists is segmented (Section 3.5). Fi-\nnally, the best matching sequence per segment is selected\nby expanding all possible sequences, and preferring the se-\nquence with the fewest harmony errors (Section 3.6).\n3.1 Feature extraction front-end\nOur work depends heavily on the Vamp plugin architec-\nture.4As feature extraction front-end we rely on the\nNNLS Chroma Vamp plugin5developed by Mauch [9].\nThe NNLS Chroma plugin transforms an audio signal into\ntwo 12-dimensional chroma vectors representing the har-\nmonic content at each frame. The ﬁrst chroma vector\n(bass) represents the low notes and emphasises the lower\nfrequencies, while the second (treble) represents the higher\nnotes, emphasising higher frequencies. The idea behind\nthis separation is to model the prominent role of the bass\nnote in chord transcription. We present a brief overview of\nthe most important properties and parameters of the NNLS\nplugin. For speciﬁc signal processing details we refer to\nMauch [9].\nWe use the sonic-annotator6(version 0.6) as Vamp host,\nand sample the audio tracks at 44,100 Hz. If the audio ﬁle\ncontains two stereo channels, the mean of both channels is\nused for analysis. We set the sonic-annotator to use a Hann\nwindow of 16,384 samples and a hop size, i.e. the amount\nof samples that overlap between two subsequent frames, of\n2,048 samples. Next, the spectrogram is calculated at each\nframe using a discrete-time Fourier transform and mapped\nto a spectrogram with bins that are linearly spaced in log-\nfrequency (similar to a constant-Q transform). The NNLS\n4http://www.vamp-plugins.org\n5http://isophonics.net/nnls-chroma\n6http://omras2.org/SonicAnnotatorC:Maj 1 0 0 0 1 0 0 1 0 0 0 0\nD:Min\n0 0 1 0 0 1 0 0 0 1 0 0\nC C] D E[ E F F] G G] A B[ B\nTable 1. A binary chord structure of a C major and\na D mi-\nnor chord, which are matched against the chroma features.\n0.93 C7\n0.96 Am\n0.94 G 0.97 C 0.94 Bm\n. . . 1.00 C 1.00 F\n1.00 Gm 1.00 Em 1.00 F 1.00 B . . .\n. . . 1 2 3 4 1 2 . . .\nTable 2. An excerpt of a sequence of chord candidate\nlists.\nThe number to the left of the chord label represents its\nnormalised Euclidean distance to the current beat aligned\nchroma vector. Below the candidate lists the beat position\nwithin the bar is printed.\nChroma Vamp plugin also accounts for tuning differences\nin the audio signal. Also, the NNLS plugin accounts for\nharmonics other then the F0 of chord notes by estimat-\ning which pitch activation generates an interference pattern\nthat best matches the partials found in a spectrum.\n3.2 Beat-synchronous chord probability estimation\nAfter obtaining bass and treble chroma features, we beat-\nsynchronise them by averaging the feature vectors between\ntwo beats. For this, we obtain a list of beat positions by\nusing the Queen Mary, University of London, Bar and Beat\nTracker plugin [2].7Besides beat timestamps, this beat\ntracker also outputs the position of the beat inside the bar.\nTo estimate the probability of a particular chord sound-\ning at a beat position, we assemble a dictionary of chords\nthat we expect to occur in the music. A chord is repre-\nsented as a binary 12-dimensional vector in which the si-\nmultaneously sounding pitch classes are denoted with a 1\n(see the examples in Table 1). This allows us to model\nany possible chord within one octave. Currently, we use a\nlimited chord dictionary with three chord structures: ma-\njor, minor, and dominant seventh. We chose these three\nchords because they map nicely to the chord classes used\nby the H ARM TRACE harmony model. In H ARM TRACE ,\nchords are categorised in four classes: major chords, minor\nchords, dominant seventh chords, and diminished seventh\nchords (see [4, Chapter 4] for details). However, because\ndiminished seventh chords are not very common in pop\nmusic, we ignored this class in this study. The bass note of\nthe chord is modelled with an additional 12-dimensional\nvector containing only one pitch for the bass note, to match\nthe bass chroma vector as outputted by the NNLS chroma\nplugin. Next, we generate the chord dictionary by cycli-\ncally rotating all chord structures for all twelve semitones,\nyielding 48 different chord candidates, and a “no chord”\nstructure containing only 0’s.\nHaving a matrix of beat-synchronised bass and treble\nchromagrams and a chord dictionary, we estimate the prob-\nability of a chord sounding at a particular beat by calcu-\n7http://vamp-plugins.org/plugin-doc/\nqm-vamp-plugins.html\\#qm-barbeattrackerPiece\nDom\nDom\nDom\nV\nB[.\n...................\n...................\n.................................................................................\nSub\nIIm\nFm7.\n..................\n........................................................................\nVd=IIm\nVIm\nCm.\n............................................................................\nVd=VIm\nIIIm\nGm.\n.........................................................................................\n.........................................................................\n..................................................................................\n...................................................................................................................................................................................................................\nSub\nIV\nA[.\n...........................................................\nA[.\n............................................................\n...........................................................................\nV=IV7\nI7\nE[7.\n............................\n...........................................................................\n....................................................................................................................................................................................................................\n.......................................................................................................................................................................................................................................................................................................................................................\nTon\nI\nE[.\n...................\n...................\n................................\nDom\nDom\nDom Maj\nDom Min\nVII[\nInserted.\n...................\n.........................\n................................................................................\nSub\nIV\nA[.\n...........................................................\nA[.\n............................................................\n...................\n.................................................................................\n..................................................................................................................................................\nSub\nIIm\nInserted.\n...................\n..................................................................\nVd=IIm\nVIm\nCm.\n...............................\n...................................................................\n...................................................................................................................................................\n.......................................................................................................................................................................................................................................................................................................................................................\nFigure 3. An excerpt of the H ARM TRA\nCEanalysis of\nThe long and winding road by the Beatles (of which the\nground-truth chord annotations were used for parsing).\nThe Vd=Xrepresents a diatonic ﬁfth succession, and a\nV=X7denotes a secondary dominant.\nlating the Euclidean distance between the chord structures\nand the chroma feature. These distances are calculated for\nevery chord candidate at every beat. Next, we sort the\nchord candidates by descending Euclidean distance. To ob-\ntain a relative measure of the ﬁt between a chord candidate\nand the chroma vector in the range [0;1], the distances are\nnormalised by dividing them by distance of the best match-\ning chord candidate. In case the information in the spec-\ntrum clearly favours a certain chord candidate, the initial\ndifferences in normalised distances will be relatively large\nand will decrease quickly after the head position. Hence,\nwe can use these differences as a measure of relative chord\ncandidate preference. If this preference is very strong, the\ntop chord candidate will be selected to represent that beat.\nIf this preference is less pronounced, we use the H ARM -\nTRACE harmony model to decide which of the chord can-\ndidates would make most sense, harmonically. Typically,\nthis results in a sequence of chord candidates similar to the\none shown in Table 2. The selection is performed by cut-\nting off the chord candidate list at a ﬁxed distance. The\ncut-off value is an important parameter to the model, in-\nﬂuencing both the speed and the transcription quality of\nthe system. After some experimentation we found that a\ncut-off value of 0: 9 gives good results.\n3.3 A model of tonal harmony\nGiven a list of chord candidates, we select a harmonically\nsensible sequence by exploiting a formal model of tonal\nharmony. This model, which is elaborately explained by\nDe Haas [4], takes a sequence of symbolic chord labels as\ninput and automatically derives a tree structure explaining\nthe function of the chords in their tonal context. Figure 3\ndepicts an excerpt of the harmony analysis of The long and\nwinding road by the Beatles.\nExtending the ideas of Rohrmeier [13], a piece is mod-\nelled as a sequence of tonic and dominant nodes (Ton and\nDom, respectively) that represent the global patterns of har-\nmonic tension and release. Every Dom node can be pre-\nceded by a subordinate sub-dominant (Sub) building up\nthe tension towards the dominant. Finally, a branch will\nalways end in a scale degree node, representing the rela-\ntion between the actual chord and the key of the piece, and\nthe leaves of the tree show the actual input chord labels. On\nthe path from functional annotation (Ton, Dom, and Sub) to\nchord label, various harmonic annotations, like secondarydominants, tritone substitutions, diatonic ﬁfth chains, di-\nminished seventh chord transpositions, etc., can occur, ex-\nplaining the role of a chord label in its tonal context. If a\nsequence does not match the harmonic speciﬁcation, like\nin the ﬁrst phrase of Figure 3, an input chord label is au-\ntomatically deleted or inserted to match the speciﬁcation.\nHence, for a sequence of chords we can always derive an\nautomatic harmonic analysis.\n3.4 Key-ﬁnding\nTo be able to use the H ARM TRACE harmony model for\nthe selection of chord sequences that are music theoreti-\ncally realistic, we require information about the key of the\npiece. To fulﬁl this requirement, we present a key-ﬁnding\nalgorithm inspired by the ideas of Temperley [14, Chapter\n7] and Krumhansl [7, Chapter 4]. Again, for feature extrac-\ntion we depend on the NNLS chroma Vamp plugin, which\nallows for exporting different kind of audio features. For\nkey-ﬁnding we export a single tuned chroma feature with-\nout the NNLS pitch activation estimation.\nTo estimate the key of a piece and the possible mod-\nulations, we use a key-proﬁles based algorithm. A key-\nproﬁle is a 12 value vector representing the stability of the\ntwelve pitch classes relative to a given key. The values\nof these proﬁles are based on empirical measurements of\nKrumhansl and Kessler [7], in which subjects were asked\nto rate how well a pitchclass ﬁts a previously established\ntonal context on a 1 to 7 scale. Given the major and minor\nkey proﬁles, a key-strength table Kis created. This table\nstores the estimated strength of all 24 keys at every beat\nposition. The key strength is estimated by calculating the\nPearson correlation coefﬁcient, r. A value of rclose to 0\nindicates that there is little to no relation between the key-\nproﬁle and chroma vector, whereas a value close to 1 or \u00001\nindicates a positive or negative linear dependence between\nthe key-proﬁle and the chroma vector, respectively.\nMatching the key-proﬁles at every beat does not yet re-\nsult in the desired key assignment; because beat size seg-\nments are rather small, key changes occur too often. To\novercome this problem, we use a simple dynamic program-\nming algorithm based on the algorithm in [14] to smooth\nthe key changes. We create a table Mstoring the cumula-\ntive key-strength of every key at every beat, and minimise\nthe number of modulations. Switching to another key, i.e.\nchanging the column jinM, is penalised. This behaviour\nis captured in the following recursive formula:\nM[0;j] = K[0;j]\nM[i;j] = max\u001aM[i\u00001;j]+K[i;j];\nM[i\u00001;j]+K[i;k]+p;\nwherefkj8x:K[i;x]6K[i;k]g\nHere, Mstores the cumulative key-strength for every ith\nbeat and every jthkey. Similarly, Kstores the correlation\nbetween every ithchroma vector and jthkey proﬁle. kde-\nnotes the index of the best matching key at beat i. The\nparameter pspeciﬁes the modulation penalty. We found a\nvalue of 1 for pto give good results. Finally, we obtain thedeﬁnite key assignment by keeping track of the maximum\ncumulative key-strength at every beat, and constraining the\nkey segments to be at least 16 beats long.\n3.5 Segmentation and grouping\nGiven a sequence of chord candidate lists, we analyse all\npossible chords sequence combinations with H ARM TRACE\nand select the simplest analysis with the least amount of er-\nrors. However, the number of possible combinations grows\nexponentially with the number of candidate lists. Hence, it\nis vital to split our sequence of chord candidate lists into\nsmaller, musically meaningful segments.\nAlso, from a musical point of view, it is unrealistic to\nexpect chords to change at every beat. Therefore, we re-\nduce the space of analysed sequences by merging subse-\nquent chord candidate lists that contain the same chords.\nThe candidate lists are merged by taking the intersection\nbetween two adjacent lists, if the intersection contains at\nleast one chord. In this procedure we take into account\nthat chords are more likely to change on strong metrical\npositions by adding two additional constraints: when two\ncandidate lists are merged, the ﬁrst and leftmost list must\nbe positioned either at the ﬁrst or third beat of the bar. The\nmerging procedure is executed sequentially, and merged\ncandidate lists can be merged again with the subsequent\nchord candidate list. For example, if the candidate lists at\nbeat position 1 and 2 are merged, the merged list can again\nmerge with beat position 3 if the intersection contains at\nleast one chord. Finally, the probabilities of the merged\ncandidate lists are summed, and the lists are sorted by de-\nscending probability.\nSubsequently, the sequence of chord candidate lists is\nsegmented on the basis of the estimated key, resulting in\nsegments that contain only a single key assignment. Nev-\nertheless, these sequences are still rather long for analysing\nall possible combinations. Within the H ARM TRACE har-\nmony model, a piece is viewed as a collection of tonics\n(Ton) and dominants (Dom) nodes. Hence, from the pars-\ning point of view, splitting a chord sequence into segments\nthat match the subtrees rooted by a Ton orDom seems\nnatural. Because local information about the key is avail-\nable, we can calculate the key-relative scale degrees of the\nchords and split a sequence at every beat where a IorV\nscale degree occurs in a chord candidate list. This gives\nus sequences that are short, but still musically meaning-\nful. In case our key-ﬁnding method is off the mark, and\nwe still end up with a rather long sequence, we enforce the\nsequences to be no longer than 12 chords, and expand into\nno more than 30 different candidate sequences.\n3.6 Chord selection by parsing\nNow that we have access to both a segmented sequence\nof chord candidate lists and local key information, we are\nready to apply the H ARM TRACE harmony model. For ev-\nery segment we parse all possible combinations of chord\nsequences and select the sequence that has the lowest error-\nratio. The error-ratio is the number of insertions and dele-\ntions of the error-correcting parser divided by the numberof chords. When two sequences have the same error-ratio,\nwe select the most simple solution by picking the sequence\nthat returns the smallest parse tree. In case the parse tree\nsize is also identical, we select the sequence returning the\nparse tree of least depth.\nThe harmony model used in MPT REEis not the exact\nsame model as the one described by De Haas [4, Chap-\nter 4]. The original H ARM TRACE harmony model exhibits\na bias towards jazz harmony. Therefore, we made several\nadaptations, but the majority of the speciﬁcations remained\nunchanged. The original harmony model was designed to\ndo an automatic harmonic analysis of a chord sequences\nand could explain a vast amount of exotic harmonic phe-\nnomena. Within the pop dataset on which the MPT REEsys-\ntem is evaluated, some of these speciﬁcations are unnec-\nessary. Hence, we remove some of the speciﬁcations ac-\ncounting for jazz-speciﬁc chord changes.8Furthermore,\nwe add two rules that account for some blues phenom-\nena.9The Haskell code of both the H ARM TRACE models\nand the MPT REEsystem is freely available online.10\n4. EXPERIMENTS\nTo measure the effect of the various modules on chord\ntranscription performance we evaluate four different ver-\nsions of the system described before. The simplest system,\nnamed SIMPLE , always selects the chord that best matches\nthe bass and treble chroma vectors. The second system,\nGROUP , also picks the best matching chord candidate, but\ndoes incorporate the grouping as described in Section 3.5.\nThe third system is the full MPT REEsystem, including key-\nﬁnding. Finally, we include a fourth system, MPT REEkey,\nto measure the effect of the key-ﬁnding. MPT REEkeydoes\nnot use key-ﬁnding, but instead uses ground-truth key an-\nnotations [10]. All systems are implemented in the func-\ntional programming language Haskell and compiled using\nthe Glasgow Haskell Compiler, version 7.4.1.\nWe evaluate the quality of an automatic chord transcrip-\ntion by comparing it to a transcription of the same piece\nmade by a human expert. We evaluate our system on 179\nsongs from 12 Beatles albums, 20 Queen songs, and 18\nZweieck songs [10].11The chord vocabulary for the MIREX\nevaluation is limited to 24 major and minor chords aug-\nmented with a “no chord” label, to be used for silence or\nnon-harmonic passages, for instance. In accordance with\nMIREX, we also use these 25 classes. The translation from\nthe three chord classes of H ARM TRACE to major and mi-\nnor chords is trivial: chords of the major and dominant\nclass are classiﬁed as major, and chords of the minor class\nare classiﬁed as minor.\nTypically in MIREX, the relative correct overlap (RCO)\nis used as a measure of transcription accuracy. The RCO\nis deﬁned as the total duration of correctly overlapping\n8The speciﬁcations with numbers 20, 21, and 22 were removed.\n9Allowing dominant seventh chords at the IVandIscale degree to\nfunction respectively as sub-dominant and tonic, to be precise.\n10http://hackage.haskell.org/package/HarmTrace-2.0\n11http://isophonics.net/content/\nreference-annotationsSIMPLE GROUP M P\nTREE MPT REEkey\nRCO 0:688 0: 736 0: 739 0: 741\nRunning time 5m1\ns 5m9s 10m23s 7m37s\nTable 3. The relative correct overlap and the running times\nfor the four evaluated chord transcription systems.\nchords divided by the total duration of the song. Both the\nground-truth and the automatic chord transcription consist\nof a chord label and an accompanying onset and offset\ntime-stamp. We approximate the RCO by sampling both\nthe ground-truth and the automatic annotations every 10ms\nand dividing the number of correctly annotated samples by\nthe total number of samples.\n5. RESULTS\nWe have compared the MPT REE,MPT REEkey,GROUP , and\nthe baseline SIMPLE system on 217 songs of the Beatles,\nQueen, and Zweieck. All runs were performed on the same\nIntel Core i7-2600 Processor running at 3.40GHz. The\nmeasured differences in RCO and running times are dis-\nplayed in Table 3.\nWe tested whether the differences in RCO are statis-\ntically signiﬁcant by performing a non-parametric Fried-\nman test12with a signiﬁcance level of a=0:05. The\nFriedman ANOV A is chosen because the underlying dis-\ntribution of the RCO data is unknown, and, in contrast to\na regular ANOV A, the Friedman does not assume a spe-\nciﬁc distribution of variance. To determine which pairs of\nmeasurements differ signiﬁcantly, a post-hoc Tukey HSD\ntest is conducted. Within the MIREX challenge the same\nstatistical procedure is followed. There are signiﬁcant dif-\nferences between the four systems, c2(3;N=217) = 339,\np<0:0001. Not all pairwise differences between systems\nare statistically signiﬁcant; the difference between GROUP\nand MPT REE, and between MPT REEand MPT REEkeyare\nnot signiﬁcant. All other pairwise differences (including\nthe difference between MPT REEkeyand GROUP ) are statis-\ntically signiﬁcant.\nConsidering the differences between the MPT REEkey\nsystem and the SIMPLE and GROUP systems, we can con-\nclude that using the H ARM TRACE harmony model for\nchord candidate selection improves chord transcription\nperformance, if correct key information is available. This\ndifference in performance cannot be attributed to the merg-\ning function described in Section 3.5 alone. However,\nclearly a lot of the performance gain must be attributed to\nthis merging function. Hence, we can conclude that forcing\nchords not to change often and mainly at strong metrical\npositions improves transcription performance. Although\nthe difference between MPT REEanMPT REEkeyis not sta-\ntistically signiﬁcant, the errors in the key-ﬁnding do have\nan effect on the transcription performance, since the differ-\nence between GROUP and MPT REEis not, but the difference\nbetween GROUP and MPT REEkeyis statistically signiﬁcant.\nThe running times as shown in Table 3 exclude the time\n12All statistical tests were performed in Matlab 2011a.taken by the Vamp feature extraction plugins. The results\nshow a trade-off between transcription performance and\ncomputation time. However, the running times are accept-\nable, less than 3 seconds per song on average.\n6. DISCUSSION\nIn this paper we aim at bridging the gap between bottom-\nup audio feature extraction and top-down symbolic mu-\nsic analysis. We demonstrate in a proof-of-concept how\nautomatic chord transcription can be improved by using\ndomain-speciﬁc knowledge about the metrical structure and\ntonal harmony. For feature extraction we rely on the NNLS\nchroma and the Bar and Beat Tracker Vamp plugin. We\nshow that preferring harmonically valid combinations of\nchords yields better chord transcriptions than just picking\nthe best matching chord at each beat, even after smooth-\ning the chord changes with a merging function. This result\nis good, especially if we consider that we have only con-\nnected the different technologies without extensively tun-\ning their parameters.\nIt is difﬁcult to compare the results of this paper with\ncurrent the state-of-the-art in an absolute manner; this must\nbe done in a next iteration of the MIREX challenge. The\ndataset used in this paper closely resembles the one used\nin MIREX in 2010 and 2011. However, although the same\nground-truth is used, many different remastered editions\nof the Beatles and Queen songs exist, and some editions\nare known to deviate from these ground-truth annotations.\nWe used the LabROSA script to improve the alignment be-\ntween our Beatles corpus and the ground-truth,13but it is\nhard to tell whether the results in Section 4 have been inﬂu-\nenced by remastering artifacts. However, if this is the case,\nthe results of all compared systems are affected equally.\nIn the 2011 edition of MIREX, all systems were eval-\nuated as described in Section 4, yielding RCO values be-\ntween 0: 126 and 0: 829, and a deliberately over-ﬁtted re-\nsult yielding an RCO of 0: 976. Clearly, a system with\na model trained in this manner will very likely perform\npoorly on unseen data. All algorithms that returned an\nRCO above 0: 740 were HMM-based machine learning ap-\nproaches, and it is unclear how much they have over-ﬁtted\non the used dataset. The chances that the H ARM TRACE\nharmony model is over-ﬁtting the used dataset are very\nlow. After all, candidate chord sequences are not selected\nbased on how often they occur in a training sample, but\nonly based on whether they follow the general rules of\ntonal harmony. Another beneﬁt of the knowledge-based\napproach presented in this article, is that we can analyse\nwhy certain chord sequences are preferred over others and\nreason about whether these choices are justiﬁed. An HMM\nremains a black box, which does not provide insights into\nthe choices made.\nNevertheless, there is still room for improvement. Per-\nhaps that using different signal processing parameters, or\ndifferent plugins improve the results. Moreover, we ex-\npect that carefully adjusting the parameters and tailoring\n13http://labrosa.ee.columbia.edu/matlab/beatles _\nfprint/the modules to maximise their interoperability will result\nin an increase of performance. Also, Mauch et al. [9] suc-\ncessfully improved chord transcription performance by av-\neraging the chroma vectors of segments that were classiﬁed\nas having very similar harmonies. Such a technique could\npossibly improve the results in the MPT REEsystem as well.\nWe have shown that connecting state-of-the-art low-level\nfeature extraction methods to high-level symbolic knowl-\nedge systems offers new capabilities to boost the analysis\nand retrieval of musical audio. We also expect similar com-\nbinations to be able to improve other common MIR related\ntasks, such as cover-song ﬁnding, music transcription, and\nstructural analysis.\nAcknowledgements We thank Anja V olk and Remco\nC. Veltkamp for providing comments on earlier drafts of\nthis article. W. Bas de Haas is supported by the Nether-\nlands Organization for Scientiﬁc Research, NWO-VIDI\ngrant 276-35-001, and José Pedro Magalhães by EPSRC\ngrant number EP/J010995/1.\n7. REFERENCES\n[1] J.P. Bello and J. Pickens. A robust mid-level representation\nfor harmonic content in music signals. In ISMIR Proceedings,\npages 304–311, 2005.\n[2] M.E.P. Davies and M.D. Plumbley. Context-dependent beat\ntracking of musical audio. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on, 15(3):1009–1020, 2007.\n[3] T. Fujishima. Realtime chord recognition of musical sound:\nA system using common Lisp music. In ISMIR Proceedings,\npages 464–467, 1999.\n[4] W.B. de Haas. Music information retrieval based on tonal\nharmony. PhD thesis, Utrecht University, 2012.\n[5] W.B. de Haas, J.P. Magalhães, F. Wiering, and R.C.\nVeltkamp. HarmTrace: Improving harmonic similarity es-\ntimation using functional harmony analysis. In ISMIR Pro-\nceedings, 2011.\n[6] W.B. de Haas and F. Wiering. Hooked on music information\nretrieval. Empirical Musicology Review, 5(4):176–185, 2010.\n[7] C.L. Krumhansl. Cognitive Foundations of Musical Pitch.\nOxford University Press, USA, 2001.\n[8] J. P. Magalhães and W. B. de Haas. Functional Modelling\nof Musical Harmony—an Experience Report. In Proceedings\nof the International Conference on Functional Programming,\npages 156–162, 2011.\n[9] M. Mauch. Automatic chord transcription from audio using\ncomputational models of musical context. PhD thesis, Queen\nMary University of London, 2010.\n[10] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Kolozali, D. Tidhar, and M. Sandler. Omras2 metadata\nproject 2009. In ISMIR Proceedings, 2009.\n[11] M. Mauch and S. Dixon. Approximate note transcription for\nthe improved identiﬁcation of difﬁcult chords. In ISMIR Pro-\nceedings, pages 135–140, 2010.\n[12] Y . Ni, M. McVicar, R. Santos-Rodriguez, and T. De Bie. An\nend-to-end machine learning system for harmonic analysis\nof music. Audio, Speech, and Language Processing, IEEE\nTransactions on, 20(6):1771–1783, 2012.\n[13] M. Rohrmeier. Towards a generative syntax of tonal harmony.\nJournal of Mathematics and Music, 5(1):35–53, 2011.\n[14] D. Temperley. The Cognition of Basic Musical Structures.\nCambridge, MA, MIT Press, 2001.\n[15] G.H. Wakeﬁeld. Mathematical representation of joint time-\nchroma distributions. In Conference on Advanced Signal\nProcessing Algorithms, Architectures, and Implementations,\npages 637–645, 1999."
    },
    {
        "title": "Building Musically-relevant Audio Features through Multiple Timescale Representations.",
        "author": [
            "Philippe Hamel",
            "Yoshua Bengio",
            "Douglas Eck"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416530",
        "url": "https://doi.org/10.5281/zenodo.1416530",
        "ee": "https://zenodo.org/records/1416530/files/HamelBE12.pdf",
        "abstract": "Low-level aspects of music audio such as timbre, loud- ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and re- quire a better representation of time dynamics. For var- ious music information retrieval tasks, one would benefit from modelling both low and high level aspects in a uni- fied feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale fea- tures. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for auto- matic tag annotation.",
        "zenodo_id": 1416530,
        "dblp_key": "conf/ismir/HamelBE12",
        "keywords": [
            "timbre",
            "loudness",
            "pitch",
            "melody",
            "harmony",
            "phrasing",
            "rhythm",
            "time dynamics",
            "feature extraction",
            "auto-tag annotation"
        ],
        "content": "BUILDING MUSICALLY-RELEVANT AUDIO FEATURES THROUGH\nMULTIPLE TIMESCALE REPRESENTATIONS\nPhilippe Hamel, Yoshua Bengio\nDIRO, Université de Montréal\nMontréal, Québec, Canada\n{hamelphi,bengioy}@iro.umontreal.caDouglas Eck\nGoogle Inc.\nMountain View, CA, USA\ndeck@google.com\nABSTRACT\nLow-level aspects of music audio such as timbre, loud-\nness and pitch, can be relatively well modelled by features\nextracted from short-time windows. Higher-level aspects\nsuch as melody, harmony, phrasing and rhythm, on the\nother hand, are salient only at larger timescales and re-\nquire a better representation of time dynamics. For var-\nious music information retrieval tasks, one would beneﬁt\nfrom modelling both low and high level aspects in a uni-\nﬁed feature extraction framework. By combining adaptive\nfeatures computed at different timescales, short-timescale\nevents are put in context by detecting longer timescale fea-\ntures. In this paper, we describe a method to obtain such\nmulti-scale features and evaluate its effectiveness for auto-\nmatic tag annotation.\n1. INTRODUCTION\nFrame-level representations of music audio are omnipresent\nin the music information retrieval (MIR) ﬁeld. Spectro-\ngrams, mel-frequency cepstral coefﬁcients (MFCC), chro-\nmagrams and stabilized auditory images (SAI) are just a\nfew examples of features that are typically computed over\nshort frames. It has been shown that using frame-level fea-\ntures aggregated over time windows on the scale of a few\nseconds yields better results on various MIR tasks [2] than\napplying learning algorithms directly on frame-level fea-\ntures. However, the aggregation of frame-level features,\nalso known as the bag-of-frames approach, does not model\nthe temporal structure of the audio beyond the timescale\nof the frames. A simple method to get some informa-\ntion about short-time dynamics is to use the derivatives of\nthe frame-level features. However, this method does not\nyield a representation that can model much longer tempo-\nral structure. Some alternative techniques to the bag-of-\nframes approach inspired by speech processing rely on the\nmodelization of the temporal structure with models such as\nHMMs [12]. A representation that could jointly model the\nshort-term spectral structure and long-term temporal struc-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.ture of music audio would certainly improve MIR systems.\nIn this paper, we take a step to improve the bag-of-\nframes approach by combining a set of features computed\nover different timescales. The idea is that longer timescale\nfeatures, by modelling temporal structure, will give some\ncontext to the shorter timescale features which model spec-\ntral structure. The combination of multiple timescales could\nyield a general representation of the music audio that would\nbe useful to solve various MIR tasks relying on audio fea-\ntures. In particular, we will show that a simple classiﬁer\ntrained over a multi-scale spectral representation of music\naudio obtains state-of-the-art performance on the task of\nautomatic tag annotation. The multi-timescale representa-\ntion that we introduce in this paper has the advantage of\nbeing a general purpose scalable method that requires no\nprior knowledge of the spectral or temporal structure of\nmusic audio.\nThe paper is divided as follows. First, in Section 2, we\ndescribe the current state of the research on multi-scale\nrepresentations. Then, in Section 3, we describe our exper-\nimental setup. In Section 4 we discuss our results. Finally,\nwe conclude in Section 5.\n2. MULTI-SCALE REPRESENTATIONS\nUsing representations at multiple scales allows much ﬂex-\nibility to model the structure of the data. Multi-scale rep-\nresentations offer a natural way to jointly model local and\nglobal aspects, without having prior knowledge about the\nlocal and global structures.\nThe idea of considering multiple scales is not new. It\nhas been applied widely in the machine vision ﬁeld. For\nexample, pyramid representations [3] and convolutional net-\nworks [8] are just a few examples of multi-scale represen-\ntations.\nRecently, the MIR community as shown interest in tak-\ning advantage of multi-scale representations. Here are a\nfew examples of recent work that has been done on multi-\nscale representation of music audio. Multi-scale spectro-\ntemporal features inspired by the auditory cortex have been\nproposed in [11]. These features are used to discriminate\nspeech from non-speech audio in a small dataset. In [10],\nstructural change of harmonic, rhythmic and timbral fea-\ntures are computed at different timescales. This repre-\nsentation is used to build meaningful visualizations, al-\nthough it has not been applied to music audio classiﬁca-DFT\nMel\nscaling\nPCA\nwhitening\nT emporal \npooling\nDFT\nMel\nscaling\nPCA\nwhitening\nT emporal \npooling\nDFT\nMel\nscaling\nPCA\nwhitening\nT emporal \npooling\nDFT\nMel\nscaling\nPCA\nwhitening\nT emporal \npooling\nFigure 1: PMSCs are computed in parallel at different timescales.\ntion. In [5], boosting is applied on features at different\ntimescales to optimize music classiﬁcation. Although the\nvalidity of this method is demonstrated, it does not obtain\nstate-of-the-art results on the CAL500 dataset. Learning\nfeatures jointly at different timescales obtains state-of-the-\nart performance for automatic tag annotation [6]. How-\never this model still depends on a bag of short timescale\nframes to build the long timescale representation, limiting\nthe potential to model temporal dynamics. Deep convo-\nlutional networks have been applied to genre recognition\nin [9]. The authors show that classiﬁcation performance\nfor genre recognition and artist identiﬁcation can be im-\nproved by using an unsupervised deep convolutional rep-\nresentation instead of raw MFCC features. Unfortunately,\nthe results presented in this work are not comparable to\nother work in the ﬁeld. In [1], scattering representations of\nMFCCs have been shown to improve music genre classiﬁ-\ncation. The performance reported are comparable to other\nresults reported on the same dataset. A bag-of-system ap-\nproach have been proposed in [4] to combine models at\nvarious time resolutions.\n3. EXPERIMENTAL SETUP\nWe used the TagATune dataset [7] in our experiments.\nTagATune is the largest dataset for music annotation avail-\nable for research that provides audio ﬁles. It contains over\n20,000 30-second audio clips sampled at 22050 Hz, and\n160 tag categories. Our train, valid and test datasets con-\ntained 14660, 1629 and 6499 clips respectively.\nWe used the area under the ROC curve (AUC) averaged\nover tags (AUC-tag) as our main performance measure.\nWe also use the AUC averaged over clips (AUC-clip) and\nprecision at k for comparison with other models. For more\ndetails on these performance measures, see [6].\n3.1 Multi-scale Principal Mel-Spectrum Components\nIn our experiments, we used Principal Mel-Spectrum Com-\nponents (PMSCs) [6] as base features. PMSCs are general\npurpose spectral features for audio. They are obtained by\ncomputing the principal components of the mel-spectrum.\nPMSCs have shown great potential for the task of music\ntag annotation.\nMoreover, it is quite simple to compute PMSCs at dif-\nferent timescales. The time length of the frame used tocompute the discrete Fourier transform (DFT) determines\nthe timescale of the features. To obtain multi-timescale\nfeatures, we simply need to compute a set of PMSCs over\nframes of different lengths (Figure 1). The smallest DFT\nwindow we used was 1024 samples (46.4 ms). The size\nof the timescales grew exponentially in powers of 2 (1024,\n2048, 4096, etc.).\nWe keep the same number of mel coefﬁcients for all\ntimescales. Thus, longer frames are more compressed by\nthe mel-scaling, since the dimensionality of the output from\nthe DFT is proportional to the frame’s length. However,\nmel-scaling is more important for high frequency bins, while\nlow-frequency bins are barely compressed by the mel-scaling.\nFortunately, these high frequencies are already represented\nin shorter timescales where they are less compressed. In\nour experiments, we used 200 mel energy bands.\nIn our experiments, we found that using the log ampli-\ntude of the mel-spectrum yields better performance than\nusing the amplitude.\nPCA whitening is computed and applied independently\non each timescale. In order to circumvent memory prob-\nlems when computing the PCA, we limit the number of\nframe examples by randomly sub-sampling frames in the\ntraining set. We typically used around 75 000 frames to\ncompute the PCA. It is also worth noting that we preserve\nall the principal components since we don’t use PCA for\ndimensionality reduction, but rather to obtain a feature space\nwith an approximate diagonal covariance matrix. The PCA\nwhitening step decorrelates the features, which allows a\nmore efﬁcient temporal aggregation.\nThe principal components obtained for different\ntimescales are shown in Figure 2. For each timescale, the\nﬁrst few principal components (those that account for the\nmost variance in the data) tend to model global spectral\nshape. Subsequent components then model harmonic struc-\nture in the lower part of the mel-spectrum, and as we go up\nin the coefﬁcients (and lower in the accounted variance),\nthe components model structure in higher frequencies. It\nis interesting to notice the periodic structure in the com-\nponents which shows how the harmonics are captured by\nthe components. Also, if we compare components between\ntimescales, we can observe that components tend to model\na larger part of the mel-spectrum and exhibit more struc-\nture in the lower frequencies as we go higher in the frame\nsize.0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 93ms\n0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 186ms\n0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 372ms\n0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 743ms\n0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 1486ms\n0 50 100 150\nPrincipal components050100150Mel-spectrum bandsPCA 2972msFigure 2: PCA Whitening matrices for different timescales. The ﬁrst few principal components tend to model global\nspectral shape. Subsequent components then model harmonic structure in the lower part of the mel-spectrum, and as we go\nup in the coefﬁcients, the components model structure in higher frequencies.\nThe next step consists of summarizing the features over\na given time window by computing meaningful statistics.\nWe refer to this step as temporal pooling. Following re-\nsults from [6], we combined four pooling functions: mean,\nvariance, maximum and minimum. These statistics are ap-\nplied independently to each principal component through\ntime and concatenated into a single feature vector for a\ngiven time window. In consequence, for each timescale\nwe obtain a feature vector having four times the dimension\nof a single frame. Again, following results from [6], we\nﬁxed the pooling window at approximately 3 seconds for\nall experiments. Although, depending on how the frames\nwere overlapped, this window length might vary for differ-\nent timescales (see Section 4). The choice of the window\nlength can be justiﬁed by the fact that 3 seconds would be\nenough for a human listener to label audio examples, but\nlonger windows would give us less meaningful statistics\nfor shorter timescales.\nBy concatenating the pooled features from each timescale,\nwe obtain multi-timescale PMSCs (Figure 1).\n3.2 Multi-Layer Perceptron\nThe classiﬁer we used is similar as the pooled feature clas-\nsiﬁer (PFC) model presented in [6]. However, in our case,\nthe input pooled feature vector will tend to be larger, since\nit is obtained by concatenating many timescales.\nWe used a one-hidden layer artiﬁcial neural network,\nalso known as multi-layer perceptron (MLP), as the classi-\nﬁer for all experiments. We kept the size of the network\nconstant at 1000 hidden units for all experiments. Thenumber of parameters (weights) in the system varies de-\npending on the dimensionality of the input.\nThe input to the MLP is a multi-timescale PMSC repre-\nsentation a window of approximately 3 seconds of audio.\nIn order to obtain tags for a full song in the test and valida-\ntion phases, we simply average the MLP outputs over all\nwindows from that song.\nThe MLP is well suited for multi-label classiﬁcation\nlike the music annotation task. The hidden layer acts as\na latent representation that can model correlation between\ninputs as well as shared statistical structure between the\nconditional distributions associated with different targets\n(tags). This gives the MLP an advantage over other models\nsuch as the multi-class SVM, for which one would have to\ntrain a separate model for each tag. Also, the MLP scales\nsub-linearly in the number of examples, so it scales well to\nlarge datasets.\n4. RESULTS\nIn our experiments, we evaluated the performance of dif-\nferent timescales individually, and their combination for\nthe task of automatic tag annotation.\nIn our ﬁrst experiment, for a given timescale, we did not\noverlap frames. In consequence, longer timescales have\nfewer frame examples. In the extreme case, the longest\ntimescale is the size of the pooling window, meaning that\nthe max, mean and min are all equal, and variance is zero.\nObviously, this is not ideal. As we can see in Figure 3a,\nlonger timescale perform worse than short timescales. How-\never, we still see a signiﬁcant advantage to using a com-46.4 92.9 185.8 371.5 743.0 1486.1 2972.2\nFrame length (ms)0.780.800.820.840.86AUC-tag vs. frame length (in ms) without overlap(a)\n46.4 92.9 185.8 371.5 743.0 1486.1 2972.2\nFrame length (ms)0.780.800.820.840.86AUC-tag vs. frame length (in ms) with overlap (b)\nFigure 3: AUC-tag for single timescale features without overlap (a) and with overlap (b). Shorter timescales tend to\nperform better than longer timescales, and performance generally improve when using overlapped frames.\n(a) no overlap\n(b) overlap\nFigure 4: Illustration of frames without overlap (a) and\nwith overlap (b).\nbination of timescales. In Figure 5a, we show the perfor-\nmance of multi-timescale features. We combined timescales\nincrementally, starting from the shortest one to the longest\none. For example, the representation with two timescales\ncombines 46.4ms and 92.9ms frames, the one with three\ntimescales combines 46.4ms, 92.9ms and 185.8ms frames,\netc.\nIn order to obtain more examples for higher timescales,\nand yield more meaningful statistics for the temporal pool-\ning, we considered using more overlapping between win-\ndows. In our second experiment, we used the same frame\nstep for all timescales, corresponding to the smallest frame\nlength, in this case, 46ms (Figure 4). We include all frames\nthat start within the pooling window in the temporal pool-\ning. This means that the longest timescale frames will\noverﬂow beyond the pooling window length up to almost\ntwice the window length. Even though this method will\ngive us the same number of frames to aggregate for each\ntimescale, the longer timescales will still have much more\nredundancy than shorter timescales. Longer timescales per-\nform signiﬁcantly better with more overlap than without\noverlap, as we can see by comparing Figure 3a and 3b. TheMulti PMSCs PMSCs PMSCs + MTSL MUSLSE\nAUC-Tag 0.870 0.858 0.868 -\nAUC-Clip 0.949 0.944 0.947 -\nPrecision at 3 0.481 0.467 0.470 0.476\nPrecision at 6 0.339 0.330 0.333 0.334\nPrecision at 9 0.263 0.257 0.260 0.259\nPrecision at 12 0.216 0.210 0.214 0.212\nPrecision at 15 0.184 0.179 0.182 0.181\nTable 1: Performance of different automatic annotation\nmodels on the TagATune dataset\noverlap also gives a boost of performance when combining\ntimescales (Figure 5b).\nIn Table 1, we show the test performance of the model\nthat obtained the best AUC-tag on the validation set. We\ncompare with two other state-of-the-art models: Multi-time-\nscale learning model (MTSL) [6] and Music Understand-\ning by Semantic Large Scale Embedding MUSLSE [13].\nThe multi-timescale PMSCs trained with the MLP obtains\nthe best performance on all measures. Moreover, this model\nis a lot faster to train than the MTSL. For the TagATune\ndataset, the training time would typically be a few hours\nfor the MLP compared to a few days for the MTSL.\n5. CONCLUSION\nMulti-timescale PMSCs are general purpose features that\naim at jointly modelling aspects salient at multiple timescales.\nWe showed that, for the task of automatic tag annotation,\nusing multi-timescale features gives an important boost in\nperformance compared to using features computed over a\nsingle timescale. Moreover, with a simple classiﬁer, we\nobtain state-of-the-art performance on the TagATune dataset.\nMulti-timescale PMSCs could potentially improve the\nperformance of more complex learning models such as MTSL\nor MUSLSE. They could most likely be useful for other\nmusic information retrieval tasks such as genre recogni-\ntion, instrument recognition or music similarity as well.1 2 3 4 5 6 7\nNumber of timescales0.8560.8580.8600.8620.8640.8660.8680.8700.872AUC-tag vs. number of timescales without overlap(a)\n1 2 3 4 5 6 7\nNumber of timescales0.8560.8580.8600.8620.8640.8660.8680.8700.872AUC-tag vs. number of timescales with overlap (b)\nFigure 5: AUC-tag in function of number of timescales used without overlap (a) and with overlap (b). The combination of\ntimescales always include the shorter timescales. For example, the representation with 2 timescales combines 46.4ms and\n92.9ms frames, the one with 3 timescales combines 46.4ms, 92.9ms and 185.8ms frames, etc.\nAlthough the timescales used in these experiments are\nnot long enough to model many aspects of the temporal\nstructure of music, the combination of multiple timescales\nof analysis allows to model some mid-level temporal dy-\nnamics that are useful for music classiﬁcation. It is also a\nimprovement on the typical bag-of-frames approach. Even\nthough we are still using frame level features, the concate-\nnation of longer timescale representations puts short-time\nfeatures in context.\nIn future work, it would be interesting to optimize the\npooling window lengths independently for each timescale.\nThis would allow longer timescale features to be aggre-\ngated over less redundant information and provide more\nrelevant and stable statistics. It would also allow us to com-\npute PMSCs over even larger timescales.\n6. REFERENCES\n[1] J. Andén and S. Mallat. Multiscale scattering for au-\ndio classiﬁcation. In Proceedings of the 12th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’11), 2011.\n[2] J. Bergstra. Algorithms for Classifying Recorded Mu-\nsic by Genre. Masters thesis, Université de Montréal,\n2006.\n[3] P. Burt and T. Adelson. The laplacian pyramid as a\ncompact image code. IEEE Trans. Communications,\n9:4(532–540), 1983.\n[4] K. Ellis, E. Coviello, and G.R.G. Lanckriet. Semantic\nannotation and retrieval of music using a bag of sys-\ntems representation. In Proceedings of the 12th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’11), 2011.\n[5] R. Foucard, S. Essid, Lagrange M., and Richard\nG. Multi-scale temporal fusion by boosting for mu-sic classiﬁcation. In Proceedings of the 12th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’11), 2011.\n[6] P. Hamel, S. Lemieux, Y . Bengio, and D. Eck. Tempo-\nral pooling and multiscale learning for automatic an-\nnotation and ranking of music audio. In Proceedings of\nthe 12th International Conference on Music Informa-\ntion Retrieval (ISMIR’11), 2011.\n[7] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie.\nEvaluation of algorithms using games: the case of\nmusic tagging. In Proceedings of the 10th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’09), 2009.\n[8] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropaga-\ntion applied to handwritten zip code recognition. Neu-\nral Comput., 1:541–551, December 1989.\n[9] H. Lee, Y . Largman, P. Pham, and A. Y . Ng. Unsu-\npervised feature learning for audio classiﬁcation us-\ning convolutional deep belief networks. In Advances\nin Neural Information Processing Systems (NIPS) 22.,\n2009.\n[10] M. Mauch and M. Levy. Structural change on multi-\nple time scales as a correlate of musical complexity. In\nProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR’11), 2011.\n[11] N. Mesgarani, M. Slaney, and S. Shamma. Content-\nbased audio classiﬁcation based on multiscale spectro-\ntemporal features. IEEE Transaction on Speech and\nAudio Processing, 2006.\n[12] J. Reed and C.-H. Lee. On the importance of mod-\neling temporal information in music tag annotation.InProceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing, ICASSP\n2009, pages 1873–1876, 2009.\n[13] J. Weston, S. Bengio, and P. Hamel. Multi-tasking with\njoint semantic spaces for large-scale music annotation\nand retrieval. Journal of New Music Research, 2011."
    },
    {
        "title": "Digital Document Image Retrieval Using Optical Music Recognition.",
        "author": [
            "Andrew Hankinson",
            "John Ashley Burgoyne",
            "Gabriel Vigliensoni",
            "Alastair Porter",
            "Jessica Thompson 0001",
            "Wendy Liu",
            "Remi Chiu",
            "Ichiro Fujinaga"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415562",
        "url": "https://doi.org/10.5281/zenodo.1415562",
        "ee": "https://zenodo.org/records/1415562/files/HankinsonBVPTLCF12.pdf",
        "abstract": "Optical music recognition (OMR) and optical character recognition (OCR) have traditionally been used for doc- ument transcription—that is, extracting text or symbolic music from page images for use in an editor while dis- carding all spatial relationships between the transcribed notation and the original image. In this paper we discuss how OCR has shifted fundamentally from a transcription tool to an indexing tool for document image collections resulting from large digitization efforts. OMR tools and procedures, in contrast, are still focused on small-scale modes of operation. We argue that a shift in OMR devel- opment towards document image indexing would present new opportunities for searching, browsing, and analyzing large musical document collections. We present a prototype system we built to evaluate the tools and to develop practices needed to process print and ma- nuscript sources.",
        "zenodo_id": 1415562,
        "dblp_key": "conf/ismir/HankinsonBVPTLCF12",
        "keywords": [
            "document transcription",
            "OCR as indexing tool",
            "document image collections",
            "shift in OMR development",
            "document image indexing",
            "searching",
            "browsing",
            "analyzing",
            "print and manuscript sources",
            "prototype system"
        ],
        "content": "DIGITAL DOCUMENT IMAGE RETRIEVAL USING OPTICAL MUSIC\nRECOGNITION\nAndrew Hankinson John Ashley Burgoyne Gabriel Vigliensoni\nAlastair Porter Jessica Thompson Wendy Liu\nRemi Chiu Ichiro Fujinaga\nCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\nSchulich School of Music, McGill University, Montréal, QC, Canada\nandrew.hankinson@mail.mcgill.ca\nABSTRACT\nOptical music recognition (OMR) and optical character\nrecognition (OCR) have traditionally been used for doc-ument transcription—that is,extracting text or symbolic\nmusic from page images for use in an editor while dis-\ncarding allspatial relationships between the transcribed\nnotation and the original image. In this paper wediscuss\nhow OCR has shifted fundamentally from a transcription\ntool to an indexing tool for document image collections\nresulting from large digitization efforts. OMR tools and\nprocedures, incontrast, are still focused on small-scale\nmodes of operation. Weargue that a shift inOMR devel-\nopment towards document image indexing wouldpresent new opportunities for searching, browsing, andanalyzing large musical document collections. We\npresent a prototype system webuilt toevaluate the tools\nandtodevelop practices needed toprocess print and ma-\nnuscript sources.\n1. INTRODUCTION\n1\nOptical character recognition (OCR) isused toconvert\ndigital images of text into computer-manipulable repres-\nentations, which inturn are used tostore and index the\ncontent of books, newspapers, scholarly journals, andmagazines. OCR has been integrated into many large-scale print digitization initiatives, and iscurrently being\nused toprovide users with the unprecedented ability to\nsearch and retrieve millions of sources instantly—a task\nthat previously would have taken many lifetimes.\nWhile OCR isopening up new avenues for users to\nsearch, discover, and analyse large quantities of textualmaterial, the content of printed music documents isstill\ntrapped almost entirely inthe physical world. Even col-\nlections that have been digitized and placed online are\nstillmerely pictures of pages, with no means of extract-\ning their contents. Unfortunately, current optical music\nrecognition (OMR) software packages have not been de-\n1Permission tomake digital or hard copies of allor part of this work for\npersonal or classroom use isgranted without fee provided that copies\nare not made or distributed for proﬁt or commercial advantage and that\ncopies bear this notice and the full citation on the ﬁrst page.\n© 2012 International Society for Music Information Retrieval signed toprocess large volumes of page images efﬁ-\nciently. Rather, they are still designed for small-scale,\nsingle user transcription. In order toprovide OMR tools\ncomparable tothe OCR tools now available there isa\nneed for developing tools, technologies, and best prac-tices for recognizing, indexing, searching, and retrieving\nlarge amounts of digital page images.\nWebelieve that large-scale OMR projects are critical\ntoresearch inMIR and also computational musicology.\nThe vast majority of human musical output from the past\n1000 years does not exist inany kind of manipulable\ndigital format but rather lieswithin the enormous collec-\ntions of printed music and music manuscripts sitting onlibrary shelves across the globe. Relying on humans to\ntranscribe and share this music isexpensive and unsat-\nisfactory for many purposes. Many human transcrip-tions, especially of early music, involve a substantial\namount of personal interpretation on the part of the tran-\nscriber, and end users may not entirely agree with a par-\nticular interpretation. Moreover, many musicologists areinterested instudying how the extra-musical content on a\npage informs the musical content, and these scholarsneed tobe able to access the orginal page images inor-\ndertodraw concusion. By combining automatic tran-\nscription with retrieval of the original page image, we\ncan build systems that permit users toretrieve docu-\nments by content but then rely on the original image for\nstudy, relaxing the need for exact or “objective” tran-\nscription. Analogous tothe situation with OCR, even\nlower-accuracy OMR would be sufﬁcient todirect a user\ntoward documents of interest, anorder of magnitude\nmore quickly than the current situation whereby re-\nsearchers must visit library shelves and manually studyevery page.\nIn this paper weargue that there are a number of\nOMR technologies that must be inplace toenable mass\nmusic recognition and retrieval projects. Wewilldiscuss\nthe development of similar OCR technologies built to\nsupport large-scale text document image indexing andretrieval systems and compare that tothe existing OMR\ntechnologies. Aspart of this discussion, wepresent a\nprototype project developed aspart of the Single Inter-\nface for Music Score Searching and Analysis (SIMSSA)initiative. This project includes the development of anOMR workﬂow system, a notation encoding format for\nstoring the results of our OMR system, and the develop-\nment of search and retrieval tools. Wediscuss our ﬁnd-\nings from this prototype, and present some of\nopportunities for future work.\n2. PREVIOUS WORK\nBoth OCR and OMR were initially conceived astran-\nscription technologies; that is,a digital page image was\nsupplied, the textual or musical content was extracted,\nand then the original image was discarded. The result\nwasthe transcribed content of the page ina format suit-\nable for further editing ina word processor or notation\neditor. This method left no direct correspondence\nbetween the page content and itslocation on the original\nimage in the output format.\nWhen computing advanced enough todisplay im-\nages, OCR began tobe used asa means of navigating\nand retrieving document images based on their textual\ncontent. Although the recognition process remained thesame, OCR ﬁleformats began preserving correspond-\nence between document content—words, paragraphs,\ncolumns, graphics—and the original page image. When\ncombined with a search engine, this provided users with\nthe ability tonavigate tothe exact occurence of a search\nword or phrase. This was the emergence of the shift in\nOCR from a technology that “merely” transcribed text,toone that permitted navigation through large numbers\nof digital document images. Many researchers atthis\nconference have probably experienced the beneﬁts of\nthese systems for locating journal articles or conferences\npapers in systems like JSTOR.\nIn the next section we will look ata number of tech-\nnologies inOCR that have supported itsemergence asa\ndocument image navigation technology. We will then\nconsider a few of the projects that have tried todosimil-\nar things with printed music documents.\n2.1. Document Image Formats\nOne of the most crucial components of a document im-\nage indexing system isthe ability tocorrelate recognized\nobjects with their location on the page. In this section we\ndiscuss a few formats developed for textual document\nindexing.\nNagy [1] describes anearly system for using OCR on\ndocument images from pages of technical journals, al-\nlowing users tosearch and retrieve image segments con-\ntaining their query terms. Henotes that a “major strength\nof the approach isthe preservation of the original layout\nof the documents, which not only augments reading\ncomprehension but also often conveys indispensable in-\nformation on itsown.” This seems tobe the ﬁrst such\nmention of retrieving document page images using OCR\nanalysis.\nStehno et al. [2] describe the METS/ALTO format\n(Metadata Encoding and Transmission Standard / Ana-\nlyzed Layout and Text) for mapping layout structures\nand text passages from book pages. This iscurrently themost widely used standard for preserving text layout de-\nrived from OCR systems.\nSeveral other formats deserve a brief mention as\nwell. The hOCR format issupported by the OCRopus\nand Tesseract software, used by the Google Book digit-\nization project [3]. Portions of the Internet Archive digit-\nization project use the DjVuXML\nformat which contains\nthe words and coordinates of a “hidden” layer [4]. The\nHathiTrust Project uses their own XML format.\nThePAGE format [5] isdesigned toencode data for\nevaluating OCR document analysis. It isdifferent than\nmost other formats presented here, inthat itencodes\nground-truth data for evaluating document layout. Thisincludes encoding features like reading order (the order\ninwhich columns or segments of a page are read by a\nhuman).\nInallof these cases, document layout and text con-\ntent are maintained in an integrated document format, al-\nlowing the OCR system tostore image coordinates for\neach document element (words, lines, paragraphs, etc.)\nrecognized by the analysis software. In the next section\nwe will brieﬂy discuss how this data may be used in an\nindexing system to retrieve page images. \n2.2. Document Image Retrieval\nIndexing for page image retrieval ismore complex than\nindexing for simple text retrieval. Anindex must be built\ncontaining allthe words that have been recognized from\nthe images, but these must be further correlated with\ntheir page and location on the page image. Retrieving\npage images requires indexing and storing key words\nand their positions inthe document images where they\noccur. This gives users the ability toenter a search query\n(a word or phrase) and retrieve the pages where the res-\nult of their query canbe found, highlighting their exact\npositions on the page image.\nThe HathiTrust has constructed a correlated text and\nimage index for their collection. They incorporate the\ngraphical locations for each recognized word into their\nindex [6]. They note that the coordinate positions for\nevery word accounts for 85% of the index sizeof a par-\nticular book. This means that for their required goal of\n10 million books, their expected index size is two tera-\nbytes of which most of the information isOCR coordin-\nate data.\n2.3. Large-Scale OCR\nTohandle large numbers of documents, OCR applica-\ntions have moved away from standalone desktop applic-\nations toserver-based solutions. This allows distributed\ntask separation, whereby multiple teams can simultan-\neously work on digitization, recognition, correction, and\npublishing without being bound toa single workstation.\nMany tasks canbe partially or fully automated, requiring\nhuman intervention only as a quality-control measure.\nIn the commercial sector there are a number of large-\nscale solutions. Perhaps the most successful example ofthis isthe Abbyy Recognition Server1, a centralized\nOCR workﬂow management system. Documents are in-\ngested by digitization, automatically recognized, thenveriﬁed, corrected, and further indexed by humans sitt-\ningatmultiple workstations. For open-source software,\nthere are a number of command-line tools that canbe\nchained together toform anautomated OCR system. The\nOCRopus and Tesseract systems [7], developed byGoogle for their book search projects, contains a number\nof tools for creating highly customizable OCR systems.\nThe recently-completed IMPACT (Improving Access to\nText) project [8], a €16.5M research project, focused on\nbuilding new OCR tools and best practices for libraries\nand archives. They have created a system that allowsmultiple image processing, OCR, and results evaluationtools tobe chained together toform anad hoc recogni-\ntion system.\nSince recognition systems willnever be perfect, tech-\nniques that enable humans tocorrect OCR and ensure\nthat recognition errors will not create problems for re-\ntrieval. Correction, however, canbe very time and la-\nbour intensive. Some unique solutions have been de-veloped tohelp offset the costs of this task. The\nAustralian Newspaper Project [9] has created a “crowd-\nsourced” correction system, where more than 9,000 vo-\nlunteers have now corrected more than 12.5 million linesof text, with more corrections added allthetime. The re-\nCAPTCHA project [10] has produced over 5 billion hu-\nman-corrected OCR words by presenting the correctiontask as a spam-ﬁghting challenge toprove that the cor-\nrector isa human and not anautomated system. Tools\nfor distributed proofreading and correction allow for aconstantly-improving search and retrieval system, and\nalso for the collection of pixel-aligned ground-truth data\nto further improve the accuracy of OCR systems. \n2.4. Music Document Image Retrieval\nThe purpose of providing a review of tools and tech-\nniques employed by text search projects is to compare\nand contrast it tosimilar work done for OMR. Unlike the\nlarge text initiatives, such asthe HathiTrust, Google\nBooks, and Internet Archive projects, weare unaware of\nany publicly available databases that allow users tore-\ntrieve page images from printed books based on auto-matic transcription of the page contents using OMR.\nThere are, however, a few projects that have developed\nsome functionality worth mentioning here.\nThe PROBADO Music Project [11],[12],[13] [14] is\nperhaps the largest and longest-running project incorpor-\nating large-scale OMR for use insearch systems. This\nproject seeks toprovide a uniﬁed interface for retrieving\nsymbolic and audio representations of music pieces. As\nof October 2010, their dataset consisted of 50,000 pagesfrom 292 books. The content of their dataset ismusic\nprinted incommon Western notation ina variety of\n1 http://www.abbyy.com/recognition_server/genres and instrumentations, including opera, symphonic\nworks, and Classical and Romantic piano music.\nThe primary goal of the PROBADO project is to al-\nlow symbolic, image, and audio synchronization, provid-ing users with the ability tonavigate a score and hear the\naudio, or navigate the audio and jump to its correspond-\ning position inthe score. Their technique generates\nMIDI ﬁles from OMR, rendered to an audio representa-\ntion, and then aligned with different audio recordings of\nthe work. The audio isthen aligned atthe measure level\nwith a score image, allowing the system tohighlight the\ncurrent measure as the audio plays.\nThe PROBADO project uses the SharpEye ASCII\nﬁleformat for storing the notation-to-pixel coordinate\ninformation. This format isdocumented at[15], but is\nonly supported by the SharpEye OMR system. Similarly,\nHankinson et al. [16] propose the use of the Music En-\ncoding Initiative (MEI) format for maintaining notation-\nto-pixel correspondence.   \nBainbridge et al. [17] describe a Greenstone plug-in\nutilizing the CANTOR OMR system. Their system tran-\nscribes the notated music and makes itavailable for\nsearching. In their system they make the original pageimage available for viewing. Unfortunately, develop-ment on this system seems tohave stopped, and no\nworking version of their retrieval system can be found.\n3. LU PROTOTYPE\nWehave created a prototype system providing notation-\nbased retrieval of document images inaweb application.\nThe Liber Usualis (LU) [18] isa liturgical service book\nproduced by the Roman Catholic church and animport-\nant source for Gregorian chant. It uses square-note\nneume notation derived from the earlier Franconian stylebut modernized by the monks atSolesmes, France inthe\nlate 19th Century. There has been very little work on\nOMR for this type of notation, with the exception of\n[19]. Weperformed OMR and OCR on all2,340 page\nimages inthis book, maintaining notation and image cor-\nrespondence. Wethen developed a web application that\nallows basic query input based on n-gram indexing of\nthe notation content, highlighting the locations of results\ninsituon the page image. In this section we will brieﬂy\nreview the components of this prototype. A full over-\nview may be found in[20], and some details of the\nspeciﬁc technologies wedeveloped may be found in\n[21], [22]. Wehave made a public demo of our retrieval\nsystem available online.\n2\n3.1. OMR Workﬂow\nOur workﬂow isillustrated inFigure 1. Webegin with a\npage image, captured by either scanning or photograph-\ning a book. In the case of the LU,webegan with a com-\nplete PDF ﬁle of pre-scanned images. Wethen sent each\npage through the workﬂow. \n2http://ddmal.music.mcgill.ca/liberThe ﬁrst step was automatic page layout analysis,\nwhich weused toseparate the textual and musical areas\nof the page. Weperformed the layout analysis using a\nversion of Aruspix, anapplication originally designed\nforOMR of Renaissance printed music, which wemodi-\nﬁed tooperate on the neume notation intheLU. The\npage images were automatically scaled and straightened.\nAruspix iscapable of automatically locating and identi-\nfying various graphical page elements, providing the\nability todistinguish between musical and textual con-\ntent: musical staves, lyrics, ornate letters, lyrics, title ele-\nments, or other text. The different page elements are giv-\nendifferent pixel colours after the automated analysis,\ncreating separable “layers” that contain either exclus-ively music or exclusively text elements. The automatedanalysis saves a considerable amount of time and labour,\nalthough any mis-classiﬁed page elements do need tobe\ncorrected manually. Figure 2 shows the correction inter-\nface inAruspix, with a pop-up context menu allowing\nthe operator toselect anarea of the image and re-classify\nit as a different type of page. The LUdoes not have a\nparticularly complex layout, and most pages took\nbetween 30 and 130 seconds to correct (median 77 s).\nFigure 1: OMR Workﬂow for the Liber Usualis\nFollowing the layout analysis, the text layers were\nsent through anOCR workﬂow stage, which allowed the\ntexttobe searched and linked tothe speciﬁc regions of\neach page on which the text occurred. We used\nOCRopus, a third-party open-source OCR engine, to\nperform the text analysis. Minimal work was done to\ncorrect the OCR output, however. Post-OCR, weused a\nsimple edit distance toauto-correct recognized text from\na dictionary of liturgical Latin words. Lyrics that were\nbroken into syllables were automatically re-joined ata\nhyphenated break toform complete words. Nofurther\nhuman correction or processing wasperformed. The res-\nulting text has a large number of errors, but itwassufﬁ-\ncient for a “proof of concept.” The output of this stagewasfully OCRed text lines with the bounding-box co-or-\ndinates for the full line.\nThe music layer was sent through anOMR work-\nﬂow. Using the Gamera toolkit [23], weﬁrst removed\nthe staff lines from each musical layer. Removing staff\nlines not only facilitated OMR butalso allowed us to\ncompute precise bounding boxes for each musical ele-\nment. These bounding boxes are essential information\nfor retrieval systems that wish toshow the results of mu-\nsical queries on the page. Gamera uses adaptive k-\nnearest-neighbour classiﬁers for musical symbols, im-\nproving itsclassiﬁcation performance by using the in-\nformation from\nthe errors corrected on previous pages. It\ntook between 7 and 16 minutes tocorrect the errors on\nmost pages (median 11 min). Gamera keeps track of the\nlocation of staff lines when itremoves them, but it clas-\nsiﬁes on the shapes, not the pitches of musical symbols.\nThelaststepof the OMR workﬂow wasa customized al-\ngorithm for combining information about the location ofthe staff lines, the bounding box for each shape, and the\nshape of the musical symbol itself to add pitch informa-\ntion to every symbol.\nFigure 2: Layout Analysis and Correction in Aruspix\nAfter extracting the pitch information and bounding\nboxes for every musical symbol and text line on every\npage, the ﬁnal step of the workﬂow was tostore the re-\ncognized page content into a standard ﬁleformat. We\nchose the MEI format for a number of reasons. MEI is\nanXML-based notation encoding scheme, but unlike\nmost notation formats it can be extended tosupport\nmany different types of notation [24]. This was particu-\nlarly valuable inourcase, since the neume notation used\nby the LU is a revival of much older plainchant notation\nwith additional symbols added toindicate breathing\nmarks or articulations. With MEI we were able todevel-\nop a custom encoding scheme tosupport neume notation\nmarkup, while maintaining the broader document frame-\nwork and markup structure of MEI.\nTosupport document image indexing and retrieval,\nMEI provides the ability todeﬁne image zones—pixel-\nbased bounding boxes that store co-ordinates on a refer-\nence image—and correlate them with the recognized\nmusical and textual elements. It isimportant tonote here\nthat this isdone while stillpreserving the musical struc-\nture; that is,the notation maintains the melodic and sym-\nbolic structures that are expected from a notation encod-\ning scheme. Each musical and textual element isthen\ncorrelated with a deﬁned zone using the MEI @facs at-\ntribute. The end result is an XML hierarchy containing\nboth the musical, textual, and graphical information\ncorrelated and ready tobe indexed by a search engine to\nfacilitate image retrieval.\n3.2. Indexing, Searching and Retrieval\nOne of the most important musicological uses for the LU\nis as a compendium of important chant melodies that ap-\npear insome form across a wide variety of ancient mu-\nsical manuscripts and many later compositions. Despite\nitsimportance, there isno thematic catalogue of itsmu-\nsical content, and atmore than 2,000 pages, it can be\nvery time-consuming for researchers or musicians toﬁnd\nwhat they are looking for. Tofacilitate retrieval wecre-\nated an efﬁcient index for retrieving musical fragments\nfrom across the LUwhile maintaining information the\nlocation of these melodic fragments on their respective\npages. Following Stephen Downie [25], wegenerated in-\ndexes on n-grams, for nfrom 2 to10, on the following\nﬁve features:\n•pitches, a concatenated string of allof the pitch\nnames;\n•intervals, represented asthe directed melodic inter-\nvals between successive pitches in musical steps;\n•semitones, represented asthe directed melodic inter-\nvals between successive pitches in semitones;\n•contour, represented assequences of “up,” “down,”\nor “repeated,” i.e., the direction of the intervals; and\n•neumes, represented by their component neume\nnames, i.e., if an n-gram was represented by the\nneume sequence “punctum clivis clivis.”\nWe also indexed the textual content of each page, in-\ncluding the co-ordinate information for each recognized\nline.\nForeach n-gram, the index also included the page on\nwhich an item appeared and itsaggregate bounding box.\nCombined, the indexes include approximately three mil-\nlion unique n-grams, which westore in an Apache Solr\ninstance3.\nUsers do not interact directly with the Solr instance.\nWebuilt a web application based on the open-source\nDiva.js viewer [26] topresent the original document im-\nages and highlight the results of queries on these images.\nTheweb application uses the indexed n-grams toprovide\na number of search capabilities:\n•strict or pitch-invariant sequences, where the usercantype ina sequence of pitch names and itwill\neither search for the literal string of pitches, or use\nthe semitone index tosearch for possible matches\n3 http://lucene.apache.org/solr/that use the same intervallic content but contain dif-\nferent pitches;\n•contour, containing the rough shape of the targetmelody, e.g., “dduurr”;\n•intervals, containing the speciﬁc shape of the target\nmelody, e.g., “d2 d2 u2 u3 r r”;\n•neumes, where the user speciﬁes a sequence of\nneume names, e.g., “punctum clivis clivis”; and\n•text, for retrieving pages based on lyrical or textual\ncontent.\nUsing the co-ordinate data from the OMR and stored\ninthe MEI, the results from a users’ query for a pitch se-\nquence willbring the user tothe page where their result\ncanbe found, with the bounding box around the search\nresult highlighting their query. Figure 3 shows a screen-\nshot from our web application with the result of the\npitch-sequence query “edcdeee” highlighted on the ori-ginal page image of the LU. \nFigure 3: The Liber Usualis Interface\n4. DISCUSSION AND CONCLUSION\nThe SIMSSA initiative isa long-term research program\nfor investigating and supporting large-scale OMR and\ndocument image retrieval for alltypes of music docu-\nments, from early manuscripts through tomodern music\nnotation. The work presented inthis paper is an initial\nattempt atbuilding systems that support processing and\nretrieval atascale and quality level that, todate, has not\nbeen achieved for musical documents. In this paper we\nhave identiﬁed technologies and techniques that have\nbeen developed tosupport OCR for transcribing and\nnavigating large numbers of document images, and have\ndemonstrated a prototype system wehave developed asa\nplatform for further research into how toshift OMR\nfrom small-scale transcription tolarge-scale document\nimage navigation and retrieval.\nThere are many open research questions arising for\nthis work that need further investigation. One of the\nmost critical isthe need for user studies and experiment-\nalinterfaces for musical document retrieval. Most cur-\nrent symbolic search systems are built around query in-\nterfaces that provide limited access to the underlying\nnotated music, typically restricted simple pitch or\nrhythm queries. More robust systems must be built to\nsupport more complex analysis-retrieval tasks, such as\ninvestigating the occurrence of speciﬁc cadential pat-\nterns or movement between multiple simultaneousvoices. More complex query and analysis systems willin\nturn require more sophisticated user interfaces, which\nwill need a deeper understanding of what what types of\nquestions musicologists, theorists, and performers would\nlike to see supported in a retrieval system.\nIn our opinion, OMR must move beyond desktop ap-\nplications and simple transcription. New modes of oper-\nation canand should be developed, including server- and\nbrowser-based recognition, distributed proofreading andcorrection, networked recognition systems, and expan-ded research on recognition evaluation by building cur-ated ground-truth datasets covering different styles and\ntypes of music notation. Our prototype system representsa ﬁrst step towards investigating many of these topics,\nand through the SIMSSA project wehope tospur further\nresearch tomake the world’s music collections available\nto all.\n5. ACKNOWLEDGEMENTS\nThis work would not have been possible without the ef-\nforts of a number of people. The authors would like to\nthank Julie Cumming, Mahtab Ghamsari, Jamie Klassen,\nSaining Li,Mikaela Miller, Laura Osterlund, Laurent\nPugin, and Caylin Smith for their contributions. The\nSIMSSA project isfunded by the Social Sciences and\nHumanities Research Council of Canada. Further fund-\ningwasprovided by the Centre for Interdisciplinary Re-\nsearch inMusic Media and Technology and the Cana-\ndian Foundation for Innovation.\n6. REFERENCES\n[1] Nagy, G. 1992. Towards a structured-document-image \nutility. In Structured Document Image Analysis, eds H. \nBaird, H. Bunke, and K. Yamamoto. Berlin: Springer.\n[2] Stehno, B., A. Egger, and G. Retti. 2003. METAe—\nAutomated encoding of digitized texts. Literary and \nLinguistic Computing 18 (1): 77–88.\n[3] Breuel, T., and U. Kaiserslautern. 2007. The hOCR \nmicroformat for OCR workﬂow and results. In Proc. Int'l. Conf. on Document Analysis and Recognition, 1063–7.\n[4] Kumar, R. 2008. Bulk access to OCR for 1 million \nbooks. http://blog.openlibrary.org/2008/11/24/bulk-access-to-ocr-for-1-million-books/.\n[5] Pletschacher, S., and A. Antonacopoulos. 2010. The \nPAGE format framework. In Proc. Int'l. Conf. on Pattern Recognition, Istanbul, TR. 257–60.\n[6] Farber, P. 2009. Large-scale full-text indexing with \nSolr. http://www.hathitrust.org/blogs/large-scale-search/large-scale-full-text-indexing-solr.\n[7] Breuel, T.. 2009. Recent progress on the OCRopus \nOCR system. In Int'l. Workshop on Multilingual OCR, Barcelona, ES. 1–10.\n[8] Balk, H., and L. Ploeger. 2009. Impact: Working \ntogether to address the challenges involving mass digitization of historical printed text. OCLC Systems \nand Services 25 (4): 233–48.\n[9] Holley, R. 2009. Many hands make light work: Public \ncollaborative OCR text correction in Australian historicnewspapers. National Library of Australia Staff Papers.\n[10] Von Ahn, L., B. Maurer, C. Mcmillen, D. Abraham, \nand M. Blum. 2008. reCaptcha: Human-based characterrecognition via web security measures. Science 321 (5895): 1465–8.\n[11] Diet, J., and F Kurth. 2007. The PROBADO music \nrepository at the bavarian state library. In Proc. Int'l. \nConf. Music Information Retrieval, Vienna, AT. \n[12] Kurth, F., M. Müller, C. Fremerey, Y. Chang, and M. \nClausen. 2007. Automated synchronization of scanned sheet music with audio recordings. In Proc. Int'l. Conf. on Music Information Retrieval , Vienna, AT. 261–6.\n[13] Fremerey, C. 2010. Automatic organization of digital \nmusic documents: Sheet music and audio. PhD diss., Mathematics and Natural Sciences, University of Bonn,Bonn, DE.\n[14] Damm, D., F. Kurth, C. Fremerey, and M. Clausen. \n2009. A concept for using combined multimodal queries in digital music libraries. Research and Advanced Technology for Digital Libraries 261-72.\n[15] Jones, G. OMR engine output ﬁle format. http:/\n/www.visiv.co.uk/tech-mro.htm.\n[16] Hankinson, A., L. Pugin, and I. Fujinaga. 2010. An \ninterchange format for optical music recognition applications. In Proc. Int'l. Society for Music Information Retrieval, Utrecht, NL. \n[17] Bainbridge, D., C.G. Nevill-Manning, I.H. Witten, L.A.\nSmith, and R.J. Mcnab. 1999. Towards a digital library of popular music. In Proc. ACM Conf. on Digital Libraries, Berkeley, CA. 161–9.\n[18] Catholic Church. 1963. The Liber Usualis, with \nintroduction and rubrics in English. Tournai, Belgium: Desclée.\n[19] Ramirez, C., and J. Ohya. 2011. OMR of early \nplainchant manuscripts in square notation: A two-stage system. In Proc. SPIE, 1-10.\n[20] Hankinson, A., J.A. Burgoyne, G. Vigliensoni, and I. \nFujinaga. 2012. Creating a large-scale searchable digital collection from printed music materials. In Proc.\nAdvances in Music Information Research, Lyon, FR. \n[21] Hankinson, A., P. Roland, and I. Fujinaga. 2011. The \nmusic encoding initiative as a document encoding framework. In Proc. Int'l. Society for Music Information Retrieval, Miami, FL. \n[22] Vigliensoni, G., J.A. Burgoyne, A. Hankinson, and I. \nFujinaga. 2011. Automatic pitch detection in printed square notation. In Proc. Int'l. Society for Music Information Retrieval, Miami, FL. \n[23] Macmillan, K, M Droettboom, and I Fujinaga. 2001. \nGamera: A structured document recognition applicationdevelopment environment. In Proc. Int'l. Symposium onMusic Information Retrieval, Bloomington, IA. 15–6.\n[24] Hankinson, A., P. Roland, and I. Fujinaga. 2011. The \nMusic Encoding Initiative as a document encoding framework. In Proc. Int'l. Conf. Music Information Retrieval, Miami, FL. \n[25] Downie, S. Evaluating a simple approach to music \ninformation retrieval: Conceiving melodic n-grams as \ntext. PhD diss., University of Western Ontario, London,Ontario.\n[26] Hankinson, A., W. Liu, L. Pugin, and I. Fujinaga. 2011.\nDiva.Js: A continuous document image viewing interface. Code4lib Journal 14."
    },
    {
        "title": "String Methods for Folk Tune Genre Classification.",
        "author": [
            "Ruben Hillewaere",
            "Bernard Manderick",
            "Darrell Conklin"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416690",
        "url": "https://doi.org/10.5281/zenodo.1416690",
        "ee": "https://zenodo.org/records/1416690/files/HillewaereMC12.pdf",
        "abstract": "In folk song research, string methods have been widely used to retrieve highly similar tunes or to perform tune family classification. In this study, we investigate how var- ious string methods perform on a fundamentally different classification task, which is to classify folk tunes into gen- res, the genres being the dance types of the tunes. A new data set Dance-9 is therefore introduced. The different string method classification accuracies are compared with each other and also with n-gram models and global feature models which have been proven to be useful in previous folk song research. They are shown to yield similar results to the global feature models, but are outperformed by the n-gram models.",
        "zenodo_id": 1416690,
        "dblp_key": "conf/ismir/HillewaereMC12",
        "keywords": [
            "string methods",
            "fundamentally different classification task",
            "dance types",
            "global feature models",
            "n-gram models",
            "dance-9 data set",
            "various string methods",
            "highly similar tunes",
            "tune family classification",
            "genres"
        ],
        "content": "STRING METHODSFOR FOLKTUNE GENRE CLASSIFICATION\nRuben Hillewae\nreandBernard Manderick\nComputationalModelingLab\nDepartmentofComputing\nVrijeUniversiteitBrussel\nBrussels, Belgium\n{rhillewa,bmanderi }@vub.ac.beDarrell Conklin\nDepartmentofComputerScienceand AI\nUniversidaddel Pa´ ıs VascoUPV/EHU\nSan Sebasti´ an,Spain\nIKERBASQUE , BasqueFoundationforScience\nBilbao,Spain\nconklin@ikerbasque.org\nABSTRACT\nIn folk song research, string methods have been widely\nused to retrieve highly similar tunes or to perform tune\nfamilyclassiﬁcation. Inthisstudy,weinvestigatehowvar-\nious string methods perform on a fundamentally different\nclassiﬁcation task,whichis toclassifyfolktunesintogen-\nres, the genres being the dance types of the tunes. A new\ndata setDance-9 is therefore introduced. The different\nstring method classiﬁcation accuracies are compared with\neachotherandalsowith n-grammodelsandglobalfeature\nmodels which have been proven to be useful in previous\nfolksongresearch. Theyareshowntoyieldsimilarresults\nto the global feature models, but are outperformed by the\nn-grammodels.\n1. INTRODUCTION\nIn the history of Music Information Retrieval (MIR), folk\nsong databases have often been used as test collections to\nevaluatecomputationalmodels,especiallytheEssen Folk-\nsong Collection [18] has been the test set for variousMIR\nmethods [19, 4]. The availability of large databases of la-\nbelled folk tunes and the fact that many of these contain\nmainlymonophonictunes,makeitanattractivetestbedfor\nmachinelearningalgorithmsappliedtomusicalsequences.\nHowever, there is also a deeper interest in folk music\nfrom an ethnomusicologicalpoint of view, which is grow-\ning with the progression of advanced music data mining\nmethodsand the computationalpossibility of dealingwith\nlarge folk song corpora. Archives of folk music are be-\ning handed over to computational musicologists to be an-\nalysed,clusteredandsubdividedintocomprehensiblesub-\ngroups. Aself-organizingmapisusedtoidentifyandanal-\nysemotivecollectionsof22folkmusicculturesinEurasia\n[10]. AutomaticpatterndiscoveryhasbeenappliedtoCre-\ntan folk songs, in order to describe the characteristic fea-\nturesofeachsongtypeandregion[6].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2012 International Society for MusicInformation Retrieval.Besidesthesedescriptivetasks, acommontask incom-\nputational folk music analysis is music retrieval, which\nis related to the concept of melodic similarity. Symbolic\nmelodicsimilaritytaskshavebeenproposedattheMIREX\ncontests in 2005, 2006 and 2007, and were reintroduced\nsince 2010, with the Essen folksong database as test set.\nMost of the methods applied at these contests rely on se-\nquence alignment algorithms, which are shown to be suc-\ncesful[7, 20,21].\nIn this paper, however,we are interested in the the pre-\ndictive task of folk music classiﬁcation, where the goal is\nto predictthe class label of an unseenfolk tune. Sequence\nalignmentmethodshavebeenusedfortheclassiﬁcationof\nfolksongsintotunefamilies,whichareensemblesoftunes\nthat all derive from the same initial tune [22]. It is shown\nthattheyoutperformglobalfeatureapproaches. Inourpre-\nvious work, we have shown that n-gram models outper-\nform global feature models for the classiﬁcation of Euro-\npeanfolktunesintotheirgeographicregion[8]. Giventhe\nconclusions of these previous papers on the topic of folk\ntuneclassiﬁcation,thequestionariseshowsequencealign-\nmentmethodsandmoregenerallystringmethodscompare\nwithn-grammodels. We willthuspursueourcomparative\nstudy by adding this third category of models, in order to\nshed some light on the existing folk music classiﬁcation\nmethodsandtheirperformance.\nThis paper investigates the performance of three string\nmethods for the task of genre classiﬁcation on a newly\ndeveloped folk tune database Dance-9, containing 2198\ndancesof9differenttypes,whichwecallthegenres. String\nmethods rely on a sequential music representation which\nviewsapieceasastringofsymbols. A pairwisesimilarity\nmeasurebetweenthestringsiscomputedandusedto clas-\nsify unlabeled pieces. Obvious examples of string meth-\nodsarethesequencealignmentmethodsmentionedabove,\nbut also lessstandardapproacheswhichhavebeenused in\nthe ﬁeld of text classiﬁcation, such as compression based\ntechniques [13] or the string subsequence kernel method\n[12]. Compression based techniques have been applied\nto music classiﬁcation [11] and clustering [5], but these\nmethods have not been thoroughly compared with other\nexistingmethodsforfolktuneclassiﬁcation. Stringsubse-\nquence kernels have never been applied to music classiﬁ-\ncation,butrelatetothemethodpresentedbyP´ erez-SanchoDancetype number ofpieces relativenumber\nBourr´ ee 59 2.7%\nHornpipe 108 4.9%\nJig 793 36.1%\nMarch 76 3.5%\nPolska 339 15.4%\nReel 453 20.6%\nSchottische 119 5.4%\nStrathspey 123 5.6%\nWaltz 128 5.8%\nTotal 2198\nTable 1. TheDance-9collec tion: the numberof piecesof\neachdancetype.\net al. [16], where n-words are used to represent musical\npiecesasBooleanfeaturevectors,inordertoclassifyMIDI\nﬁlesintojazz orclassical music.\nThese string methods will be compared with both n-\ngram models and global feature models which we have\nstudiedindepthbefore[8],andthehypothesisofthisstudy\nisthatn-grammodelswill outperformboththeglobalfea-\nturemodelsandthestringmethodsonthetaskoffolktune\ngenre classiﬁcation. It is unclear how the string methods\nwill compare with the global feature models, since this\nclassiﬁcationtaskisessentiallydifferentthanthetunefam-\nily classiﬁcation proposed by van Kranenburg [22]. Two\nfolk dances, say for example two random waltzes, gener-\nallydiffermorethantwotunesbelongingtothesametune\nfamily.\nThe remainderof thispaper is structuredas follows. In\nthe next section we discuss the data set and its representa-\ntionthatwillbeusedforourexperiments,thenwedescribe\nthe three string methods in detail, recapitulate the n-gram\nmodels and global feature models, before describing the\nexperimentalsetup andreportingthe results. We conclude\nwith adiscussionandfuturework.\n2. DATASET AND MUSIC REPRESENTATION\nIn this section we introduce a new folk tune database for\nourexperiments,weillustratetwotypesofmusicrepresen-\ntationandthefeaturesthatwill beused.\n2.1 Dataset : Dance-9\nThe corpus Dance-9is a large collectionof Europeanfolk\ntuneswhicharesubdividedintoninedancetypecategories,\nthelargestonesbeingjigs,reelsandpolskas. Anoverview\nof the nine dance types and the class sizes is displayed in\nTable 1. The associated classiﬁcation task is to predictthe\ndancetypeofanunseentune,whichiswhatwecalla genre\nclassiﬁcation task.\nThiscorpushasbeen extractedfroma much largercol-\nlection of approximately 14,000 folk songs transcribed in\nthe ABC format, most of which are available on the web\n[1]. Many tunes contain metadata about their type of folk\ndance, and to construct Dance-9 we only selected thosewithanunambiguousdancetypeannotation. Furthermore,\nwediscardedalldancetypesthatoccurredinsufﬁcientlyto\nhave any statistical signiﬁcance. To the remaining 2198\npieces, two preprocessing steps have been applied in or-\nder to end up with core melodies that ﬁt for our research\npurpose: the ﬁrst step ensures that all pieces are purely\nmonophonic by retaining only the highest note of dou-\nble stops which occured in some of the tunes, and in the\nsecondstepweremovedallperformanceinformationsuch\nas grace notes, trills, staccato, etc. Repeated sections and\ntempo indications were also ignored. Key and time signa-\nture information has been retained, even though they will\nnot be explicitly used as musical features, as we explain\nin section 3.3. Finally, a conversion to clean quantized\nMIDI ﬁles is carried out with abc2midi. We removed all\ndynamic indications generated by the style interpretation\nmechanismofabc2midi.\n2.2 Music representation\nInMIDIformat,thefolktunesarereducedtoalistofmusic\nevents which are speciﬁed by their onset time, their pitch\nand duration. For the purpose of music data mining, one\ncan represent a piece in various ways based on this infor-\nmation, and the chosen music representation is associated\nto the type of model one intends to use. We will discuss\ntwo maintypesofrepresentation:\n•global feature vector : a global feature describes an\naspectofthewholepiecewithonesinglevalue,such\nas the average pitch or the fraction of ascending in-\ntervals. With a collection of global features, one\ncan represent the piece as a multidimensional fea-\nture vector. There is a wide range of standard ma-\nchine learning techniques available in toolboxes to\nclassifysuchvectorizeddata.\n•string representation : a piece can also be viewed\nas an ordered sequence of events, and every event\nis represented by an event feature of one’s choice.\nIn our case, the music events are note objects, with\npitchanddurationasbasiceventfeatures,fromwhich\none can for example derive the melodic interval be-\ntween the current and the previous note. Other ex-\namples are “duration ratio” or “melodic contour”.\nThisevent featuresequencecan be used directlyfor\nmodelling, or it can ﬁrst be mapped onto an ASCII\nsymbolstring.\nFigure 1 illustrates these types of representationon the\nﬁrst measures of the Scottish jig “With a hundredpipers”.\nThe two upper lines show two global features “average\npitch” and“rel. freq. M2”, whichis the relativefrequency\nof major seconds. Some event features are illustrated on\nthe next three lines, “pitch” being a basic one from which\n“melodic interval” is derived. The “interonset interval”\ntells the time span between the onset times of two succes-\nsive notes (given in MIDI ticks here), which is similar to\nnoteduration,exceptwhentherearerestsinthepiece. The\nlowertwolinesshowapossiblemappingintostringsgiven86\naverage pitch 67.125\nrel. freq. M2 0.714\npitch 6971736464666466\nmelodic interval ⊥+2+2-90+2-2+2\ninteronset int. ⊥12124824242424\nint-string ‘ffqjfdf’\nioi-string ‘lltiiii’\nFigure 1. Excerpt of the Scottish jig “With a hundred\npipers”,illu\nstratingthedifferencebetweenglobalfeatures,\neventfeaturesandthestringrepresentation.\ntwo event features, the melodic interval feature is mapped\nto “int-string”andtheinteronsetintervalto“ioi-string”.\nEach type of representation allows us to describe the\nmusicpiecesondifferentlevelsofabstraction. Whendeal-\ning with global features, one can create a large collection\noffeatures,eachofthemcapturinginformationaboutadif-\nferent musical aspect. The entire collection will be used\nto vectorize the pieces, and classiﬁcation is achieved with\nstandard machine learning algorithms. In the context of\nevent features or the string representation, only one event\nfeature is chosen for modelling, and this choice entirely\nimpliesthemusicalaspecttomodelanditsgranularity.\nIn order to do a fair comparison between the methods,\nwe will only consider features that directly derive from\npitchon the one hand and duration on the other hand,\nregardless of the used method. More precisely, for the\nstring methods and n-gram models, separate models are\nbuilt with the event features “melodic interval” and “in-\nteronsetinterval”. Fortheglobalfeaturemodels,wemanu-\nallycreatedtwocollectionsofglobalfeatures,onecontain-\ning features derived from the pitches, and the other with\nfeatures derived from the note durations. Any attributes\nthat makeuse of otherinformationsuchas thekeyor time\nsignatureare notincluded.\n3. METHODS\n3.1 Stringmethods\nIn this section we give a detailed description of the string\nmethods and the implementations that are used in our ex-\nperiments.\n3.1.1 Sequencealignment\nTheﬁrstcategoryofstringmethodsarethesequencealign-\nment methods, which are very common in computational\nbiologytocompareproteinsequencesforexample. Align-\nment algorithms deﬁne a similarity measure between two\nsequences of symbols, by estimating the minimal cost it\ntakes to transform one sequence into the other by meansof edit operations, such as substition, insertion and dele-\ntion. Therefore, this method is often referred to as “edit\ndistance”, which is in fact the Levenshtein distance. For\nexample, the edit distance between the strings ‘ismir’ and\n‘music’isequalto 4,sincetheoptimalalignmentbetween\nthemisgivenby\nismir\n❏❏\nmusic\nwhich means fou\nr edit operations are needed: two substi-\ntutions(‘i’to‘m’and‘r’to‘c’),oneinsertion(the‘u’)and\nonedeletion(the‘m’).\nMoreadvancedalignmentalgorithmsandalignmentscor-\ningmechanismshavebeendevelopeddependingontheap-\nplicationﬁeld. MongeauandSankoff[15]wereamongthe\nﬁrst who designed a variant speciﬁcally for the alignment\nof musical sequences. For the purpose of our current re-\nsearch, we have used W EKA’s implementation of the edit\ndistance [2]. In a preliminary experiment, we tested this\nimplementation on the melodic interval and interonset in-\nterval strings of the exact tune family database used by\nvan Kranenburg [22]. We obtained one nearest neighbour\nclassiﬁcation accuracies of 94.2% and 80.6% respectively\nin comparison with his 92.0% and 74.0%, which shows\nthatthegeneraleditdistancealgorithmissufﬁcientforour\ncomparativestudyathand.\n3.1.2 Compressionbaseddistance\nThe second type of string methodsare compression based\ntechniques, which also deﬁne a distance measure between\ntwo strings, by using the concept of information distance\ninherited from information theory. Ideally, this distance\nwouldberepresentedas\nd(x,y) =max(K(x|y),K(y|x))\nmax(K(x),K(y)),\nwhereK(x)is the Ko\nlmogorov complexity of string x,\nandK(x|y)istheconditionalcomplexityofstring xgiven\nstringy. Theunderlyingmotivationbehindthisdistanceis\nto compute how much information is not shared between\nthetwostringsrelativelytotheinformationthattheycould\nmaximallyshare. SincetheKolmogorovcomplexity K(x)\ncan not be exactly computed, it is approximated by the\nlengthofthecompressedversionofthestringusingacom-\npressorC, denoted by C(x). The information distance\nis thus approximated by the normalized compression dis-\ntance[5]:\nNCD(x,y) =C(xy)−min(C(x),C(y))\nmax(C(x),C(y)),\nwherexyrepres\nentstheconcatenationofthestrings xand\ny. Various types of compressors can be used to estimate\nthe Kolmogorov complexity, in our experiments we used\nbzlib, which is a block sorting text compression algo-\nrithm. For instance, the normalized compression distancebetween‘ismir’and‘music’is computedasfollows:\nC(‘ismir’) = 344 ,\nC(‘mu\nsic’) = 336 ,\nC(‘ismirmusic’ ) = 352 ,\nwhicharethecompressedsizesinbits. So,\nNCD(‘ismir’,‘music’) =352−336\n344= 0.046512.\nThis nu\nmber represents how different the two strings are,\nit isgenerallycontainedin [0,1].\n3.1.3 Stringsubsequencekernel\nThe third kind of string method is the string subsequence\nkernel method (SSK), which has been developed for text\nclassiﬁcation [12]. This approach computes a similarity\nmeasure between strings based on the number and form\nof their common subsequences. Given any pair of two\nstrings,SSKwillﬁndallcommonsubsequencesofaspec-\niﬁed length k, also allowing non-contiguous matches, al-\nthough these are penalized with a decay factor λ∈(0,1).\nForexample,\nSSK(k= 2,‘ismir’,‘music’ ) =λ5+λ6,\nbecausetherearetwocommonsubsequences‘si’and‘mi’,\nandthelengthsofthematchesaretheexponentsof λ:\n‘ismir’ ‘music’\nmatchl1match l2l1+l2\n‘si’‘smi’ 3 ‘si’ 2 5\n‘mi’‘mi’ 2 ‘musi’ 4 6\nTo speed up the algorithm, one can reduce the search\nspace of subseq\nuences by specifying a maximal exponent\nmofλ,whichiscalled λ-pruning;ithasbeenshownthere\nislittlequalitylossduetothispruning. Inourexperiments,\nwe looked for short subsequences ( k= 2,3,5) allowing\nfew or no non-contiguousmatches. The parameter λwas\nset to adefaultvalueof0.5.\nThe general idea behind these three string methods is\nto determine a similarity measure between two “stringi-\nﬁed” music pieces xandy. A similarity measure between\nstrings is either derived from a distance metric d(x,y),\nsuch as the edit distance or the normalized compression\ndistance (NCD), or else it is computed directly from the\nstrings,whichiswhatthestringsubsequencekernel(SSK)\ndoes. Givenadistancemetric d(x,y),onecansimplyusea\nnearest neighbour approach to classify unseen test pieces,\nreplacingtheusual Euclideandistancewith d(x,y). Inthe\ncase of the string subsequence kernel, the computed simi-\nlaritymeasureisconsideredasthekernelfunctionofasup-\nport vector machine, a state of the art classiﬁer that learns\nnon-lineardecisionboundariesbetweenclasses.\n3.2n-grammodels\nIn this section we brieﬂy recall how an n-gram model can\nbe employed for classiﬁcation of music pieces, for moredetails we refer to our previous work [8]. In a ﬁrst stage,\nevery piece of the music data is transformed into an event\nfeature sequence according to a feature of choice. In the\ntraining phase, for each class the n-grams are counted to\nestimatetheprobabilitydistributionofthemusical“words”\ninthatparticularclass. Givenatestpiecerepresentedbyits\nevent feature sequence, the piece probability is then com-\nputed as the joint probability of the individual events in\nthepieceaccordingtothelearneddistribution,withtheas-\nsumption that the probabilityof an event only dependson\nthen−1previousevents. Finally,thetestpieceisassigned\ntotheclasswith thehighestpieceprobability,whichisthe\nmostlikely tohave“generated”thepiece.\nNotethatthemusicrepresentationisbasicallythesame\nas for the string methods, but the essential difference be-\ntweenthesemethodsisthatan n-grammodelaimstomodel\nthe transitions for a given class, whereas a string method\ncomputesa pairwisesimilaritymeasurebetweenpieces.\n3.3 Globalfeaturemodels\nInthissection,wedescribewhatglobalfeatureswerecho-\nsen for our experiments. Two separate global feature sets\nweremade,withfeaturesderivedfromthepitchontheone\nhand and from the note durations on the other hand. The\nfeatureswerechosenamongthefollowing(seeTable2):\n•TheAlicanteset of 28 global features, proposed by\nP.J. Ponce de L´ eon and J.M. I˜ nesta in [17] to clas-\nsify a collection of 110 MIDI tunes in the genres\njazz and classical. Amongthese, 7 are derivedfrom\npitch, e.g. “average melodic interval” and 12 from\nduration,like“durationrange”.\n•TheJesserset, containing 39 statistics designed by\nB. Jesser [9], 31 of which are pitch-based features.\nMost of these are basic relative interval counts, like\n“dminthird”, measuring the fraction of descending\nminorthirds,forallascendinganddescendinginter-\nvalsintherangeoftheoctave. Thissetalsoincludes\n6featuresderivedfromthenotedurations.\n•TheMcKayset of 101 global features [14], which\nwere used in the winning 2005 MIREX symbolic\ngenre classiﬁcation experiment and computed with\nMcKay’s software package jSymbolic [3]. This set\niscomposedofawiderangeoffeatures,sinceitwas\nintended to classify orchestrated MIDI ﬁles. We re-\ntained 34 features based on pitch, for example “Di-\nrectionofmotion”,i.e.thefractionofmelodicinter-\nvals that are rising rather than falling, and 4 based\nonduration.\nAll featuresderivedfrom pitch were joined to obtain a set\nof 73 features, since there are not many overlapping fea-\ntures. Withthesameprocedureappliedtothedurationfea-\ntures a set of 22 features was formed. We recall that any\nfeatures derived from the meter or the key signature have\nnot been retained, since we want to compare the methods\nandrepresentationsonthebasisofthesame information.Globalfeatureset pitch duration\nAlicante 8 12\nJesser 31 6\nMcKay 34 4\nTotal 73 22\nTable 2. Global features that were selected for our ex-\nperiments,di\nvidedintothose derivedfrompitchandthose\nfromduration.\nSinceglobalfeaturesrepresenteveryinstanceasamul-\ntidimensional feature vector, any standard machine learn-\ningclassiﬁercanbeappliedtogetaperformanceaccuracy.\n4. RESULTS\nIn this section we describe the experimentalsetup and the\nclassiﬁcation results on the folk tune data set Dance-9.\nSince we are interested in the relative performance of the\nstring methods, the n-gram models and the global feature\nmodels,wehavecomputed10-foldcrossvalidationclassi-\nﬁcation accuraciesforeach ofthe methods. Care hasbeen\ntaken to use the exact same cross validation folds in all\nexperiments, and the classiﬁer parameters (if applicable)\nhave always been set to standard values to do an unbiased\ncomparisonbetweenthemethods.\nThe string methods edit distance and NCD have been\nevaluated using a one nearest neighbour approach (1NN),\nwhereasSSKimpliesoneworkswithasupportvectorma-\nchine. Different lengths of short subsequences have been\nexamined( k= 2,3,5), and it was foundthat the best per-\nformances were obtained with contiguous matches; only\nthose will be displayed. For the n-gram models, we con-\nstructedtrigramandpentagrammodels,in directcompari-\nsonwith theSSK method. Theglobalfeaturevectorshave\nbeen classiﬁed with nearest neighbourapproachesas well\nas with a regular SVM kernel with a Radial Basis kernel\nFunction (RBF). For all experiments with SVM, the pa-\nrameter determiningthe softnessof the decision boundary\nhas been set to its default value, after verifying this does\nnotpenalizeanyofthemethods.\nThe results are reported in Table 3, which have to be\ncompared to a baseline classiﬁcation accuracy of 36.1%\noneobtainsbyalwayschoosingthelargestclass“Jig”. The\nﬁrstcolumncontainstheresultsusingonlyfeaturesrelated\nto pitch or melodic interval sequences, whereas the sec-\nondcolumngivestheresultswiththedurationfeaturesand\ninteronset interval sequences. It appears immediately that\nthelatterleadstosuperiorclassiﬁcationaccuraciesregard-\nlessofthemethod,withadifferenceofapproximately 20%\non average. This shows that the recognition of folk dance\ntypes on this corpus is easier to achieve with the duration\nrepresentations than with melodic ones, which is not sur-\nprising since folk dances are commonly distinguished by\ntheirrhythmicpatterns.\nSSKappearstobethemostpowerfulstringmethod,es-\npecially with contiguous subsequences of length k= 3.\nHowever, when we increase the length to k= 5the per-Stringmethods melodicint. interonsetint.\nEditDist (1NN) 50.0 70.0\nNCD (1NN) 48.0 68.0\nSSK (k= 2,m= 4) 54.0 71.2\nSSK (k= 3,m= 6) 60.8 72.9\nSSK (k= 5,m= 10)38.4 68.9\nn-grammodels melodicint. interonsetint.\nn= 3 60.7 71.9\nn= 5 66.1 76.1\nGlobalfeaturemodels pitch duration\n1NN 40.3 66.9\n5NN 44.8 69.3\nSVM, RBF-kerne\nl 53.5 67.7\nTable 3. The 10-fold cross validation classiﬁcation accu-\nracieswithal\nlmethodsusingtheintervalanddurationrep-\nresentation.\nformance drops when using the melodic interval strings.\nNCD does not lead to any promising result, whereas the\nedit distancedoesreasonablywell, especially if one keeps\nin minditscomputationtimeis alot shorterthanforSSK.\nThe comparison across all the methods reveals that the\npentagrammodelclearlyoutperformstheotherapproaches,\nwith both the melodic interval and the interonset interval\nfeatures. The trigam model also outperforms most other\nmethods with both representations, except for SSK with\nk= 3that achieves very similar results. On this corpus,\nthe string methods and global feature models yield simi-\nlar results with the melodic features, but on the rhythmic\nfeatures there is a slight advantage for all the string meth-\nods except NCD. For the global feature models, the SVM\nwith RBF-kernel performs better than both nearest neigh-\nbourmodelswith themelodicfeatures,but withthe rhyth-\nmic features there is no beneﬁt in using the more sophis-\nticated SVM classiﬁer over the simple nearest neighbours\napproach.\n5. CONCLUSIONS ANDFUTUREWORK\nInthispaper,wethorouglyexaminedtheclassiﬁcationper-\nformancesofthreestringmethodsandcomparedthemwith\nwell-known other classiﬁcation methods on a large folk\ndance dataset with nine classes. We have described the\ndifference between the underlying types of music repre-\nsentationandfeatures,andhaveshownthatfeaturesbased\non duration lead to better classiﬁcation models than fea-\nturesbasedonpitch,nomatteriftheyareusedtorepresent\nthemusicwithaneventfeaturesequenceorstring,orwith\na collectionofglobalfeatures.\nThecomparisonbetweenthemethodshasrevealedthat\nthen-grammodelsoutperformboththestringmethodsand\nthe global feature models, which is in agreement with our\nearlier survey [8]. This result proves the effectiveness of\nmodelling the transitions within a musical sequence and\nsupports our hypothesis that the n-gram model should be\nthedefaultmodelforfolktuneclassiﬁcation.\nHowever,thestringmethodsgenerallyperformslightlybetter than the globalfeaturemodels,particularlywith the\nstring subsequ\nencekernel which obtainsthe highest accu-\nraciesamongthestringmethods. Thisﬁrstresultonmusic\nclassiﬁcationwiththestringsubsequencekernelisencour-\nagingforfuturework. Thealignmentmethodswhichhave\nbeenshowntobeefﬁcientintunefamilyclassiﬁcation[22]\ncannot measure up to the pentagram model on this genre\nclassiﬁcation task. We are currently doing more research\non other folk song databases to get a broader view of the\nalignmentmethodperformance. Inparticularwe areinter-\nested in discoveringwhich modelsare most effective with\nrespecttothe preciseclassiﬁcationtaskat hand.\n6. REFERENCES\n[1]http://trillian.mit.edu/ ˜jc/cgi/abc/\ntunefind .\n[2]http://www.cs.waikato.ac.nz/ml/\nweka/.\n[3]http://jmir.sourceforge.net/\njSymbolic.html .\n[4] W. Chai and B. Vercoe. Folk music classiﬁcation us-\ning hidden Markov models. In Proceedings of Inter-\nnational Conference on Artiﬁcial Intelligence , Seattle,\nWashington,USA,2001.\n[5] R. Cilibrasi and P. Vit´ anyi. Clustering by compres-\nsion.Information Theory, IEEE Transactions on ,\n51(4):1523–1545,2005.\n[6] D.ConklinandC.Anagnostopoulou.Comparativepat-\nternanalysisofCretanfolksongs. JournalofNewMu-\nsic Research ,40(2):119–125,2011.\n[7] C. G´ omez, S. Abad-Mota, and E. Ruckhaus. An anal-\nysis of the Mongeau-Sankoff algorithm for music in-\nformation retrieval. In Proceedings of the 8th Inter-\nnational Conference on Music Information Retrieval ,\npages109–110,Vienna,Austria,2007.\n[8] R. Hillewaere, B. Manderick, and D. Conklin. Global\nfeatureversuseventmodelsforfolksongclassiﬁcation.\nInProceedings of the 10th International Society for\nMusic Information Retrieval Conference , pages 729–\n733,Kobe,Japan,2009.\n[9] B. Jesser. Interaktive Melodieanalyse. Peter Lang,\nBern,1991.\n[10] Z.Juh´ asz.Motiveidentiﬁcationin22folksongcorpora\nusingdynamictimewarpingandselforganizingmaps.\nInProceedings of the 10th International Society for\nMusic Information Retrieval Conference , pages 171–\n176,Kobe,Japan,2009.\n[11] M. Li and R. Sleep. Melody classiﬁcation using a\nsimilarity metric based on Kolmogorov complexity.\nInSound and Music Computing Conference , Paris,\nFrance,2004.[12] H.Lodhi,C.Saunders,J.Shawe-Taylor,N.Cristianini,\nandC.Watkins.Textclassiﬁcationusingstringkernels.\nThe Journal of Machine Learning Research , 2:419–\n444,2002.\n[13] Y.Marton,N.Wu,andL.Hellerstein.Oncompression-\nbased text classiﬁcation. In Advances in Information\nRetrieval, volume 3408 of Lecture Notes in Computer\nScience, pages300–314.SpringerBerlin / Heidelberg,\n2005.\n[14] C.McKayandI.Fujinaga.Automaticgenreclassiﬁca-\ntionusinglargehigh-levelmusicalfeaturesets.In Pro-\nceedings of the 5th International Conference on Mu-\nsic Information Retrieval , pages 525–530, Barcelona,\nSpain,2004.\n[15] M. Mongeau and D. Sankoff. Comparison of musical\nsequences. ComputersandtheHumanities ,24(3):161–\n175,1990.\n[16] C. P´ erez-Sancho, J. M. I˜ nesta, and J. Calera-Rubio.\nStyle recognition through statistical event models.\nJournalofNewMusicResearch ,34(4):331–339,2005.\n[17] P. J. Ponce de L´ eon and J. M. I˜ nesta. Statistical de-\nscription models for melody analysis and characteri-\nzation. In Proceedingsof the 2004InternationalCom-\nputerMusicConference ,pages149–156,Miami,USA,\n2004.\n[18] H. Schaffrath. The Essen folksong collection in the\nHumdrum Kern format. Stanford, California: Cen-\nterforComputerAssisted Researchin theHumanities ,\n1995.\n[19] P. Toiviainen and T. Eerola. A method for compara-\ntive analysis of folk music based on musical feature\nextraction and neural networks. In 3rd International\nConference on Cognitive Musicology , pages 41–45,\nJyv¨ askyl¨ a,Finland,2001.\n[20] A. L. Uitdenbogerd.N-grampattern matchingand dy-\nnamicprogrammingforsymbolicmelodysearch. Pro-\nceedings of the Third Annual Music Information Re-\ntrievalEvaluationeXchange ,2007.\n[21] J. Urbano, J. Llor´ ens, J. Morato, and S. S´ anchez-\nCuadrado. Mirex 2010 symbolic melodic similarity:\nLocal alignment with geometric representations. Mu-\nsic InformationRetrievalEvaluationeXchange ,2010.\n[22] P. van Kranenburg. A computational approach to\ncontent-based retrieval of folk song melodies. SIKS\ndissertatiereeks ,2010(43),2010."
    },
    {
        "title": "One in the Jungle: Downbeat Detection in Hardcore, Jungle, and Drum and Bass.",
        "author": [
            "Jason Hockman",
            "Matthew E. P. Davies",
            "Ichiro Fujinaga"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417054",
        "url": "https://doi.org/10.5281/zenodo.1417054",
        "ee": "https://zenodo.org/records/1417054/files/HockmanDF12.pdf",
        "abstract": "Hardcore, jungle, and drum and bass (HJDB) are fast- paced electronic dance music genres that often employ resequenced breakbeats or drum samples from jazz and funk percussionist solos. We present a style-specific method for downbeat detection specifically designed for HJDB. The presented method combines three forms of metrical information in the prediction of downbeats: low- level onset event information; periodicity information from beat tracking; and high-level information from a regression model trained with classic breakbeats. In an evaluation using 206 HJDB pieces, we demonstrate superior accuracy of our style specific method over four general downbeat detection algorithms. We present this result to motivate the need for style-specific knowledge and techniques for improved downbeat detection.",
        "zenodo_id": 1417054,
        "dblp_key": "conf/ismir/HockmanDF12",
        "keywords": [
            "Hardcore",
            "jungle",
            "drum and bass",
            "downbeat detection",
            "electronic dance music",
            "resequenced breakbeats",
            "drum samples",
            "jazz and funk percussion",
            "regression model",
            "classic breakbeats"
        ],
        "content": "ONE IN THE JUNGLE: DOWNBEAT DETECTION IN HARDCORE,\nJUNGLE, AND DRUM AND BASS\nJason A. Hockman1;2, Matthew E.P. Davies3, and Ichiro Fujinaga1;2\n1Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)\n2Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada\n3Sound and Music Computing Group, INESC TEC, Porto, Portugal\njason.hockman@mail.mcgill.ca, mdavies@inescporto.pt, ich@music.mcgill.ca\nABSTRACT\nHardcore, jungle, and drum and bass (HJDB) are fast-\npaced electronic dance music genres that often employ\nresequenced breakbeats or drum samples from jazz and\nfunk percussionist solos. We present a style-speciﬁc\nmethod for downbeat detection speciﬁcally designed for\nHJDB. The presented method combines three forms of\nmetrical information in the prediction of downbeats: low-\nlevel onset event information; periodicity information from\nbeat tracking; and high-level information from a regression\nmodel trained with classic breakbeats. In an evaluation\nusing 206 HJDB pieces, we demonstrate superior accuracy\nof our style speciﬁc method over four general downbeat\ndetection algorithms. We present this result to motivate\nthe need for style-speciﬁc knowledge and techniques for\nimproved downbeat detection.\n1. INTRODUCTION\nIn the early 1990s, affordable sampling technologies (e.g.,\nAkai S900 and Commodore Amiga) and the popularity\nof rave culture provided the impetus for the creation of\nthree related genres—hardcore, jungle, and drum and bass\n(HJDB)—unique in their fast tempi and drum sounds,\nwhich are mostly derived from samples of percussion\nsolos in 1960s–80s funk and jazz recordings known\nasbreakbeats. Since 1990, over 25,000 artists have\ncontributed over 132,000 tracks on almost 6,000 labels.1\nHJDB became so popular in the mid-1990s that it was\nshowcased on BBC’s Radio 1 program, “One In The\nJungle”. Both popular press [1,16] and academic literature\n[10] have mostly treated HJDB from a sociology/cultural\nstudies perspective, presenting the music within larger\ncontextual issues, e.g., race, drugs, and cultural politics.\nA notable exception [3], provides tools for automated\nbreakbeat splicing and resequencing.\n1http://www.rolldabeats.com/stats\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.In this study, we present a downbeat detection model\ncreated with the intention of ﬁnding downbeats within\nmusic containing breakbeats, and provide a comparison of\nits performance against four pre-existing algorithms on a\ndatabase of 206 HJDB excerpts. We view this as a ﬁrst step\nin an automated analysis of the musical surface of HJDB\nfrom a computational musicology perspective, towards the\neventual goal of understanding how individual artists use\nbreakbeats (e.g., slice ordering and pitch adjustment) in\nmodern music.\n1.1 Hardcore, Jungle, and Drum and Bass\nHardcore began around 1990, and was the ﬁrst of the\nHJDB genres to fully embrace the use of breakbeats.\nTracks soon left the 120–130 beats per minute (BPM)\nhouse and techno standard and steadily became faster\n(upwards of 180 BPM), with longer, more intricate drum\npatterns. The less synth-driven, breakbeat collage art of\njungle appeared around 1992. By 1994, many artists\nabandoned the rhythmic complexity of jungle in favor of\nsimpler rhythms associated with drum and bass. As is the\nstandard workﬂow in these genres, breakbeats are recorded\ninto a sampler’s memory, segmented, and assigned to\nMIDI note values. HJDB artists create the rhythmic\n(and sometimes harmonic and melodic) structure of their\narrangements using these samples. While hundreds of\nbreakbeats have been employed in HJDB, many artists\nuse a handful of standards such as the “Amen” breakbeat,\noriginally from The Winston’s Amen, Brother [17].\n1.2 Downbeat Detection\nThe meter of a piece of music implies a counting mecha-\nnism for hierarchical stressed and unstressed beats within\na measure. A downbeat is the ﬁrst beat within a\nmeasure (or if counting beats, the one). While the\ncomputational task of downbeat detection has received\nlittle attention, the related task of beat tracking has\nreceived much more attention in recent years [9,13,15].\nA possible reason for this imbalance may be related to\nthe increased complexity of the task; prior to extracting\ndownbeats, the estimation of additional subtasks (e.g.,\nonset detection and beat detection) is often required, which\ncan propagate errors into downbeat estimation. Robust\ndownbeat detection would beneﬁt information retrievaltasks such as structural analysis [8], and would facilitate\nanalysis of phrase structure and hypermeter; both useful\nin improving automated mixing and DSP effects that\nrely on musically relevant change-point positions. More\nrelevant to our interests, downbeat detection provides key\nsegmentation points that allow for a comparison of HJDB\nartists’ drum usage.\nGeneralized downbeat detection methods have been\nproposed in the literature. Goto [11] employs rhythmic\ntemplate patterns to the output of a drum detection\nalgorithm. In non-percussive music, downbeats are\nassumed to be present at temporal locations of large\nspectral change, and are detected through a process of\npeak-picking spectral frames, grouping of the resultant\nsegments into beats, and a comparison of beats for\nharmonic change. Davies and Plumbley [5] present\na similar approach, in which downbeats are found by\nselection of beat positions that maximize spectral change.\nKlapuri et al. [13] extract the temporal evolution of a\nhidden metrical sequence exhibited in the output of a comb\nﬁlter bank. The joint-state estimates of the beat, sub-beat,\nand meter periods are chosen through a ﬁrst-order Markov\nprocess. Papadopoulos and Peeters [14] propose a method\nfor joint estimation of harmonic structure and downbeats\nusing an HMM that models chords and their metrical\nposition. They present an additional method in [15] that\nalso formulates the problem within an HMM framework,\nin which beat templates are ﬁrst estimated from the data,\nand beats are then associated with positions in a measure\nby reverse Viterbi decoding.\nUnlike the aforementioned algorithms, which are gen-\neralized for arbitrary musical input, Jehan [12] presents a\nregression model that predicts downbeat positions based\non learning style-speciﬁc characteristics from training data\ncontaining rhythmic and timbral characteristics akin to\nthose in the testing data. Evaluation is presented in\nconstrained circumstances, in which testing is performed\non part of the same song used for training, or on a test\nsong from the same album on which the remaining songs\nare used as training.\nIt is our belief that while generalized downbeat de-\ntection models will perform well in many circumstances,\nthere remain niche genres that fall outside the scope of\nthese methods [12]. HJDB, while heavily percussive and\nalmost exclusively in 4/4, presents challenges due to its\ncharacteristic fast tempo, high note density, non-standard\nuse of harmony and melody, and emphasis on offbeats.\n1.3 Motivation\nWith the exception of [12,15], the above methods rely on\ngeneral approaches to downbeat detection, and do not infer\ninformation about content between estimated downbeats.\nOur eventual aim is to use detected downbeats towards an\nestimation of the ordering of drum segments, and their\nsource, i.e., the breakbeat from which the drums were\nsampled. To do so, our particular application requires\nan understanding of likely solo percussion performances.\nWe therefore attempt to leverage knowledge of breakbeattimbres and patterns from the 1960s–80s to inform an\nunderstanding of three modern genres that utilize them.\nAt the core of the presented model is a top-down support\nvector regression technique, similar to [12] trained on these\nbuilding blocks of the music under analysis. Although\nHJDB artists often resequence segments of breakbeats, the\nresequenced patterns often reﬂect knowledge of standard\nbreakbeat patterns. To improve the robustness of this\nmodel we incorporate additional stages including beat\ntracking, and low-level onset detection to focus on kick\ndrum frequencies.\nThe remainder of this paper is structured as follows:\nSection 2 outlines our HJDB-speciﬁc downbeat detection\nmethod. Section 3 presents our evaluation methodology\nand dataset. Section 4 presents evaluation results and\ndiscussion, and Section 5 provides conclusions and future\nwork.\n2. METHOD\nOur main interest is to determine if an algorithm trained\non breakbeat patterns and timbres can ﬁnd downbeats in\nmodern forms of music that employ them. We began by\nre-implementing the algorithm as described in [12], with\nthe aim of utilizing it within the full range of HJDB music.\nExact parameterization of the model is not provided in\n[12], so we ﬁrst tuned our model by optimizing results on\nexamples described in the paper.\n2.1 Support Vector Regression for Downbeats\nIn [12], support vector regression (SVR) is employed to\ninfer likely downbeat positions. Audio is segmented by\nonset detection or a tatum grid. Each audio segment, S,\nis associated with a metrical position, t, within a measure\nwith downbeats at t=0, and last sample points before the\nnext downbeat at t=3. We used the LibSVM2epsilon-\nSVR algorithm in MATLAB with a RBF kernel.\nTo train the regression model, we require a feature\nmatrixFand associated class vector C, which we derive\nfrom breakbeats. Two HJDB artists selected 29 breakbeats\nfrom several lists of breakbeats commonly used in HJDB.\nAudio for each breakbeat was trimmed to the portion of\nthe signal containing only the percussion solos. Each\nbreakbeat,\f, is then segmented using an eighth-note grid,\nand a class vector, c\f, is created using the metrical position\nof each eighth-note segment in a measure.\nThe feature matrix f\fis comprised of 58 features\nextracted from each segment in \fconsisting of: mean\nsegment Mel-frequency spectral coefﬁcients; loudness of\nthe onset (dB) of each segment; maximum loudness (dB)\nof the onset envelope; and chroma. Segments are then\nassociated with metrical positions in c\fas in [12]. f\f\nis normalized to have zero-mean and unit variance across\neach row (all segments). Features are shingled (time-\nlagged and weighted linearly) [2] to emphasize more\nrecent segments. We then aggregate feature matrices and\n2http://www.csie.ntu.edu/\u0018cjlin/libsvm/class vectors across all breakbeats, creating an aggregate\nfeature matrix Fand aggregate class vector C. A feature\nand parameter optimization stage found best results using\n40 Mel-frequency spectral coefﬁcients and as in [12], 8\nto 16 past segments (equivalent to 1 to 2 bars). Principal\nComponent Analysis (PCA) feature reduction is applied to\nFto extract the top ten features across all breakbeats. A\nmodel is then trained using FandC.\nTo test the regression model using test audio, A, we\nrequire a feature matrix FA. We ﬁrst segment the audio\nusing an eighth-note grid created by interpolating the\ntemporal location of beats (we assume beats are found at\nthe quarter-note level), \r, as found by Beatroot [7]. FAis\ncreated similarly to f\f. The PCA model prepared in the\ntraining set is applied for feature reduction. We then use\nthe trained model created above with feature matrix FA\nto predict class values, CA, which contain the estimated\nmetrical position of each segment. In [12], the derivative of\nCAis used as a detection function from which downbeats\nare chosen.\nWhile we were able to recreate the examples in [12]\nusing the reimplemented method, training on breakbeats\nand testing on HJDB music showed that CAoften differed\nsigniﬁcantly from the idealized output (i.e., pure sawtooth\nwaveform), which resulted in the derivative of CAbeing\nan unreliable detection function on its own.\n2.2 Limitations of the Model\nWe now discuss three conditions that might cause these\nirregularities in CA. First, breakbeat patterns are not\nuniversal; i.e., one breakbeat may employ a kick drum\non beat one and snare drum on beat two, yet another may\ncontain a kick drum on beats one and two, and a snare on\nthe offbeat of two. As a result, CAmay not monotonically\nincrease between downbeats. Second, HJDB artists\noften re-order slices, which will also cause undesirable\noutput between downbeats. However, breakbeats almost\ninvariably begin with kick drums, and drum-types most\nassociated with downbeats are kick drums. This is\nalso the case for breakbeat usage within HJDB, where\nartists mostly apply downbeat-preserving transformations,\nin which segments are reordered and manipulated in such\na way to preserve the perception of downbeats. Third,\nCAmay diverge due to a mismatch in training and testing\ndata. The training data contains percussion-only sections\nof audio, while the testing data is comprised of excerpts\nof full HJDB pieces, which may include a variety of\ntransformations (e.g., pitch modiﬁcations) to the original\nbreakbeats. To overcome these potential problems, we\npropose subsequent stages to improve the accuracy of the\nmodel: post-processing of CA(Section 2.3); extraction of\nadditional metrical information—namely, a low-frequency\ndetection function (Section 2.4) and weighting at beat-\ntimes (Section 2.5); and information fusion with a ﬁnal es-\ntimation of downbeats by dynamic programming (Section\n2.6). An overview of the complete algorithm is presented\nin Figure 1.\nbeat \ntracking\nSVR\nlow-freq \nonset \ndetectiondynamic \nprogrammingbeatsdownbeat-level \nperiod\ndownbeat \ndetection \nfunction\neighth \nnotes\nDOWNBEA TSlikely \ndownbeat \npositions\nlow-freq \ndetection \nfunctionFigure 1: Overview of proposed method. Circles denote\nstages in the method; solid lines point to variables created\nin these stages; and dotted lines point to variables created\nin subsequent steps.\n2.3 Regression Output Post-processing\nAs we are unable to rely solely on the derivative of CA\nfor an exact location of downbeats, we propose its use in\nproviding a coarse estimation of downbeats. We create\nlikely downbeat position function, E, as the ﬁrst-order\ncoefﬁcient of the linear regression at each eighth-note\nposition, by applying linear regression of a sliding buffer\nof eight segments (equivalent to the length of a measure)\nacrossCA. If the eight points of CAunder analysis\nresemble a positive linear slope, as they do at downbeats,\nthe value of Ewill be positive. As the buffer shifts, such\nthat it no longer begins on a downbeat (but now includes a\ndownbeat at buffer position 8), the value of Ewill decrease\nas it will no longer maintain a positive linear slope. Once\nthe buffer has reached the end of CA,Eis normalized to\nvalues between 0 and 1.\n2.4 Low-Frequency Onset Detection\nThe coarseness of Eled us to incorporate low-level onset\nevent information related to salience and timing. We\nintroduce a low-frequency onset detection function, L,\nas follows: As in [6], we segment the input audio into\n40 ERB-spaced sub-bands and calculate complex spectral\ndifference across each (with a temporal resolution of 11.6\nmsec per onset detection function sample). We apply our\nknowledge of standard usage of basic rock drum kit drum-\ntypes (i.e., kick drum, snare drum, and hi-hats) within\nbreakbeats and HJDB music. Since drum types found at\ndownbeats are likely to be kick drums, we focus on lower\nfrequencies and sum the output of the lowest \u001abands to\nproduceL. While the precise number of bands is not\ncritical, we found \u001a=5 to provide adequate results.\n2.5 Beat-Time Weighting\nIn Section 2.1, beat time locations, \r, are used to create the\neighth-note grid used in the segmentation of the test audio\nfor the SVR model. We also use \rto generate a beat-time\nweighting,U, for emphasis in L. At\r(here quantizedto the resolution of L),U=!, and otherwise U=1. The\nprecise value of !is not crucial, however we found !=1.3\nto perform well. To contend with alignment issues of beat\ntimes and peaks in L, we additionally weight U=!at\u00062\ndetection function samples of \r.\n2.6 Information Fusion and Decision\nIn this stage, we combine low-frequency onset detection\nfunction,L, with beat-time weighting, U, and likely\ndownbeat position function, E, to create a ﬁnal detection\nfunction, \u0002, used in the determination of downbeat times.\nOur motivation in combining these three forms of in-\nformation is as follows: Lprovides low-level information\npertaining to event location and salience, while Eprovides\ninformed knowledge of likely downbeat positions based on\nsimilarity of the test segment patterns to patterns of drums\nin the breakbeat training set. The integration of beat-time\nweighting provides alternate possible downbeat positions\nthatEhas either missed or erroneously measured.\nAs none of these information sources alone is capable\nof accurate downbeat detection, our hope is that fusing\nthem in a meaningful way will create a hybrid detection\nfunction that imparts the key attributes of each, resulting in\na more robust detection function from which we will select\ndownbeats. We ﬁrst interpolate Eto match the temporal\nresolution of L. We then combine L,E, andU:\n\u0002 = (L(1 +E))\u0003U; (1)\nwhere \u0003refers to element-wise multiplication.\nAn example of the usefulness of both EandUin\nemphasizing peaks of Lat likely downbeat positions (and\nsuppressing peaks not likely associated with downbeats)\nis presented in Figure 2. The top graph shows L(solid\nline) without scaling by E(dot-dashed line), and annotated\ndownbeat positions (vertical dashed line). The middle\ngraph shows Lafter scaling by E(solid line). The bottom\ngraph depicts Lafter scaling by EandU(solid line).\nFor the ﬁnal selection of downbeat positions from \u0002,\nwe require a peak-ﬁnding method capable of ﬁnding strong\npeaks that exist at regular intervals. Dynamic program-\nming (DP) has been shown useful for such purposes in beat\ndetection [9]. We similarly adopt DP to ﬁnd downbeats\nwithin \u0002, with a likely downbeat period \u001c. Given a\nhigh probability of 4/4 time signature and steady tempo in\nHJDB, it is sufﬁcient to estimate \u001cas 4 times the median\nof all inter-beat intervals derived from \r.\n3. EVALUATION\nThe aim of our evaluation is to determine the efﬁcacy of\nour method and four general models on a dataset solely\nconsisting of HJDB. In this section, we present our dataset,\nalgorithms under evaluation, and methodology.\n2 4 6 8 1001\n  \n2 4 6 8 1001Normalized Amplitude  \n2 4 6 8 1001\nTime (seconds)  L\nE\nannotations\nL,E\nannotations\nL,E,U\nannotationsFigure 2: Effect of stages in information fusion: (top) L\nwith no scaling, E, and annotations; (middle) Lscaled by\nE, and annotations; (bottom) Lscaled byEandU, and\nannotations.\n3.1 Hardcore, Jungle and Drum and Bass Dataset\nOur dataset is comprised of 236 excerpts3of between\n30 seconds and 2 minutes in duration. Each excerpt was\nselected from a full-length HJDB piece digitized from its\noriginal vinyl format to a 16-bit/44.1kHz WA V ﬁle. The\npieces span the ﬁve years (1990–4) of hardcore’s subtle\ntransformation through jungle and into drum and bass.\nWell-known, popular HJDB pieces were chosen for in-\nclusion in the dataset. An effort was taken to ensure a wide\ndistribution of artists, styles, and breakbeats used; three\nprofessional HJDB DJs were consulted for their opinions.\nDownbeat annotations were made by a professional drum\nand bass musician using Sonic Visualiser.430 excerpts\nwere removed from the test dataset to create a separate\nparameter tuning dataset used to optimize the parameters\nin the algorithm presented in Section 2. The remaining\n206 excerpts were then used in our evaluation.\n3.2 Evaluation Methodology\nFor evaluation metrics, we chose to modify the continuity-\nbased beat tracking evaluation metrics used in the MIREX\n2011 beat-tracking evaluation [4]. The principal difference\nis that we assess downbeats as the subject of evalua-\ntion, rather than beats. Additional modiﬁcations include\nadjustment of the tolerance window threshold, alteration\nof the possible interpretations of the downbeat to reﬂect\nwhole beat offsets, and exclusion of the longest continually\ncorrect segment metric in [4]. We create a tolerance\nwindow of 1/16th note around each annotated downbeat in\nour dataset (i.e., 6.25% of the inter-annotation-interval).\nFor an estimated downbeat to be correct, it must fulﬁll\nthree conditions: First, it must be located within the\n6.25% tolerance window around the nearest annotation.\nSecond, the previous estimated downbeat must be located\n3For the track list, see: http://ddmal.music.mcgill.ca/breakscience/dbeat/\n4http://www.sonicvisualiser.org/within the 6.25% tolerance window around the previous\nannotation. Finally, the inter-downbeat-interval must be\nwithin 6.25% of the inter-annotation-interval. We then\ncount the total number of correct downbeats and provide\na mean accuracy for a given excerpt. Among the various\nbeat offsets allowed by our evaluation measure, our main\ninterest is in the 1statistic, which indicates how well\nthe estimated downbeats align with annotations. 1is the\nmean accuracy across all excerpts. We provide additional\nstatistics, 2,3, and 4, to quantify errors in downbeat\nestimations, offset by whole beats. A potential problem for\ngeneral models is HJDB’s fast tempo. We therefore include\nan additional metric, 1/2x, which provides an error statistic\nfor estimated downbeats found at the half-tempo rate. 1/2x\nis calculated by using the evaluation method above, with\nthe annotations sub-sampled by a factor of two.\n3.3 Algorithms Included in Evaluation\nOur evaluation focuses on a comparison of the perfor-\nmance of the HJDB-specialized model with four general-\nized models. We expect this evaluation to be challenging\nfor generalized models due to the lack of harmonic change,\nfast tempo, and high note density in HJDB music. We\ncompare the following ﬁve models: commercial soft-\nware #1 (CS1); commercial software #2 (CS2); Klapuri\net al. (KL) [13]; Davies and Plumbley (MD) [5]; and our\nHJDB specialized method (HJ). The MD and KL methods\nare brieﬂy described in Section 1.2. CS1 and CS2 are\ncommercial products from two separate companies.5As\nwe do not have access to the methods in CS1 or CS2, we\ntreat them as black boxes.\n4. RESULTS AND DISCUSSION\n4.1 Parameter-Tuning Set Results\nWe ﬁrst compare results of four possible conﬁgurations\nof our model using the 30-excerpt parameter-tuning set,\nto determine the best system to use in the full evaluation\n(Section 4.2). Table 1 presents results for these con-\nﬁgurations using the 1,2,3, and 4statistics described\nabove. While two of the conﬁgurations do not contain\nbeat-time weighting, U, all conﬁgurations contain the\ndynamic programming stage with likely downbeat-level\nperiodicity\u001c, derived from beats. Informal evaluation\nof Beatroot’s performance on our dataset resulted in an\nF-measure of 83.0%. The base system (labeled LDF )\ncontaining low-frequency detection function, L, performs\nwell, which demonstrates the effectiveness of focusing\non kick drum frequencies. Adding either emphasis U\n(LDF ,U) at estimated beat times or estimated likely\ndownbeat detection function E(LDF ,E) has a similar\npositive effect. Adding both UandEhas a further positive\neffect, indicating independence between these features. In\naddition, errors in statistics 2,3, and 4in eitherLDF ,U\norLDF ,Eare reduced by addition of the other features—\ne.g., the 6% error found in LDF ,Ein the 4statistic is\n5of which one was a beta versionreduced to 3.3%. Similarly, the 2.8% error found in the\nLDF ,Uon the 2statistic is reduced to 0.6%. Addition of\neither or both emphasis UorEresults in an improvement\nin accuracy over LDF alone, and a reduction in error rates\n2,3, and 4.\n1 2 3 4\nLDF 72.8 3.7 3.4 6.4\nLDF, E 79.3 0.8 9.6 6.0\nLDF, U 79.9 2.8 2.8 4.8\nLDF, U, E 83.4 0.6 3.1 3.3\nTable 1: Accuracy measure 1and error metrics 2,3,4\n(in percentages) for four conﬁgurations of the presented\nsystem using the parameter-tuning dataset. Bold scores\ndenote highest accuracy in 1, and lowest error in 2,3,4.\n4.2 HJDB Evaluation Results\nEvaluation performance for the ﬁve compared methods\nis displayed in Table 2. Our specialized algorithm HJ\n(using theLDF;U;E conﬁguration) performs best in the\n1statistic. In addition, HJ achieves the smallest 2and\n1/2x error statistics (with a low 4error rate), which when\ncoupled with high 1performance, is seen rather favorably.\n1 2 3 4 1/2x\nCS1 38.5 2.8 4.0 4.2 2.8\nCS2 7.4 11.7 9.5 6.7 1.1\nKL 51.3 2.8 9.6 0.2 3.0\nMD 29.3 4.7 5.5 3.0 1.2\nHJ 74.7 2.3 5.8 2.0 0.0\nTable 2: Accuracy measure 1and error metrics 2,3,4,\n1/2x (in percentages) for the ﬁve models under evaluation\nusing HJDB test dataset. Bold scores denote highest\naccuracy in 1, and lowest error in 2,3,4,1/2x.\nWhen a model ﬁnds a downbeat on beats two or four\nin HJDB music, it is likely to indicate a preference for\nhigh-energy note events such as snares (often played on\nbeats two and four). All models have some degree of error\nreported in the 3metric, possibly due to similarities in\nbreakbeat drum patterns starting on beats one and three,\nwhich results in a confusion of phrase boundaries at these\npositions. Surprisingly, none of the models displayed an\nafﬁnity for the 1/2x metric that our intuition led us to\nbelieve generalized models would ﬁnd more favorable.\n4.3 Discussion\nWhile our specialized method outperformed the gen-\neralized models, results should be examined with the\nunderstanding that only our approach had access to the\nparameter-tuning set used to adjust parameters of the SVR\nalgorithm. While this may make the comparison somewhatimbalanced, our model is the only algorithm necessitating\nsuch parametric tuning, as the other models are general\napproaches. We have incorporated speciﬁc attributes of\nHJDB music in a model used for its analysis: information\nabout timbre, pitch, and loudness of segments; knowledge\nof likely patterns; and emphasis on kick drum events and\npotential downbeat candidates at beat locations. Intuition\ntells us that the model in its present conﬁguration may not\nperform as well in a generalized evaluation or niche genres\nexcluding breakbeats, as downbeats in these datasets may\nnot be conveyed similarly.\n5. CONCLUSIONS AND FUTURE WORK\nWe have presented a style-speciﬁc model for ﬁnding\ndownbeats in music that we applied to hardcore, jungle and\ndrum and bass. At the core of our approach is a learning\ntechnique trained on classic breakbeats that form the rhyth-\nmic and timbral basis of these musical styles. We expanded\nthis model to incorporate information related to likely\nonsets in low-frequency bands and beat tracking. Through\nfusion of these complementary information sources we\ncreate a downbeat detection function from which we infer\ndownbeats using dynamic programming.\nEvaluation of our style-speciﬁc model with generalized\ndownbeat detection methods demonstrates a wide gap in\nperformance. This not only highlights the efﬁcacy of our\napproach in the conﬁnes of HJDB, but also provides further\nevidence towards the style-speciﬁc nature of downbeat\ndetection. We consider the latter conclusion more critical,\nand expect our method to be less effective in music without\nbreakbeats, and in music in which downbeats are conveyed\nby chord changes.\nIn building our model we have attempted to keep as\nmany components as general as possible, leaving the\ntraining of the SVR as the sole part explicitly style-adapted\nto HJDB. In this way, we believe our approach could\nbe readily adapted to other music styles through style-\nspeciﬁc training of the SVR. This strategy will form a key\ncomponent of our future work; both by training multiple\nmodels on different styles and investigating methods for\nautomatic selection between these models. We believe\nthe most proﬁtable future advances in downbeat detection\nwill be style-speciﬁc, rather than generalized models.\nWithin the domain of HJDB music, we intend to harness\nthe knowledge of downbeats to explore the relationships\nbetween the musical corpus and speciﬁc breakbeats amid a\nlarge-scale study of the genres.\n6. ACKNOWLEDGEMENTS\nThis work is partially funded by the Social Sciences\nand Humanities Research Council of Canada, the ERDF\nthrough the Programme COMPETE and by the Portuguese\nGovernment through FCT—Foundation for Science and\nTechnology, project ref. PTDC/EAT-MMU/112255/2009.\nThe authors would like to thank Conor O’Dwyer (Code),\nJason Chatzilias (0=0), and Daniel Lajoie (ESB) for their\ncontributions and fruitful discussion.7. REFERENCES\n[1] B. Belle-Fortune, All crews, Vision, London, 2004.\n[2] A. Z. Broder, S. C. Glassman, M. Manasse, and G.\nZweig, “Syntactic clustering of the web.” J. of Comp.\nNetworks, V ol. 29, No. 8, pp. 1157–66, 1997.\n[3] N. Collins, Towards autonomous agents for live\ncomputer music: Realtime machine listening and\ninteractive music systems. PhD. diss., Cambridge\nUniversity, 2006.\n[4] M. E. P. Davies, N. Degara, and M. D. Plumbley,\n“Evaluation methods for musical audio beat tracking\nalgorithms.” Queen Mary University of London,\nCentre for Digital Music, Tech. Rep. C4DM-TR-09-\n06, 2009.\n[5] M. E. P. Davies and M. D. Plumbley, “A spectral\ndifference approach to downbeat extraction in musical\naudio.” In Proc. of EUSIPCO, 2006.\n[6] M. E. P. Davies, M. D. Plumbley, and D. Eck,\n“Towards a musical beat emphasis function.” In Proc.\nof WASPAA, pp. 61–4, 2009.\n[7] S. Dixon, “Evaluation of the audio beat tracking\nsystem BeatRoot.” JNMR, V ol. 36, No. 1, pp. 39–50,\n2007.\n[8] S. Dixon, F. Gouyon, and G. Widmer, “Towards\ncharacterization of music via rhythmic patterns.” In\nProc. of 5th ISMIR Conf., pp. 509–16, 2004.\n[9] D. P. W. Ellis, “Beat tracking by dynamic program-\nming.” JNMR, V ol. 36, No. 1, pp. 51–60, 2007.\n[10] E. Ferrigno, Technologies of emotion: Creating and\nperforming drum ’n’ bass. PhD. diss., Wesleyan\nUniversity, 2008.\n[11] M. Goto, “An audio-based real-time beat tracking\nsystem for music with or without drum-sounds.”\nJNMR, V ol. 30, No. 2, pp. 159–71, 2001.\n[12] T. Jehan, Creating music by listening. PhD. diss.,\nMassachusetts Institute of Technology, 2005.\n[13] A. P. Klapuri, A. J. Eronen and J. T. Astola, “Analysis\nof the meter of acoustic musical signals.” IEEE TASLP ,\nV ol. 14, No. 1, pp. 342–55, 2006.\n[14] H. Papadopoulos and G. Peeters, “Joint estimation of\nchords and downbeats from an audio signal.” IEEE\nTASLP , V ol. 19, No. 1, pp. 138–52, 2010.\n[15] G. Peeters and H. Papadopoulos, “Simultaneous\nbeat and downbeat-tracking using a probabilistic\nframework: Theory and large-scale evaluation.” IEEE\nTASLP , V ol. 19, No. 6, pp. 1754–69, 2011.\n[16] S. Reynolds, Energy ﬂash: A journey through rave\nmusic and dance culture (2nd Ed.). Picador, London,\n2008.\n[17] “Seven seconds of ﬁre.” The Economist, pp. 145–6,\n17th December 2011."
    },
    {
        "title": "User-centered Measures vs. System Effectiveness in Finding Similar Songs.",
        "author": [
            "Xiao Hu 0001",
            "Noriko Kando"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416868",
        "url": "https://doi.org/10.5281/zenodo.1416868",
        "ee": "https://zenodo.org/records/1416868/files/HuK12.pdf",
        "abstract": "User evaluation in the domain of Music Information Re- trieval (MIR) has been very scarce, while algorithms and systems in MIR have been improving rapidly. With the maturity of system-centered evaluation in MIR, time is ripe for MIR evaluation to involve users. In this study, we compare user-centered measures to a system effective- ness measure on the task of retrieving similar songs. To collect user-centered measures, we conducted a user ex- periment with 50 participants using a set of music re- trieval systems that have been evaluated by a system- centered approach in the Music Information Retrieval Evaluation eXchange (MIREX). The results reveal weak correlation between user-centered measures and system effectiveness. It is also found that user-centered measures can disclose difference between systems when there was no difference on system-effectiveness.",
        "zenodo_id": 1416868,
        "dblp_key": "conf/ismir/HuK12",
        "keywords": [
            "Music Information Retrieval",
            "user evaluation",
            "scarce",
            "rapid improvement",
            "system-centered evaluation",
            "user-centered measures",
            "system effectiveness",
            "task of retrieving similar songs",
            "user experiment",
            "MIREX"
        ],
        "content": "USER-CENTERED MEASURES VS. SYSTEM \nEFFECTIVENESS IN FI\nNDING SIMILAR SONGS  \nXiao Hu Noriko Kando \nFacult\ny of Education \nThe University of Hong Kong \nxiaoxhu@hku.hk  National Insti t\nute of Informatics \nJapan \nkando@nii.ac.jp \nABSTRACT \nUser evaluat\nion in the domain of Music Information Re-\ntrieval (MIR) has been very scarce, while algorithms and \nsystems in MIR have been improving rapidly. With the \nmaturity of system-centered evaluation in MIR, time is \nripe for MIR evaluation to involve users. In this study, we \ncompare user-centered measures to a system effective-\nness measure on the task of retrieving similar songs. To \ncollect user-centered measures, we conducted a user ex-\nperiment with 50 participants using a set of music re-\ntrieval systems that have been evaluated by a system-\ncentered approach in the Music Information Retrieval \nEvaluation eXchange (MIREX). The results reveal weak \ncorrelation between user-centered measures and system \neffectiveness. It is also found that user-centered measures \ncan disclose difference between systems when there was \nno difference on system-effectiveness.  \n1. INTRODUCTION \nWith the rapid growth of digital music, research on Music \nInformation Retrieval (MIR) has been flourishing in \nrecent years. Many algorithms and systems have been \ndeveloped to facilitate searching and retrieving music \npieces automatically. As a crucial aspect of system \ndevelopment, evaluation of MIR systems has attracted \ncontinuous attention among researchers. However, so far, \nMIR evaluation has been dominated by system-oriented \napproaches, while users, whom MIR systems would \nultimately serve, have rarely been considered in MIR \nevaluation. \nThe system-centered evaluation approach, also known \nas the Cranfield evaluation [3], has been adopted by the \nMusic Information Retrieval Evaluation eXchange \n(MIREX), a community-based annual evaluation \ncampaign for MIR. Since its inception in 2005, MIREX \nhas evaluated and compared more than a thousand \nsystems on various MIR tasks such as genre classification, \nartist identification, query-by-humming, etc. [5]. MIREX \nnot only greatly enhances the development of MIR, but \nalso provides rich evaluation data on system effectiveness.  \nDespite its long tradition and popularity, system-centered approach has been criticized for excluding users \nfrom the evaluation process. Researchers argue that the \ngoal of MIR systems is to facilitate users’ music \ninformation tasks, and thus the evaluation of MIR should \ninevitably take users into consideration [8]. Furthermore, \nsince music appreciation is more or less a subjective \nprocess, users’ perceptions about whether a MIR system \nis useful might be different from a system-centered point \nof view. However, there have been no formal studies \ninvestigating whether there are correlations between user-\ncentered measures and system effectiveness measures in \nthe MIR domain. Thus, people remain puzzled when they \nsee the precision and recall numbers of certain systems. \nWould they be helpful to users? Would users be satisfied \nwith them? This study aims to fill the research gap and \nanswer the following research question: to what extent is \nsystem effectiveness related to user-centered measures?  \nIn particular, this study focuses on one MIR task, \naudio music similarity and retrieval where systems search \nfor songs similar to a given query song. There are two \nmajor reasons for choosing this task. First, finding similar \nsongs, as a query-by-example scenario, is a prevailing \nmusic information need. For instance, many people \nsearch for similar songs to build playlists [4]. Second, the \nMIREX has the same task  and thus provides system-\ncentered measures needed in this study.  \n2. RELATED WORK \n2.1 User Evaluation in MIR \nThere have been very few studies on formal user evalua-\ntion MIR systems. The Philips Research Laboratories in \nthe Netherlands is the leader on this topic. During 2002 to \n2005, they conducted a series of controlled user experi-\nments to evaluate their playlists generation systems \n[9][10][14]. The general approach was to recruit 22 to 24 \nparticipants to use a novel system they developed as well \nas one or two control systems for the task of generating \nplaylists in some pre-defined music listening situations \n(e.g., “soft music” and “lively music”). The experiments \nmay consist of one to four sessions. The researchers then \ncompared the novel system to control systems using user-\ncentered measures including users’ ratings on playlist \nquality, time spent on the task, number of button presses \nin accomplishing the task, as well as perceived useful-\nness, ease-of-use and preference reported by the users.  \nA more recent study by Hoashi and colleagues [7] \ncompared the effectiveness of three visualization methods \nfor a content-based MIR system. Besides user effective- \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nness and use\nr satisfaction, they also employed self-\nreported usability measures on perceived system accura-\ncy, explicitness and enjoyability. It is particularly valua-\nble that the authors also advocated for the user-centered \napproach as necessary to evaluate MIR systems.  \n2.2 User and System Effectiveness in Text Information \nRetrieval  \nEvaluating retrieval systems from the users’ perspective \nhas been active in the domain of text Information Re-\ntrieval (IR). Studies have been done to examine the rela-\ntionship between user-centered measures and system-\neffectiveness. Hersh et al. investigated this question in the \ntasks of instance recall [6] and question answering [12]. \nThey conducted user experiments and found user effec-\ntiveness and system effectiveness did not yield the same \nconclusion. More recently, Turpin and Scholer [13] eval-\nuated systems with large differences in system effective-\nness and again found no significant relationship between \nuser and system effectiveness for precision-based tasks \nand a weak relationship for recall-based tasks. \nIn contrast, there were also studies finding significant \ncorrelations. For instance, Allen et al. [1] studied the task \nof text passage retrieval and found user effectiveness (as \nmeasured by task completion time and number of rele-\nvant passages) was correlated with system effectiveness \nwhen the latter was either low or high, but not in the \nmiddle. Last but not least, Al-Maskari et al. [2] controlled \nthe variance of system effectiveness and reported signifi-\ncant correlations between multiple user-centered \nmeasures and system effectiveness.    \nAs a first study investigating the relationship between \nuser-centered measures and system effectiveness in the \nMIR domain, this study is inspired by the aforementioned \nprevious work in text IR. Many of these studies used \nTREC (Text Retrieval Conference) evaluation results to \nselect systems to be evaluated by users and to obtain data \non system effectiveness. In this study, we resort to \nMIREX, the counterpart of TREC in the MIR domain, for \nobtaining system effectiveness measures and the underly-\ning MIR systems.  \n3. METHOD AND RESEARCH DESIGN \n3.1 The AMS Task in MIREX \nThe MIREX has included the Audio Music Similarity and \nRetrieval (AMS) task every year except for 2008. In this \ntask, systems are given a number of queries (i.e. audio \nsong clips) and a large collection of music audio clips. \nThe goal of the systems is to retrieve clips from the col-\nlection that sound similar to the queries. In 2010, the \nAMS task had 100 queries sampled from 10 different \ngenres, and the candidate collection contained 7,000 mu-\nsic clips also evenly sampled from the 10 genres. There \nwere eight systems evaluated in this task1 and all of them \nwere consid e\nred in this study except for one system \n                                                           \n1The MIREX 2010 AMS task results: http://www. m usic-\nir.org/mirex/wiki/2010:Audio_Music_Similarity_and_Retrieval_Results  (RZ1) which was a random baseline and performed poor-\nly. For each query, the top five song clips retrieved by \neach system were collected for similarity judgment by \nhuman experts. This is much like the pooling method for \nrelevance judgment in TREC [15]. However, unlike \nTREC, the pooled candidates were deliberately random-\nized when presented to the human judges so as to elimi-\nnate any cues given by the order of candidates. In the \n2010 cycle of the AMS task, each query candidate pair \nwas judged by one assessor. Based on the similarity \njudgments on the pooled candidates, system effectiveness \nmeasures were calculated for evaluating and comparing \nthe systems. \nThis study is built upon the 2010 cycle of the MIREX \nAMS task. For each query, two systems were selected \nand user-centered measures on both systems were col-\nlected in formal user experiments. Then the research \nquestion is answered by comparing the user-centered \nmeasures to system effectiveness.  \n3.2 The Systems \nIn selecting systems, we adopted the approach proposed \nby Al-Maskari et al. [2]: different systems are selected for \ndifferent queries so that the differences of system effec-\ntiveness between systems can be better controlled. As \nthere are no previous studies of this kind in the MIR do-\nmain, we also followed [2] in using average precision \n(AP) as the system effectiveness measure. AP is calculat-\ned as the mean of precisions at the point of each relevant \ndocument in the ranked sequence. This measure rewards \nrelevant documents retrieved at high ranks. In MIREX \nAMS task, human judges evaluated each candidate using \nternary relevance: very similar, somewhat similar, and \nnot similar. In calculating AP scores in this study, we \nconvert the judgments into binary relevance by combin-\ning “very similar” and “somewhat similar” into “similar”. \nIn the future we will evaluate system effectiveness \nmeasures based on ternary relevance.   \nThe AP scores of the seven participating systems vary \nacross queries. For 79 out of the 100 queries, the differ-\nences of AP values among systems are from 0.01 to 0.6. \nFor the rest of 21 queries, the systems had exactly the \nsame AP. Unlike [2] where the best and worst performing \nsystems were chosen for each query, we choose the best \nperforming system (denoted as the “superior” system in \nthis paper) and the second best system (denoted as the \n“inferior” system in this paper) for each query. This is \nbecause the systems tend to have lower AP than those in \n[2] (99% are lower than 0.5 in this study), and the differ-\nence between the best and worst performing systems can \nbe very obvious, making it a trivial task to decide system \npreference. In addition, the worst performing systems \nsometimes are so bad that our pilot testers felt it was bor-\ning to listen to songs very dissimilar to the queries. Final-\nly, using systems with different but close AP scores \nmakes it possible to investigate whether user-centered \nmeasures can differentiate system quality when system \neffectiveness had little difference. For the 21 queries \nwithout system difference, two systems were randomly   \n \nselected. Fi\ngure 1 shows the AP scores of the two select-\ned systems across queries where the queries are ordered \nby the difference of AP scores between the two systems. \n \nFigure 1. System AP scores across queries. \n3.3 Participants \n50 Japanese undergraduate and graduate students from 13 \ndifferent universities were recruited, including 24 females \nand 26 males. Their average age was 21.7 years old \n(standard deviation was 4.30, range from 18 to 50). Their \nmajors ranged from engineering, medicine to social sci-\nences and humanities. Statistics of participants’ back-\nground on music knowledge, computer and English \nskills, as well as familiarity with the genres of the songs \nare shown in Table 1. Self-reported English abilities were \ncollected because some of the songs had English lyrics \nand the pre- and post- experiment questionnaires were \nwritten in English. As the songs were associated with \nAmerican genre classification system, the participants’ \nfamiliarity levels with the genres were collected.    \n Median Max.  Min. \nMusic knowledge* 4 6 2 \nExpertise with computers* 4 6 3 \nExpertise with online searching*  5 6 2 \nAbility in reading English* 5 7 3 \nAbility in listening to English* 4 7 2 \nFamiliarity with the genres  ǂ 3 5 0 \n*: in a Likert scale from 1 to 7. 1: novice, 7: expert; ǂ: in a \nLikert scale from 1 to 5. 1: very unfamiliar, 5: very familiar  \nTable 1. Stat\nistics of participants’ background. \n3.4  Tasks \nAll of the 100 queries in the MIREX AMS tasks are in-\ncluded in this study. The queries are evenly distributed \ninto ten different genres, namely Baroque, Romantic, \nClassical, Country, Jazz, Blues, RocknRoll, Rap/HipHop, \nMetal, and Edance. Each participant was assigned ten \nqueries with one in each genre. Since there are 50 partici-\npants, each query was evaluated by five participants. The \norders of the ten query genres were distributed to partici-\npants using a Latin Square design so as to reduce the ef-\nfect of genre order on results. \nFor each query, a participant evaluated the list of can-\ndidate songs retrieved by the two selected systems. Spe-\ncifically, a participant needed to play and listen to the \nquery song and indicate his/her familiarity level with it, as well as his/her personal preference on the query. Then \nhe or she proceeded to listen to each of the five songs re-\nturned by one system and indicate whether it sounded \nsimilar to the query. Just like in MIREX, we used ternary \nsimilarity scale: participants needed to indicate whether a \ncandidate was very similar, somewhat similar or not simi-\nlar to the query. In this experiment, the candidate songs \nwere presented to users in the original ranked order as \nretrieved by the systems. This setting mirrors a real life \nretrieval system where a higher ranked item is expected \nto be more relevant. In contrast, MIREX human judg-\nments had no information on the rank of the candidates \nnor did they know which candidates were retrieved by the \nsame system.   \nAfter evaluating all the five songs returned by one sys-\ntem, the participant was asked to indicate his/her satisfac-\ntion level towards this system and the perceived task eas-\niness in a Likert scale. Then the participant proceeded to \nthe other system. The relative difference of the systems \nwas not revealed to the user and the order of two systems \nwas randomized. After listening to candidates from both \nsystems, the participant was asked to indicate his or her \npreference between the systems and how easy it was to \ncompare the two systems. The audio of each song (either \nquery or candidate) was 30 second long, but it could be \npaused and/or replayed at any time. A participant could \nalso change answers when working with one system. \nHowever, once proceeding to the other system or the next \nquery, a participant could not go back to change answers. \nThis is to prevent influences from other systems on users’ \njudgments. A screenshot of the working interface is \nshown in Figure 2. \n \nFigure 2. Screenshot of the evaluation interface.  \n3.5 Procedure \nThe experiment was conducted in a batch manner, with 5 \nto 7 subjects in each batch performing the tasks at the \nsame time. Before the experiment started, each subject \nread and signed a consent form. After that, she or he \nfilled an online pre-experiment questionnaire with regard \nto demographic information, music background and \n  \n \nsearch exper\nience. Then, the experiment facilitator intro-\nduced the experiment system and the experiment proce-\ndure in Japanese. The training sessions lasted about 10 \nminutes.  \nAccording to our pre-tests of the procedure, the partic-\nipants were given 55 minutes to finish all the 10 assigned \nqueries. Most participants finished the process within 45 \nminutes. During the process, the experiment system rec-\norded users’ interactions including play and pause queries \nand candidates, answers to each question as well as \nchanges of answers. After all queries were finished, each \nsubject filled an online post-experiment questionnaire \nwhich asked for his or her general impression on the \nevaluated music retrieval systems and the experiment in \ngeneral. The entire procedure lasted about 1.5 hours and \neach participant was paid 2000 yen for their participation. \n3.6 User-centered Measures \nThe following user-centered measures were collected and \ncompared to system effectiveness.  \nUser effectiveness:  \n1) Number of similar songs found using each system. A \ncandidate is “similar” to a query if the user chooses “very \nsimilar” or “somewhat similar” option. \n2) Task completion time: time spent on making judg-\nments on all candidates of one system.  \n3) Time spent in finding the first similar song using each \nsystem. If there is no similar song found among the five \ncandidates, the time is assumed to be 3 minutes which is \nthe time needed for listening to 6 candidates in full \nlength. \n4) Rank of the first similar song using each system. If \nthere is no similar song found among the five candidates, \nthe rank is assumed to be 6. \nUser perception:  \n1)  Task easiness in evaluating results of each system.  \n2)  User satisfaction with each system.  \n3)  Easiness in comparing two systems.  \nEach of these measures was on a Likert scale from 1 to 5, \nwith 1 indicating very difficult/very dissatisfied and 5 in-\ndicating very easy/very satisfied.   \nUser preference:  \n1) The system a user prefers: the superior one, inferior \none or neither.   \n3.7 Hypotheses \nTo answer our research question, we compared AP scores \nof the two systems to the aforementioned user-centered \nmeasures by testing a series of hypotheses: \nH1: When the systems’ AP scores were different, users \nwere more effective and more satisfied with the superior \nsystems than the inferior systems;  \nH2: When the systems’ AP scores were the same, users \nwere similarly effective and satisfied with both systems;   \nH3: When the systems’ AP scores were different, users \npreferred the superior systems to the inferior systems;  H4: When the systems’ AP scores were the same, users \ndid not have a preference between the systems; \n H5: User perceived higher easiness level when compar-\ning systems with AP score difference than comparing \nthose without AP score difference. \n   To test the correlation between user-centered measures \nand AP score, we examined the following hypotheses:  \nH6: User-centered measures are highly correlated with \nAP score; \nH7: When the difference of systems’ AP scores gets larg-\ner, users would tend to prefer the superior systems and \nfeel it is easier to compare the two systems. \n4. RESULS AND DISCUSSIONS \n4.1 User Effectiveness and Satisfaction \nTable 2 presents means and standard deviations (in paren-\nthesis) of AP scores and user effectiveness and perception \nmeasures for the superior and inferior systems on the 79 \nqueries where the two systems had different AP scores. In \norder to test H1, we employed the non-parametric Wil-\ncoxon signed rank sum test because studies have shown \nthat system performance data rarely comply with normal \ndistribution [5] and the Wilcoxon test does not assume \nnormal distribution of tested variables. \nMeasure Superior Inferior p value \naverage precision 0.30 (0.13) 0.20(0.08) < 0.001*  \nnumber of similar songs  3.53 (0.99) 3.00 (1.07) < 0.001*  \ntask completion time \n(seconds) 76.75 (21.99)  77.09 (25.44)  0.688 \ntime finding first similar \nsong (seconds) 19.91 (14.42)  28.55 (26.74)  0.042* \nrank of first similar song  1.48 (0.63) 1.72 (0.99) 0.047* \ntask easiness 3.47 (0.55) 3.48 (0.53) 0.714 \nuser satisfaction 3.44 (0.73) 3.04 (0.77) < 0.001*  \nN=79.     *: significant at p < 0.05 level \nTable 2. Measures for queries with different AP scores. \nAs shown in\n Table 2, the difference between the AP \nscores of the superior and inferior systems was significant \nacross the 79 queries. The user-centered measures indi-\ncate that, using the superior systems, users found more \nsimilar songs, spent less time in finding the first similar \nsong which had a higher rank, and were more satisfied \nthan using the inferior systems. However, there was little \ndifference on the time used to judge all the five candi-\ndates of each system. In addition, users perceived the \ntasks were about the same easiness level when using both \nsystems.  \nTherefore, hypothesis H1 is partially supported by \nfour out of six user-centered measures under considera-\ntion. In other words, when the AP scores were signifi-\ncantly different, some user-centered measures could also \ndifferentiate the systems. The little differences on task \ncompletion time and perceived task easiness are related to \neach other. If a task is difficult, it will likely take more \ntime. The insignificant result indicates systems with   \n \nhigher AP sc\nores did not make the task of music similari-\nty judgment easier.    \nTable 3 presents means and standard deviations (in \nparenthesis) of the aforementioned measures and Wil-\ncoxon test results for the two systems on the 21 queries \nwhere the two systems had the same AP scores. As can \nbe seen from Table 3, H2 is also partially supported by \nfour out of six user-centered measures. That is, the two \nsystems had no significant difference on number of simi-\nlar songs found, task completion time, rank of first simi-\nlar songs and perceived task easiness. However, the two \nsystems were significantly different in terms of time \nspent finding the first similar songs and users’ satisfac-\ntion towards systems, even though the AP scores of the \ntwo systems were exactly the same. This evidences that \nsome user-centered measures can tell the differences be-\ntween systems that system-effectiveness cannot. In par-\nticular, the difference on user satisfaction on systems \nwith the same AP scores is remarkable since user satis-\nfaction has been called by IR researchers as a main crite-\nrion of IR system evaluation (e.g., [11]). \nMeasure System 1 System 2 p value \naverage precision 0.20 (0.07) 0.20 (0.07) - \nnumber of similar songs  3.59 (0.95) 3.46 (1.01) 0.470 \ntask completion time \n(seconds) 78.61 (21.19)  78.83 (24.49)  0.776 \ntime finding first similar \nsong (seconds) 14.37(10.83)  24.50 (18.27)  0.009* \nrank of first similar song 1.27 (0.47) 1.50 (0.63) 0.197 \ntask easiness 3.22 (0.43) 3.12 (0.54) 0.616 \nuser satisfaction 3.37 (0.64) 3.09 (0.61) 0.032* \nN=21. *: significant at p < 0.05 level  \nTable 3. Measures for queries with same AP scores. \n4.2 User Pref\nerence \nStatistics on user preferences between the systems are \nshown in Table 4. It is interesting to see that users per-\nceived no difference between the two systems 25% of the \ntime while the AP scores of the systems were different. In \ncontrast, for queries where the two systems had exactly \nthe same AP scores, 80% of users thought the systems \nwere different.  A Wilcoxon test was conducted on each \nset of the queries to see if the differences on number of \nvotes of the two systems were significant. The results \nsupport both H3 and H4: users preferred the superior sys-\ntems when the systems’ AP scores differed ( p < 0.001) \nwhile users did not have significant preferences when the \nsystems had the same AP scores ( p = 0.06). However, the \nlow p value and the low percentage of “no preference” \nvotes on queries with the same AP scores (20%) indeed \nsuggest that the difference on AP scores may not be a \ngood indicator of system preference.  \n4.3 Perceived Easiness in Comparing Systems \nThe test results on hypothesis H5, perceived easiness lev-\nel in comparing the two systems are shown in Table 5. \nThe average easiness score is 3.06 (easier) across the 79 \nqueries with AP difference and 2.81 (harder) across the 21 queries without AP difference. As the two sample siz-\nes are not equal, a two sample unequal variance t-test was \nemployed to test the significance of the difference on eas-\niness level. The test result is significant and thus hypothe-\nsis H5 is supported: users perceived it was easier to com-\npare the two systems when there were AP differences be-\ntween the systems. So far, the results of the analysis gen-\nerally support our hypotheses H1 to H5. That is, user-\ncentered measures tend to agree with system-\neffectiveness (as measured by AP score). However, the \nexceptions in H1 and H2 are also noteworthy. In the next \nsubsection, we continue to investigate the correlation be-\ntween user-centered measures and AP scores.  \n79 queries with differ-\nent AP scor e\ns Superior  Inferior No pref.  Total \nNumber of pref. votes 190 105 100 395 \nPercentage of preference  48.10% 26.58% 25.32%  100% \n21 queries with same \nAP scores  System 1  System 2  No pref. Total \nNumber of pref. votes 48 36 21 105 \nPercentage of preference  45.71% 34.29% 20.00%  100% \n Table 4. Votes of system preferences. \n With AP dif-\nference Without AP \ndifference p value \ndifficulty level 3.06 (1.26) 2.81(1.29) 0.002* \nsample size 395 105  \n*: significant at p < 0.05 level \nTable 5. Perceived difficulty in comparing systems \n4.4 Correlation between User-centered Measures and \nSystem Effectiveness \nTo test Hypothesis H6, Pearson’s correlation coefficients \nwere calculated for measures on interval scales: number \nof similar songs, task completion time and time of finding \nthe first similar songs. For measures on ordinal scales \nsuch as rank of first similar songs, task easiness and user \nsatisfaction, Spearman's rank correlation coefficient was \ncalculated. The results are shown in Table 6.  \nMeasure Coefficient  p value \nnumber of similar songs  0.111 (Pearson) 0.059 \ntask completion time -0.069 (Pearson) 0.177 \ntime finding first similar song -0.142 (Pearson) 0.022*  \nrank of first similar song  -0.163 (Spearman) < 0.021*  \ntask easiness  0.057 (Spearman) 0.423 \nuser satisfaction 0.246 (Spearman) < 0.001*  \nN = 200. *: significant at p < 0.05 level \nTable 6. Correlation between user-centered measures \nand AP score. \nNumber of similar songs, task completion time and \ntask easiness have no significant correlation with AP \nscore while the correlation between AP score and other \nuser-centered measures are fairly weak despite being sig-\nnificant. Our hypothesis H6 is not supported. The fact \nthat there is no significant relationship between perceived   \n \ntask easines\ns and AP score confirms an earlier finding in \nSection 4.1 that higher AP scores did not make the task of \nmusic similarity judgment easier. \nTable 7 shows Spearman’s correlation coefficients be-\ntween the AP scores difference of the two systems and \nthe two user-centered measures related to system compar-\nison: system preference and easiness in system compari-\nson. System preference is encoded as an ordinal variable \nwith values 1, 0, and -1 indicating preferring the superior \nsystem, no preference, and preferring the inferior system, \nrespectively. From Table 7 we can see that hypothesis H7 \nis not supported: the correlations are either insignificant \nor fairly weak. The insignificance between system prefer-\nence and AP score difference helps explain an earlier ob-\nservation that 80% of the users indicated system prefer-\nence when there was no difference on the AP scores. \nMeasure Correlation  with \nAP difference p value \nsystem preference \n 0.080 (Spearman) <0.053 \neasiness in system comparison  0.174 (Spearman) <0.001* \nN=100. *: significant at p < 0.05 level.  \nTable 7. Correlation between user-centered measures and \nAP score difference \n5. CONCLUSIONS AND FUTURE WORK \nThis paper presents a user experiment on evaluating re-\nsults of music similarity retrieval systems in the AMS \ntask in MIREX 2010, with the goal of comparing a well-\naccepted system effectiveness measure to user-centered \nmeasures. Such comparison has rarely been explored in \nthe MIR domain. The results revealed none or weak cor-\nrelations between system effectiveness and eight user-\ncentered measures. In particular, significant differences \non two user-centered measures, including user satisfac-\ntion, were found between systems with the same system \neffectiveness. As a first study on user-centered vs. sys-\ntem-centered measures in MIR, this research prompts \nmany interesting observations for future research. More \nuser behavior measures can be examined such as number \nof times a query song was played, number of changes a \nuser made to his or her answers, as well as measures \nbased on ternary relevance judgment. In addition, similar \nevaluations could be done for other MIR tasks such as \ngenre and mood classification in the future. \n6. ACKNOWLEGEMENTS \nThe research is partially supported by the JSPS Grand-in-\nAid (#21300096) and the Non-MOU Grant funded by the \nNational Institute of Informatics in Japan, and was con-\nducted when the first author was in the University of \nDenver. We also thank the IMIRSEL in the University of \nIllinois for providing the MIREX AMS data.  \n7. REFERENCES \n[1] J. Allan, B. Carterette and J. Lewis: “When will \ninformation retrieval be ‘good enough’? User effectiveness as a function of retrieval accuracy,” \nProceedings of the ACM SIGIR Conference on \nInformation Retrieval , pp. 433-440, 2005. \n[2] A. Al-Maskari, M. Sanderson, P. Clough and E. \nAirio: “The good and the bad system: Does the test \ncollection predict users’ effectiveness?” Proc. of the \nACM SIGIR Conference , pp. 59-66, 2005. \n[3] C. W. Cleverdon and E. M. Keen: “Factors \ndetermining the performance of indexing systems,” \nVol. 1: Design. Vol 2: Results. Cranfield, U.K: Aslib \nCranfield Research Project , 1966. \n[4] S. J. Cunningham, D. Bainbridge and A. Falconer: \n“‘More of an art than a science’: supporting the \ncreation of playlists and mixes,” Proceedings of the \n7th International Conference on Music Information \nRetrieval (ISMIR) , 2006. \n[5] J. S. Downie: “The Music Information Retrieval \nEvaluation eXchange (2005–2007): A window into \nmusic information retrieval research,” Acoustical \nScience and Technology , 29(4), pp. 247–255. 2008. \n[6] W. Hersh, A. Turpin, S. Price, B. Chan, D. Kraemer, \nL. Sacherek, and D. Olson: “Do batch and user \nevaluations give the same results?” Proc. of the \nACM SIGIR Conference , pp. 17–24, 2000.  \n[7] K., Hoashi, S. Hamawaki, H. Ishizaki, Y. \nTakishima, and J. Katto: “Usability evaluation of \nvisualization interfaces for content-based music \nretrieval systems,” Proc. of ISMIR , 2009. \n[8] X. Hu, and J. Liu: “Evaluation of music information \nretrieval: towards a user-centered approach,” \nProceedings of the 4th Workshop on Human-\nComputer Interaction and Information Retrieval \n(HCIR), 2010. \n[9] S. Pauws, and B. Eggen: “PATS: Realization and \nevaluation of an automatic playlist generator,” Proc. \nof ISMIR,  2002. \n[10] S. Pauws, and S. van de Wijdeven: “User evaluation \nof a new interactive playlist generation concept,” \nProc. of ISMIR , 2005. \n[11] K. Spärck Jones: Information Retrieval Experiment , \nLondon, Butterworths. 1981. \n[12] A. Turpin and W. Hersh: “Why batch and user \nevaluations do not give the same results,” Proc. of \nthe ACM SIGIR Conference , pp. 225-231, 2001.  \n[13] A. Turpin and F. Scholer: “User performance versus \nprecision measures for simple search tasks,” Proc. of \nthe ACM SIGIR Conference , pp.11-18, 2006. \n[14] F. Vignoli and S. Pauws: “A music retrieval system \nbased on user driven similarity and its evaluation,” \nProc. of ISMIR , 2005. \n[15] E. M. Voorhees and D. K. Harman: TREC: \nExperiments in Information Retrieval Evaluation , \nMIT Press, 2005."
    },
    {
        "title": "A Cross-cultural Study of Music Mood Perception between American and Chinese Listeners.",
        "author": [
            "Xiao Hu 0001",
            "Jin Ha Lee 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417799",
        "url": "https://doi.org/10.5281/zenodo.1417799",
        "ee": "https://zenodo.org/records/1417799/files/HuL12.pdf",
        "abstract": "Music mood has been recognized as an important access point for music and many online music services support browsing by mood. However, how people judge music mood has not been well studied in the Music Information Retrieval (MIR) domain. In particular, people's cultural",
        "zenodo_id": 1417799,
        "dblp_key": "conf/ismir/HuL12",
        "keywords": [
            "Music mood",
            "Music Information Retrieval",
            "MIR domain",
            "Cultural aspects",
            "Online music services",
            "Browsing by mood",
            "Peoples judgment",
            "Access point",
            "Browsing",
            "Music"
        ],
        "content": "A CROSS-CULTURAL STUDY OF  \nMUSIC MOOD PERCEP\nTION  \nBETWEEN AMERICAN AND CHINESE LISTENERS \nXiao Hu Jin Ha Lee \nFaculty\n of Education \nThe University of Hong Kong \nxiaoxhu@hku.hk  The Information Sc\nhool \nUniversity of Washington \njinhalee@uw.edu \nABSTRACT \nMusic mood h\nas been recognized as an important access \npoint for music and many online music services support \nbrowsing by mood. However, how people judge music \nmood has not been well studied in the Music Information \nRetrieval (MIR) domain. In particular, people's cultural \nbackground is often assumed to be an important factor in \nmusic mood perception, but this assumption has not been \nverified by empirical studies. This paper reports on a \nstudy comparing mood judgments on a set of 30 songs by \nAmerican and Chinese people. Results show that mood \njudgments do indeed differ between American and Chi-\nnese respondents. Furthermore, respondents’ mood \njudgments tended to agree more with other respondents \nfrom the same culture than those from the other group. \nBoth the song characteristics (e.g., genre, lyrical or in-\nstrumental) and the non-cultural background of the re-\nspondents (e.g., age, gender, familiarity with the songs) \nwere analyzed to further examine the difference in mood \njudgments. Findings of this study help further our under-\nstanding on how cultural background affects mood per-\nception. Also discussed in this paper are implications for \ndesigning MIR systems for cross-cultural music mood \nclassification and recommendation. \n1. INTRODUCTION \nThe number of studies on music mood has been increas-\ning in the Music Information Retrieval (MIR) domain as \nmany perceive music mood as a potential feature for or-\nganizing and recommending music. However, previous \nresearch asking people to provide mood tags for short \nmusic clips found that it is a highly subjective feature and \nthe vocabulary of music mood varies widely among users \n[8]. In addition to the features inherent in music itself \nthere are a number of features that can affect how people \ndetermine the mood of music (e.g., their current state of \nmind, life events). We believe that one important factor is \nthe cultural context of the user. However, currently there \nare no cross-cultural studies that specifically compare \nhow people perceive and determine the mood of music in the MIR domain.  \nIn this study, we explore if users from China and the \nUnited States perceive music mood in different ways. We \nchose to compare these cultures for several reasons. First, \nChina is a dominant Eastern culture while the United \nStates is a dominant Western culture. Second, although \nthe influence of American pop culture is gradually in-\ncreasing in China, due to historical and political factors, \nChinese people are far less affected by Western culture as \ncompared to people from other East Asian countries such \nas Korea or Japan [13]. Third, one of the authors is fluent \nin Chinese and English, which is important as translating \nthe mood labels while preserving the subtle nuances can \nbe challenging for non-native speakers.   \n2. LITERATURE REVIEW \n2.1 Cross-Cultural Studies in Music Psychology \nThere are a number of studies in music psychology on \ncomparing responses on Western and non-Western music \nfrom Western and non-Western listeners but many of \nthem focused on aspects such as memorability of music \n(e.g., [9]) or perception of complexity (e.g., [2]) rather \nthan perception of music moods. \nBalkwillm and Thompson [1] are among the first in-\nvestigating whether judgments on music mood can trans-\ncend cultural boundaries. They recruited 30 Western lis-\nteners to judge 12 Hindustani raga excerpts in order to \nsee if people can identify the intended emotion in music \nfrom an unfamiliar tonal system. Their findings showed \nthat the emotions of joy, sadness and anger (but not peace) \nwere identifiable by the listeners and the emotion judg-\nments were significantly related to psychophysical char-\nacteristics of the pieces. However, their study did not \ncompare judgments of people from different cultures.   \nGregory and Varney [5] compared mood descriptors \napplied to Indian classical, Western classical, and new \nage music by Indian and European listeners and revealed \n“many subtle differences in affective response” in mood \ndescriptors. They suggested that “the affective response \nto music is determined more by cultural tradition than by \nthe inherent qualities of the music (p.47).” More recently, \nWong et al. [14] found that Indian and Western listeners \nshowed in-culture bias when judging the tension in West-\nern and Indian music. However, Fritz et al. [4] found that \nnative African (Mafa) listeners could recognize three \nbasic emotions (happy, sad, scared/fearful) expressed in  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nWestern musi\nc with above chance accuracy and suggest-\ned that, “the expression of these basic emotions in West-\nern music can be recognized universally (pp.253).” The \nconflicting results in these studies highlight the need for \nmore empirical research on cross-cultural music mood \nperception. In addition, these studies generally focused on \nclassical and/or ethnic music whereas our study focuses \non popular music.  \n2.2 Cross-Cultural Studies in Music Information Re-\ntrieval  \nIn the MIR domain, there are few studies that examined \ncross-cultural aspects related to how users interact with \nand search for music. Lee et al. [7] collected music relat-\ned questions from Q&A websites based in North Ameri-\nca (i.e., Google Answers) and Korea (Naver Knowledge-\niN) and did a comparative analysis. They found that Ko-\nrean users experienced a number of challenges in cross-\ncultural/multilingual music searches: 1) they often failed \nto provide bibliographic metadata such as composer, per-\nformer or title in their queries, 2) they had difficulties in \nusing Western music genres and instead relied on associ-\nation-based concepts (i.e. where the music was used), \nand, 3) they had difficulties in using and transliterating \nlyrics information. The authors suggest that new access \npoints for accommodating cross-cultural/multilingual \nmusic searching including associate metadata (i.e., usage) \nare necessary.  \nNettamo et al. [11] also conducted a cross-cultural \nstudy of mobile music retrieval, management, and con-\nsumption behaviors of people in New York vs. Hong \nKong. They found several differences in how music was \nbeing managed, shared, and used. For example, New \nYorkers sought music information through various chan-\nnels including blogs, websites, magazines, etc., and mood \nand context of use were factors affecting how they gener-\nated playlists. Hong Kong users, on the other hand, \nsought for music only through limited channels such as \nranking websites or through friends, and did not use \nplaylists at all.  \nBoth of these studies indicate that there were in fact \ndifferences between users from different cultures with \nregards to their music related behaviors. However, none \nof these studies examined how they perceive music mood \nwhich is the gap this work is attempting to bridge.   \n3. STUDY DESIGN \nThis study focuses on the following set of research ques-\ntions: \n1. Do Americans and Chinese have different perceptions \nof mood on the same set of popular songs? In other \nwords, do people from the same culture tend to agree \nmore with each other?  \n2. Do some moods tend to be more agreeable among \npeople from certain culture? \n3. Do characteristics of the songs (i.e., genre, vocal or \ninstrumental) affect the difference on mood judgments \nbetween Americans and Chinese?  4. Do users’ non-cultural background (i.e., gender, age, \nand familiarity with the songs) affect the difference on \nmood judgments between Americans and Chinese?  \n \nTo answer these questions, we created an online sur-\nvey in which each user listened to thirty 30-second music \nclips and selected the most appropriate mood cluster \namong the five given clusters for each piece. We used the \nsame mood clusters that are used in MIREX1 Audio \nMood Classi f\nication (AMC) Task [6] (reprinted in Table \n1). Users can choose “other” if they think none of the \nmood clusters is applicable to the music piece.  \nCluster 1 \n(C_1) passionate, rousing, confident, boister-\nous, rowdy \nCluster 2 \n(C_2) rollicking, cheerful, fun, sweet, amiable/ \ngood natured \nCluster 3 \n(C_3) literate, poignant, wistful, bittersweet,  \nautumnal, brooding \nCluster 4 \n(C_4) humorous, silly, campy, quirky, whimsi-\ncal, witty, wry \nCluster 5 \n(C_5) aggressive, fiery, tense/anxious, intense, \nvolatile, visceral \nTable 1. Five Mood Clusters used in MIREX [6].  \nHalf of the 30 pieces were selected from the MIREX \nAMC task tes\nt collection with the help of IMIRSEL. This \nAMC test collection was created based on the APM (As-\nsociated Production Music)2 collection, and covers a va-\nriety of dif\nferent music genres. The mood of each piece in \nthe AMC test collection was judged by three MIREX \nevaluators whose cultural backgrounds ranged from Eu-\nrope, America and Asia [6]. In order to avoid including \nthe “obvious” examples that received high agreement \nfrom evaluators in the test data set, we selected the songs \nfor which there was greater disagreement among the \nMIREX evaluators. As these pieces were instrumental \nwithout a vocal part, we balanced our test set by drawing \nthe other half of the 30 pieces from the USPOP collection \n[3] and ensured that they all had vocal components. The \nsongs from the USPOP collection were chosen similarly \nto the APM songs; we selected songs that had greater dis-\nagreement in the mood judgments from six IMIRSEL \nmembers (c.f., [6]).  \n We recruited two user groups for the survey: people \nwho were raised in Mainland China and considered them-\nselves “Chinese”, and people who were raised in the \nUnited States, and considered themselves “American”. \nAll subjects were recruited from large universities in the \nU.S. The survey was deployed in both Chinese (Mandarin) \nand English; all the mood labels in English were translat-\ned into Chinese by the first author for the Chinese survey. \nIn both surveys, we asked the users if they had heard the \n                                                           \n1Music Information Retrieval Evaluation eXchange is the annual evalu-\nation campai\ngn for various music information retrieval algorithms host-\ned by the International Music Information Retrieval Systems Evaluation \nLab (IMIRSEL) at the University of Illinois at Urbana-Champaign.  \n2 http://www.apmmusic.com/pages/aboutapm.html    \n \nmusic clip b\nefore and if they could name the artist and the \nsong title in order to gauge their familiarity with the \nsongs. Figure 1 shows the screenshot of the online survey \nin English. \n \nFigure 1. Survey interface for American listeners \n4. DATA AND DISCUSSION \n4.1 Overview  \nThere were a total of 55 responses from Chinese and 45 \nfrom Americans; however, not all responses were com-\nplete. 31 listeners completed the Chinese and American \nsurveys, respectively, for a total of 62 complete responses. \nTable 2 shows the demographic information of the select-\ned respondents. Among the Chinese respondents; 23 of \nthem had been living in the U.S. for less than 2 years, 3 \nhad been in the U.S. for 3-5 years, 1 for 6-8 years, and 4 \nfor 9-11 years.   \nCultural \nbackground Age Gender \nMin Max Avg. Male Female  \nAmerican 22 55 31.8 6 25 \nChinese 19 46 26.2 10 21 \nTable 2. Demographics of survey respondents \nIn the following data analysis, chi square ( χ2) statistics \nare used to \ntest whether distributions of two categorical \nvariables (e.g., cultural background, mood categories) are \nindependent from each other [12] unless noted otherwise. \nIn the following subsections, we will answer each re-\nsearch question based on analysis of the survey responses. \n4.2 Difference Between Cultural Groups \n4.2.1 Mood Judgments on All Songs \nFigure 2 shows the distribution of mood judgments of the \ntwo groups on all 30 songs. Chinese users selected mood \nCluster 1 ( passionate, rousing, confident, boisterous, rowdy) and Cluster 3 ( literate, poignant, wistful, bitter-\nsweet, autumnal, brooding ) more often than Americans \nwhereas Americans chose Cluster 2 ( rollicking, cheerful, \nfun, sweet, amiable/good natured ) more often than Chi-\nnese. Both groups had similar numbers of judgments on \nCluster 4 ( humorous, silly, campy, quirky, whimsical, wit-\nty, wry) and Cluster 5 ( aggressive, fiery, tense/anxious, \nintense, volatile, visceral ). More Americans than Chinese \nchose the “other” option. A chi square test indicates that \nlisteners’ selection of mood clusters significantly depends \non the cultural group they belong to ( χ2 = 73.64, df = 5, p \n< 0.0001). I\nn other words, there was a significant differ-\nence between Americans and Chinese when examining \ntheir mood judgments as a whole. A follow-up Tukey \nmultiple comparison test [15] showed that judgments of \nthe two cultural groups on the “other” cluster were signif-\nicantly different from those on all the other clusters (at p \n< 0.05), judgments on Cluster 2 were significantly differ-\nent from those on Cluster 1 and 3 (at p < 0.05), and there \nwere no significant differences between judgments on \nother pairs of mood clusters. \n \nFigure 2. Distribution of mood judgments of the two \ngroups \n4.2.2 Agreement on Mood Judgments  \nIn order to find out whether listeners from the same cul-\ntural group would agree more with each other than with \nlisteners from another cultural group, we calculated the \nlevel of agreement on mood judgments among individual \nlisteners. For categorical data such as the mood judg-\nments, agreement is typically calculated based on the \nSokal-Michener coefficient, which is a ratio of the num-\nber of pairs with the same values and the total number of \nvariables (songs in this case) [12]. For instance, if two \nlisteners i and j had the same mood judgment on 15 of the \n30 songs, the agreement ratio between them will be 0.5. \nTable 3 shows the average agreement ratio among pairs \nof listeners within and across cultural groups in this study. \nWithin each cultural group, users show 0.35 agreement \nrate. However, across cultural groups the agreement rate \ndrops to 0.30. A non-pair wise t-test was conducted to \ntest the significance of the difference on agreement ratio \nwithin each cultural group and across cultural groups. \nBoth tests revealed a statistically significant difference. \nTherefore, our data support the hypothesis that listeners \n  \n \ntended to ag\nree more with others from the same cultural \nbackground than those from another cultural background. \n American  Chinese T statistics  p-value \nAmerican 0.35 0.30 11.44  <0.001 \nChinese 0.30 0.35 12.24  <0.001 \nTable 3. Average agreement ratio within and between \ncultural groups \n4.3 Mood Clusters vs. Cultural Groups \nWe also investigated which mood clusters received high-\ner agreement from people in each cultural group. We ex-\namined all pairs of responses from each group. Since \nthere were 31 responses from each group, there were a \ntotal of 465 pairs of responses within each group. Each \nresponse had 30 mood judgments, thus there were 13,950 \npairs of judgments in each group. Between the two cul-\ntural groups, there are 31 * 31 = 961 pairs of responses \nand 961 * 30 = 28,830 pairs of judgments. Table 4 lists \nthe number of agreed pairs of judgments on each mood \ncluster within each cultural group and across cultural \ngroups. It shows that American listeners agreed more on \nCluster 2 and 5 while Chinese listeners agreed more on \nCluster 1 and 3. The difference between the two groups is \nstatistically significant ( χ2 = 668, df = 5, p < 0.0001).  \n \nC_1 C_2 C_3 C_4  C_5 Other Total \nAmerican 706 1477  778 587 1094  270 4912  \nChinese 1355 995 1203  443 894 11 4901  \nAcross 1704 2122 1713 881 1999  131 8550  \nTable 4. Number of agreed pairs of judgments across \nmood clusters \n4.4 Song Characteristics vs. Cultural Groups \nHalf of the test songs were instrumental and the other half \nwere vocal. The two cultural groups showed significant \ndifference in judging the mood for both instrumental ( χ2 = \n88.09, df =  \n5, p < 0.0001) and vocal songs ( χ2 = 28.98, df \n= 5, p < 0.000\n1). Table 5 shows the agreement ratios \namong all judgment pairs on instrumental and vocal \nsongs. Lyrics definitely seem to help achieve a higher \nagreement for Americans while they have essentially no \neffect on Chinese (it should be noted that all lyrics were \nin English). In addition, cross-culturally the two groups \nwere more likely to provide different judgments on in-\nstrumental pieces than vocal ones. As discussed in [8], \neven if Chinese listeners cannot comprehend the lyrics as \nwell as American listeners, the delivery of the singer may \nstill affect how they determine the mood of the song. \n Instrumental  Vocal All \nAmerican 0.28 0.41 0.35 \nChinese 0.36 0.35 0.35 \nAcross 0.25 0.34 0.30 \nTable 5. Agreement ratio on instrumental vs. vocal songs  \nWe also looked at the genres of the songs as provided \nby APM and USPOP. Table 6 shows the genre distribu-tion of the songs as well as instrumental vs. vocal infor-\nmation. Dance and Easy-listening songs were all instru-\nmental while songs in the remaining genres were mostly \nvocal. For each of the five genres, mood judgments were \nsignificantly dependent on cultural groups ( χ2 = 21.91 ~ \n46.68, df =  \n5, p < 0.001). Table 6 also shows the agree-\nment ratios across genres. As it can be seen, Americans \nagreed more on Pop songs whereas Chinese agreed more \non songs in Other and Easy-listening. Cross-cultural \nagreement levels are generally lower than those within \ncultural groups. Among all the genres, Dance and Easy-\nlistening songs had the least cross-cultural agreement. \n Dance  Easy-\nlistening Pop Rock  Other Total \nInstru. 4 5 2 1 3 15 \nVocal 0 0 5 7 3 15 \nAmerican  0.30 0.29 0.46 0.35 0.31 0.35 \nChinese 0.29 0.38 0.32 0.35 0.41 0.35 \nAcross 0.22 0.28 0.33 0.31 0.30 0.30 \nTable 6. Song distribution and agreement ratio across \ngenres \n4.5 Listener Characteristics vs. Cultural Groups \nThe aggregated mood judgments across songs and cultur-\nal groups were still statistically significant when we con-\nsider the gender of the listeners (i.e., Chinese male vs. US \nmale, χ2 = 18.28, df = 5, p = 0.0026; C hinese female vs. \nUS female, χ2 = 52.83, df = 5, p < 0.0001). \nA\ns Table 2 shows, the Chinese respondents in this \nstudy were generally younger than the American re-\nspondents. To minimize the possible influence of age on \nmood judgments, we compared the answers from listen-\ners of the same age range (22-46 years old) in both cul-\nture groups (24 Chinese and 28 American). The mood \njudgments of two cultural groups were still significantly \ndifferent (χ2 = 61.85, df = 5, p < 0.0001).  \nIn this stu d\ny, a listener’s familiarity with a song is \nmeasured by their answers to two questions: 1) whether \nhe or she had heard the song before; and 2) whether he or \nshe can identify the artist name and song title. A “no” an-\nswer to both questions indicates low familiarity, a “yes” \nto both questions indicates high familiarity, and a “yes” \nand a “no” indicates medium familiarity. The reason for \nusing these two questions instead of directly asking the \nlisteners their level of familiarity is because people may \nhave different interpretations on song familiarity. Some \npeople might consider a song familiar if it invokes any \nmemory while other people might not think it is familiar \nunless they could actually sing part of the song. The two \nquestions are objective, and thus are easier to answer and \navoid personal biases. Table 7 shows the distribution of \nthe level of familiarity across Americans and Chinese. As \nthe test songs were Western songs, it is not surprising that \nAmerican listeners were more familiar with the songs \nthan Chinese listeners. \nIn order to see whether the level of familiarity has an \neffect on mood judgment agreement, we calculated the \nagreement ratio with various combinations of familiarity   \n \nlevels in ea\nch cultural group as well as across cultural \ngroups. Each cell in Table 8 shows the agreement ratio \namong all judgment pairs with corresponding familiarity \nlevels. \n Unfamiliar  Medium Familiar N/A Total  \nAmerican 617 120 192 1 930 \nChinese 836 75 14 5 930 \nTable 7. Distribution of the level of familiarity American  Unfamiliar  Medium Familiar \nUnfamiliar  0.32 0.36 0.36 \nMedium - 0.43 0.44 \nFamiliar - - 0.44 Chinese Unfamiliar  0.35 0.37 0.23 \nMedium - 0.36 0.22 \nFamiliar - - 0.24 Across Unfamiliar  0.28 0.31 0.29 \nMedium - 0.37 0.33 \nFamiliar - - 0.24 \nTable 8. Agreement ratio across different levels of famil-\niarity \nFor American listeners, being familiar with the songs \ndid improve the odds of agreeing. However, medium and \nhigh familiarity did not appear to have much effect on \nagreement. For Chinese listeners, having heard the songs \nbefore (medium familiarity) slightly increased the agree-\nment ratio, but high familiarity with the songs appears to \nactually decrease agreement, which is unintuitive. We \nsuspect that this might be due to the sparseness of the \nsamples: there were only 14 (out of 930) cases where the \nChinese listeners were highly familiar with a song [Table \n7]. It may also indicate that a mere identification of title \nand/or artist name from Chinese listeners does not imply \nthat they understood what the song was about. Table 8 \nalso shows that the agreement ratios between Americans \nand Chinese are lower than those of Americans. \n5. DISCUSSION \nOur analysis suggests that there is in fact a significant dif-\nference between how Americans and Chinese perceived \nmusic mood. From the total number of mood judgments \nacross mood clusters, Chinese listeners chose Cluster 1 \nmore often than American listeners. We conjecture that \nthis difference may be attributed to the differences be-\ntween Chinese and Western cultures. In Chinese culture, \npeople tend to restrain the expression of feelings and \nChinese people are generally more introverted compared \nto Western people [9], and thus may be more likely to \nthink a Western music piece is “passionate,” “rousing,” \nor “boisterous” (Cluster 1). Previous research also found \nthat Chinese value low-arousal positive affect (e.g., calm) \nwhereas Westerners value high-arousal positive affect \n[11]. This may help explain the higher responses on Clus-\nter 3 for Chinese listeners and Cluster 2 for American lis-\nteners. When the mood of the song is not clear, people may end up selecting moods that they generally prefer \nsince they are more likely to focus on those moods; in \nother words, they hear what they want to hear.  \nIt is also interesting to see fewer judgments of “other” \namong Chinese users. This may be related to the collec-\ntivism commonly seen in Eastern cultures and the indi-\nvidualism in Western cultures. Chinese listeners used one \nof the given five mood clusters 96.6% of the time. How-\never, American listeners disagreed with the presented \nmood clusters more often, using the “other” option for \n11.9% of their judgments, more than 3 times as often as \nChinese.   \nThese findings have implications for designing MIR \nsystems for people with different cultural backgrounds. \nFor example, a mood classification system may be de-\nsigned so that it treats songs with mixed moods in a dif-\nferent way: categorizing them into Cluster 2 for Ameri-\ncans vs. Cluster 1 or 3 for Chinese reflecting their expec-\ntations. In addition, while we generally think it is more \nuser-friendly to let users browse for music with different \nmoods than asking users to search with their own mood \nterms, this would appear to be even more critical for Chi-\nnese listeners, as they seem to prefer using given organ-\nizational structures rather than providing their own input \n(via the “other” option as shown in Figure 1).      \nAmong all the 30 test songs, the one with the highest \ndisagreement between Americans and Chinese was Got to \nget you into my life  by The Beatles. Figure 3 shows the \njudgment distribution across mood clusters for this song. \nMost of the Americans answered Cluster 2 while the \nChinese’ answers were spread out across multiple mood \nclusters with most answers in Cluster 4. A closer look at \nthe data revealed that none of the Chinese listeners had \nlistened to this song prior to this survey whereas 29 out of \n31 American listeners had listened to this song and 19 of \nthem were familiar enough to this song that they could \nname the artist. Cultural background evidently played an \nimportant role in mood judgments on this song. The Beat-\nles are a symbol of the Western pop culture. Western lis-\nteners had probably been influenced by their background \nknowledge of the song and the band that this song should \nexpress a rollicking and cheerful (Cluster 2) or passionate \n(Cluster 1) mood. It is also possible that they were able to \nprovide a mood judgment based on the whole song rather \nthan the 30 seconds clip provided in the survey. In con-\ntrast, the Chinese listeners had no prior influence on how \nothers had felt about this song or rest of the Beatles’ \nsongs, thus their answers were not as consistent as those \nfrom the American listeners.  \nLyrics seem to affect how the Americans judge the \nmood of the songs, but not the Chinese [Table 5]. Alt-\nhough most of the Chinese listeners in this study could \nunderstand English, the music pieces in the survey are \nprobably too short (30 seconds) for them to fully com-\nprehend the lyrics and use them in mood judgments. Gen-\nre also affects people’s agreement on music moods, but in \ndifferent ways for different cultural groups. The fact that \nAmericans agreed more on Pop songs is possibly related \nto their being familiar with the songs.    \n \n \nFigure 3.  Moo\nd judgment distributions for Got to get \nyou into my life \nAs previously discussed, listeners’ familiarity with the \nsongs affects the level of agreement on mood judgments, \nbut the influence is much stronger within the same cultur-\nal group. This makes it challenging to build an MIR sys-\ntem for users with cultural backgrounds that are different \nfrom the particular culture the music is from. For cross-\ncultural MIR systems, perhaps more flexibility should be \nprovided to users. It may help to allow users to provide \nannotations so that they can complement the given “cor-\nrect” mood labels.  In such systems, it will be possible for \nusers to assign multiple mood labels to a song, change a \nsong’s mood labels, or add alternative labels to the songs. \n6. CONCLUSION AND FUTURE WORK \nIn this paper, we presented a study comparing mood \njudgments on a common set of Western music pieces by \nAmerican and Chinese listeners. Listeners from the two \ncultural groups indeed have different mood judgments \nand they tended to agree more with users from the same \ncultural group. Some genres seemed to be more difficult \nto reach user agreement across two groups, although fur-\nther studies with a larger music samples should be con-\nducted to validate the result. The cultural difference per-\nsists even when we consider the age and gender. The lis-\nteners’ familiarity with the songs had a positive influence \non the agreement level among users from the same cul-\ntural background of the songs. Findings of this study not \nonly help further our understanding on how cultural \nbackground affects mood perception, but also have impli-\ncations for designing cross-cultural MIR systems. \nIt should be noted that the Chinese respondents in this \nstudy have lived in the U.S., but there were not enough \ndata to analyze the influence of this factor on their music \nmood judgments. In our future study, we will collect re-\nsponses from Chinese people in China and compare the \nresults. We will also investigate why people assign tracks \nto certain mood clusters by conducting in-depth inter-\nviews. In addition, we plan to increase the diversity of \nour user group by including users from countries other \nthan China and United States such as Korea. Although \nKorea also represents non-Western culture, and Chinese \nand Korean cultures historically share a great deal of sim-\nilarities, Korea is much more heavily influenced by \nAmerican pop culture than China. Thus, comparing user \ngroups from these countries may provide insights into how the exposure to other pop culture can affect the way \npeople perceive the mood of music.    \n7. REFERENCES \n[1] L. Balkwillm and W. F. Thompson: “A Cross-cultural \ninvestigat i\non of the perception of emotion in music: \npsychophysical and cultural cues,” Music Perception , Vol. \n17, No. 1, pp. 43-64, 1999. \n[2] T. Eerola, T. Himberg, P. Toiviainen and J. Louhivuori: \nPerceived complexity of Western and African folk \nmelodies by Western and African listeners, Psychology of \nMusic, Vol. 34, No. 3, pp. 337-371, 2006. \n[3] D. Ellis, A. Berenzweig and B. Whitman: “The USPOP \n2002 Pop Music Data Set,” Retrieved from \nhttp://labrosa.ee.columbia.edu/projects/musicsim/uspop20\n02.html, 2003. \n[4] T. Fritz, S. Jentschke, N. Gosselin, D. Sammler, I. Peretz, \nR. Turner, A. D. Friederici and S. Koelsch: “Universal \nrecognition of three basic emotions in music,” Current \nBiology,Vol.19, pp.573–576, 2009. \n[5] A. H. Gregory and N. Varney: “Cross-cultural \ncomparisons in the affective response to music,” \nPsychology of Music , Vol. 24, pp. 47-52, 1996. \n[6] X. Hu, J. S. Downie, C. Laurier, M. Bay and A. F. \nEhmann: “The 2007 MIREX Audio Mood Classification \ntask: Lessons learned,” Proceedings of the 9th Interna-\ntional Society for Music Information Retrieval (ISMIR) \nConference , pp. 462-467, 2008. \n[7] J. H. Lee, J. S. Downie, and S. J. Cunningham: \n“Challenges in cross-cultural/multilingual music \ninformation seeking,” Proc. of ISMIR , pp. 1-7, 2005. \n[8] J. H. Lee, T. Hill, and L. Work: “What does music mood \nmean for real users?”  Proceedings of the iConference , \n2012.  \n[9] S. J. Morrison, S. M. Demorest and L. A. Stambaugh: \n“Enculturation Effects in Music Cognition: The Role of \nAge and Music Complexity,” Journal of Research in \nMusic Education , Vol. 56, No. 2 pp. 118-129, 2008. \n[10] R. R. McCrae, P. T. Costa, Jr. and M. Yik: “Universal \naspects of Chinese personality structure,” In M. H. Bond \n(Ed.) The handbook of Chinese psychology . Hong Kong: \nOxford University Press, pp.189-207, 1996. \n[11] E. Nettamo, M. Norhamo, and J. Häkkilä: “A cross-\ncultural study of mobile music: Retrieval, management \nand consumption,” Proceedings of OzCHI 2006 , pp. 87-94, \n2006. \n[12] R. R. Sokal and C. D. Michener: “A statistical method for \nevaluating systematic relationships,” University of Kansas \nScience Bulletin,  Vol. 38, pp. 1409–1438, 1958. \n[13] J. L. Tsai, B. Knutson and H. H. Fung: “Cultural variation \nin affect valuation,” Journal of Personality and Social \nPsychology , Vol.90, No.2, pp. 288–307, 2006. \n[14] P. C. M. Wong, A. K. Roy and E. H. Margulis: \n“Bimusicalism: The implicit dual enculturation of \ncognitive and affective systems,” Music Percept , Vol. 27, \nNo. 2, pp. 81–88, 2009. \n[15] J. H. Zar: Biostatistical Analysis , Fourth Edition, Prentice \nHall, 1999."
    },
    {
        "title": "Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics.",
        "author": [
            "Eric J. Humphrey",
            "Juan Pablo Bello",
            "Yann LeCun"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415726",
        "url": "https://doi.org/10.5281/zenodo.1415726",
        "ee": "https://zenodo.org/records/1415726/files/HumphreyBL12.pdf",
        "abstract": "The short history of content-based music informatics re- search is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hope- fully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful rep- resentations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only re- cently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike.",
        "zenodo_id": 1415726,
        "dblp_key": "conf/ismir/HumphreyBL12",
        "keywords": [
            "hand-crafted feature design",
            "de facto standards",
            "commendable progress",
            "diminishing returns",
            "heuristic feature design",
            "shallow processing architectures",
            "hope-fully irrelevant information",
            "creativity",
            "intuition",
            "deep learning"
        ],
        "content": "MOVING BEYOND FEATURE DESIGN: DEEP ARCHITECTURES AND\nAUTOMATIC FEATURE LEARNING IN MUSIC INFORMATICS\nEric J. Humphrey, Juan Pablo Bello\nMusic and Audio Research Lab, NYU\n{ejhumphrey, jpbello}@nyu.eduYann LeCun\nCourant School of Computer Science, NYU\nyann@cs.nyu.edu\nABSTRACT\nThe short history of content-based music informatics re-\nsearch is dominated by hand-crafted feature design, andour community has grown admittedly complacent with afew de facto standards. Despite commendable progress inmany areas, it is increasingly apparent that our efforts areyielding diminishing returns. This deceleration is largelydue to the tandem of heuristic feature design and shallowprocessing architectures. We systematically discard hope-fully irrelevant information while simultaneously callingupon creativity, intuition, or sheer luck to craft useful rep-resentations, gradually evolving complex, carefully tunedsystems to address speciﬁc tasks. While other disciplineshave seen the beneﬁts of deep learning, it has only re-cently started to be explored in our ﬁeld. By reviewingdeep architectures and feature learning, we hope to raiseawareness in our community about alternative approachesto solving MIR challenges, new and old alike.\n1. INTRODUCTION\nSince the earliest days of music informatics research (MIR),content-based analysis, and more speciﬁcally audio-basedanalysis, has received a signiﬁcant amount of attention fromour community. A number of surveys (e.g. [8, 22, 29])amply document what is a decades-long research effortat the intersection of music, machine learning and signalprocessing, with wide applicability to a range of tasks in-cluding the automatic identiﬁcation of melodies, chords,instrumentation, tempo, long-term structure, genre, artist,mood, renditions and other similarity-based relationships,to name but a few examples. Yet, despite a heterogeneityof objectives, traditional approaches to these problems arerather homogeneous, adopting a two-stage architecture offeature extraction and semantic interpretation, e.g. classi-ﬁcation, regression, clustering, similarity ranking, etc.\nFeature representations are predominantly hand-crafted,\ndrawing upon signiﬁcant domain-knowledge from musictheory or psychoacoustics and demanding the engineeringacumen necessary to translate those insights into algorith-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.\nc\u00002012 International Society for Music Information Retrieval.mic methods. As a result, good feature extraction is hard\nto come by and even more difﬁcult to optimize, often tak-ing several years of research, development and validation.Due in part to this reality, the trend in MIR is to focus onthe use of ever-more powerful strategies for semantic in-terpretation, often relying on model selection to optimizeresults. Unsurprisingly, the MIR community is slowly con-verging towards a reduced set of feature representations,such as Mel-Frequency Cepstral Coefﬁcients (MFCC) orchroma, now de-facto standards. This trend will only be-come more pronounced given the growing popularity oflarge, pre-computed feature datasets\n1.\nWe contend the tacit acceptance of common feature ex-\ntraction strategies is short-sighted for several reasons: ﬁrst,the most powerful semantic interpretation method is onlyas good as a data representation allows it to be; second,mounting evidence suggests that appropriate feature rep-resentations signiﬁcantly reduce the need for complex se-mantic interpretation methods [2, 9]; third, steady incre-mental improvements in MIR tasks obtained through per-sistence and ingenuity indicate that the the costly practiceof manual feature optimization is not yet over; and fourth,task-speciﬁc features are ill-posed to address problems forwhich they were not designed (such as mood estimationor melody extraction), thus limiting their applicability tothese and other research areas that may emerge.\nIn this paper we advocate a combination of deep sig-\nnal processing architectures and automatic feature learn-ing as a powerful, holistic alternative to hand-crafted fea-ture design in audio-based MIR. We show how deeper ar-chitectures are merely extensions of standard approaches,and that robust music representations can be achieved bybreaking larger systems into a hierarchy of simpler parts(Section 3). Furthermore, we also show that, in light of ini-tial difﬁculties training ﬂexible machines, automatic learn-ing methods now exist that actually make these approachesfeasible, and early applications in MIR have shown muchpromise (Section 4). This formulation provides severalimportant advantages over manual feature design: ﬁrst, itallows for joint, fully-automated optimization of the fea-ture extraction and semantic interpretation stages, blurringboundaries between the two; second, it results in general-purpose architectures that can be applied to a variety ofspeciﬁc MIR problems; and lastly, automatically learnedfeatures can offer objective insight into the relevant mu-sical attributes for a given task. Finally, in Section 5, we\n1Million Song Dataset: http://labrosa.ee.columbia.edu/millionsong/conclude with a set of potential challenges and opportuni-\nties for the future.\n2. CLASSIC APPROACHES TO CLASSIC\nPROBLEMS\n2.1 Two-Stage Models\nIn the ﬁeld of artiﬁcial intelligence, computational percep-\ntion can be functionally reduced to a two-tiered approachof data representation and semantic interpretation. A sig-nal is ﬁrst transformed into a data representation where itsdeﬁning characteristics are made invariant across multiplerealizations, and semantic meaning can subsequently be in-ferred and used to assign labels or concepts to it. Often thegoal in music informatics is to answer speciﬁc questionsabout the content itself, such as “is this a C major triad?”or “how similar are these two songs?”\nMore so than assigning meaning, the underlying issue\nis ultimately one of organization and variance. The betterorganized a representation is to answer some question, thesimpler it is to assign or infer semantic meaning. A rep-resentation is said to be noisy when variance in the data\nis misleading or uninformative, and robust when it pre-\ndictably encodes these invariant attributes. When a rep-resentation explicitly reﬂects a desired semantic organiza-tion, assigning meaning to the data becomes trivial. Con-versely, more complicated information extraction methodsare necessary to compensate for any noise.\nIn practice, this two-stage approach proceeds by feature\nextraction – transforming an observed signal to a hope-fully robust representation – and either classiﬁcation or re-gression to model decision-making. Looking back to ourrecent history, there is a clear trend in MIR of applyingincreasingly more powerful machine learning algorithmsto the same feature representations to solve a given task.In the ISMIR proceedings alone, there are twenty docu-ments that focus primarily on audio-based automatic chordrecognition. All except one build upon chroma features,and over half use Hidden Markov Models to stabilize clas-siﬁcation; the sole outlier uses a Tonnetz representation,which are tonal centroid features derived from chroma.Though early work explored the use of simple binary tem-plates and maximum likelihood classiﬁers, more recentlyConditional Random Fields, Bayesian Networks, and Sup-port Vector Machines have been introduced to squeeze ev-ery last percentage point from the same features.\nIf a feature representation were truly robust, the com-\nplexity of a classiﬁer – and therefore the amount of vari-ance it could absorb – would have little impact on per-formance. Previous work in automatic chord recognitiondemonstrates the signiﬁcance of robust feature representa-tions, showing that the appropriate ﬁltering of chroma fea-tures leads to a substantial increase in system performancefor the simplest classiﬁers, and an overall reduction of per-formance variation across all classiﬁers [9]. Additionally,researchers have for some time addressed the possibilitythat we are converging to glass ceilings in content-basedareas like acoustic similarity [2]. Other hurdles, like the is-sue of hubs and orphans, have been shown to be not merelya peculiarity of the task but rather an inevitability of thefeature representation [20]. As we consider the future ofMIR, it is necessary to recognize that diminishing returnsin performance are far more likely the result of sub-optimalfeatures than the classiﬁer applied to them.\n2.2 From Intuition to Feature Design\nMusic informatics is traditionally dominated by the hand-\ncrafted design of feature representations. Noting that de-sign itself is a well-studied discipline, a discussion of fea-ture design is served well by the wisdom of “getting theright design and the design right” [6]. Reducing this apho-rism to its core, there are two separate facets to be con-sidered: ﬁnding the right conceptual representation for agiven task, and developing the right system to produce it.\nConsider a few signal-level tasks in MIR, such as onset\ndetection, chord recognition or instrument classiﬁcation,noting how each offers a guiding intuition. Note onsets aretypically correlated with transient behavior. Chords are de-ﬁned as the combination of a few discrete pitches. Classicstudies in perception relate timbre to aspects of spectralcontour [12]. Importantly, intuition-based design hingeson the assumption that someone can know what informa-tion is necessary to solve a given problem.\nHaving found conceptual direction, it is also necessary\nto craft the right implementation. This has resulted in sub-stantial discourse and iterative tuning to determine betterperforming conﬁgurations of the same basic algorithms.Much effort has been invested in determining which ﬁl-ters and functions make better onset detectors [3]. Chroma– arguably the only music-speciﬁc feature developed byour community – has undergone a steady evolution sinceits inception, gradually incorporating more levels of pro-cessing to improve robustness [28]. Efforts to characterizetimbre, for which a meaningful deﬁnition remains elusive,largely proceed by computing numerous features or, morecommonly, the ﬁrst several MFCCs [11].\nIn reality, feature design presents not one but two chal-\nlenges – concept and implementation – and neither haveproven easy to solve. First off, our features are ultimatelyconstrained to those representations we can conceive orcomprehend. Beyond relatively obvious tasks like onsetdetection and chord recognition, we can only begin to imag-ine what abstractions might be necessary to perform ratherabstract tasks like artist identiﬁcation. Furthermore, recog-nizing that feature extraction is still an open research topic,the considerable inertia of certain representations is causefor concern: 19 of 26 signal-based genre classiﬁcation sys-tems in the ISMIR proceedings are based on MFCCs, forexample, many using publicly-available implementations.While sharing data and software is a commendable trend,now is a critical point in time to question our acceptanceof these representations as we move toward the widespreaduse of pre-computed feature collections, e.g. the MillionSong Dataset. Finally, above all else, the practice of hand-crafted feature design is simply not sustainable. Manuallyoptimizing feature extraction methods proceeds at a glacialpace and incurs the high costs of time, effort and funding.\nSomewhat ironically, the MIR community has collectivelyrecognized the beneﬁts of automatically ﬁtting our classi-ﬁers, but feature optimization – the very data those meth-ods depend on – remains largely heuristic.\nAlternatively, data-driven approaches in deep learning\nhave recently shown promise toward alleviating each andevery one of these issues. Proven numerical methods canadapt a system inﬁnitely faster than is attainable by our cur-rent research methodology, and the appropriate conceptualrepresentations are realized as a by-product of optimizingan objective function. In the following section, we will il-lustrate how robust feature representations can be achievedthrough deep, hierarchical structures.\n3. DEEP ARCHITECTURES\n3.1 Shallow Architectures\nTime-frequency analysis is the cornerstone of audio sig-\nnal processing, and modern architectures are mainly com-prised of the same processing elements: linear ﬁltering,matrix transformations, decimation in time, pooling acrossfrequency, and non-linear operators, such as the complexmodulus or logarithmic compression. Importantly, the com-bination of time-domain ﬁltering and decimation is oftenfunctionally equivalent to a matrix transformation – theDiscrete Fourier Transform (DFT) can be easily interpretedas either, for example – and for the sake of discussion, werefer to these operations collectively as projections.\nNow, broadly speaking, the number of projections con-\ntained within an information processing architecture deter-mines its depth. It is critical to recognize, however, that\nthe extraction of meaningful information from audio pro-ceeds by transforming a time-varying function – a signal– into an instantaneous representation – features; at somespeciﬁcity, all signals represent static concepts, e.g., a sin-gle piano note versus the chorus of a song. Therefore, thedepth at which a full signal is summarized by a stationaryfeature vector is characteristic of a signal processing archi-tecture, and is said to be particularly shallow if an entire\nsystem marginalizes the temporal dimension with only asingle projection.\nThis is a subtle, but crucial, distinction to make; fea-\ntureprojections, lacking a time dimension, are a subset of\nsignal projections. As we will see, shallow signal process-\ning architectures may still incorporate deep feature projec-tions, but the element of time warrants special attention. Asignal projection that produces a ﬁnite set of stationary fea-tures attempts to capture allrelevant information over the\nobservation, and any down-stream representations are con-strained by whatever was actually encoded in the process.Importantly, the range of observable signals becomes inﬁ-nite with increasing duration, and it is progressively moretaxing for signal projections – and therefore shallow archi-tectures – to accurately describe this data without a sub-stantial loss of information.\nTo illustrate the point further, consider the two signal\nprocessing architectures that produce Tonnetz and MFCCFiltering /\nDownsamplingAudio Signal\nConstant-Q Mel-scale\nNon-Linear \nOperatorsModulus /\nLog-ScalingModulus /\nLog-Scaling\nPoolingOctave \nEquivalnce\nFeature\nProjectionTonal Centroid\nProjectionDiscrete Cosine \nProjection\nFeatures Tonnetz MFCC\nFigure 1: Tonnetz and MFFCs from Shallow Architectures\nfeatures. As shown in Figure 1, the processing chains are\nnearly identical; note that the penultimate representationwhen computing Tonnetz features is chroma. Both beginwith a signal projection that maps a time-domain signal toan instantaneous estimation of frequency components, andconclude with a feature projection that reorganizes the esti-mated frequencies in task-speciﬁc ways. The overwhelm-ing majority of music signal processing architectures op-erate in this paradigm of shallow signal transformations.Subject to the Fourier uncertainty principle, these systemsexhibit time-frequency trade-offs and are constrained inpractice to the analysis of short observations.\nThe vast majority of musical experiences do not live in\nshort signals however, and it is therefore necessary to char-acterize information over longer durations. Previous ef-forts recognize this deﬁciency and address it through oneof a few simple methods: a bag of frames (BoF) models\nfeatures as a probability distribution, shingling concate-\nnates feature sequences into a vector, or delta-coefﬁcients\nrepresent low-order derivatives calculated over local fea-tures. These naive approaches are ill-posed to characterizethe temporal dynamics of high-level musical concepts likemood or genre, and arguably contribute to the “semanticgap” in music informatics. It will become clear in the fol-lowing discussion why this is the case, and how deeperarchitectures can alleviate this issue.\n3.2 Motivating Deeper Architectures\nThis previous discussion begs a rather obvious question:\nwhy are shallow architectures poorly suited for music sig-nal processing? If we consider how music is constructed,it is best explained by a compositional containment hier-\narchy. The space of musical objects is not ﬂat, but ratherpitch and intensity combine to form chords, melodies andrhythms, which in turn build motives, phrases, sections andentire pieces. Each level uses simpler elements to producean emergent whole greater than the sum of its parts, e.g., amelody is more than just a sequence of pitches.\nIn a similar fashion, deeper signal processing structures\ncan be realized by stacking multiple shallow architectures,\nand are actually just extensions of modern approaches. Fora signal projection to marginalize time with a minimal lossof information, the observation must be locally stationary,and clearly this cannot hold for long signals. Sequencesof instantaneous features, however, are again time-varyingdata and, when appropriately sampled, arethemselves lo-\ncally stationary signals. There are two remarkable conclu-sions to draw from this. First, everything we know aboutone-dimensional signal processing holds true for a time-feature signal and can be generalized thusly. And further-more, simply cascading multiple shallow architectures re-laxes previous constraints on observation length by pro-ducing locally stationary signals at various time-scales.\nThis hierarchical signal processing paradigm is at the\nheart of deeper architectures. There are many beneﬁts de-tailed at length in [4], but two are of principal importancehere: one, multi-layer processing allows for the emergenceof higher-level attributes for two related reasons: deep struc-tures can break down a large problem into a series of eas-ier sub-problems, and each requires far fewer elements tosolve than the larger problem directly; and two, each layercan absorb some speciﬁc variance in the signal that is dif-ﬁcult or impossible to achieve directly. Chord recognitioncaptures this intuition quite well. One could deﬁne everycombination of absolute pitches in a ﬂat namespace andattempt to identify each separately, or they could be com-posed of simpler attributes like intervals. Slight variations,like imperfect intonation, can be reconciled by a composi-tion of intervals, whereas a ﬂat chord-space would need toaddress this explicitly.\nBoth of these beneﬁts are observed in the successful ap-\nplication of convolutional neural networks (CNN) to hand-written digit classiﬁcation [25]. Most prior neural networkresearch in computer vision proceeded by applying multi-layer perceptrons (MLP) directly to a pixel values of an im-age, which struggles to cope with spatial variation. Adopt-ing a CNN architecture introduces a hierarchical decom-position of small, locally-correlated areas, acting as signalprojections in space rather than time. Emergent propertiesof images are encoded in the visual geometry of edges, cor-ners, and so on, and the architecture is able to develop aninvariance to spatial translations and scaling.\nWithin audio signal processing, wavelet ﬁlterbanks, as\ncascaded signal projections, have been shown to capturelong-term information for audio classiﬁcation [1]. Thesesecond-order features yielded better classiﬁcation resultsthan ﬁrst-order MFCCs over the same duration, even al-lowing for convincing signal reconstruction of the originalsignals. This outcome is evidence to the fact that deepersignal processing architectures can lead to richer repre-sentations over longer durations. Observing that multi-layer architectures are simply extensions of common ap-proaches, it is fascinating to discover there is at least oneinstance in MIR where a deep architecture has naturallyevolved into the common solution: tempo estimation.\nSubband Decomposition\nOnset Detection\nPeriodicity AnalysisAudio\nFilterbank\nRectiﬁcation / Modulus\nNon-linear Compression\nPooling\nFigure 2: Tempo Estimation with Deep Signal Processing\nArchitectures.\n3.3 Deep Signal Processing in Practice\nUpon closer inspection, modern tempo estimation archi-\ntectures reveal deep architecture with strong parallels toCNNs and wavelets. Rhythmic analysis typically proceedsby decomposing an audio signal into frequency subbands[31]. This time-frequency representation is logarithmicallyscaled and subbands are pooled, reducing the number ofcomponents. Remaining subbands are ﬁltered in time bywhat amounts to an edge detector, rectiﬁed, pooled alongsubbands and logarithmically scaled to yield a novelty func-tion [23]. A third and ﬁnal stage of ﬁltering estimatestempo-rate frequency components in the novelty signal,producing a tempogram [13].\nOver the course of a decade, the MIR community has\ncollectively converged to a deep signal processing archi-tecture for tempo estimation and, given this progress, it ispossible to exactly illustrate the advantages of hierarchicalsignal analysis. In Figure 2, two waveforms with identi-cal tempi but different incarnations – a trumpet playing anascending D major scale and a series of bass drum hits,set slightly out of phase – are shown at various stages ofthe tempo estimation architecture. It is visually apparentthat each stage in the architecture absorbs a different typeof variance in the signal: pitch and timbre, absolute ampli-tude, and phase information, respectively. By ﬁrst breakingthe problem of tempo estimation into two sub-tasks – fre-quency estimation and onset detection – it becomes possi-ble to characterize subsonic frequencies at both lower sam-pling frequencies and with a fewer number of components.\nRealistically though, progress in tempo estimation is theresult of strong intuition that could guide system design.\nThe inherent challenge in building deep, hierarchical sys-tems is that intuition and understanding quickly depart af-ter more than even a few levels of abstraction. Therein liesthe most exciting prospect of this whole discourse; given awell-deﬁned objective function, it is possible to automati-cally learn both the right conceptual representation and theright system to produce it for a speciﬁc application.\n4. FEATURE LEARNING\n4.1 From Theory to Practice\nFor some time, a concerted effort in computer science has\nworked toward the development of convex optimizationand machine learning strategies. Unfortunately, the initialsurge of activity and excitement surrounding artiﬁcial in-telligence occurred well before technology could handlethe computational demands of certain methods, and as aresult many approaches were viewed as being intractable,unreasonable, or both. Over the last two or so decades, thestate of affairs in machine learning has changed dramati-cally, and for several reasons feature learning is now notonly feasible, but in many cases, efﬁcient.\nAlmost more importantly than its success as an image\nclassiﬁcation system, the work in [25] proved that stochas-tic gradient descent could be used to discriminatively trainlarge neural networks in a supervised manner. Given a suf-ﬁcient amount of labeled data, many applications in com-puter vision immediately beneﬁted from adopting these ap-proaches. Such datasets are not always available or evenpossible, and recent breakthroughs in unsupervised train-ing of Deep Belief Networks (DBNs) have had a similarimpact [17]. This work has also been extended to a convo-lutional variant (CDBNs), showing great promise for deepsignal processing [26]. Additionally, auto-encoder archi-tectures are a recent addition to the unsupervised traininglandscape and offer similar potential [21].\nThe signiﬁcance of ever-increasing computational power\nis also not to be overlooked in the proliferation of auto-matic feature learning. Steady improvements in processingspeed are now being augmented by a rise in parallel com-puting solutions and toolkits [5], decreasing training timesand accelerating research. Taken together, these strategiesencompass a set of deep learning approaches that hold sig-niﬁcant potential for applications in music informatics.\n4.2 Early Efforts in Music Informatics\nIt is necessary to note that leveraging data to automatically\nlearn feature representations is not a new idea. The earliesteffort toward automatic feature learning is that of [7, 33],where genetic algorithms were used to automatically learnoptimized feature transformations.Though not a deep ar-chitecture in the classic sense, this work formally recog-nized the challenge of hand-crafting musical representa-tions and pioneered feature learning in MIR.\nWith respect to deeper architectures, the ﬁrst successful\ninstance of deep feature learning is that of CNN-based on-set detection by [24]. More recently, CNNs have been ap-\nFigure 3: Learned Features for Genre Recognition\n(Reprinted with permission)\nplied to automatic genre recognition [27], instrument clas-\nsiﬁcation [19] and automatic chord recognition [18]. Alter-natively, DBNs have seen a noticeable rise in frame-levelapplications, such as instrument classiﬁcation [15], pianotranscription [30], genre identiﬁcation [14] and mood pre-diction [32], out-performing other shallow, MFCC-basedsystems. Incorporating longer time-scales, convolutionalDBNs have also been explored in the context of variousspeech and music classiﬁcation tasks in [26], and for artist,genre and key recognition [10]. Predictive sparse codinghas also been applied to genre recognition, earning “BestStudent Paper” at ISMIR 2011 [16].\nThe most immediate observation to draw from this short\nbody of work is that every system named above achievedstate-of-the-art performance, or better, in substantially lesstime than it took to get there by way of hand-crafted rep-resentations. Noting that many of these systems are theﬁrst application of deep learning in a given area of MIR,it is only reasonable to expect these systems to improve inthe future. For instance, DBNs have been primarily usedfor frame-level feature learning, and it is exciting to con-sider what might be possible when all of these methods areadapted to longer time scales and for new tasks altogether.\nA more subtle observation is offered by this last ef-\nfort in genre recognition [16]. Interestingly, the featureslearned from Constant-Q representations during trainingwould seem to indicate that speciﬁc pitch intervals andchords are informative for distinguishing between genres.Shown in Figure 3, learned dictionary elements capturestrong ﬁfth and octave interval relationships versus quar-tal intervals, each being more common in rock and jazz,respectively. This particular example showcases the po-tential of feature learning to reformulate established MIRtasks, as it goes against the long-standing intuition relatinggenre to timbre and MFCCs.\n5. THE FUTURE OF DEEP LEARNING IN MIR\n5.1 Challenges\nRealistically speaking, deep learning methods are not with-\nout their own research challenges, and these difﬁculties arecontributing factors to limited adoption within our commu-nity. Deep architectures often require a large amount oflabeled data for supervised training, a luxury music infor-matics has never really enjoyed. Given the proven success\nof supervised methods, MIR would likely beneﬁt a gooddeal from a concentrated effort in the curation of sharabledata in a sustainable manner. Simultaneously, unsuper-vised methods hold great potential in music-speciﬁc con-texts, as they tend to circumvent the two biggest issues fac-ing supervised training methods: the threat of over-ﬁttingand a need for labeled data.\nAdditionally, there still exists a palpable sense of mis-\ntrust among many toward deep learning methods. Despitedecades of fruitful research, these approaches lack a solid,foundational theory to determine how, why, and if they willwork for a given problem. Though a valid criticism, thisshould be appreciated as an exciting research area and nota cause for aversion. Framing deep signal processing archi-tectures as an extension of shallow time-frequency analysisprovides an encouraging starting point toward the develop-ment of more rigorous theoretical foundations.\n5.2 Impact\nDeep learning itself is still a ﬂedgling research area, and it\nis still unclear how this ﬁeld will continue to evolve. In thecontext of music informatics, these methods offer seriouspotential to advance the discipline in ways that cannot berealized by other means. First and foremost, it presentsthe capacity for the abstract, hierarchical analysis of musicsignals, directly allowing for the processing of informationover longer time scales. It should come as no surprise thatdetermining the similarity of two songs based on small-scale observations has its limitations; in fact, it should beamazing that it works at all.\nMore practically, deep learning opens the door for the\napplication of numerical optimization methods to accel-erate research. Instead of slowly converging to the bestchroma transformation by hand, an automatically trainedsystem could do this in a fraction of the time, or ﬁnd a bet-ter representation altogether. In addition to reframing well-known problems, deep learning also offers a solution tothose that lack a clear intuition about how a system shouldbe designed. A perfect example of this is found in auto-matic mixing; we know a “good” mix when we hear one,but it is impossible to articulate the contributing factors ina general sense. Like the work illustrated in Figure 3, thiscan also provide insight into what features are informativeto a given task and create an opportunity for a deeper un-derstanding of music in general.\n6. REFERENCES\n[1] J. And ´en and S. Mallat. Multiscale scattering for audio classiﬁcation. In Proc.\nISMIR, 2011.\n[2] J. J. Aucouturier. Music similarity measures: What’s the use? In Proc. ISMIR,\n2002.\n[3] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies, and M. Sandler.\nA tutorial on onset detection in music signals. IEEE Trans. Audio, Speech and\nLanguage Processing, 13(5):1035–1047, 2005.\n[4] Y . Bengio. Learning deep architectures for AI. Foundations and Trends in Ma-\nchine Learning, 2:1–127, 2009.\n[5] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins,\nJ. Turian, D. Warde-Farley, and Y . Bengio. Theano: a CPU and GPU math\nexpression compiler. In Proc. SciPy, 2010.[6] B. Buxton. Sketching User Experiences: Getting the Design Right and the Right\nDesign. Morgan Kaufmann, 2007.\n[7] G. Cabral and F. Pachet. Recognizing chords with EDS: Part One. Computer\nMusic Modeling and Retrieval, pages 185 – 195, 2006.\n[8] M. Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes, and M. Slaney.\nContent-based music information retrieval: Current directions and future chal-lenges. Proc. IEEE, 96(4):668–696, 2008.\n[9] T. Cho, R. J. Weiss, and J. P. Bello. Exploring common variations in state of the\nart chord recognition systems. In Proc. SMC, 2010.\n[10] S. Dieleman, P. Brakel, and B. Schrauwen. Audio-based music classiﬁcation\nwith a pretrained convolutional network. In Proc. ISMIR, 2011.\n[11] S. Essid, G. Richard, and B. David. Musical instrument recognition by pairwise\nclassiﬁcation strategies. IEEE Trans. Audio, Speech and Language Processing,\n14(4):1401–1412, 2006.\n[12] J. M. Grey. Multidimentional perceptual scaling of musical timbre. Jnl. Acous-\ntical Soc. of America, 61:1270–1277, 1977.\n[13] P. Grosche and M. M ¨uller. Extracting predominant local pulse information\nfrom music recordings. IEEE Trans. Audio, Speech and Language Processing,\n19(6):1688–1701, 2011.\n[14] P. Hamel and D. Eck. Learning features from music audio with deep belief\nnetworks. In Proc. ISMIR, 2010.\n[15] P. Hamel, S. Wood, and D. Eck. Automatic identiﬁcation of instrument classes\nin polyphonic and poly-instrument audio. In Proc. ISMIR, 2009.\n[16] M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y . LeCun. Unsupervised learning\nof sparse features for scalable audio classiﬁcation. In Proc. ISMIR, 2011.\n[17] G. E. Hinton, S. Osindero, and Y . Teh. A fast learning algorithm for deep belief\nnets. Neural Computation, 18(7):1527–1554, 2006.\n[18] E. J. Humphrey, T. Cho, and J. P. Bello. Learning a robust tonnetz-space trans-\nform for automatic chord recognition. In Proc. ICASSP, 2012.\n[19] E. J. Humphrey, A. P. Glennon, and J. P. Bello. Non-linear semantic embedding\nfor organizing large instrument sample libraries. In Proc. ICMLA, 2010.\n[20] I. Karydis, M. Radovanovic, A. Nanopoulos, and M. Ivanovic. Looking through\nthe “glass ceiling”: A conceptual framework for the problems of spectral simi-larity. In Proc. ISMIR, 2010.\n[21] K. Kavukcuoglu, P. Sermanet, Y . Boureau, K. Gregor, M. Mathieu, and Y . Le-\nCun. Learning convolutional feature hierarchies for visual recognition. In Proc.\nNIPS, 2010.\n[22] A. Klapuri and M. Davy. Signal Processing Methods for Music Transcription.\nSpringer, 2006.\n[23] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis of the meter of acous-\ntic musical signals. IEEE Trans. Audio, Speech and Language Processing,\n14(1):342–355, 2006.\n[24] A. Lacoste and D. Eck. A supervised classiﬁcation algorithm for note onset\ndetection. EURASIP Jnl. on Adv. in Signal Processing, pages 1–14, 2007.\n[25] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied\nto document recognition. Proc. IEEE, 86(11):2278–2324, 1998.\n[26] H. Lee, R. Grosse, R. Ranganath, and A. Y . Ng. Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. InProc. ICML, 2009.\n[27] T. Li, A. Chan, and A. Chun. Automatic musical pattern feature extraction using\nconvolutional neural network. In Proc. IMECS, 2010.\n[28] M. M ¨uller. Information Retrieval for Music and Motion. Springer Verlag, 2007.\n[29] M. M ¨uller, D.P.W. Ellis, A. Klapuri, and G. Richard. Signal processing for mu-\nsic analysis. Jnl. Selected Topics in Sig. Proc., 5(6):1088–1110, 2011.\n[30] J. Nam, J. Ngiam, H. Lee, and M. Slaney. A classiﬁcation-based polyphonic\npiano transcription approach using learned feature representations. In Proc. IS-\nMIR, 2011.\n[31] E. D. Scheirer. Tempo and beat analysis of acoustic musical signals. Jnl. Acous-\ntical Soc. of America, 103(1):588–601, 1998.\n[32] E. M. Schmidt and Y . E. Kim. Modeling the acoustic structure of musical emo-\ntion with deep belief networks. In Proc. NIPS, 2011.\n[33] A. Zils and F. Pachet. Automatic extraction of music descriptors from acoustic\nsignals using EDS. In Proc. AES, 2004."
    },
    {
        "title": "Bridging Printed Music and Audio Through Alignment Using a Mid-level Score Representation.",
        "author": [
            "Özgür Izmirli",
            "Gyanendra Sharma"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414770",
        "url": "https://doi.org/10.5281/zenodo.1414770",
        "ee": "https://zenodo.org/records/1414770/files/IzmirliS12.pdf",
        "abstract": "We present a system that utilizes a mid-level score repre- sentation for aligning printed music to its audio rendition. The mid-level representation is designed to capture an approximation to the musical events present in the printed score. It consists of a template based note detection front- end that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff and the approach is extendable to other types of scores. The image processing consists of page segmentation into lines followed by multiple stages that optimally orient the lines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. Alignment is performed using dynamic time warping with a specially designed distance measure. The insuffi- cient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tol- erant distance measure. Evaluation is carried out at the beat level using annotated scores and audio. The results demonstrate that the approach provides an efficient and practical alternative to methods that rely on symbolic MIDI-like information through OMR methods for align- ment.",
        "zenodo_id": 1414770,
        "dblp_key": "conf/ismir/IzmirliS12",
        "keywords": [
            "mid-level representation",
            "note detection",
            "audio rendition",
            "musical events",
            "grand staff",
            "page segmentation",
            "line orientation",
            "reference grid",
            "dynamic time warping",
            "distance measure"
        ],
        "content": "BRIDGING PRINTED MU SIC AND AUDIO THROUGH \nALIGNMENT USING A MID-LEVEL SCORE \nREPRESENTATION \nÖzgür İzmirli,  Gyanendra Sharma \nCenter for Arts and Technology \nComputer Science Department \nConnecticut College \n{oizm, gsharma}@conncoll.edu  \n \nABSTRACT \nWe present a system that utilizes a mid-level score repre-\nsentation for aligning printed music to its audio rendition. \nThe mid-level representation is designed to capture an \napproximation to the musical events present in the printed score. It consists of a temp late based note detection front-\nend that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff \nand the approach is extendable to other types of scores. \nThe image processing consists of page segmentation into \nlines followed by multiple stages that optimally orient the \nlines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. \nAlignment is performed using dynamic time warping \nwith a specially designed distance measure. The insuffi-cient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tol-erant distance measure. Eval uation is carried out at the \nbeat level using annotated scores and audio. The results demonstrate that the approach  provides an efficient and \npractical alternative to methods that rely on symbolic MIDI-like information through OMR methods for align-ment. \n1. INTRODUCTION \nMusic can be represented in mainly three forms: audio, \nsymbolic (such as MIDI) and printed. Historically these \nforms of data have remained disparate in archives and \nhave only been associated through metadata. More re-\ncently the field of music information retrieval has been actively exploring ways to bridge the content across their \ndifferent forms of existence.  MIR systems dealing with \nlarge music collections depend on basic operations such \nas searching, matching and a lignment. These operations \nare required to not only work with audio or MIDI formats but they should be capable of handling multi-format data including printed and hand-written scores. Finding matches and similarities across representations is of inter-est because these will pave th e way to buildin g integrated \nsystems that have broad implications in research and edu-cation.   Traditional libraries contain vast collections of music \non paper as well as recorded audio but lack the fine-level \nconnection between the two fo rmats. Incorporation of \nmethods that connect the different representations can \nresult in applications being more capable and multi-\nmodal. Some applications include: score retrieval by au-dio example; structure and harmonic analysis by audio \ninput; transcription of performance parameters from au-\ndio superimposed onto existing scores; score following in \nthe literal sense – following the music automatically on \nthe printed score.  \nAudio is sonically rich but sound mixtures are hard to \nanalyze and separate automatically. Symbolic data on the other hand represents music very efficiently at the note level but contains very little timbral and expressive in-\nformation. The visual nature of the printed score allows \nmusicians to read, experience  and analyze music in dif-\nferent ways and is an indispensible part of musicians’ \nevery day experience. Each representation type has its \nown advantages and by conn ecting them in meaningful \nways we can achieve greater  musical understanding as \nwell as convenient access to many forms of representa-\ntion. The different kinds of information in these represen-tations can greatly leverage our overall understanding and \naid us in searching with multiple perspectives. Today, \nconversion between these form s presents many challeng-\nes and can be performed with varying levels of success. It \nis, however, easier to bridge collections in different forms \nthrough fine-grain alignment. \nIn this paper, we present an approach to aligning audio \nand printed representations of music using a mid-level score representation. We will use the term score  to denote \nthe sheet or printed version throughout the paper and note that it is different from the usage in score following work where it is commonly used to depict the MIDI-like sym-\nbolic sequence. The proposed mid-level representation \nenables us to capture sufficient pitch and score position information to guide the alignment process. Key signa-\ntures and accidentals are ignored in the recognition and \ntherefore a tolerant distance measure that compensates for \nthis shortcoming is proposed. In the remainder of the pa-\nper the next section outlines related and previous work. \nSection 3 presents the mid-level representation which is followed by a section in which a distance measure is de-\n \nPermission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for prof it or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval  fined. We finally present an evaluation of the method on \na small set of piano music and close with concluding re-\nmarks. \n2. RELATED WORK \nSince the introduction of optical music recognition (OMR), multiple works have been carried out in mapping and aligning different music representations, namely, the \nmusic score, audio recordings and MIDI. Multiple         \napproaches have been employed  to build state-of-the-art \naudio-to-score alignment algorithms. Some are based on \ngraphical and statistical models such as the ones in \n[3,8,16] whereas some use the Dynamic Time Warping (DTW) algorithm to align the sequences of features ex-\ntracted from both the audio and the score as in [7,12]. \nWork done in [13] carry out a multi-pass algorithm where they propose a method which estimates the onset times of \nindividual notes in a post processing step to obtain an ac-\ncurate audio-to-score alignment. Earlier works in audio-to-score alignment such as [14] employ the DTW algo-\nrithm and generate spectral approximations from the symbolic form in order to compute the local distance \nmeasures for the DTW.    When audio is in the mix, chroma based representa-\ntions are often used for alignment. In [9] the authors maintain that “chroma vect ors drawn from representa-\ntions using a logarithmic frequency scale are the most ef-ficient features, and lead to a good precision, even with a \nsimple alignment strategy.” Here, we not only utilize a \nchroma based representation obtained from audio analy-\nsis but also create one from the score. \nIn [8] Joder et al. propose a statistical model for mu-\nsic-to-symbolic score ali gnment where a hidden state \nmodel uses two features: chroma vectors, to model pitch content of the signal and spectral flux, to model note on-sets. The approach employed in this work claims to have \nachieved a very precise alignment with a low complexity \ncompared to other DTW systems.  More recent work by Cont [3] discusses the use of hierarchical hidden Markov \nmodels for online and real-time audio-to-score alignment.   \nAll these works approach the problem of alignment \nbased on fully-notated MIDI score. To the authors’ knowledge, alignment work solely based on the music \nsheet and its corresponding audio recordings without the use of intermediate MIDI format have not been formally \nused. Work on mapping, synchronization and identifica-\ntion of the music with audio recordings has been carried \nout by [5,6,11]. In [5] the authors discuss two different \napproaches in iden tifying the corresponding sections of \nan audio interpretation of a musical piece given the sec-\ntions of the score for the same piece. The first approach \nwhere it is assumed that th e performance sequence is \nknown uses a semi-automatic approach using synchroni-zation whereas, the second ap proach where the perfor-\nmance sequence is unknown us es matching techniques. \nHowever, OMR is used to obtain the symbolic score be-fore employing any of the identification techniques.  \nWork has also been done in aligning semi-improvised \nmusic with its lead sheet [4]. This is generally more diffi-cult as the lead sheet specifi es only essential elements \nsuch as the melody, harmony , and a basic musical form. \nThis work also stems from using the symbolic data ob-tained after the OMR techniques on the score. \nA lot of work has been focused on solving specific \nproblems of the OMR such as staff line detection [1] and recognizing musical symbols. Recently, in [17] the au-\nthors have employed template matching and grammati-cally formulated top-down m odels as a means of per-forming OMR on scanned sheet music. Since the purpose \nof this paper is precisely not to perform detailed OMR we \nrefer the interested reader to two overviews of the state-of-the-art in optical music recognition [2, 18].  \n3. MID-LEVEL REPRESENTATION \nIn this work we restrict our method to pieces using the \ngrand staff in which a system consists of the top staff no-tated with the treble clef and the lower staff with the bass clef. We have been using scanned scores from the Inter-national Music Score Library Project (IMSLP). These \nimages are particularly challe nging due to the fact that \nthey contain skew within the page, have different print \nstyles, their original resolutions vary and they are quite noisy. Our purpose is to perform some basic image pro-cessing operations on the dig itized score and extract the \nrelevant sections to arrive at an intermediate representa-\ntion that would be useful for alignment. As a first step, \nprior to any image processing a binarization step is per-\nformed using Otzu’s method [15] which optimizes the \nforeground/background classification of pixels through \nan exhaustive threshold search.    \n3.1 Overall Page Structure \nThe first step is to identify the overall page structure in \nterms of systems. A horizontal projection P\np is calculated \nby summing the pixel values across the page. This pro-jection is generally a quite good representation to identi-fy line positions in scanned scores that are reasonably \nstraight. We assume that the original rotation of the \nscanned page produces a projection in which the staff lines are identifiable through local peaks. We can option-ally perform an automated page rotation to correct for \nscanning errors using a procedure similar to the one de-\nscribed in the next section for individual systems. The \nprojection P\np is then smoothed with a truncated Gaussian \nfilter with width equal to a single staff. The position of \neach staff is determined by peak picking and simultane-\nously, the positions of the top and bottom lines of each system are determined by fi nding the local peaks of the \nunsmoothed projection in the vicinity of the peaks of the smoothed projection. Figure 1 shows part of the original score at the top and the projection resulting from that im-age below. The projection is aligned with the image of the two systems shown at the top. This process results in fairly reliable vertical position estimates of the systems \non a single page. This segmentation is performed for all \nsubsequent pages in the score for the piece in question.  \n \nFigure 1 . Top: first two systems from a scanned score. \nBottom: horizontal projection and Gaussian smoothing \nof the top figure for locating systems in a page. \n \n3.2 Aligning Systems and Automatic Calibration \nWe extract systems one by one according to their posi-\ntions on a page as described above. Each system In, runs \nfrom C2, two ledger lines below the bottom line in the bass clef, to approximately E6 on the third ledger line above the treble clef. This image is then corrected for \noptimal rotation by fitting parabolas onto local peaks in \nthe projection. The position of each line is determined by \npeak picking on the horizontal  projection of the extracted \nsystem. We observe that the shape of the intensity distri-bution around each peak is co rrelated to how well the \nsystem is aligned – the narrowest distribution is consid-ered the best rotation for I\nn which results in maximally \nhorizontal staff lines. We th erefore, fit a least-squares \nparabola on the points neighboring each peak that ex-ceeds a threshold. Since the parabolas are opening downward we find the rotation angle θ that minimizes \nthe sum of the coefficients of  the second degree terms of \nthe parabolas for all 10 peaks. The optimal rotation angle is given by \n)) i ), , I ( ( ( min arg\nin n∑ θ Ψ Θ = θ\nθ                  (1) \nwhere Ψ represents the rotation of In by θ and Θ is the \ncoefficient of the second degree term in the parabola \nequation for local peak i  that serves as the relative width \nestimate in the horizontal projection of the rotated image.  \nFor our purposes the rota tion correction for each sys-\ntem turns out to be quite important. We have observed that even systems on the same page can have different rotation and skew values. In order to correctly identify \nnotes, an adaptive reference grid delineating the note \nranges is required for each system. In contrast to other approaches, our method does not remove the staff lines \nbecause the templates which are taken from actual imag-es already include parts of those lines. 3.3 Compressing the Grand Staff \nThe process of finding the optimal rotation for each sys-tem also allows us to more accurately identify the posi-tions of the lines. Next, we se parate each system into two \nimages by cutting it in half with a horizontal line that lies between the lowest line in the treble clef (E4) and the highest line in the bass clef (A3). We then merge the up-\nper and lower halves of each system by multiplying (ORing – with pixel intensity values between 0 and 1) the content such that the positions of C4 in each part co-incide. Figure 2 shows the compressed image for the first system given in Figure 1. Note that, for example, the \nnote D4 that originally app ears on the lower staff now \nhas the correct position with respect to the upper staff. The reference grid which contains positions for the note \nboundaries is calculated from the distance between the top and bottom staff lines. Figure 3 shows the grid on top of a fragment of the rotated image. The regions between \nlines of the grid represent the C major diatonic set re-\ngardless of any accidentals that are in use. \n \nFigure 2 . Compressed image of first system in Figure 1. \n \nFigure 3 . The reference grid for note boundaries super-\nimposed on the optimally rotated image. The space be-\ntween each pair of lines co rresponds to a diatonic note. \n3.4 Note Identification \nThe process of note identification follows a template \nmatching approach. Three templates are constructed: one for a filled-in note head positioned between lines, one for a filled-in note head on a line and one for an empty note \nhead. The templates are slightly larger than the oval note \nhead and include a small portion of the surrounding \nlines. Their registration point is at the center of the sym-\nbol and ideally should fall either on a line or midway be-\ntween two lines. The only symbols of interest are the \nnote heads and other symbols su ch as stems, accidentals, \nrests, clefs, beams etc. are not considered. Notes are \nfound by convolving the optimally rotated system image Ψ(I\nn,θn) separately with each of the templates Tl. The \noriginal templates are scaled according to the line spac-ing of the system under consideration. The two images \nare represented with bipolar encoding (±1) for the convo-\nlution. Local peaks indicate matches between a template \nand a system. We then obtain the set of all recognized \nnotes by the union of notes r ecognized in all systems in \nthe piece. \n()[]U\nl , nl n nT * , I Q θ Ψ Γ =\n (2) \nHere * is the 2D convolution operator and Γ[ ] is the \nfunction for finding local peaks. Since the note recogni-tion is done without regard to key signature or any pre-ceding accidentals, only the notes correspond ing to white \nkeys are found. This process results in a set of recog-\nnized notes Q with 2-tuple elements q\nv=(n v, ov) each with \na note index n v that corresponds to the bins of the \nchromagram and an onset frame (column) number o v. An \nexample of the output is shown in Figure 4. \n \nFigure 4 . Recognized notes from the image in Figure 2. \nThe original has been lightened and ‘+’ indicates a note \nhead on a line, ‘o’ between lines and ‘x’ an unfilled note \nhead. \n3.5 Chroma Representation from the Score \nBefore we define a distance function to establish a rela-\ntionship between the audio and printed score representa-\ntions we would like to find the most compatible frame \nbased features that could be  practically calculated from \neach form of the music. On the audio side we calculate an open ‘audio chromagram,’ A, using constant Q spec-\ntral analysis, that is not folded into one octave. The bins represent logarithmically sp aced frequency ranges that \nare each a semitone wide and calculated with respect to a reference of A4=440Hz. We th en proceed to construct a \nsimilar feature using the note s recognized from the score \nto form the ‘score chromagram,’ S . Each recognized note \nis placed into the chromagram in the bin representing the note and at the corresponding frame. In addition to the \nfundamental frequency component, the note’s harmonics are also added with amplitude 1/h , where h  is the har-\nmonic number and h=1..H . All components incur a fixed \nexponential decay to account for the passage of time, i.e. \nto not have the same values for the duration of the note \nand give more weight to the onset. The note model for a \ngiven note q\nv is represented by a sequence of k-element \nchroma vectors   \nT j\nkj j j\nvj) r , r , r , r ( ) q ( R1 2 1 0 − = K  (3) \nv)vo j ( c j\n) h ,vq ( o j , ehr ≥ =− −\nϕ1 (4) \nwhere c is the decay rate and ov is the frame on which the \nnote starts. The function φ (qv,h) is the index of the bin in \nthe chromagram that corresponds to note qv and harmon-\nic h. The resultant score chromagram is given by the summation of the note events calculated for all recog-\nnized notes: \n ) q ( R E Sv\nH .. h, Q vqj j∑\n=∈ ∀=\n1o                              (5)\n  \nAfter the summation of note sp ectra, a spectral weighting \nis applied to the score chromagram to match its long-term \nspectral shape with that of the audio. The weighting E is \ncalculated from the audio chromagram by simply averag-ing it across time and dividing by the maximum element. \nThe operator ◦ denotes the elementwise multiplication of \nthe vector E with each colu mn of the summation that \nholds the unweighted score chromagram. Figure 5 shows the score chromagram for the notes of Figure 4 and the audio chromagram for the same fragment of music. \n \n \nFigure 5 . Top: audio chromagram. Bottom: score \nchromagram obtained from r ecognized notes as shown \nin Figure 4. \n4. LOCAL DISTANCE AND ALIGNMENT \nThe defined system would have worked if the piece be-\ning analyzed was in C majo r and a standard distance \nsuch as a Euclidean or a cosine distance was being used. \nHowever, due to the limitation of the front-end and its \nnotational system which is based on diatonic pitch spac-ing, these standard distance measures would become progressively meaningless as the keys pick up more ac-cidentals. We therefore defi ne a tolerant distance func-\ntion between two \nk-element chroma vectors S (score) at \nframe i and A (audio) at frame  j: \n    ,   T i\nki i i i) s , s , s , s ( S1 2 1 0 − = KT j\nkj j j j) a , a , a , a ( A1 2 1 0 − = K\n       ( )∑−\n=+ −=2\n11 1k\npj ij\npi\npj\npi\npj\npi\np\nj , iA . Sa s , a s , a s maxb\n               (6)              \n \nmax j , i j , i b b d−=1                                   (7) \nwhere bmax represents the maximum value of bi,,j. \nThe alignment of the score chromagram and the audio \nchromagram is performed using DTW. The following \nstep size condition constrains the slope of the warping path \n( )j , i j , i j , i j , i j , i d D , D , D min D+ =− − − − − −2 1 1 2 1 1           (8) \nNote that vertical and horizo ntal moves are not allowed. \nThis ensures that the two se quences move forward at ei-\nther the same frame rate or twice the other, and also that a single frame in one sequence does not map to multiple \nframes in the other. This preserves the monotonicity \ncondition of the DTW for our purpose.   \n5. EVALUATION \nWe have evaluated the proposed method in various ways. \nPrimarily the evaluation has concentrated on the accuracy \nof the alignment on the printed score. For this we needed \nthe audio as well as the printed score to be annotated. The scores were taken from IMSLP’s Petrucci Library which \nis a web site that has scanned scores for which the copy-\nright has expired. The audio annotations for the Chopin Mazurkas were taken from The Mazurka Project \n(http://www.mazurka.org.uk) in which Craig Sapp col-\nlected beat-level onsets for di fferent performances of the \nsame piece. \nWe calculate the alignment accuracy with respect to \ntwo frames of reference. The first is the score where the \nalignment error is reported as a percentage of the staff width. The times of all beats in the audio (given by the  \nground truth) are mapped to score positions using the \nwarping path produced by the DTW algorithm. The error is calculated by taking the average of the absolute differ-ences between these numbers and the beat locations in the score given by the ground truth. The second frame of \nreference is the audio where the alignment error is found \nin seconds. The two measures are similar in nature and are not meant to provide different viewpoints, rather, \nthey give a good sense of the average and maximum er-rors in the two modalities of experience: visually follow-ing the printed score while listening to the performance. \nThe following parameters were used for all scores and \nperformances in the evaluation. The audio analysis was \ndone with 50 percent overlapped windows of duration 50 \nmilliseconds. Each column in  the score chromagram rep-\nresents a group of pixels in the input image. The number \nof pixels in a group is calculated separately for each au-\ndio file in order to make the number of score frames \ncomparable to the audio frames. The decay rate, \nc, was \ndetermined empirically to be on the order of one beat as \nseen in Figure 5 but will vary from score to score depend-ing on the density of the typesetting. Four harmonics (H) were used for the note model. The range of the note \nrecognition was restricted to the range C2 to E6 and any \nnotes beyond this range were ignored. The scores were scanned at 300 pixels per inch. \nTable 1 shows the list of piece/score edition/performer \ncombinations tested. The second column lists the align-ment results with respect to the audio. The average abso-lute error and maximum error figures are given. The \nalignment error with respect to the score is given in the \nthird column. The error is in pixel real distances on the digitized image. It shows th e horizontal distance between \nthe ground truth and result of the alignment as a percent-age of the width of the score. It is reported as a percent-age to make it independent of image resolution, however, \nby the same token, it could be affected by the number of \nmeasures that the publisher ch ose to fit in a single line. \nThe same edition has been tested with different perform-\ners as well as different pieces from the same editor. In our \ntests a number of scores with heavy fonts and poor quali-\nty images did not produce ac ceptable alignments mainly \ndue to the errors in the front -end. We observed that these \nwere primarily grouped around certain publishers and that the template matching could be made more adaptive in \nfuture work to cater to even wider stylistic variations.  \n \n \nPiece/ Edition Av (Max) Err. Audio (seconds) Av (Max) Err. % score width  Performer \nMazurka 30-2 Mikuli 0.24 ( 1.78)  3.49 (28.16) Mohovich \nMazurka 30-2 Mikuli 0.34 ( 2.07)  4.07 (19.13) Fou \nMazurka 30-2 Mikuli 0.13 ( 0.84)  2.40 (13.53) Ashkenazy \nMazurka 30-2 Klindworth 0.13 ( 1.27)  1.53 (11.05) Mohovich \nMazurka 30-2 Klindworth 0.21 ( 2.51)  1.87 (14.31) Fou \nMazurka 30-2 Klindworth 0.11 ( 0.89)  1.65 (14.19) Ashkenazy \nMazurka 30-2 Scholtz 0.18 ( 1.50)  2.39 (18.41) Mohovich \nMazurka 30-2 \nScholtz 0.31 ( 2.17)  3.46 (19.69) Fou \nMazurka 30-2 Scholtz 0.12 ( 1.14)  1.93 (17.04) Ashkenazy \nMazurka 63-3 Mikuli 0.16 ( 1.97)  1.37 (10.85) Ashkenazy \nMazurka 63-3 Joseffy 0.29 ( 3.41)  2.17 (23.65) Ashkenazy \nMazurka 63-3 Kullak 0.29 ( 2.29)  1.99 (14.77) Ashkenazy \nMazurka 67-1 Joseffy 0.17 ( 1.65)  2.22 (17.89) Chiu \nMazurka 67-1 Klindworth 0.21 ( 1.70)  2.65 (18.51) Chiu \nMazurka 68-3 Joseffy 0.36 ( 1.84)  4.49 (30.09) Chiu \nTable 1.  Alignment errors for a number of Chopin Ma-\nzurkas by different performers and various editions of \nprinted scores. \nResults of the evaluation show that the method is able \nto align real-world printed scores to expressive audio per-\nformances. We have evaluated the method at the beat \nlevel to explore the possibility of more precise alignment. It can be seen from the table that the average time accura-cy is quite good. We have implemented a test application that displays the score position as the music is playing based on the alignment. The tracking can be comfortably \nfollowed by eye and the appli cation allows the viewer to \nsee the errors as the perfo rmance unfolds. The average \nerror figures on the score si de are also good. However,   \n \nthe maximum errors appear to be somewhat high. The \nreason for this seems to be the fact that when the last beat \nin a system is carried over to the next system (or a beat is \naligned early from the next system) the calculated error includes the distance of the margins in between the two adjacent systems. Therefore, we do not think that these \nfigures are as drastic as they look but appear as a delayed \nresponse while following.     \nApproximate matching offers many advantages to the \nproblem at hand. With the relatively simple mid-level \nfeature and the complexity of the recognition problem with the given less-than-ideal  historical scores, recogni-\ntion errors are frequent. However, the method allows for \ngraceful recovery due to two r easons. One is the tolerant \ndistance measure which inhere ntly absorbs pitch errors. \nThe other is the step conditi on of the DTW algorithm that \nprevents one sequence from stalling for extended periods.  \nThis allows for catch-up af ter a sequence of misdetec-\ntions, rests or page segmentation errors. While selecting the templates and their detection thresholds a balance was struck between false positives and false negatives. \n6. CONCLUSIONS \nWe have presented a mid-level score representation for aligning printed music to audio. The mid-level represen-\ntation allows us to bypass sophisticated OMR techniques \nused for recognition and semantic analysis. The method \nallows for alignment through use of approximate pattern \nmatching between the compatible features obtained from \naudio and score representations and therefore performs \nalignment within a framework in which symbolic recog-\nnition accuracy is not the primary concern. At this stage \nof the ongoing project, the model has been evaluated on piano music and a number of scanned scores at the beat level and the results are encouraging. \nFuture work will concentrate on adding sectioning and \nsupport for repeats, timbre learning from audio mixtures for more accurate note m odeling, adding duration and \ndynamics into the note model, and catering to clef chang-es in the sheet. An extension of the proposed method to scores that employ systems other than the grand staff is of interest and would enable the method’s application to \nsymphonic as well as ensemble music. \n7. REFERENCES \n[1] Cardoso, J. S., Capela, A., Rebelo, A., Guedes, C., and \nCosta, J. P., “Staff Detection with Stable Paths,” IEEE  \nTransactions on Pattern Anal ysis and Machine Intelli-\ngence, 31(6), 1134–1139, 2009. \n[2] Choudhury, G. S., T. DiLauro,  M. Droettboom, I. Fujinaga, \nB. Harrington, and K. MacMi llan, “Optical Music Recog-\nnition System within a Large-Scale Digitization Project,” \nProc. 1st International Society  for Music Information Re-\ntrieval Conference  (ISMIR), 2000. \n[3] Cont, A., “A Coupled Dura tion-Focused Architecture for \nReal-Time Music-to-score Alignment,” IEEE  Transactions on Pattern Analysis and Machine Intelligence , 32(6), 974–\n987, 2010. \n[4] Duan, Z., and Pardo, B., “Aligning Semi-Improvised Mu-\nsic with its Lead Sheet,” Proc. 12th International Society \nfor Music Information Retrieval Conference (ISMIR), Mi-\nami, 2011. \n[5] Fremerey C., Clausen, M., Ewert S., and Muller, M., \n“Sheet Music-Audio Identification,” Proc. 10th Interna-\ntional Society for Music Information Retrieval Conference  \n(ISMIR), Kobe, 2009. \n[6] Fremerey, C., Damm, D., Muller, M., Kurth, F., and \nClausen, M., “Handling Sca nned Sheet Music and Audio \nRecordings in Digital Music Libraries,” Proc.  Interna-\ntional Conference on Acoustics NAG/DAGA , 2009. \n[7] Hu, N., Dannenberg, R. B ., and Tzanetakis G., “Polyphon-\nic Audio Matching and Alignment for Music Retrieval,” Proc. IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics\n (WASPAA), New Palz, \nNew York, 2003. \n[8] Joder, C., Essid, S., and Richard, G., “An Improved Hier-\narchical Approach for Music-to-symbolic Score Align-ment,” Proc. 11th International Society for Music Infor-\nmation Retrieval Conference (ISMIR), Utrecht, 2010. \n[9] Joder, C., Essid, S., and Richard, G., “A Comparative \nStudy of Tonal Acoustic Features for a Symbolic Level Music-To-Score Alignment,” Proc. IEEE International \nConference on Acoustics, Speech and Signal Processing  \n(ICASSP), Dallas, TX, US, March 2010. \n[10] Joder, C., Essid, S., a nd Richard, G.,”A Conditional Ran-\ndom Field Framework for Robu st and Scalable Audio-To-\nScore Matching,” IEEE Transactions on Audio, Speech \nand Language Processing,  19(8), 2385 - 2397, November, \n2011. \n[11] Kurth, F., Muller, M., Fremerey, C., Chang, Y., and \nClausen, M., “Automated Synchronization of Scanned Sheet Music with Audio Recordings,” Proc. 8th Interna-\ntional Conference on Music Information Retrieval (ISMIR), \nVienna, 2007. \n[12] Muller, M., Mattes, H., and Kurth, F., “An Efficient \nMultiscale Approach to Audio Synchronization,” Proceed-\nings of the 7th International Conference on Music Infor-mation Retrieval (ISMIR), Victoria, 2006. \n[13] Niedermayer, B., and Widmer, G., “A Multi-pass Algo-\nrithm for Accurate Audio-to-score Alignment,” Proc. 11th \nInternational Society for Musi c Information Retrieval Con-\nference (ISMIR), Utrecht, 2010. \n[14] Orio, N., and Schwarz D., “Alignment of Monophonic and \nPolyphonic Music to a Score,” Proc. International Com-\nputer Music Conference (ICMC), Havana, Cuba, 2001. \n[15] Otsu, N., \"A Threshold Se lection Method from Gray-Level \nHistograms,\" IEEE Transactions on Systems, Man, and \nCybernetics , Vol. 9, No. 1, 62-66, 1979. \n[16] Raphael, C., “Aligning Music Audio with Symbolic Scores \nUsing a Hybrid Graphical Model,” Machine Learning , Vol. \n65 (2-3), 389–409, 2006. \n[17] Raphael, C., and Wang, J., “New Approaches to Optical \nMusic Recognition,” Proc. 12th International Society for \nMusic Information Retrieval Conference (ISMIR), Miami, \n2011. \n[18] Rebelo, A., Fujinaga, I., Paszkiewicz, F., Marcal, A. R. S., \nGuedes, C., and Cardoso, J. S., “Optical Music Recogni-\ntion: State-of-the-art and Open Issues,” International Jour-\nnal of Multimedia Information Retrieval  (IJMIR), 2012."
    },
    {
        "title": "Interpreting Rhythm in Optical Music Recognition.",
        "author": [
            "Rong Jin 0004",
            "Christopher Raphael"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415848",
        "url": "https://doi.org/10.5281/zenodo.1415848",
        "ee": "https://zenodo.org/records/1415848/files/JinR12.pdf",
        "abstract": "We present a method for understanding the rhythmic con- tent of a collection of identified symbols in optical mu- sic recognition, designed for polyphonic music. Our ob- ject of study is a measure of music symbols. Our model explains the symbols as a collection of voices, while the number of voices is variable throughout a measure. We introduce a dynamic programming framework that identi- fies the best-scoring interpretation subject to the constraint that each voice accounts for the musical time indicated by the known time signature. Our approach applies as well to the situation in which their are multiple possible hy- potheses for each symbol, and thus combines interpreta- tion with recognition in a top-down manner. We present experiments demonstrating a nearly 4-fold decrease in the number of false positive symbols with monophonic music, identify missing tuplets, and show preliminary results with polyphonic music.",
        "zenodo_id": 1415848,
        "dblp_key": "conf/ismir/JinR12",
        "keywords": [
            "rhythmic content",
            "optical music recognition",
            "polyphonic music",
            "measures music symbols",
            "dynamic programming framework",
            "variable number of voices",
            "time signature constraint",
            "multiple hypotheses",
            "top-down interpretation",
            "false positive symbols"
        ],
        "content": "INTERPRETING RHYTHM IN OPTICAL MUSIC RECOGNITION\nRong Jin\nSchool of Informatics and Computing\nIndiana University, Bloomington\nrongjin@imail.iu.eduChristopher Raphael\nSchool of Informatics and Computing\nIndiana University, Bloomington\ncraphael@indiana.edu\nABSTRACT\nWe present a method for understanding the rhythmic con-\ntent of a collection of identiﬁed symbols in optical mu-\nsic recognition, designed for polyphonic music. Our ob-\nject of study is a measure of music symbols. Our model\nexplains the symbols as a collection of voices, while the\nnumber of voices is variable throughout a measure. We\nintroduce a dynamic programming framework that identi-\nﬁes the best-scoring interpretation subject to the constraint\nthat each voice accounts for the musical time indicated by\nthe known time signature. Our approach applies as well\nto the situation in which their are multiple possible hy-\npotheses for each symbol, and thus combines interpreta-\ntion with recognition in a top-down manner. We present\nexperiments demonstrating a nearly 4-fold decrease in the\nnumber of false positive symbols with monophonic music,\nidentify missing tuplets, and show preliminary results with\npolyphonic music.\n1. INTRODUCTION\nThroughout the history of the ISMIR community symbol-\nically-represented music has ﬁgured prominently in a wide\nvariety of applications, analysis techniques, as well as search\nand retrieval schemes. In spite of this demonstrated need,\nsymbolic music data are still in short supply, greatly limit-\ning the scale and variety of scientiﬁc music research. For\nmusic in machine-generated common Western notation, op-\ntical music recognition (OMR) provides, in principle, a di-\nrect path to create rich and extensive symbolic databases,\nthus OMR is among the most important problems for the\nclassically-oriented music scientist. Signiﬁcant advances\nin core OMR technology would lead to large scale sym-\nbolic music libraries, digital music stands, content-based\nsearch, as well as many speciﬁc applications well-known\nin this community.\nOMR is a deeply challenging problem, well-known to\nISMIR stalwarts [1–3], though less well-represented over\nrecent years in published research. Blostein and Baird [4]\npresent a 1992 OMR overview that is not so different from\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.a more current description [5]. One does not work long\nin this domain without encountering longstanding themes\nand conﬂicts of recognition science.\nOur strong bias is for top-down recognition: approaches\nthat begin by clearly articulating the world of possible hy-\npotheses or answers, then scoring these hypotheses accord-\ning to their a priori plausibility and their agreement with\nthe data. The HMM approach to speech recognition is one\nof the most famous and successful examples of this kind,\ncombining top-down modeling with computationally pow-\nerful dynamic programming (DP) techniques for search\nand training. However, the real-world recognition prob-\nlems admitting feasible top-down approaches appear to con-\nstitute a small minority. All OMR approaches we know,\nexcept [3,6], instead proceed bottom-up — beginning with\nthe image data, gradually trying to piece together progres-\nsively higher-level constructions, ultimately concluding with\nthe overall interpretation of the data. The preference for\nbottom-up strategies by nearly all OMR researchers (in-\ncluding ourselves) stems from their computational feasibil-\nity. The hallmark of a bottom-up approach is a series of in-\ntermediate andirreversible decisions as one climbs the lad-\nder connecting the image data and its interpretation. The\nAchilles’ heel of the bottom-up paradigm is the inevitable\nincorrect intermediate decision constituting a blind alley\nthat cannot be retraced. In OMR, the most obvious exam-\nple would be an incorrect segmentation of the data leading\nto unrecognizable symbols, though many others exist.\nThe greatest virtue of the top-down recognition approach\nis its simultaneous focus on recognition and interpretation\n— primitive hypotheses such as “note head here” are only\nconsidered when they ﬁt into a meaningful interpretation\nof the scene (say measure) at hand. While it seems too op-\ntimistic to hope to formulate OMR in an entirely top-down\nmanner, there are many sub-problems where one can em-\nploy this philosophy. We do this whenever possible. While\nwe begin bottom-up, seeking various self-contained ob-\njects without regard for their overall organization, each in-\ndividual search procedure is itself top-down. For example,\nwe ﬁnd isolated chords (including single notes) by explor-\ning candidate stem locations through grammar-induced DP\nstrategies that consider every meaningful chord presenta-\ntion and result in a globally optimal interpretation. Simi-\nlarly, we recognize a beamed group by building a grammar\nof the possible presentations and computing the globally\noptimal meaningful structure. Further details can be found\nin [7].Figure 1. Numbering of symbols for polyphonic rhythm\ndecoding.\nFigure 2. Example of a voice split.\nThe result of this process is a collection of mutually\ninconsistent and overlapping hypotheses that share “body\nparts” in impossible ways — this is a typical pitfall of a\nbottom-up approach where it is difﬁcult to formulate the\nconcept of a hypothesis’ unique “claim” to a particular im-\nage pixel. As described in [7], we resolve these conﬂicts by\na phase seeking non-overlapping variants of the identiﬁed\nobjects, completely discarding some of them, resulting in a\ncollection of separate symbols that best explain the image\ndata. This is where our present discussion begins.\nAn OMR system may attempt many different levels of\nmusic understanding. The most superﬁcial approach would\nonly record the primitive symbols (note head, stem, ledger\nline, etc.) found on the page, “punting” on any deeper in-\nterpretation of their meaning. Many levels of increasing\ndepth could conceivably be added to this baseline. While\nwe remain uncertain about the right depth of interpretation\nfor our OMR system, it is hard to imagine a useful ap-\nproach that does not understand rhythm and pitch. Without\nsuch interpretation, we cannot even play back the music,\nsigniﬁcantly limiting the value of the resulting symbolic\ndata. Of these problems rhythm is, by far, the more chal-\nlenging one.\nIn the simplest case — single voice music with no un-\nmeasured notes — rhythm interpretation is rather straight-\nforward: symbols can be clearly ordered from left to right,\nwith the onset time of each symbol within a measure given\nas the sum of all preceding symbol durations. This situ-\nation quickly breaks down when the music uses multiple\nvoices on a single staff as in Figure 1. Identifying the on-\nset times here requires that we partition the symbols into\nthree simultaneous voices, thus allowing the application\nof our monophonic strategy to each voice. Unfortunately,\nthe number of voices is not known a priori and frequently\nchanges throughout the duration of the measure as in Fig-ure 2. In this work we propose a method of rhythmic in-\nterpretation that understands the music in terms of voices,\nallowing voices to be added or dropped anywhere in the\nbar.\nOf course, this rhythmic understanding is an essential\npart of the symbolic data we seek to create, thus valuable\nin its own right. However, the process of understanding\nrhythm can be combined with the recognition process to\nimprove the accuracy of our recognized results. The sim-\nplest example of this idea would leverage the “time signa-\nture constraint” — the note values in a voice must add up to\nthe time signature when viewed as a rational number. For\nexample, [8] has used this idea as a post-process to identify\npotential recognition errors. In this context we consider\nmultiple hypotheses for each recognized symbol, choos-\ning the best scoring overall measure interpretation obey-\ning this constraint. More generally, we present a model\nfor the possible polyphonic presentations of a measure,\nseeking the best scoring interpretation generated by the\nmodel, given our recognized symbols. The most signif-\nicant contribution of our present work is a top-down ap-\nproach for rhythm interpretation that integrates recognition\nwith higher-level understanding.\n2. RHYTHM DECODING\n2.1 Monophonic Rhythm Decoding\nThe basic processing unit of our system is the measure,\nwhose identiﬁcation is discussed in [7]. In order to rec-\nognize the contents of a measure, we must both correctly\nsegment the measure into meaningful pieces while inter-\npreting the meaning of each piece. We ﬁrst treat the case of\nmonophonic music, here meaning that the notes and rests\nform a single stream of events. The most typical example\nwould be music played by an instrument that produces a\nsingle note at a time, though our approach also applies to\nsequences of chords, as long as the notes of each chord\nshare a stem.\nSuppose we have partitioned the monophonic measure\ninto a sequence of Ksymbols that can be unambiguously\nordered from left to right. These objects could be rests,\nisolated notes, beamed groups, as well as objects without\nassociated musical time such as clefs. If extraneous sym-\nbols, not corresponding to actual document symbols, have\nbeen (mis)recognized, it won’t matter how these symbols\nﬁgure in this ordering.\nWe let Skbe a collection of possible interpretations for\nthekth object. For instance, for a rigid isolated symbol\nsuch as a rest, we consider all possible position and la-\nbel hypotheses, retaining the best scoring position for each\nlabel (quarter rest, eighth rest, etc). In the case of an iso-\nlated note, our recognition result may involve a closed note\nhead, though an open note head may match nearly as well.\nOr perhaps we were uncertain about the number of aug-\nmentation dots or ﬂags attached to the note. We revisit the\nDP analysis of the note, modifying the trace-back phase to\ncreate the “N-best” interpretations of the object [9]. Thus\nour isolated note analysis produces a list of possible inter-pretations along with scores measuring the quality of ﬁt to\nthe image data.\nSimilarly we construct an N-best list for the interpre-\ntations of a beamed group. These hypotheses may dif-\nfer in the number of beams that connect any pair of ad-\njacent notes, existence of partial beams, or number of aug-\nmentation dots attached to a note. In summary, the input\nto our rhythm recognizer is an ordered list of Kobjects,\neach with collection of possible hypotheses, Sk. For each\nsk2Skwe let D(sk)denote the musical time consumed\nby the hypothesis, with recognition score H(sk). Our con-\nvention for musical time gives a quarter note duration1\n4,\nand eighth note1\n8, with similar rational numbers for other\nnotes, rests, or beamed groups. In each collection, Skwe\ninclude the “null” interpretation with duration and score\n0, corresponding to the case of a false positive recognition\nerror.\nIn many cases we ﬁnd that the best scoring hypothe-\nses collectively make rhythmic sense. That is, we ﬁnd thatP\nkD(^sk) =Twhere ^sk= arg max sk2SkH(sk)and\nTis the measure’s time signature represented as a ratio-\nnal number (e.g. T=3\n4for 3/4 time). In such a case there\nwould be no reason to consider any other possible interpre-\ntation of the symbols. However, it is common to encounter\nscenarios where the best scoring hypotheses do not“add\nup,” while the correct interpretations of some symbols are\nfound “further down” in the hypothesis list. In such a case\nit makes sense to look for the sequence s\u0003\n1; : : : ; s\u0003\nKwith\ns\u0003\nk2Skgiven by\ns\u0003\n1; : : : ; s\u0003\nK= arg maxP\nkD(sk)=TX\nkH(sk)\nwhere the maximum is taken over all sequences s1; : : : ; s K\nwithsk2Skfork= 1; : : : ; K .\nThe identiﬁcation of this optimal conﬁguration is a sim-\nple exercise in dynamic programming. To this end we let\nPkdenote the possible measure positions for the kth ob-\nject:\nPk=fkX\nk0=1D(sk0) :sk02Sk0g\nfork= 1: : : ; K , with P0=f0g. These are the “states”\nof the DP calculation. Then we initialize M0(0) = 0 and\nrecursively deﬁne\nMk(pk) = max\npk\u000012Pk\u00001;sk2Sk\npk\u00001+D(sk)=p kMk\u00001(pk\u00001)+H(sk)(1)\nfork= 1; : : : K andpk2Pk. The optimal path, s\u0003\n1; : : : ; s\u0003\nK,\nhas score MK(T)— it is a simple exercise to recover the\npath that generates the optimal score MK(T).\nBy enforcing the time signature constraint on our in-\nterpretation we guarantee that the result makes rhythmic\nsense and ﬁx recognition errors in the process, analogous\nto the decoding of an error-correcting code.\n2.2 Recognizing System Rhythm\nA system groups together a collection of staves that are\nplayed simultaneously. Usually systems align symbols oc-curring at the same musical time to the same horizontal\nposition. For instance, corresponding bar lines of a sys-\ntem generally occur at a common horizontal position — in\nfact, our system recognizer identiﬁes systems by partition-\ning the staves into groups having common bar line posi-\ntions. As always with music notation, there are exceptions\nto this general rule, such as when whole rests are centered\nrather than “left aligned,” or when symbols must be off-\nset from their idealized positions to avoid overlap, as with\nunison whole notes.\nThis alignment convention can be used to extend the\nidea of the preceding section by adding a term to the score\nfunction penalizing misalignment of simultaneous events.\nSuppose we begin with a system of Lstaves and write\nsl\n1; : : : ; sl\nKwithsl\nk2Sl\nkfor an interpretation of the lth\nstaff. As a minor abuse of notation, we write P(sl\nk) =Pk\nk0=1D(sl\nk0)for the measure position of sl\nk, though clea-\nrlyP(sl\nk)depends on the entire history leading to sl\nk. For\nevery pair of simultaneous rhythmic events in a system\nmeasure — that is, sl\nk; sl0\nk0withP(sl\nk) =P(sl0\nk0), we pe-\nnalize their misalignment by Q(jX (k; l)\u0000X(k0; l0)j), with\nsome non-decreasing function, Q, where X(k; l)gives the\nhorizontal location of the kth event in the lth staff. For\na rest, we would take this location to be the horizontal\ncomponent of its center, while for an isolated note would\nwould take the horizontal component of the note head cen-\nter, since this is normally what the layout tries to align. For\nbeamed groups we simply use the horizontal component of\nthe ﬁrst note head center.\nWe now optimize the criterion:\nJ=LX\nl=1X\nkH(sl\nk)\u0000LX\nl;l0=1\nl6=l0\nP(sl\nk)=P (sl0\nk0)Q(jX (k; l)\u0000X (k0; l0)j)\n(2)\nsubject to the usual time signature constraint on each mea-\nsure. Due to the very large state space that would ensue,\nit may not be feasible to perform simultaneous optimiza-\ntion over allfsl\nkgvariables by DP. A computationally more\ntractable approach would be the familiar Gauss-Seidel or\n“coordinate-wise” optimization. That is, we ﬁrst recognize\neach staff measure independently according to the tech-\nnique of the previous section. Then we iteratively revisit\neach staff in turn, holding the interpretation of the other\nstaff measures ﬁxed while optimizing over the current staff\nmeasure. This calculation is possible since, when consider-\ning staff l0, the measure positions, P(sl\nk),l6=l0, are known\nsince the sl\n1; : : : ; sl\nKare ﬁxed, while for l=l0,P(sl\nk)is\nthe DP state.\n2.3 Missing Tuplets and Symbol Overloading\nRhythmic notation allows for various abbreviations that\nmay not literally make sense, but are clear in context. Of-\nten the correct interpretation is reinforced by the horizon-\ntal alignment of coincident symbols, as in the previous\nsection. For instance, it is common to omit the ’3’ onFigure 3. Example of implicit triplets. Our state model\nrequires a triplet to begin on a beat and continue for entire\nbeat before returning to duple rhythm or beginning another\ntriplet.\na beamed group of three notes, when the triplet interpre-\ntation is obvious, though this convention also allows for\nmixing rests and notes implicitly grouped into 3’s (or some\nother tuplet number) as in Figure 3. Another common ab-\nbreviated notation uses the half rest or whole rest to denote\nan empty measure even when the rest doesn’t account for\nthe correct bar length. While it may be literally correct to,\nfor instance, write a dotted half rest for a blank measure of\n3/4 time, there doesn’t seem to be any possibility for mis-\ninterpreting the plain half rest, so the shorthand persists.\nExamples such as the “overloaded” half rest are easy to\ntreat with the preceding methodology. When the half rest\nappears as a possible interpretation of a symbol in 3/4 time,\nwe simply add an identically-scored interpretation corre-\nsponding to the full length of the measure. The case of\nthe missing triplet on a group of three beamed notes can\nbe handled similarly, allowing both “straight” and triplet\ninterpretations of the group (while in duple meter).\nThe same ideas can apply in the more complex missing\ntriplets of Figure 3, where the implicit grouping involves\nseveral musical symbols. To do this we must multiply our\nstate space by 2 allowing each state to occur in a “straight”\nand “triplet” version. When we are in a triplet state, all\nnote values count for 2/3 their nominal length. We can\nleave the triplet state, reverting to the literal interpretation\nof rhythm, only when the measure position has no factor of\n3 in the denominator (i.e. when the triplet is completed).\nWe may also limit the places where triplets can begin (i.e.\nwhere we can transition from a non-triplet state to a triplet\nstate) to quarter note or eighth note pulses.\n2.4 Polyphonic Rhythm Decoding\nAs discussed in Section 1, the rhythmic intent of poly-\nphonic notation is often ambiguous, deriving its meaning\nfrom implicit use of voices which may appear or disappear\nat any place within a measure. In this section we present an\nalgorithm for the rhythmic decoding of a measure of poly-\nphonic symbols. For clarity’s sake we focus on the sim-\nplest statement of the problem, assuming correctly identi-\nﬁed symbols, a single staff, and no missing tuplets. How-\never, this technique can be extended using any of the ideas\nof the previous three subsections. For instance, the ideas\nof Section 2.1 can be included in an obvious way to cover\nthe case where we have multiple rhythmic hypotheses for\neach symbol, as in Section 3, with analogous extensions\nfor missing tuplets and staff measures.We ﬁrst consider the situation in which the number of\nvoices, V, is known, while the voices persist throughout\nthe entire measure. In such a case, the sum of rhythmic\nvalues over all symbols in the measure would be V T. Here\nthe interpretation problem simply separates these symbols\ninto voices, as is necessary for their rhythmic understand-\ning. We begin by numbering the Ksymbols of the measure\nfrom left to right, breaking ties arbitrarily, as in Figure 1:\nwe require only that the resulting sequence of the symbols’\nmeasure positions is non-decreasing. We represent a possi-\nble interpretation as a sequence of states, one state for each\nof the Ksymbols, where a state consists of three quanti-\nties for each active voice: the index of the voice’s most\nrecent symbol and two rational numbers giving the onset\nand offset times of the most recent symbol. For instance,\nthe correct state sequence associated with Figure 1 would\nbe:\nvoice 1 voice 2 voice 3\n1(1;0\n1;3\n8) — —\n2(1;0\n1;3\n8) (2;0\n1;3\n8) —\n3(1;0\n1;3\n8) (2;0\n1;3\n8) (3;0\n1;6\n8)\n4(4;3\n8;6\n8) (2;0\n1;3\n8) (3;0\n1;6\n8)\n5(4;3\n8;6\n8) (5;3\n8;6\n8) (3 ;0\n1;6\n8)\n6(6;6\n8;9\n8) (5;3\n8;6\n8) (3;0\n1;6\n8)\n7(6;6\n8;9\n8) (7;6\n8;9\n8) (3 ;0\n1;6\n8)\n8(6;6\n8;9\n8) (7;6\n8;9\n8) (8;6\n8;9\n8)\nThis sequence is “legal” since all voices account for the\nnumber of beats expressed by the time signature (9/8), as\nseen by the 3rd member of each voice in the last row of the\ntable.\nOf course, the true state sequence is not known, in prac-\ntice. We proceed by considering allpossible state sequences,\nscoring them according to the their plausibility in search\nof the best scoring candidate. In doing so we generate a\nsearch tree where the kth level of the tree treats the kth\nsymbol in our list. At the kth level we expand each branch\nby adding the kth symbol to all possible voices, while scor-\ning this extension according to several criteria. Perhaps\nthe most important criterion is the degree to which musi-\ncally coincident symbols align horizontally. When a new\nsymbol enters a voice, we must ﬁrst consult the state to\nsee if it contains symbols sharing the new symbol’s onset\ntime. This is why the symbol’s starting position is included\nas part of the state. For each such coincident symbol in\nthe state, we compute the difference in horizontal position\nwith that of the entering symbol. This is why the state\nalso retains the index of the symbol. The state information\ncan also be used to penalize the addition of a new symbol\nwhose stem direction does not agree with that of the most\nrecent symbol, etc.\nThe search proceeds over Kiterations — one for each\nincoming symbol, generating a search tree in the process.\nEach iteration begins by expanding each surviving branch\nby adding the current symbol to one of the voices, or cre-\nating a new voice if available voices exist. These new hy-\npotheses are then scored according to the criteria discussed\nabove. At this point it is possible that we have generated\nmultiple paths to the same state, and, if so, we only retainthe best scoring state. That is, we perform DP cutoffs. In\ndoing so, the particular voice numbering is not considered\nrelevant, so two states that differ only by the labeling of\nvoice numbers are considered identical. After performing\nDP cutoffs, we may still need to prune the tree further to\nrender the search feasible, retaining only the best scoring\nBhypotheses after each iteration.\nOf course, it is not reasonable to assume a priori that\nweknow the number of voices. For that matter, the num-\nber of voices may change throughout the duration of the\nmeasure. The most common instance of this phenomena\noccurs when a multi-voice measure begins or ends with a\nrest, in which case it is common to use a single rest for\nall voices. More generally, it is common to allow voices to\ncome in, or go out, of existence when the resulting notation\nuses less ink and still suggests the right idea to the reader.\nFigure 2 shows an example where a voice is added midway\nthrough the measure (we regard stems with multiple note\nheads as a single voice).\nWe address this problem by adding some ﬂexibility to\nour state production rules. Regardless of the number of\nvoices, we begin each measure with a single voice. At the\nbeginning of each iteration, any voice is allowed to split\ninto two identical voices, as long as some maximum num-\nber of voices has not yet been reached. The incoming sym-\nbol is then allowed to extend any currently active voice.\nAdditionally, any two voices sharing the same ending time\ncan merge into a single voice. Since we want to discour-\nage gratuitous use of these kinds of productions, we add\na penalty term when they are invoked. The correct state\nsequence associated with Figure 2 is as follows:\nvoice 1 voice 2\n1(1;0\n1;1\n2) —\n2(2;1\n2;5\n8) —\n3(2;1\n2;5\n8) (3;5\n8;7\n8)\n4(4;5\n8;4\n4) (3;5\n8;7\n8)\n5(4;5\n8;4\n4) (5;7\n8;4\n4)\n3. EXPERIMENTS\nWe tested the algorithm of Section 2.1 on the 2nd move-\nment of the Mozart Quintet for Clarinet and Strings, K. 581.\nThe original images of the four pages of this movement can\nbe seen at\nhttp://www.music.informatics.indiana.edu/papers/ismir12.\nAs with all experiments presented here, we begin by ﬁnd-\ning our best representation of the image data in terms of\nnon-overlapping isolated symbols, isolated chords, and beamed\ngroups. This phase implicitly segments the image into dis-\ntinct objects. Using the N-best techniques discussed above,\nwe then identify a list of possible interpretations of each\nsymbol or symbol group, thus forming the input to our\nrhythm decoder. The best scoring hypothesis for each sym-\nbol is superimposed in blue in the referenced images. As\ndiscussed above, the collection of best scoring individual\nhypotheses may not make rhythmic sense, thus we seek the\nbest scoring meaningful interpretation through our rhythm\ndecoder.Best Score Rhythm Decoding\nsymbol name False+ False- False+ False-\nsolid note head 6/898 14/908 5/891 18/908\nopen note head 1/34 4/37 1/32 6/37\nnote stem 36/921 10/927 32/913 14/927\n1 beam 4/429 9/434 3/427 10/434\n2 beam 1/77 4/80 0/76 4/80\n3 beam 1/90 2/91 1/91 1/91\naug. dot 113/153 1/39 3/38 4/39\nsingle ﬂag down 0/7 0/7 7/14 0/7\nsingle ﬂag up 0/9 3/12 0/12 0/12\ndouble ﬂag up 0/0 1/1 0/0 1/1\nwhole rest 27/27 13/13 1/14 0/13\nhalf rest 30/30 0/0 2/2 0/0\nquarter rest 12/36 1/25 1/25 1/25\neighth rest 12/30 1/19 2/20 1/19\n16th rest 0/1 3/4 0/2 2/4\n32th rest 0/6 0/6 1/7 0/6\ntotal 243/2748 66/2603 59/2544 62/2603\ndecimal .088 .025 .023 .024\nTable 1. False positives and false negatives for each prim-\nitive symbol with and without rhythm decoding. The table\nshows a nearly 4-fold decrease in false positives with es-\nsentially no change in false negatives.\nEach of these images was hand-marked with ground\ntruth by identifying bounding boxes of the primitive sym-\nbols of Table 1, as well as some rhythmically neutral sym-\nbols (clefs, accidentals, etc.) that don’t appear in the table.\nAs can be seen from the images and the table, the origi-\nnal recognition contained many small false positive sym-\nbols such as augmentation dots and whole/half rests. From\na statistical point of view, almost any data model will be\nprone to such “small symbol” errors, due to the higher vari-\nability of small-sample estimates. However, many of these\nunwanted symbols have only marginal data scores and do\nnot appear in the best scoring measure hypothesis subject\nto the time signature constraint. In fact, the Table 1 shows\na nearly 4-fold decrease in false positives with virtually\nno change in false negatives. False negatives, for the most\npart, cannot be corrected by our rhythm decoder, since they\nstem mostly from errors in which the correct hypothesis\ndoes not appear anywhere in our input to the algorithm.\nThe last page of the Mozart Quintet 2nd movement, vis-\nible at the website reference above, contains a number of\nunmarked triplets, as well as several marked ones, as in\nFigure 3. We tested the algorithm of Section 2.3 which\nincludes unmarked triplets among the hypotheses that are\nconsidered. Since a number of the triplets on our page\ninvolve two symbols, a rest and two beamed notes, we\nmust modify our state space in the manner described in\nSection 2.3, giving two versions of each rhythmic posi-\ntion: “triplet” and “straight.” We recognized the page us-\ning the rhythm decoder of Section 2.1, both with and with-\nout accounting for unmarked triplets. When allowing for\ntriplets we correctly recognized all of the triplets on the\npage, while the larger associated state space caused no ad-\nditional errors on the measures that did not contain triplets.\nThis is, of course, a small “proof of concept” experiment,\nrather than a large scale validation.A ﬁnal experiment treats the ﬁrst page of the Rach-\nmaninov Etudes Tableaux, op. 33 for piano, also displayed\nat the aforementioned web page. Piano music is particu-\nlarly difﬁcult for OMR, due to the higher symbol density,\nimplicit uses of voices, as well as other idiosyncrasies of\nkeyboard notation. However, the frequent use of implicit\nvoices poses an appropriate challenge for our polyphonic\nrhythm decoder of Section 2.4 — most measures in the\nright hand of this page contain two voices.\nOur goal now is simultaneously to choose from the avail-\nable hypotheses for each object, and to explain the sym-\nbols’ rhythm in terms of several possible voices. In this\nway we integrate recognition and interpretation, as is con-\nsistent with our philosophy, rather than treating them as\ndistinct phases of OMR. While the page uses voices in a\nconsistent manner (two for the right hand and one in the\nleft), we do not assume this knowledge. Rather we assume\na maximum of two voices that are allowed to come and\ngo in each measure, as described in Section 2.4. Since we\ndo not yet recognize time signatures, we assume the time\nsignature is known for each measure.\nEvaluating OMR in terms of symbol primitives, as in\nTable 1, is relatively straightforward and common in the\nOMR literature. We can imagine various useful notation\napplications based only on such primitive information, jus-\ntifying a limited place of this kind of evaluation. However,\nwe expect that most uses of OMR will require a higher\nlevel of music understanding than that expressed by sym-\nbol primitives. One possible approach to OMR evalua-\ntion represents each measure as a list of notes (or notes\nand rests), with each note having several attributes such\nas position within measure, length, pitch, coordinates of\nnote head, etc. When both ground truth and recognized re-\nsults are represented in this manner, a false negative can\nbe identiﬁed as any note in the ground truth that cannot\nbe “matched” with a note in the recognized results. Here\na match requires agreement of allattributes of the ground\ntruth note with one of the recognized notes. False posi-\ntives are computed by reversing the roles of ground truth\nand recognized results. Since our current emphasis is on\nrhythm, we evaluate our approach in this manner describ-\ning each note in terms of its note head coordinates and\nrhythmic onset position within the measure. While not ex-\nplored here, we believe this general evaluation paradigm\n(with suitable modiﬁcations) is serviceable in a wide range\nof OMR scenarios.\nUsing this procedure we achieved false negative and\nfalse positive rates of 30/402 and 8/380 on the Rachmani-\nnov page. While evaluations in terms of musical quantities\nsuch as pitch and rhythm may better measure the useful-\nness of the OCR results, they don’t clearly convey what\nactually goes wrong in recognition — in contrast, primi-\ntive evaluation is quite speciﬁc in this regard. On this one-\npage test, all errors were due to one of three things. One\nmeasure simply had misrecognized rhythm, however, the\nrhythmic result was quite syncopated, suggesting we may\nbe able to further improve by penalizing unusual rhythm.\nMost of the false negatives were due to the second kindof error — missing note heads on chords that were other-\nwise correctly recognized. Our approach cannot possibly\nrecover from such errors. The last type of error results\nfrom the unusual ﬁgure in the left hand of measures 5-\n9, in which an eighth and sixteenth are beamed together\nwith a sixteenth rest written in the interior of the beamed\ngroup. This situation violates our assumption that notes in\nbeamed group are executed in sequence without the pos-\nsibility of intervening notes/rests from other symbols. We\nbelieve this type of error could be corrected with simple\nmodiﬁcations of our approach. As before, numerous false\npositives from recognition are corrected by this procedure.\n4. REFERENCES\n[1] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fu-\njinaga, B. Harrington, and K.MacMillan, (2000), “Op-\ntical Music Recognition System within a Large-Scale\nDigitization Project,” in Proceedings, International\nSymposium on Music Information Retrieval, 2000.\n[2] D. Bainbridge and T. Bell: “The Challenge of Opti-\ncal Music Recognition,” Computers and the Humani-\nties35: pp. 95-121, 2001.\n[3] L. Pugin, J. A. Burgoyne, I. Fujinaga: “MAP Adap-\ntation to Improve Optical Music Recognition of Early\nMusic Documents Using Hidden Markov Models”: in\nProceedings of the 8th International Conference on\nMusic Information Retrieval (ISMIR 2007), pp. 513–\n16, Vienna, Austria, 2007.\n[4] D. Blostein and H. S. Baird: “A Critical Survey of Mu-\nsic Image Analysis,” In Structured Document Image\nAnalysis, ed. H. S. Baird, H. Bunke and K. Yamamoto,\npp. 405-34. Berlin: Springer-Verlag, 1992.\n[5] A. Rebelo and G. Capela and J. S. Cardoso: “Optical\nRecognition of Music Symbols,” in International Jour-\nnal on Document Analysis and Recognition 13:19-31,\n2009.\n[6] G. Kopec, P. Chou, D. Maltz: “Markov Source Model\nfor Printed Music Decoding,” Journal of Electronic\nImaging, 5(1), 7-14, 1996.\n[7] C. Raphael and J. Wang: “New Approaches to Optical\nMusic Recognition” in Proceedings of the 12th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR 2011), Miami, USA, pp. 305-310, 2011.\n[8] F. Rossant and I. Bloch: “Robust and Adaptive OMR\nSystem Including Fuzzy Modeling, Fusion of Musical\nRules, and Possible Error Detection,” EURASIP Jour-\nnal on Applied Signal Processing, vol: 2007.\n[9] R. Schwartz, Y .-L. Chow: “The N-Best Algorithm: An\nEfﬁcient and Exact Procedure for Finding the Most\nLikely Sentence Hypotheses,” Proc. of ICASSP-90,\npp. 81–84, Albuquerque, NM, 1990."
    },
    {
        "title": "Score-Informed Leading Voice Separation from Monaural Audio.",
        "author": [
            "Cyril Joder",
            "Björn W. Schuller"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415882",
        "url": "https://doi.org/10.5281/zenodo.1415882",
        "ee": "https://zenodo.org/records/1415882/files/JoderS12.pdf",
        "abstract": "Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a dif- ficult problem for automatic systems, in particular in the blind case, where no information is known about the sig- nal. However, in the case where a musical score is avail- able, one can take advantage of this additional informa- tion. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporally- aligned MIDI Score. The model used is based on Nonnegative Matrix Factor- ization (NMF), whose solo part is represented by a source- filter model. We exploit the score information by con- straining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popu- lar songs show that the use of these constraints can sig- nificantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics.",
        "zenodo_id": 1415882,
        "dblp_key": "conf/ismir/JoderS12",
        "keywords": [
            "separating",
            "musical recording",
            "automatic systems",
            "difficult problem",
            "blind case",
            "additional information",
            "temporally-aligned MIDI Score",
            "source-filter model",
            "constrain source activations",
            "score information"
        ],
        "content": "SCORE-INFORMED LEADING VOICE SEPARATION FROM\nMONAURAL AUDIO\nCyril Joder, Bj ¨orn Schuller\nInstitute for Human-Machine Communication, Technische Universit ¨at M ¨unchen, Germany\ncyril.joder@tum.de, schuller@tum.de\nABSTRACT\nSeparating the leading voice from a musical recording\nseems to be natural to the human ear. Yet, it remains a dif-\nﬁcult problem for automatic systems, in particular in the\nblind case, where no information is known about the sig-\nnal. However, in the case where a musical score is avail-\nable, one can take advantage of this additional informa-\ntion. In this paper, we present a novel application of this\nidea for leading voice separation exploiting a temporally-\naligned MIDI Score.\nThe model used is based on Nonnegative Matrix Factor-\nization (NMF), whose solo part is represented by a source-\nﬁlter model. We exploit the score information by con-\nstraining the source activations to conform to the aligned\nMIDI ﬁle. Experiments run on a database of real popu-\nlar songs show that the use of these constraints can sig-\nniﬁcantly improve the separation quality, in terms of both\nsignal-based and perceptual evaluation metrics.\n1. INTRODUCTION\nExtracting the main melody from a musical signal can be\nof interest, for example, for the remixing of a musical piece\nor the creation of a ‘play-along’ version of a recording, in\nthe context of karaoke or classical concertos. Whereas this\ntask is quite natural to the human ear, the automated solv-\ning of such a separation problem is notoriously difﬁcult.\nIn the past, many works have considered the separa-\ntion of musical sources as a blind audio source separa-\ntion problem, assuming only general knowledge about the\nsources, such as temporal and harmonicity priors [16] or\ntimbre information [14]. On the other hand, audio source\nseparation approaches which integrate speciﬁc information\nabout the content of each recording (see [15] for exam-\nple) have recently received a large interest. In the case\nof music, valuable information about the sources can be\nfound in the score when it is available. Hence, the topic\nof score-informed source separation, exploiting a tempo-\nrally aligned score, has recently emerged. The score in-\nformation is used to initialize the parameters of a model,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.which are then re-estimated in order to precisely match the\ndata. Several kinds of models have been proposed, such\nas a sinusoidal model [12], Nonnegative Matrix Factoriza-\ntion (NMF) [4] or Probabilistic Latent Component Analy-\nsis (PLCA) [6]. The model of [10] exploits MIFI synthe-\nses of the score, and operates a trade-off between ﬁdelity\nto the synthesized sound and to the actual data to be sep-\narated. In [1], a multipitch estimator is used to model the\nspectral shape of each note. The authors of [9] employ\na parametric NMF model which estimates a constant har-\nmonic structure for each source. The speciﬁc problem of\nextracting the main melody part has also been addressed\nin [7] with an NMF-like probabilistic model, where each\nnote is represented as a harmonic template.\nIn the present work, we exploit the physically-motivated\nsource-ﬁlter NMF model proposed in [2], which is specif-\nically designed for the extraction of the leading voice. We\ntake advantage of the aligned score through time and pitch\nconstraints. These constraints are similar to the ones al-\nready applied in [8] to a source-ﬁlter NMF model. How-\never, while the latter work exploits the information given\nby a multipich estimator, we make use of actual MIDI tran-\nscriptions of the pieces. We evaluate the beneﬁt of the\nscore-based information on a database of real data, com-\nposed of nine multi-track recordings of popular songs. The\nscores are constituted by real-life MIDI scores, which are\nsynchronized using a state-of-the-art alignment algorithm\n[11]. Several signal-based and perceptual evaluation crite-\nria are used and the results show that both the interferences\nand the separation artifacts are reduced thanks to the score\ninformation. Furthermore, the use of time-frequency con-\nstraints applied on the leading voice components allows for\na multi-pass approach for the removing of the reverberated\nvoice, which improves the perceived quality of the sepa-\nrated accompaniment.\nThe rest of this paper is organized as follows: in Section\n2 we present the source-ﬁlter NMF model used as base-\nline system for blind leading voice separation. Section 3\nexplains how the aligned MIDI score is exploited in the\nproposed methods for score-informed leading voice sep-\naration. We ﬁnally report the performed experiments in\nSection 4, before drawing some conclusions.\n2. BASELINE SYSTEM: BLIND SEPARATION\nAs baseline system for the blind separation of the leading\nvoice, we use the model proposed in [2]. Let us now detailthe main features of this model.\n2.1 Signal Model\nLetSbe the matrix representing the short-time power spec-\ntrum of the musical recording, which is assumed to be of\nsingle-channel nature. We suppose that this matrix can be\ndecomposed as:\nS=SV+SA; (1)\nwhereSVandSAare the short-time power spectrum of the\nleading voice and of the musical accompaniment, respec-\ntively. Furthermore, a source-ﬁlter model is assumed for\nthe solo part. Thus, the matrix SVcan be written as the\nelement-wise product of a ‘source’ matrix SF0by a ‘ﬁlter’\nmatrixS\b:\nSV=S\b\fSF0; (2)\nwhere\fdenotes the element-wise product.\nFor the contributions SA,SF0andS\b, an NMF model\nis assumed. Each of these terms is modeled as the product\nof two nonnegative matrices WandH. The former is a\ndictionary of spectrum templates (in columns) and the lat-\nter contains the corresponding activation amplitudes over\ntime. Finally, we have:\nS=\u0000\nW\bH\b\u0001\n\f\u0000\nWF0HF0\u0001\n+WAHA(3)\nThe ‘source spectral shapes’ of matrix WF0are set to\nﬁxed harmonic combs with logarithmically-spaced funda-\nmental frequencies, with 20 F0values per semitone (in\norder to take into account tuning variations or vibratos)\nbetween 100 Hz and 800 Hz. This range is sufﬁcient for\nmost popular songs. In order to estimate smooth ﬁlters,\nthe elements of the ﬁlters dictionary W\bare modeled as\nthe combination of overlapping Hann windows (in the fre-\nquency domain). As default parameters, the size of the\nﬁlter dictionary is 10 and the rank of the accompaniment\ndecomposition is set to 40.\n2.2 Separation Strategy\nThe leading voice separation procedure consists of several\nsteps. First, the matrices of eq. (3) (except for WF0) are\nestimated from the processed signal using an NMF opti-\nmization algorithm based on the Itakura-Saito divergence.\nFrom this ﬁrst result, only the estimated main source acti-\nvation matrix HF0is kept. It can be interpreted as the in-\nstantaneous ‘power’ of the corresponding fundamental fre-\nquencies. Since the signal of interest is the leading voice,\nit is assumed to correspond to the dominant pitch. How-\never, in order to avoid spurious ‘jumps’ in the case where\nthe voice stops or if another instrument has a higher energy\nin a distant pitch, a tracking algorithm is used to estimate\nthe whole sequence of F0values for the leading voice.\nIn the second step, another estimation of the model (3)\nis performed, in which HF0is constrained so that only the\nvalues around the tracked pitch are allowed to be non-zero.\nFinally, the leading voice is reconstructed by Wiener ﬁlter-\ning. The estimate ^SVof the short-time power spectrum isthen given by:\n^SV=\u0000\nW\bH\b\u0001\n\f\u0000\nWF0HF0\u0001\n(W\bH\b)\f(WF0HF0) +WAHA(4)\nand the time-domain signal is retrieved by inverse Fourier\ntransform (using the phase of the original mixture) and\noverlap-add.\n3. EXPLOITATION OF THE SCORE\nINFORMATION\nWe now explain how we exploit the additional information\ngiven by the aligned musical score.\n3.1 Information Conveyed by the Score\nBy musical score, we designate a set of notes character-\nized by their pitch, onset time and duration. In this work,\nwe additionally assume that the notes in the score corre-\nsponding to the leading voice can be discriminated from\nthe other notes. This is the case in most score MIDI\nﬁles, where the instruments correspond to different tracks.\nHence, the score can provide valuable information for the\nleading voice separation task. However, the score employs\na temporal scale (expressed in beats), whose correspon-\ndence with the actual time in second is in general both\nunknown and variable. Fortunately, systems for accurate\nmusic-to-score alignment have been proposed to overcome\nthis problem [5, 11, 13].\nThe aligned score then provides the pitch, onset and off-\nset time of the notes played in the musical piece. Neverthe-\nless, some limitations have to be taken into account. In par-\nticular, the pitches of the score are expressed in semitones,\nwhich constitutes a coarser frequency resolution than the\nshort-time Fourier transform representation of the audio.\nFurthermore, there can be various sources of imprecision\nor mismatch between the score and the actual recording.\nFor example, vibratos can strongly alter the fundamental\nfrequency of a note. There may also be transcription er-\nrors or different interpretations of the music. In particular,\nthe synchronization between the instruments or voices in\npolyphonic music may not be perfect, yielding a tempo-\nral indeterminacy and thus a possible imprecision in the\nalignment. For these reasons, the information conveyed\nby the aligned score cannot be fully trusted at a precise\nlevel. Nevertheless, it can be used at a coarser level, so as\nto narrow the search for the voice components in the spec-\ntrogram. In this work, we use only the ‘voice track’ of the\naligned score. Indeed, in most cases of popular music, no\nreference musical score exists and the available transcrip-\ntions can often resemble ‘lead sheet’ scores, which focus\non the main melody and only describe the global harmony\nof the accompaniment.\n3.2 Time and Pitch Constraints\nWe propose to exploit the score information through two\ntypes of constraints applied in the model (3). The ﬁrst\napproach only makes use of the information regarding\nwhether the leading voice is present or not in each frame.This corresponds to the case where the pitch of the aligned\nscore is not sufﬁciently reliable. This temporal constraint\nconsists in forcing all the activations of the source element\n(contained in the matrix HF0) to be equal to zero when the\nvoice is known to be absent. A time tolerance window is\nallowed, in order to overcome the possible temporal impre-\ncision of the score alignment. The voice is then considered\nas absent in a frame when no note of the aligned score is\npresent inside a temporal window of length \u0012t. The value\nof this tolerance threshold is a trade-off between two goals.\nIf it is short, one may ‘miss’ the voice in the case where\nthe score is imperfectly aligned. On the other hand, a long\ntolerance window may result in the extraction of another\ninstrument as the leading voice, when the latter is absent.\nThe second approach takes advantage of both, time and\npitch information, on the aligned score. As previously, the\nconstraint used consists in forcing zero values of the source\nactivation matrix where the source is known to be absent.\nThis implies the use of an additional tolerance threshold \u0012f\non the fundamental frequency, in order to limit the pitch\nimprecisions. Hence, a component HF0\ni;jor the source ac-\ntivation matrix (corresponding to fundamental frequency i\nin framej) is allowed to be non-zero only if there is a note\nin the aligned score, of pitch p, onset time t1and offset\ntimet2, such that:\njp\u0000ij\u0014\u0012fandt1\u0000\u0012t\u0014j\u0014t2+\u0012t: (5)\n4. EXPERIMENTS\n4.1 Database and Settings\nThe database used in this work is composed of nine\nseparated-track versions of well-known popular songs, for\nwhich a MIDI transcription was found on the internet. The\nlist of the songs is displayed in Table 1. Unfortunately,\nthese data cannot be shared due to copyright restrictions.\nIn all these pieces, the source of interest is a human voice.\nSome of the songs contain vocal harmonies (several vocal\nparts), which introduce an ambiguity about the determi-\nnation of the main source. In some others, mistakes are\nfound in the MIDI score, where some vocal parts are not\ntranscribed. In the corresponding pieces, only an excerpt\nwhere these problems do not occur has been used. All the\nﬁles were converted to mono signals with 44.1 kHz sam-\npling rate. For each piece, the ﬁle to be processed was cre-\nated by linearly mixing the leading voice with the accom-\npaniment. This procedure is much simpler than the mixing\nphase of professionally processed music, which often in-\nvolves additional ﬁltering or dynamic range compression.\nHowever, it was necessary to ensure that the ﬁnal mixtures\nperfectly correspond to the separated tracks.\nThe MIDI scores were aligned by the method presented\nin [11]. This results in very accurate alignment, and the\nimprecision between the recording and the synchronized\nMIDI are most of the time not noticeable. As this system\nis reported to detect almost all the notes within a 300 ms\nwindow around their actual position, we set the threshold\n\u0012tto this value. The frequency tolerance threshold \u0012fis\nheuristically set to 3 semitones.Title Original Artist\n1 A Day in the Life The Beatles\n2 Genie in a Bottle Christina Aguilera\n3 I Heard it Through the Grapevine Marvin Gaye\n4 Is This Love Bob Marley\n5 Long Train Running The Doobie Broth-\ners\n6 Sgt Pepper’s Lonely Hearts Club\nBandThe Beatles\n7 She’s Leaving Home The Beatles\n8 Stop Me If You Think You’ve\nHeard This One BeforeThe Smiths\n9 With a Little Help From My\nFriendsThe Beatles\nTable 1. List of the songs in the database.\nIn the experiments, we compare the separation obtained\nwith both systems described in Section 3 with the base-\nline system of Section 2. We also introduce an additional\nmethod, which exploits the temporal information of the\naligned score for a post-processing of the baseline system.\nIn this approach, a ‘temporal mask’ is applied on the lead-\ning voice estimate: when the voice is considered as absent\n(in the sense of Subsection 3.2), the corresponding signal\nframes are shifted to the accompaniment estimate.\nThe separation quality is measured by the criteria de-\nscribed in [3]. They comprise three signal-based and four\nperceptual metrics, namely the Signal-to-Distortion Ra-\ntio (SDR), Signal-to-Interference Ratio (SIR), Signal-to-\nArtifacts Ratio (SAR), Overall Perceptual Score (OPS)\nand Target-, Interference- and Artifacts-related Perceptual\nScores (TPS, IPS and APS respectively).\n4.2 Results\nThe results of the evaluations, averaged over the nine\npieces, are compiled in Table 2. One can ﬁrst notice that\nthe system exploiting the time-frequency constraint obtains\nthe best results according to almost all the measures used.\nIn particular, the average OPS of the leading voice esti-\nmates improves from 21.5 to 32.5 and the average SDR\nincreases by 1.5 dB. This indicates that the proposed ap-\nproach does improve the leading voice separation quality,\nsince both the interferences and artifacts are reduced com-\npared to the baseline system.\nThe use of the temporal indications of the score, which\nindicate when the leading voice is active, results in an im-\nprovement of the quality of both leading voice and accom-\npaniment estimates. As expected, the interferences of the\naccompaniment in the leading voice estimates are greatly\nreduced with the ‘temporal mask’ post-processing of the\nbaseline system. Hence, the average SIR increases from\n8.4 dB to 11.0 dB. Moreover, the constraints detailed in\nSubsection 3.2, which forces the voice to be active only\nwhere the main melody is actually present, leads to a fur-\nther improvement for most of the songs. The use of these\nconstraints results in a more precise estimation of the spec-\ntral components of the NMF and, as a consequence, in a re-\nduction of the artifacts. For instance, the average APS on\nthe accompaniment parts increases from 59.0 to 63.8 withSDR (dB) SIR (dB) SAR (dB) OPS TPS IPS APS\nLV Ac LV Ac LV Ac LV Ac LV Ac LV Ac LV Ac\nBaseline 5.8 9.1 8.4 12.4 15.2 19.3 21.5 37.0 39.5 62.9 50.8 55.6 31.4 50.6\nBaseline + Tem-\nporal Mask6.7 10.0 11.0 12.9 15.8 20.5 29.5 43.6 41.4 68.1 58.3 57.6 35.1 59.0\nTime Constraint 7.0 10.3 11.5 13.3 16.1 20.8 31.6 43.3 45.4 67.9 58.9 57.5 37.4 62.6\nTime-Frequency\nConstraint7.3 10.5 11.9 13.7 16.9 21.5 32.5 42.9 46.4 68.3 57.9 58.1 39.9 63.8\nTable 2. Average evaluation criteria, measured on the leading voice (LV) and accompaniment (Ac) parts. In boldface are\nthe best value of each column.\ntime and frequency constraints.\nA more precise representation of the SDR values for\nevery tested song is displayed in Figure 1. This ﬁgure\nconﬁrms that the use of the the information conveyed in\nthe musical score is valuable. Indeed, in terms of SDR,\nthe baseline system is outperformed by all the other ap-\nproaches. An observation which can seem surprising is\nthat in many of the pieces, the addition of the frequency\nconstraint does not improve the SDR measure. This is ex-\nplained by the efﬁciency of the tracking algorithm used\nfor the determination of the fundamental frequency of the\nleading voice. Hence, when the leading voice is strongly\ndominant in the recording, this tracking does not need to\nbe constrained. On the other hand, the constraint has a vis-\nible effect on recording no. 4: Is This Love. Indeed, this\nsong contains background vocals which can incidentally\nbe tracked as main voice, when the lead singer is not domi-\nnant (for example in the case of breaths). Hence, the global\naverage SDR slightly increase from 8.7 dB to 8.9 dB.\nThe OPS criterion measured on the database is dis-\nplayed in Figure 2. In general, this metric exhibits the same\ntendencies as the SDR. However, there are some noticeable\ndifferences concerning the accompaniment estimates. In-\ndeed, for the ﬁrst three songs, the best OPS is obtained\nwith the original system. A more speciﬁc analysis reveals\nthat these correspond to cases where the score does not\nperfectly match the performance.\nOne of the main sources of deviation is the length of the\nnotes in the MIDI score. Indeed, whereas the note onsets\ncan be relatively well deﬁned, determining the offsets is\na notoriously hard problem, which can even be ill-posed.\nThe score often indicates how the notes are to be played,\nwhich can actually be different from how the notes are\nheard in the recording, mainly because of the reverbera-\ntion phenomenon (which is often increased by artiﬁcial ef-\nfects). This phenomenon is strongest in song no. 1 A Day\nin the Life. In this piece, with the proposed constraints,\nthe voice is ‘cut’ at the end of some musical phrases, be-\ncause it is considered as absent while it can still be heard in\nthe recording. This phenomenon is not prominent from the\n‘signal’ point of view: indeed, the SIR criteria measured on\nthe accompaniment estimates of this piece are 16.3 dB with\nthe time-frequency constraint and 15.6 dB with the base-\nline system. However, this results in intermittent ‘bursts’\nof voice in the accompaniment part, which is more strongly\npenalized by the perceptual measures. Hence, the value of\nthe IPS degrades from 60.4 to 50.9.In the songs no. 2 Genie in a Bottle and no. 3 I Heard\nit Through the Grapevine, this note length problem is also\nvisible. Besides, the lead singer sometimes adds ‘orna-\nments’ to the transcribed score, in particular through ‘vo-\ncalises’, which are common in the soul music style. Hence,\nboth time and frequency priors indicated in the MIDI ﬁle\ncan be misleading at some point. As previously, this does\nnot have a large inﬂuence on the signal-based measures,\nsince the SIR of the accompaniment estimate only de-\ncreases from 13.3 dB to 12.6 dB. However, the perceptual\nimportance of these separation errors is greater: the OPS\ndrops from 45.3 to 34.1.\n4.3 Constrained Second Pass\nIn order to reduce the problem caused by the reverberation\nof the leading voice, we experimented with the use of a sec-\nond pass of the separation algorithm. Indeed, the reverber-\nation often introduces ‘polyphony’, in the sense that sev-\neral notes of the leading voice can be present at the same\ntime in the recording. Since the separation model is inher-\nently monophonic, because it is motivated by the physics\nof voice production, a multi-pass approach is needed for\nthe handling of several simultaneous notes.\nHence, after the ﬁrst separation with the time-frequency\nconstraint, we apply the same algorithm on the accompa-\nniment estimate, where some reverberated leading voice is\nsupposed to remain. In this second pass however, the time\ntolerance for the offset is modiﬁed, so that each note is al-\nlowed to be active for 800 ms after its annotated extinction\nin the synchronized MIDI score. The threshold for the on-\nset time is unchanged. The estimated reverberated voice is\nthen added to the voice estimate of the ﬁrst separation.\nFigure 3 displays the inﬂuence of this approach on the\nOPS criteria. While it has little effect on the OPS of the\nleading voice estimate, it can recover from some of the\npreviously described problems of the accompaniment. In-\ndeed, on six of the nine tested pieces, the two-pass sep-\naration visibly increases the OPS. Furthermore, the use of\nthis approach leads to an improvement on every piece com-\npared to the baseline system, except for song no. 9 With a\nLittle Help From My Friends, where the score is equivalent.\nMore thorough analysis reveals that the second pass ac-\ntually slightly degrades the leading voice estimates, ac-\ncording to many evaluation metrics. In particular, it adds\nsome interference in the vocal track, since in many places,\nthe reverberated voice is dominated by the accompani-\nment. Thus, the average SIR decreases from 11.9 dB to1 2 3 4 5 6 7 8 9 Avg246810Leading Voice\nSongSDR (dB)\n1 2 3 4 5 6 7 8 9 Avg46810121416Accompaniment\nBaseline\nBaseline + Temporal \nMask\nTime Constraint\nTime & Frequency \nConstraints\nSongSDR (dB)Figure 1. Signal to Distortion Ratio (SDR) measured on each of the tested songs and average.\n1 2 3 4 5 6 7 8 9 Avg20253035404550Accompaniment\nBaseline\nBaseline + Temporal \nMask\nTime Constraint\nTime & Frequency \nConstraints\nSongOPS\n1 2 3 4 5 6 7 8 9 Avg10203040Leading Voice\nSongOPS\nFigure 2 . Overall Perceptual Score (OPS) measured on each of the tested songs and average. For the accompaniment of\nsong no. 5, the extraction is nearly perfect, since all proposed methods obtain an OPS of 99 (not represented here).\n9.6 dB. However, the artifacts are somewhat reduced and\nthe voice seems to be better preserved. Hence, the average\nTPS improves from 46.4 to 53.7.\nThe opposite effect is observed on the accompaniment\nestimates, since more artifacts are measured. Hence, the\naverage APS decreases from 63.8 to 55.3. However, these\nartifacts, which are very limited in terms of signal energy\n(the average SAR is 21.2 dB), are counterbalanced by the\nreduction of the interferences: the average SIR increases\nfrom 13.7 dB to 15.0 dB.\n5. CONCLUSION\nIn this work, we exploited of a time-aligned MIDI ﬁle to\nperform a score-informed separation of the leading voice\nfrom a musical recording. The source-ﬁlter model as-\nsumed for the leading voice allowed for a natural use of the\nscore information, by means of time and frequency con-\nstraints on the source components. We evaluated the use-\nfulness of these constraints on a database of real recordings\nof popular songs and corresponding MIDI scores.\nThe results show that the score-guided constraints ap-\nplied to the model not only reduce the interferences of the\naccompaniment in the leading voice separated track, but\nalso allow for a more accurate estimation of the spectral\nshapes of all the components. Hence, this results in a re-\nduction of the separation artifacts on both leading voice\nand accompaniment estimates. These improvement can be\nmeasured with perceptual metrics as well as signal energy-based criteria. Furthermore, a two-pass approach is made\npossible by the time-frequency constraints on the voice\ncomponents. This allows for the removal of the remaining\nreverberated voice in the accompaniment estimate, while\nlimiting the artifacts introduced when the voice has been\ncorrectly eliminated.\nHowever, some problems are observed when the score\ndoes not exactly match the performance.In these cases, the\nscore-based constraints can prevent the system from es-\ntimating the right components. Although these problems\ndo not generally represent much in terms of signal energy,\nthey can have some perceptual importance. Thus, future\nwork for the improvement of the separation could involve\nmusically-motivated modiﬁcations of the constraints, for\nexample allowing more frequency deviation in the begin-\nning and at the end of the notes, in order to account for\nglissandi. One could also investigate a ‘soft constraint’\napproach, which would penalize source activations which\nare far from the score indication, without completely for-\nbidding them. The inﬂuence of the separation parameters\n(number of components for the accompaniment, size of\nthe ﬁlter dictionary) could also be more thoroughly inves-\ntigated. In particular, the search for a relation between the\noptimal parameters and some features extracted from the\nmusical score could be interesting. Finally, another per-\nspective can be the exploitation of the score information\nfor the extraction of the unvoiced components of the lead-\ning voice, which were not taken into account in this work.1 2 3 4 5 6 7 8 9 Avg1020304050Leading Voice\nSongOPS\n1 2 3 4 5 6 7 8 9 Avg20253035404550Accompaniment\nBaseline\nTime & Frequency \nConstraints\nWith Second Pass\nSongOPSFigure 3. Inﬂuence of the second pass of leading voice separation on the OPS criterion. The same remark as in Figure 2\nholds for song no. 5.\n6. REFERENCES\n[1] Zhiyao Duan and Bryan Pardo. Soundprism: An on-\nline system for score-informed source separation of\nmusic audio. IEEE J. Select. Topics Signal Processing,\n5(6):1205–1215, October 2011.\n[2] Jean-Louis Durrieu, Bertrand David, and Ga ¨el\nRichard. A musically motivated mid-level represen-\ntation for pitch estimation and musical audio source\nseparation. IEEE J. Select. Topics Signal Processing,\n5(6):1180 –1191, October 2011.\n[3] Valentin Emiya, Emmanuel Vincent, Niklas Harlander,\nand V olker Hohmann. Subjective and objective qual-\nity assessment of audio source separation. IEEE Trans.\nAudio, Speech, Language Processing, 19(7):2046 –\n2057, September 2011.\n[4] Sebastian Ewert and Meinard M ¨uller. Using score-\ninformed constraints for NMF-based source separa-\ntion. In Proc. IEEE ICASSP, Kyoto, Japan, 2012.\n[5] Sebastian Ewert, Meinard M ¨uller, and Peter Grosche.\nHigh resolution audio synchronization using chroma\nonset features. In Proc. IEEE ICASSP, pages 1869–\n1872, Taipei, Taiwan, 2009.\n[6] Joachim Ganseman, Paul Scheuners, Gautham J.\nMysore, and Jonathan S. Abel. Evaluation of a score-\ninformed source separation system. In Proc. ISMIR,\npages 219–224, Utrecht, Netherlands, August 2010.\n[7] Yushen Han and Christopher Raphael. Desoloing\nmonaural audio using mixture models. In Proc. ISMIR,\npages 145–148, Vienna, Austria, 2007.\n[8] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen.\nMusical instrument recognition in polyphonic audio\nusing source-ﬁlter model for sound separation. In Proc.\nISMIR, pages 327–332, Kobe, Japan, 2009.\n[9] Romain Hennequin, Bertrand David, and Roland\nBadeau. Score informed audio source separation us-\ning a parametric model of non-negative spectrogram.\nInProc. IEEE ICASSP, pages 45–48, Prag, Czech Re-\npublic, 2011.[10] Katrsutoshi Itoyama, Masataka Goto, Kazumori Ko-\nmatani, Tetsuya Ogata, and Hiroshi G. Okuno. Integra-\ntion and adaptation of harmonic and inharmonic mod-\nels for separating polyphonic musical signals. In Proc.\nIEEE ICASSP, pages 57–60, Honolulu, Hawaii, USA,\n2007.\n[11] Cyril Joder, Slim Essid, and Ga ¨el Richard. A condi-\ntional random ﬁeld framework for robust and scalable\naudio-to-score matching. IEEE Trans. Audio, Speech,\nLanguage Processing, 19(8):2385–2397, November\n2011.\n[12] Yipeng Li, John Woodruff, and DeLiang Wang.\nMonaural musical sound separation based on pitch\nand common amplitude modulation. IEEE Trans. Au-\ndio, Speech, Language Processing, 17(7):1361–1371,\nSeptember 2009.\n[13] Bernhard Niedermayer and Gerhard Widmer. A multi-\npass algorithm for accurate audio-to-score alignment.\nInProc. ISMIR, pages 417–422, Utrecht, the Nether-\nlands, 2010.\n[14] Alexey Ozerov, Pierrick Philippe, Fr ´ed´eric Bimbot,\nand R ´emi Gribonval. Adaptation of bayesian mod-\nels for single-channel source separation and its ap-\nplication to voice/music separation in popular songs.\nIEEE Trans. Audio, Speech, Language Processing,\n15(5):1564–1578, July 2007.\n[15] Alexey Ozerov, Emmanuel Vincent, and Fr ´ed´eric\nBimbot. A general ﬂexible framework for the han-\ndling of prior information in audio source separation.\nIEEE Trans. Audio, Speech, Language Processing,\n20(4):1118–1133, May 2012.\n[16] Emmanuel Vincent. Musical source separation us-\ning time-frequency source priors. IEEE Trans. Audio,\nSpeech, Language Processing, 14(1):91–98, January\n2006."
    },
    {
        "title": "Pitch Content Visualization Tools for Music Performance Analysis.",
        "author": [
            "Luis Jure",
            "Ernesto López",
            "Martín Rocamora",
            "Pablo Cancela",
            "Haldo Sponton",
            "Ignacio Irigaray"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414860",
        "url": "https://doi.org/10.5281/zenodo.1414860",
        "ee": "https://zenodo.org/records/1414860/files/JureLRCSI12.pdf",
        "abstract": "This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is briefly reviewed. Its application to mu- sic analysis is exemplified with two pieces of non-notated music: a field recording of a folkloric form of polyphon- ic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting com- plex pitch evolution, difficult to analyze and notate with precision using Western common music notation. By us- ing novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difficult or impos- sible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional mu- sicological analysis. Two software tools are released that allow the practical use of the described methods.",
        "zenodo_id": 1414860,
        "dblp_key": "conf/ismir/JureLRCSI12",
        "keywords": [
            "pitch content visualization",
            "audio recordings",
            "computational method",
            "pitch contours",
            "music analysis",
            "non-notated music",
            "polyphonic singing",
            "commercial blues recording",
            "vocal parts",
            "complex pitch evolution"
        ],
        "content": "PITCH CONTENT VISUALIZATION TOOLS FOR\nMUSIC PERFORMANCE ANALYSIS\nLuis Jure1ErnestoL ´opez2Mart´ın Rocamora12P\nablo Cancela2Haldo Sponton2Ignacio Irigaray2\n1School of Music and2Faculty of Engineering, Universidad de la Rep ´ublica, Uruguay\nlj@eumus.edu.uy {elopez,rocamora,pcancela,haldos,irigiaray}@fing.edu.uy\nABSTRACT\nThis work deals with pitch content visualization tools for\nthe analysis of music performance from audio recordings.\nAn existing computational method for the representation\nofpitchcontoursisbrieﬂyreviewed.Itsapplicationtomu-\nsic analysis is exempliﬁed with two pieces of non-notated\nmusic: a ﬁeld recording of a folkloric form of polyphon-\nic singing and a commercial recording by a noted blues\nmusician.Bothexampleshavevocalpartsexhibitingcom-\nplex pitch evolution, difﬁcult to analyze and notate with\nprecision using Western common music notation. By us-\ningnoveltime-frequencyanalysistechniquesthatimprove\nthe location of the components of a harmonic sound, the\nmelodic content representation implemented here allows\na detailed study of aspects related to pitch intonation and\ntuning. This in turn permits an objective measurement of\nessentialmusicalcharacteristicsthataredifﬁcultorimpos-\nsible to properly evaluate by subjective perception alone,\nand which are often not accounted for in traditional mu-\nsicological analysis. Two software tools are released that\nallow the practical use of the described methods.\n1. INTRODUCTION\nMost of the established techniques for musical analysis\ndo not work directly on the acoustic signal, but on some\nkind of symbolic representation of it [1]. This representa-\ntion reduces the continuous and complex sound ﬂow into\na set of discrete events, usually determined by their most\nsalientparameters,suchastemporallocation,durationand\npitch. Applications of spectrographic analysis of sound to\nthedevelopmentofnewtechniquesofmusicalanalysisbe-\ngantobeexploredsystematicallywiththeworkbyRobert\nCogan[3].Usingtime-frequencyrepresentationsoftheau-\ndio signal, Cogan proposes an analytical method applica-\nble to both structural and local aspects of a musical piece,\nthat exempliﬁes analyzing music from very varied corpus.\nRecently, techniques based on sonographic representation\nhave been applied extensively to the analysis of electroa-\ncousticmusic[8].Thesetoolsarealsobeingappliedtono-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbear this noticeand thefull citation on theﬁrst page.\nc/circlecopyrt2012International Society for Music Information Retrieval.tated music or music from traditions not based on scores,\ntodiscussaspectsofmusicnotrepresentedinsymbolicno-\ntationbytheanalysisofrecordings.Thismayincludeboth\ncomponents that depend on the performance [7] (such as\ntemporal and tuning micro-deviations), or the precise de-\ntermination of the tuning system of a certain music [6].\nDifferentsoftwaretoolsforcomputer-aidedanalysis,vi-\nsualization and annotation of recorded music have been\ndeveloped, for instance Sonic Visualiser.1They typical-\nly include traditional time-frequency representations and\ndigitalsignalprocessingtoolsintendedformusicinforma-\ntionretrieval,suchasonsetsorpitchdetection.Somemid-\nlevelrepresentationsarealsoavailable,i.e.,signaltransfor-\nmations that tend to emphasize higher semantics than the\nenergy in the time-frequency plane [4]. Those mid-level\nrepresentations are usually devised to facilitate the sub-\nsequent feature extraction and processing of an automat-\nic algorithm. However, as suggested in [5], they can also\nbe used by humans to study performance nuances such as\npitch modulations or expressive timing.\nIn this article, examples are given of the type of anal-\nysis that can be done with an implementation of the pitch\nsaliencerepresentationproposedin[2]byanend-userwith\namusicologicalbackground(ﬁrstauthor).Inaddition,two\ngraphical software tools are released that allow the practi-\ncal use of the described methods by the research commu-\nnity. The representation proposed, called F0gram, is based\non the Fan Chirp Transform (FChT) [13] and seeks two\nmain goals: ﬁrstly, the precise time-frequency location of\nthe components of a complex sound, using recent analysis\ntechniques that overcome the limitations of the classical\ntools; secondly, the automatic grouping of all the compo-\nnents that are part of the spectrum of a single harmonic\nsource, highlighting the fundamental frequency, f0. This\nmakesitpossibletoobtainanaccurategraphicalrepresen-\ntation of the temporal evolution of the melodic content of\namusicrecording,thatallowsthedetailedstudyofperfor-\nmance aspects related to pitch intonation and timing (e.g.\ntuning system, vibrato, glissando, pitch slides).\nTheremainingofthedocumentisorganizedasfollows.\nSections 2 and 3 brieﬂy describe the time-frequency anal-\nysis and the pitch salience computation respectively. Ex-\namples of performance music analysis using the released\ntools are provided in section 4. The paper ends with some\ndiscussion on this work and ideas for future research.\n1http://www.sonicvisualiser.org/2. TIME-FREQUENCY ANALYSIS\nMusicaudiosignalsoftenexhibitamplefrequencymod-\nulation, such as\nthe typical rapid pitch ﬂuctuations of the\nsinging voice. Precisely representing such modulations is\na challenging problem in signal processing. It is reason-\nable to look for a signal analysis technique that concen-\ntrates the energy of each component in the time-frequency\nplane as much as possible. In this way, the representation\nof the temporal evolution of the spectrum is improved and\ntheinterferencebetweensoundsourcesisminimized,sim-\nplifying the task of higher level algorithms for estimation,\ndetection and classiﬁcation.\nThe standard method for time-frequency analysis is the\nShortTimeFourierTransform(STFT),whichprovidescon-\nstant resolution in the time-frequency plane. A typical al-\nternative for multi-resolution analysis is the Constant Q\nTransform (CQT). Both representations produce a Carte-\nsian tiling of the time-frequency plane, as depicted in Fig-\nure 1. This may be inappropriate for non-stationary sig-\nnals, for instance a frequency modulated sinusoid, namely\na chirp. The virtue of the FChT is that it offers optimal\nresolutionsimultaneouslyforallthepartialsofaharmonic\nlinear chirp, i.e. harmonically related chirps of linear fre-\nquency modulation. This is well suited for music analysis\nsincemanysoundshaveaharmonicstructureandtheirfre-\nquency modulation can be approximated as linear within\nshort time intervals.\nThe FChT can be formulated as [2],\nX(f,α)/defines/integraldisplay∞\n−∞x(t)φ′\nα(t)e−j2πfφα(t)dt,(1)\nwhereφα(t) = (1 +1\n2αt)t, is a time warping function.\nThe parameter α, called the\nchirp rate, is the variation rate\nof the instantaneous frequency of the analysis chirp. No-\ntice that by the variable change τ=φα(t), the formulation\ncanberegardedastheFourierTransformofatimewarped\nversion of the signal x(t), which enables an efﬁcient im-\nplementationbasedontheFFT.Ifaharmonicchirpisana-\nlyzed and the correct αvalue is selected for the transform,\nthe warping yields sinusoids of constant frequency so the\nspectral representation is a set of very narrow peaks.\nFigure 1. Time-frequency tiling sketch for the STFT, the\nShort Time CQT and the Short Time FChT and the\nresult-\ning resolution for a two-component harmonic linear chirp.\nA time-frequency representation can be built by com-\nputing the FChT for consecutive short time signal frames,\nnamely a Short Time FChT (STFChT). This requires the\ndeterminationoftheoptimal αvalueforeachsignalframe.Forpolyphonicmusicanalysisthereisnosingleoptimal α\nvalue, so the approach followed in [2] is to compute sev-\neral FChT instances with different αvalues. This yields a\nmultidimensional representation made up of various time-\nfrequency planes. The selection of the αvalues that pro-\nducethebetterrepresentationofeachsoundpresentisper-\nformed by means of pitch salience. A comparison of the\nSTFT and the STFChT applied to a polyphonic music au-\ndio clip is provided in Figure 2.\nTime (s)Frequency (Hz)STFT (N=2048 samples with sr=44100 Hz)\n8989.1 89.2 89.3 89.4 89.5 89.6 89.70100020003000400050006000\nTime (s)STFCHT\n8989.1 89.2 89.3 89.4 89.5 89.6 89.7\nComparison of the F0gram computed with the Fourier Transform and the Fan Chirp Transform\nEb4E4F4F#4G4Ab4A4Bb4\nTime (s)89 89.1 89.2 89.3 89.4 89.5 89.6 89.7Eb4E4F4F#4G4Ab4A4Bb4\nFigure2.Above:ComparisonoftheSTFTandSFChTfor\nan excerpt from the example of section 4.1. The chirp\nrate\nof the most prominent sound source is selected for each\nframe. Note the improved representation obtained for this\nsource while the rest is blurred. Below: F0grams obtained\nfromtheDFTandFChT.Rapidpitchﬂuctuationsarebetter\nrepresented in the latter.\n3. PITCH SALIENCE REPRESENTATION\nArepresentation intended for visualizing the pitch con-\ntentofpolyphonicmusicsignalsshouldprovideanindica-\ntionofprominenceorsalienceforallpossiblepitchvalues\nwithin the range of interest. A common approach for pitch\nsalience calculation is to deﬁne a fundamental frequen-\ncy grid, and compute for each frequency value a weight-\ned sum of the partial amplitudes in a whitened spectrum\n[5,13].Amethodofthiskindwasusedin[2]andisbrieﬂy\ndescribed in the following.\nGiven the FChT of a frame X(f,α), salience of fun-\ndamental frequency f0is obtained by summing the log-spectrum at the positions of the corresponding harmonics,\nρ(f0,α) =1\nnHnH/summationdisplay\ni=1log|X(if0,α)|, (2)\nwherenHis the number of harmonics located up\nto a cer-\ntain maximum analysis frequency. This is computed for\neach signal frame in a certain range of f0values.\nSomepostprocessingstepsarecarriedoutinordertoat-\ntenuatespuriouspeaksatmultiplesandsubmultiplesofthe\ntruepitches,andtobalancedifferentfundamentalfrequen-\ncy regions [2]. Finally, for each f0 in the grid, the highest\nsalience value is selected among the different available α\nvalues. In this way, a representation that shows the evolu-\ntionofthepitchoftheharmonicsoundsintheaudiosignal\nis obtained, namely an F0gram. Examples of the resulting\nrepresentation are depicted in Figure 3 for two short audio\nclips. The F0gram produces a fairly precise pitch evolu-\ntion representation, contrast balanced and without spuri-\nous noticeable peaks when no harmonic sound is present.\nNotethatsimultaneoussourcescanbecorrectlyrepresent-\ned,eveninthecasethattheycoincideintimeandfrequen-\ncy if their pitch change rate is different. Figure 2 shows\na comparison of the F0gram obtained from the DFT and\ntheFChT.Theimprovementintime-frequencylocalization\nprovides a more accurate representation of pitch.\nFrequency (Hz)Examples of F0grams\n0.5 1 1.5 2 2.5 3 3.5 480160320640\nTime (s)Frequency (Hz)\n0.5 1 1.5 2 2.5 3 3.5 4 4.51603206401280\nTime (s)Frequency (Hz)\n0.20.220.240.260.28192222257\nDetailed visualization\nTime (s)1.55 1.6 1.65166192222257297\nTime (s)2.2 2.4 2.6 2.8461533\nFigure 3. Above: F0gram examples for audio excerpts of\npop1.wav andoperafem4.wav from the MIREX melody\nextractiontestset.Below:Detailedvisualization.Crossing\npitchcontoursarewell\nresolved,andsimultaneoussources\nand rapid pitch ﬂuctuations are precisely represented.4. CASE STUDIES\nInordertoexemplifytheapplicationofthesetechniques\nto musicological analysis, we have selected two pieces of\nnon-notated music, both of them with vocal parts exhibit-\ning complex pitch evolution, very difﬁcult or downright\nimpossibletonotatewithprecisionusingWesterncommon\nmusicnotation:aﬁeldrecordingofafolkloricfemalevocal\ntrio from west-central Bulgaria, and a commercial record-\ning by noted blues singer and guitarist Muddy Waters.\n4.1 Diaphonic chant of the Shope country\nThroughout the world, folkloric forms of polyphonic\nsinging are relatively scarce, one of the most notable ex-\nceptionsbeingthediaphonicsingingoftheShoperegionin\nwest-centralBulgaria.Acloselyrelatedformcanbefound\nin the Pirin region in the south-west of the country, and\nextending into the Republic of Macedonia.\nAsageneralrule,thesepolyphonicsongsareperformed\nbyfemalesingers,andthesounditselfofthevoicesisusu-\nally enough to impress listeners not familiar with this id-\niom. But the treatment of pitch in these two-part songs, or\ndvuglas, also has some unique characteristics.\nInatypicalsetting,themelodypartissungbyonesinger,\nand the second part by two or sometimes more. The up-\nper part has several classiﬁed melodic gestures, one of the\nmost characteristic being a sort of—usually fast—glottal\ntrill called tresene. Another characteristic gesture is the\nizvikvane, a form of ending the phrases with a fast upward\nleap on the vowel sound “eee”. The second part is more\nstatic,andhasbeendescribedasa“drone”orpedal.Itusu-\nally stays on the tonic of the mode, with occasional devi-\nations to the sub-tonic when the melody descends to the\ntonic. Both parts join, however, to perform the izvikvane\ntogether. Apart from some fast swoops, the melody part\nmoveswithinaverylimitedrange,especiallyintheShope\nregion. Thisresultsinapreponderance ofnarrowintervals\nbetween the voices [9,10].\nForourcasestudy,acommerciallyavailableﬁeldrecord-\ning of a folkloric group from the Shope region was used\n[12]. The recording is identiﬁed, without further informa-\ntion,asa“HarvestSong”performedbyafemalevocaltrio\nfrom the village of Zheleznitsa. The recording date can be\nplaced around 1980. The song consists of 9 short phrases\nofsimilarduration(ca.10 ∼12s),structuredasthreevaria-\ntions of a group of three distinct phrases.\nFigure 4 shows an F0gram of the third of these phras-\nes, exhibiting all the characteristics described above: the\nsecond part begins in the sub-tonic and soon moves to the\ntonic for the rest of the phrase, while the ﬁrst part moves\nboth above and below the tonic, singing a more embel-\nlished melody that includes faster and slower tresene. The\ncadential izvikvane coversanarrowoctavebeforedescend-\ning back to the tonic area, and sounds like a unison of the\nthree voices, in accordance with the prevailing description\nofizvikvane.TheF0gramallowsustoappreciate,however,\nthat there is actually a slight separation of the voices, very\ndifﬁcult to perceive by listening alone.Time (s)23 24 25 26 27 28 29 30 31 32C4C#4D4Eb4E4F4F#4G4Ab4A4Bb4B4C5C#5D5Eb5E5F5F#5\nFigure4.OnephraseoftheHarvestSong,showingcharacteristictraitsofShopediaphonicsinging:melodicornamentation\nintheﬁrstpart(including treseneandaﬁnal izvikvane),andapedal\nonthe“tonic”inthesecondpart,withaslowglissando.\nAn analysis of the simultaneities conﬁrms that narrow\nintervals prevail, and a variety of intervals can be found\nbetween the unison and the major third. The F0gram ob-\ntained from the FChT permits a precise measurement of\nthese type of intervals, as can be appreciated in Figure 2.\nOf special interest was the location of the sub-tonic, and\nthe interval most frequently found lies half-way between\none and two semitones below the tonic (sec. 23–24). This\nsamekindof“second”canoftenbefoundabovethetonic,\nin the upper part (sec. 28–29). The speed and range of the\ntresenecan also be assessed with good precision. Typical\nrates are around 8 ∼9 Hz (sec. 26–27), but slower rates can\nalso be found (sec. 30–31). The width is variable, extend-\ning through intervals of up to three semitones (sec. 26).\nSo far, the analysis of the F0gram conﬁrms—and per-\nmitsabettermeasurementof—thecharacteristicsdescribed.\nObserving the second part with more detail, however, its\nbehaviorshouldbestriking:insteadofremainingonaﬁxed\nnote, as its supposed character of “drone” would suggest,\nit performs a slow upward glissando, covering roughly the\nequivalent of a semitone (from F to F♯) during approxi-\nmately 7 seconds. This displacement of the tonic not only\noccurs in various degrees in all the phrases throughout the\nsong, but it also covers different pitch areas in each one\n(e.g., between E and F, or F♯ and G), resulting in a sort of\n“roving” tonic. Field recordings from different villages in\nthe Shope region were analysed, and a similar behaviour\nwas found in most of them, with glissandi of the “pedal”\nnotes typically spanning between 50 cents and a semitone\nwithinaphrase.Inthecourseofthesong,theintonationof\nthe local “tonics” can vary as much as three semitones.\nTheimplicationsaretwo-foldandofparamountimpor-\ntance:unlikeatypicaldroneorpedalpoint,essentiallystat-\nic, this second part has a dynamic character, and this kind\nof slowly ascending movement imposes on the polyphony\na very particular tension and expressiveness. Additional-\nly, the fact that the “tonic” varies between phrases, turns\nsomewhat fuzzy the idea itselfof modal tonic.\nThisphenomenonisnotmentionedintheconsultedbib-\nliography and is not represented in the available transcrip-tions, although it was found in various degrees in several\nrecordings analysed, suggesting that these traits conform\na characteristic feature of the Shope musical idiom and\nshouldbeconsideredanessentialcomponentofthepower-\nfulexpressivenessofthisparticularformoffolkloricpoly-\nphonic singing. These analysis techniques should be ap-\nplied to a wider corpus to properly assess the importance\nof this performance practice.\n4.2 Muddy Waters - Long Distance Call\nThe Blues is a genre of popular music deeply rooted in\ntheAfrican-AmericanfolksongtraditionoftheruralSouth\nof the United States, and as such it shows several traits\nthat differ considerably from those found in the traditional\nEuropeanmusicalsystem.Themostcharacteristicofthese\ntraits are the so-called “blue notes”, the precise deﬁnition\nof which has been elusive and even somewhat controver-\nsial. A simplistic but widely circulating deﬁnition reduces\nthem to the use of the minor third and minor seventh de-\ngrees (sometimes also the diminished ﬁfth) in a major-\nkey context, for example, E♭ and B♭in C major. Actual-\nly, this performance practice is much more complex, and\nentails two related but distinct aspects: the use of pitches\nthat lie outside the standard Western tuning system, and\ncontinuous variations ofpitchwithincertaintonalregions.\nRather than ﬁxed tones in a discrete scale system, blue\nnotes would be ﬂexible areas in the pitch space. For the\nanalysis of the behaviour of these pitch complexes in ac-\ntualperformance,wechosearecordingbyMuddyWaters,\none of the most important blues musicians of all time, re-\ngardedasanunsurpassedperformerbothasaguitarist and\nas a singer.\nOn January 23, 1951 he recorded his own composition\n“Longdistancecall”forChessRecords.Hesingsandplays\nelectricguitar,andisaccompanied byMarion“LittleWal-\nter”JacobsonharmonicaandWillieDixonondoublebass.\nThesongisastandard12-bar,three-linestanzablues,where\nthesecondlineineachverserepeatstheﬁrst,andthethird\nis a rhyming conclusion. After a 4-bar introduction, the5.2 5.3 5.4 6.1 6.2 6.3 6.4\n A2  C3    F3   A3  C4    F4\non/noteheads.s2\nthe/noteheads.s2\nme/accidentals.flat.arrowup/noteheads.s2\nphone/noteheads.s2\n6/noteheads.s2\ncall/accidentals.flat/noteheads.s2\n8/accidentals.flat/clefs.G /rests.3\n\u0000/noteheads.s2\ntime. some-/noteheads.s2/accidentals.natural.arrowdown\n/noteheads.s2/accidentals.natural.arrowdown/noteheads.s2\nyou/noteheads.s2\nme/noteheads.s2\nlove3\nYou3\n/accidentals.flat.arrowup/noteheads.s2/rests.3 /noteheads.s2\n\u0001\nsay/accidentals.flat\n3/rests.2 /rests.2\nPlease,/noteheads.s2\ndar-/accidentals.flat.arrowup/noteheads.s2/accidentals.natural/accidentals.flat/noteheads.s2\nlin',/noteheads.s2\n9.2 9.3 9.4 10.1 10.2 10.3 10.4\n A2  C3    F3   A3  C4    F4\n/noteheads.s2/accidentals.flat.arrowup\non/noteheads.s2\n/noteheads.s2 /noteheads.s2\nsome-/accidentals.flat\n\u0001\nphone3/noteheads.s2/accidentals.flat.arrowup/accidentals.flat.arrowup\nme/noteheads.s2\ncall/noteheads.s2 /clefs.G/accidentals.flat\n8 /accidentals.natural.arrowdown/noteheads.s2\nti3\nme./noteheads.s2 /accidentals.flat/noteheads.s2\nsay\u0002/noteheads.s2\nYou3\n/noteheads.s2\nyou/noteheads.s2/accidentals.flat /noteheads.s2\nPlease,/noteheads.s2\nthe/rests.3/noteheads.s2\nlin',/noteheads.s2/rests.2\n3/rests.2\nme/noteheads.s2\nlove3\n/noteheads.s2\ndar-/accidentals.flat.arrowup /accidentals.flat /noteheads.s2/accidentals.natural\n13.2 13.3 13.4 14.1 14.2 14.3 14.4\n A2  C3    F3   A3  C4    F4\n3\n/noteheads.s2 /accidentals.natural /noteheads.s2\nmy/noteheads.s2\nwo-/noteheads.s2\nease/accidentals.natural.arrowdown /noteheads.s2/clefs.G\n8/accidentals.flat /accidentals.flat.arrowup/noteheads.s2\nrried/accidentals.natural.arrowup/noteheads.s2\nmind/accidentals.flat.arrowdown/noteheads.s2\nhear/accidentals.flat.arrowup /noteheads.s2\nyour/noteheads.s2 /rests.3/noteheads.s2\nWhen vo/accidentals.natural.arrowup/noteheads.s2\nI i[ce],/accidentals.flat.arrowup /rests.2/noteheads.s2 /rests.2\n3/noteheads.s2/noteheads.s2\nFigure 5. Muddy Waters, “Long distance call” (1951): F0gram showing continuous pitch contours of the voice, and ap-\nproximate musical transcription informed by the analysis of the F0gram.\nﬁrst stanza e\nxtends from measure 5 to 16. Figure 5 shows\ntheF0gramandthetranscriptionofthesixmeasureswhere\nMuddy Waters sings the lyrics: mm. 5-6, 9-10 and 13-14.\nEach of these 2-bar vocal phrases is followed by a 2-bar\ninstrumental response, omitted in the ﬁgure.The musical transcription offered here is informed by\nthe analysis of the F0gram,2and differs in many substan-\ntial details from published transcriptions [11], as well as\nfrom what was perceived by highly trained musicians that\n2Justaswithpitches,MuddyWaters’treatmentofdurationsisequally\nﬂexible.Thenotevalueschosenforthetranscriptionareapproximate,and\nthevertical alignment with theF0gram is notalways perfect.wereaskedtolistentotherecording.ObservingtheF0gram\nit is easy to see why: the melody consists mostly of\ncon-\ntinuously varying pitches, with few moments of stability\nother than the resolution on the tonic, and these exhibit\na wide terminal vibrato. In this context, the perception of\ndeﬁnitenotesrequiresadecisiononthepartofthelistener,\nthat is partly subjective. For example, the ﬁrst three notes\n(“You say you”) are normally perceived as F-A♭-F, but a\ncloser inspection reveal that the ﬁrst note is actually a fast\n“scoop” around a slightly high E♭, and the second a con-\ntinuous glide from below A♭ to around A♮. This behavior\nis consistent when the phrase is repeated (m. 9). A simi-\nlar treatment of the third as a blue note (i.e., as a ﬂexible\npitcharea)canbeobservedontheword“phone”onmm.6\nand 10. The previous words (“call me on the”), also move\nwithin a continuous pitch region, this time around the 4th\nand 5th degrees (B♭ and C). The long notes that begin the\nsecond half of each line (“plea-se” around E♭ on the ﬁrst\nand second line, and “ea-se” around Con thethird)exhib-\nit all the same arch-like melodic contour, with wider and\nfaster ascending and descending movements at the begin-\nning and the end of the note, and a slow curve during the\nsustain part. A particularly expressive effect results from\nthe fact that the C is hardly reached for an instant at the\npeak of the arch, the rest of the time the melody is kept\nmoving slowly around a somewhat ﬂat ﬁfth. The F0gram\nalso shows, through different shades in the grayscale, that\nthe dynamics of the phrase follow a similar arch-like con-\ntour. The most ambiguous moments in terms of pitch are\ntheendingsoftheﬁrstandsecondlines(“sometime”),with\nfastportamentosintothelowerregisterthatgivethesepas-\nsages a speech-like quality.\nInmanyWesternvocalpractices,continuousinﬂections\nof pitch are common when connecting the different —sta-\nble—notes of a melody (legato singing), as well as in the\nform of vibrato, ﬂuctuations of pitch around a central per-\nceivednote.Theapplicationoftheanalysistoolsproposed\nhere permits a clear visualization of two salient traits of\nthis passage: a melody consisting mostly of time varying\npitches with relatively few moments of stability, and the\nestablishmentofcontinuoustonalregionsnonreducibleto\nsingle pitches in a discrete scale.\n5. DISCUSSION AND FUTURE WORK\nBy means of the analysis of two music recordings, the\nusefulnessoftheintroducedtechniquesforcomputeraided\nmusicologywasillustrated,inparticularfordiscussingex-\npressive performance nuances related to pitch intonation.\nThe result of the analysis by itself reveals important as-\npects of the music at hand, difﬁcult to asses otherwise.\nThe computational techniques implemented are oriented\ntowards the precise representation of pitch ﬂuctuations.\nTwographicalsoftwaretoolsarereleasedwiththiswork\ntoallowtheapplicationofthedescribedmethodsbythere-\nsearch community.3One of them is a Vamp plugin4for\n3Available, as well as the audio clips, from http://iie.fing.\nedu.uy/investigacion/grupos/gpa/ismir2012\n4http://www.vamp-plugins.org/Sonic V\nisualiser that computes the pitch contours repre-\nsentation.Withinthisapplicationseveralotherfeaturesare\navailablethatcanassisttheanalysis.AMatlabR/circlecopyrtGUIisal-\nso released that includes additional functionalities and in-\nformation, better suited for signal-processing researchers.\nThe improvement of the pitch contours representation\nand its application to different music scenarios are the fol-\nlowing directions for future research.\n6. ACKNOWLEDGMENTS\nThis work was supported by the R+D Program of the\nComisi´onSectorialdeInvestigaci ´onCient´ıﬁca(CSIC),Uni-\nversidad de la Rep ´ublica, Uruguay.\n7. REFERENCES\n[1] I.BentandA.Pople. “Analysis”. GroveMusicOnline.\nAccessed June 16, 2012.\n[2] P. Cancela, E. L ´opez, and M. Rocamora. Fan chirp\ntransform for music representation. In 13th Int. Conf.\non Digital Audio Effects, Austria, Sep. 2010.\n[3] R.Cogan. Newimagesofmusicalsound.HarvardUni-\nversity Press. Cambridge, Massachusetts, 1985.\n[4] J. Durrieu, B. David, and G. Richard. A musically\nmotivatedmid-levelrepresentationforpitchestimation\nandmusicalaudiosourceseparation. SelectedTopicsin\nSignal Processing, IEEE Journal of, 5(6):1180 –1191,\nOct. 2011.\n[5] A. Klapuri. A method for visualizing the pitch content\nof polyphonic music signals. In 10th Int. Society for\nMusic Information Retrieval Conf., Japan, 2009.\n[6] A. Krishnaswamy. Pitch measurements versus percep-\ntion of south indian classical music. In Proc. of the\nStockholmMusicAcousticsConf.,Sweden,Aug.,2003.\n[7] D. Leech-Wilkinson. The Changing Sound of Mu-\nsic: Approaches to Studying Recorded Musical Perfor-\nmance. Published online, London: CHARM, 2009.\n[8] T. Licata, editor. Electroacoustic Music - Analytical\nPerspectives. Greenwood Press, 2002.\n[9] L. Litova-Nikolova. Bulgarian folk music. Bulgarian\nacademic monographs. Marin Drinov Academic Pub.\nHouse, 2004.\n[10] S.Petrov,M.Manolova,andD.Buchanan.“Bulgaria”.\nGrove Music Online. Accessed April 5, 2012.\n[11] F.SokolowandD.Rubin. MuddyWaters-DeepBlues.\nHal Leonard Corporation, 1995.\n[12] Musics & musicians of the world: Bulgaria. AU-\nVIDIS/UNESCO, 1983.\n[13] L. Weruaga and M. K ´epesi. The fan-chirp transform\nfornonstationaryharmonicsignals. SignalProcessing,\n87(6):1504–1522, 2007."
    },
    {
        "title": "A Survey on Music Listening and Management Behaviours.",
        "author": [
            "Mohsen Kamalzadeh",
            "Dominikus Baur",
            "Torsten Möller"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415742",
        "url": "https://doi.org/10.5281/zenodo.1415742",
        "ee": "https://zenodo.org/records/1415742/files/KamalzadehBM12.pdf",
        "abstract": "We report the results of a survey on music listening and management behaviours. The survey was conducted online with 222 participants with mostly technical backgrounds drawn from a college age population. The median size of offline music collections was found to be roughly 2540 songs (sum of physical media and digital files). The major findings of our survey show that elements such as famil- iarity of songs, how distracting they are, how much they match the listener’s mood, and the desire of changing the mood within one listening session, are all affected by the activity during which music is listened to. While people want to have options for manipulating the above elements to control their experience, they prefer a minimal amount of interaction in general. Current music players lack such flexibility in their controls. Finally, online recommender systems have not gained much popularity thus far.",
        "zenodo_id": 1415742,
        "dblp_key": "conf/ismir/KamalzadehBM12",
        "keywords": [
            "survey",
            "music listening",
            "behaviours",
            "online",
            "222 participants",
            "college age",
            "technical backgrounds",
            "median size",
            "offline music collections",
            "familiarity of songs"
        ],
        "content": "A SURVEY ON MUSIC LISTENING AND MANAGEMENT BEHA VIOURS\nMohsen Kamalzadeh\nSimon Fraser University\nmkamalza@sfu.caDominikus Baur\nUniversity of Calgary\ndominikus.baur@gmail.comTorsten M ¨oller\nSimon Fraser University\ntorsten@sfu.ca\nABSTRACT\nWe report the results of a survey on music listening and\nmanagement behaviours. The survey was conducted online\nwith 222 participants with mostly technical backgrounds\ndrawn from a college age population. The median size\nof ofﬂine music collections was found to be roughly 2540\nsongs (sum of physical media and digital ﬁles). The major\nﬁndings of our survey show that elements such as famil-\niarity of songs, how distracting they are, how much they\nmatch the listener’s mood, and the desire of changing the\nmood within one listening session, are all affected by the\nactivity during which music is listened to. While people\nwant to have options for manipulating the above elements\nto control their experience, they prefer a minimal amount\nof interaction in general. Current music players lack such\nﬂexibility in their controls. Finally, online recommender\nsystems have not gained much popularity thus far.\n1. MOTIV ATION\nSince the advent of mp3 ﬁles and the fast spread of high\nbandwidth Internet connectivity, there has been an extreme\nincrease in the number of songs listeners can have im-\nmediate access to. In the past decade, the size of per-\nsonal digital music libraries has seen a similar fast growth.\nMoreover, subscription based on-demand streaming ser-\nvices like Spotify have made millions of songs readily avail-\nable to their users. Many studies exist on music listening\nand management behaviours [2,4,10,11], but with the im-\nmense speed at which technology advances, new questions\nfrequently arise on how listeners interact with the immense\namount of music available to them.\nAs Downie et al. [6] point out, one of the main chal-\nlenges that ISMIR currently faces is encouraging the par-\nticipation of potential users of Music Information Retrieval\n(MIR) systems. In this study we investigate the issues such\nusers have in their day-to-day interaction with music. We\nﬁrst divide music interaction into two main categories: (a)\nmusic listening, and (b) management of music collections.\nMusic listening comprises the process of deciding what\nto listen to in a music listening session and the kind of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.control exerted by the user on the played music. Man-\nagement includes obtaining music, managing tags, creating\nand maintaining playlists, sorting, and so on. Music listen-\ning can be both a personal or a social experience. Here, we\nconcentrate on personal music listening. Methods of play-\nback range from low control methods like shufﬂing one’s\nwhole collection along with skipping songs, to higher con-\ntrol ones like having pre-compiled playlists for various oc-\ncasions or even choosing songs one after another.\nAlthough the amount of user studies on music informa-\ntion retrieval and browsing has been growing as of late,\nthere is a lack of studies when it comes to understanding\nwhat factors inﬂuence a user’s music listening choices in\nvarious contexts, what methods of playback are used and\nwhy, and what devices and services are more frequently\nused. Previous studies have focused mainly on users’ in-\nformation seeking behaviours [1,7,9], discovering new mu-\nsic [3], digital music library management [4, 11], use of\nphysical or digital media [2], playlist generation behaviours\n[10], music listening contexts [2, 5, 8, 10], reasons for lis-\ntening [5,8], and social aspects of music consumption [10].\nIn this study, we focus on the act of music listening\nby investigating our participants’ listening behaviours, and\ntrying to understand inﬂuential factors in their choice of\nplayback method, and the amount of control and interac-\ntion they desire. We compare some of our results regard-\ning playback methods and playlist creation to what Vig-\nnoli [11] and Stumpf and Muscroft [10] found. We also\nstudy our participants’ use of music recommendation ser-\nvices like Grooveshark, iTunes Genius, and Last.fm. Fi-\nnally, we discuss some implications for the design of future\nmusic listening tools.\n2. RELATED WORK\nThere is a close relation between searching (or browsing)\nand managing libraries, in the sense that the most frequently\nused cues and properties in searching and browsing can be\na good basis for organizing a personal library. This is to\nsome extent conﬁrmed by similar observations by Bain-\nbridge et al. [1], Lee and Downey [9], and Vignoli [11]. In\nthe ﬁrst two studies, which focus on ﬁnding music or mu-\nsic information, the most used properties are reported to\nbe “performer” and “song title”. Vignoli [11] asks partici-\npants about the attributes they mostly use when retrieving\nsongs from their personal libraries, and again artist name\nand song title come out on top.\nThe study by Vignoli [11] is one of the very few that\ndiscusses issues relating to the acts of music listening aswell as collection management. Vignoli asks participants\nhow often they use various playback methods and reports\nthat the most favorite method is “I choose one or more al-\nbums”, while “I search for a single song” ranks second.\nIt is notable that these two are both highly controlled ex-\nperiences, compared to choosing an artist or shufﬂing the\nwhole collection. Also, users liked to create playlists as\nopposed to using existing ones, which is also indicative of\nthe higher level of control desired.\nRegarding how these playlists are created, a recent pa-\nper by Stumpf and Muscroft [10] reports that the concepts\nmost frequently mentioned by participants in a think-aloud\nplaylist creation task were tempo and mood. However, the\nstudy had only 7 participants, so a generalization is difﬁ-\ncult. In this paper, among other things, we also discuss\nour own results regarding these attributes and playback\nmethods for listening to music during various activities,\nand look for similarities with what Vignoli, Stumpf, and\nMuscroft observed.\n3. METHODS\nOur online survey included a total of 32 questions covering\nboth collection management (13 questions), music listen-\ning (14 questions) and demographics (5 questions). The\nLikert scale is the most used question format throughout\nthe survey. Medians and modes are used for reporting the\nresults of Likert scales, as opposed to averages.\nThe survey population consisted mostly of Simon Fraser\nUniversity’s (Canada) Computing Science and Engineer-\ning faculty and students who were invited to take part in the\nsurvey with mailing lists. We also initiated snowball sam-\npling by encouraging the respondents to spread the word to\ntheir friends and family. The questionnaire went through\nseveral revisions and was pilot tested with a total of 10\nrespondents before being sent out to participants. The re-\nsults presented in this paper were gathered in two stages.\nThe ﬁrst stage, which targeted only Computing Science\ngraduate students and faculty, had 79 participants. After\nthe ﬁrst stage, we analyzed the results for questions with\nlow response rates and revised three of them for the second\nstage, which had 143 participants (Computing Science un-\ndergraduates and Engineering students). We present aggre-\ngate results for all of the 222 participants in case of identi-\ncal questions, and stage 2 results for revised questions.\nOur participants had an average age of 25.85 with a\nstandard deviation of 9.02, and a median of 23. The ma-\njority of our participants (73%) were males, and 95% had\na Computing Science or Engineering background.\n4. RESULTS\nIn this section, we present an overview of the results for\nmajor questions, in both listening and management cat-\negories. Naturally, not all participants answered all the\nquestions, so for each question, only the participants that\nhave answered it are included in computing averages, me-\ndians, etc. Whenever we look at results of two or more\nquestions together, we only include the subset of partici-\npants who have answered all of them.4.1 Music Listening\nAverage hours of music listening per day (both active\nand passive): During active listening, one is listening to\nmusic for the sake of listening, not doing other activities.\nPassive listening happens when music is listened to dur-\ning other activities to get in and out of moods, to cancel\nout ambient noise, to go through boring activities, and so\non [5]. Average hours (per day) of active and passive lis-\ntening were asked in form of ranges. For active listening,\nthese ranges were: Less than 1 hour, 1-2, 2-4, 4-8, and\nmore than 8 hours. In case of passive listening, since it\ngenerally happens more than active listening, the choices\nwere changed to cover a larger range: less than 1, 1-2, 2-4,\n4-6, 6-8, 8-10, 10-12, and more than 12 hours. For the 174\nparticipants that answered these questions, the median and\nmode choice for number of active listening hours per day\nwas “less than 1 hour”. Both median and mode jump to\n“2-4 hours” in case of passive listening.\n48%\n28%\n10%\n4%\n6%\n29%\n24%\n23%\n41%\n23%\n12%\n13%\n8%\n16%\n36%\n23%\n0%\n20%\n40%\n60%\n80%\n100%\nrank 1\nrank 2\nrank 3\nrank 4\npercentage of respondents\ncommuting\nexercising\nwork\nhousework\nFigure 1. Activities ranked based on their portions of a\nparticipant’s overall passive listening hours (based on 178\nresponses).\nTo understand during what activities passive listening\nhappens, we asked participants to rank 4 activities: com-\nmuting, exercising, work, and housework. These are also\nthe top activities reported by Lamont and Webb in [8], ex-\ncept for exercising. This is because we considered exercis-\ning as an activity which is reliant on playlists more than the\nother 3 and could thus broaden our scope when we later ask\nabout playback methods during these activities. As seen in\n0%20%40%60%80%\n100%\nactive\nlisteningcommuting exercising work houseworkradio\nonline\nrecommendation\nshuffle on\ncollection\nplaylist or folder\nartist, album, or\ngenre\nsong after song\n50%\n6% 4% 5% 3%24%\n27%\n18%25%\n18%10%\n25%43% 28%\n18%6%\n22%12%\n13%\n22%9%\n4%6%\n14%11%\n3%3%\nFigure 2. Preferred methods of playback for various si-\nmultaneous activities (passive listening) and active listen-\ning.prefer familiar songs\nprefer new songs\ndon't care if familiar or\nnot\ndon't listen during such\nactivities\nnon-attention attention0% 20% 40% 60%\n64%\n45%\n1%\n17%\n18%\n39%\n19%\n1%(a) Familiarity of songs\nchoose each song\ncarefully\nit's good enough if it\nmatches my mood\nmood is irrelevant, it's\ngood enough if it doesn't\ndistract\nany music is good0% 20% 40% 60%\nnon-attention attention27%\n46%\n30%\n38%\n36%\n7%\n5%\n9% (b) Importance of individual songs / pickiness of\nthe listener\nconstant mood\nvarious moods\nattention\nnon-attention0% 20% 40% 60%\n66%\n34%\n37%\n70%(c) Constant vs. varying mood\nFigure 3. Results for questions regarding familiarity, importance, and mood variance of songs.\nFigure 1, commuting and work take similarly large chunks\nof the ﬁrst rank, with exercise and housework following\non ranks two and three. We also provided a comment sec-\ntion for the corresponding question to be able to pinpoint\nother important activities that we might have missed. Surf-\ning the Internet, playing video games, and during/before\nsleep were the three activities most often mentioned (13,\n10, and 6 times, respectively). Brown et al. [2] found out\nthat the most popular places for listening to music were the\ncar (82% of the time), the living room (61%), and work\n(38%). Our results show a shift towards work. The reason\ncan be both our population’s age and technical background\nand the fact that with the rapid growth of technology since\n2001, nowadays much of people’s work happens on their\ncomputer which also contains a large collection of music.\nOne distinction between the different activities comes from\nthe amount of attention they need and the amount of con-\ntrol on the music a listener would want. These factors in-\nﬂuence the chosen playback methods.\nPreferred playback methods for each activity: We\nstudy the methods of playback our participants preferred\nfor the same list of activities as before, namely commut-\ning, exercising, work, and housework, along with active\nlistening. For each activity, the respondent was asked to\nchoose one of 6 playback methods. In Figure 2 we see the\npercentages for each activity and method out of all 169 par-\nticipants who answered this question. Respondents were\ntold not to choose any method if an activity didn’t apply to\nthem, therefore the sum of the columns isn’t always 100%.\nChoosing song after song dominates the active listen-\ning portion and this is not surprising. To ﬁgure out which\nmethod is generally preferred for passive listening, we ex-\nclude active listening results, sum the total number of times\neach method was chosen and divide that by the number\nof all the choices made by all participants. We observe\nthat the overall preferred method is “a prepared playlist or\nfolder of songs” with a 29% share. “Picking an artist, al-\nbum, or genre” and “a shufﬂe on your whole collection”\nare second and third with 22% and 19% shares. “Radio,\nincluding online stations”, “song after song”, and “online\nrecommendation services” end up with quite small shares\nof 8%, 5%, and 4%, respectively. For the same reason as\nabove, the sum of these percentages isn’t necessarily 100.\nIn comparison, Vignoli [11] observed that “I choose oneor more albums” was the top choice, which is in line with\nwhat we see here: an overall preference for higher control.\n“I search for a single song” is second there, which could\nbe because Vignoli does not classify methods based on ac-\ntivities, resulting in active listening skewing the results.\nImportance, familiarity, and mood of songs, and in-\nteraction tolerance: As mentioned earlier, a distinction\nbetween activities during which music is listened to can\nbe the amount of attention the activities need. We believe\nthat work and commuting (if it is not driving) can lie on\ntwo opposing ends of this spectrum, with work needing\nvery high attention from the listener and commuting need-\ning much less. As both these activities contribute heavily\nin our participants’ listening hours, it is crucial to have a\nbetter understanding of listening behaviours during each.\nWe hypothesized that having to pay (or not) pay attention\nto the activity will affect the following 4 aspects:\n(a) How familiar the songs are.\n(b) How picky the listener usually is (we call this impor-\ntance of songs)\n(c) If a constant mood is preferred or if there need to be\nvarious moods (in one session of listening).\n(d) What the maximum amount of desired interaction is.\nOur questionnaire contained a question on each of the\nabove for both activities that need attention (we will call\nthese “attention activities”) and those that do not (we will\ncall these “non-attention activities”).\nFigure 3(a) shows that familiar songs are generally pre-\nferred for both attention and non-attention activities, and\nin case of attention activities, participants strongly pre-\nferred familiar songs with nearly 0% preferring new ones.\nFigure 3(b) shows that while it is important that the music\nduring attention activities does not distract the listener, a\nlarge fraction of participants expressed a need for matching\nmoods and choosing each song carefully, even during at-\ntention activities. This is expected for non-attention activ-\nities, but is somewhat surprising for attention ones, and in-\ndicates a general preference for high control on the music.\nThis is in agreement with our results for playback methods\ndiscussed earlier. Figure 3(c) shows that although constant\nmood was the dominant choice for attention activities, still\nnearly 40% preferred various moods.\nTo target issue d (maximum amount of desired interac-\ntion), we asked participants what their maximum amount0%\n20%\n40%\n60%attention non-attention\nvery low\nlow\nmedium\nhigh43%\n17%\n28%\n11%\n21%\n36%\n13%\n41%Figure 4. Results from questions on amount of interaction\nwith music player (based on 82 responses).\nof desired interaction would be if they wanted to change\nthe mood. One metric for “amount” of interaction can\nbe the amount of time it takes for the user to perform it.\nChoices included examples that gave our respondents an\nidea of this time. These were: (a) “very low interaction”:\ne.g. skipping tracks; (b) “low”: e.g. specifying your de-\nsired change in mood but not having to ﬁnd any particular\nsong; (c) “medium”: e.g. switching to another playlist; (d)\n“high”: e.g. ﬁnding speciﬁc songs one after another.\nThe question was more complex in the ﬁrst stage. Due\nto high non-response, it was changed to the one described.\nThe results discussed here are from the second stage.\nFor attention activities (see Figure 4), although a pref-\nerence for lower interaction is expected, it is interesting to\nsee that along with “very low interaction”, “low interac-\ntion” was also acceptable by a large margin. During non-\nattention activities, participants preferred to have higher\ncontrol on the music source with the medium and high\nchoices dominating the scene.\nUse of online music services: It is clear from the above\nresults that online recommendation services are not popu-\nlar at all even among our survey population which consists\nmostly of college-age people with technical backgrounds.\nIndeed, when asked about what music services they have\never used, 33.8% (75 out of 222) said they haven’t ever\nused any of the provided choices (Last.fm, iTunes Genius,\nGrooveshark, Zune Smart DJ, Pandora, Spotify, iLike, and\nMusicovery) and didn’t provide any other service in the\n“other” comment section. For the remaining 147 partici-\npants, Grooveshark, iTunes Genius, and Last.fm were the\nmost prevalent choices with 46%, 45%, and 42%. 23% had\nused Pandora, and 7% Spotify. YouTube was the most pop-\nular “other” choice with 6 participants (3%). When asked\nabout their favourite service, 61% of participants who an-\nswered the question said they didn’t normally use these\nservices. The rest of the responses reﬂect what we have\nabove, with Grooveshark, iTunes Genius, and Last.fm be-\ning the top three. This result, however, seems to be more\ndependent on the popularity of these services than con-\nscious choice as we had only a small number of partici-\npants that had tried all or almost all services. For an in-\ndepth analysis more than our 222 participants are needed.\n4.2 Management of Music Collections\nPrimary sources of music (CD, mp3 player, radio, etc.):\nParticipants were asked to choose between 5 frequency ad-\nphyical media like CDs,\nLPs, cassettes, etc.offline collection on portable\ndevices like smartphones,\nmp3 players, etc.\noffline collection on\ncomputers\nradio including FM/AM,\nsatellite radio, online\nstations, etc.\ninteractive online services\nlike Grooveshark, Pandora,\nLast.fm, etc.\n135135135135135 92\n28\n21\n11\n10\n71\n45\n30\n12\n4\n13\n32\n35\n51\n31\n22\n22\n19\n48\n51\n5\n10\n35\n54\n58Figure 5. Popularity of music sources. Y axis: 1=Never;\n2=Rarely; 3=Sometimes; 4=Often; 5=Very Often. X axis:\nnumber of participants who chose each option with the\ndarker bars being the median choice.\nverbs in a Likert scale. In Figure 5, the number of partic-\nipants who chose each choice for each source is shown,\nwith the darker bars being the median for each source.\nWhile ofﬂine collections on portable devices score the high-\nest, we need to keep in mind that most of our participants\nhave a technical background.\nCollection statistics: Our participants had a median of\n15 pieces of physical media and 2000 digital songs1. Most\nparticipants (65%) said they were likely to correct inaccu-\nrate tags that they ﬁnd in their collections. The median\nrespondent maintained between 2 to 4 playlists.\nHandling of digital collections (manually with a folder\nstructure, or using an application like iTunes): Almost\nhalf of the respondents (83 out of 155: 53%) preferred to\nmanually manage their music folders rather than relying on\nan application. Applications ended up second with 26%,\nand 21% said they used both. It appears that management\nusing applications has gained much more traction since Vi-\ngnoli’s study [11] in 2004, which reports that all the 7 par-\nticipants used manual folders. Participants were also asked\nabout what application they preferred for managing their\nlibraries, with the choices offered being Windows Media\nPlayer, iTunes and “other”. In Figure 6 we see the choices\nmade by the 144 participants who answered the question,\nalong with the difﬁculties expressed with each of these ap-\nplications.\nImportant factors for managing music collections (al-\nbum, artist, genre, etc.): Participants were asked to spec-\nify how important various factors were for them in man-\naging their collections by choosing between “Very Impor-\ntant”, “Important”, “Somewhat Important”, and “Not Im-\nportant” for each factor. Artist, with a median of “Very\nImportant” was the top choice here. This conﬁrms the\nﬁndings reported by Vignoli [11] and Bainbridge et al. [1].\nSecond were album and genre with a median of “Impor-\n1For digital collections, participants had a choice of providing number\nof songs or gigabytes. In cases were gigabytes were provided, they were\nconverted to number of songs, assuming 4 megabytes per song.30%\n10%50%\n40%\n20%\nlack of support in\nremembering old\nfavouriteslack of support in\ndiscovering new\nmusiclack of support in\ngetting an\noverview of one's\ncollectionlack of proper\nsearch toolslack of support in\nplaylist creation\nor managementlack of support\nfor social\nnetworkinglack of support\nfor purchasing\nmusicnumber of respondentsMedia \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes Media \nPlayeriTunes other19%\n11%\n6%50%\n40%47%\n30%\n21%19%25%37%\n2%32%\n17%3562\n47Figure 6. Applications used for library management and difﬁculties faced with them. Each column shows the perceived\nlack of support for a task among each application’s users. The right-most column shows the number of respondents who\nchose each application.\ntant” and a mode of “Very Important”. These were also\namong the top factors in both the above studies, along with\nsong title.\nImportant factors in creating playlists: We also asked\nparticipants who said they created and managed playlists,\nabout important factors in doing so. Due to high non-\nresponse, this question was altered for stage 2 and only\nstage 2 results are reported in this paper. The format was a\nthree-choice Likert scale for ”importance” of factors. This\nand the question regarding factors important in managing\ncollections were located far apart from each other in the\nquestionnaire. We also altered the choices to not be sim-\nilar to those offered in the management question, so as to\nprevent participants from recalling their management an-\nswers. The choices for each factor were one of “Not Im-\nportant”, “Somewhat Important”, and “Very Important”.\nMood came out on top with both a median and mode of\n“Very Important”. Genre, artist, and tempo all had a me-\ndian of “Important” and a mode of “Very Important”. In\ncase of mood, our results conﬁrm Stumpf and Muscroft’s\nﬁndings [10], but not for tempo. They reported that tempo\nwas actually the most important factor for their partici-\npants, with mood and rhythmic quality being 2ndand 3rd.\nHere, tempo is only 4th.\nTo summarize all the results for important factors in\nmanagement and playlist creation, we scale all of them to a\nrange between 1 and 4 were 4 is the highest score possible\nfor each factor. The results are shown in Table 1.\nplaylist creation management\nmood 3.42 (1st) 2.39\ngenre 3.17 (2nd) 2.52 (3rd)\nartist 2.95 (3rd) 3.54 (1st)\ntempo 2.80 1.90\nalbum 2.31 2.73 (2nd)\ninstruments 2.25 1.78\nTable 1. Factors important in playlist creation and man-\nagement of music collections. Scores are out of 4.5. DISCUSSION\nUnderstanding our participants’ listening behaviours starts\nfrom knowing when they listen to music. Commuting and\nwork were the most popular activities making up our par-\nticipants’ listening hours. A good music listening tool has\nto cater to at least the most prominent activities by support-\ning the playback methods that best ﬁt them.\nIn Figure 2, we can see that more controlled methods\nlike a playlist or choosing certain artists, albums, or genres\nare generally more popular than less controlled methods\nlike shufﬂe, radio, and online recommendation services,\nand this is more pronounced for ”work” and ”exercising”.\nThis is expected, because exercising requires very speciﬁc\ntempo and rhythm, and work generally needs high atten-\ntion, so with a shufﬂe on one’s whole collection, the songs\nare unlikely to satisfy the needed degrees of familiarity,\nmood, and not being distracting.\nBut the question is: Are these conventional playback\nmethods enough? According to Figure 3(c), nearly 40% of\nour participants expressed a desire to have various moods\neven during attention activities. To achieve this, listeners\nhave to resort to switching playlists (assuming they even\nhave prepared ones), applying various ﬁlters of artist, al-\nbum, genre, etc. while listening, or even creating a playlist\nevery time, not to mention choosing songs one after an-\nother. According to Figure 4, all of these require amounts\nof interaction more than what a person would normally\nwant to have with the music source during attention activ-\nities. It is interesting to note that 71% of our participants\nwere OK with very low or low interaction. Having in mind\nthat very low interaction is essentially a shufﬂe on one’s\ncollection and that shufﬂe is not appropriate for many ac-\ntivities, we see a need for novel interaction methods in the\n”low interaction” range.\nWe set out to understand what the users would want to\nhave control over, in a Utopian music player. We hypothe-\nsized that the familiarity of the songs, their mood and how\ndistracting they are, and if they should have similar moods\nor not, are among the elements that are affected by the kind\nof activity during which music is listened to. Judging by\nFigure 3 and Figure 4 we claim that our hypothesis was\nconﬁrmed with all the results showing notable differences\nbetween attention and non-attention activities.One could say that online recommendation services like\nLast.fm, Pandora, and Grooveshark support low interac-\ntion while also introducing the listener to new music. Our\nresults show that these services have not really gained trac-\ntion with users. Several reasons can be speculated for that,\nlike price, sub-par interfaces, availability (different coun-\ntries), accessibility (computer only or mobile too?). For\ninstance, accessibility can be the reason why online ser-\nvices are used mostly during ”work”, which is in case of\nour survey population, mostly done on the computer while\nonline. But there is also the quality of recommendations.\nRight now, all the noteworthy online recommendation ser-\nvices operate on the basis of similarity to a seed song or a\nuser’s library of favourite songs. The maximum control the\nlistener has is skipping songs or in some services, inserting\nsongs into the playlist (e.g. Grooveshark). The listener can\nhave no control over what aspects of the songs are taken\ninto account for computing the “similarity”. It is evident\nfrom our results that, contrary to the idea behind playlists\nwhich are pre-compiled lists for various occasions, there\nseems to be an inherent impulsiveness in the choice of mu-\nsic. That is, at any point during a session of listening, the\nlistener might want to steer the experience to a new di-\nrection. The elements mentioned above are only some of\nthe aspects that should be controllable in this “steering”\nact. Spotify apps like Moodagent or EchoNest’s steerable\nplaylist API are promising developments in this regard.\n6. CONCLUSION\nThe results of studying our 222 participants’ music man-\nagement and listening behaviours were reported and anal-\nysed. We discussed how our participants manage their mu-\nsic collections, during what activities they listen to music,\nhow many hours a day they listen to music, if and how\nthey manage playlists, what methods of playback they pre-\nfer, what their primary sources of music are, and if they\nuse online recommendation services.\nThe most important attributes of songs for collection\nmanagement were artist, album, and genre, which is in\nagreement with the ﬁndings by Vignoli [11] and Bainbridge\net al. [1]. We found that mood, genre, and artist were\nmost important for creating playlists, which partly con-\nﬁrms what Stumpf and Muscroft [10] found with 7 par-\nticipants. They reported that tempo was actually the top\nchoice, with mood and rhythmic quality being 2ndand\nthird, and genre 5th. The median size of personal music\ncollections was found to be 2540 songs. Participants lis-\ntened to these collections on portable devices and comput-\ners more than any other source. This was mostly during\ncommuting and work. Only half of the respondents said\nthey only used manual folder structures for managing their\ncollections rather than applications such as iTunes. This\nis in contrast with what Vignoli [11] reports from 2004,\nwhere all the respondents only used manual folders. The\nvery limited popularity of online music services was sur-\nprising to us, considering our population’s mostly technical\nbackgrounds and young ages.\nOverall, for passive listening (listening to music duringother activities), more controlled playback methods like\nprepared playlists and ﬁlters of album, artist, etc. were\nmore popular than shufﬂing. We discussed these in rela-\ntion to elements such as familiarity of songs, how distract-\ning they are, how much they match the listener’s mood, and\nif various moods are desired in a session of listening or not,\nand concluded that there’s a need for novel interfaces with\neasy and efﬁcient support for manipulating these elements\ndynamically and with a low amount of required interaction.\nWe would like to note that one issue with our current\nresults is the heavy focus on participants with technical\nbackground. To have more reliable results, we are cur-\nrently extending the survey to other population groups.\n7. REFERENCES\n[1] D. Bainbridge, S. J. Cunningham, and J. S. Downie:\n“How People Describe Their Music Information\nNeeds: A Grounded Theory Analysis of Music\nQueries,” Proc. 4thISMIR, pp. 221–222, 2003.\n[2] B. A. T. Brown, E. Geelhoed, and A. J. Sellen: “The\nUse of Conventional and New Music Media: Implica-\ntions for Future Technologies,” Proc. 8thINTERACT,\npp. 67–75, 2001.\n[3] S. J. Cunningham, D. Bainbridge, and D. McKay:\n“Finding New Music: A Diary Study of Everyday En-\ncounters with Novel Songs,” Proc. 8thISMIR, pp. 83–\n88, 2007.\n[4] S. J. Cunningham, M. Jones, and S. Jones: “Organiz-\ning Digital Music for Use: An Examination of Per-\nsonal Music Collections,” Proc. 5thISMIR, pp. 447–\n454, 2004.\n[5] T. DeNora: Music in Everyday Life, Cambridge Uni-\nversity Press, 2000.\n[6] J. S. Downie, D. Byrd, and T. Crawford: “Ten Years of\nISMIR: Reections on Challenges and Opportunities,”\nProc. 10thISMIR, pp. 13–18, 2009.\n[7] J. S. Downie and S. J. Cunningham: “Toward a Theory\nof Music Information Retrieval Queries: System De-\nsign Implications,” Proc. 3rdISMIR, pp. 13–17, 2002.\n[8] A. Lamont and R. Webb: “Short-and long-term musi-\ncal preferences: what makes a favourite piece of mu-\nsic?,” Psychology of Music, vol. 38, no. 2, pp. 222–241,\n2009.\n[9] J. H. Lee and J. S. Downie: “Survey of Music Infor-\nmation Needs, Uses, and Seeking Behaviours: Prelim-\ninary Findings,” Proc. 5thISMIR, pp. 441–446, 2004.\n[10] S. Stumpf and S. Muscroft: “When Users Generate\nMusic Playlists: When Words Leave Off, Music Be-\ngins?,” Proc. ICME 2011, pp. 1–6, 2011.\n[11] F. Vignoli: “Digital Music Interaction Concepts: A\nUser Study,” Proc. 5thISMIR, pp. 415–420, 2004."
    },
    {
        "title": "Context-free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms.",
        "author": [
            "Hirokazu Kameoka",
            "Kazuki Ochiai",
            "Masahiro Nakano",
            "Masato Tsuchiya",
            "Shigeki Sagayama"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415252",
        "url": "https://doi.org/10.5281/zenodo.1415252",
        "ee": "https://zenodo.org/records/1415252/files/KameokaONTS12.pdf",
        "abstract": "This paper proposes a Bayesian model for automatic mu- sic transcription. Automatic music transcription involves several subproblems that are interdependent of each other: multiple fundamental frequency estimation, onset detec- tion, and rhythm/tempo recognition. In general, simultane- ous estimation is preferable when several estimation prob- lems have chicken-and-egg relationships. This paper pro- poses modeling the generative process of an entire music spectrogram by combining the sub-process by which a mu- sically natural tempo curve is generated, the sub-process by which a set of note onset positions is generated based on a 2-dimensional tree structure representation of music, and the sub-process by which a music spectrogram is gen- erated according to the tempo curve and the note onset po- sitions. Most conventional approaches to music transcrip- tion perform note extraction prior to structure analysis, but accurate note extraction has been a difficult task. By con- trast, thanks to the combined generative model, the present method performs note extraction and structure estimation simultaneously and thus the optimal solution is obtained within a unified framework. We show some of the tran- scription results obtained with the present method.",
        "zenodo_id": 1415252,
        "dblp_key": "conf/ismir/KameokaONTS12",
        "keywords": [
            "Bayesian model",
            "automatic music transcription",
            "subproblems",
            "interdependent",
            "chicken-and-egg relationships",
            "generative process",
            "music spectrogram",
            "musically natural tempo curve",
            "note onset positions",
            "note extraction and structure estimation"
        ],
        "content": "CONTEXT-FREE 2D TREE STRUCTURE MODEL OF MUSICAL NOTES\nFOR BA\nYESIAN MODELING OF POLYPHONIC SPECTROGRAMS\nHirokazu Kameoka1,2, Kazuki Ochiai1, Masahiro Nakano2, Masato Tsuchiya1, Shigeki Sagayama1\n1Graduate School of Information Science and Technology, The University of Tokyo\nHongo 7-3-1, Bunkyo, Tokyo, 113-8656, Japan\n2NTT Communication Science Laboratories, NTT Corporation\nMorinosato Wakamiya 3-1, Atsugi, Kanagawa, 243-0198, Japan\nABSTRACT\nThis paper proposes a Bayesian model for automatic mu-\nsic transcription. Automatic music transcription involves\nseveral subproblems that are interdependent of each other:\nmultiple fundamental frequency estimation, onset detec-\ntion, and rhythm/tempo recognition. In general, simultane-\nous estimation is preferable when several estimation prob-\nlems have chicken-and-egg relationships. This paper pro-\nposes modeling the generative process of an entire music\nspectrogram by combining the sub-process by which a mu-\nsically natural tempo curve is generated, the sub-process\nby which a set of note onset positions is generated based\non a 2-dimensional tree structure representation of music,\nand the sub-process by which a music spectrogram is gen-\nerated according to the tempo curve and the note onset po-\nsitions. Most conventional approaches to music transcrip-\ntion perform note extraction prior to structure analysis, but\naccurate note extraction has been a difﬁcult task. By con-\ntrast, thanks to the combined generative model, the present\nmethod performs note extraction and structure estimation\nsimultaneously and thus the optimal solution is obtained\nwithin a uniﬁed framework. We show some of the tran-\nscription results obtained with the present method.\n1. INTRODUCTION\nMusic transcription is the process of automatically convert-\ning a given audio signal into a musical score. Although\nthere are a number of viable ways of transcribing mono-\nphonic music, polyphonic music still poses a formidable\nchallenge.\nSeveral subproblems must be solved if we are to tran-\nscribe polyphonic music automatically, namely source sep-\naration, multiple fundamental frequency estimation (the es-\ntimation of the fundamental frequencies of concurrent mu-\nsical sounds), onset detection (the detection of the position\nin the signal where each note begins), and rhythm recog-\nnition (the estimation of the tempo, beat locations, and the\nnote value of each note). The difﬁculty is that these sub-\nproblems involve many ambiguities.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2012 International Society for Music Information Retrieval.An audio signal of a musical note typically consists of\nmany overtones, some of which usually overlap when mul-\ntiple notes are played simultaneously. To detect which\nnotes are present at a certain time instant, we need to know\nwhich musical note each frequency component belongs to.\nSince this information is missing for the spectrum of a mix-\nture signal, there can be multiple interpretations of how the\nspectrum of each sound should appear as well as which\npitches are present in the mixture. On the other hand, a\nmusic performance often involves temporal ﬂuctuation in\nterms of both rhythm and tempo, which means performers\ndo not always play notes with a perfectly timed rhythm and\nconstant tempo. Since we cannot deﬁne a note value with-\nout having a notion for tempo and vice versa, there can be\ninﬁnite interpretations regarding what the intended rhythm\nwas and how the tempo varied if both types of information\nare missing.\nMany methods have already been developed for poly-\nphonic music transcription, most of which try to tackle the\nproblem by dealing with the abovementioned subproblems\nseparately [1]. However, the inherent difﬁculty of the mu-\nsic transcription problem lies in the chicken-and-egg inter-\ndependency between these subproblems [2]. Firstly, if the\ngiven signal is already decomposed into individual notes, it\nis a simple matter to detect their fundamental frequencies.\nOn the other hand, the decomposition of a given spectro-\ngram into individual notes can be accomplished more accu-\nrately when the fundamental frequencies of the concurrent\nsounds are given. Also, if we know the fundamental fre-\nquencies of all the underlying notes in the signal, they can\nconstitute very useful information for accurately estimat-\ning their onset times and vice versa. Furthermore, as the\nonset times of notes are usually governed by the rhythmic\nstructure of a piece of music, the “chicken and egg” situ-\nation also applies to the detection of note onsets and the\ndetermination of beat locations and tempo. If we know the\nbeat locations of a piece of music, then it is much easier\nto detect the onset times of notes and vice versa, since the\ninter-onset times are likely to be multiples or fractions of\nthe beat period.\nSimultaneous estimation is generally preferable when\nseveral estimation problems are interdependent. Thus, we\nconsider it necessary to introduce a uniﬁed model, which\ncould be used to jointly solve the problems of determining\nthe pitch and onset time of each musical note, the rhythm\nand the overall tempo variation of a piece of music. In this\npaper, we take a Bayesian approach (a generative modelapproach) as in [3–6] to formulate and solve this simulta-\nneous estimation problem.\n2.\nGENERATIVE MODEL OF SPECTROGRAM\n2.1 Overview\nMotivated by the above, this paper proposes modeling the\ngenerative process of an entire spectrogram of a piece of\nmusic by formulating the following three sub-processes\nand combining them into one process: (1) the sub-process\nby which the tempo curve of a piece of music is gener-\nated, (2) the sub-process by which a set of note onset po-\nsitions (in terms of the relative time) is generated based\non a 2-dimensional tree structure representation of music,\nand (3) the sub-process by which a music spectrogram is\ngenerated according to the tempo curve generated by sub-\nprocess 1 and the set of note onset positions generated by\nsub-process 2. In the following, we model sub-process 1 in\n2.2, sub-process 2 in 2.3 and sub-process 3 in 2.4, respec-\ntively. Our aim is to use this complete generative model to\nexplain how a given spectrogram is generated. The most\nlikely model parameters given the observation would then\ngive a musically likely interpretation of what is actually\nhappening in the spectrogram ( i.e., a musical score). To\nthis end, we employ a Bayesian approach to infer the pos-\nterior distributions of all the latent parameters. An approx-\nimate posterior inference algorithm is derived, which is de-\nscribed in Section 3.\n2.2 Sub-process for generating tempo curve\nThe tempo of a piece of music is not always constant and in\nmost cases it varies gradually over time. If we use a1“tick”\nas a relative time notion, an instantaneous (or local) tempo\nmay be deﬁned as the length of 1 tick in seconds. Now\nlet us use µdto denote the real duration (in units of sec-\nonds) corresponding to the interval between dandd+ 1\nticks. Thus, µdcorresponds to the local tempo and so the\nsequence µ1, . . . , µ Dcan be regarded as the overall tempo\ncurve of a piece of music. One reasonable way to ensure\na smooth overall change in tempo is to place a Markov-\nchain prior distribution over the sequence µ1, . . . , µ Dthat\nis likely to generate a sequence µ1, . . . , µ Dsuch that µ1≃\nµ2, µ2≃µ3, . . . , µ D−1≃µD. Here, we assume a Gaussian-\nchain prior for convenience:\nµd|µd−1∼ N (µd;µd−1,(σµ)2) (d = 2, . . . , D ),(1)\nwhere N(x;µ, σ)∝e−(x−µ)2\n2σ2. If we use ψdto denote\nthe absolute time (in units of seconds) corresponding to\ndticks, ψdcan\nthus be written as ψd=ψd−1+µd−1,\nwhich plays the role of mapping a relative time in units\nof ticks (integer) to an absolute time in units of seconds\n(continuous value).\n2.3 Sub-process for generating note onset positions\nHere we describe the generative model of the set of some\nnumber Rof note onset positions S1, . . . , S R(in units of\nticks). Most people would probably agree that music has\n1Tick is a relative measure of time represented by the\nnumber of dis-\ncrete divisions a quarter note has been split into. So, if we consider 16\ndivisions per quarter note, for instance, the duration of 40 ticks corre-\nsponds to two-and-a-half beats.\nFigure 1 . Generative model of a 2-dimensional tree struc-\nture representation\nof musical notes.\na 2-dimensional hierarchical structure. Frequent motifs,\nphrases or melodic themes consist of a hierarchy that can\nbe described as time-span trees. In addition, polyphony of-\nten consists of multiple independent voices. That is, we\ncan assume that music consists of a time-spanning tree\nstructure and a synchronizing structure of multiple events\nat several levels of a hierarchy. We would like to describe\nthis 2-dimensional tree structure representation of music in\nthe form of a generative model. This can be accomplished\nby introducing a generative model that is conceptually sim-\nilar to the one proposed in [6].\nFig. 1 shows an example of the generative process of\nfour musical notes in one bar of 4/4. In this example, a\nwhole note is ﬁrst split into two consecutive half notes.\nWe call this process “time-spanning.” Next, the former\nhalf note is copied in the same location, thus resulting in\na chord of two half notes. We call this process “synchro-\nnization.” A chord with an arbitrary number of notes can\nthus be generated by successively employing this type of\nbinary production. Finally, the latter half note is split into a\nquaver and a dotted quarter note via the time-spanning pro-\ncess. This kind of generative process can be modeled by\nextending the idea of the probabilistic context-free gram-\nmar (PCFG) [7]. For simplicity, this paper focuses only on\nChomsky normal form grammars, which consist of only\ntwo types of rules: emissions and binary productions. A\nPCFG is a pair consisting of a context-free grammar (a\nset of symbols and productions of the form A→BCor\nA→w, where A,B, and Care called “nonterminal sym-\nbols” and wis called a “terminal symbol”) and production\nprobabilities, and deﬁnes a probability distribution over the\ntrees of symbols. The parameters of each symbol consist\nof (1) a distribution over rule types, (2) an emission distri-\nbution over terminal symbols, and (3) a binary production\nover pairs of symbols.\nTo describe the generative process shown in Fig. 1,\nwe must introduce an extension of PCFG. As we explain\nlater, we explicitly incorporate a process of stochastically\nchoosing either “time-spanning” or “synchronization” in\nthe binary production process. Fig. 2 deﬁnes the pro-\nposed generative process of the set of the onset positions\nof some number Rof musical notes. In our model, each\nnode nof the parse tree corresponds to one musical note\n(with no pitch information) and a pair consisting of the on-\nset position Snand duration Lnof that note is considered\nto be a nonterminal symbol. We ﬁrst draw a “switching”\ndistribution (namely, a Bernoulli distribution) ϕTover the\ntwo rule types {EMISSION, B INARY-PRODUCTION }from a\nBeta distribution. Next, we draw another “switching” dis-Draw rule probabilities:\nϕT∼Beta(ϕT; 1, βT)\n[Probability of choosing either of two\nrule types]\nϕN∼Beta(ϕN; 1, βN)\n[Probability of choosing either of two binary-production types]\nFor each duration l:\nϕB\nl∼Dirichlet( ϕB\nl;βB\nl)\n[Probability of position at which segment of length lis split]\nFor each node nin the parse tree:\nbn∼Bernoulli( bn;ϕT)\n[Choose either E MISSION or BINARY-PRODUCTION ]\nIfbn=EMISSION\nSr∼δSr,Sn, Lr∼δLr,Ln\n[Emit terminal symbol]\nIfbn=BINARY-PRODUCTION\nρn∼Bernoulli( ρn;ϕN)\n[Choose either SYNCHRONIZATION or TIME-SPANNING ]\nIfρn=SYNCHRONIZATION\nSn1∼δSn1,Sn, Sn2∼δSn2,Sn\nLn1∼δLn1,Ln, Ln2∼δLn2,Ln\n[Produce two copies of note n]\nIfρn=TIME-SPANNING\nSn1∼δSn1,Sn, Sn2∼δSn2,Sn+Ln1\nLn1∼δLn1,Ln−Ln2\nLn2∼Discrete( Ln2;ϕB\nLn)\n[Split note ninto two consecutive notes n1andn2]\nFigure 2. The probabilistic speciﬁcation of the present\ngenerative model\nof a 2-dimensional tree structure rep-\nresentation of musical notes. δdenotes Kronecker’s\ndelta. Thus, x∼δx,ymeans x=y(with prob-\nability 1). Bernoulli( x;y)andBeta(y ;z)are deﬁned\nasBernoulli( x;y) = yx(1−y)1−xandBeta( y;z)∝\nyz1−1(1−y)z2−1, where x∈ {0, 1},0≤y≤1andz=\n(z1, z2), respectively. Discrete( x;y)andDirichlet( y;z)\nare deﬁned as Discrete( x;y) =yxandDirichlet( y;z)∝∏\niyzi−1\ni where y= (y1, . . . , y I)withy1+···+yI= 1\nandz= (z 1, . . . , z I), respectively.\ntribution ϕNover the two binary-production types {TIME-\nSPANNING, SYNCHRONIZATION }similarly from a Beta dis-\ntribution. Finally, we generate a discrete distribution ϕB\nl=\n(ϕB\nl,1, . . . , ϕB\nl,l)over the position l′at which the segment\nof duration lis split when BINARY-PRODUCTION is chosen.\nThe shapes of all the Beta distributions and the Dirichlet\ndistribution in our model are governed by concentration\nhyperparameters: βT,βNandβB\n1, . . . ,βB\nD.\nGiven a grammar, we generate a parse tree in the fol-\nlowing manner: start with a root node that has the desig-\nnated root symbol, SRoot = 0 andLRoot =Dwhere D\ndenotes the overall length of a piece of music in ticks. For\neach nonterminal node n, we ﬁrst choose a rule type bnus-\ningϕT. Ifbn=EMISSION, we produce a terminal symbol\nSrwith the value of Sn, namely the onset position of note\nr. Ifbn=BINARY-PRODUCTION , we then choose a binary-\nproduction type ρnusing ϕN. Ifρn=SYNCHRONIZATION ,we produce two nonterminal children n1andn2such that\nSn1=Sn2=Sn,Ln1=Ln2=Ln. This means that the\nnotes of the child nodes have exactly the same onset and\nduration. If ρn=TIME-SPANNING , we produce two nonter-\nminal children n1andn2withSn1=Sn,Ln1=Ln−Ln2,\nSn2=Sn+Ln1where Ln2is drawn from a discrete dis-\ntribution ϕB\nLn.Ln2corresponds to the position at which\nthe segment of duration Lnis divided. We apply the pro-\ncedure recursively to any nonterminal children and ﬁnally\nobtain a sequence S1, . . . , S Rcorresponding to the onset\npositions of Rmusical notes.\nNone of the notes ryet contains pitch information. We\nassign a pitch index κrto each note rin the same way as\nan ordinary cluster assignment process:\nϕK\nr∼Dirichlet( ϕK\nr;αK), (2)\nκr∼Discrete( κr;ϕK\nr), (3)\nwhere Discrete( x;y) =yx(where y= (y 1, . . . , y I)with\ny1+···+yI= 1) and Dirichlet( y;z)∝∏\niyzi−1\ni (where\nz= (z 1, . . . , z I)). The k-th element of ϕK\nrdeﬁnes how\nlikely each pitch index is to be chosen. It should be noted\nhere that the generative processes of Srandκrshould not\nbe considered independently, since harmony and rhythm\nare in general interdependent of each other. An interesting\ndirection for future work is the joint modeling of these two\ngenerative processes.\n2.4 Sub-process for generating spectrogram\nWe now turn to describing the sub-process by which a mu-\nsic spectrogram is generated. Here, we consider that a mu-\nsic spectrogram is generated according to the tempo curve\nand the set of note onset positions, that have been gen-\nerated by the sub-processes described in 2.2 and 2.3. To\nmodel a spectrogram of a musical audio signal, we make\nthe following assumptions about musical notes:\n(A1) Each musical note has a static spectral proﬁle char-\nacterized by a particular pitch.\n(A2) The magnitude spectrum of music at a certain time\ninstant is represented by a superposition of the spec-\ntra of multiple musical notes.\n(A3) The power of each musical note varies smoothly in\ntime in the interval between the onset and offset.\nFrom assumption (A1), a magnitude spectrogram of a mu-\nsical note rcan be described as\nXω,t=R∑\nr=1Hω,κrWr,t, (4)\nwhere ωandtare frequency and time indices, respectively.\nA set consisting of H1,k, . . . , H Ω,k≥0represents the\nstatic spectrum of the k-th pitch and so a set consisting of\nH1,κr, . . . , H Ω,κrsigniﬁes the spectrum of note r.Wr,t≥\n0denotes the power of note rat time t. As the assumptions\n(A1) and (A2) do not always hold exactly in reality, an\nactual music spectrogram Yω,tmay diverge from the “ideal\nmodel” Xω,tto some extent. One way to simplify this kind\nof deviation process is to assume a probability distributionpower density\ntimepower densityFigure 3 . Power envelope Wr,tof musical note r.\nonYω,twith the\nexpected value of Xω,t. Here, we assume\nthatYω,tfollows a Poisson distribution with mean Xω,t:\nYω,t∼Poisson( Yω,t;Xω,t), (5)\nwhere Poisson( y;x) =xye−x/y!. It should be noted that\nthe maximization of the Poisson likelihood with respect to\nXω,tamounts to optimally ﬁtting Xω,ttoYω,tby using\nthe I-divergence as the ﬁtting criterion [3, 8]. To avoid any\nindeterminacy in the scaling of Hω,κrandWr,t, we assume\n∑\nωHω,k= 1 (k = 1, . . . , K ). (6)\nEach spectral proﬁle Hω,kmust have the harmonic struc-\nture of a particular pitch. One way of ensuring this is to\nassume a prior distribution over Hω,kso that it is likely to\ngenerate a spectrum with a certain harmonic structure of\nthek-th pitch. Here, we choose to place a Gamma prior\noverHω,k, namely\nHω,k∼Gamma(H ω,k;γ¯Hω,k+ 1, β), (7)\nwhere Gamma(x ;a, b)∝xa−1e−bx. The mode of this\nprior distribution is given by ¯Hω,k, which should be de-\nﬁned such that it corresponds to the most likely spectral\nproﬁle for the k-th pitch. βdetermines the peakiness of\nthe density around the mode.\nTo incorporate assumption (A3) into Wr,t, we propose\ndescribing Wr,tusing a parametric model expressed as a\nsum of Gaussians [8] (Fig. 3):\nWr,t=M∑\nm=1Gr,m,t, (8)\nGr,m,t =wrur,m√\n2πφe−(t−(m− 1)φ−τ r)2/2φ2,\nwhere wris the\ntotal energy of note r, and τris the center\nof the ﬁrst Gaussian, which can be considered the onset\ntime of note r(in seconds). The centers of the Gaussians\nare constrained so that they are equally spaced with the\ndistance φ, which is equal to the “standard deviation” of\nall the Gaussians. ur,1, . . . , u r,Mare weights associated\nwith the MGaussians, which determine the overall shape\nof the power envelope. To avoid any indeterminacy in the\nscaling of wrandur,m, we assume\n∀r:M∑\nm=1ur,m= 1. (9)The number of consecutive Gaussians with non-zero weights\ncorresponds to the duration of the note, which we hope to\ninfer automatically from an observed spectrogram. To this\nend, we choose to use a stick-breaking representation [9]\nto describe the generative process of ur,1, . . . , u r,M:\nVr,m∼Beta(V r,m; 1, βV\nr) (10)\nur,m=Vr,mm−1∏\nm′=1(1−Vr,m′), (11)\nwhich contributes to sparsifying the Gaussian weights.\nNow, recall that the onset position Sr(in ticks) of note\nris assumed to have been generated via the generative\nprocess described in 2.3. The onset position τrof note r\nshould thus be placed near the absolute time into which Sr\nis converted. Recall also that ψd, which can be considered\na function that takes a relative time das an input and re-\nturns the corresponding absolute time as an output, is also\nassumed to have been generated (via the generative process\ndescribed in 2.2). Given Srandψd, we ﬁnd it convenient\nto write the generative process of τras\nτr∼ N (τr;ψSr,(στ)2). (12)\n2.5 Expansion of generative process\nWe can describe an expanded version of the generative pro-\ncess of Yω,tas\nCr,m,ω,t ∼Poisson( Cr,m,ω,t ;Hω,κrGr,m,ω,t )\nYω,t∼δ(\nYω,t−∑\nr,mCr,m,ω,t)\n, (13)\nby introducing an auxiliary variable Cr,m,ω,t . For conve-\nnience of analysis, we use this generative process instead\nof (5) in the following. Note that it can be readily veriﬁed\nthat marginalizing out Cr,m,ω,t reduces (13) to (5).\n3. APPROXIMATE POSTERIOR INFERENCE\n3.1 Variational Bayesian approach\nIn this section, we describe an approximate posterior in-\nference algorithm for our generative model based on vari-\national inference. The random variables of interest in our\nmodel are\nH={Hω,k}ω,k: spectrum of pitch k,\nw={wr}r : total energy of note r,\nV={Vr,m}r,m: shape of power envelope of note r,\nτ={τr}r : onset time (sec) of note r,\nκ={κr}r : pitch index assigned to note r,\nψ={ψd}d : absolute time corresponding to dticks,\nµ={µd}d : local tempo between dandd+ 1ticks,\nS={Sr}r : onset position of note r(in ticks),\nL={Lr}r : duration of note r(in ticks), and\nϕB,ϕT,ϕN,ϕK: rule probabilities,\nwhich we denote as Θ. Our goal is to compute the poste-\nriorp(Θ, C|Y)where Y={Yω,t}andC={Cr,m,ω,t }\nare sets consisting of observed magnitude spectra and aux-\niliary variables, respectively. By using the conditional dis-\ntributions deﬁned in 2.2, 2.3, 2.4, and 2.5, we can write thejoint distribution p(Y,Θ, C)as\np(Y, H, w, V,\nτ, κ, ψ, µ, S, L, ϕB, ϕT, ϕN,ϕK, C)\n=p(Y|C)p(C|H, w, V, τ, κ )p(H)p(V)p(w)\np(τ|ψ, S )p(ψ|µ)p(µ) p(κ|ϕK)p(ϕK)\np(S, L |ϕB, ϕT, ϕN)p(ϕB)p(ϕT)p(ϕN),(14)\nbut to obtain the exact posterior p(Θ, C|Y), we need to\ncompute p(Y), which involves many intractable integrals.\nWe can express this posterior variationally as the solu-\ntion to an optimization problem:\nargmin\nq∈QKL(q (Θ, C)∥p(Θ, C|Y)), (15)\nwhere KL(·∥·) denotes the Kullback-Leibler (KL) diver-\ngence between its two arguments. Indeed, if we let Qbe\nthe family of all distributions over ΘandC, the solution to\nthe optimization problem is the exact posterior p(Θ, C|Y),\nsince KL divergence is minimized exactly when its two\narguments are equal. Of course, solving this optimization\nproblem is just as intractable as directly computing the pos-\nterior. Although it may appear that no progress has been\nmade, having a variational formulation allows us to con-\nsider tractable choices of Qin order to obtain principled\napproximate solutions.\nFor our model, we deﬁne the set of approximate distri-\nbutions Qas those that factor as follows:\nQ={\nq:q(C)q(H)q(w)q(V)q(τ, ψ, µ )q(κ)\nq(S, L )q(ϕK)q(ϕB)q(ϕT)q(ϕN)}\n.(16)\nWe admit that this is a strong assumption. Its validity and\nhow it affects the parameter inference result must be inves-\ntigated in the future.\n3.2 Coordinate ascent\nWe now present an algorithm for solving the optimization\nproblem described in (15) and (16). Unfortunately, the op-\ntimization problem is non-convex, and it is intractable to\nﬁnd the global optimum. However, we can use a sim-\nple coordinate ascent algorithm to ﬁnd a local optimum.\nThe algorithm optimizes one factor in the mean-ﬁeld ap-\nproximation of the posterior at a time while ﬁxing all the\nother factors. The mean-ﬁeld update equations for the vari-\national distributions are given in the following form:\nq(Cω,t) = Multinomial( Cω,t;Yω,t,fC\nω,t),(17)\nq(Hω,k) = Gamma( Hω,k;ξH\nω,k, ζH\nω,k), (18)\nq(wr) = Gamma( wr;ξw\nr, ζw\nr), (19)\nq(Vr,m) = Beta( Vr,m;ξV\nr,m, ζV\nr,m), (20)\nq(τ,ψ,µ) =N(χ;ξχ,ζχ), (21)\nq(κr) = Discrete( κr;fκ\nr), (22)\nq(ϕK\nr) = Dirichlet(ϕK\nr;ξK\nr), (23)\nq(Sr, Lr) = Discrete( Sr, Lr;fSL\nr), (24)\nq(ϕB\nl) = Dirichlet(ϕB\nl;ξB\nl), (25)\nq(ϕT) = Beta( ϕT;ξT, ζT), (26)\n \nWolfgang Amadeus MozartSonate Opus KV 331 - Tema\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3/flags.u3\n/flags.d3/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.d3/noteheads.s2\n/dots.dot /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/flags.d3/clefs.G/accidentals.2/accidentals.2/accidentals.2\n86\n/clefs.F/accidentals.2/accidentals.2/accidentals.2\n86/dots.dot /noteheads.s2TEMA\nAndante grazioso\np/dots.dot/noteheads.s2/noteheads.s2p\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/flags.d3\n/flags.u3\n/flags.d3/noteheads.s2/dots.dot\n/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/flags.d3/flags.u3\n/flags.d3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/flags.u3\n/flags.u3\n/flags.d3/noteheads.s2\n/noteheads.s2/noteheads.s2\n/dots.dot/dots.dot/dots.dot/dots.dot\n/dots.dot/dots.dot/dots.dot/dots.dot/noteheads.s2/noteheads.s2\n/flags.d3\n/dots.dot/noteheads.s2/noteheads.s2/dots.dot /noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.u3\n/flags.d3/dots.dot/noteheads.s2\n/dots.dot /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s25\n/clefs.G/accidentals.2/accidentals.2/accidentals.2\n/clefs.F/accidentals.2/accidentals.2/accidentals.2/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3/flags.u3\n/flags.d3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3/flags.u3\n/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2sf/noteheads.s2/noteheads.s2/noteheads.s2\nsf/flags.u3\n/flags.d3p/noteheads.s2p/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/rests.3/rests.3\n/noteheads.s2/dots.dot /noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/scripts.staccato/noteheads.s2\nsf/noteheads.s2/noteheads.s2sf/noteheads.s2/noteheads.s2\n/rests.3/scripts.staccato/noteheads.s2\nsf/noteheads.s2/noteheads.s2sf/noteheads.s2/noteheads.s2\n/rests.3/scripts.staccato/noteheads.s2\nsf/noteheads.s2/noteheads.s2sf/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/scripts.staccato/noteheads.s2\n/flags.d3/accidentals.2/dots.dot/noteheads.s2\n/dots.dot /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/flags.d3\np/dots.dot/noteheads.s2/noteheads.s2p/dots.dot /noteheads.s2\n/noteheads.s2/noteheads.s210\n/clefs.G/accidentals.2/accidentals.2/accidentals.2\n/clefs.F/accidentals.2/accidentals.2/accidentals.2 /noteheads.s2/noteheads.s2/noteheads.s2\n/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.u3\n/flags.d3\n/dots.dot/dots.dot\n/dots.dot/dots.dot /flags.u3/dots.dot /noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\nf/noteheads.s2\n/noteheads.s2f/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/flags.d3\n/flags.d3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/noteheads.s2\n/flags.d3/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.d3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/rests.3/rests.314\n/clefs.G/accidentals.2/accidentals.2/accidentals.2\n/clefs.F/accidentals.2/accidentals.2/accidentals.2/dots.dot /noteheads.s2/noteheads.s2/dots.dot/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.u3\n/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/flags.d3\n/flags.u3\n/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2\nsf/noteheads.s2sf/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/flags.d3p/noteheads.s2p/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\nThis sheet music has been placed in the public domain by the typesetter, for details see: http://creativecommons.org/licenses/publicdomainTypeset using www. LilyPond .orgby Stelios Samelis. Reference: Mutopia-2005/10/29-614Sheet music from www. MutopiaProject .org •Free to download, with the freedom to distribute, modify and perform.Public Domain(a) Correct score\nBar NumberNote Number\n1 2 3 4486072\n(b) Detected beat locations along with the estimate of Wr,t\n(c) Score transcribed with the proposed method\nFigure 4 . Transcription result obtained\nwith the proposed\nmethod applied to Morzart: Piano Sonata No. 11 in A ma-\njor, K. 331/300i under the situation where τ1, . . . , τ Rare\ngiven. In (b), the red and green lines indicate the estimates\nof bar lines and the positions of beat locations obtained\nwith the present method, respectively.\nq(ϕN) = Beta( ϕN;ξN, ζN), (27)\nwhere\nχ=\nτ\nψ\nµ\n,ξχ=\nητ\nηψ\nηµ\n,ζχ=\nντντψντµ\nντψνψνψµ\nντµνψµνµ\n.\n(25)–(27) are performed only when we want to learn the\nrule probabilities. (24)–(27) can be updated using the inside-\noutside algorithm. The update formulas of the variational\nparameters are all given in analytical form, but they are\nomitted here owing to space limitations.\n4. EXPERIMENTAL RESULTS\nWe now present experimental results obtained with our\nproposed model. We ﬁrst conducted a preliminary experi-\nment to conﬁrm that our model can transcribe a score (ap-\npropriately estimate the note values of musical notes, beat\nlocations, and the tempo of a music piece) when the on-\nset times of all the musical notes (namely, τr’s) are given.\nWe then show an example of transcription results obtained\nusing the complete model directly from an audio spectro-\ngram.\nFor the ﬁrst experiment, we used a few piano record-\nings (RWC-MDB-C-2001 No. 26, 27, 30) excerpted from\nthe RWC music database [12]. The data were the ﬁrst 10 s,\nmixed down to a monaural signal and resampled to 16 kHz.\nThe constant-Q transform was used to compute spectro-\ngrams where the time resolution, the lower bound of the\nfrequency range, and the frequency resolution were set at\n16 ms, 30 Hz and 12 cents, respectively. In this experi-\nment, all the values τ1, . . . , τ Rwere given manually. The\nhyperparameters and initial parameters were set at K=\n74, M = 40, φ= 3, αH\nω,k=βH\nω,k¯Hω,k+1, βH\nω,k= 500, αw\nr=(a) Detected beat locations along with the estimate of Wr,t\n(b) Score transcribed with the proposed method\nFigure 5 . Transcription result obtained\nwith the proposed\nmethod applied to Morzart: Piano Sonata No. 11 in A ma-\njor, K. 331/300i. In (a), the red and green lines indicate\nthe estimates of bar lines and the positions of beat loca-\ntions obtained with the present method, respectively. In\n(b), the red, green and blue circles indicate the deletion\nerrors, pitch errors and octave errors, respectively.\nβw\nr= 0, βV\nr,m= 10e−m/8/∑\nm′e−m′/8, στ= 2, σψ=\n1, σµ= 0.5, α r,k= 2, βT= 1, βN= 2. The initial values\nofHω,kand¯Hω,kwere set at the value obtained with the\nnon-netaive matrix factorization [13] applied to the mag-\nnitude spectrogram of the piano excerpts from the RWC\nmusical instrument sound database [11]. We set the res-\nolution of the relative time at 4 ticks per quarter note. D\nand the initial values of ψdwere set at the values obtained\nwith [10]. The algorithm was run for 10 iterations. After\nconvergence, we took the expected values of the posteriors\nand regarded them as the parameter estimates.\nFig. 4 shows an example of the score we obtained when\nwe applied the present method to Mozart’s Sonata (RWC-\nMDB-C-2001 No. 26). As can be seen from this example,\nthe note values and the beat locations were appropriately\nestimated. We also conﬁrmed that reasonably good results\nwere obtained for other recordings such as Chopin’s Noc-\nturne No. 2 in E ♭-maj, Op. 9 (RWC-MDB-C-2001 No. 30).\nFor the second experiment, we applied our method with-\nout providing any information about τ. The experimental\nconditions were the same as above except that we assumed\nthatτwas unknown. Fig. 5 shows an example of the esti-\nmates of Wr,t(namely, the power envelope of note r) and\nthe score obtained with the present method applied to the\nsame data in Fig. 5. The result showed that many octave\nerrors had occurred. This kind of error often occurs when\nthere is a mismatch between a spectrum model and an ac-\ntual spectrum. The validity of the assumptions we have\nmade about the spectra of musical sounds in 2.4 must be\ncarefully examined in the future.\n5. CONCLUSION\nThis paper proposed a Bayesian model for automatic mu-\nsic transcription. Automatic music transcription involves\nseveral interdependent subproblems: multiple fundamental\nfrequency estimation, onset detection, and rhythm/tempo\nrecognition. To circumvent the chicken-and-egg problem,we modeled the generative process of an entire music spec-\ntrogram by combining the sub-process by which a musi-\ncally natural tempo curve is generated, the sub-process by\nwhich a set of note onset positions is generated based on\na 2-dimensional tree structure representation of music, and\nthe sub-process by which a music spectrogram is gener-\nated according to the tempo curve and the note onset po-\nsitions. Thanks to this combined generative model, the\npresent method performs note extraction and structure esti-\nmation simultaneously and thus an optimal solution is ob-\ntained within a uniﬁed framework. We described some of\nthe transcription results obtained with the present method.\n6. REFERENCES\n[1] N. Bertin, R. Badeau, and G. Richard, “Blind signal de-\ncompositions for automatic transcription of polyphonic\nmusic: NMF and K-SVD on the benchmark,” In Proc.\nICASSP2007, Vol. 1, pp. 65–68, 2007.\n[2] K. Ochiai, H. Kameoka, and S. Sagayama, “Ex-\nplicit beat structure modeling for non-negative ma-\ntrix factorization-based multipitch analysis,” in Proc.\nICASSP2012, pp. 133–136, 2012.\n[3] A. T. Cemgil, “Bayesian inference in non-negative ma-\ntrix factorisation models,” Technical Report CUED/F-\nINFENG/TR.609, University of Cambridge, 2008.\n[4] M. D. Hoffman, D. M. Blei, and P. R. Cook, “Bayesian\nnonparametric matrix factorization for recorded mu-\nsic,” in Proc. ICML2010 , pp. 439–446.\n[5] K. Yoshii, and M. Goto, “A nonparametric Bayesian\nmultipitch analyzer based on inﬁnite latent harmonic\nallocation,” IEEE Trans. Audio, Speech, Language\nProcess. , Vol. 20, No. 3, pp. 717–730, 2012.\n[6] M. Nakano, Y. Ohishi, H. Kameoka, R. Mukai, and\nK. Kashino, “Bayesian nonparametric music parser,”\ninProc. ICASSP2012 , pp. 461–464, 2012.\n[7] P. Liang, S. Petrov, M. I. Jordan, and D. Klein, “The\ninﬁnite PCFG using hierarchical Dirichlet processes,”\ninEMNLP2007, pp. 688–697.\n[8] H. Kameoka, T. Nishimoto, and S. Sagayama, “A mul-\ntipitch analyzer based on harmonic temporal structured\nclustering,” IEEE Trans. on Audio, Speech, Language\nProcess. , Vol. 15, No. 3, pp. 982–994, 2007.\n[9] J. Sethuraman, “A constructive deﬁnition of Dirichlet\npriors,” Statistica Sinica, Vol. 4, pp. 639–650, 1994.\n[10] D. P. W. Ellis, “Beat tracking by dynamic program-\nming,” Journal of New Music Research, 36(1):51–60,\n2007.\n[11] M. Goto, “Development of the RWC music database,”\nInProc. the 18th International Congress on Acoustics\n(ICA 2004), pp. I-553–I-556, 2004.\n[12] M Goto, H Hashiguchi, T Nishimura, and R Oka,\n“RWC music database: Popular, classical, and jazz\nmusic database,” In Proc. ISMIR, pp. 287–288, 2002.\n[13] P. Smaragdis and J. C. Brown, “Non-negative matrix\nfactorization for music transcription,” in Proc. WAS-\nPAA2003 , pp. 177–180, 2003."
    },
    {
        "title": "A Turkish Makam Music Symbolic Database for Music Information Retrieval: SymbTr.",
        "author": [
            "Mustafa Kemal Karaosmanoglu"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": "https://zenodo.org/records/1284501/files/turkish_makam_music_audio-score_alignment_1.0.zip",
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\nŞentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 - 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmanoğlu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., Şentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/Karaosmanoglu12",
        "keywords": [
            "Turkish Makam Music",
            "Audio-Score Alignment",
            "Music Transcription",
            "Music Information Retrieval",
            "SymbTr Collection",
            "Automatic Transcription",
            "Folk Music Analysis",
            "Lyrics-to-Audio Alignment",
            "Ottoman-Turkish Makam Music",
            "Dataset"
        ]
    },
    {
        "title": "Multi-Template Shift-Variant Non-Negative Matrix Deconvolution for Semi-Automatic Music Transcription.",
        "author": [
            "Holger Kirchhoff",
            "Simon Dixon",
            "Anssi Klapuri"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418207",
        "url": "https://doi.org/10.5281/zenodo.1418207",
        "ee": "https://zenodo.org/records/1418207/files/KirchhoffDK12.pdf",
        "abstract": "For the task of semi-automatic music transcription, we ex- tended our framework for shift-variant non-negative matrix deconvolution (svNMD) to work with multiple templates per instrument and pitch. A k-means clustering based learn- ing algorithm is proposed that infers the templates from the data based on the provided user information. We experi- mentally explored the maximum achievable transcription accuracy of the algorithm and evaluated the prospective performance in a realistic setting. The results showed a clear superiority of the Itakura-Saito divergence over the Kullback-Leibler divergence and a consistent improvement of the maximum achievable accuracy when each pitch is represented by more than one spectral template.",
        "zenodo_id": 1418207,
        "dblp_key": "conf/ismir/KirchhoffDK12",
        "keywords": [
            "semi-automatic",
            "music transcription",
            "shift-variant",
            "non-negative matrix",
            "deconvolution",
            "templates per instrument",
            "pitch",
            "k-means clustering",
            "learning algorithm",
            "maximum achievable transcription accuracy"
        ],
        "content": "MULTI-TEMPLATE SHIFT-VARIANT NON-NEGATIVE MATRIX\nDECONVOLUTION FOR SEMI-AUTOMATIC MUSIC TRANSCRIPTION\nHolger Kirchhoff, Simon Dixon, Anssi Klapuri\nQueen Mary University of London, Centre for Digital Music\n{holger.kirchhoff, simon.dixon, anssi.klapuri}@eecs.qmul.ac.uk\nABSTRACT\nFor the task of semi-automatic music transcription, we ex-\ntended our framework for shift-variant non-negative matrix\ndeconvolution (svNMD) to work with multiple templates\nper instrument and pitch. A k-means clustering based learn-\ning algorithm is proposed that infers the templates from the\ndata based on the provided user information. We experi-\nmentally explored the maximum achievable transcription\naccuracy of the algorithm and evaluated the prospective\nperformance in a realistic setting. The results showed a\nclear superiority of the Itakura-Saito divergence over the\nKullback-Leibler divergence and a consistent improvement\nof the maximum achievable accuracy when each pitch is\nrepresented by more than one spectral template.\n1. INTRODUCTION\nAutomatic music transcription describes the process of\ntransforming a recording of a piece of music into a score\nor an intermediate score-like representation. It has been\nan active area of research over the last decades and a mul-\ntitude of approaches has been proposed. An overview of\nthe main computational techniques for music transcription\ncan be found in [1]. Despite this long research history, the\naccuracy of fully automatic music transcription systems is\nstill considerably below the accuracy achieved by trained\nmusicians.\nAs a step towards a more accurate transcription system,\nwe address the task of user-assisted orsemi-automatic mu-\nsic transcription. These terms refer to systems in which\nthe user provides a certain amount of information about the\nrecording under analysis which can then be used to guide\nthe transcription process. In this paper, we assume that\nthe user labels a certain number of notes for each instru-\nment, which is then used to build instrument models that\nare tailored to the speciﬁc instruments in the mixture. In\na practical application, the user could either be presented\nwith a magnitude spectrogram and be asked to graphically\nmark a few fundamental frequency trajectories, or — if a\nmore musical approach is desired — with the result of a\nfully-automatic transcription system for which he is asked\nto assign some of the detected notes to the instruments.\nWe address this task by means of a non-negative matrix\ndeconvolution framework. Since the introduction of non-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.negative matrix factorisation (NMF) [2] which was ﬁrst ap-\nplied to music analysis by Smaragdis and Brown [3], a num-\nber of modiﬁcations to this algorithm have been proposed.\nIn this work, we build on our shift-variant non-negative ma-\ntrix deconvolution (svNMD) framework [4] which is itself\na modiﬁcation of Schmidt and Mørup’s NMF2D model [5].\nIn the svNMD framework, a single spectral template for\neach pitch of each instrument is estimated which is then\nused to detect fundamental frequencies in the constant-Q\nmagnitude spectrogram of the recording. Here, we extend\nthe model to work with multiple templates per pitch. The\nmotivation for having multiple templates per pitch is given\nby the fact that the spectral shape of a particular note can\nvary based on dynamics or playing style and to model a\ntime-varying spectral envelope of a note.\nOther related work can be found in NMF-based ap-\nproaches to score-informed source separation, where mid-\nlevel score representations are used to infer models for the\nsource instruments. Hennequin et al. [6] modify the NMF\nmodel to work with parametric spectral templates. The\nmodel allows templates to be shifted in frequency while\npreserving the overtone amplitudes. The parameters are\nlearned by initialising the NMF gain matrix and succes-\nsively applying update functions for the template parame-\nters and the gains. In [7], Ganseman et al. use a synthesised\nand time-aligned score as priors for the PLCA system pro-\nposed in [8]. In addition to note information, this approach\nrequires knowledge about the timbre of each source in order\nto facilitate a fast convergence.\nThe remainder of this paper is organised as follows:\nIn the following section we present our extension to the\nsvNMD framework that works with multiple templates per\npitch (Sect. 2.1) and illustrate the algorithm for learning\nthese templates (Sect. 2.2). In Sect. 3 we evaluate the\nproposed algorithm in two different experiments and discuss\nthe results. Conclusions are ﬁnally drawn in Sect. 4.\n2. MULTIPLE-TEMPLATE SHIFT-VARIANT\nNON-NEGATIVE MATRIX DECONVOLUTION\nIn this section we present our non-negative matrix deconvo-\nlution framework which decomposes a constant-Q spectro-\ngram into a structured dictionary of instrument templates\nand corresponding gain values (see Sect. 2.1). The frame-\nwork represents each pitch of each instrument by a prede-\nﬁned number of spectral templates. Furthermore, in Sect.\n2.2 we describe a procedure that allows us to extract multi-\nple templates for each note previously labelled by the user.\nThis procedure is applicable to polyphonic material where\npartials might overlap.relative frequency\ninstrument idx.\ntempl. idx.pitchW2;0\nW1;0\nW0;0W0;1W0;2\nframeinstr. idx.templ.\nidx.pitch\nH2;0\nH1;0\nH0;0\nH0;1\nH0;2Basis functions Gainsfrequency\nframeVConstant-Q spectrogram\nFigure 1 : svNMD framework with multiple templates per\ninstrument and pitch.\n2.1 Framework\nThe proposed non-negative matrix deconvolution frame-\nwork decomposes a constant-Q spectrogram into 4-dimen-\nsional structures for the basis functions and the gains, re-\nspectively. Figure 1 illustrates the framework graphically.\nEach instrument in the mixture under analysis is represented\nby a 3-dimensional structure (tensor) that contains a ﬁxed\nnumber of basis functions for each pitch. The pitch res-\nolution is determined by the frequency resolution of the\nconstant-Q spectrogram under analysis and the number of\ntemplates per pitch can be chosen arbitrarily. Likewise,\nfor each instrument a 3-dimensional structure contains the\ncorresponding gains for the spectral templates. Each layer\ndisplayed on the right-hand side of Fig. 1 contains the gain\ntrajectories at a ﬁxed template index over time. In order\nto arrive at a single pianoroll-like representation for each\ninstrument, the gains of the layers can be summed up verti-\ncally.\nIn mathematical terms, we denote the constant-Q magni-\ntude spectrogram by V2RN\u0002M\n+ , whereNis the number\nof frequency bins and Mthe number of frames. The matrix\nW\u001e;i2RN\u0002T\n+ contains in its columns the spectral tem-\nplates of instrument iat pitch\u001e(see Fig.1).Tdenotes the\nspeciﬁed number of spectral templates. All templates have\ntheir ﬁrst partial at the ﬁrst row index of W\u001e;iand likewise\nall other partials appear each roughly at their corresponding\nrow index due to the use of the constant-Q spectrogram.\nH\u001e;i2RT\u0002M\n+ on the other hand denotes the matrix that\ncontains the corresponding gains for the templates of instru-\nmentiat pitch\u001eover time. Note that in Fig. 1, this matrix\ncorresponds to a slice through one of the banks of layers,\nas shown in the ﬁgure.\nGiven these matrices we approximate our original spec-\ntrogram Vby\nV\u0019\u0003=I\u00001X\ni=0\b\u00001X\n\u001e=0\u001e#\nW\u001e;iH\u001e;i; (1)\nwhere \u00032RN\u0002M\n+ has the same dimensions as V. Here,I\ndenotes the number of instruments in the mixture and \bthenumber of pitches. \bandNdo not necessarily need to be\nthe same, in our case, however, they are. The operator \u001e#\ndenotes a downward shift of the matrix elements by \u001erows\nwhile the upper \u001erows are ﬁlled with zeros. This mixture\nmodel shifts each spectral template to the correct frequency\nposition and scales them by the corresponding gains at each\nframe.\nUpdate equations were derived for both W\u001e;iandH\u001e;i\nby computing the gradient of the \f-divergence between V\nand\u0003. The\f-divergence is given by\nC\f=NX\nn=1MX\nm=1[V]\f\nn;m\n\f(\f\u00001)+[\u0003]\f\nn;m\n\f\u0000[V]n;m[\u0003]\f\u00001\nn;m\n\f\u00001;\n(2)\nfor\f2Rnf 0;1gand\nC0=NX\nn=1MX\nm=1[V]n;m\n[\u0003]n;m\u0000log \n[V]n;m\n[\u0003]n;m!\n\u00001 (3)\nC1=NX\nn=1MX\nm=1[V]n;mlog \n[V]n;m\n[\u0003]n;m!\n+[\u0003]n;m\u0000[V]n;m:\n(4)\nThe update equations are given by\nW\u001e;i W\u001e;i\u000f \n\u001e\"\nV\u000f\u001e\"\n\u0003\f\u00002!\n\u0002\nH\u001e;i\u0003T\n\u001e\"\n(\u0003\f\u00001) [H\u001e;i]T(5)\nH\u001e;i H\u001e;i\u000fh\u001e#\nW\u001e;iiT\u0000\nV\u000f\u0003\f\u00002\u0001\nh\u001e#\nW\u001e;iiT\n\u0003\f\u00001(6)\nIn these equations, \u000fdenotes an elementwise multiplica-\ntion and all divisions and power operations are likewise\ncarried out per element. We can obtain the well-known\nleast squares (LS), Kullback-Leibler (KL) and Itakura-Saito\n(IS) cost functions by setting \f= 2,\f= 1 and\f= 0,\nrespectively. The derivation of Eqs. 5 and 6 is provided in a\nsupplementary document [9].\n2.2 Learning the basis functions\nFigure 2 illustrates the iterative procedure of learning a\nnumber of templates for a single note labelled by the user.\nThe user provides information about the start frame, the end\nframe and the pitch \u001e0of a note of a particular instrument\ni0. This information can be illustrated by a pianoroll that\ncontains a single line representing the note, as shown on the\nleft-hand side of panel (a). Given this information, we can\nidentify the matrix W\u001e0;i0in which the learned templates\nwill be stored and the matrix H\u001e0;i0that contains the gains\nfor each of the templates over time (grey-shaded matrices\non the right-hand side of panel (a)). Since only those two\nmatrices W\u001e0;i0andH\u001e0;i0are relevant for learning the\ntemplates from the labelled note, we isolate them from their\ntensors when illustrating the learning algorithm in panels\n(b)–(f).\nPanels (b)–(f) display the algorithmic steps for estimat-\ning the spectral templates. This procedure is in fact verysimilar to applying k-means clustering to the spectra of a\nnote at all time frames within the spectrogram V. In this\nanalogy, each spectral template corresponds to a cluster\nmean and thus represents a set of spectra at different time\nframes. Since the learning procedure is carried out within\nthe nonnegative framework, the correponding k-means clus-\ntering steps might not be obvious. For that reason, we\nillustrate these on the right hand side of panels (b)–(f). In\nthese graphs, each data point corresponds to a spectrum of\nthe note at a particular time frame in the N-dimensional\nspace which is here for the sake of illustration reduced to 2\ndimensions.\n1. Initialisation: The algorithm starts by initialising the\nspectral templates in W\u001e0;i0with nonnegative random\nvalues (panel (b)). In the gain matrix H\u001e0;i0each frame\nof the note is randomly assigned to exactly one spectral\ntemplate by setting the corresponding gains to a value of\n1 while all other entries of the matrix are set to 0. In the\nk-means example, this corresponds to assigning the data\npoints randomly to one of the three clusters: crosses,\ncircles and squares.\n2. Update: In the second step (panel (c)), we update the\nspectral templates in W\u001e0;i0based on the gains that\nwere set in the previous step. This modiﬁes the spec-\ntral templates in such a way that the resulting templates\nminimise the \f-divergence at the assigned frames. Thus,\neach resulting spectral template can be seen as an av-\nerage of the instrument spectra at the time frames that\nwere assigned to it. In k-means clustering terms, this is\nequivalent to computing the average of the data points\nthat were assigned to the same class. Note that in order\nto eliminate scale-ambiguities in the nonnegative frame-\nwork, all spectral templates in W\u001e0;i0are scaled to have\na power of 1 and the gains are adjusted accordingly.\n3. Assignment: In order to assign the spectra of the note\nat all frames to the template that best resembles their\nspectral shape, we set the template gains at each note\nframe to equal values (panel (d)) and update the gains\nbased on the given spectral templates (panel (e)). This\nway, the gain matrix contains the contributions of each\ntemplate to the audio spectra of each time frame when\nlinearly combining the templates. This can be seen as a\nsimilarity measure between the templates and the spectra.\nWe assign each frame to the template with the highest\ngain value, here indicated by the grey-shaded entries. In\nthe k-means clustering example, this corresponds to the\nassignment step, in which each data point is assigned\nto the closest mean. We setup a new matrix H\u001e0;i0\n(panel (f)) that contains at each frame and each assigned\ntemplate index the gains from step 2 (cf. panel (d)).\nThe algorithm iterates over steps 2 and 3.\nThe reason for assigning each frame to just a single\nspectral template in step 1 and 3 is that we want to avoid the\npartials of a note to be split among the different templates.\nA template that only contains a subset of partials might be\nused by the algorithm to explain partials of other notes from\nthe same or another instrument. An intuitive example for\nthis case would be a spectral template that only contains\na single partial (i.e. a single spectral peak) which can be\nused by the algorithm to approximate a partial of any notestart\nframeend\nframe\u001e0pitch\nframeW\u001e0;i0relative frequency\ntempl. idx.pitchH\u001e0;i0\nframetempl.\nidx.pitch\n(a) Piano roll and svNMD framework\n000\n000\n000 0000 1 1\n00 000 1\n0 0 0 1 11 000\n000\nstart frame end frameW\u001e0;i0:H\u001e0;i0: k-means clustering:\n(b) Initialisation\n000\n000\n000 0000 1.2 1.2\n00 000 0.6\n0 0 0 0.9 0.90.9 000\n000\nstart frame end framemeans\n(c) Update\n000\n000\n0000.40.30.20.30.30.4\n0.40.30.20.30.30.4\n0.40.30.20.30.30.4 000\n000\nstart frame end frame\n(d) Assignment (1)\n000\n000\n0000.60.20.10.30.40.8\n0.20.10.80.70.50.1\n0.20.50.30.40.60.9 000\n000\nstart frame end frame\n(e) Assignment (2)\n000\n000\n000 00000 1.2\n00 00 0.60.9\n0 00 0.9 0.91.2 000\n000\nstart frame end frame\n(f) Assignment (3)\nFigure 2: Learning algorithm\nat that position of the same or another instrument. This\nwould produce a gain value either at the wrong fundamental\nfrequency or the wrong instrument or both and thereby\nadulterate the transcription accuracy.\nIn k-means clustering, there is a chance of producing\nempty clusters when assigning the data points to the new\nmeans. The same problem applies to our proposed learn-\ning algorithm. In our algorithm this problem can occur inpanel (e), when for a certain template none of the frames\ncontains the largest gains. In this case, we detect the largest\ncluster (i.e. the template with the largest number of assigned\nframes) and randomly assign half of its frames to the empty\ncluster. The spectral template of the empty cluster is then\ndiscarded and replaced by a duplicate of the spectral tem-\nplate of the largest cluster.\nAlthough the learning procedure was here illustrated\nby an individual note of a single instrument, the proce-\ndure is applicable to and intended for polyphonic audio.\nA MATLAB implementation of the learning algorithm\nis available at http://code.soundsoftware.ac.\nuk/projects/svnmdmt.\n3. EVALUATION\nThe evaluation of the proposed framework and the tem-\nplate learning algorithm was carried out in two experiments.\nIn the ﬁrst experiment (Sect. 3.3) we explored the upper\nlimit of performance of the algorithm when used for semi-\nautomatic transcription. The results of this experiment\nprovide some intuition about the potential of the framework\nto accurately approximate a spectrogram. The second exper-\niment (Sect. 3.4) looked at a more realistic semi-automatic\ntranscription setting in which only a part of the notes are\nemployed for learning the templates which are then applied\nto transcribe the remainder of the recording.\n3.1 Dataset\nFor both experiments described below, the same dataset\nas in [4] was used. The dataset was based on monophonic\nrecordings of musical phrases from 12 different instruments,\neach with a length of approximately 30s. Mixtures of 2 to 5\ninstruments were produced by combining the monophonic\nsignals. For each polyphony level (2 to 5 instruments), 50\ndifferent combinations were generated. At the same time,\nthe hand-annotated notes of the 12 monophonic ﬁles were\navailable in MIDI format. Those MIDI ﬁles acted as the\nground truth for the evaluation.\nIn addition to that, we evaluated the algorithm on more\nharmonically related instrument parts and computed results\nfor a wind quintet excerpt (cf. [10]). This example had a\nlength of 54s and for each instrument part hand-annotated\nMIDI ground-truth was available.\n3.2 Accuracy\nIn order to measure the transcription accuracy, we refrained\nfrom using the common measures precision, recall orF-\nscore. Those measures are used to compare detected note\nevents to ground truth notes. Combining gains into note\nobjects, however, would require a subsequent note-tracking\nalgorithm which will have an inﬂuence on the results. Our\naim is here to study the performance of the proposed algo-\nrithm in isolation.\nAs an accuracy measure, we therefore compute the per-\ncentage of energy in the gain matrices that is concentrated\nin the ground truth fundamental frequencies. This is done\nfor each instrument individually. In order to achieve that, a\nsummary gain matrix Giis computed for each instrument i\nin the mixture by\n\u0002\nGi\u0003\n\u001e;n=TX\nt=1\u0002\nH\u001e;i\u0003\nt;n: (7)Intuitively, in Fig. 1 this corresponds to summing all the\ndisplayed gain layers for each instrument. Based on the\nsummary gain matrices Gi, the per-instrument accuracies\nAcciare computed by\nAcci=NP\nn=1P\n\u001e2Fn\u0010\u0002\nGi\u0003\n\u001e;n\u00112\nNP\nn=1\bP\n\u001e0=1\u0010\n[Gi]\u001e0;n\u00112: (8)\nIn this equation,Fndenotes the set of frequency bins of\nthe annotated pitches in the n-th frame. Since the test set\nonly contains monophonic instruments, Fnonly contains\nthe bins of at most one note at each time frame. Ideally, we\nwould like to see all energy concentrated in the fundamen-\ntal frequencies which would make it easy to detect notes\nwithin the gain matrices. This case would correspond to an\naccuracy Acciof 1.\n3.3 Experiment 1: Exploring the upper performance\nlimit\nIn the ﬁrst experiment we explored the upper performance\nlimit of the nonnegative framework when used for a semi-\nautomatic transcription task. The upper performance limit\nis given when a user labels allnotes of allinstruments in\nthe mixture under analysis. Although this scenario may\nseem trivial, because no transcription algorithm would be\nrequired if all notes were known beforehand, this evaluation\nprovides an intuition about the expressivity of the algorithm\nand reveals any methodological ﬂaws.\n3.3.1 Experimental setup\nFor each ﬁle in the dataset, we extracted T= 1,3and5\ntemplates per pitch, by running 50 iterations of the tem-\nplate learning algorithm described in Sect. 2.2. The user\ninformation was given by the ground truth MIDI ﬁles of\nthe instruments contained in the mixture which contained\nonset, offset and pitch information of the notes of the in-\nstruments. Once the basis functions were learned from the\nconstant-Q magnitude spectrogram of the recording, the\ngain matrices were computed. This was done by randomly\ninitialising all matrices H\u001e;iwith nonnegative values and\napplying 10 iterations of the update equation for the gains\n(Eq. 6). Transcription accuracies were computed as de-\nscribed in Sect. 3.2. The experiment was conducted for the\nIS-divergence (\f = 0) and the KL-divergence (\f = 1).\n3.3.2 Results\nThe results of this experiment are displayed in Fig. 3. The\nupper panels display the results obtained by using the Itakura-\nSaito (IS) divergence, the lower panels the results of the\nKullback-Leibler (KL) divergence. From left to right, the\npanels show the results of the different polyphony levels\n— from 1 to 5 instruments — and on the right-hand side\nthe results of the wind quintet. In each panel, we compare\nthe per-instrument transcription accuracies of all instru-\nments of all ﬁles when represented with different numbers\nof templates per pitch. The results are displayed as box-\nplots: the upper and lower edges of the box represent the\nﬁrst (Q1) and third quartile ( Q3), the median is displayed\nin between. The whiskers extend to the data points thatFigure 3 : Results of experiment 1. The upper and lower rows display the per-instrument accuracies for the IS-divergence\nand KL-divergence, respectively. From left to right, the panels contain the accuracies for different polyphony levels and for\nthe wind quintet. Within each panel the results for different numbers of templates per pitch are presented as boxplots.\nare furthest away from the median, but within the inter-\nval[Q1\u00001:5\u0001(Q3\u0000Q1):::Q 3+ 1:5\u0001(Q3\u0000Q1)]. All\ndata points outside that range are marked by crosses and\nconsidered as outliers.\nWhen comparing the different cost functions for the\nrandom instrument mixtures, it becomes obvious that the\nItakura-Saito divergence outperforms the Kullback-Leibler\ndivergence in all cases. A possible explanation for the good\nperformance of the IS-divergence is its scale-invariance\nproperty (cf. [11]) which is in compliance with Weber’s\nlaw applied to the perception of loudness. An interesting\naspect we found here is that by using the IS-divergence, the\naccuracies do not even noticeably decay when the number\nof instruments is increased.\nWhen we compare the results for different numbers\nof spectral templates per pitch, a clear tendency towards\nhigher accuracies can be observed when more templates\nare learned for each note. The improvement is consistent\nwhen the number of templates is increased from 1 to 3 and\nranges between 2% and almost 10% for different poyphony\nlevels when considering the median accuracies for the IS-\ndivergence. Increasing the number of templates from 3 to\n5 improves the accuracy even further, but not in the same\nconsistent way as from 1 to 3.\nThe results of the wind quintet generally conﬁrm the\nabove ﬁndings, particularly the increasing accuracy when\nmultiple templates are used. The median accuracy is how-\never slightly lower than for the data set of random instru-\nment mixtures, which can be attributed to the larger number\nof overlapping partials.\n3.4 Experiment 2: Real case scenario\nIn the second experiment, we estimated the performance of\na semi-automatic transcription system in a more realistic\nenvironment. We assumed that the user had labelled a\ncertain number of notes for each instrument, which we use\nto estimate template spectra at the corresponding pitches.\nThese template spectra are then used to build complete\nmodels for the instruments which are then applied to the\nremainder of the piece in order to obtain the transcription.3.4.1 Experimental setup\nFor this experiment, we split each ﬁle in the dataset in two\nhalves, each containing approx. 15 s of audio. We assumed\nthat the user had labelled all notes of all instruments in\nthe ﬁrst half and used these to learn the basis functions as\ndescribed above. The basis functions were then replicated\nat the surrounding pitches to cover the whole pitch range\nand were applied to estimate the gains of the second half of\nthe audio.\nAs in the ﬁrst experiment, we applied all combinations of\ncost functions (IS-divergence and KL-divergence), number\nof instruments (1–5) and number of templates per pitch (1,3\nand 5). We again ran 50 iterations of the learning algorithm\nand 10 iterations for the estimation of the gain matrices.\n3.4.2 Results\nFigure 4 shows the results for the second experiment. The\norder of the results is the same as for the previous results.\nFor the random instrument mixtures, the results of this\nexperiment differ from the results of the previous experi-\nment. In general, there is a considerably larger variance in\nthe results for each conﬁguration. Several trends are clearly\nvisible in the diagram: For both cost functions, the accuracy\ndecreases when the number of instruments in the mixture\nis increased. The impression from the ﬁrst experiment that\nthe IS-divergence generally yields better results than the\nKL-divergence is here conﬁrmed, the only exception being\nthe polyphony level of one instrument. However, since the\nresults for the monophonic audio ﬁles are only based on 12\naccuracies, this fact needs to be put in perspective.\nIn terms of the different numbers of templates per pitch,\nthe results for 1, 3 and 5 templates consistently stay in\nthe same range and no clear trend can be found. It has to\nbe considered here that the results of this experiment are\nnot only inﬂuenced by the number of templates, but also\nby the fact that templates of non-annotated pitches were\nestimated by replicating adjacent pitches. It seems that the\nerror introduced by this rough assumption outweighs the\ngain of having multiple templates per pitch.\nThe results for the quintet recording only show a small\nloss in accuracy to the previous experiment. The reason forFigure 4: Results of experiment 2. The results are displayed in the same order as the ones in Fig. 3.\nthis can be seen in the fact that in this excerpt large parts of\nthe ﬁrst half are repeated in the second half, so that almost\nthe same pitch range was covered for training and testing.\nThe experiments show a certain discrepancy between the\nmaximum achievable accuracy and the accuracies that can\nbe expected in a more realistic setting. There are several\nexplanations for the fact that the accuracy of the second ex-\nperiment is decreased: First, there was twice more training\ndata in experiment 1. Second, in the ﬁrst experiment the\nbasis functions will have been better adjusted to the spectra\nof the second half of the audio ﬁles, which were not used in\nthe learning process in the second experiment. And third,\nas indicated above, ﬁlling the gaps in the basis function\ntensors by merely replicating the estimated basis functions\nin the second experiment leads to a loss in accuracy.\n4. CONCLUSION\nWe presented a shift-variant non-negative matrix deconvo-\nlution (svNMD) framework that represents each note of\neach instrument by multiple spectral templates. A learning\nalgorithm was presented that allows the different templates\nto be estimated within the svNMD framework. The steps\nof this algorithm are comparable to a k-means clustering\nalgorithm. We investigated the use of the framework for\nthe task of semi-automtic music transcription in which the\nuser provides a priori information about some notes in the\nmixture under analysis. Two experiments were carried out.\nIn the ﬁrst experiment, the upper performance limit of the\nalgorithm was investigated which is given when the user\nprovides information about all notes of all instruments. The\nresults showed the superiority of the IS-divergence over the\nKL-divergence and a consistent improvement when more\nthan one template per pitch was used. The second experi-\nment expoited a more realistic use case in which the user\nmerely labels a subset of the notes. Here, the superiority of\nthe IS-divergence could be conﬁrmed. In this experiment,\nhowever, no improvement could be found by using multiple\ntemplates per pitch.\n5. REFERENCES\n[1]A. Klapuri and M. Davy, ed.: Signal Processing Meth-\nods for Music Transcription, Springer, New York, 2006.[2]D. D. Lee and H. S. Seung: “Learning the parts of\nobjects by non-negative matrix factorization,” Nature,\nV ol. 401, Nr. 6755, pp. 788-–791, 1999.\n[3]P. Smaragdis and J.C. Brown: “Non-negative matrix\nfactorization for polyphonic music transcription,” IEEE\nWorkshop on Applications of Signal Processing to Audio\nand Acoustics (WASPAA), pp. 177–180, 2003.\n[4]H. Kirchhoff, S. Dixon, and A. Klapuri: “Shift-variant\nnon-negative matrix deconvolution for music transcrip-\ntion,” IEEE International Conference on Acoustics,\nSpeech and Signal Processing, 2012.\n[5]M. Schmidt and M. Mørup: “Nonnegative matrix fac-\ntor 2-D deconvolution for blind single channel source\nseparation,” 6th International Conference on Indepen-\ndent Component Analysis and Blind Source Separation,\npp. 700–707, Charleston, USA, 2006.\n[6]R. Hennequin, B. David, and R. Badeau: “Score in-\nformed audio source separation using a parametric\nmodel of non-negative spectrogram,”. IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 45-–48, 2011.\n[7]J. Ganseman, G. J. Mysore, J. S. Abel, and P. Scheun-\nders: “Source separation by score synthesis,” Interna-\ntional Computer Music Conference, pp. 462–465, 2010.\n[8]P. Smaragdis and G. J. Mysore: “Separation by hum-\nming: User-guided sound extraction from monophonic\nmixtures,” IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics (WASPAA), New\nPaltz, USA, 2009.\n[9]H. Kirchhoff, S. Dixon, and A. Klapuri: “Derivation\nof update equations for multiple-template shift- vari-\nant non-negative matrix deconvolution based on \f-\ndivergence,” Tech. Rep. C4DM-TR-06-12, Queen Mary\nUniversity of London, 2012, http://www.eecs.\nqmul.ac.uk/~holger/C4DM-TR-06-12.\n[10] E. Benetos and S. Dixon: “Joint multi-pitch detection\nusing harmonic envelope estimation for polyphonic mu-\nsic transcription,” IEEE Journal of Selected Topics in\nSignal Processing, V ol. 5, No. 6, pp. 1111–1123, 2011.\n[11] C. Févotte, N. Bertin, and J. L. Durrieu: “Nonnegative\nmatrix factorization with the Itakura-Saito divergence:\nWith application to music analysis,” Neural Computa-\ntion, V ol. 21, No. 3, pp. 793–830, 2009."
    },
    {
        "title": "Characterization of Intonation in Carnatic Music by Parametrizing Pitch Histograms.",
        "author": [
            "Gopala K. Koduri",
            "Joan Serrà",
            "Xavier Serra"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416902",
        "url": "https://doi.org/10.5281/zenodo.1416902",
        "ee": "https://zenodo.org/records/1416902/files/KoduriSS12.pdf",
        "abstract": "Intonation is an important concept in Carnatic music that is characteristic of a raaga, and intrinsic to the musical ex- pression of a performer. In this paper we approach the de- scription of intonation from a computational perspective, obtaining a compact representation of the pitch track of a recording. First, we extract pitch contours from automat- ically selected voice segments. Then, we obtain a a pitch histogram of its full pitch-range, normalized by the tonic frequency, from which each prominent peak is automati- cally labelled and parametrized. We validate such parame- trization by considering an explorative classification task: three raagas are disambiguated using the characterization of a single peak (a task that would seriously challenge a more na¨ıve parametrization). Results show consistent im- provements for this particular task. Furthermore, we per- form a qualitative assessment on a larger collection of raa- gas, showing the discriminative power of the entire repre- sentation. The proposed generic parametrization of the in- tonation histogram should be useful for musically relevant tasks such as performer and instrument characterization.",
        "zenodo_id": 1416902,
        "dblp_key": "conf/ismir/KoduriSS12",
        "keywords": [
            "intonation",
            "Carnatic music",
            "raaga",
            "musical expression",
            "pitch contours",
            "pitch histogram",
            "tonic frequency",
            "prominent peak",
            "parametrization",
            "explorative classification"
        ],
        "content": "CHARACTERIZATION OF INTONATION IN CARNATIC MUSIC BY\nPARAMETRIZING PITCH HISTOGRAMS\nGopala K. Koduri1, Joan Serr `a2, and Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain.\n2Artiﬁcial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain.\ngopala.koduri@upf.edu, jserra@iiia.csic.es, xavier.serra@upf.edu\nABSTRACT\nIntonation is an important concept in Carnatic music that\nis characteristic of a raaga, and intrinsic to the musical ex-\npression of a performer. In this paper we approach the de-\nscription of intonation from a computational perspective,\nobtaining a compact representation of the pitch track of a\nrecording. First, we extract pitch contours from automat-\nically selected voice segments. Then, we obtain a a pitch\nhistogram of its full pitch-range, normalized by the tonic\nfrequency, from which each prominent peak is automati-\ncally labelled and parametrized. We validate such parame-\ntrization by considering an explorative classiﬁcation task:\nthree raagas are disambiguated using the characterization\nof a single peak (a task that would seriously challenge a\nmore na ¨ıve parametrization). Results show consistent im-\nprovements for this particular task. Furthermore, we per-\nform a qualitative assessment on a larger collection of raa-\ngas, showing the discriminative power of the entire repre-\nsentation. The proposed generic parametrization of the in-\ntonation histogram should be useful for musically relevant\ntasks such as performer and instrument characterization.\n1. INTRODUCTION\nCarnatic music is the south Indian art music tradition and\nRaaga is the melodic framework on which Indian art mu-\nsic thrives. The intonation of a single swara1can be dif-\nferent due to the melodic context established by different\nraagas. Therefore, to understand and model Carnatic music\ncomputationally, intonation analysis becomes a fundamen-\ntal step.\nWe deﬁne intonation as the pitches used by a performer\nin a given musical piece. From this deﬁnition our approach\nwill consider a performance of a piece as our intonation\nunit. In Carnatic music practice, it is known that the in-\ntonation of a given swara can vary signiﬁcantly depending\n1A swara-sthana is a frequency region which indicates the note and its\nallowed intonation in different melodic contexts.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.on the artist and the raaga [8, 15]. The study of intona-\ntion differs from that of tuning in its fundamental empha-\nsis. When we talk about tuning we refer to the discrete\nfrequencies with which we tune an instrument, thus it is\nmore of a theoretical concept than the one of intonation,\nwith which we focus in the pitches used during a perfor-\nmance. The two concepts are basically the same when we\nstudy instruments that can only produce a ﬁxed set of dis-\ncrete frequencies, like the piano.\nGiven than in Indian music there is basically no instru-\nment with ﬁxed frequencies (the harmonium is an impor-\ntant exception), in practice tuning and intonation can be\nconsidered the same. Here we will maintain the terms, tun-\ning or intonation, used by the different studies.\nKrishnaswamy [7] discusses various tuning studies in\nthe context of Carnatic music, proposing a hybrid tuning\nscheme based simple frequency ratios plus various tun-\ning systems, specially equal temperament. His work also\npoints out the lack of empirical evidence thus far. Recently,\nSerr`a et al. [12] have shown important quantitative differ-\nences between the tuning systems in modern Carnatic and\nHindustani2musics. In particular, they show that Carnatic\nmusic follows a tuning system which is very close to just-\nintonation, whereas Hindustani music follows a tuning sys-\ntem which tends to be more equi-tempered. Although there\nare several studies on intonation and tuning in Indian mu-\nsic, the emphasis so far has been in the interpretation of\nancient texts rather than on analysing real musical prac-\ntise(see [7, 8, 12] and references therein).\nIn a study that was conducted with Hindustani music\nperformances [8], pitch consistency is shown to be highly\ndependent on the nature of gamaka3usage. The swaras\nsung with gamakas were often found to have a greater vari-\nance within and across the performances, and across dif-\nferent performers. Furthermore, the less dissonant swaras\nwere also found to have greater variance. However, it was\nnoted that across the performances of the same raaga by\na given performer, this variance in intonation was minor.\nThe same work concludes that the swaras used in the an-\nalyzed performances do not strictly adhere to either just-\nintonation or equal-tempered tuning systems. Belle et al [1]\nuse the intonation information of swaras to classify Hin-\ndustani raagas. Another recent experiment conducted with\n2The north Indian art music tradition.\n3Gamakas are a class of short melodic movements sung around and\nbetween swaras.Carnatic music performances draws similar conclusions\nabout the variance in intonation [15]. However, the method-\nology employed in these experiments cannot easily be scaled\nto a larger set of recordings due to the human involvement\nat several phases of the study, primarily in cleaning the data\nand the pitch tracks, and also in interpreting of the obser-\nvations made.\nApproaches to tuning analysis of real musical practise\nusually follow a so-called ‘stable region’ approach, in which\nonly stable frequency regions are considered for the anal-\nysis (cf. [12]). However, it is known [14] that the most\nportion of the performance in Carnatic music is gamaka-\nembellished. Since gamakas are characteristic to a given\nraaga, such an approach is not suitable to understand the\ncrucial information provided by them. So far, the tuning\nanalysis was approached to explain the interval positions\nof Carnatic music with one of the known tuning methods\nlike just-intonation or equal-temperament. But consider-\ning that these intervals are prone to be inﬂuenced by factors\nlike raaga, performer [8] and instrument [7], computational\nanalysis of swara intonation for different raagas, artists and\ninstruments has much more relevance to the Carnatic mu-\nsic tradition.\n2. HISTOGRAM PEAK PARAMETRIZATION\nIn this contribution we propose a methodology based on\nhistogram peak parametrization that helps to describe the\nintonation of a given recording by characterizing the distri-\nbution of pitch values around each swara. From the obser-\nvations made by Krishnaswamy [7] and Subramanian [14],\nit is apparent that steady swaras only tell us part of the\nstory that goes with a given Carnatic music performance.\nThe gamaka-embellished swaras pose a difﬁcult challenge\nfor automatic swara identiﬁcation. Therefore, alternative\nmeans of deriving meaningful information about the into-\nnation of swaras becomes important. The gamakas and the\nrole of a swara are prone to inﬂuence the aggregate dis-\ntribution of a swara. We believe that this information can\nbe derived by parametrizing the distribution around each\nswara.\nOur intonation description method can be broadly di-\nvided into six steps. In the ﬁrst step, the prominently vocal\nsegments of each performance are extracted using a trained\nsupport vector machine (SVM) model. In the second step,\nthe pitch corresponding to the voice is extracted using mul-\ntipitch analysis. In the third step, using all the performan-\nces of each raaga, a pitch histogram for every raaga is com-\nputed and its prominent peaks detected (we will refer to\nthem as reference peaks). In the fourth step, we compute\nthe pitch histogram for each single performance, detecting\nthe relevant peaks and valleys using information from the\noverall histogram of the corresponding raaga. In the ﬁfth\nstep, each peak is characterized by using the valley points\nand an empirical threshold. Finally, in the sixth step, the\nparameters that characterize each of the distributions are\nextracted.2.1 Segmentation\nCleaning the data is a crucial pre-processing step for our\nexperiments. All the Carnatic vocal performances are ac-\ncompanied by a violin and one or more percussion instru-\nments. We just use the sections in which the voice is alone\nor very prominent. In order to do this automatically, we\ntrain an SVM model [5] on 300 minutes of audio data,\nequally split between vocal, violin and percussion sections\nof 10 seconds each. The features extracted from the audio\nand used in the classiﬁcation task are [4]: Mel-frequency\ncepstral coefﬁcients, pitch conﬁdence, spectral ﬂatness,\nspectral ﬂux, spectral rms, spectral rolloff, spectral strong-\npeak, zero crossing rate and tristimulus. This method scores\nan accuracy of 96% in a 10-fold cross validation test.\n2.2 F0 Analysis\nWith the segmentation module in place, we minimize to\na large extent pitch errors due to the interfering accompa-\nnying instruments. However, there is a signiﬁcant number\nof the obtained voice segments in which the violinist ﬁlls\nshort pauses or in which the violin is present in the back-\nground, mimicing the vocalist very closely with a small\ntime lag. This is one of the main problems we encountered\nwhen using pitch tracking algorithms like YIN [3], since\nthe violin was also being tracked in quite a number of por-\ntions. The solution has been to extract the predominant\nmelody [10] using a multi-pitch analysis approach. Then,\ngiven that the pitch accuracy of YIN is better, we com-\npare the pitch obtained from the multi-pitch analysis with\nYIN at each time frame, and we only keep the pitch from\nthose frames where both methods agree within a threshold.\nThough it is a computationally intensive step, this helps in\nobtaining clean pitch tracks, free of f0-estimation and oc-\ntave errors. The frequencies are then converted to cents and\nnormalized with the tonic frequency obtained using [11].\nThe octave information is retained.\n2.3 Histogram Computation\nAs Bozkurt et al. [2] point out, there is a trade-off in choos-\ning the bin resolution of a pitch histogram. A good bin res-\nolution keeps the precision high, but signiﬁcantly affects\nthe peak detection accuracy. However, unlike Turkish-\nmaqam music where the octave is divided into 53 Holdrian\ncommas, Carnatic music uses roughly 12 swaras [13].\nHence, in this context, choosing a ﬁner bin width is not\nas much a problem as it is in Turkish-maqam music. In\norder to retain the preciseness in estimating the parame-\nters for such distribution, we keep the bin resolution at one\ncent. We then compute the histogram Hby placing the\npitch values into their corresponding bins:\nHk=NX\nn=1mk; (1)\nwhere Hkis the k-th bin count, Nis the number of pitch\nvalues, mk= 1 ifck\u0014P(n)\u0014ck+1andmk= 0 other-\nwise, Pis the array of pitch values and (ck; ck+1)are the\nbounds on k-th bin.Features/Classiﬁer Naive Bayes 1-Nearest Neigh. SVM Logistic Regression Random Forest\nMean and Height 63.43% 56.67% 61.81% 56.33% 64.62%\nAll parameters combined 63.76% 68.90% 65.19% 68.86% 70.71%\nTable 1 . Results of an exploration raaga classiﬁcation test with 42 recordings in 3 raagas using different classiﬁers. The\nrandom baseline accuracy in this case is 28.57%.\nFeatures/Classiﬁer Naive Bayes 1-Nearest Neigh. SVM Logistic Regression Random Forest\nMean and Height 39.6% 39.85% 41.25% 43.65% 48.85%\nAll parameters combined 58.05% 67.6% 74.25% 77.45% 74.45%\nTable 2 . Results of an exploration raaga classiﬁcation test with 26 recordings in 2 raagas using different classiﬁers. The\nrandom baseline accuracy is 20% in this case.\nFigure 1. Histograms corresponding to the recordings of\nKalyani raaga shown in thin lines of different colors, to-\ngether with the average histogram labelled with peaks and\nvalleys, shown in thick green line. Only the middle octave,\nwhich contains the most information, is shown.\nIn the histograms of a few performances, we observe\nthat a number of pitch distributions for speciﬁc swaras are\nvery narrow. However we can observe that the distribu-\ntions still play a role in characterizing the performance.\nTo validate this observation, an average histogram is com-\nputed for each raaga with all the performances in the raaga.\nThis histogram has a clearer distribution for each swara in\nthe raaga and it serves as a reliable reference to verify the\npeaks identiﬁed in individual performances (Fig. 1).\n2.4 Peak Detection and parametrization\nA given histogram is convolved with a Gaussian kernel us-\ning a standard deviation of ﬁve cents. This step is neces-\nsary for the peak detection algorithm to avoid identifying\nthe spurious peaks. The peaks are identiﬁed using a depth\nparameter (D p) and with an empirically set lookahead pa-\nrameter (L p). A local maxima is labelled as a peak only\nif it has valleys deeper than Dpon either side, and is also\nthe maxima at least within Lpvalues ahead. In the case\nofHavg,DpandLpare set high (in our experiment, theycorrespond to 2:5\u000110\u00005and 20 respectively), which re-\nsult in fewer, but reliable peaks. In addition, for each peak\ninHavg, an upper and lower octave peak is added if there\ndoes not already exist a peak in a given proximity. We call\nthese extended reference peaks. To compute the histogram\nof a given performance, DpandLpare set to lower val-\nues (in our experiment, they correspond to 2\u000110\u00005and\n15 respectively), which result in more peaks which include\nseveral unwanted ones. However, only those peaks which\nhave a corresponding match in extended reference peaks\nofHavg(within a given proximity) are retained. This step\nnot only helps to identify all the possible peaks for a given\nperformance, but also compensates the choice of higher\nbin resolution, which otherwise generally results in some\nunwanted peaks.\nIn order to parametrize a given peak in the performance,\nit needs to be a bounded distribution. Generally, we ob-\nserve that two adjacent peaks are at least 80 cents apart.\nThe valley point between the peaks becomes a reasonable\nbound if the next peak is close by. But in cases where they\nare not, we have used a 50 cent bound to limit the distri-\nbution. The peak is then characterized by ﬁve parameters:\npeak location, mean, variance, skew and kurtosis.\n3. RESULTS & DISCUSSION\nOne of the crucial factors that inﬂuence intonation of a\ngiven swara is raaga. This can be attributed to the factors\nlike the role the swara plays in the raaga, the gamakas used\nwith it, and the characteristics of neighbouring swaras. For\na given swara, all of them change with the raaga [13]. This\naffects the distribution of pitches around it. Therefore,\nwe choose to evaluate our approach by using intonation\nof swaras to characterize raagas. Hence, we focused on a\ndata set which is representative enough of the variations al-\nlowed on swaras: 170 performances in 16 raagas each with\nat least 5 recordings per raaga are selected. These perfor-\nmances feature 35 vocal artists in total.\nWe evaluate our approach using three self-contained\ntasks. The ﬁrst one is an explorative raaga disambigua-\ntion task in which the proposed parameters of the peak are\nshown to consistently increase the accuracy of the system.\nThe second task is a qualitative study showing the use-Figure 2. (a). Kurtosis values for swaras of Bhairavi, Mukhari and Manji raagas. (b). Pearson’s ﬁrst skewness coefﬁcient\nvalues for swaras of Bhairavi, Mukhari and Manji raagas. (c). Kurtosis values for swaras of Begada and Kambhoji raagas.\n(d). Variance values for swaras of Begada and Kambhoji raagas. The x-axis corresponds to swara names in all the subplots.\nfulness of peak parameters in discriminating raagas which\nshare the same set of swaras. In the third task, the peak po-\nsitions are used in deriving a general template for preferred\nmean values of swaras. Further, this general template is\njuxtaposed against different raagas showing notable devia-\ntions.\n3.1 Raaga classiﬁcation task\nPrevious raaga classiﬁcation techniques employ only two\nparameters extracted by histogram analysis: peak position\nand height [6]. However, in using just these two parameters\nfor classifying raagas which share the same set of swaras,\nthere is a high chance of error. In order to assert the use-\nfulness of our approach to describe intonation, we take 42\nrecordings in three raagas (Bhairavi, Thodi and Hindolam)\nwhich share ﬁve common swaras. Due to the limitation on\nthe number of available recordings, choosing many swaras\nin our task will make it difﬁcult to assess the complemen-\ntarity of the new parameters, and could also potentially re-\nsult in over-ﬁtting (more features than instances). There-\nfore, we chose one swara to perform the raaga classiﬁca-\ntion task.\nThe task is performed using two feature sets, both hav-\ning four features. One set consists of just the position\nand height of the swara in the middle and upper octaves\n(common swara parametrization, hence used as a base-\nline). To ensure fairness, we have used two feature se-\nlection methods and different classiﬁers [5, 9]4over sub-\n4The implementations provided in Weka were used with default pa-\nrameters.\nFigure 3. Skewness values for swaras of Surati and\nKedaragowla raagas.\nsampled data sets in a 10-fold cross validation test. The\nother feature set is obtained by using information gain-\nbased and correlation-based feature selection methods [9]\non a combination of position, height, skewness, kurtosis,\nvariance and the mean of the distribution. Both feature se-\nlection methods select new parameters different from the\nposition and the height.\nTable 1 shows the averaged results obtained. Though\nthe accuracy increments are not statistically signiﬁcant, they\nare indicative of the worthiness of additional parameters.\nHowever, if we just consider the classiﬁcation of two raa-\ngas with 26 recordings and perform the same test, the re-\nsults show a signiﬁcant increase in the overall accuracy\n(Table 2). These two explorative tasks show a consistent\nincrease in the accuracy of the system, which indicatesthe usefulness and complementarity of the proposed peak\ncharacteristics.\n3.2 Allied Raagas\nIn the experiment described in the previous section the raa-\ngas share a subset of the swaras. However, there is a class\nof raagas which share exactly the same set of swaras, but\nhave different characteristics, called allied raagas. Since\nthe swaras are common for the raagas, the discriminative\ncapability of the peak position and/or mean will be con-\nsiderably low. Therefore, these raagas constitute a good\nrepertoire to test our approach. We consider three sets of\nallied raagas which together have 60 recordings in 7 raa-\ngas. The ﬁrst set has three raagas, and the second and the\nthird sets have two raagas each.\nFig. 2 (a) shows the kurtosis values for swaras of the\nﬁrst set of allied raagas (Bhairavi, Mukhari and Manji). In\nall the ﬁgures that follow, we have only shown the val-\nues for the most relevant swaras due to space constraints.\nR2=G^\n15andM1swaras can be seen to play a notable role\nin discriminating the raagas in this set. Fig. 2 (b) shows the\nskewness values for swaras of the same set of allied raagas.\nThe distinction is observed even better through the skew-\nness values of D2=N1,PandP^swaras. Generally, in a\ngiven raaga, the melodic context of a swara is kept consis-\ntent across octaves, which means that the intonation char-\nacteristics of a swara across octaves should be consistent.\nThe skewness values of PandP^swaras assert this.\nFigs. 2 (c) & (d) show the kurtosis and the variance val-\nues, respectively, for swaras of the second set of allied raa-\ngas (Begada and Kambhoji). All the swaras can be ob-\nserved to play an equal role in distinguishing raagas of this\nset. The consistency of the variances of G3, andR2=G1,\nand kurtosis of G3andD2=N1across octaves is quite ev-\nident. Fig. 3 shows the skewness values for swaras of the\nthird set of allied raagas (Surati and Kedaragowla). The\nobservations from this set further reinforce the usefulness\nof the peak parametrization approach to describe intona-\ntion.\n3.3 Analysis of peak positions\nTable 3 shows the average of peak positions of each swara\nacross all the available recordings (we now use the full\ndata set), and the absolute sum of the differences of all\nobservations from the corresponding equi-tempered and\njust-intonation intervals. The swaras which are observed\nin less than 20 recordings are not shown. There is a gen-\neral tendency to just-intonation intervals compared to equi-\ntempered, which is in agreement with the results obtained\nby Serr `a et al. [12]. However, this tendency is not very ev-\nident, supporting the claims in [7]. What interests us more\nhere is the relevance of the values shown in the table for\nunderstanding the intonation of swaras in different raagas.\nFor that we consider the set of the average values (Table 3)\nfor each swara to be a general template. A similar template\nis obtained for each raaga, and the differences between the\n5^denotes the swara in upper octave.Swara Mean DE DJ Recordings\nSa 1.92 6.43 6.43 142\nR2=G1 200.93 10.93 11.62 68\nG3 384.67 16.34 10.51 56\nM1 495.05 9.94 9.56 123\nP 700.18 6.01 6.21 164\nD2=N 1 889.58 13.99 11.06 87\nD3=N 2 987.87 16.0 14.11 56\nSa^1196.62 6.35 6.35 174\nR2=G1^1401.37 8.86 8.97 96\nG3^1583.63 17.21 10.04 61\nM1^1693.05 12.71 12.17 91\nP^1897.86 7.45 8.08 118\nD2=N 1^2095.6 8.78 13.15 54\nD3=N 2^2192.15 14.17 13.33 28\nTable 3. Mean of peak positions of each swara, and the\ndifferences from corresponding Equi-tempered (D E) and\nJust-Intonation (D J) intervals.\ntwo templates are analysed and interpreted to check if they\nare musically meaningful.\nFigs. 4 (a), (b) and (c) show the boxplots for positions of\nD2,N2andM1respectively, in various raagas. They were\nexamined by a trained musician who interpreted them, and\nasserted that they made sense in the context of today’s\npractice of the raagas. For instance, it is said that D2\nin Khamas raaga is sung without any gamaka, whereas\nthe same swara in Kalyani raaga is sung with a particular\ngamaka that might have been responsible for the observed\nphenomenon. This explains the observations made from\nFig. 4 (d). Furthermore, as a sanity test, we have plotted the\npositions of the swara Pin various raagas. This swara is\nnormally expected to be sung without any gamaka. Hence,\nwe expect the peak positions corresponding to Pof all the\nraagas to be centered around the mean position observed\nin the general template (Table 3). Fig. 4 asserts this, except\nfor minor deviations.\n4. CONCLUSIONS\nWe have proposed a peak parametrization approach to de-\nscribe intonation in Carnatic music and evaluated it quali-\ntatively using three tasks. All the tasks discriminate raagas\nwith the obtained information of swara intonation. How-\never there are a few challenges in this approach. Few swaras,\nby the nature of the role they play, will not be manifested as\npeaks at all. Rather, they will appear as a slide that cannot\nbe identiﬁed by a peak detection algorithm. Characteriz-\ning pitch distributions near all the theoretical intervals or\nfrom the general template shown in Sec. 3.3, irrespective\nof whether it is identiﬁed as a peak or not, is one possi-\nble way to address this issue. However, identifying few\nheuristics that will help in locating such slides can be a\ngood substitute, since it falls in line with our methodology\nin not assuming any particular tuning. The future direction\nof this work is to extend it to the Hindustani music tradi-Figure 4. (a). Peak positions of D2for recordings in Khamas, Kalyani and Sourashtram raagas. (b). Peak positions of N2\nfor recordings in Ananda Bhairavi, Hindoam, Khamas, and Sourashtram raagas. (c). Peak positions of M1for recordings\nin several raagas. (d). Peak positions of Pfor recordings in several raagas. The dashed line shows the mean of the\ncorresponding swara obtained from the general template.\ntion, and also to characterize performers and instruments\nby their preferred intonation.\n5. ACKNOWLEDGMENTS\nThis research was partly funded by the European Research\nCouncil under the European Union’s Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583). JS acknowledges JAEDOC069/2010\nfrom Consejo Superior de Investigaciones Cient ´ıﬁcas, 2009-\nSGR-1434 from Generalitat de Catalunya, TIN2009-13692-\nC03-01 from the Spanish Government, and EU Feder Funds.\n6. REFERENCES\n[1] Shreyas Belle, Rushikesh Joshi, and Preeti Rao. Raga Iden-\ntiﬁcation by using Swara Intonation. Journal of ITC Sangeet\nResearch Academy, 23, 2009.\n[2] B. Bozkurt, O. Yarman, M. K. Karaosmanolu, and C. Akkoc ¸.\nWeighing Diverse Theoretical Models on Turkish Maqam\nMusic Against Pitch Measurements: A Comparison of Peaks\nAutomatically Derived from Frequency Histograms with\nProposed Scale Tones. Journal of New Music Research,\n38(1):45–70, 2009.\n[3] A. de Cheveign ´e and H. Kawahara. YIN, a fundamental fre-\nquency estimator for speech and music. The Journal of the\nAcoustical Society of America, 111(4):1917–1930, 2002.\n[4] J. D. Deng, C. Simmermacher, and S. Craneﬁeld. A Study\non Feature Analysis for Musical Instrument Classiﬁcation.\nSystems, Man, and Cybernetics, Part B: Cybernetics, IEEE\nTransactions on, 38(2):429–438, 2008.\n[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,\nand I. H Witten. The WEKA data mining software: an update.\nSIGKDD Explor. Newsl., 11(1):10–18, 2009.[6] G. K. Koduri, P. Rao, and S. Gulati. A Survey Of Raaga\nRecognition Techniques And Improvements To The State-Of-\nThe-Art. In Sound and Music Computing, 2011.\n[7] A. Krishnaswamy. On the twelve basic intervals in South In-\ndian classical music. Audio Engineering Society Convention,\npage 5903, 2003.\n[8] M. Levy. Intonation in North Indian Music. Biblia Implex\nPvt. Ltd, New Delhi, 1982.\n[9] T. Ngo. Data mining: practical machine learning tools and\ntechnique, third edition by Ian H. Witten, Eibe Frank, Mark\nA. Hell. SIGSOFT Softw. Eng. Notes, 36(5):51–52, 2011.\n[10] Justin Salamon and E. G ´omez. Melody extraction from poly-\nphonic music signals using pitch contour characteristics. Au-\ndio, Speech, and Language Processing, IEEE Transactions\non, (99):1–1, 2012.\n[11] Justin Salamon, Sankalp Gulati, and Xavier Serra. A Mul-\ntipitch Approach to Tonic Identiﬁcation in Indian Classical\nMusic. In ISMIR, page In press, 2012.\n[12] J. Serr `a, G. K. Koduri, M. Miron, and X. Serra. Assessing\nthe tuning of sung indian classical music. In ISMIR, pages\n157–162, 2011.\n[13] V . Shankar. The art and science of Carnatic music. Music\nAcademy Madras, Chennai, 1983.\n[14] M. Subramanian. Carnatic Ragam Thodi Pitch Analysis\nof Notes and Gamakams. Journal of the Sangeet Natak\nAkademi, XLI(1):3–28, 2007.\n[15] D. Swathi. Analysis of Carnatic Music : A Signal Processing\nPerspective. Masters thesis, IIT Madras, 2009."
    },
    {
        "title": "Unsupervised Chord-Sequence Generation from an Audio Example.",
        "author": [
            "Katerina Kosta",
            "Marco Marchini",
            "Hendrik Purwins"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415534",
        "url": "https://doi.org/10.5281/zenodo.1415534",
        "ee": "https://zenodo.org/records/1415534/files/KostaMP12.pdf",
        "abstract": "A system is presented that generates a sound sequence from an original audio chord sequence, having the following characteristics: The generation can be arbitrarily long, pre- serves certain musical characteristics of the original and has a reasonable degree of interestingness. The proce- dure comprises the following steps: 1) chord segmentation by onset detection, 2) representation as Constant Q Pro- files, 3) multi-level clustering, 4) cluster level selection, 5) metrical analysis, 6) building of a suffix tree, 7) gen- eration heuristics. The system can be seen as a computa- tional model of the cognition of harmony consisting of an unsupervised formation of harmonic categories (via multi- level clustering) and a sequence learning module (via suf- fix trees) which in turn controls the harmonic categoriza- tion in a top-down manner (via a measure of regularity). In the final synthesis, the system recombines the audio ma- terial derived from the sample itself and it is able to learn various harmonic styles. The system is applied to various musical styles and is then evaluated subjectively by mu- sicians and non-musicians, showing that it is capable of producing sequences that maintain certain musical charac- teristics of the original.",
        "zenodo_id": 1415534,
        "dblp_key": "conf/ismir/KostaMP12",
        "keywords": [
            "sound sequence",
            "chord sequence",
            "arbitrarily long",
            "musical characteristics",
            "interestingness",
            "chord segmentation",
            "Constant Q Profiles",
            "multi-level clustering",
            "cluster level selection",
            "metrical analysis"
        ],
        "content": "UNSUPERVISED CHORD-SEQUENCE GENERATION FROM AN AUDIO\nEXAMPLE\nKaterina Kosta1;2, Marco Marchini2, Hendrik Purwins2;3\n1Centre for Digital Music, Queen Mary, University of London, Mile End Road, London E1 4NS, UK\n2Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain\n3Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany\nmarco.marchini@upf.edu, katkost@gmail.com, hpurwins@gmail.com\nABSTRACT\nA system is presented that generates a sound sequence from\nan original audio chord sequence, having the following\ncharacteristics: The generation can be arbitrarily long, pre-\nserves certain musical characteristics of the original and\nhas a reasonable degree of interestingness. The proce-\ndure comprises the following steps: 1) chord segmentation\nby onset detection, 2) representation as Constant Q Pro-\nﬁles, 3) multi-level clustering, 4) cluster level selection,\n5) metrical analysis, 6) building of a sufﬁx tree, 7) gen-\neration heuristics. The system can be seen as a computa-\ntional model of the cognition of harmony consisting of an\nunsupervised formation of harmonic categories (via multi-\nlevel clustering) and a sequence learning module (via suf-\nﬁx trees) which in turn controls the harmonic categoriza-\ntion in a top-down manner (via a measure of regularity). In\nthe ﬁnal synthesis, the system recombines the audio ma-\nterial derived from the sample itself and it is able to learn\nvarious harmonic styles. The system is applied to various\nmusical styles and is then evaluated subjectively by mu-\nsicians and non-musicians, showing that it is capable of\nproducing sequences that maintain certain musical charac-\nteristics of the original.\n1. INTRODUCTION\nTo what extent can a mathematical structure tell an emo-\ntional story? Can a system based on a probabilistic con-\ncept serve the purpose of composition? Iannis Xenakis dis-\ncussed the role of causality in music in his book “Formal-\nized Music, Thought and Mathematics in Composition”,\nwhere it is mentioned that a fertile transformation based\non the emergence of statistical theories in physics played a\ncrucial role in music construction and composition [20].\nStatistical musical sequence generation dates back to\nMozart’s “Musikalisches W ¨urfelspiel” (1787) [8], and more\nrecently to “The Continuator” by F. Pachet [14], D. Con-\nklin’s work [3], the “Audio oracle” by S. Dubnov et al.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.[6] and the “Rhythm Continuator” by M. Marchini and\nH. Purwins (2010) [13]. The latter system [13] learns the\nstructure of an audio recording of a rhythmical percussion\nfragment in an unsupervised manner and synthesizes mu-\nsical variations from it. In the current paper this method\nis applied to chord sequences. It is related to work such\nas a harmonisation system described in [1] which, using\nHidden Markov Models, it composes new harmonisations\nlearned from a set of Bach chorals.\nThe results help to understand harmony as an emergent\ncognitive process and our system can be seen as a music\ncognition model of harmony. “Expectation plays an im-\nportant role in various aspects of music cognition” [18]. In\nparticular, this holds true for harmony.\n2. CHORD GROUPING\nHarmony is a unique feature distinguishing Western music\nfrom most other predominantly monophonic music tradi-\ntions. Different theories account for the phenomenon of\nharmony, mapping chords e.g. to three main harmonic\nfunctions, seven scale degrees, or even ﬁner subdivisions\nof chord groups, such as separating triads from seventh or\nninth chords. The aim of this paper is to suggest an unsu-\npervised model that lets such harmonic categories emerge\nfrom samples of a particular music style and model their\nstatistical dependencies.\nAs Piston remarks in [15] (p. 31), “each scale degree\nhas its part in the scheme of tonality, its tonal function”.\nFunction theory by Riemann concerns the meanings of the\nchords which progressions link. The term “function” can\nbe used in a stronger sense as well, for specifying a chord\nprogression [10]. A problem arises from the fact that scale\ndegrees cannot be mapped to the tonal functions in a unique\nway [4] [16] (p. 51-55). In our framework, the function of\na chord emerges from its cluster and its statistical depen-\ndency on the other chord clusters.\nIt is considered that the tonic (I), dominant (V) and\nsubdominant (IV) triads constitute the tonal degrees since\n“they are the mainstay of the tonality” and that the last two\ngive an impression of “balanced support of the tonic” [15].\nThis hierarchy of harmonic stability has been supported by\npsychological studies as well. One approach involves col-\nlecting ratings of how one chord follows from another. As\nit is mentioned in [11], Krumhansl, Bharucha, and Kesslerused such judgments to perform multidimensional scaling\nand hierarchical clustering techniques [9]. The psycholog-\nical distances between chords reﬂected both key member-\nship and stability within the key; “chords belonging to dif-\nferent keys grouped together with the most stable chords\nin each key (I, V , and IV) forming an even smaller clus-\nter. Such rating methods also suggest that the harmonic\nstability of each chord in a pair affects its perceived rela-\ntionship to the other, and this depends upon the stability of\nthe second chord in particular” [9].\n3. METHODOLOGY\nThe goal of this system is the analysis of a chord sequence\ngiven as audio input, with the aim of generating arbitrar-\nily long, musically meaningful and interesting sound se-\nquences maintaining the characteristics of the input sam-\nple.\nFrom audio guitar and piano chord sequences, we de-\ntect onsets, key and tempo, and group the chords, applying\nagglomerative clustering. Then, Variable Length Markov\nChains (VLMCs) are used as a sequence model. In Fig-\nure 1 the general architecture is presented.\naudio\ninput   chord segments  chord grouping   onset detection\nclustering   modelCQ-profiles\n audiooutputre-shuffle  chordsVLMCmodel\nnew chordsequence \nFigure 1. General system architecture.\n3.1 Onset Detection\nIn order to segment the audio into a sequence of chords\nwe employed an onset detection algorithm. Different ap-\nproaches have been considered since a simpliﬁed onset de-\ntection method based only on the energy envelope would\nnot be sufﬁcient. After trying a bunch of available algo-\nrithms from the literature we found that the complexdo-\nmain from Aubio [21] was suited for our propose.\nA crucial parameter of this algorithm is the sensitivity\nwhich required an ad hoc tuning. We selected a piano\nperformance of Bach’s choral ”An Wasserﬂussen Babylon\n(Vergl. Nr. 209) in G major - from here on referred as\n“test -Bach choral” - as a ground truth test set for onset de-\ntection. Although with an optimal sensitivity we were still\nobtaining an incorrect merge of two consecutive segments\nin the 5.88% of the cases out of a total of 68 segments con-\nsidered. In Figure 2, the ﬁrst ﬁve segments that were ob-\ntained for the test-Bach choral are presented. An example\nof incorrect merge is shown on the 5th segment, the twoconsecutive chords of which get still gathered together, as\ntheir common notes are still resonating during the passing.\n1 2 345\nFigure 2. The ﬁrst 5 segments of the test - Bach choral us-\ning Aubio [21] for onset detection. The ﬁfth excerpt should\nbe splitted into two parts -vertical black line- since two dif-\nferent kind of chords are identiﬁed and could be used sep-\narately.\n3.2 Constant Q Proﬁles and Sound Clustering\nFrom the audio input we extract chroma information based\non Constant Q (CQ) proﬁles, which are 12 - dimensional\nvectors, each component referring to a pitch class. The\nidea is that every proﬁle should reﬂect the tonal hierarchy\nthat is characteristic for its key [2].\nThe calculation of the CQ proﬁles is based on the CQ\ntransform; as decribed by Schorkhuber and Klapuri in [19],\n“it refers to a time-frequency representation where the fre-\nquency bins are geometrically spaced and the Q factors\nwhich are ratios of the center frequencies to bandwidths,\nof all bins are equal”. This is the main difference between\nthe CQ transform and Fourier transform. In our implemen-\ntation we have used 36 bins per octave, the square root of\na Blackman-Harris window and a hop size equal to 50%\nof the window size. The CQ proﬁles are closely related\nto the probe tone ratings by Krumhansl [17]. Also the sys-\ntem employs a method described by Dixon in [5] for tempo\nestimation.\nIn the clustering part, as each event is characterized by\na 12-dimensional vector, they can thus be seen as points in\na 12-dimensional space in which a metric is induced by the\nEuclidean distance. The single linkage algorithm has been\nused to discover event clusters in this space. As deﬁned\nin [13], this algorithm recursively performs clustering in a\nbottom-up manner. Points are grouped into clusters. Then\nclusters are merged with additional points and clusters are\nmerged with clusters into super clusters. The distance be-\ntween two clusters is deﬁned as the shortest distance be-\ntween two points, each in a different cluster, yielding a bi-\nnary tree representation of the point similarities. The leaf\nnodes correspond to single events. Each node of the treeoccurs at a certain height - level, representing the distance\nbetween the two child-nodes (cf. [7] p. 517-557 for de-\ntails).\nThen the regularity concept described in [13] is com-\nputed for each sequence of each clustering level. Firstly,\nwe compute the histogram of the time differences (CIOIH)\nbetween all possible combinations of two onsets. What we\nobtain is a sort of harmonic series of peaks that are more\nor less prominent according to the self-similarity of the se-\nquence on different scales. Secondly, we compute the au-\ntocorrelation ac(t) (where tis the time in seconds) of the\nCIOIH which, in case of a regular sequence, has peaks at\nmultiples of its tempo. Let tuspbe the positive time value\ncorresponding to its upper side peak. Given the sequence\nofmonsets x= (x 1; : : : ; x m)we deﬁne the regularity of\nthe sequence of onsets xto be:\nRegularity(x) =ac(t usp)\n1\ntuspRtusp\n0ac(t)dtlog(m)\nThis regularity is then used to select the most regular level\nfor tempo detection and a small amount of representative\nlevels for the VLMC generation.\nIn Figure 3, there is a tree representation of the cluster-\ning results for the audio test - Bach choral. The system has\nselected 10 clustering levels, and the cluster hierarchy for\nthe levels 1 - 6 is presented. We have only considered the\nclusters with more than one element.\n12345678Level 2Level 3Level 4Level 5Level 6\n9121011131415161718\n19\nFigure 3. Base line: the clusters generated at Level 1 as\ncircles; the black ones contain one single element.\nIn Table 1, the clustering results on levels 1 - 4 of the\nanalyzed Bach choral are shown in more detail. It is noti-\ncable that we get a rich group, containing a large amount\nof G Major dominant chords.\n3.3 Statistical Model for Sequence Generating\nHaving the segments of the input sound categorized prop-\nerly, the next step is to re-generate them in a different or-\nder than the original one, taking into account that they are\nnot independent and identically distributed, but dependent\non the previous segments. For implementing this idea itCluster # of Elements Recognition\nLevel 1:\ncl. 1 3 2 G I, 1 G V\ncl. 2 3 1 G I, 1 a V , 1 d IV\ncl. 3 2 2 G IV\ncl. 4 2 1 G I, 1 G V\ncl. 5 10 5 G V , 1 a I, 1 d I,\n1 d VI, 1 d V , 1 G I\ncl. 6 2 1 G IV , 1 a I\ncl. 7 4 1 G II, 1 a V , 1 a I,\n1 d V\ncl. 8 2 2 G V\nLevel 2:\ncl. 9 (cl.5)+2 6 G V , 1 a I, 2 d I,\n1 d VI, 1 d V , 1 G I\ncl. 10 (cl.2+cl.7)+1 1 G I, 2 a V , 1 d IV ,\n1 G II, 1 a I, 1 d V\ncl. 11 (cl.4)+1 2 G I, 1 G V\ncl. 12 (cl.1+cl.6)+1 2 G I, 1 G V , 1 G IV ,\n1 a I\nLevel 3:\ncl. 13 (cl.11)+1 3 G I, 1 G V\ncl. 14 (cl.3+cl.9+cl.10) 2 G I, 1 G II, 2 G IV ,\n+2 6 G V , 2 a I, 2 a V ,\n2 d I, 1 d IV , 2 d V ,\n1 d VI\nLevel 4:\ncl. 15 (cl.8+cl.13+cl.14) 9 G V , 5 G I, 1 G II,\n+ 2 2 G IV , 2 a I, 2 a V ,\n3 d I, 1 d IV , 2 d V ,\n2 d VI\nTable 1. the clustering results on levels 1 - 4 of the an-\nalyzed Bach choral. At the ﬁrst column, we deﬁne each\ncluster by a number and at the second column we present\nthe number of elements inside that cluster. At the third col-\numn we recognize these elements and label them based on\nour score’ s harmonic analysis for each one separately (for\nexample: “2 G I” means “2 of the elements are the root\nof G major” and “5 a V” means “5 of the elements are the\ndominant of A minor”).\nwould be impractical to consider a general dependence of\nfuture observations on all previous observations because\nthe complexity of such a model would grow without limit\nas the number of observations increases. This leads us to\nconsider Markov models in which we assume that future\npredictions are independent of all but the most recent ob-\nservations.\nA VLMC of order p is a Markov chain of order p, with\nthe additional attractive structure that its memory depends\non a variable number of lagged values [12]. This can be\nevaluated on our system as follows; Let’s assume that we\nhave, as an input, two sequences of events - elements of a\ncategorical space having length `= 4. Be (A,B,C,A) and\n(B,C,C,D), which are parsed from right to left. As seen in\n[14], context trees are created where a list of continuations\nencountered in the corpus are attached to each tree node.The ”continuations” are integer numbers which denote the\nindex of continuation item in the input sequence. In Fig-\nure 4, the procedure of the context tree creation based on\nsequences (A,B,C,A) and (B,C,C,D) is shown, where the\nindex numbers show with which element one can proceed.\n(A, B, C, A)1  2 34\n(A, B)\n(A, B, C)\n(A, B, C, A)A{2}\nB\nA{3}\n{3}\nC\nB\nA{4}\n{4}\n{4}e\nC\nB\nA{4}\n{4}\n{4}B\nA{3}\n{3}A{2}e\nC\nB{8}\n{8}C\nB{7,8}\n{7}B{6}(B, C, C, D)5  6 78\n(B, C)\n(B, C, C)\n(B, C, C, D)B{6}\nC\nB{7}\n{7}\nC\nC\nB{8}\n{8}\n{8}\ne\nC\nB\nA{4, 7, 8 }\n{4, 7}\n{4}B\nA{3, 6}\n{3}A{2}\nC\nB{8}\n{8}\nFigure 4. Top left and right: Context trees built from the\nanalysis of the sequences (A B C A) and (B C C D) respec-\ntively. Bottom: Merge of the context trees above.\nExploring the ﬁnal graph in Figure 4, where the trees\nabove are merged, we have all the possible sequence sit-\nuations, following each path that is created from bottom\nto up and considering the index number of the ﬁrst ele-\nment. For example, if we want to ﬁnd which is the next\nelement of the sequence (A,B,C), we follow this speciﬁc\npath from the bottom of the tree and then we see the in-\ndex number of the ﬁrst element, A, so we take the element\nwith this index number, which is A and the sequence now\nbecomes (A,B,C,A). For ”e” (the empty context) we con-\nsider a random selection of any event. Also the length `\ncan be variable.\nFor the generation we use the sufﬁx trees for all previ-\nously selected levels. If we ﬁx a particular level, the con-\ntinuation indices are drawn according to a posterior proba-\nbility distribution determined by the longest context found.\nDepending on the sequence, it could be better to do pre-\ndictions based either on a coarse or a ﬁne level. In order\nto increase recombination of blocks and still provide good\ncontinuation we employ the heuristics detailed in Section\n3.1. in [13] taking into account multiple levels for the pre-\ndiction.\n4. EV ALUATION\nFive audio inputs have been selected to evaluate the method:\na guitar chord sequence based on the song “If I fell in\nlove with you” by the Beatles, a Bach choral played on\nthe piano, part of the “Funeral March” by Chopin, a guitar\nﬂamenco excerpt and a piano chord sequence by a non-\nmusician (Examples No.1-5).\nThe next step was to create generations, using these ﬁve\ndifferent piano and guitar audio inputs followed each one\nby generations of one minute duration. All the audio ex-\namples, some meta data, as well as the generations, and theresults of the evaluation are available on the web site [22].\nThere are two carefully selected generations presented per\npiece, except for Example No. 5, where there is only one.\nThe following characteristics of the system are assessed:\nthe selected clustering level, the similarity between the in-\nput sample and the generation, and how many times an\nevent is followed by another event in the generation that\nis not the event’s successor in the original (i.e. how many\n“jumps” the generated sound contains).\nSince the opinion of a musician rather than an objective\nmeasure is a more suitable evaluation measure for the aes-\nthetic value of a generated music sample, a questionnaire\nfor each input and its generations was created and given to\nﬁve musicians1and ﬁve non-musicians at ages between 22\nand 28. They had to listen to and rate each audio (from 1-\n“not at all” to 5- “very much”) for their familiarity with the\npiece and the interestingness of the piece. In addition, the\nsubject had to select the most interesting 10-second parts\nof it and they had to determine a similarity value compar-\ning two audio examples. Original and the generations were\npresented without indicating which was which. For Ex-\namples 2 and 3 (Bach and Chopin) another question was\nadded, asking to rate how clear the structure of the piece\nis.\nThrough the results of this experiment (details in Table\n2), we can highlight that only 3% of the responses found\nthe generation example as not similar to the original input.\nAlso through the Examples 1, 4 and 5 we notice that 20%\nof the responses found the generation example more inter-\nesting than the original and 26% of the responses found\nthe generation example less interesting, although the range\nfrom the rate of the original one is not big.\nIn general the cumulative results for the similarity mod-\nule show small differences between musician’s and non-\nmusician’s replies. Another measure of comparison be-\ntween these groups is their response concerning the 10\nmost interesting seconds; ten groups of overlapping sec-\nonds have emerged and seven of these groups were indi-\ncated by both musicians and non-musicians.\nThe comments made by the subjects gave us additional\ninsight into the behaviour of the system. Metrical phase er-\nrors have been spotted in the generations of Example No.\n4, resulting in rhythmic pattern discontinuities. Some of\nthe musician subjects considered these sections as “confus-\ning” and some others as “intriguing expertise”. Another\nimportant issue is the quality of the generation, in terms of\nits harmonic structure. A representative comment on Ex-\nample No.5 is: “In the second audio (i.e. the Original) I\ncould hear more harmonically false sequences”.\n5. DISCUSSION AND CONCLUSION\nThe system generates harmonic chord sequences from a\ngiven example, combining machine learning and signal pro-\ncessing techniques. As the questionnaire results highlight,\nthe generation is similar to the original sample, maintain-\n1They are deﬁned as individuals, having at least ﬁve years of music\ntheory studies and instrument playing experience.ing key features of the latter, with a relatively high degree\nof interestingness.\nAn important extension of this work would incorporate\nand learn structural constraints as closing formulae and\nmusical form. Other future work comprises an in-depth\ncomparison of the chord taxonomies generated by the sys-\ntem and taxonomies suggested by various music theorists,\ne.g. Riemann, Rameau, or the theory of jazz harmony and\npossibly the experimental veriﬁcation of such harmonic\ncategories in the brain, e.g. in an EEG experiment.\nHowever, for an automatic music generation system,\nthere remains still a long way to go in order to comply with\nthe idea of music as Jani Christou puts it: ”The function of\nmusic is to create soul, by creating conditions for myth, the\nroot of all soul”.\n6. ACKNOWLEDGEMENT\nThe work of the second author (M. M.) was supported\nin part by the European project ”Social Interaction and\nEntrainment using Music PeRformance Experimentation”\n(SIEMPRE, Ref. No. 250026). The work of the third\nauthor (H. P.) was supported in part by the German Bun-\ndesministerium f ¨ur Forschung und Technologie (BMBF),\nGrant No. Fkz 01GQ0850:\n7. REFERENCES\n[1]Moray Allan and Christopher K. I. Williams, Harmon-\nising Chorales by Probabilistic Inference, Advances in Neu-\nral Information Processing Systems 17, 2005.\n[2]J. C. Brown, M. S. Puckette. An efﬁcient algorithm for\nthe calculation of a constant Q transform.,J. Acoust. Soc.\nAm., 92(5):2698-2701, 1992.\n[3]D. Conklin, Music Generation from Statistical Models,\nProceedings AISB, p. 30-35, 2003.\n[4]C. Dahlhaus, Untersuchungen ber die Entstehung der\nharmonischen Tonalitt, volume 2 of Saarbrcker Studien zur\nMusikwissenschaft. Brenreiter-Verlag, Kassel, 1967\n[5]S. Dixon, Automatic extraction of tempo and beat from\nexpressive performances, Journal of New Music Research,\n30(1):39-58, 2001.\n[6]S. Dubnov, G. Assayag, A. Cont, Audio Oracle: A New\nAlgorithm for Fast Learning of Audio Structures, in Pro-\nceedings of ICMC, 2007.\n[7]R. O. Duda, Peter E. Hartl, D. G. Stork, Pattern Classi-\nﬁcation (2nd edition), 2001.\n[8]K. Jones, Dicing with Mozart: Just a whimsical musi-\ncians in the 18th, NewScientist, Physics & Math, 1991.\n[9]T. Justus, J. Bharucha, Stevens’ Handbook of Experi-\nmental Psychology, V olume 1: Sensation and Perception,\nThird Edition, pp. 453-492, New York: Wiley, 2002.\n[10]D. Kopp, On the Function of Function, Music Theory\nOnline, Society for Music Theory, V olume 1, Number 3,\nMay 1995, ISSN: 1067-3040, 1995.\n[11] Krumhansl, C.L., Bharucha, J.J., Kessler, E.J. Per-\nceived harmonic structure of chords in three related mu-\nsical keys. Journal of Experimental Psychology: Human\nPerception and Performance, vol. 8, pp. 24-36, 1982.[12] M. Machler, P. Buhlmann, Variable Length Markov\nChains: Methodology, Computing and Software, ETH, Re-\nsearch Report No. 104, March 2002.\n[13]M. Marchini, H. Purwins, Unsupervised Generation of\nPercussion Sound Sequences from a Sound Example, MSc\nthesis, UPF, 2010.\n[14] F. Pachet, The continuator: Musical interaction with\nstyle, in Proceedings of ICMC (ICMA ed.), pp. 211-218,\nSeptember 2002.\n[15] W. Piston, Harmony, Victor Gollancz Ltd, London,\n1959.\n[16] H. Purwins, Proﬁles of Pitch Classes Circularity of\nRelative Pitch and Key - Experiments, Models, Compu-\ntational Music Analysis, and Perspectives, Ph.D. Thesis,\nBerlin Institute of Technology, 2005.\n[17]H. Purwins, B. Blankertz, K. Obermayer, A New Method\nfor Tracking Modulations in Tonal Music in Audio Data\nFormat, Proceedings of the IJCNN. vol.6, pp. 270-275,\n2000\n[18] H. Purwins, M. Grachten, P. Herrera, A. Hazan, R.\nMarxer, and X. Serra, Computational models of music per-\nception and cognition II: Domain-specic music processing,\nPhysics of Life Reviews, vol. 5, pp. 169-182, 2008.\n[19]C. Schorkhuber, A. Klapuri, Constant-Q transform tool-\nbox for music processing, in: 7th Sound and Music Com-\nputing Conference, July 2010.\n[20]Iannis Xenakis, Formalized Music: Thought and Math-\nematics in Composition, Bloomington: Indiana University\nPress, 1971.\n[21]http://aubio.org, April 2012.\n[22]http://soundcloud.com/chordsequencegenerator , April\n2012.Example 1 Musicians Non-musicians\nFamiliarity Interesting Familiarity Interesting\nOriginal 2,1,3,1,4 2,2,4 (22-30s),4 (11-16s),3 2,2,3,2,4 3,4(38-42s),4 (22-32s),2,3\nGeneration 1 2,1,3,1,3 2,2,3,5 (4-12s),2 2,3,2,2,2 4 (1-11s),3,3,2,3\nGeneration 2 5,1,3,1,2 2,2,3,2,3 3,3,4,2,4 2,5 (48-58s),4 (40-50s),2,\n4 (45-55s)\nSimilarity Org.-Gen.1 Org.-Gen.2 Org.-Gen.1 Org.-Gen.2\nNot similar\nSomewhat similar ++ ++ ++ +++\nVery similar +++ +++ +++ ++\nExample 2 Musicians Non-musicians\nFamiliarity Familiarity\nOriginal 4,4,4,5,4 3,3,4,2,5\nClearness Interesting Clearness Interesting\nGeneration 1 4,5,5,3,2 3,5 (30-40s),4 (30-40s),2,1 4,5,4,4,4 4 (1-11s),5 (1-11s),\n4 (45-55s),4 (30-40s),3\nGeneration 2 5,4,3,2,3 1,4 (23-32s),3,3,2 4,4,3,3,3 2,3,4,2,4\nSimilarity Org.-Gen.1 Org.-Gen.2 Org.-Gen.1 Org.-Gen.2\nNot similar + +\nSomewhat similar + + ++ +++\nVery similar ++++ +++ ++ ++\nExample 3 Musicians Non-musicians\nFamiliarity Familiarity\nOriginal 5,5,4,5,5 4,5,5,5,5\nClearness Interesting Clearness Interesting\nGeneration 1 5,5,5,3,2 5 (0-10s),5 (43-53s),3,3,1 5,5,3,4,3 5 (33-43s),5 (43-48s),3,3\n,5 (30-40s)\nGeneration 2 5,4,4,2,4 5 (34-44s),4 (43-51s),4,3,2 4,5,3,5,4 5 (17-24s),5 (34-44s),4 (45-52s),\n4 (20-30s),4 (40-50s)\nSimilarity Org.-Gen.1 Org.-Gen.2 Org.-Gen.1 Org.-Gen.2\nNot similar\nSomewhat similar ++ ++++ ++\nVery similar +++ + +++++ +++\nExample 4 Musicians Non-musicians\nFamiliarity Interesting Familiarity Interesting\nOriginal 1,2,1,5,2 4 (0-10s),2,4 (34-38s), 3,2,3,2,4 4 (1-8s),3,3,1,3\n4 (28-38s),4 (10-20s)\nGeneration 1 1,2,1,5,2 4 (0-10s),2,3,4 (8-14s), 4,1,4,2,5 3,3,3,1,4 (10-20s)\n4 (9-13s)\nGeneration 2 1,2,1,5,2 1,2,5 (7-15s),3,3 2,1,3,2,5 3,3 (32-42s),4 (45-55s),1,3\nSimilarity Org.-Gen.1 Org.-Gen.2 Org.-Gen.1 Org.-Gen.2\nNot similar\nSomewhat similar +++ + ++++\nVery similar +++++ ++ ++++ +\nExample 5 Musicians Non-musicians\nFamiliarity Interesting Familiarity Interesting\nOriginal 1,2,1,3,4 1,2,2,2,4 (20-30s) 1,1,3,3,3 2,2,2,2,3\nGeneration 1,2,1,4,4 1,2,3,3,4 (11-16s) 1,1,2,3,2 2,2,3,2,3\nSimilarity Org.-Gen. Org.-Gen.\nNot similar +\nSomewhat similar +++++ +++\nVery similar +\nTable 2. We present the questionnaire responses for Examples 1 - 5; the ratings (from 1 to 5) that both musicians and non musicians\nhave given for each audio thus the rate for similarity comparing speciﬁc audio couples are shown. At the interesting part, there is a\npotential mention of the most interesting 10 seconds, in case the response in that section was 4 or 5."
    },
    {
        "title": "Robust Singer Identification in Polyphonic Music using Melody Enhancement and Uncertainty-based Learning.",
        "author": [
            "Mathieu Lagrange",
            "Alexey Ozerov",
            "Emmanuel Vincent 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416526",
        "url": "https://doi.org/10.5281/zenodo.1416526",
        "ee": "https://zenodo.org/records/1416526/files/LagrangeOV12.pdf",
        "abstract": "Enhancing specific parts of a polyphonic music signal is believed to be a promising way of breaking the glass ceiling that most Music Information Retrieval (MIR) sys- tems are now facing. The use of signal enhancement as a pre-processing step has led to limited improvement though, because distortions inevitably remain in the enhanced sig- nals that may propagate to the subsequent feature extrac- tion and classification stages. Previous studies attempting to reduce the impact of these distortions have relied on the use of feature weighting or missing feature theory. Based on advances in the field of noise-robust speech recognition, we represent the uncertainty about the enhanced signals via a Gaussian distribution instead that is subsequently prop- agated to the features and to the classifier. We introduce new methods to estimate the uncertainty from the signal in a fully automatic manner and to learn the classifier directly from polyphonic data. We illustrate the results by consid- ering the task of identifying, from a given set of singers, which one is singing at a given time in a given song. Exper- imental results demonstrate the relevance of our approach.",
        "zenodo_id": 1416526,
        "dblp_key": "conf/ismir/LagrangeOV12",
        "keywords": [
            "polyphonic music signal",
            "Music Information Retrieval (MIR)",
            "signal enhancement",
            "distortions",
            "feature extraction",
            "classification stages",
            "noise-robust speech recognition",
            "Gaussian distribution",
            "uncertainty",
            "classifier"
        ],
        "content": "ROBUST SINGER IDENTIFICATION IN POLYPHONIC MUSIC USING\nMELODY ENHANCEMENT AND UNCERTAINTY-BASED LEARNING\nMathieu Lagrange\nSTMS - IRCAM -\nCNRS - UPMC\nmathieu.lagrange@ircam.frAlexey Ozerov\nTechnicolor\nResearch & Innovation, France\nalexey.ozerov@technicolor.comEmmanuel Vincent\nINRIA, Centre de Rennes -\nBretagne Atlantique\nemmanuel.vincent@inria.fr\nABSTRACT\nEnhancing speciﬁc parts of a polyphonic music signal\nis believed to be a promising way of breaking the glass\nceiling that most Music Information Retrieval (MIR) sys-\ntems are now facing. The use of signal enhancement as a\npre-processing step has led to limited improvement though,\nbecause distortions inevitably remain in the enhanced sig-\nnals that may propagate to the subsequent feature extrac-\ntion and classiﬁcation stages. Previous studies attempting\nto reduce the impact of these distortions have relied on the\nuse of feature weighting or missing feature theory. Based\non advances in the ﬁeld of noise-robust speech recognition,\nwe represent the uncertainty about the enhanced signals via\na Gaussian distribution instead that is subsequently prop-\nagated to the features and to the classiﬁer. We introduce\nnew methods to estimate the uncertainty from the signal in\na fully automatic manner and to learn the classiﬁer directly\nfrom polyphonic data. We illustrate the results by consid-\nering the task of identifying, from a given set of singers,\nwhich one is singing at a given time in a given song. Exper-\nimental results demonstrate the relevance of our approach.\n1. INTRODUCTION\nBeing able to focus on speciﬁc parts of a polyphonic musi-\ncal signal is believed to be a promising way of breaking the\nglass ceiling that most Music Information Retrieval (MIR)\ntasks are now facing [3]. Many approaches were recently\nproposed to enhance speciﬁc signals (e.g., vocals, drums,\nbass) by means of source separation methods [7, 19].\nThe beneﬁt of signal enhancement has already been pro-\nven for several MIR classiﬁcation tasks, such as singer\nidentiﬁcation [10, 16], instrument recognition [12], tempo\nestimation [4], and chord recognition [20]. In most of those\nworks, signal enhancement was used as a pre-processing\nstep. Since the enhancement process must operate with\nlimited prior knowledge about the properties of the spe-\nciﬁc parts to be enhanced, distortions inevitably remain in\nthe enhanced signals that propagate to the subsequent fea-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.ture extraction and classiﬁcation stages resulting in limited\nimprovement or even degradation of the classiﬁcation ac-\ncuracy.\nA few studies have attempted to reduce the impact of\nthese distortions on the classiﬁcation accuracy. In [10,15],\nfeature weighting and frame selection techniques were pro-\nposed that associate a constant reliability weight to each\nfeature over all time frames or to all features in each time\nframe. In practice, however, distortions affect different fea-\ntures in different time frames so that the assumption of con-\nstant reliability does not hold. A more powerful approach\nconsists of estimating and exploiting the reliability of each\nfeature within each time frame. A ﬁrst step in this direction\nwas taken in [8], where recognition of musical instruments\nin polyphonic audio was achieved using the missing fea-\nture theory. This theory adopted from noise-robust speech\nrecognition assumes that only certain features are observed\nin each time frame while other features are missing and\nthus discarded from the classiﬁcation process [5].\nNevertheless, the approach in [8] has the following three\nlimitations. First, such binary uncertainty (either observed\nor missing) does not account for partially distorted features\nnor for correlations between the distortions affecting dif-\nferent features. To avoid this limitation, it was proposed in\nthe speech recognition ﬁeld to use the so-called Gaussian\nuncertainty [6], where the distortions over a feature vec-\ntor are modeled as a zero-mean multivariate Gaussian with\npossibly non-diagonal covariance matrix. Second, this ap-\nproach necessitates clean data to train the classiﬁers, while\nfor some tasks, e.g., singer identiﬁcation, collecting such\nclean data may be impossible. Third, the approach in [8]\nrelies on manual f0 annotation and its use in a fully auto-\nmatic system has not been demonstrated.\nThe contribution of this paper is threefold: (1) promot-\ning the use of Gaussian uncertainty instead of binary un-\ncertainty for robust classiﬁcation in the ﬁeld of MIR, (2)\nusing a fully automatic procedure for Gaussian uncertainty\nestimation, (3) learning classiﬁers directly from noisy data\nwith Gaussian uncertainty.\nTo illustrate the potential of the proposed approach we\nconsider in this paper the task of singer identiﬁcation in\npopular music and address it, in line with [10, 16], us-\ning Gaussian Mixture Model (GMM)-based classiﬁers and\nMel-frequency cepstral coefﬁcients (MFCCs) as features.\nWe consider this task since it is one of the MIR classiﬁca-\ntion tasks for which the beneﬁt of signal enhancement isMixtureMFCC\ncomputationModel \nlearningLearning\nDecoding\nGMM\nLikelihood\ncomputation\nLikelihood\n MixtureMFCC\ncomputation\nFigure 1. The standard classiﬁcation scheme.\nmost obvious. Indeed, the information about singer iden-\ntity is mostly concentrated in the singing voice signal.\nThe remainder of this paper is organized as follows.\nSome background about singer identiﬁcation and baseline\napproaches is provided in Section 2. The proposed ap-\nproach based on Gaussian uncertainty is detailed in Sec-\ntion 3. Experiments are presented in Section 4 and a dis-\ncussion is provided in Section 5.\n2. SINGER IDENTIFICATION\n2.1 Background\nWhen it comes to characterizing a song from its content,\nidentifying the singer that is performing at a given time in\na given song is arguably an interesting and useful piece of\ninformation. Indeed, most listeners have a strong commit-\nment to the singer while listening to a given song. How-\never, the literature about automatic singer identiﬁcation is\nrelatively scarce, compared for example with musical genre\ndetection. This may be explained by several difﬁculties\nthat pose interesting challenges for research in machine lis-\ntening.\nFirst, the human voice is a very ﬂexible and versatile in-\nstrument and very small changes in its properties have not-\nicable effects on human perception. Second, the musical\naccompaniment that forms the background is very diverse\nand operates at about the same loudness as the singing\nvoice. Hence, very little can be assumed on both sides and\nthe inﬂuence of the background cannot be neglected.\nFor humans, though, it is relatively easy to focus on the\nmelody sung by the singer as our hearing system is highly\nskilled at segregating human vocalizations within cluttered\nacoustical environments. This segregation is also made\npossible by compositional choices. For example, most of\nthe time in pop music, only one singer is singing at a time,\nand if not, the others are background vocals that are usu-\nally more easily predictable and sung at a relatively low\nvolume.\nFrom an application perspective, singing voice enhance-\nment is expected to be useful for the identiﬁcation of singers\nwhich have sung with different bands or with different in-\nstrumentations, such as unplugged versions. More on the\nso-called album effect can be found in [14]. In this case,\nclassifying the mixture signal will induce variability in the\nsinger models due to occlusion, while classifying the singing\nvoice signal alone should provide better identiﬁcation. Thesame remark applies to the case where a song features\nmultiple singers and one needs to identify which singer is\nsinging at a given time. For some other repertoires where\nthe notions of singer and artist/band are very tightly linked,\nit is questionable whether the singing voice signal sufﬁces\nfor classiﬁcation, because the musical background can also\nprovide discriminative cues. Nevertheless, singing voice\nenhancement is likely to remain beneﬁcial by enabling the\ncomputation of separate features over the singing voice and\nover the background and their fusion in the classiﬁcation\nprocess. In this paper, for simplicity, we illustrate the po-\ntential of our approach by considering the singing voice\nsignal only unless otherwise stated.\n2.2 Baseline Approaches\nMore formally, let us assume that each recording xfn(also\ncalled mixture), represented here directly in the Short Term\nFourier Transform (STFT) domain, f= 1;:::;F andn=\n1;:::;N being respectively frequency and time indices, is\nthe sum of two contributions: the main melody (here the\nsinging voice) vfnand the accompaniment afn. This can\nbe written in the following vector form:\nxn=vn+an; (1)\nwhere xn= [x 1n;:::;x Fn]T,vn= [v 1n;:::;v Fn]Tand\nan= [a1n;:::;a Fn]T.\nWe assume that there are Ksingers to be recognized,\nand for each singer there is a sufﬁcient amount of train-\ning and testing mixtures. In line with [10, 16], we adopt\na singer identiﬁcation approach based on MFCC features\nand GMMs.\nWithout any melody enhancement such an approach con-\nsists in the following two steps [13] (Fig. 1):\n1.Learning: For each singer k= 1;:::;K , the cor-\nresponding GMM model is estimated in the maxi-\nmum likelihood (ML) sense from the features (here\nMFCCs) \u0016ycomputed directly from the training mix-\ntures of that singer.\n2.Decoding: A testing mixture xis assigned to the\nsingerkfor which the likelihood of model \u0012keval-\nuated on the features extracted in the same way is\nmaximum1.\nIn order to gain invariance with respect to the accom-\npaniment, one needs to separate the contribution of the ac-\ncompaniment and the singer within the mixture. This sepa-\nration may be embedded within the classiﬁer, as in [22]. In\nthis case, the separation has to be performed in the feature\ndomain, usually the log Mel spectrum.\nAlternatively, melody enhancement can be applied as\na pre-processing step [10, 16] over the spectrogram of the\nmixture. since the spectrogram have better spectral reso-\nlution than the log Mel spectrum, this approach can po-\ntentially achieve better discrimination, as in that case, the\nfeatures (MFCCs) are no longer computed from the au-\ndio mixture, but from the corresponding melody estimate\n\u0016v(Fig. 2).\n1In order not to overload the notations, the singer index kis omitted\nhereafter, where applicable.Melody \nenhancement\nMixtureMFCC\ncomputationModel \nlearningLearning\nDecoding\nGMM\nLikelihood\ncomputation\nLikelihoodMelody \nenhancement\nMixtureMFCC\ncomputation\nMFCC\ncomputation\nFigure 2. Considering melody enhancement as a pre-\nprocessing step.\n3. PROPOSED APPROACH\nInspired by some approaches in speech processing [6], we\npropose to consider Gaussian uncertainty by augmenting\nthe melody estimates \u0016vby a set of covariance matrices \u0006v\nrepresenting the errors about these estimates. This Gaus-\nsian uncertainty is ﬁrst estimated in the STFT domain, then\npropagated through MFCC computation, and ﬁnally ex-\nploited for GMM learning and decoding steps (Fig. 3).\n3.1 Melody Enhancement\nGiven the mixture, we assume that each STFT frame vnof\nthe melody is distributed as\nvnjxn\u0018N(\u0016vn;\u0016\u0006v;n); (2)\nand we are looking for an estimate of \u0016vnand\u0016\u0006v;n.\nIn this study, we have chosen the melody enhancement\nmethod2proposed by Durrieu et al. [7]. This method has\nshown very promising results for vocals enhancement task\nwithin the 2011 Signal Separation Evaluation Campaign\n(SiSEC 2011) [2] and its underlying probabilistic model\nfacilitates STFT domain uncertainty computation.\nThe main melody v, usually a singer, is modeled thanks\nto a source/ﬁlter model, and the accompaniment ais mod-\neled using Non-negative Matrix Factorization (NMF) model.\nThe leading voice is assumed to be harmonic and mono-\nphonic. The separation system mainly tracks the leading\nvoice following two cues: ﬁrst its energy, and second the\nsmoothness of the melody line. Therefore, the resulting\nseparated leading voice is usually the instrument or voice\nthat is the most salient in the mixture, over certain dura-\ntions of the signal. Overall this modeling falls into the\nframework of constrained hierarchical NMF with Itakura-\nSaito divergence [19], which allows a probabilistic Gaus-\nsian interpretation [9].\nMore precisely the method is designed for stereo mix-\ntures. Let mixing equation\nxj;fn=vj;fn+aj;fn (3)\nbe a stereophonic version of the monophonic mixing equa-\ntion (1), where j= 1;2is the channel index and equations\n(1) and (3) are related for any signal sj;fnas\nsfn= (s1;fn+s2;fn)=2: (4)\n2The Python source code is available at http://www.durrieu.\nch/research/jstsp2010.html\nMelody \nenhancement\nMixtureMFCC\ncomputationModel \nlearningLearning\nDecoding\nGMM\nLikelihood\ncomputation\nLikelihoodMelody \nenhancement\nMixtureMFCC\ncomputation\nFigure 3. Proposed approach with melody enhancement\nand Gaussian uncertainty.\nA probabilistic Gaussian interpretation of modeling in\n[7] assumes vj;fnandaj;fnare zero-mean Gaussians that\nare mutually independent and independent over channel j,\nfrequencyfand timen. The corresponding constrained\nhierarchical NMF structured modeling allows the estima-\ntion of their respective variances \u001b2\nv;j;fn and\u001b2\na;j;fn from\nthe multichannel mixture. With these assumptions the pos-\nterior distribution of vj;fngivenxj;fncan be shown to be\nGaussian with mean\n\u0016vj;fn=\u001b2\nv;j;fn\n\u001b2\nv;j;fn+\u001b2\na;j;fnxj;fn (5)\nobtained by Wiener ﬁltering, as in [7], and the variance\n[21]\n\u0016\u001b2\nv;j;fn =\u001b2\nv;j;fn\u001b2\na;j;fn\n\u001b2\nv;j;fn+\u001b2\na;j;fn: (6)\nFinally, thanks to the posterior between-channel inde-\npendence of vj;fnand the down-mixing (4), \u0016vnand\u0016\u0006v;n\nin (2) are computed as\n\u0016vn=\b\n(\u0016v1;fn+ \u0016v2;fn)=2\t\nf; (7)\n\u0016\u0006v;n= diag\u0014n\n(\u0016\u001b2\nv;1;fn + \u0016\u001b2\nv;2;fn)=2o\nf\u0015\n: (8)\nNote that any Gaussian model-based signal enhance-\nment method, e.g., one of the methods implementable via\nthe general source separation framework in [19], is suitable\nto compute this kind of uncertainty in the time-frequency\ndomain.\n3.2 Uncertainty Propagation during MFCC\nComputation\nLetM(\u0001) be the nonlinear transform used to compute an\nM-dimensional MFCC feature vector yn2RM. It can be\nexpressed as [1]\nyn=M(v n) =Dlog(Mjv nj); (9)\nwhere Dis theM\u0002MDCT matrix, Mis theM\u0002Fmatrix\ncontaining the Mel ﬁlter coefﬁcients, and j\u0001jandlog(\u0001) are\nboth element-wise operations.\nIn line with (2), we assume that the clean (missing) fea-\ntureyn=M(v n)is distributed as\nynjxn\u0018N(\u0016yn;\u0016\u0006y;n); (10)which is an approximation because of the Gaussian as-\nsumption (2) and the nonlinear nature of M(\u0001).\nTo compute the feature estimate \u0016ynand its Gaussian\nuncertainty covariance \u0016\u0006y;nwe propose to use the Vector\nTaylor Series (VTS) method [17] that consists in lineariz-\ning the transformM(\u0001) by its ﬁrst-order vector Taylor ex-\npansion in the neighborhood of the voice estimate \u0016vn:\nyn=M(v n)\u0019M (\u0016vn) +JM(\u0016vn) (vn\u0000\u0016vn);(11)\nwhereJM(\u0016vn)is the Jacobian matrix of M(v n)com-\nputed in vn=\u0016vn. This leads to the following estimates of\nthe noisy feature value \u0016ynand its uncertainty covariance\n\u0016\u0006y;n(10), as propagated through this (now linear) trans-\nform:\n\u0016yn=M(\u0016vn); (12)\n\u0016\u0006y;n=DM\nMj\u0016vnj11\u0002F\u0016\u0006v;n\u0014\nDM\nMj\u0016vnj11\u0002F\u0015T\n;(13)\nwhere 11\u0002Fis an 1\u0002Fvector of ones and the magnitude\nj\u0001jand the division are both element-wise operations.\n3.3 GMM Decoding and Learning with Uncertainty\nEach singer is modeled by a GMM \u0012=f\u0016i;\u0006i;!igI\ni=1,\nwherei= 1;:::;I are mixture component indices, and\n\u0016i,\u0006iand!i(P\ni!i= 1) are respectively the mean, the\ncovariance matrix and the weight of the i-th component.\nIn other words, each clean feature vector ynis modeled as\nfollows:\np(ynj\u0012) =XI\ni=1!iN(ynj\u0016i;\u0006i); (14)\nwhere\nN(ynj\u0016i;\u0006i),\n1p\n(2\u0019)Mj\u0006ij\u0014\n\u0000(yn\u0000\u0016i)T\u0006\u00001\ni(yn\u0000\u0016i)\n2\u0015\n:(15)\nSince the clean feature sequence y=fyngnis not\nobserved, its likelihood, given model \u0012, cannot be com-\nputed using (14). Thus in the “likelihood computation”\nstep (Fig. 3), we rather compute the likelihood of the noisy\nfeatures \u0016ygiven the uncertainty and the model, that can be\nshown to be equal to [6]:\np(\u0016yj\u0016\u0006y;\u0012) =NY\nn=1IX\ni=1!iN(\u0016ynj\u0016i;\u0006i+\u0016\u0006y;n):(16)\nWe see that in this likelihood Gaussian uncertainty covari-\nance \u0016\u0006y;nadds to the prior GMM covariance \u0006i, thus\nadaptively decreasing the effect of signal distortion.\nIn the “model learning” step (Fig. 3), we propose to esti-\nmate the GMM parameters \u0012by maximizing the likelihood\n(16). This can be achieved via the iterative Expectation-\nMaximization (EM) algorithm introduced in [18] and sum-\nmarized in Algorithm 1. The derivation of this algorithm\nis omitted here due to lack of space and the Matlab source\ncode for GMM decoding and learning is available at http:\n//bass-db.gforge.inria.fr/amulet.Algorithm 1 One iteration of the EM algorithm for the\nlikelihood integration-based GMM learning from noisy\ndata.\nE step. Conditional expectations of natural statistics:\n\ri;n/!iN(\u0016ynj\u0016i;\u0006i+\u0016\u0006y;n);\nandX\ni\ri;n= 1; (17)\n^yi;n=Wi;n(\u0016yn\u0000\u0016i) +\u0016i; (18)\nbRyy;i;n=^yi;n^yT\ni;n+ (I\u0000Wi;n)\u0006i; (19)\nwhere\nWi;n=\u0006i\u0002\n\u0006i+\u0016\u0006y;n\u0003\u00001: (20)\nM step. Update GMM parameters:\n!i=1\nNNX\nn=1\ri;n; (21)\n\u0016i=1PN\nn=1\ri;nNX\nn=1\ri;n^yi;n; (22)\n\u0006i=1PN\nn=1\ri;nNX\nn=1\ri;nbRyy;i;n\u0000\u0016i\u0016T\ni: (23)\n4. EXPERIMENTS\n4.1 Database\nFor our evaluation, we consider a subset of the RWC Pop-\nular Music Database [11] which has previously been con-\nsidered in [10] for the same task. It consists of 40 songs\nsung by 10 singers, ﬁve of which were male (denoted by a\ntoe) and the ﬁve others female (denoted by ftoj). This\nset is then divided into the four groups of songs considered\nin [10], each containing one song by each singer .\nEach of those songs is then split into segments of 10\nseconds duration. Among those segments, only the ones\nwhere a singing voice is present (not necessarily during the\nwhole duration of the segment) are kept unless otherwise\nstated.\nConsidering short duration segments instead of the whole\nsong is done for two reasons. First, it makes the task more\ngeneric in the sense that multiple singers can also poten-\ntially be tracked within a same song. Second, it allows us\nto gain statistical relevance during the cross validation by\nenlarging the number of tests.\n4.2 Methods\nFor each of those segments, features are computed and\nclassiﬁed using the three methods depicted in Figures 1 to\n3. The ﬁrst one, named mix, consists in computing the fea-\ntures directly from the mixture, and serves as a baseline.\nThe second method, termed v-sep, consider melody en-\nhancement as a pre-processing step. The main melody en-\nhancement system considered in this study is available un-\nder two versions: a version focusing on the voiced part ofAccuracy (%) per 10 sec. singing segment per song\ninput Fold 1 Fold 2 Fold 3 Fold 4 Total all seg. sung seg.\nmix 51 53 55 38 49 57 64\nv-sep 60 63 53 43 55 57 64\nv-sep-uncrt 71 72 84 83 77 85 94\nTable 1 . Average accuracy of the tested methods per singing segment and per song, considering either all the segments or\nonly those segments where singing voice is present in the latter case.\nthe singing voice and another version attempting to jointly\nenhance the voiced and the unvoiced parts of the singing\nvoice (see [7] for details). In the following, only the re-\nsults of the former are reported since the latter led to much\nsmaller classiﬁcation accuracy. When the estimated vocals\nsignal has zero power in a given time frame, the result-\ning MFCCs may be undeﬁned. Such frames are discarded.\nThe last method, termed v-sep-uncrt, consists in exploiting\nthe estimated uncertainty about the enhancement process.\nFor all those methods, we considered MFCC features\nand dropped the ﬁrst coefﬁcient, thus discarding energy in-\nformation. Mixtures of 32 Gaussians are then trained using\n50 iterations of the EM algorithm for each singer. For test-\ning, the likelihood of each singer model is computed for\neach segment and the one with the highest likelihood is\nselected as the estimate.\n4.3 Results\nThe aforementioned four groups of songs are considered\nfor a 4-fold cross validation. For each fold, the selected\ngroup is used for testing and the data of the three remain-\ning ones are used for training the models. The average\ndetection accuracy are shown in Table 1. Compared to\nthe baseline, v-sep andv-sep-uncrt achieve better perfor-\nmance while considering segments, indicating that focus-\ning on the main harmonic source within the segment is\nbeneﬁcial for identifying the singer. That is, the level of\nfeature invariance gained by the separation process more\nthan compensates for the distortions it induces.\nConsidering the uncertainty estimate adds a signiﬁcant\nlevel of improvement in the v\u0000sepcase. We assume that\nthis gain of performance is obtained because the use of un-\ncertainty allows us to focus on the energy within the spec-\ntrogram that effectively belongs to the voice and that the\nuse of the uncertainty allows us to robustly consider stan-\ndard features (MFCCs).\nPerforming a majority vote over the all the segments (in\nthis case the likelihood of each singer is taken into account\neven if no singing voice is present) of each song gives an\naccuracy of 85% and restricting the vote to only the sung\nsegments gives a 94% accuracy. These numbers can re-\nspectively be considered as worst and best cases. It is\ntherefore likely that a complete system that would incor-\nporate a music model to discard segments with only music\nwould achieve an accuracy that is between those bounds.\nAlthough a more formal comparison would be needed, we\nbelieve that those results compare favorably with the per-\nformances obtained in [10] using specialized features on\nthe same dataset while standard MFCC features were usedhere. It is also interesting to notice that in this case of song-\nlevel decisions, considering the separation without uncer-\ntainty does not give any improvement compared to the mix\nbaseline.\n5. DISCUSSION\nWe have presented in this paper a computational scheme\nfor extracting meaningful information in order to tackle a\nmusic retrieval task: singer identiﬁcation. This is done by\nconsidering an enhanced version of the main melody that is\nmore or less reliable in speciﬁc regions of the time/frequency\nplane. Instead of blindly making use of this estimate, we\npropose in this paper to consider how uncertain the separa-\ntion estimate is during the modeling phase. This allows us\nto give more or less importance to the features depending\non how reliable they are in different time frames, both dur-\ning the training and the testing phases. For that purpose,\nwe adopted the Gaussian uncertainty framework and intro-\nduced new methods to estimate the uncertainty in a fully\nautomatic manner and to learn GMM classiﬁers directly\nfrom polyphonic data.\nOne should notice that the proposed scheme is not tied\nto the task considered in this paper. It is in fact completely\ngeneric and may be easily applied to other GMM-based\nMIR classiﬁcation tasks where the prior isolation of a spe-\nciﬁc part of the music signal could be beneﬁcial. The\nonly part that would require adaptation is the derivation\nof VTS uncertainty propagation equations for other fea-\ntures than MFCCs. Uncertainty handling for other classi-\nﬁers than GMM has also received some interest recently in\nthe speech processing community.\nThe experiments reported in this paper provide us with\nencouraging results. Concerning this speciﬁc task of singer\nidentiﬁcation, we intend to exploit both the enhanced singing\nvoice and accompaniment signals and to experiment on\nother datasets with a wider range of musical styles. In\nparticular, we believe that the hip-hop/rap musicals genres\nwould be an excellent testbed both from a methodologi-\ncal and application point of view, as many songs feature\nseveral singers: knowing which singer is performing at a\ngiven time is a useful piece of information. Finally, we\nwould like to consider other content based retrieval tasks\nin order to study the relevance of this scheme for a wider\nrange of applications.\n6. ACKNOWLEDGMENTS\nThis work was partly supported by the Quaero project funded\nby Oseo and ANR-11-JS03- 005-01. The authors wish tothank Jean-Louis Durrieu for kindly providing his melody\nestimation source code to the community and Mathias Rossig-\nnol for his useful comments.\n7. REFERENCES\n[1] K. Adilo ˘glu and E. Vincent. An uncertainty estimation\napproach for the extraction of individual source fea-\ntures in multisource recordings. In EUSIPCO, 19th Eu-\nropean Signal Processing Conference, 2011.\n[2] S. Araki, F. Nesta, E. Vincent, Z. Koldovsky, G. Nolte,\nA. Ziehe, and A. Benichoux. The 2011 signal separa-\ntion evaluation campaign (SiSEC2011): - audio source\nseparation. In Proc. Int. Conf. on Latent Variable Anal-\nysis and Signal Separation, pages 414–422, 2012.\n[3] J.-J. Aucouturier and F Pachet. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n[4] P. Chordia and A. Rae. Using source separation to im-\nprove tempo detection. In Proc. 10th Intl. Society for\nMusic Information Retrieval Conference, pages 183–\n188, Kobe, Japan, 2009.\n[5] M Cooke. Robust automatic speech recognition with\nmissing and unreliable acoustic data. Speech Commu-\nnication, 34(3):267–285, June 2001.\n[6] L. Deng, J. Droppo, and A. Acero. Dynamic compen-\nsation of HMM variances using the feature enhance-\nment uncertainty computed from a parametric model\nof speech distortion. IEEE Transactions on Speech and\nAudio Processing, 13(3):412–421, 2005.\n[7] J.L. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n18(3):564–575, 2010.\n[8] J. Eggink and G. J. Brown. Application of missing fea-\nture theory to the recognition of musical instruments in\npolyphonic audio. In Proc. 4th International Confer-\nence on Music Information Retrieval, 2003.\n[9] C. F ´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative\nmatrix factorization with the Itakura-Saito divergence.\nWith application to music analysis. Neural Computa-\ntion, 21(3):793–830, Mar. 2009.\n[10] H. Fujihara, M. Goto, T. Kitahara, and H. G. Okuno.\nA modeling of singing voice robust to accompaniment\nsounds and its application to singer music information\nretrieval. IEEE Trans. Audio, Speech and Language\nProcessing, 18(3):638–648, 2010.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: popular, classical, and jazz\nmusic databases. Proc. International Conference for\nMusic Information Retrieval (ISMIR), pages 287–288,\n2003.[12] T. Heittola, A. Klapuri, and T. Virtanen. Musical in-\nstrument recognition in polyphonic audio using source-\nﬁlter model for sound separation. in Proc. 10th Intl.\nSociety for Music Information Retrieval Conference,\nKobe, Japan, pages 327–332, 2009.\n[13] Y . E. Kim. Singer identiﬁcation in popular music\nrecordings using voice coding features. In Proceedings\nof the International Conference on Music Information\nRetrieval (ISMIR), Paris, France, 2002.\n[14] Y . E. Kim, D. S. Williamson, and S. Pilli. Towards\nquantifying the album effect in artist identiﬁcation. In\nProc. of Int.Conf. on Music Information Retrieval (IS-\nMIR), pages 393–394, 2006.\n[15] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and\nH. G. Okuno. Instrument identiﬁcation in polyphonic\nmusic: Feature weighting to minimize inﬂuence of\nsound overlaps. EURASIP Journal on Advances in Sig-\nnal Processing, 2007, 2007. article ID 51979.\n[16] A. Mesaros and T. Virtanen. Singer identiﬁcation in\npolyphonic music using vocal separation and pattern\nrecognition methods. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR), pages 375–378, 2007.\n[17] P. J. Moreno, B. Raj, and R. M. Stern. A vector\nTaylor series approach for environment-independent\nspeech recognition. In IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing\n(ICASSP’96), volume 2, pages 733 – 736, 1996.\n[18] A. Ozerov, M. Lagrange, and E. Vincent. GMM-based\nclassiﬁcation from noisy features. In Proc. 1st Int.\nWorkshop on Machine Listening in Multisource En-\nvironments (CHiME), pages 30–35, Florence, Italy,\nSeptember 2011.\n[19] A. Ozerov, E. Vincent, and F. Bimbot. A general ﬂexi-\nble framework for the handling of prior information in\naudio source separation. IEEE Transactions on Audio,\nSpeech and Language Processing, 20(4):1118 – 1133,\n2012.\n[20] J. Reed, Y . Ueda, S. M. Siniscalchi, Y . Uchiyama,\nS. Sagayama, and C. H. Lee. Minimum classiﬁcation\nerror training to improve isolated chord recognition. In\nProc. 10th Intl. Society for Music Information Retrieval\nConference, pages 609–614, Kobe, Japan, 2009.\n[21] R. C. Rose, E. M. Hofstetter, and D. A. Reynolds. In-\ntegrated models of signal and background with appli-\ncation to speaker identiﬁcation in noise. IEEE Trans.\nSpeech and Audio Processing, 2(2):245–257, April\n1994.\n[22] W.H. Tsai and H.M. Wang. Automatic singer recog-\nnition of popular music recordings via estimation and\nmodeling of solo vocal signals. IEEE Transactions on\nAudio Speech and Language Processing, pages 1–35,\n2006."
    },
    {
        "title": "The Impact (or Non-impact) of User Studies in Music Information Retrieval.",
        "author": [
            "Jin Ha Lee 0001",
            "Sally Jo Cunningham"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416272",
        "url": "https://doi.org/10.5281/zenodo.1416272",
        "ee": "https://zenodo.org/records/1416272/files/LeeC12.pdf",
        "abstract": "Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system. The number of user studies in the MIR domain has been gradually in- creasing since the early 2000s reflecting the need for empirical studies of users. However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood. In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies. This is followed by a discussion of a number of is- sues/challenges in conducting MIR user studies and dis- tributing the research results. We conclude by making recommendations to increase the visibility and impact of user studies in the field.",
        "zenodo_id": 1416272,
        "dblp_key": "conf/ismir/LeeC12",
        "keywords": [
            "understanding users",
            "user studies",
            "impact in field",
            "system development",
            "evaluation tasks",
            "music similarity",
            "music mood",
            "growth",
            "publication patterns",
            "design"
        ],
        "content": "THE IMPACT (OR NON-IMPACT) OF USER STUDIES  \nIN MUSIC INFORMATION RETRIEVAL \nJin Ha Lee  Sally Jo Cunningham  \nThe Information School  \nUniversity of Washin gton \njinhalee@uw .edu Department of Computer Sc ience \nUniversity of Waikato  \nsallyjo@waikato.ac.nz  \nABSTRACT \nMost Music Information Retrieval (MIR) researchers will \nagree that understanding users' needs and behaviors is \ncritical for developing a good MIR  system. The number \nof user studies in the MIR domain has been gradually i n-\ncreasing since  the early 2000s reflecting the need for \nempirical studies  of users. However, despite  the growing \nnumber of user studies and the wide recognition of their \nimportance, it is unclear how large their impact has been \nin the field; on how systems are developed, evaluation \ntasks are  created, and how we understand critical \nconcepts such as music similarity or music mood. In this \npaper, we present our analysis on the growth, pu blication \nand citation patterns, and design of 15 5 user studies. This \nis followed by a discussion o f a number of is-\nsues/challenges in conducting MIR user studies and di s-\ntributing the research results. We conclude by making \nrecommendations to increase the visibility and impact of \nuser studies in the field.  \n1. INTRODUCTION \nUnderstanding users is a fundamental step in developing \nsuccessful Music Information Retrieval (MIR) systems \nand services. Most MIR researchers will agree with this \nidea, and furthermore , it is not uncommon to hear various \nspeakers at MIR related conference s specifically arguing \nfor the importance of  user studies, academically as well as \ncommercially. Despite  the growing number of user \nstudies and the wide recognition of their importance  in the \nMIR domain, it is unclear as to what impact these studies \nhave really made. Have these studies in fact chang ed how \nMIR systems are developed  or evaluation tasks are \ndesigned? Have they really changed how we understand \ncritical concepts such as music similarity or music mood ? \nFor MIR researchers specializing in user studies  to move \nforward in this domain, it is n ecessary to understand our \npast: what have we been doing and what kind of impact \nhave we made or not? In order to lay the foundation for \nthis discussion, we collected 1 55 user studies related to \nmusic, reviewed the content, and analyzed the publication \nand citation patterns, and research design of these studies .  2. STUDY DESIGN \n2.1 Definition of “User Studies” \nOur first challenge wa s to define and set the boundaries  \nfor “user studies.” From our analysis of relevant literature, \nwe found two major categories of user studies: “studies of \nusers” (e.g., music information needs) and “studies \ninvolving users ” (e.g., usability testing). Weigl and \nGuastavino [7], in their recent review article of user \nstudies in MIR literature, defi ned user studies as \n“documents report(ing) on empirical investigations of \nuser requirements or interactions with systems primarily \naimed at providing access to musical information, \nincluding musical recordings, scores, lyrics, photography \nand artwork, and other associated metadata (p. 335).” In \nthis study, we adopt a broader definition of “user studies” \nas studies reporting on 1) empirical investigation of needs, \nbehaviors, perceptions, and opinions of human s, 2) \nexperiments and usability testing involving humans, 3) \nanalysis of user -generated data , or 4) review of the stu dies \nabove. This is because a broader definition will allow for \na comparison of these different types of user studies and \nenable us to see patterns of concentration with regards to \nparticular types of user studies related to MIR.  \n2.2 Data Collection \nWe conducted an extensive literature search in multiple \ndomains related to music (e.g., MIR, Library and \nInformation Science (LIS), Human Computer Interaction \n(HCI), Computer Science (CS), Engineering, Psychology, \nMusicology) to identify these studies . We conducted \nsearches in multiple databases including WorldCat, \nEBSCO, Web of Knowledge, IEEE Xplore , ACM DL, \nInfoPsych, and Google Scholar. We used the different \ncombinations of the following search te rms: music, user, \nhuman, people, need, use, behavior, tes ting, involvement, \nlearning, interaction, design, accessibility, usability, user -\ncentered, etc. After retrieving the relevant studies, we a lso \nfollowed the citations in order to broaden our search. In \ntotal, we found 155 studies related to music users . \n3. PUBLICATION PATTERNS OF USER STUDIES \n3.1 Growth of the Publications \nFirst, we analyzed several aspects related to the public a-\ntion patterns of the user studies. We examined the publ i-\ncations dates of the user studies in order to learn more \nabout the growth pattern. Figure 1  shows the distribution  \nPermission to make digital or hard copies of  all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012 Internation al Society for Music Information Retrieval    \n \nof the number of user studies published by year. We can \nobserve the steady increase in the number of publication s \nover the years.  There were a small number of user st udies \npre-dating 2000, but the substantial growth started in \nearly 2000s when the need for empirical user studies was \npointed out in works such as [1], [2], and [3]. There was \nalso a noticeable  increase in 2009 and we expect that this \ngrowth pattern will c ontinue for the coming years , at least \nin the near future . Although this growth pattern is encou r-\naging, when compared with the number of studies focu s-\ning on the system aspect of MIR, the overall number of \nuser studies is still relatively small [7].  \n \nFigure 1.  Distribution of the number of user stu d-\nies by the year of publication \n3.2 Types of Publications \nWe also examined the publication venues of these studies. \nOf the 155 studies, there were 91 conference publications, \n56 journal articles, 6 workshop papers, 1 book chapter , \nand 1 white paper . There were a total of 83 different ve n-\nues where music user studies appeared. The primary \nsource of user studies was the ISMIR conference procee d-\nings with 41 user studies , and all the other journals and \nconference proceedings included 5 or fewer user studies. \n65 of the 155 user studies (42%) were the only music user \nstudy published in the particular venue. This pattern of \nconcentration in a small number of  core publ ications can \nbe explained by  Bradford’s law which characterize s the \npattern of diminishing returns in searching for references \nin scholarly publications  [1]. The concentr ation of MIR \nuser studies in the ISMIR proceedings is perhaps stronger \nthan Bradford’s predicted 1:n:n2 ratio of journals  (where \neach proportion contain s approximat ely the same number \nof articles ). The top sources in the order of number of re l-\nevant papers are:  ISMIR (41); ACM Conference on H u-\nman Factors in  Computing Systems  (5); ACM Interna-\ntional Conference on Multimedia  (4), ACM/IEEE -CS \nJoint Conference on Digital Libraries  (4), International \nConference on Information Visualization  (4); Internatio n-\nal Conference on Mobile and Ubiquitous Multimedia  (4), \nJournal of New Music Research (3), Music Perception \n(3), Psychology of Music  (3), IEEE International Confe r-\nence on Mu ltimedia and Expo  (3), etc.  \nThe skewed distribution of publications poses a cha l-\nlenge for researchers of user studies as well as readers \nwho are interested in finding these studies. We confirmed that it is in fact impossible to find all these studies using a \nsingle database or search engine. Also many researchers \ntend to conduct their literature search in their own d o-\nmain, which will exclude many relevant works published \nin other domains (e.g., psychology scholars not citing \nMIR literature in CS domain). Although the ISMIR pr o-\nceedings are freely available on the Web, a large number \nof other publications are fee-based. Unless the researc h-\ners’ or readers’ institutions have subscriptions to these \ndifferent publications, it will be difficult and expensive to \naccess these works. This also raises a question about di s-\ntributing our knowledge to the general public who are \nsimply interested in music and also people who are in m u-\nsic industry. Much of the MIR research aims to not only \ncontribute to improving the general knowledge of music \nand how people interact with music, but also to create \nbetter systems and services related to music. If there is a \nbarrier for general public and people outside of academia \nto access these works, then without a doubt, the impact \nwe can make in the field will also be diminished.  \n3.3 Co-authorship Analysis \nWe performed a co-authorship analysis to further unde r-\nstand the patterns of publication. Figure 2 shows the co-\nauthorship graph generated by using NodeXL, a tool for \nvisualization and exploration of networks [6]. The graph’s \nvertices were grouped based on the Clauset-Newman-\nMoore clus ter algorithm and the graph was laid out using \nthe Harel-Koren Fast Multiscale layout algorithm. The \nnodes represent the authors and the line connecting the \nnodes represents the co-authorship between the two a u-\nthors. The size of the node is scaled based on the number \nof publications by a particular author, and the width of the \nline connecting two nodes is scaled based on the number \nof times the pair of authors have co-authored a user study.  \nA few strong networks emerge d. The most notable \nnetwork is grouped a round Sally Jo Cunningham, J. St e-\nphen Downie, Jin Ha Lee, David Bainbridge and 20 other \nscholars. The two networks formed around Jukka Holm \nand Arto Lehtiniemi, and Charlie Inskip, Andrew \nMacFarlane, and Pauline Rafferty are also very prom i-\nnent. These stro ng networks seem to be forming based on \nthe particular lab /university and r egions: University of \nIllinois at Urbana -Champaign and University of Waikato \nfor the first group, Fi nland for the second group, and UK \nfor the third group. Another notable network f ormed \naround Adrian C. North and David Hargreaves in UK \nrepresents many user studies published in psychology. \nAnother aspect to note is that the network is very disco n-\nnected, with a large nu mber of small components, each \nconsisting of a small number of aut hors. Part of the reason \nfor this pattern could be b ecause MIR is still a relatively \nnew field, and there have not been many o pportunities for \ncross-institutional ties to be formed. Or, it may reflect the \nwidespread appeal of music as a subject for researc h \n(which is corroborated by the number and diversity of \npublication venues surveyed for this study). Further ana l-\nysis will be necessary to determine the re asons for seeing \nthis kind of co -authorship patterns.     \n \n \nFigure 2.  Co-authorship network among the authors \n4. CITATION PATTERNS OF USER STUDIES \nAs part of the effort in understanding the impact of these \nstudies, we investigated how often they were cited as of \nApril 24, 2012 using the citation data from Google Scho l-\nar (GS). The reason for using GS is because the major \npublications in the field such as ISMIR conference pr o-\nceedings are not indexed in other major databases such as \nEBSCO, Web of Science, etc. Also since we are interes t-\ned in the scholarly as well as commercial impact of the \nuser studies, being able to search for patents in addition to \nscholarly work on GS was deemed useful. We found a \ntotal of 3097 citations of 154 user studies in research pu b-\nlications and patents (one study did not show up). Figure \n3 shows the distribution of the citation counts of the user \nstudies in other materials. The X-axis represents the nu m-\nber of citations and the Y-axis represents the number of \nuser studies that had the specified range of citation counts. \nThe average number was 20.1 with the standard deviation \nof 44.5, the median of 5 .5, and the maximum of 348.  \n  \nFigure 3.  Distribution of the number of references \nof the user studies in other articles and patents \n \nFigure 4.  Distribution of the number of years for \nuser studies to get cited \nWe were also interested in how long it takes for user \nstudies to get cited. Figure 4 shows the distribution of the \nnumber of years it took for the user studies to get cited. \nThis is based on the publication dates of the 2864 out of \nthe 3097 citing articles and patents we were able to \nretrieve on GS . The X-axis represents the number of years \npassed after the publication of user studies and t he Y-axis \nrepresents the number of citing articles/patents.  The nega-\ntive numb ers (-1,-2) represent the cases where the author \nwas self-citing a study that was yet to be published, or ci t-\ning a study that was made available online before the \nprint publication.  The mean number of years was 5.48 \nwith the standard deviation of 0.10, med ian of 5, and \nmaximum of 90. About 40% (1144 out of 2864) were ci t-\ned in 3 years or less after the user study was published  \nand about 60% (1710) in 5 years or less . The citation pa t-\ntern gradually decreases, and only about 15% (422) were \ncited after 10 years  or more, and about 4% (119) after 15 \nyears or more. The citation pattern suggests that the “pe r-\nceived” relevance of the results quickly diminishes over   \n \ntime. Since the majority of the user studies were pu b-\nlished after 2000, for a more complete picture, this analy-\nsis will have to be replicated in 10 or 20 years.   \n \nFigure 5.  Distribution of the citing articles/patents \nby the publication year of citing articles/patents \nFigure 5 shows the distribution of the citing articles/ \npatents by their publication dates. Overall the numbers of \nciting articles/patents are showing a pattern of steady i n-\ncrease. Figure 3 , 4, and 5, altogether seem to suggest that \nthe user studies are in fact making growing impact to the \nfield, although the impact of the studies tend to quickly \ndiminish over time based on citation patterns. \nAuthor/Year Title Ref \nMcNab  \net al./96 Towards the digital music l ibrary:  \ntune retrieval from acoustic input  348 \nBerenzweig  \net al./04 A large-scale evaluation of acoustic and \nsubjective music -similarity measures  230 \nNorth et al./00  The importance of music to adole scents 224 \nLevitin, D.  \nJ./94 Absolute memory for musical pitch:  \nevidence from the production of learned \nmelodies 184 \nVoida et al. /05 Listening in: practices surroun ding \niTunes music sha ring 121 \nEllis &  \nWhitman/02 The quest for ground truth in mus ical \nartist similarity  111 \nNorth et al./04  Uses of music in everyday life  111 \nBoltz et al. / 91 Effects of background music on the  \nremembering of filmed event  100 \nPauws &  \nEggen/03 Realization an d user evalu ation of an \nautomatic playlist gener ator 100 \nLee &  \nDownie/04 Survey of music information needs,  \nuses, and seeking behaviours:  \npreliminary findings  82 \nTable 1. The top 10 most cited user studies   \nTable 1 presents the top 10 most cited user studies in \nthe field. There is a mix of user experiments, evaluation \nof particular systems, studies of information behaviors \nand user-generated data, etc. The most heavily cited user \nstudy was by McNab et al. In this study, 10 users were \nasked to sing 10 songs from memory  which were taped \nfor analysis of key, pitch, contour, etc. The article was \npublished in Proceedings of the First ACM International \nConference on Digital Libraries and was cited widely in \nvarious papers on content -based music retrieval systems \nand measures . We believe that the heavy citation of this \npaper and also Levitin  was at leas t partly due to the fact \nthat they were early paper s which dealt with content -based MIR, a topic which has domi nated MIR research \nfor the past decade.  Studies by Berenzweig et al.  and Ellis \n& Whitman explore measures for generating ground truth \nbased on user data which is strongly relevant  to the \nevaluation of algorithms , another big accomplishment of \nthe past deca de (i.e., MIREX). Studies of more general \nuser needs and behaviors (North et al., Lee & Downie) \nmay have had a broader impact to multiple areas related \nto music. The popularity of particular music application \n(Voida), association of music and other multime dia (Boltz \net al.), and particular organizational measures (Pauws & \nEggen) also seem to affect the heavy citation patterns.        \n5. RESEARCH DESIGN OF USER STUDIES  \nLastly, we examined the studies more deeply in order to \nlearn more about the research design of these user studies.  \nWe analyzed the content of the studies to discover the \ntypes and frequency of the various methods used ( Fig. 6). \n \nFigure 6.  Research methods used in user studies \nExperiment and usability testing were most commonly \nused (42%). The predominance of these methods may \nsuggest that we are heavily focusing on evaluating what is \nout there rather than focusing on deeper problems or \nquestions, a similar issue noted in other areas such as HCI \n[5]. These studies are primarily evaluating performance \n(e.g., error rate/time to perform task with a new system); \nidentifying usability issues (i.e., interface design \nproblems); or investigating acceptability of new system/ \ninterface. The full user-centered design process should \ninclude stages supporting coming to an understanding of \nthe users, development of system prototype(s), and \nevaluation of the prototypes with users. However, \nrelatively few papers presenting a new system include \nboth an initial user requirements elicitation study and a \nfollow-up performance/usability/acceptability study.  \nWe also investigated the scale of these user studies by \ntabulating how many human subjects were involved in \nthese studies. 124 user studies involved human subjects, \nand 26 analyzed human- generated data such as queries, \ntags, etc. 7 studies did not directly involve human su b-\njects or human-generated data, as they were papers based \non literature review, meta-analysis, or theoretical reaso n-\ning. Figure 7 shows the distribution of the number of h u-\nman subjects included in the studies of real users. Many \nstudies are of fairly small scale: 57 of the 124 studies \n(46%) involve 20 or fewer human subjects, and 102 stu d-  \n \nies (82%) involve 100 or fewer subjects.  \nNote that the active involvement of participants is lim-\nited for lab experiments and usability tests, which typica l-\nly run at most a couple of hours. Ethnographic observ a-\ntions are constrained by the time available to the r e-\nsearcher to conduct observations. Interviews, surveys, and \nfocus groups are attractive in that they may invite intr o-\nspection and comment on music-related behavior over the \nlong term, but at the cost of relying primarily on retr o-\nspection rather than direct, measurable experience. Only \ndata analysis and the diary study naturally offer the o p-\nportunity to examine authentic music information beha v-\nior over the long term, though 'long term' studies are \nmainly of one to four weeks. In evaluations of specific \nsystems, the common finding is that the users like the new \nsystem and find the new interface entertaining or nov el-—\nbut it is generally unclear how or whether participant b e-\nhavior may change after the novelty effect wears off. \n \nFigure 7.  Number of subjects in user studies \n6. DISCUSSION \nBased on the results of our analyses  as well as our own \nexperience s in conducting music user studies , we provide \na list of challenges/issues facing researchers who conduct \nmusic user studies which require further discussion. We \nbelieve that these issues are stemming from the uniqu e-\nness of the subject and the r esearch domain.     \n6.1 Fast-changing Field \nWe believe that the speed with which the MIR field has \nevolved has had a strong affect on both the scale of user \nstudies as well as the longevity of the research findings of \nthese studies. The rapid development of tools and tec h-\nnologies for music storage , distribution, and experience in \nthe past few decades has been remarkable. Some of the \nmost popular music related services today such as Spotify \nor YouTube launched less than 6 years ago . This implies \nthat how our users envision and expect from music se r-\nvices are most likely changing rapidly as well. Most of \nthe young adults today probably never had to deal with \nphysical media and grew up with various music streaming \nservices. The results from studies that investigated how \npeople find and purchase music on such physical media \nwill have limited applicability today.   \nWe conjecture that th e fast-changing field  is at least \none of the reasons for the prevalence of small -scale \nstudies. Large -scale studies take longer, in terms of  recruiting human subjects, as well as collecting and \nprocessing data, in particular if researchers want to \nincorporate a qualitative component. Longitudinal studies \nare by definition time -consuming. Due to the rapidly \nchanging e nvironment, researchers are constantly under \npressure to conduct and publish studies swiftly. This can \nbe especially true for those who are trying to test a parti c-\nular system or met hods for providing access to music , as \nthere is a good chance that  by the time the research gets \npublished in a journal, the results are already outdated.  \nThis may also explain a large proportion of user studies \nbeing published in conference or workshop proceedings.  \n6.2 Issue of Generalization  \nA large proportion of MIR user studies  are small to mod-\nerate scale studies investigating a limited number of \nusers. How does the scale of the study  affect the genera l-\nizability of its results? Can we in fact make any reasona-\nble inferences from studies  of this scale  that are \ngeneralizable to a l arger user population?  In addition, at \nleast in certain parts of the world, it is not possible to o b-\ntain a comprehensive list of email addresses for the pu r-\npose of survey due to privacy concerns. This means that \nwe often have to resort to convenience sampl ing, and \nstudy participants  are in fact most frequently drawn from \nstudents or co -workers of the researchers which again \ncan negatively affect the generalizability of our findings.      \nA point worth noting here is that researchers of music \nusers are  trying to grapple with this nebulous idea of \nusers. Who really are our users? Where do we draw the \nboundaries? M usic is so pervasive in our li ves that it is \ndifficult to know who is and is not affected by music. \nMoreover music is often enjoyed and sought out ac ross \ndifferent regions and cultures. Many of the MIR systems \nand services are now being used by global user base. \nThus researchers of music users, in some sense, are e x-\npected to derive findings that can potentially have global \nimplications on a wide range of users across space and \ntime. Then how do we define and randomly sample this \npopulation in a practical sense?  Even if we draw an \nartificial boundary and try to sample a smaller population, \nthe subjects who participate in our studies will most likely \nbe people who are interested in music to some degree. In \nthis sense, the results are always likely to be biased.  \nDue to these issues, we believe that rather than aiming \nfor generalizing the research findings, it might be helpful \nto take an alternative approach to understanding the pu r-\npose of these studies that each of these studies is disco v-\nering some piece of information about the users that is \ncorrect, but not comprehensive. When multiple pieces are \nput together, common themes emerge which we can ge n-\neralize over multiple groups of users, as well as unique \nthemes that can only apply to a particular user group.    \n6.3 Lack of Systematic Synthesis of Research Results  \nAlthough a large proportion (26%) of user studies were \npublished in the proceedings of ISMIR conference, other \nstudies were published in journals and conferences in \nmultiple domains including LIS, HCI, Musicology, Ps y-  \n \nchology, etc. We had to repeat our search in multiple d a-\ntabases in order to retrieve all these studies scattered in \nmultiple domains. Despite of our best efforts, we would \nnot be surprised if there were studies we were not able to \nfind. We suspect that this is probably one of the reasons \nhindering the synergic impact of these studies. Without \nbeing able to easily find all the previous user studies that \nhave dealt with similar research questions and user pop u-\nlations, we will essentially reinvent the wheel every time. \nIn order to resolve this issue, there  is a need for addition-\nal review art icles such as [7] and also an archive of all \nthe citation information of user studies related to music.  \nAs the first step, we made our list of u ser studies with full \ncitation available on the web1. However, a static webpage \nis far from an ideal way for collecting and sharing this \ntype of information. We believe a more sustainable sol u-\ntion is needed, managed by multiple stakeholders.   \n6.4 The Disconnect Between System/Evaluation Task \nDesigners and User Studies Researchers  \nIn MIREX, the evaluation task is typically propos ed by \nresearcher(s) who are involved in developing algorithms \nrelated to the task. In the MIR domain, however, \nresearchers who conduct user studies are not always al-\ngorithm developers themselves; this is e specially true for \nresearchers  engaged in studies of music users focusing on \ninformation needs or behaviors . This disconnect may be \none of the rea sons why we have not seen a significant \nchange in the way evaluation tasks have been  run over \nthe past seven year s since MIREX started in 2005 . Some \nof the sugge stions made in th e user studies might be l o-\ngistically impossible to implement, or the evaluators \nmight not even agree with those suggestions. Without a \nmore thorough investigation asking the system develo p-\ners and the organizers of  evaluation tasks, it will be \npremature to determine what the exact reasons are.  \n7. CONCLUSION AND FUTURE WORK \nIn this paper, we reflected on how music user studies have \nbeen conducted  and published , and what impact these \nstudies have had on the field. Findings from our analysis \nsuggest that there may be multiple layers of ba rriers for \nthe user studies to make a strong impact: lack of findabi l-\nity due to the scattered patterns of publication, weak co n-\nnections among scholars, dominance of small scaled stu d-\nies that are difficult to gener alize, etc. The purpose of this \nwork is to provide an opportunity for starting a discussion \nat the ISMIR where many stak eholders involved in MIR \nresearch can together explore potential solutions to the \nissues raised in this paper.  Thus, we want to conclude  our \npaper with a set of questions that need further discu ssion: \nFor researchers conducting user studies: \n How can we provide systematic and intelligent access \nto the work we produce? Is there a sustainable met h-\nod? Maybe a collaboratively managed resource ?  \n                                                           \n1 http://www.jinhalee. com/miruserstudies   Is it necessary to change our research questions, \nmethods, study populations, or venues in increase \nimpact and affect change in the field? \nFor system and evaluation task designers/developers: \n How do you find out about new research on users and \nkeep yourself updated? Are there particular kinds of \npublications do you seek often?  \n What kinds of user studies do you find most and least \nuseful? What do you see as the grand challenge in the \narea of MIR user studies? \nIn our future studies, we plan to survey and interview \ndesigners/developers of music related services and sy s-\ntems as well as organizers of MIREX evaluation tasks in \norder to more deeply understand the impact of these user \nstudies. Specifically, we are interested in how the info r-\nmation on users are disseminated and diffused in the MIR \ndomain, and how that knowledge may or may not affect \nthe ways music services/systems are designed and mod i-\nfied. A deeper understanding on what kind of user info r-\nmation is actually sought by system designers/developers \nwill be significant for researchers of MIR user studies.  \n8. ACKNOWLEDGMENTS \nWe thank Gary Gao  and Tiffany Huang  at University of \nWashington  for their valuable contribution s to this project.  \n9. REFERENCES \n[1] S. Bradford: “Sources of information on specific \nsubjects,” Journal of Information Science,  Vol. 10, \nNo. 4, pp. 173-180, 1985.  \n[2] S. J. Cunningham : “User studies: a first step in \ndesigning an MIR testbed,” In The MIR/MDL \nEvaluation Project White Paper Collection  (3rd ed.). \nChampaign, Illinois: GSLIS , pp. 19-21, 2003. \n[3] J. S. Downie: “Music information retrieval,” Annual \nReview of Information Science and Technology , Vol. \n37, ed. B. Cronin, Medford, NJ: Information Today, \npp. 295-340, 2003. \n[4] J. Futrelle and J. S. Downie: “Interdisciplinary \ncommunities and research issues in music \ninformation retrieval,” Proceedings of the 3rd ISMIR \nConference , pp. 215-221, 2002. \n[5] S. Greenberg and B. Buxton: “Usability evaluation \nconsidered harmful (some of the time),” Proceedings \nof the CHI’ 08, pp. 111-120, 2008. \n[6] D. L. Hansen, B. Schneiderman , and M. A. Smith: \nAnalyzing social media networks with NodeXL: \nInsights from a connected world , Burlington, MA: \nMorgan Kaufmann, 2011.  \n[7] D. M. Weigl  and C. Guastavino: “User studies in the \nmusic information retrieval literature,” Proceedings \nof the 12th ISMIR Conference , pp. 335-340, 2011."
    },
    {
        "title": "Understanding User Requirements for Music Information Services.",
        "author": [
            "Jin Ha Lee 0001",
            "Nichole Maiman Waterman"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417625",
        "url": "https://doi.org/10.5281/zenodo.1417625",
        "ee": "https://zenodo.org/records/1417625/files/LeeW12.pdf",
        "abstract": "User studies in the music information retrieval and music digital library fields have been gradually increasing in re- cent years, but large-scale studies that can help detect common user behaviors are still lacking. We have con- ducted a large-scale user survey in which we asked nu- merous questions related to users’ music needs, uses, seeking, and management behaviors. In this paper, we present our preliminary findings, specifically focusing on the responses to questions of users’ favorite music related websites/applications and the reasons why they like them. We provide a list of popular music services, as well as an analysis of how these services are used, and what qualities are valued. Our findings suggest several trends in the types of music services people like: an increase in the popularity of music streaming and mobile music con- sumption, the emergence of new functionality, such as music identification and cloud music services, an appreci- ation of music videos, serendipitous discovery of music, and customizability, as well as users’ changing expecta- tions of particular types of music information.",
        "zenodo_id": 1417625,
        "dblp_key": "conf/ismir/LeeW12",
        "keywords": [
            "user studies",
            "music information retrieval",
            "music digital library",
            "large-scale studies",
            "user behaviors",
            "music needs",
            "uses",
            "seeking",
            "management behaviors",
            "preliminary findings"
        ],
        "content": "UNDERSTANDING USER REQUIREMENTS FOR  \nMUSIC INFORMATION SERVICES  \nJin Ha Lee  Nichole  Maiman Waterman  \nThe Information School  \nUniversity of Was hington  \njinhalee @uw.edu The Information School  \nUniversi ty of Was hington  \nmaiman@uw.edu \nABSTRACT \nUser studies in the music information retrieval and music \ndigital library fields have been gradually increasing in re-\ncent years, but large-scale studies that can help detect \ncommon user behaviors are still lacking. We have co n-\nducted a large-scale user survey in which we asked n u-\nmerous questions related to users’ music needs, uses, \nseeking, and management behaviors. In this paper, we \npresent our preliminary findings, specifically focusing on \nthe responses to questions of users’ favorite music related \nwebsites/applications and the reasons why they like them. \nWe provide a list of popular music services, as well as an \nanalysis of how these services are used, and what qualities \nare valued . Our findings suggest several trends in the \ntypes of music services people like : an increase in the \npopularity of music streaming and mobile music co n-\nsumption, the emergence of new functionality, such as \nmusic identification and cloud music services, an apprec i-\nation of music videos, serendipitous discovery of music, \nand customizability, as well as users’ changing expect a-\ntions of particular types of music information .  \n1. INTRODUCTION \nUnderstanding what kinds of music information services \npeople use, how they use them, and what they expect \nfrom them is critical in designing successful services. We \nhave seen a gradual increase in different types of user and \nusability studies in recent years. However, many of these \nstudies are based on a limited number of subjects, and \ntend to employ analysis of qualitative research meth ods, \nlike in-depth interviews or focus groups . While these \nkinds of studies can help uncover rich data about music \nusers, large-scale user studies are also necessary in order \nto test the generalizability of results and to complement \nthe insights obtained from smaller qualitative studies. \nTo fill this gap, we have conducted a large-scale user \nsurvey questioning people’s music needs, uses, and music \nseeking and management behaviors. This survey is an ex-\ntension of previous research conducted in 2004 by Lee and Downie [7]. The information we acquired through \nthis new study can help improve our general understan d-\ning of music users and their behaviors, as well as how \nthey have changed as compared to the 2004 survey r e-\nsults. \n2. LITERATURE REVIEW \nWe conducted an extensive literature search in order to \nfind out how many large-scale user studies exist in the \nMIR domain. Of the 87 studies discovered, only 6 involve \nmore than 100 subjects (with the exception of studies ana-\nlyzing user generated content such as queries/reviews). \nEllis et al. [4] developed a web-based game named “M u-\nsicSeer,” which collected over 6,200 responses ; they \nfound that “subjective artist similarities are quite variable \nbetween users,” suggesting that the concept of a single \nground truth may be problematic. Barrington et al. [1] \nstudied 185 subjects and asked them to evaluate results \nfrom multiple music recommender systems. Both of these \nstudies focused on highly specific ideas, such as respon s-\nes about artist- to-artist relationships [4] or recommend a-\ntion results [1], rather than general music behaviors. \nSome studies dealt with particular organizations’ user \ngroups. Lai and Chan [5] surveyed 244 Hong Kong Ba p-\ntist University Music Library users to improve unde r-\nstanding of their needs, usage patterns, and preferences \ntoward various collections. The authors found that parti c-\nipants used scores and multimedia more frequently than \nother types of library materials, although they believed \nthat electronic journal databases, books, and online music \nlistening were also important to their academic and pe r-\nformance needs. In their survey of visitors to the Exper i-\nence Music Project in Seattle, Maguire et al. [9] found \nthat improving the user interface was the most important \nsuggestion for changes to the museum’s digital collection. \nOther studies dealt with broader topics and more ge n-\neral user populations. Lesaffre et al. [8] collected 663 \nqualified survey responses to clarify the influence of d e-\nmographics and musical background on how people d e-\nscribe music’s semantic qualities. Their research listed \nseveral characteristics that average MIR system users \nlikely would have, and found that gender had the most \nsignificant influence on music perception. Brinegar and \nCarpa [2] also surveyed 184 respondents on how they \nmanage music across multiple devices, and provided e m-\npirical data on the sizes of user collections, the met h-\nods/reasons for synchronization, how users dealt with m u-Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012 Internationa l Society for Music Information Retrieval    \n \nsic loss, and so on. Lee and Downie [7] conducted a \nlarge-scale music survey in 2004 asking two groups of \nrespondents (University of Illinois community, and ge n-\neral adult public) about their music information needs, \nuses, and search/browse patterns. The ir analysis revealed \nthe social aspects of music information seeking – that it \ncan be a public and shared process, and many users felt \npositively towards reviews, ratings, and recommendations \nfrom other people. The authors also stressed the i m-\nportance of providing context metadata (i.e., metadata on \na music item’s relationships with other items, and its as-\nsociations with other works). Our study aims to add fu r-\nther insights into music users’ behaviors; in particular, \nthis paper focuses on discovering how people use curren t-\nly available music services and why they favor them.  \n3. STUDY DESIGN \n3.1 Study Population and Sam pling \nThe design specifics of the 2004 and 2012 surv eys are \nsummarized in Table 1. For the 2004 survey, the cand i-\ndate respondents for Group I were randomly selected \nfrom a list of students, faculty, and staff from the Uni ver-\nsity of Illinois at Urbana-Champaign. For Group II, inv i-\ntations to the survey were posted in music-related mailing \nlists/forums in order to recruit participants. For the 2012 \nsurvey, we posted invitations on mailing lists at Univers i-\nty of Washington as well as music-related mailing lists. \nWe also recruited participants through the authors’ social \nmedia network such as Facebook, Twitter, and Google+. \n 2004 Survey  2012 Survey \nStudy  \npopulation Group I: \nUIUC  \ncommunity Group II:  \nGeneral  \npopulation UW community \n+ General  \npopulation \nSampling Random Convenience Convenience \n# questions  19 21 23 \n# responses 436 312 520 \nTable 1. Basic statistics of the surveys \nIn the 2004 survey we asked 19 questions for Group I \nand 21 questions for Group II (2 additional questions on \njob type and education level). The questions covered \nwhy, where, how, and how often users seek and obtain \nvarious kinds of music information; who they ask for \nhelp; how they use music information; what music-related \nwebsites/apps they use; and so on. The design of the 2012 \nsurvey was based on the previous survey to facilitate r e-\nsults comparison. The 2012 survey included 4 additional \nquestions about how users manage physical and digital \nmusic collections, what devices they use to listen to m u-\nsic, and any comments related to the survey. In both sur-\nveys, there were follow up questions that were asked \nbased on how users answered the main questions.  \n3.2 Limitation \nOne concern is that the different populations and sa m-\npling methods might affect the comparability of the r e-\nsults. For the 2004 survey, we were able to obtain a full \nlist of all UIUC community members, and thus were able to randomly select participants. I n the 2012 survey, it was \nnot possible to obtain such a listing of the UW communi-\nty for survey purposes, due to privacy concerns. When we \ncompared the demographic information of respondents, \n[Table 2], there were in fact some differences. The ave r-\nage age was slightly higher for the 2012 survey respon d-\nents and the dominant gender was also different. Howe v-\ner, most of the respondents did come from the United \nStates for both surveys. In the article reporting the full \nsurvey results, we will be presenting the results contro l-\nling for these particular variables in order to see if there \nare significant differences between these sub-user groups. \nNevertheless, it is important to be aware of th is limitation \nin interpreting the findings and implications of this study.        \n 2004 Survey  2012 Survey \nAge Average: 30  Average: 37  \nGender (excluding \nunanswered)  M (50.4%)  \nF (46.1%)  Male (36.2%)   \nFemale (58.8%)  \nGeographic loc ation 73.8% US  60.3% US  \nTable 2. Demographic information of respondents  \n4. DATA AND DISCUSSION \n4.1 Overview \nIn this section, we present a detailed analysis of one of the \nopen-ended questions, which ask ed about users’ favorite \nmusic-related websites /applications. We also compare the \nresponses we obtained for this question in 2004 and 2012 , \nand present excerpts from users’ responses and quantit a-\ntive data o n user responses from other relevant questions. \n4.2 Summary of Results \n4.2.1 Favorite Music-related Websites/Applications \nThe exact question asked was “ What are your favorite \nmusic-related websites or apps? What do you like about \nthem?” We received a total number of 237 responses \nfrom Group I, 229 from Group II in the 2004 survey, and \n419 responses in the 2012 survey. Many users mentioned \nmore than one website/application in their responses, so \nthe total number of references to individual websites/apps \nadded up to 1002 for the 2004 survey (combined) and \n945 for the 2012 survey. Table 3 summarizes the services \nthat received 5 or more responses from both surveys.  \nWe can observe that a variety of different types of ser-\nvices were mentioned: Internet radio/streaming, music \nmanagement and purchase, music identification, dictio n-\nary-type sources, reviews, etc. Many of our users seemed \nvery savvy and knowledgeable, and specified multiple \nfavorite websites and applications, explaining that they \nuse each of them for very specific purposes.  \nWhen we compared the results from both surveys, we \nnoticed a heavier concentration of responses with partic u-\nlar websites in 2012 survey. Only 5 websites were me n-\ntioned more than 5 times in both surveys and another 16 \nwere new (in bold). There are a few completely new types \nof services, such as music identification (e.g., Shazam, \nSoundhound), and cloud music (e.g., Spotify, Groove-\nshark, Google Music). Peer- to-peer file sharing applic a-  \n \ntions like Kazaa (in 2004) or general search engines like \nGoogle disappeared from the top list in 2012. The signif i-\ncant increase in the popularity of iTunes can probably be \nexplained with the increasing use of mobile devices. \n2004 Survey (co mbined) 2012 Survey  \nWebsites # % Websites/Apps  # % \nAmazon 58 12.4 Pandora 149 35.6 \nAll Music Guide  36 7.7 YouTube  68 16.2 \nLaunch 25 5.4 Spotify 57 13.6 \nMTV 20 4.3 iTunes 56 13.4 \nKazaa 19 4.1 Shazam 32 7.6 \nCD Now 18 3.9 Amazon 30 7.2 \niTunes 17 3.6 Naxos 25 6.0 \nMudcat Café  15 3.2 Last.fm 25 6.0 \nRolling Stone  12 2.6 Grooveshark  25 6.0 \nBillboard  10 2.1 Pitchfork 20 4.8 \nPitchfork 10 2.1 All Music Guide  20 4.8 \nGoogle 9 1.9 NPR 16 3.8 \nLyrics.com  8 1.7 Grove Music \nOnline 12 2.9 Grove Music \nOnline 7 1.5 Wikipedia  11 2.6 \neBay 6 1.3 IMSLP 11 2.6 \nNetscape Radio  6 1.3 Soundhound 10 2.4 \nTower 6 1.3 Rhapsody  8 1.9 \nAndante 5 1.1 Google M usic 8 1.9 \nCMT 5 1.1 KEXP 7 1.7 \n   Soundcloud  6 1.4 \n   ArkivMusic  5 1.2 \nTable 3. Services mentioned by 5 or more users. \nWe conduct ed a more thorough analysis of how these \nservices were being used. Table 41 shows a list of how \nusers specified they used different services , and how often \nthose behaviors were mentioned in the surveys. We n o-\nticed a general trend of greater direct music consumption \nfrom these websites and applications, mostly due to the \nincreased number of streaming and cloud music services. \nThere was a significant drop among the several responses \nrelated to general music-information seeking, i.e., “To \nlearn about the artists/bands.” We conjecture that this has \nto do with the emergence and rising popularity of major \nmusic related websites and applications that serve pur-\nposes other than just providing music information. The \nexistence and popularity of these websites now seem to \nheavily affect users’ perception of what to expect from \nmusic services. Considering that 16 of the 21 top-rated \nservices did not exist in 2004, this is not very surprising. \nThe data also suggest that the expectations from users \nregarding access to particular types of music information \nmay have changed. For instance, websites providing ly r-\nics information were sought by 7.3% of users in the 2004 \nsurvey, whereas in 2012, only 1.2% of respondents men-\ntioned the need for lyrics information. Instead of visiting \na particular website for lyrics, we suspect that many users \nare able to utilize a phrase search option in search engines \n                                                           \n1 Table 4 and 5 are based on responses where the user specified the re a-\nson for liking the service. Some responses only specified the name or \nURL. 646 responses specified the reasons in 2004 and 644 in 2012.  like Google and are able to find links to numerous we b-\nsites that provide lyrics. In addition, certain websites such \nas YouTube are not lyrics websites, but provide video \ncontent that often incorporates lyrics information. Thus, \nusers might not even think of particular lyrics websites as \none of their favorite music related websites. This may a l-\nso be true for information on local events, which is much \neasier to find through social media in 2012.  \nWe also saw a drop in responses indicating particip a-\ntion in or value for activities of social interaction [Table \n5]. In 2004, online forums were extremely popular as a \nplace to interact with other people. However, in 2012, s o-\ncial media such as Facebook, Twitter, and Google+ are \nnow providing a space for users to discuss music, and u s-\ners may not even think of these websites as specifically \nmusic-related, thus not showing up in the survey da ta.  \nThe other category included: To track listening (new in \n2012); save wish lists; find blogs; compare versions; etc.  \n                                                   Response  \nUsage 2004  \nSurvey 2012  \nSurvey \n# % # % \nTo listen to music recordings  70 10.8 143 22.2 \nTo discover new music/artists  14 2.2 80 12.4 \nTo obtain/purchase m usic recordings  95 14.7 46 7.1 \nTo obtain music information (ge neral) 56 8.7 35 5.4 \nTo identify /verify a particular song  6 0.9 34 5.3 \nTo learn about the artists/bands  110 17.0 31 4.8 \nTo read reviews  37 5.7 30 4.7 \nTo search for/browse  music recor dings 22 3.4 23 3.6 \nTo listen to samples  before purchase  32 5.0 23 3.6 \nTo get recomme ndations 11 1.7 22 3.4 \nTo interact with other people  48 7.4 22 3.4 \nTo obtain current news/information  26 4.0 21 3.3 \nTo watch performances/music vid eos 14 2.2 18 2.8 \nTo learn more about recordings  37 5.7 18 2.8 \nTo obtain information for work/research  20 3.1 15 2.3 \nTo obtain scores  29 4.5 13 2.0 \nTo create playlists/stations  0 0.0 13 2.0 \nTo store/manage music and metadata  0 0.0 9 1.4 \nTo obtain lyrics  47 7.3 8 1.2 \nTo find out about events  29 4.5 7 1.1 \nTo share music  recordings  2 0.3 6 0.9 \nTo obtain ranking/rating information  11 1.7 4 0.6 \nOther 22 3.4 16 2.5 \nTable 4. How the websites/applications are used \n4.2.2 Reasons for liking the Websites/Applications \nFrom the user responses on why they like these we b-\nsites/applications, we were able to infer what kinds of \nqualities users perceive to be important for these services. \nAs shown in Table 5, there was a variety of different qua l-\nities mentioned by users in both surveys. We found it su r-\nprising that the quality mentioned most often was actually \nbeing exposed to new artists/music and serendipitous dis-\ncovery, even more so than being free or inexpensive. The \ndesign aspects of the system (e.g., easy and convenient \naccess to music; user-friendly system) were also pe r-\nceived as important qualities. In fact, users’ expectations \non these aspects seem to be much higher compared to   \n \nhow they were in 2004. Being able to customize or pe r-\nsonalize the service was also highly appreciated. The r e-\nsponses for comprehensive coverage of music, including \nparticular styles of music, and good music content that is \nupdated frequently and matches users’ interests/tastes, all \ndropped. We think it is unlikely that users do not believe \nthese qualities are important anymore; rather, users pro b-\nably just expect that current music services have these \nqualities to begin with. With the increasing use of mobile \ndevices and a variety of applications, compatibility also \nsurfaced as a new important quality for users.  \nThe other category included: innovative, high quality \nrecordings and writing, different purchase options, \nproviding alerts, being able to listen to the whole album, \nnot posting to Facebook, not hogging resources, directly \npaying artists, fewer bugs, etc. \n                                           Response  \nQuality 2004  \nSurvey 2012  \nSurvey \n# % # % \nExposure to new things/Serendipity  18 2.8 80 12.4 \nFree/Inexpensive  50 7.7 68 10.6 \nEase of access /Convenience  9 1.4 52 8.1 \nCustomizability/ Personalization  8 1.2 49 7.6 \nUser-friendly/Ease of use  28 4.3 46 7.1 \nComprehensive/ Exhaustive  coverage 64 9.9 37 5.7 \nVariety/Wide sele ction 51 7.9 36 5.6 \nAccess to particular style of m usic 69 10.7 28 4.3 \nCompatibility /Use with other d evices 1 0.2 25 3.9 \nAccess to music samples  18 2.8 23 3.6 \nGood search/browse functions  8 1.2 23 3.6 \nSocial/Ability to interact with ot hers 52 8.0 22 3.4 \nMatches user ’s interest/taste  67 10.4 21 3.3 \nGood music/content  61 9.4 16 2.5 \nQuick/Instant service  7 1.1 16 2.5 \nComparative data/Similar music  8 1.2 14 2.2 \nNo rights management/restrictions  0 0.0 10 1.6 \nFun/High entertainment value  2 0.3 9 1.4 \nAuthority/Credibility  of information 7 1.1 8 1.2 \nDoes not require much user input 1 0.2 8 1.2 \nRare/Obscure recor dings/information  17 2.6 7 1.1 \nFamiliarity /Set as default  8 1.2 6 0.9 \nAbility to store/archive recor dings 0 0.0 6 0.9 \nNew content/Updated frequently  48 7.4 5 0.8 \nAccuracy/Reliability of info rmation 5 0.8 5 0.8 \nAccess to local information  5 0.8 4 0.6 \nGood organization /design 11 1.7 3 0.5 \nNo or fewer ads  6 0.9 3 0.5 \nOther 12 1.9 31 4.8 \nTable 5. The list of qualities valued by users \n4.3 Discussion of the Trends in 2012 \n4.3.1 Popularity of Streaming Services \nAnalyzing the responses from both surveys clearly reveal \nthe increasing popularity of Internet radio/music strea m-\ning services. With the rising use of various mobile devi c-\nes such as tablets and smartphones, it is not surprising that \nstreaming service is also becoming increasingly prevalent . \nMusic is only one of many types of digital media users \nstore on mobile devices, in addition to photos, videos, games, documents, etc., and numerous apps. This means \nthat even though the storage space of these devices is a l-\nways growing, the space allocated for music will always \nbe limited. Listening to streaming music services rather \nthan carrying one’s own collection is one way to resolve \nthat issue, as noted in comments below. Some comments \nalso implied that there are songs users want to own vs. \nsongs they just want to listen to now and then. \n“I like them because I can still listen to music without cluttering \nup my phone or work computer with extra files.” \n“I also use things like spotify and pandora to listen to music \nthat I don't necessarily want to own but have a hankering for \nnow and again.”  \nThe quantitative data also support this trend. Table 6 \nshows various response statistics to questions related to \nInternet radio/streaming services and mobile music co n-\nsumption. When we compare the frequency of users li s-\ntening to these services from the 2004 and 2012 surveys \n(the first and second rows), we see a significant increase \nin the proportion of respondents (+30.7%) who use these \nservices 5 or more times per month as well as a large d e-\ncrease in the users (-16.5%) who never use these services. \nTwo new questions were asked in the 2012 survey about \nhow often people use music/music-themed apps on m o-\nbile phones (third row), and search for music heard \nthrough online streaming services (fourth row). 20.7% \nindicated they use music related apps “a few times a \nweek” (8.4%) to “almost every day” (12.3%), implying a \nheavy mobile consumption of music by these users . \nStreaming music was also an important trigger for music \nsearching ; a total of 77% of respondents indicated that \nthey search for music heard on streaming services at least \nonce a month, and 22.8% do it “a few times a week” \n(12.0%) or “almost every day” (10.8%).  \n                                  \n                                    Response    \n \n \n   \n \nSource Positive Never Count \nFrequency  \n(times per \nmonth) Total Total \n1 2-4 5 \n% % % % # \nListening to strea ming music/ \nonline radio (old)  25.5 27.5 25.5 21.6 1066 \nListening to strea ming music/ \nonline radio (new)  13.3 25.4 56.2 5.1 488 \nUsing music or m usic-themed \napps on m obile phone (new)  14.0 18.4 20.7 46.9 478 \nSearching for music  heard from \nonline streaming se rvice (new)  24.6 29.6 22.8 23.2 501 \nTable 6.  Various statistics related to streaming \nservices and mobile music consumption \n4.3.2 Emergence of Music Identification Services \nMusic identification services like Shazam and Soun d-\nhound also seem quite popular (42 responses combined). \nTo provide a baseline for comparison, Table 7 shows the \nresponses to different options for the question, “ How of-\nten to do you ask the following people/services for help   \n \nwhen you search for music or music information? ” \nFriends and family members received the most positive \nresponses. Over half (56.6%) of the responses indicated \nusers consulted their social networks, and 43.5% of the \n503 users said they have used music identification se r-\nvices. Overall the proportion of users who use this kind of \nservice is sti ll less than those who ask other people.  \n         Responses from                   \n                            2012 \n  Frequency  Friends and  \nfamily People on  \nSocial \nNetwork Music ID  \nservice \nAlmost every day  2.4% 1.6% 1.0% \nA few times a week  7.9% 6.3% 4.6% \nAbout once a week  10.4% 6.5% 5.6% \n2 or 3 times a month  20.0% 12.4% 5.6% \nOnce a month or less  37.7% 29.9% 21.1% \nNever 21.6% 43.3% 56.5% \nTotal respon ses 509 508 503 \nTable 7. Frequency of users asking for help when \nsearching for music or music information \nHowever, when asked about how likely they would be \nto use the search/browse option of a music identification \nservice, only 28.1% answered positively (Very likely + \nSomewhat likely), 62.4% answered negatively (Not very \nlikely + Not at all likely), and 9.5% said “Don’t know.” \nWe may infer that some people might use it out of cur i-\nosity but would not use it again. Considering that this type \nof service is still relatively new, we suspect that users’ \nresponses may change over tim e. We have observed sim i-\nlar results for a number of other search/browse options \nwhen we compared the responses from 2004 and 2012 \nsurveys (e.g., “purchase patterns” (+20.6%), “recomme n-\ndations from other people” (+14.9%), “mood/emotional \nstate induced” (+7%) of positive responses in 2012).  \n4.3.3 Music combined with other Multimedia \nIt is interesting to note that YouTube was the second most \npreferred service by users in the 2012 survey , despite that \nthe main objective of the website is not to provide music \ncontent. In  addition to the benefit of being able to see m u-\nsic videos and concert/performance footage, the extensive \ncoverage of YouTube was also highly valued by users as \nnoted in the comments below: \n“…gives an incredibly large choice of uploaded music to listen \nto (once again, including some specialized and rare items I \nwouldn’t be able to find in my local library).” \n“…I practically never searched for a song I didn’t find on their \nservers.” \nYouTube was also seen as a place where many new \nartists post their work, where you can find “official” m u-\nsic videos, and hear a great deal of covers or different \nversions of songs. Some users also think of YouTube as \nan archive of old and rare music related materials. In a d-\ndition, users appreciated that they do not need specia l \nhardware/software to use the service.: “…the clips will play on any reasonable electronic platform \n(not restricted to certain brands or types, not requiring certain \nsoftware beyond what it [sic] is likely to be on computers or \nsmartphones already).”    \n4.3.4 Serendipitous Discovery of New Music \nExposure to and serendipitous discovery of new m u-\nsic/artists were very important to users in the 2012 survey \n[Table 5]. We conjecture that the popularity of services \nlike Pandora or Spotify is greatly affecting user expecta-\ntion. 32 responses specified that serendipitous discovery \nis the very reason why they like Pandora. Users gave split \nresponses to the question asking how likely they would be \nto use the search/browse option “by recommendations \nfrom recommender systems”: 50.8% positive, 46.9% ne g-\native, and 2.2% “Don’t know.” We saw a few responses \nthat shed insight into why some people may not be \npleased with current recommender systems: \n“I've found a few new songs and artists I like through it, but I \nget frustrated with it sometimes when it thinks I like a whole \ngenre because of one song, and it doesn't repeat the songs I \nREALLY like often enough.” \n“…even though it has stupid ads and plays music I don't like \nhalf the time... just because it's easy.” \nRecommender systems, of course, play a major role in the \nserendipitous discovery of music, but users mentioned \nemploying other resources (e.g., YouTube, Pitchfork) to \nfind new music, as well. By interviewing users about how \nthey evaluate playlists, Lee [6] found that people definite-\nly like learning new things, but still want them contextua l-\nized in familiar territory. We saw several comments that \nresonate with this finding: \n“it exposes me to artists I'd never heard of before in genres I \nenjoy.” \n“they either play music I already like/know or introduce me to \nmusic that suits my tastes in an easy, unobtrusive way.”  \n4.3.5 Customizability vs. Not Requiring User Input   \nAs shown in Table 5 , users’ belief that a service is cu s-\ntomized for them or that they are able to personal ize it \nthemselves seems very important and has a strong, posi-\ntive effect on how they feel about th at service. Some e x-\namples of comments include: “ stations tailored to my m u-\nsical tastes and moods ,” “nice customization opportun i-\nties,” “I like that you can say you like or dislike songs ,” \nand “I can adjust it to play music I like .” It is actually dif-\nficult to say how much these beliefs are objectively justi-\nfied; for instance, do users understand the technical pr o-\ncess of what happens after they like or dislike particular \nsongs recommended by the service? We suspect that most \nusers do not know how much these services actually i n-\ncorporate their input to modify the results presented, e x-\ncept for the vague idea that they are somehow making it \nbetter to suit their taste s. This sense of control seemed to \nbe what was important to them, rather than a set of perfect \nresults [further discussion in Section 5].   \n \nIt is also important to note that there exist a smaller \nnumber of users who prefer “not doing much.” I n order to \nappeal to these users, it will be important to provide an \noption to have an automatic algorithm learn their tastes \nand do the work on their behalf. Some of their comments \ninclude: “ I like that Pandora streaming radio lets me be \nlazy,” “making playlists is too much work most of the \ntime,” “they are free and do not require me to download \nor own anything . Streaming is key. ” \n5. CONCLUSION AND FUTURE WORK \nThis work is part of a bigger research agenda that aims to \nprovide an empirical basis for the development of music \nservices reflecting the needs of real users. Our findings \nsuggest several changes in the kinds of music services \npeople like: an increase in the popularity of streaming \nservices and mobile music consumption, an emergence of \nnew types of services like music identification or cloud \nmusic services, an appreciation of music videos, serendi p-\nitous music discovery, and customizability, etc. \nHowever, it also became apparent that many of the u s-\ners’ music information needs in 2012 did exist in 2004. \nThe difference we see is that in 2012, a few dominant \nservices are being used to fulfill those needs rather than a \nnumber of different websites. For instance, access to m u-\nsic videos /performances has always been important; users \nin the 2004 surveys were going to Yahoo! Launch, MTV , \nand VH1, and now seem to use YouTube for the same \npurpose. Users also told us that getting recommendations \nand discovering new music was important in 2004 [7] . In \n2012, Pandora has become one of the major applications \nthat serve those needs. The social aspect of music search \nwas revealed in 2004 survey responses, where users said \nthey were asking friends and family members about music \nand going to different forums to talk to other users [7] . \nNow we have Spotify and last.fm, where people can find \nout what their friends are listening to, and Shazam and \nSoundhound to help identify music. \nAnother interesting aspect of the survey was that many \nusers recognize and accept the limitations of the services \nthey like. As noted by the excerpts below, users seem \nwilling to accept and forgive a few flaws if there are some \nother attractive aspects:  \n “…incredibly easy to use, awesome service, great wide-\nranging library, integrated information, always being updated - \nglitchy at times and doesn't have everything but more than \nmakes up for it in convenience and design.” \n“…it will do song identification, including humming/singing \n(still a little buggy, but a great idea),it pulls up lyrics for songs \nidentified, gives you links to where you can purchase the music \nor to listen to it via the Slacker Radio app . It's really great!”    \nWe believe that this is an important point with strong \nimplications for developers of music systems and services . \nMuch of the efforts in the MIR domain have been focused \non improving the accuracy of particular algorithms, r e-\nsulting in the “glass ceiling” problem where the effectiv e-\nness of techniques has reached its limits [3]. Maybe we should also start asking about what really matters to users; \nas the users in our survey told us, ease of use, a wide var i-\nety of music, innovative ideas, compatibility with other \ndevices/apps, etc. are maybe as important as getting “ac-\ncurate” results. We hope that the list of qualities valued \nby users of music related websites/applications will help \ninform system designers and developers in modifying e x-\nisting services or creating new services .  \nA journal article reporting the detailed analysis of the \n2012 survey and comparison of the results from 2012 and \n2004 surveys is in preparation. For our future work, we \nplan to conduct additional user studies surrounding the \nexpectations of specific music services, in particular, \ncloud music services. We are also interested in analyzing \nthe failed cases, asking people what kinds of music rela t-\ned websites/applications they do not like and why .  \n6. REFERENCES \n[1] L. Barrington, O. Reid, and G. Lanckriet: “Smarter \nthan Genius? Human evaluation of music reco m-\nmender systems,” Proceedings of the ISMIR , pp. \n357-362, 2009. \n[2] J. Brinegar and R. Capra: “Managing music across \nmultiple devices and computers,” Proceedings of the \niConference , 2011. \n[3] J. S. Downie: “The music information retrieval eva l-\nuation exchange (2005-2007): A window into music \ninformation retrieval research,” Acoustical Science \nand Technology , Vol. 29, No. 4, pp. 247-255, 2008. \n[4] D. P. W. Ellis and B. Whitman: “The quest for \nground truth in musical artist similarity,” Proceed-\nings of the ISMIR , pp. 170-177, 2002. \n[5] K. Lai and K. Chan: “Do you know your music u s-\ners’ needs? A library user survey that helps enhance \na user-centered music collection,” Journal of Ac a-\ndemic Librarianship , Vol.36, No.1, pp. 63-69, 2010.  \n[6] J. H. Lee: “How similar is too similar?: Exploring \nusers’ perceptions of similarity in playlist evalu a-\ntion,” Proceedings of the ISMIR , pp. 109-114, 2011 \n[7] J. H. Lee and J. S. Downie: “Survey of music info r-\nmation needs, uses, and seeking behaviours: Prelim i-\nnary findings,” Proceedings of the ISMIR , pp. 441-\n446, 2004 \n[8] M. Lesaffre, L. D. Voogdt, M. Leman, B. D. Baets, \nH. D. Meyer, and J. P. Martens: “How potential u s-\ners of music search and retrieval systems describe \nthe semantic quality of music,”  Journal of American \nSociety for Information Science and Technology , \nVol. 59, No. 5, pp. 695-707, 2008. \n[9] M. Maguire, D. E. Motson, G. Wilson, and J. Wolfe: \n“Searching for Nirvana: Cataloging and the Digital \nCollection at the Experience Music Project,” Journal \nof Internet Cataloging , Vol. 7, No. 1, pp. 9-31, \n2004."
    },
    {
        "title": "Semi-supervised NMF with Time-frequency Annotations for Single-channel Source Separation.",
        "author": [
            "Augustin Lefèvre",
            "Francis R. Bach",
            "Cédric Févotte"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415156",
        "url": "https://doi.org/10.5281/zenodo.1415156",
        "ee": "https://zenodo.org/records/1415156/files/LefevreBF12.pdf",
        "abstract": "We formulate a novel extension of nonnegative matrix fac- torization (NMF) to take into account partial information on source-specific activity in the spectrogram. This infor- mation comes in the form of masking coefficients, such as those found in an ideal binary mask. We show that state-of- the-art results in source separation may be achieved with only a limited amount of correct annotation, and further- more our algorithm is robust to incorrect annotations. Since in practice ideal annotations are not observed, we propose several supervision scenarios to estimate the ideal mask- ing coefficients. First, manual annotations by a trained user on a dedicated graphical user interface are shown to provide satisfactory performance although they are prone to errors. Second, we investigate simple learning strate- gies to predict the Wiener coefficients based on local in- formation around a given time-frequency bin of the spec- trogram. Results on single-channel source separation show that time-frequency annotations allow to disambiguate the source separation problem, and learned annotations open the way for a completely unsupervised learning procedure for source separation with no human intervention.",
        "zenodo_id": 1415156,
        "dblp_key": "conf/ismir/LefevreBF12",
        "keywords": [
            "nonnegative matrix factorization",
            "source-specific activity",
            "spectrogram",
            "masking coefficients",
            "state-of-the-art results",
            "ideal binary mask",
            "robust to incorrect annotations",
            "supervision scenarios",
            "manual annotations",
            "graphical user interface"
        ],
        "content": "SEMI-SUPERVISED NMF WITH TIME-FREQUENCY ANNOTATIONS\nFOR SINGLE-CHANNEL SOURCE SEPARATION\nAugustin Lef `evre\nINRIA team SIERRA\naugustin.lefevre@inria.frFrancis Bach\nINRIA team SIERRA\nfrancis.bach@ens.frC´edric F ´evotte\nLTCI/Telecom ParisTech\nfevotte@telecom-paristech.fr\nABSTRACT\nWe formulate a novel extension of nonnegative matrix fac-\ntorization (NMF) to take into account partial information\non source-speciﬁc activity in the spectrogram. This infor-\nmation comes in the form of masking coefﬁcients, such as\nthose found in an ideal binary mask. We show that state-of-\nthe-art results in source separation may be achieved with\nonly a limited amount of correct annotation, and further-\nmore our algorithm is robust to incorrect annotations. Since\nin practice ideal annotations are not observed, we propose\nseveral supervision scenarios to estimate the ideal mask-\ning coefﬁcients. First, manual annotations by a trained\nuser on a dedicated graphical user interface are shown to\nprovide satisfactory performance although they are prone\nto errors. Second, we investigate simple learning strate-\ngies to predict the Wiener coefﬁcients based on local in-\nformation around a given time-frequency bin of the spec-\ntrogram. Results on single-channel source separation show\nthat time-frequency annotations allow to disambiguate the\nsource separation problem, and learned annotations open\nthe way for a completely unsupervised learning procedure\nfor source separation with no human intervention.\n1. INTRODUCTION\nDuring the past decade, nonnegative matrix factorization\n(NMF) has become the core algorithm in single-channel\nsource separation. A rich literature has been developed\nto adapt NMF to difﬁcult scenarios in which sources are\nhighly synchronized, and little or no development data is\navailable.\nIn the past years, intensive research on Bayesian mod-\nelling and parameterized methods have been conducted to\nimprove the identiﬁability of basis elements by restricting\nthe complexity of the estimated model. More recently, an-\nother category of contributions consider incorporating in-\nformation that is directly relevant to the data at hand, and\nspeciﬁed by the user. In [2], time activation of the sources\nis used to specify direct constraints on the activation coefﬁ-\ncients of the decomposition. Pitch estimates [5] were used\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2012 International Society for Music Information Retrieval.for lead voice extraction. In [8], detailed score information\nis provided so that each individual note can be separated.\nWhile these contributions may use different NMF models,\na common trait is that user information is used to spec-\nify the support of decomposition coefﬁcients at the coding\nstage. A quite different line of work is proposed in [1, 3],\nwhere isolated signals are used as proxy for the source sig-\nnals, so that information on both the basis functions and\nthe activation coefﬁcients can be used to constrain the fac-\ntorization.\nIn this paper, we propose to annotate directly the time-\nfrequency representation that is used to perform source\nseparation. We assume that we are given recordings where\na large fraction of time-frequency bins of the spectrogram\nmay be assigned unambiguously to one dominant source.\nThis hypothesis holds as long as there are not too many\nsources, and post-processing of the recording does not in-\nvolve heavily non-linear effects. As illustrated in Figure\n1, some patches in the spectrogram are cues for source-\nspeciﬁc activity, which may be exploited as information\non the optimal binary mask.\ntime (s)freq (Hz)\n3.00 3.99 4.99 1077 2153\nInstrument\npatchesVoice patches\nFigure 1: Cues from computational audio source analysis\nmay be used as information on the optimal masking coef-\nﬁcients\nIn this article we make three contributions : we propose\nin Section 2 a novel modiﬁcation of NMF (semi-supervised\nNMF) to take into account time-frequency annotations of\nthe spectrogram, that is robust to errors in the annotations.\nIn Section 2.2, we present a graphical user interface to re-\ntrieve such time-frequency annotations. In Section 3, wepropose supervised learning algorithms to automatize an-\nnotations, and explain how to combine them with semi-\nsupervised NMF. Finally, we illustrate our contributions on\npublicly available source separation databases in Section 4.\n2. SEMI-SUPERVISED NMF\n2.1 Model and interpretation\nIn this section we propose a novel modiﬁcation of NMF\nto incorporate annotations in the spectrogram. Let us ﬁrst\nbrieﬂy summarize our NMF model and introduce mathe-\nmatical notations, before proceeding to the main part of\nthe contribution.\nGiven the short time Fourier transform of a signal X2\nCF\u0002N(in the following findexes frequency and ntime),\nwe assume that X=P\ngS(g), whereS(g)2CF\u0002Nis the\nspectrogram of each source signal for g2f1;:::;Gg. De-\nﬁne the power spectrograms of the sources V(g)\nfn=jS(g)j2.\nThey are assumed to follow a linear model :\nV(g)\nfn=PKg\nk=1W(g)\nfkH(g)\nkn, whereW(g)2RF\u0002Kg\n+ ,H(g)2\nRKg\u0002N. DeﬁneK=P\ngKg,W= (W(1);:::;W(G))2\nRF\u0002K\n+ andH>= ((H(1))>;:::; (H(G))>)2RK\u0002N\n+ .\nThen, depending on the assumed distribution of S(g), es-\ntimation ofWandHamounts to minimizing d(V;WH )\nwheredis a measure of ﬁt between data and the under-\nlying model. In this article we will use the Itakura-Saito\ndivergence, but actually any \f-divergence may be used.\nGiven estimates ^V(g)\nfnof the power spectrogram of each\nsource, time domain estimates of the sources are then com-\nputed by Wiener ﬁltering, where the Wiener coefﬁcients\nof the source in the time-frequency domain are given by :\nM(g)\nfn=^V(g)\nfn\n^Vfn.\nThe key idea in our contribution is the following : sup-\npose we have at hand a set Lof annotated time-frequency\nbins and a set of time-frequency masks M(g)\nfnsuch that :\nM(g)\nfn2[0;1], andP\ngM(g)\nfn= 1if(f;n)2L,P\ngM(g)\nfn=\n0otherwise.\nFor annotated time-frequency bins, we deﬁne target val-\nues for each source spectrogram : ~V(g)\nfn=M(g)\nfnVfn.\nThe remaining, un-annotated entries of ^Vare then com-\nputed so as to ﬁt the observed spectrogram. This idea trans-\nlates into the following optimization problem :\nminX\n(f;n)dIS(Vfn;^Vfn)+\u0015X\n(f;n)2L\ng=1;:::;G\u0016fndIS(~V(g)\nfn;^V(g)\nfn);\n(1)\nwheredIS(x;y) =x\ny\u0000logx\ny\u00001is the Itakura-Saito diver-\ngence1, and optimization is subject to the constraints that\nW\u00150(point-wise nonnegativity), H\u00150, andP\nfWfk=\n1to avoid scaling ambiguity. We interpret the second term\nin Eq. (1) as a relaxed version of the constraints that ^V(g)\nfn\nbe equal to their target value M(g)\nfnVfn, for all annotated\nbins(f;n)2L.\n1Given that some values are set to zero, we replace the ISdivergence\ndIS(x;y )bydIS(\u000f+x;\u000f+y)(where\u000f= 10\u00007) in our optimization\nproblem, in order to deal with ill-conditioning of the objective function.We may tune the relative importance of annotation by\nvarying parameter \u0015, from\u0015= 0(standard NMF), to \u0015!\n+1 (in which case (WH )fn=V(g)\nfnis enforced exactly\nif there are any feasible solutions). Thus, robustness to\nuncertainty in the annotations is introduced by replacing\nhard constraints by penalty terms in the NMF optimization\nproblem. Note that since annotations dictate the assign-\nment of components to sources, there is no need to group\ncomponents by hand. We will discuss the role of \u0016fnin\nthe next section : in the case of user annotations, \u0016fn= 1.\nLet us discuss two cases :\n(a)M(g)\nfn2f0;1g: this is the case of user annotations,\nwhere time-frequency bins are labelled by hand. In this\ncase, there can be only one active source at each time-\nfrequency bin, sinceP\ngM(g)\nfn= 1. This is a strong as-\nsumption, which is veriﬁed for a large fraction of the mix-\ntures that are found in blind source separation.\n(b)M(g)\nfn2[0;1]:this general case is relevant to the learn-\ning procedures we introduce in the next section, since they\noutput decision values in [0;1].\nDiscussing the algorithm is beyond the scope of this pa-\nper : we used a multiplicative updates algorithm with ap-\npropriate modiﬁcations to deal with the additional terms in\nEq. (1) [6].\nFigure 2: Example of user annotations in a ten seconds’\naudio track: green regions are assigned to voice, and red\nregions to accompaniment (best seen in color).\n2.2 Relation with previous work\nAs in [2, 8, 5], annotations are used to constraint some\nsources to be inactive. In fact, time annotations are a spe-\ncial case of our model, where annotations are such that\nM(g)\nfn=M(g)\nf0nfor all (f;f0)(i.e., zeroes come in columns).\nOur model deals with that case when there are two sources.\nThe only difference between our model and [2] is that in-\nstead of enforcing Hkn= 0 as a hard constraint, we in-\ntroduce a soft penalty to enforce WfkHkn= 0, with the\nadded beneﬁt that incorrect annotations are dealt with in a\nrobust fashion. The case of more than two sources is dealt\nwith a simple extension of Eq. (1), which we omit here for\nlack of space.2.3 A graphical user interface for time-frequency\nannotation of spectrograms\nIn this section, we investigate manual annotation of the\nspectrogram. A GUI was designed in Matlab to anno-\ntate spectrograms (see Figure 2), with some extra sound\nfunctionalities to help the user. It takes sound ﬁles as in-\nput, applies some basic preprocessing (re-sampling at user-\nspeciﬁed rate, down-mixing to mono), computes a time-\nfrequency representation via user-speciﬁed parameters, and\ndisplays the spectrogram. Zooming and slide-rule navi-\ngation are enabled for better visualization. Annotation of\nsources is done with a simple rectangle drawing utility :\none color for each source, as illustrated in Figure 2. An-\nnotations are stored in an annotation mask of dimension\nF\u0002N\u0002G(where (F;N )is the size of the spectrogram\nandGthe number of sources). Several annotation masks\nmay be loaded into memory and displayed alternatively,\nso the user can compare, for instance, manual annotations\nwith the output of a blind source separation algorithm. An-\nnotation masks may be exported to .mat format for further\nprocessing. Finally, we implemented playback functional-\nities to help the user annotate the spectrogram.\nWe designed the GUI to make the annotation process\neasier and faster : indeed, in our experience, while time an-\nnotations are easy and require only listening once or twice\nto the mix, time-frequency annotations are hard even for\ntrained users : it takes up to one hour to annotate 20% of a\ntwenty seconds track.\n3. TOWARDS A SUPERVISED ALGORITHM FOR\nANNOTATION\nResearch in computational audio scene analysis (CASA)\nhas emphasized the role of frequency tracks in source iden-\ntiﬁcation : indeed by looking at a spectrogram, it is easy to\nassign a signiﬁcant number of frequency tracks either to a\nvoiced source or a musical source (see Figure 1). In previ-\nous works, such cues have been used to compute a similar-\nity matrix that would then be used to perform clustering see\n[9, 4]. We propose here a supervised learning procedure to\npredict annotations automatically. At train stage, we have\nat hand separate sources so that we observe not only the\nmix, but also the Wiener coefﬁcients M(g)\nfncomputed on\nthe ground truth, while at test stage we only observe V.\nThus, the goal is to predict E(M(g)jV). In order to allevi-\nate the computational burden2, we make two restrictions\non the learning procedure : each vector (M(1)\nfn;:::;M(G)\nfn)\nfor a given time-frequency bin (f;n)is predicted inde-\npendently of the others, and based only on the values of\npatches centered at that time-frequency bin.\nWe now introduce the features and algorithms used to\ntrain our predictor.\n3.1 Features\nThe basic input to our learning algorithms consists in rect-\nangular time-frequency blocks extracted from the input power\n2indeed even for ten seconds’ excerpts, there are more than 500\u0002\n1000 time-frequency bins for standard STFT parameters\n10010110210310−1100101102103104105106\nFigure 3: Samples of patches extracted from the SISEC\ndatabase. Intensity reﬂects amplitude, patches which are\nlabeled as accompaniment are in red, while patches which\nare labeled as voice are in green. Patches in brown have\nmixed Wiener coefﬁcients (best seen in color).\nspectrogram. The size of the rectangular blocks is ﬁxed as\na parameter of the algorithm. They are then normalized to\nhave unit`1-norm so the features are scale invariant. We\nalso considered taking the log of patches, adding coordi-\nnates of the patch as additional information, and taking a\nGabor transform of the patches. The Gabor transform in\nparticular was introduced so that correlations between pix-\nels in each time-frequency blocks is taken into account.\nFinally, we also tried averaging the ground truth Wiener\ncoefﬁcients before learning, so that predicted regression\nsurfaces are smoother in time-frequency space.\n3.2 Learning algorithms\nDue to space limitation, we restrict ourselves to naming\nthe algorithms we chose and highlighting the key parame-\nters to tune. We refer the reader to standard textbooks on\nmachine learning for more details (e.g., [7]).\nK-nearest neighbors (knn): for each test point x(test)\ni , the\nCnearest points x(train)\nj ,j2f1;:::;Cgfrom the train set\nare used to predict M(g)\ni= 1=CP\njM(g)\nj.\nQuantized knn (km): We learnCclusters from the train\nset using K-means; for each cluster, we compute average\nprediction coefﬁcients M(g)\nc. For each test point, we pre-\ndictM(g)\ncfrom the nearest cluster c.\nRandom Forests (rf): We learnCregression trees of depth\ndfrom the train set and average over the Cpredictions for\neach test point.\nWe will refer to this supervised learning procedure as\nautomatic annotations, no matter which algorithm is used.\n3.3 Computation of \u0016fnfor automatic annotations\nWhile the learning algorithms presented above predict Wie-\nner coefﬁcients, output values near 0:5reﬂect uncertainty\nin the Wiener coefﬁcients rather than prediction of mixed\nvolumes. For this reason we introduce an additional tun-\ning parameter \u0016fnin Eq. (1), so that output values near 0:5are less taken into account than values near f0;1g. As a\nrule, we choose \u0016fn= 1\u0000G\nG\u00001P\ngM(g)\nfn(1\u0000M(g)\nfn), so\nthat0\u0014\u0016fn\u00141and\u0016fn= 0 if allM(g)\nfnare equal.\nMoreover, when annotations are in f0;1g, we always have\n\u0016fn= 1.\n4. EXPERIMENTAL RESULTS\n4.1 Description of music databases\nWe used two publicly available databases in our experi-\nments: the QUASI database3and the SISEC database for\nProfessionally Produced Music Recordings4. All source\ntracks were down-sampled from 44100 Hz to 16000 Hz,\nand down-mixed to mono by taking the average of left and\nright channels. A voice track and accompaniment track\nare then created by aggregating the various source ﬁles,\nand then a ﬁnal mix is created by summing the two tracks.\nSine-bell windows of size 1024 with512overlap were used\nto compute short time Fourier transforms. The QUASI\ndatabase contains longer tracks that are amenable to time\nannotations. The SISEC database contains short tracks\nwhere only time-frequency annotations can be used. Al-\nthough detailed instrumental tracks are provided for most\nof the mixtures, we work only on single-channel signals.\nSince we are dealing with under-determined mixtures, we\nrestrict ourselves to separating voice from accompaniment\nin each track, in order to alleviate the difﬁculty of the prob-\nlem.\n4.2 Ideal performance of semi-supervised NMF and\nrobustness to wrong annotations\nSDR1 SDR2 SIR1 SIR2 SAR1 SAR2\n0.1% -0.02 -0.60 5.15 5.16 3.62 2.33\n1% 0.70 0.24 4.59 6.25 4.39 2.85\n10% 6.71 6.68 13.57 16.53 7.95 7.40\n100% 10.40 10.41 19.88 20.88 11.00 10.88\nTable 1: Mean results on the SISEC database, as the pro-\nportion of annotation increases.\nTable 1 displays source separation results achieved by\nsemi-supervised NMF on the SISEC database when fed\nwith the actual Wiener coefﬁcients computed from the ground\ntruth sources. Source separation performance is measured\nby Source to Distortion Ratio (SDR), Source to Interfer-\nence Ratio (SIR), and Source to Artefact Ratio (SAR). Higher\nvalues indicate better performance. As we can see, sat-\nisfactory results are obtained with as little as 10% of an-\nnotations. When 100% of annotations are given, NMF\ndoes nothing and the computed masks are simply the ideal\nWiener coefﬁcients computed from the sources.\nWe study the robustness of our NMF routine by replac-\ning part of the ideal annotations by noise to simulate hu-\nman errors. Table 2 displays average SDRs obtained when\nﬁxing the annotation rate to 10% and varying either the rate\n3www.tsi.telecom-paristech.fr/aao/\n4sisec.wiki.irisa.frwrong annotations por the optimization parameter \u0015. As\nexpected, for ﬁxed \u0015the average SDR drops as pincreases.\nWhenpis ﬁxed, there is an optimal value of \u0015that trades\noff the beneﬁts and drawbacks of annotations. Fixing the\ntarget annotation rate to 10%, satisfactory results are ob-\ntained with up to 10% of wrong annotations (i.e.1% of the\nspectrogram).\n\u0015p= 0p= 0:05p= 0:1p= 0:2p= 0:5\n10\u000010.11 -0.08 -1.76 -1.47 -1.47\n1005.59 4.10 3.50 2.29 1.20\n1017.59 6.53 5.32 3.43 0.59\n1027.07 5.66 4.54 3.15 0.77\nTable 2: Mean SDR value as \u0015and the proportion of\nwrong annotations vary. The proportion of annotations is\nset to 0:1\n4.3 Automatic annotation : comparison of algorithms\nand experimental results\nmethod mean error (% improvement)\n48 loggabor km avg 0.141\u00060.018 (14.9)\n416 wcoords knn avg 0.140\u00060.015 (15.9)\n48 wcoords knn avg 0.138\u00060.015 (16.8)\n432 loggabor rf avg 0.137\u00060.013 (17.4)\n432 loggabor knn avg 0.137\u00060.010 (17.4)\nTable 3: Mean error on Wiener coefﬁcient predictions on\nthe SISEC database (% improvement over random predic-\ntion), for various learning strategies .\nLearning algorithms were trained by dividing the SISEC\ndatabase in two sets of tracks. For each set, we train detec-\ntors and test them on the other set. Thus we may compute\nannotations and run semi-supervised NMF for all tracks\nwithout the risk of overﬁtting. We emphasize the fact that\neach track is annotated with a detector that has never seen\nthe spectrogram before : our method is purely supervised\nwith no adaptation to test data. Parameters of the learning\nalgorithms were selected at train stage by cross-validation.\nTime-frequency patches of size in f4;8g\u0002f8; 16;32gwere\nextracted. Out of each track we extract 5\u0002103patches at\ntrain time, and 105patches a test time, so approximately\n10% of the track is annotated at test time when semi-super-\nvised NMF is called.\nWe display in Table 3 the results of the best 5 detec-\ntors, in terms of mean prediction error (ﬁrst column) and\nin terms of relative improvement over a purely random\npredictor. Detectors are named after the following rule :\nfpatch sizegffeaturegflearning methodgfaveraging or\nidenticalg. For instance, the tag loggabor corresponds to\ntaking log then Gabor transform of patches, and wcoords\nadding frequency coordinates of the patches as side infor-\nmation. Note that we used exact Wiener coefﬁcients to\ncompute errors, so that all detectors can be compared even\nwhen averaging was used at train stage. The improvement\nover a random predictor is consistent across the features\nand the algorithms that were used. Figure 4 compares an-\nnotations provided by the best detectors from Table 3 with(a) Automatic\n (b) Correct\nFigure 4: Comparison of automatic annotations and correct annotations (at the same time-frequency bins). Gray-scale\ntime-frequency bins are not annotated, red bins are annotated as accompaniment, green bins as voice(best seen in color).\nideal annotations at the same points were automatic anno-\ntations were made. Red time-frequency bins correspond\nto accompaniment, and green to voice. The most strik-\ning observation is that, while ideal annotations are in very\nbright colors (few Wiener coefﬁcients are different from 0\nor1), automatic annotations, on the other hand, are gen-\nerally biased towards 0:5. This is to be expected since\npredicting 0:5incurs a risk of losing at most 0:25 (since\nwe use a regression loss), while predicting 0or1incurs\na maximum loss of 1. The main asset of automatic an-\nnotations is that pitch tracks with varying frequency are\nsuccessfully predicted as voice. Automatic annotations are\nbiased towards predicting voice in the higher frequencies\n: however the learning algorithm in this example did not\nhave the information of frequency. This might be because\ntransients “look” a lot like patches of unvoiced speech. Fi-\nnally, one may spot inconsistencies in the predictions in\nthe sense that points belonging to the same pitch tracks\nare sometimes classiﬁed incoherently, which is not surpris-\ning since the learning algorithms we have proposed predict\ntime-frequency bins independently.\nTo sum up, predictions of Wiener coefﬁcients from lo-\ncal patches are not perfect but provide a good starting point\nfor further modelling of the spectrogram. We expect that\nbetter performance could be obtained by using more ad-\nvanced cues from CASA, such as pre-clustering the spec-\ntrogram into pitch tracks and transient tracks, before learn-\ning5.\n4.4 Overall results\nWe now turn to results obtained by semi-supervised NMF\ncombined with various annotation methods. On the SISEC\ndatabase, manual time-frequency annotations were done\nwith the GUI presented in Section 2.2. On the QUASI\ndatabase, tracks were amenable to signiﬁcant time anno-\n5This is very similar to what is done in vision, where super-pixels\nhelp deal with consistency in prediction and alleviate the computational\nburden of predicting all pixel values.tations, so by comparing results on both databases we can\ncompare the respective beneﬁts of time-frequency annota-\ntions VS time annotations.\nIn both scenarios, we compare ﬁve methods :\nauto : Automatic annotations and semi-supervised NMF.\nThe best detector from Table 3 was chosen.\nuser : User annotations and semi-supervised NMF (time-\nfrequency annotations for SISEC, manual annotations for\nQUASI). We tried K2f5;10;20gfor the SISEC database\nandf10;20;50gfor the QUASI database, as well as \u00152\nf1;10;100g, and selected parameters yielding highest SDR\nfor fair comparison with the baseline.\nbaseline : Run NMF and permute factors to obtain op-\ntimal SDR. We set K= 8 because it already takes a 10\ntimes as long to evaluate SDRs for all permutation on a\nsingle track as it takes to run semi-supervised NMF.\nself : sets(g)=1\nGxas estimates for the sources, it serves\nto estimate the difﬁculty of the source separation problem\nfor a given database.\noracle : results obtained with Wiener coefﬁcients com-\nputed from the ground truth. In addition we display track\nby track annotation accuracy for user annotations, for com-\nparison with Table 2. For each method, we ran NMF three\ntimes for 1000 iterations to avoid local minima, and kept\nthe run with the lowest objective cost value.\nTables 5a and 5b display average evaluation metrics for\neach source (source 1 is always the accompaniment, and\nsource 2 is always the voice), on two different databases :\n% annotated % correct\ntrack 1 0.23 0.91\ntrack 2 0.10 0.89\ntrack 3 0.29 0.91\ntrack 4 0.17 0.81\ntrack 5 0.22 0.95\nTable 4: Evaluation of user annotations on the SISEC\ndatabase.auto user (t-f) baseline self oracle\nSDR1 0.97 6.21 6.16 3.09 14.79\nSDR2 0.51 2.58 1.61 -3.18 11.53\nSIR1 3.17 18.64 9.91 3.09 24.00\nSIR2 4.57 11.35 5.09 -3.18 23.90\nSAR1 6.74 6.91 9.26 279.17 15.41\nSAR2 4.18 3.91 5.58 279.17 11.84\n%ann. 8.69 19.81 0.00 0.00 100.00\n(a) SISECauto user (t) baseline self oracle\nSDR1 6.76 7.59 6.29 6.21 16.88\nSDR2 -4.33 -4.57 -1.71 -6.22 10.37\nSIR1 6.97 15.05 13.81 6.21 25.62\nSIR2 -3.75 4.09 1.88 -6.22 24.83\nSAR1 21.91 9.00 7.71 268.45 17.66\nSAR2 10.28 0.21 4.29 268.45 10.60\n%ann. 6.91 100.00 0.00 0.00 100.00\n(b) QUASI\nTable 5: Results on the evaluated databases: (a) time-frequency annotations, (b) time annotations.\non the SISEC database, we experimented with time-fre-\nquency annotations since the tracks were too short for time\nannotations. Overall, results on the SISEC database are\nbetter than those on QUASI. Our interpretation is that since\nmost of the time the accompaniment is active, the dictio-\nnaries tend to overﬁt the accompaniment and underﬁt the\nvoice. Time-frequency annotations on SISEC yield SDRs\nthat are a few points below that predicted by our bench-\nmark from Table 2 : indeed human errors are not dis-\ntributed randomly as was the case in our benchmark. Time-\nfrequency annotations outperform the baseline by 1point\nin SDR, which is signiﬁcant because in semi-supervised\nNMF there is no manual grouping of the components. Time\nannotations loose to the baseline by \u00001in SDR, but they\nare still signiﬁcantly correlated with the true sources when\ncompared with the baseline.\nOn the SISEC database, automatic annotations are also\nbelow the baseline, however they are also signiﬁcantly cor-\nrelated with the true sources, when compared with the “self”\ncolumn. Signal to Interference Ratios are even comparable\nwith those of the baseline on the SISEC database. Auto-\nmatic annotations do not perform as well on the QUASI\ndatabase since we trained detectors only on tracks from\nSISEC, so that more supervision would signiﬁcantly im-\nprove those ﬁgures.\nTo conclude, we have shown that time-frequency anno-\ntations can improve signiﬁcantly over NMF with ideally\ngrouped components. On longer tracks, time only anno-\ntations yield reasonable results, but even when 100% of\nthe track is annotated, the estimated sources contain strong\ninterferences. Automatic annotations yield similar results,\nbut leave considerable room for improvement, since with\ntime-frequency annotations there will always be a point\nwhere enough annotations with limited errors will provide\naudible estimates of the sources.\n5. CONCLUSION\nWe have proposed a novel formulation of semi-supervised\nNMF that successfully takes into account annotations to\nenhance the discriminative power of NMF. Semi-supervised\nNMF is deﬁned so that when a certain amount of annota-\ntions is reached, source separation quality is near that of\nideal binary masks. Manual annotations retrieved with our\ngraphical user interface yield satisfactory results. We areinvestigating ways to deﬁne annotations independently of\nthe particular time-frequency representation that is used.\nFinally, semi-supervised NMF opens the way for inter-\naction with methods from computational audio scene anal-\nysis. As such, the simple features and textbook pattern\nmatching algorithms we have presented show promising\nresults.\n6. REFERENCES\n[1] P. Smaragdis and G. Mysore. “Separation by “hum-\nming”: User-guided sound extraction from mono-\nphonic mixtures.”, WASPAA, 2012.\n[2] B. Wang. “Musical Audio Stream Separation”, Msc\nThesis, 2009.\n[3] D. Fitzgerald. “User Assisted Source Separation using\nNon-negative Matrix Factorisation”, Irish Signals and\nSystems Conference, 2011.\n[4] F. Bach and M.I. Jordan. “Blind one-microphone\nspeech separation: a spectral learning approach.” NIPS,\n2004.\n[5] J.-L. Durrieu and J.-P. Thiran. ”Musical audio source\nseparation based on user-selected f0 track.” (LVA/ICA),\n2012.\n[6] C. F ´evotte, N. Bertin, and J.-L. Durrieu. ”Nonnegative\nmatrix factorization with the Itakura-Saito divergence:\nWith application to music analysis.” Neural Computa-\ntion, 2009.\n[7] T. Hastie, R. Tibshirani, and J. Friedman. The elements\nof statistical learning. Springer, 2nd edition edition,\n2009.\n[8] R. Hennequin, B. David, and R. Badeau. ”Score in-\nformed audio source separation using a parametric\nmodel of non-negative spectrogram.” ICASSP, 2011.\n[9] M. Lagrange, L.G. Martins, J. Murdoch, and G. Tzane-\ntakis. ”Normalized cuts for predominant melodic\nsource separation.” TASLP, 2008."
    },
    {
        "title": "Ranking Lyrics for Online Search.",
        "author": [
            "Robert Macrae",
            "Simon Dixon"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416168",
        "url": "https://doi.org/10.5281/zenodo.1416168",
        "ee": "https://zenodo.org/records/1416168/files/MacraeD12.pdf",
        "abstract": "When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the user with cus- tomary search engine page ranking formula deciding the ordering of these results based on links, views, clicks, etc. However the content, and specifically, the accuracy of the lyrics in question are not analysed or used in any way to determine the rank of the lyrics, despite this being of con- cern to the searcher. In this work, we show that online lyrics are often inaccurate and the ranking methods used by search engines do not distinguish the more accurate an- notations. We present an alternative method for ranking lyrics based purely on the collection of lyrics themselves using the Lyrics Concurrence.",
        "zenodo_id": 1416168,
        "dblp_key": "conf/ismir/MacraeD12",
        "keywords": [
            "online",
            "search",
            "lyrics",
            "transcription",
            "accuracy",
            "search",
            "engine",
            "ranking",
            "formula",
            "concern"
        ],
        "content": "RANKING LYRICS FOR ONLINE SEARCH\nRobert Macrae\nCentre for Digital Music\nQueen Mary University of London\nrobert.macrae@eecs.qmul.ac.ukSimon Dixon\nCentre for Digital Music\nQueen Mary University of London\nsimon.dixon@eecs.qmul.ac.uk\nABSTRACT\nWhen someone wishes to ﬁnd the lyrics for a song they\ntypically go online and use a search engine. There are\na large number of lyrics available on the internet as the\neffort required to transcribe and post lyrics is minimal.\nThese lyrics are promptly returned to the user with cus-\ntomary search engine page ranking formula deciding the\nordering of these results based on links, views, clicks, etc.\nHowever the content, and speciﬁcally, the accuracy of the\nlyrics in question are not analysed or used in any way to\ndetermine the rank of the lyrics, despite this being of con-\ncern to the searcher. In this work, we show that online\nlyrics are often inaccurate and the ranking methods used\nby search engines do not distinguish the more accurate an-\nnotations. We present an alternative method for ranking\nlyrics based purely on the collection of lyrics themselves\nusing the Lyrics Concurrence.\n1. INTRODUCTION\nMultiple copies of song lyrics are available on the internet\nfor almost any song. Due to this free availability, search\nengines have become the common tool for ﬁnding lyrics.\nAs lyrics are relatively easy to mine from the web, and\ngiven that the words to a song contain rich semantic infor-\nmation, lyrics are also used for information retrieval such\nas for karaoke data production, song-browsing, and thumb-\nnailing [2, 3, 12, 15, 16]. The content of a song’s lyrics can\nindicate the topic of the song [4], which genre it belongs to\n[13], or be used for music indexing and artist similarity [8].\nAnother example of lyric based information retrieval uses\nnatural language processing to extract language, structure,\ncategorisation, and similarity from lyrics [11].\nA contributing factor to the abundance of lyrics and a\npotential problem for research in this area is the lack of\nrequirements, such as training or language knowledge, that\nare typically necessary for professionally annotating lyrics.\nDue to these issues, there is a high potential for song lyrics\nto contain errors. This can lead to inaccurate lyrics being\npresented to those using search engines to ﬁnd lyrics as\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.well as music information retrieval researchers who wish\nto mine the rich semantic content within lyrics.\nIn this paper we are concerned with ranking web based\nsong lyrics. Whilst previous work has focused on using\nmultiple sequence alignment to determine the single most\naccurate lyrics for a song [5, 6], ours is concerned with\nranking lyrics, so that users can apply their own selection\nshould the ﬁrst result not be appropriate. To the best of\nour knowledge, lyrics ranking has only previously been at-\ntempted as part of more generalised web resource rank-\ning methods [14]. In order to evaluate song lyrics ranking\nwe ﬁrst describe a test data set for this purpose and we\nthen proceed to mine the web for lyrics of the songs in\nthis dataset. We then formulate a metric to compare each\nlyric to the ground truth, as an accuracy measurement, and\nto other versions to calculate the Lyrics Concurrence, an\nadaptation of the Chords Concurrence and Structure Con-\ncurrence used to rank guitar tablature [10]. We then adapt\nthe ranking methods outlined previously to evaluate these\nmethods by measuring their correlation with the lyrics’ ac-\ncuracy.\n2. TEST DATA: THE MUSIXMATCH DATASET\nThe Million Song Dataset (MSD)1is a collection of meta-\ndata for a million popular music tracks [1] produced by\nLabROSA in collaboration with The Echo Nest. A subset\nof this data, called the musicXmatch Dataset (MXMD),2\nconsists of 237,662 lyrics to songs within the MSD pro-\nvided in a Bag-of-words format with the 5000 most com-\nmon (stemmed) words.\n2.1 Bag-of-words Format\nThe Bag-of-words format (BOW) is primarily a means of\nsummarising text by listing the unique words with the num-\nber of occurrences of each word in the text, with all punc-\ntuation removed. These word and count pairs are ordered\nby their count with the most common coming ﬁrst. For\nexample:\n“On mules we ﬁnd two legs behind and two we ﬁnd before.\nWe stand behind before we ﬁnd what those behind be for. ”\ncan be represented in BOW format as:\n“we:4, ﬁnd:3, behind:3, two:2, before:2, on:1, mules:1,\nlegs:1, and:1, stand:1, what:1, those:1, be:1, for:1”\n1http://labrosa.ee.columbia.edu/millionsong/\n2http://labrosa.ee.columbia.edu/millionsong/musixmatchAdditionally the words are stemmed [9] so that words with\ndifferent endings are reduced to their root form, reduc-\ning the number of unique words. Using this BOW format\navoids copyright issues with sharing lyrics for the purposes\nof research.\n3. LYRICS MINING\nFor each of the 237,662 tracks in the MXMD we searched\nDogPile3for lyrics using the following terms:\n“<artist><song>lyrics -video”. DogPile was chosen as\nit returns results from all the popular search engines and\nyet is more easy to data mine. Previous web mining ap-\nproaches have used the Google Web API in a similar fash-\nion [5, 6], however we required a search engine with an\nunrestricted number of searches. From the list of URLs re-\nturned by this search we selected only those that contained\nthe song title in the URL. This set of URLs provides a sim-\nilar representation of the URLs a user might select when\nmanually searching for lyrics. 888,745 URLs were found\nusing this method for the 237,662 tracks. In order to extract\nthe lyrics from the URLs we separated and analysed each\nline to determine whether it contained lyrics-like text and\nthen selected the longest sequence of lyrics-like text lines\nin the page. Any lyrics that were less than three lines or\nover 200 lines long were discarded. As we are interested\nin comparing with Concurrence, we discarded songs and\ntheir lyrics if they had less than three lyrics associated with\nthe song. The lyrics extraction process is demonstrated in\nFigure 1.\n,\u0003RQFH\u0003KDG\u0003D\u0003JLUO\u000f\u0003RU\u0003VKRXOG\u0003,\u0003VD\\\u000f\u0003VKH\u0003RQFH\u0003KDG\u0003PH\u0011\u0011\u0011\u0003\n6\nK\nH\n\u0003\nV\nK\nR\nZ\nH\nG\n\u0003\nP\nH\n\u0003\nK\nH\nU\n\u0003\nU\nR\nR\nP\n\u0003\nL\nV\nQ\n\nW\n\u0003\nL\nW\n\u0003\nJ\nR\nR\nG\n\u0003\nQ\nR\nU\nZ\nH\nJ\nL\nD\nQ\n\u0003\nZ\nR\nR\nG\n6\nK\nH\n\u0003\nD\nV\nN\nH\nG\n\u0003\nP\nH\n\u0003\nW\nR\n\u0003\nV\nW\nD\n\\\n\u0003\nD\nQ\nG\n\u0003\nV\nK\nH\n\u0003\nW\nR\nO\nG\n\u0003\nP\nH\n\u0003\nW\nR\n\u0003\nV\nL\nW\n\u0003\nD\nQ\n\\\nZ\nK\nH\nU\nH\n6\nR\n\u0003\n,\n\u0003\nO\nR\nR\nN\nH\nG\n\u0003\nD\nU\nR\nX\nQ\nG\n\u0003\nD\nQ\nG\n\u0003\n,\n\u0003\nQ\nR\nW\nL\nF\nH\nG\n\u0003\nW\nK\nH\nU\nH\n\u0003\nZ\nD\nV\nQ\n\nW\n\u0003\nD\n\u0003\nF\nK\nD\nL\nU\n,\n\u0003\nV\nD\nW\n\u0003\nR\nQ\n\u0003\nD\n\u0003\nU\nX\nJ\n\u0003\nE\nL\nG\nL\nQ\nJ\n\u0003\nP\n\\\n\u0003\nW\nL\nP\nH\n\u0003\nG\nU\nL\nQ\nN\nL\nQ\nJ\n\u0003\nK\nH\nU\n\u0003\nZ\nL\nQ\nH\n:\nH\n\u0003\nW\nD\nO\nN\nH\nG\n\u0003\nX\nQ\nW\nL\nO\n\u0003\nW\nZ\nR\n\u0003\nD\nQ\nG\n\u0003\nW\nK\nH\nQ\n\u0003\nV\nK\nH\n\u0003\nV\nD\nL\nG\n\u0003\n,\nW\n\nV\n\u0003\nW\nL\nP\nH\n\u0003\nI\nR\nU\n\u0003\nE\nH\nG\n6\nK\nH\n\u0003\nW\nR\nO\nG\n\u0003\nP\nH\n\u0003\nV\nK\nH\n\u0003\nZ\nR\nU\nN\nH\nG\n\u0003\nL\nQ\n\u0003\nW\nK\nH\n\u0003\nP\nR\nU\nQ\nL\nQ\nJ\n\u0003\nD\nQ\nG\n\u0003\nV\nW\nD\nU\nW\nH\nG\n\u0003\nW\nR\n\u0003\nO\nD\nX\nJ\nK\n6\nK\nH\n\u0003\nW\nR\nO\nG\n\u0003\nP\nH\n\u0003\nV\nK\nH\n\u0003\nZ\nR\nU\nN\nH\nG\n\u0003\nL\nQ\n\u0003\nW\nK\nH\n\u0003\nP\nR\nU\nQ\nL\nQ\nJ\n\u0003\nD\nQ\nG\n\u0003\nV\nW\nD\nU\nW\nH\nG\n\u0003\nW\nR\n\u0003\nO\nD\nX\nJ\nK\n,\n\u0003\nW\nR\nO\nG\n\u0003\nK\nH\nU\n\u0003\n,\n\u0003\nG\nL\nG\nQ\n\nW\n\u0003\nD\nQ\nG\n\u0003\nF\nU\nD\nZ\nO\nH\nG\n\u0003\nR\nI\nI\n\u0003\nW\nR\n\u0003\nV\nO\nH\nH\nS\n\u0003\nL\nQ\n\u0003\nW\nK\nH\n\u0003\nE\nD\nW\nK\n$\nQ\nG\n\u0003\nZ\nK\nH\nQ\n\u0003\n,\n\u0003\nD\nZ\nR\nN\nH\n\u0003\n,\n\u0003\nZ\nD\nV\n\u0003\nD\nO\nR\nQ\nH\n\u0003\nW\nK\nL\nV\n\u0003\nE\nL\nU\nG\n\u0003\nK\nD\nG\n\u0003\nI\nO\nR\nZ\nQ\n6\nR\n\u0003\n,\n\u0003\nO\nL\nW\n\u0003\nD\n\u0003\nI\nL\nU\nH\n\u0003\nL\nV\nQ\n\nW\n\u0003\nL\nW\n\u0003\nJ\nR\nR\nG\n\u0003\nQ\nR\nU\nZ\nH\nJ\nL\nD\nQ\n\u0003\nZ\nR\nR\nG\nFigure 1. An example lyrics web page and the lyrics ex-\ntracted from it.\n4. LYRICS EV ALUATION METRICS\nIn this section we give an overview of the metrics used\nin judging the accuracy and similarity of lyrics. The ﬁrst\nmethod, Levenshtein Edit Distance, is a well known Dy-\nnamic Programming method for comparing strings. We\n3http://www.dogpile.com/use the Levenshtein Edit Distance to judge the similarity\nof lyrics and this is used later in the Lyrics Concurrence\nranking method.\n4.1 Levenshtein Edit Distance\nThe Levenshtein Edit Distance (LED) [7] counts the num-\nber of “edits” required to transform one string into an-\nother. An edit is classed as an insertion, deletion, or sub-\nstitution of a single character. LED uses a cost of 0for\nmatches and 1for any edit (insertion, deletion or alter-\nation). As such the LED of “sun” and “sing” is 2 (substi-\ntution of the letter ‘u’ for ‘i’ and insertion of the letter ‘g’).\nThe LED cost is found by calculating a path P(U;V ) =\n(p1;p2;:::;p W)through a matrix of costs between strings\nU= (u 1;u2;:::;u M)andV= (v 1;v2;:::;v N). This cost\nmatrix is described as dU;V(m;n) wherem2[1 :M]and\nn2[1 :N]where each position in the path is designated\naspk= (mk;nk). A simple bottom-up algorithm for cal-\nculating the LED in O(N2)time and space is shown in\nAlgorithm 1. In this example a matrix of edit costs is cal-\nculated between two strings, so that the cell in the ﬁnal row\nand column would contain the total number of required ed-\nits. Additionally, an example of the “cost matrix” and the\nsolution this algorithm produces can be seen in Table 1.\nInput: String A, StringB\nOutput: Levenshtein Edit Distance LED\nMatrixm;m[0; 0] := (A[0] ==B[0]? 0 : 1);\nfora2[1::A:length ]do\nm[a; 0] := (A[a] == B[0]? 0 : 1) + m[a\u00001;0];\nend\nforb2[1::B:length] do\nm[0;b] := (B[b] ==A[0]? 0 : 1) + m[0;b\u00001];\nend\nfora2[1::A:length] do\nforb2[1::B:length] do\nm[a;b] := (A[a] == B[b]?m[a\u00001;b\u00001] :\n1 +min(m[a\u00001;b];m [a\u00001;b\u0000\n1];m [a;b\u00001]));\nend\nend\nreturnLED :=m[A:length;B:length];\nAlgorithm 1: The Levenshtein Edit Distance.\n4.2 Lyric Accuracy (LA)\nIn order to calculate the accuracy of the lyrics we ﬁrst con-\nvert the lyrics to the BOW format with the 5000 most com-\nmon stemmed words (as designated by the MXMD set) us-\ning the same stemming code the MXMD set used. We de-\nscribe the ground truth MXMD BOW G= (g1;g2;:::;g M)\nand the lyrics BOW L= (l 1;l2;:::;l N)as sets of word\n(wi)and count (xi)pairs where gi= (w i;xi). Each word\nin the ground truth BOW Gis looked for in the lyrics BOW\nLso that if a match is found i.e.gm(w) =ln(w). There-\nfore each ground truth word yields an expected word count\ngm(x)and a found word count of lk(x)if the word was\npresent in the lyrics BOW and 0 if not. If the found wordString A: all the other kids\nString B: with their pumped up kicks\na l l t h e o t h e r k i d s\nw 12 3 4 5 6 7 8 9 10 11 12 13 14 15\ni 2 23 4 5 6 7 8 9 10 11 12 12 13 14\nt 3 33 3 4 5 6 7 8 9 10 11 12 13 14\nh 4 4 44 3 4 5 6 7 8 9 10 11 12 13\nt 5 5 5 44 4 5 5 6 7 8 9 10 11 12\nh 6 6 6 5 45 5 6 5 6 7 8 9 10 11\ne 7 7 7 6 5 45 6 6 5 6 7 8 9 10\np 8 8 8 7 6 55 6 7 6 6 7 8 9 10\nu 9 9 9 8 7 6 66 7 7 7 7 8 9 10\nm 10 10 10 9 8 7 7 77 8 8 8 8 9 10\np 11 11 11 10 9 8 8 8 88 9 9 9 9 10\ne 12 12 12 11 10 9 9 9 9 89 10 10 10 10\nd 13 13 13 12 11 10 10 10 10 99 10 11 10 11\nu 14 14 14 13 12 11 11 11 11 1010 10 11 11 11\np 15 15 15 14 13 12 12 12 12 11 1111 11 12 12\nk 16 16 16 15 14 13 13 13 13 12 12 1112 12 13\ni 17 17 17 16 15 14 14 14 14 13 13 12 1112 13\nc 18 18 18 17 16 15 15 15 15 14 14 13 1212 13\nk 19 19 19 18 17 16 16 16 16 15 15 14 13 1313\ns 20 20 20 19 18 17 17 17 17 16 16 15 14 14 13\nTable 1. An example of a Levenshtein Edit Distance\n(LED) requiring 13 edits (with spaces removed).\ncount is greater than the expected word count, the found\ncount is replaced as the expected count minus the differ-\nence or 0 if this difference is greater than the expected\ncount. The LA is calculated as the sum of the found word\ncounts divided by the sum of the expected word counts\nmultiplied by 100 and divided by the sum of the ground\ntruth counts expected, so as to be expressed as a percent-\nage. Equation 1 shows this calculation and Table 2 shows\nan example of the LA measurement.\nLA(G,L) =Pmax(g m(x)\u0000jgm(x)\u0000lk(x)j; 0)Pgm(x)\u0002100\n(1)\nGround Truth: “Are we human or are we dancer? My\nsign is vital, my hands are cold”\nLyrics: “Are we human or are we dancers? My signs\nare vital, my hands are cold”\nLyrics Accuracy (LA): (12=14)\u0002100= 85.7%\n(wrong count for “is” and wrong count for “are”)\nTable 2. Lyrics Accuracy (LA) example.\n4.3 Lyrics Similarity (LS)\nThe Lyrics Similarity is a measure of how similar two lyrics,\nL1andL2are. We use the LED of the entire sequence\nof characters in both lyrics, not stemmed and with all the\npunctuation included. We convert the LED to a similarityscore by normalising to the perfect score, then inverting\nand multiplying by 100 to give a value from 0to100:\nLS(L 1;L2) =\u0012\n1\u0000LED(L 1;L2)\nmax(L 1;L2)\u0013\n\u0002100 (2)\nFor the Lyrics Ranking experiments we additionally tried\na variation of the LS called LS nswhere spaces are removed\nfrom the input lyrics L1andL2. The incentive for remov-\ning spaces is that, as the average english word length is 5\ncharacters, spaces make up roughly1\n6of the text and pos-\nsibly contain less relevant information than the rest of the\ntext. As the LED has quadratic costs, reducing the input\nsequences by1\n6reduces the processing time and memory\nrequirements of this method by 31%.\nLyrics 1: “On mules we ﬁnd two legs behind and two\nwe ﬁnd before.”\nLyrics 2: “We stand behind before we ﬁnd what those\nbehind be for.”\nLyrics Similarity (LS): 43.8%\nLyrics Similarity no spaces (LS ns): 45.7%\nLyrics 1: “Are we human or are we dancer?\nMy sign is vital, my hands are cold’ ’\nLyrics 2: “Are we human or are we dancers?\nMy signs are vital, my hands are cold”\nLyrics Similarity (LS): 92.9%\nLyrics Similarity no spaces (LS ns): 90.9%\nLyrics 1: “Scaramouche, Scaramouche,\nwill you do the Fandango”\nLyrics 2: “Scallaboosh, Scallaboosh,\nwill you to the banned tango”\nLyrics Similarity (LS): 69.1%\nLyrics Similarity no spaces (LS ns): 66.0%\nLyrics 1: Radiohead - High and Dry\n(azlyrics.com/lyrics/radiohead/highdry.html)\nLyrics 2: Jamie Cullum - High and Dry\n(azlyrics.com/lyrics/jamiecullum/highanddry.html)\nLyrics Similarity (LS): 86.6%\nLyrics Similarity no spaces (LS ns): 86.0%\nTable 3. Lyrics Similarity (LS) examples.\n5. LYRICS STATISTICS\nThe ﬁnal list of lyrics included 358,535 lyrics for 67,156\nsongs with an average Lyrics Accuracy of 38.6%. The dis-\ntribution of the lyrics over these songs can be seen in Fig-\nure 2. This lyrics distribution shows a quick drop off in\nthe number of lyrics per song after the songs with less than\nthree lyrics were removed. The range of lyrics accuracy\nresults can be seen in the histogram in Figure 3. The large\nnumber of low accuracy lyrics and the low average Lyrics\nAccuracy suggest the lyrics mining procedure failed to ﬁl-\nter out all the non-text lyrics, however, this is not a triv-\nial task for users browsing the web either and so we al-\nlow these non lyrics to be considered within the ranking\nmethod experiments as one purpose of these methods is todifferentiate between lyrics and non-lyrics. In Section 7.1\nwe examine the possibility of removing these non-lyrics\nto judge their effect on the ranking experiments. Table 4\nshows the top twenty lyrics domains based on their aver-\nage Lyrics Accuracy. The increase in Lyrics Accuracy of\nthese domains over the average suggests that a simple ﬁl-\nter restricting the results to known accurate lyrics domains\nwould remove most of the non-lyrics.\n\u0018\u0013\n \u0019\u0013\n \u001a\u0013\n\u0017\u0013\n\u0017\u0013\n\u0016\u0013\n\u0016\u0013\n\u0015\u0013\n\u0015\u0013\n\u0014\u0013\n\u0014\u0013\n\u0013\n\u0013\n\u0016\u0018\n\u0014\u0018\n\u0018\n\u0015\u0018\n 1 X P EHU\u0003RI\u0003O\\ULFV\u0003SHU\u0003V R Q J\n1XPEHU\u0003RI\u0003VRQJV\n\u000b\nW\nK\nR\nX\nV\nD\nQ\nG\nV\n\f\nFigure 2. A histogram showing the distribution of lyrics\nfor the 61,755 songs.\n\u001b\u0013\n \u0019\u0013\n \u0014\u0013\u0013\n \u0017\u0013\n\u0016\u0013\n\u0015\u0013\n\u0015\u0013\n\u0014\u0013\n\u0013\n\u0013\n\u0014\u0018\n\u0018\n\u0015\u0018\n / \\ ULFV\u0003$FFXUDF\\\u0003\u000b\b \f\n1XPEHU\u0003RI\u0003O\\ULFV\n\u000b\nW\nK\nR\nX\nV\nD\nQ\nG\nV\n\f\nFigure 3. A histogram showing the distribution of the\nlyrics accuracies.\nLA Domain Lyrics\n55.82% www.alivelyrics.com 123\n52.75% www.sing365.com 15798\n52.53% www.popular-lyrics.com 142\n52.43% www.plyrics.com 127\n52.34% www.musicsonglyrics.com 3307\n52.33% www.lyricspond.com 535\n52.25% www.songteksten.nl 1178\n51.97% www.lyricsdepot.com 3301\n51.93% www.azlyrics.com 7006\n51.30% www.1songlyrics.com 253\n51.11% www.absolutelyrics.com 1360\n51.02% www.lyricsondemand.com 2909\n50.85% www.sarkisozum.gen.tr 138\n50.72% www.christian-lyrics.net 167\n50.62% www.lyricsdomain.com 925\n50.57% www.lyricstop.com 235\n50.084% www.cowboylyrics.com 1656\n49.26% www.lyriczz.com 682\n49.08% www.lyricsreg.com 1877\n49.01% www.lyricmania.com 155\nTable 4. Average accuracy rates for different lyrics do-\nmains.6. LYRICS RANKING METHODS\nThe following methods describe how we apply the ranking\nmethods to the lyrics.\n6.1 Search Engine Results Page Rank\nThe lyric’s Search Engine Results Page Rank (SERP Rank)\ncorresponds to where the URL of the lyric is found in the\nordered list of DogPile’s ranked search results. Values\nrange from 1 (best) to 100 (worst known), as our mining\nwas restricted to the top 100 results (see Section 3). All\nthe lyrics were mined using DogPile and as such had an\nassociated SERP Rank.\n6.2 Date Modiﬁed\nThe Date Modiﬁed value is expressed as the number of\nmilliseconds since 00:00:00 January 1, 1970 GMT. 137,875\nof the 358,535 lyrics had an associated last date modiﬁed\nthat was greater than 0. Any value of 0 is ignored as it was\npresumed that such a date was unknown.\n6.3 Lyrics Concurrence\nTo determine the extent to which lyrics of songs agree with\na set of lyrics, we measure the Lyrics Concurrence as the\naverage of the Lyrics Similarities between a lyric Lkand\nthe other lyrics of the same song Li(i6=k).\nLC(L k) =nX\ni=1;i 6=kLS(L k;Li)=(n\u00001) (3)\n6.4 Lyrics Concurrence NS (LC ns)\nAdditionally, we measure the Lyrics Concurrence No Spaces\nas the average of the LS nsbetween a lyrics’ Lkand the\nother Lyrics of the same song Li(i6=k).\nLCns(Lk) =nX\ni=1;i 6=kLSns(Lk;Li)=(n\u00001) (4)\n7. LYRICS RANKING EV ALUATION\nIn order to measure correlation we use two alternative mea-\nsurements, the Pearson Product-Moment Correlation Co-\nefﬁcient (PCC), and the Spearman’s Rank Correlation Co-\nefﬁcient (SCC). Table 5 shows the correlations found be-\ntween the lyrics LA and the 4 ranking methods described\nabove. Figure 4 shows scatter graphs of the accuracy and\nrank of the lyrics using two of the methods: SERP Rank\nand Lyrics Concurrence. The correlations show the Lyrics\nConcurrence having the strongest correlation, the SERP\nRank having a weak correlation (the negative correlation is\nexpected as lower values indicate a better SERP Rank) and\nthe Date Modiﬁed having a very low correlation. Compar-\ning LC and LC nswe ﬁnd that discarding spaces improves\nthe correlation slightly therefore LC nsimproves perfor-\nmance in both accuracy and efﬁciency. The results of this\nexperiment show that analysing the content of the meta-\ndata in comparison to the other metadata available leads toFigure 4. Scatter graphs showing the trends between LA\nand respectively the SERP Rank (above) and Lyrics Con-\ncurrence (below) on 358,535 lyrics.\na better ranking system than methods based on user statis-\ntics and link analysis or the date modiﬁed.\nRanking Method PCC (r ) SCC (\u001a) Samples\nLA\nLyrics Concurrence 0.654 0.607 358535\nLyrics Concurrence NS 0.657 0.609 358535\nSERP Rank -0.206 -0.190 358535\nDate Modiﬁed 0.016 0.012 137875\nTable 5. Number of samples and correlation values be-\ntween various ranking methods and the Lyrics Accuracy\n(LA).7.1 To What Extent do Non-Lyrics Affect Ranking\nCorrelations?\nAs mentioned previously, the Lyrics data contains many\nﬁles that are not lyrics at all (as is evident from the dark\ncluster of low accuracy results in Figure 4) and this may\naffect the correlations. We therefore repeat the ranking\nmethods experiment excluding the ﬁles that have a Lyrics\nAccuracy of less than 10%, the results of which are shown\nin Table 6. The ranking methods all see a reduction in the\ncorrelation between rank and Lyrics Accuracy. However,\nthis difference also suggests that the methods could be used\nto help distinguish lyrics from non-lyrics.\nRanking Method PCC (r ) SCC (\u001a) Samples\nLA\nLyrics Concurrence 0.477 0.477 289346\nLyrics Concurrence NS 0.484 0.484 289346\nSERP Rank -0.191 -0.191 289346\nDate Modiﬁed 0.009 0.033 107661\nTable 6. A modiﬁed version of Table 5 showing corre-\nlation values between various ranking methods and the\nLyrics Accuracy (LA) without the lyrics with an LA of less\nthan 10%.\n7.2 Is Lyrics Concurrence Dependent on Sample Size?\nTo see if the number of lyrics available for a particular\nsong effects the correlation of Lyrics Concurrence with\nlyrics Accuracy, we calculate the correlation between N\n(the number of lyrics for a particular song) and C (corre-\nlation between LA and LC) for each of the 61,755 songs.\nThe result, 0.074, is not statistically signiﬁcant for the sam-\nple size, suggesting that Lyrics Concurrence is a relevant\nindicator of accuracy providing the sample size is at least\n3 as is the case in these tests.\n7.3 Song Lyrics Detection\nWe also attempt to use the lyrics ranking methods as lyrics\ndetection systems by taking the highest ranking lyrics for\neach of the 61,755 songs. Table 7 shows the average ac-\ncuracy of the ranking methods. Of the ranking methods,\nthe Lyrics Concurrence is the most successful feature for\nselecting the most accurate lyrics to use.\nDetection Method Lyrics Accuracy\nLyrics Concurrence 47.3%\nDate Modiﬁed 43.6%\nSERP Rank 42.5%\nRandomly Selected 38.6%\nTable 7. The average Lyrics Accuracy of the top ranked\nlyrics over 61,755 tracks (41,614 tracks in the case of Date\nModiﬁed as 38.5% of the lyrics don’t have an associated\ndate). The ﬁnal row shows the average as if the lyrics were\nrandomly selected.8. DISCUSSION\nIn this paper we have examined the need for greater rank-\ning of online music metadata and proposed a solution to\nthis problem. The Lyrics Concurrence is a method for\nranking music lyrics based on the similarity of its lyrical\ncontent to other lyrics of the same song. The rationale of\nthe Concurrence factor is that the correctness of metadata,\nis determined by agreement of expert human annotators.\nWe have shown that Lyrics Concurrence is a reliable in-\ndicator of accuracy, providing a greater correlation with\nthe accuracy of the lyrics than the date modiﬁed or SERP\nRank. During the time of this experiment there were no\nratings available for the lyrics, however, some lyrics web-\nsites have started to incorporate this feature. User ratings\ncan act as an additional ranking method and future work\ncould compare this method with those evaluated here, how-\never, a similar study found user ratings to be a poor ranking\nmethod for guitar tablature [10].\nIt is hoped that the Concurrence ranking method can be\nutilised in search engines to ensure that accurate annota-\ntions are ranked more favourably, although the computa-\ntional costs involved in comparing hundreds of lyrics with\neach other may limit the usage of such a technique to off-\nline cases. Future ranking methods might focus on com-\nbining Concurrence with SERP Rank, User Rating, or link-\ning lyrics with other sources of metadata such as chords,\nin order to improve the correlation of the ranking with the\naccuracy. Such an approach may allow a more complete\nannotation of a different type to ﬁll out any missing or ab-\nbreviated segments by repeating the aligned section.\n9. REFERENCES\n[1] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The Million Song Dataset.\nInProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR), pages 591–596,\n2011.\n[2] Hiromasa Fujihara, Masataka Goto, Jun Ogata,\nKazunori Komatani, Tetsuya Ogata, and Hiroshi G.\nOkuno. Automatic Synchronization between Lyrics\nand Music CD Recordings Based on Viterbi Alignment\nof Segregated V ocal Signals. In Proceedings of the 8th\nIEEE International Symposium on Multimedia (ISM),\npages 257–264, Washington, DC, USA, 2006.\n[3] Min-Yen Kan, Ye Wang, Denny Iskandar, Tin Lay\nNwe, and Arun Shenoy. LyricAlly: Automatic Syn-\nchronization of Textual Lyrics to Acoustic Music Sig-\nnals. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 16(2):338–349, 2008.\n[4] Florian Kleedorfer, Peter Knees, and Tim Pohle. Oh\nOh Oh Whoah! Towards Automatic Topic Detection\nIn Song Lyrics. In Proceedings of the 9th International\nConference on Music Information Retrieval (ISMIR),\npages 287–292, September 2008.[5] Peter Knees, Markus Schedl, and Gerhard Widmer.\nMultiple Lyrics Alignment: Automatic Retrieval of\nSong Lyrics. In Proceedings of the 6th International\nConference on Music Information Retrieval (ISMIR),\npages 564–569, London, UK, September 2005.\n[6] Jan Korst and Gijs Geleijnse. Efﬁcient Lyrics Retrieval\nand Alignment. In Proceedings of the 3rd Philips Sym-\nposium on Intelligent Algorithms (SOIA), Eindhoven,\nthe Netherlands, December 2006.\n[7] V . I. Levenshtein. Binary Codes Capable of Correcting\nDeletions, Insertions, and Reversals. Soviet Physics,\n10(8):707–710, February 1966.\n[8] Beth Logan, Andrew Kositsky, and Pedro Moreno.\nSemantic Analysis of Song Lyrics. In Proceedings\nof the IEEE International Conference on Multimedia\nand Expo (ICME), volume 2, pages 827–830, Balti-\nmore,Maryland, USA, 2004.\n[9] Julie Beth Lovins. Development of a Stemming Al-\ngorithm. Mechanical Translation and Computational\nLinguistics, 11:22–31, 1968.\n[10] Robert Macrae and Simon Dixon. Guitar Tab Min-\ning, Analysis and Ranking. In Proceedings of the 12th\nInternational Conference on Music Information Re-\ntrieval (ISMIR), pages 453–458, 2011.\n[11] Jose P. G. Mahedero, ´Alvaro Mart ´Inez, Pedro Cano,\nMarkus Koppenberger, and Fabien Gouyon. Natural\nLanguage Processing of Lyrics. In Proceedings of the\n13th Annual ACM International Conference on Mul-\ntimedia (ACM-MM), pages 475–478, New York, NY ,\nUSA, 2005. ACM.\n[12] Matthias Mauch, Hiromasa Fujihara, and Masataka\nGoto. Lyrics-to-Audio Alignment and Phrase-Level\nSegmentation Using Incomplete Internet-Style Chord\nAnnotations. In Proceedings of the 7th Sound and Mu-\nsic Computing Conference (SMC), 2010.\n[13] Rudolf Mayer, Robert Neumayer, and Andreas Rauber.\nRhyme and Style Features for Musical Genre Classiﬁ-\ncation by Song Lyrics. In Proceedings of the 9th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR), 2008.\n[14] L. Page, S. Brin, R. Motwani, and T. Winograd. The\nPageRank Citation Ranking: Bringing Order to the\nWeb. Technical report, Stanford InfoLab, 1999.\n[15] Ye Wang, Min-Yen Kan, Tin Lay Nwe, Arun Shenoy,\nand Jun Yin. LyricAlly: Automatic Synchronization of\nAcoustic Musical Signals and Textual Lyrics. In Pro-\nceedings of the 12th Annual ACM International Con-\nference on Multimedia (ACM-MM), pages 212–219,\nNew York, NY , USA, 2004.\n[16] Chi Wong, Wai Szeto, and Kin Wong. Automatic\nLyrics Alignment for Cantonese Popular Music. Multi-\nmedia Systems, 12:307–323, 2007."
    },
    {
        "title": "BLAST for Audio Sequences Alignment: A Fast Scalable Cover Identification Tool.",
        "author": [
            "Benjamin Martin 0001",
            "Daniel G. Brown 0001",
            "Pierre Hanna",
            "Pascal Ferraro"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417687",
        "url": "https://doi.org/10.5281/zenodo.1417687",
        "ee": "https://zenodo.org/records/1417687/files/MartinBHF12.pdf",
        "abstract": "Searching for similarities in large musical databases is com- mon for applications such as cover song identification. The- se methods typically use dynamic programming to align the shared musical motifs between subparts of two record- ings. Such music local alignment methods are slow, as are the bioinformatics algorithms they are closely related to. We have adapted the ideas of the Basic Local Align- ment Search Tool (BLAST) for biosequence alignment to the domain of aligning sequences of chroma features. Our tool allows local music sequence alignment in near-linear time. It identifies small regions of exact match between sequences, called seeds, and builds local alignments that include these seeds. Seed determination is a key issue for the accuracy of the method and closely depends on the database, the representation and the application. We intro- duce a particular seeding approach for cover detection, and evaluate it on both a 2000-piece training set and the million song dataset (MSD). We show that the heuristic alignment drastically improves time computation for cover song de- tection. Alignment sensitivity is still very high on the small database, but is dramatically weakened on the MSD, due to differences in chroma features. We discuss the impact of different choices of these features on alignment of musical pieces.",
        "zenodo_id": 1417687,
        "dblp_key": "conf/ismir/MartinBHF12",
        "keywords": [
            "music local alignment",
            "dynamic programming",
            "BLAST",
            "chroma features",
            "seed determination",
            "cover song identification",
            "alignment sensitivity",
            "heuristic alignment",
            "training set",
            "MSD"
        ],
        "content": "BLAST FOR AUDIO SEQUENCES ALIGNMENT:\nA FAST SCALABLE\nCOVER IDENTIFICATION TOOL\nBenjamin Martin1, Daniel G. Brown2, Pierre Hanna1, Pascal Ferraro1\n1Universit´ e de Bordeaux\nCNRS, LaBRI, UMR 5800\n{bmartin,hanna,ferraro }@labri.fr2University of Waterloo\nCheriton School of Computer Science\ndan.brown@uwaterloo.ca\nABSTRACT\nSearching for similarities in large musical databases is com-\nmon for applications such as cover song identiﬁcation. The-\nse methods typically use dynamic programming to align\nthe shared musical motifs between subparts of two record-\nings. Such music local alignment methods are slow, as\nare the bioinformatics algorithms they are closely related\nto. We have adapted the ideas of the Basic Local Align-\nment Search Tool (BLAST) for biosequence alignment to\nthe domain of aligning sequences of chroma features. Our\ntool allows local music sequence alignment in near-linear\ntime. It identiﬁes small regions of exact match between\nsequences, called seeds, and builds local alignments that\ninclude these seeds. Seed determination is a key issue for\nthe accuracy of the method and closely depends on the\ndatabase, the representation and the application. We intro-\nduce a particular seeding approach for cover detection, and\nevaluate it on both a 2000-piece training set and the million\nsong dataset ( MSD ). We show that the heuristic alignment\ndrastically improves time computation for cover song de-\ntection. Alignment sensitivity is still very high on the small\ndatabase, but is dramatically weakened on the MSD , due to\ndifferences in chroma features. We discuss the impact of\ndifferent choices of these features on alignment of musical\npieces.\n1. INTRODUCTION\nDuring the last decade, an increasing number of large mu-\nsic datasets have become available. One may now access\na huge amount of music audio, stored for instance on per-\nsonal computers, mobile devices or online. In this context,\nMusic Information Retrieval (MIR) focuses on automatic\nclassiﬁcation, organization, description of music content.\nTo assess musical similarities between pieces, for exam-\nple, a major challenge of MIR is analysing acoustic con-\ntent. Using signal processing techniques, music features\nare ﬁrst infered from audio content. Each of them are re-\nlated to a speciﬁc aspect of music. A fairly typical MIR ap-\nproach consists in obtaining such a feature for short frames\nof audio signal, hence computing a symbolic string repre-\nsentation that carries the change in some musical property\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2012 International Society for Music Information Retrieval.along a given music track. Such symbolic representations\ncan be further analysed and compared to detect meaning-\nful similarities. MIR studies typically employ comparison\ntechniques either custom built or adapted from other ﬁelds\nof information science [6]. Many such techniques account\nfor slight variations in musical features. The way we per-\nceive music is known to work at different time scales, and\nallows for slight differences in music information received\nover time. For instance, one may easily recognize a chorus\nsung twice in a song although the singer may sing different\nlyrics, the melody may have changed, or the instruments\nplaying may be different in both occurrences.\nAmong the many applications of automatic assessment\nof musical similarities in music datasets, cover song iden-\ntiﬁcation has been of major concern over the last few yea-\nrs [16]. Cover songs are usually deﬁned as multiple rendi-\ntions of the same original music piece. They may be played\nby other performers from different music genres or with\ndistinct recording environments [16]. Although two cover\nversions of an original song may differ widely in instru-\nmentation, singing voices, background noise, key, struc-\ntural arrangement, genre, etc., they should hold enough\nmusical similarity for human perception to identify them\nas renditions of the same piece. To detect such similarities,\nmost retrieval systems use dynamic programming [16]. A\nkey drawback of such systems is their inability to efﬁ-\nciently scale to the range of musical data available in mu-\nsical platforms, i.e. to the order of tens of millions of\ntracks [6, 16].\nWe propose an indexing method that substantially in-\ncreases the efﬁciency of alignment-based retrieval systems.\nOur method uses a widely known bio-sequence indexing\ntechnique, BLAST [2]. We adapt this method by investi-\ngating the distribution of symbols among features sequen-\nces, and deducing a strategy for efﬁcient indexing. An em-\npirical study and an evaluation are performed on a custom-\nbuilt cover song dataset, as well as on the Million Song\nDataset ( MSD ) [5]. The remainder of this paper is orga-\nnized as follows. Previous works are described in Sec-\ntion 2. Section 3.1 presents audio representations and align-\nment techniques used, and describes the principle of the\nbio-indexing tool we propose to adapt. Section 3 details\nthe investigation over feature sequences, and describes the\nparticular settings of our music application. Finally, Sec-\ntion 5 presents results obtained for a practical cover song\nidentiﬁcation, while concluding remarks and perspectives\nare depicted in Section 6.2. RELATED WORK\nSeveral heuris\ntics for reducing the computational cost of\ndynamic programming have been introduced for MIR ap-\nplications. Dannenberg and Hu [7] proposed to discover\npatterns in audio sequences by partially exploring the search\nspace around bands, relying on global thresholds that indi-\ncate limits on the deviation from diagonals, implicitly as-\nsuming that insertions and deletions are rare. Combined\nto a clustering technique, this process was used to deduce\nsuboptimal alignments and infer the structure of audio pie-\nces.\nKilian and Hoos [12] already introduced BLAST in MIR\ncontext, in order to search for approximate patterns in sym-\nbolic music. They tested the technique on MIDI excerpts\nand emphasized the efﬁciency of the alignment of similar\npatterns with the bi-directional extension of exact match\nregions. Authors infered a high potential for BLAST in\na non-symbolic input conﬁguration, but did not test it ex-\nplicitly [13]. Although the work presented in this paper\nadapts the same BLAST algorithm, it is subtantially dif-\nferent from this study. First, our technique is applied to\nfeature sequences obtained from audio signals. More im-\nportantly, we aim at comparing distinct songs with possi-\nbly similar regions, not discovering similar patterns inside\na single piece. Finally, the tool is used in our case as a\nﬁltering technique to allow efﬁcient identiﬁcation of cover\nsongs.\nAnalogously, many studies adapt exact audio identiﬁca-\ntion systems to account for musical variations (see [14] and\nreferences therein). For instance, Kurth and Muller [14]\npresented an efﬁcient matching technique robust to typi-\ncal variations in interpretation of classical music. The ap-\nproach here is reversed relative to our: an exact fast ﬁn-\ngerprinting system is adapted to account for local slight\nvariations, whereas we propose to enhance the efﬁciency\nof an accurate slow approximate identiﬁcation technique.\nThey report an acceleration of the matching process by a\nfactor of15to20while keeping a high robustness to inter-\npretation variations. However, they emphasize that such a\nspeed-up factor is suitable for efﬁciently handling datasets\nin the order of tens of thousands tracks [14]. To handle\nfast search in larger datasets, an indexing system for cover\nsong identiﬁcation on the MSD was recently proposed [4].\nLandmarks estimations are performed from MSD chroma\nfeatures, and heuristic jumpcodes between landmarks en-\ncode variations of pitch content along cover versions. Au-\nthors substantially reduce the problem to binary identiﬁca-\ntion tasks, and report a fair effectiveness/efﬁciency trade-\noff with a speed of about 200seconds to query the MSD .\nThe method we propose in this paper aims at reducing this\ncover song querying time to the order of a few seconds\nwhile keeping a good accuracy in a standard cover retrieval\ntask.\n3. COMPARING MUSIC SEQUENCES\n3.1 Music representation\nPitch content plays an important role in the structure of au-\ndio pieces, in particular for Western music genres. Com-\nmon compositional processes in such music are organized\naround melodic and harmonic sequences that listeners iden-\ntify, consciously or not, as independent phrases or themes.Pitch Class Proﬁles (PCP), also known as chroma fea-\ntures , are frequently used to describe these types of in-\nformation. These features classify spectral energies into\nbins corresponding to the frequency class where they ap-\npear, each class taking into account the cyclical perception\nof pitch in human auditory system. The number of pitch\nclassespcorresponds to the number of frequency bands\nconsidered in each octave. The parameter pis usually set\nto12to respect the common note scale, but higher val-\nues (generally multiples of 12) can improve the robustness\nto tuning issues [9]. Chroma features are usually consid-\nered as the most robust representation for cover song de-\ntection [16].\n3.2 Music sequences alignment\nThis symbolic representation as a sequence of symbols or\nstring can be used to deﬁne a similarity measure between\ntwo audio pieces. Such a metric is expected to isolate sig-\nniﬁcantly similar sections, or repetitions, assessing their\nresemblance.\n3.2.1 Relevance for music information\nRepetitions in strings have been studied extensively, either\nfor locating exact repeats or for identifying substrings that\nare duplicated within a certain tolerance. In the context of\nmusic sequences, musical similarity does not rely on exact\nmatches since variations, such as transpositions, interpreta-\ntion variations, rhythmic irregularities, background noise,\nmay alter the representing sequences.\nAlignment approaches are well suited for an accurate\nrecognition of such variations. Widely used for biologi-\ncal sequences, such techniques have been extremely suc-\ncessful in identifying approximate repetitions between lo-\ncal patterns in DNA or RNA strings that reﬂect, for in-\nstance, gene homologies. The relevance of such methods\nfor music information lies in the same “evolutionary” as-\npect of musical patterns that may slightly change along a\nsingle piece or accross similar pieces. Playing variations of\nsome musical theme implies changing sound events such\nas notes, rhythms, or lyrics, which echoes the mutation of\nnucleotides of proteins during biological evolution. There-\nfore, alignment techniques are frequently used for identi-\nfying similar patterns in cover songs.\n3.2.2 Alignment and dynamic programming\nThe ﬁrst accurate distance measure for approximate string\ncomparison in the context of biogical sequences is often\ncredited to Needleman and Wunsch [15]. A string uof\nlengthncan be transformed into a string vof length m\nby applying edit operations on the symbols of uandv.\nThese operations are insertions, deletions or substitutions,\nand each is assigned a cost. The edit distance betweenu\nandvis deﬁned by the minimum total cost of edit oper-\nations required to transform uintov. The global align-\nment ofuandvidentiﬁes the positions in the sequence u\nthat are not changed during the process of transforming u\ntov, and their new position in that sequence. A variant\nof this comparison method, local alignment [18], allows\nﬁnding and extracting a pair of regions, one from each\nstring, which exhibit the highest similarity according to the\nscoring scheme assigned to edit operations. In a musical\ncontext, this might correspond to ﬁnding matches betweentwo verses of a song, or between smaller approximately\nduplicated har\nmonic ﬁgures. Its computation is typically\nperformed by a dynamic programming algorithm ﬁlling a\n(n+1)×(m+1) matrix. The local alignment of uandvcan\nbe seen as the best scoring path in the dynamic program-\nming matrix. This edit path is easily computed in practice\nby tracing back the series of operations performed. More\ninformation about alignment algorithms can be found in\n[10].\nIn the context of pitch content, an improvement of local\nalignment [1] allows taking into account a frequent varia-\ntion of musical patterns in terms of harmony, namely local\ntransposition. In Western popular music, for instance, local\ntransposition happens when an occurrence of a structural\npattern ( e.g. chorus) is be played a few semitones higher\nthan usual. The improvement of the alignment technique\nconsists in adding a new edit operation, the local transposi-\ntion of a string versus another. Consequently, we compute\nseveral matrices that estimate every possible local trans-\nposition, and allow a jump from one matrix to another by\npaying a corresponding transposition cost [1]. This vari-\nant yields more accurate alignments of pitch content, but it\nis much slower to compute in practice. If the alphabet of\nsymbols has asymbols, the slowdown is a factor of a.\n3.2.3 Complexity\nDynamic programming gives Θ(nm)running time for com-\nputing optimal alignment scores. Tracing back the effec-\ntively aligned substrings requires O(n+m). Therefore,\nto compare a new song of length nwith a database of k\npieces, each of average length m, requires Θ(knm)time.\nThe naive space complexity is Θ(nm), where we store\nthe entire dynamic programming matrix, although a sim-\nple trick allows reducing it to Θ(max(m,n))by keeping\nonly the last computed lines [10].\nLocal alignment techniques are particularly useful for\nthe accurate identiﬁcation of strong similarities between\nsequences [16]. However, the slowness of the dynamic\nprogramming makes them heavy to compute and unadapted\nto fast querying of large-scale datasets that comprise mil-\nlions of sequences. Facing a similar challenge, bioinfor-\nmatics researchers developed a fast heuristic-based search\ntool dedicated to efﬁcient indexing for local alignment.\n3.3 BLAST\nThe Basic Local Alignment Search Tool (BLAST) [2], re-\nduces the computational cost of local alignment. BLAST\nrelies on the observation that when querying a new se-\nquence to a large database, there are likely only a small\nnumber of good alignments, so it ﬁlters the database to\navoid computing irrelevant alignments of unrelated sequen-\nces. BLAST partially explores the dynamic programming\nsearch space to ﬁlter out many irrelevant comparisons be-\nfore computing local alignments. It consists of several\nheuristic layers of rules for reﬁning the potential regions\nof strong similarity, as described in the next sections.\n3.3.1 Seeding the search space\nThe main heuristic of BLAST lies in the assumption that\nsigniﬁcant local alignments include small exact matches.\nAs represented in Fig. 1- (i), the dashed edit path of the lo-\ncal alignment of uandvcontains diagonal sections, thatݒ \nݑ ݒ \nݑ ݒ \nݑ \nݒ \nݑ ߜ \nߜ \nߜ \nFigure 1 . Seeding the search space. Top-Left: exact\nsimilar secti\nons (plain lines) inside a local alignment path\n(dashed line). Top-Right: automatic seeding of the search\nspace, where identiﬁed regions may belong to the local\nalignment path (plain lines) or not (dashed lines). Bottom-\nleft: as result of F1, seeds are quickly clustered and iso-\nlated grey dashed seeds are eliminated. Bottom-right: as a\nresult ofF2, seed extensions roughly depict the local align-\nment path. Background grey parts highlight the number of\ndynamic programming computations required.\npartially correspond to runs of matches (plain lines), i.e.\nexact repetitions between small sections of uandv. The\nﬁrst step of BLAST ﬁnds these small common substrings\nin order to seed the search space for later local alignments.\nA practical way of indexing the search space is to ﬁx a seed\nlengthN, and index every N-length substring (or words)\nof every sequence of the dataset in a fast access data struc-\nture.\n3.3.2 Filtering seeds\nOnce seeded as in Fig. 1 −(ii), the search space includes hit\nregions that may correspond to high-scoring local align-\nment of sequences (plain segments) or to suboptimal re-\ngions (dashed segments), where the exact seed match arose\ndue to coincidence, not true similarity. The second step of\nBLAST ﬁlters out most seeds that do not correspond to\ndesired local alignments.\nA ﬁrst ﬁltering technique F1consists in quickly clus-\ntering seeds among the search space, and identifying iso-\nlated hits. As illustrated in Fig.1, every correct heuris-\ntic alignment should have several seeds around a diago-\nnal (plain lines) that sketches the actual local alignment.\nConsequently, a pair of sequences that does not have re-\ngions comprising a signiﬁcant number of hits may be ﬁl-\ntered out. We use a threshold δto stand for the maximum\ninter-diagonal distance allowed between two consecutive\nseeds to be considered as around the same diagonal, i.e.\npotentially belonging to the same alignment.\nA more accurate, also common, ﬁlter F2is seed exten-\nsion. Each seed is extended in both directions to determine\nwether it corresponds to a local similar region or not. We\ndenote by (i,k)and(j,l)the coordinates of a seed in the\nsearch space, i.e.the hit is between the exactly matching\nsubstrings u[i···j]andv[k···l]. We compute two small\nalignments, one starting from (i,k)rolling up towards the\ntop left corner of the search space, and the other one start-\ning from(j,l)going towards the bottom right corner. Eachof these alignments, that may be gapped or ungapped, are\nquickly stoppe\nd if their score drops off under a threshold\nvalueX. This way, extending unrelated sequences quickly\nresults in stopping the computation, while seeds from lo-\ncally similar regions grow towards alignments [3].\n4. INDEXING MUSIC DATA\n4.1 Evaluation framework\n4.1.1 Database\nTwo datasets were used to evaluate our system. The ﬁrst\nset,TSD, consists of 2,514Western popular music pieces\ncomprising 514cover songs distributed in 17cover classes,\ncoming from personal music collections1. We elected as\na second set the million song dataset ( MSD ) [5], that com-\nprises one milliong songs of various musical genres2. It\nincludes the Second Hand Songs dataset3, which com-\nprises18,196cover songs distributed in 5,854cover classes.\nThis set is to our knowledge the largest available set of\ncover song audio features.\nBoth datasets provide chroma feature sequences. How-\never, it is worth noting that the implementation of such fea-\ntures signiﬁcantly differs in both cases. In TSD, we used\nour own implementation of Harmonic Pitch Class Pro-\nﬁles [9] with a constant frame size and a resolution of 36\nsubdivisions per octave. This representation was success-\nfully applied for past MIREX tasks4. In MSD , chroma\nfeatures were computed using The EchoNest API5, and\nconsist of segment-synchronized 12-dimensional chroma\nfeatures, as described in [11]. Thus, each MSD chroma\nrepresents pitch content on 12dimensions for a variable\naudio frame (generally between 80to300ms with no over-\nlap), whereas each TSD chroma represents pitch content on\n36dimensions for a constant audio frame ( 743ms with half\noverlap). The difference between these two approaches\nturns out to have major implications.\n4.1.2 Alphabet deﬁnition\nIn its general deﬁnition, each B-dimensional chroma fea-\nture consists of Bbins that may take any positive value,\nso their domain is inﬁnite. However, as explained in Sec-\ntion 3.3, the heuristic alignment relies on the identiﬁcation\nof exact similar regions in discrete sequences. Hence, it is\ncritical for a BLAST approach to ﬁrst project representing\nsequences onto a ﬁnite alphabet. A natural quantization is\nto detect the most probable chord, by looking for the high-\nest scoring triad (root, major/minor third, ﬁfth), and assign-\ning a symbol corresponding to the index of this best triad.\nWe represent each chroma feature over a 12-letter alpha-\nbet∆ ={a,b,c,...,k,l }by the root of the predominant\nchord played. The chord mode (major/minor) is not taken\ninto account since a root-exclusive representation seems to\nprovide sufﬁciently meaningful audio representations (see\nSection 4.2). The 36-dimensional chroma features are also\nreduced down to 12possible root chord symbols by keep-\ning the multiple of 3closest to the index of the highest triad\n(reduction to a 12note scale with robustness to de-tuning).\n1See http://www.labri.fr/perso/bmartin/ISMIR12 for the list\n2http://labrosa.ee.columbia.edu/millionsong/\n3http://www.secondhandsongs.com\n4MHRAF submissions in Struct.Seg-11, Cover song-10\n5http://the.echonest.com/TSD MSD\nSeed False False False False\nsize negative positive negative positive\n(i)3 0.00 8.91 0.11 4.70\n4 0.03 3.69 4.28 1.15\n5 0.84 1.68 18.3 0.39\n6 4.50 0.82 37.2 0.16\n7 11.7 0.44 54.7 0.08\n8 21.8 0.25 68.6 0.04\n(ii)3 0.00 1.06 0.84 1.33\n4 0.21 0.41 7.08 0.23\n5 2.59 0.18 22.6 0.06\n6 6.67 0.08 41.8 0.02\n7 14.9 0.04 58.9 0.01\n8 26.0 0.02 72.1 .003\nTable 1 . Sensitivity/speciﬁcity tradeoff on TSD andMSD .\nScores\nare given as percentages. In (i), all words are in-\ndexed in the dataset. In (ii), mono-symbolic words are not\nindexed.\nThis projection is likely to introduce inconsistencies in\ncompared sequences, due to such a simplistic analysis. How-\never, in practice the method only requires small sections of\naligned sequences to be identical in order to assess their\nsimilarity, and is tolerant to sparse analysis errors to some\nextent.\n4.1.3 Transposition invariance\nTransposition is a very common variation among cover\nversions. Either globally on an entire rendition or locally\naccross structural parts, transpositions have to be taken into\naccount in similarity analysis [17]. As highlighted in [14],\nthe indexing strategy should hash harmonic progressions\ninstead of absolute pitch content. While looking for ex-\nact matches between two sequences, compared regions are\nthus transposed down to a common key. For instance, the\nsequencesbcbbfd anddeddhd are in fact an exact match\nsince they describe the same chord variations regardless of\ntheir local key, which differ by a major second. Practically,\nthis can be seen as translating all words such that they start\nwith the symbol a.\n4.2 Seed determination\nThe heuristic alignment strongly relies on the selection of\nmeaningful parts. A key issue for the method is to deter-\nmine a seed both sensitive enough to correctly index align-\nments between cover songs, and speciﬁc enough to index\nas few spurious hits as possible. The ﬁrst parameter for\noptimizing the sensitivity/speciﬁcity tradeoff is the length\nof the seed. In the following, given a seed length N, we\ndenote by ∆Nthe set of all possible words of length N\nover∆.\n4.2.1 Sensitivity evaluation\nAssessing the sensitivity of a seed can be done by analyz-\ning the alignments of similar sequences. Let Nbe a seed\nlength. We must determine how many alignments actually\ncontain seeds of size N,i.e.how many alignments contain\nat least one run of Nmatches. Thus, we ﬁrst computed on\nboth TSD andMSD local alignments between cover songs\nusing the local transposition variant described in 3.2.2. By\ntracing back the alignments, we were able to identify exact\nruns of matches. An alignment is considered validated ifTSD MSD\nWord % Word %\naaaaaaa 6.36aaaaaaa 14.1\nahhhhhh 0.55aaaaaah 1.45\naaaaaah 0.53aaaaaaf 1.31\naffffff 0.49affffff 1.13\naaaaaaf 0.46ahhhhhh 0.98\nTable 2 . The ﬁve most probable words in TSD andMSD\nand the\nir frequency of occurrence (in % of the words in\neach database) for N= 7and a12-letter alphabet.\nit can be indexed, i.e.if at least one long enough run is\nfound. The second and fourth columns of Tab. 1-(i) show\nthe probability Pr[false negative ]that an alignment can not\nbe indexed by the method, as a function of the seed length,\non both of the datasets.\n4.2.2 Speciﬁcity evaluation\nTo evaluate the speciﬁcity of a seed, we need to estimate\nthe probability of ﬁnding two identical runs in unrelated\naudio sequences. Practically, for a given word wover∆N,\nwe count the number of occurrences of win the database\nof unrelated sequences (no cover songs). This computa-\ntion is stored in a list Land repeated for each possible\nword. In the end, L[j]contains the number of instances\nof a particular word, and/summationtext\niL[i]is the total number of\nN-long words in the dataset. The probability of ﬁnding\nonewordwin a random chunk of the database is given\nbyL[j]/summationtext\niL[i], wherejis the ind ex inLcorresponding to w.\nSubsequently, the overall probability of ﬁnding twoidenti-\ncal words in the database is given by Pr[false positive ] =\n1\n(/summationtext\niL[i])2/summationtext\njL[j]2. The third and ﬁfth columns of Tab. 1-\n(i) provides the Pr[false positive ]computed for each seed\nlength and for both of the datasets, as percentages.\n4.2.3 Word distribution\nTable 2 shows the ﬁve most probable words in both datasets\nfor a ﬁxed seed length of 7. The most probable bin is the\nmono-symbolic word. Following bins correspond to fre-\nquent intervals in tonal music: up-by-ﬁfth ( a→h) or down-\nby-ﬁfth (a→f). In both of the datasets, mono-symbolic\nwords occur far more often than other words, representing\n6.36% and14.1% of the words in TSD andMSD , respec-\ntively. Thus, mono-symbolic words are likely to be respon-\nsible for many false positive hits while not capturing very\nsensitive regions in true alignments. We hence re-evaluated\nin Tab. 1-(ii) the sensitivity and speciﬁcity tradeoff on both\ndatasets without indexing mono-symbolic words. As a re-\nsult, for a seed length of 7symbols, speciﬁcity is increased\nby a factor between 8and10, while sensitivity is reason-\nably decreased by around 3%in both datasets.\n5. RESULTS AND DISCUSSION\nStatistical results emphasize a signiﬁcant difference bet-\nween sequences in both datasets. First, cover song align-\nments share much fewer words in MSD than in TSD. For\ninstance, for a seed size of 7, only41.1%of the cover song\nalignments in MSD share common multi-symbolic words,\nas compared to 85.1% inTSD. Subsequently, the charac-\nterization of cover songs in MSD should be more difﬁcult\nthan in TSD. Moreover, false negative rates suggest that thedistributions of words between both datasets are different,\nhence it would not be relevant to put them in common for\nevaluation purpose.\nTo test the relevance of our system in comparison with\nalignment methods, we implemented the alignment tech-\nniques described in 3.2.2. Since indexing is computed in a\ntransposition invariant manner, we tested local alignments\nwith the accurate local transposition variant [1]. We eval-\nuated the identiﬁcation technique using the Mean of Aver-\nage Precision (MAP), the standard metric for cover song\nretrieval evaluation [8, 16, 17].\nFrom every cover class in TSD, we computed align-\nments of each member to the rest of the class and to confus-\ning songs. Average MAP values are presented in Tab. 3-(i),\nand detailed among 8cover classes of TSD in Fig. 2. We\nexperimented with the same approach on MSD , more pre-\ncisely on a subset that made the alignment computation\npracticable. This subset MSD2kwas formed by randomly\nchoosing 30 cover song classes and 2000 confusing songs\nfrom MSD . We discovered that the identiﬁcation method\ndid not extend, as indicated by the MAP scores (Tab. 3-\n(ii)). We identify two possible reasons: 1) Cover songs\nare particularly different from each other in MSD ; 2)MSD\nchroma features are not as suitable as TSD features for se-\nquence alignment. To isolate the second possibility, we re-\ncomputed TSD with the audio features used in MSD , via\nthe EchoNest API6. We repeated the same cover identi-\nﬁcation experiment on this new dataset /tildewidestTSD and obtained\nMAP scores indicated in Tab. 3-(iii). The signiﬁcant drop\nin MAP scores between TSD and/tildewidestTSD evaluations suggest\nthatMSD chroma features do not make for alignable se-\nquences. This result, already infered in [4], is substantiated\nby the high false negative rate found in MSD sequences\n(Tab. 1). We think this is due to critical implementation\ndifferences in chroma features between both datasets such\nas chroma dimension, temporal ﬁltering and segment syn-\nchronism, which may not be adapted to standard alignment\ntechniques, as highlighted in [16] for instance.\nWe implemented the indexing strategy described in 4\nonTSD andMSD sequences. Logically, previous results\nmade MSD heuristic alignments ineffective. MSD is to be\nconsidered here only as a computation performance indi-\ncator. From Tab. 1, we chose the seed length N= 7, that\nfeatures a reasonable false negative rate of 11.7%, elim-\ninating the most dissimilar cover songs, for a low false\npositive rate of 0.44% that guarantees high performance\nwith few spurious hits. For each dataset, we built a ta-\nble hashing every Nwords in sequences and storing their\npositions. Then, for each query, all matching songs were\nﬁltered using either F1orF1and thenF2. Resulting MAP\nscores are given in 3-(iv) and (v). Highly depending on\nthe dataset, our scores are not intended to be compared to\nstate-of-the-art results. Their relevance lies in the compar-\nison between basic method and heuristic alignments. As\nshown in Fig. 2, BLAST ﬁlters seem to slightly decrease\nthe accuracy of the cover identiﬁcation. Combining both\nﬁlters seems effective for quickly identifying most covers.\nIndeed, the overall accuracy of BLAST method on TSD\nreaches a MAP score of 30.11%, corresponding to a loss\nof14.71% as compared to an accurate sequence alignment.\n6http://developer.echonest.com/Method DatasetMAP Runtime\n(%) (s/query)\n(i)\nAlignmentTSD 44.82 129\n(ii) MSD2k 5.71 388\n(iii) /tildewidestTSD 7.20 273\n(iv) MSD - 193,765\n(v)BLAST-{F1}TSD 18.06 0.24\n(vi) MSD - 12.20\n(vii)BLAST-{F1,F2}TSD 30.11 0.33\n(viii) MSD - 16.9\nTable 3 . MAP results and computing times for cover iden-\ntiﬁcation on TS\nDandMSD .\n5.1 Computational efﬁciency\nDue to the very high number of entries in the hash table, we\nimplemented an efﬁcient memory key/value lookup system\nin C. Building the index from chord sequences required\nabout 16 minutes for the whole MSD on our server7. The\naverage querying runtimes for each approach are given in\nTab. 3. Note that we did not use parallel computing in this\nstudy. As expected, alignments imply slow computation,\nresulting in about 129seconds per query on TSD and388\nseconds per query on MSD2k. By counting the number of\nsymbols in the whole MSD dataset, we infer that approxi-\nmately53hours would be required to compute alignments.\nNote that removing the local transposition variant would\nspeed-up by a factor of 12, still inadequate for practical\ncomputation. Thanks to BLAST heuristics, computation is\ndrastically improved with around 12.2seconds per query\nonMSD (0.24seconds on TSD) with the coarse ﬁlter F1,\nand16.9seconds per query with both ﬁlters on MSD (0.33\nseconds on TSD), on average.\n6. CONCLUSION\nWe presented a new method for practical cover identiﬁ-\ncation on large scale datasets. Inspired by bioinformat-\nics heuristics, we applied BLAST to audio features and\ninvestigated the distribution of music sequences. Results\nobtained on our dataset suggest a reasonable loss of ac-\ncuracy of the retrieval system in exchange for a substan-\ntial gain in computing time: estimating cover songs of a\n3 minutes feature sequence can be acheived in less than\n15seconds in a million song database. We see this out-\ncome as a signiﬁcant step towards the practical search for\napproximate patterns in large datasets. Another main re-\nsult of our study, although quite unexpected, is the appar-\nent limitation of MSD chroma features regarding sequence\nalignment. Future studies on the MSD involving align-\nment techniques should investigate further the distribution\nof chroma sequences, and maybe combine them to other\ndata ( e.g. loudness, timbre). Future work will be partic-\nularly focused on enhancing the sensitivity of the identiﬁ-\ncation of cover songs, considering for instance spaced or\nvariable length seeds for BLAST.\n7. REFERENCES\n[1] J. Allali, P. Ferraro, P. Hanna, and C. Iliopoulos. Local trans-\npositions in alignment of polyphonic musical sequences. In\nString Processing and Information Retrieval , volume 4726,\npages 26–38. Springer Berlin / Heidelberg, 2007.\n7Hardware: Intel Xeon X5675 @ 3.07GHz, 12M cache, 32GB RAM020406080\nC1 C2 C3 C4 C5 C6 C7 C8MAP (%) \nFigure 2 . MAP scores obtained on 8cover classes ofTSD.\nBlack: alignment, grey: BLAST- {F1}, BLAST- {F1,F2},\nwhite: alignment with MSD chroma features ( /tildewidestTSD).\n[2] S.F. Altschul, W. Gish, W. Miller, E.W. Myers, D.J. Lipman,\net al. Basic local alignment search tool. Journal of molecular\nbiology , 215(3):403–410, 1990.\n[3] S.F. Altschul, T.L. Madden, A.A. Sch¨ affer, J. Zhang,\nZ. Zhang, W. Miller, and D.J. Lipman. Gapped blast and psi-\nblast: a new generation of protein database search programs.\nNucleic acids research , 25(17):3389–3402, 1997.\n[4] T. Bertin-Mahieux and D.P.W. Ellis. Large-scale cover song\nrecognition using hashed chroma landmarks. In IEEE Work-\nshop on Applications of Signal Processing to Audio and\nAcoustics , pages 117–120, 2011.\n[5] T. Bertin-Mahieux, D.P.W. Ellis, B. Whitman, and P. Lamere.\nThe million song dataset. In Proc. of the 12th International\nConference on Music Information Retrieval , 2011.\n[6] M.A. Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes,\nand M. Slaney. Content-based music information retrieval:\nCurrent directions and future challenges. Proc. of the IEEE ,\n96(4):668–696, 2008.\n[7] R.B. Dannenberg and N. Hu. Pattern discovery techniques\nfor music audio. In Proc. of the 3rd International Conference\non Music Information Retrieval , pages 63–70, 2002.\n[8] J.S. Downie, M. Bay, A.F. Ehmann, and M.C. Jones. Audio\ncover song identiﬁcation: Mirex 2006-2007 results and anal-\nyses. In Int. Symp. on Music Information Retrieval (ISMIR),\npages 468–473, 2008.\n[9] E. G´ omez. Tonal Description of Music Audio Signals . PhD\nthesis, Universitat Pompeu Fabra, pages 63–100, 2006.\n[10] D. Gusﬁeld. Algorithms on strings, trees, and sequences:\ncomputer science and computational biology . Cambridge\nUniversity Press, pages 215–253, 1997.\n[11] T. Jehan. Creating Music by Listening . PhD thesis, Mas-\nsachusetts Institute of Technology, pages 57–59, 2005.\n[12] J. Kilian and H.H. Hoos. Musicblast - gapped sequence\nalignment for mir. In Proc. of the 5th International Confer-\nence on Music Information Retrieval , pages 38–41, 2004.\n[13] J.F. Kilian. Inferring Score Level Musical Information From\nLow-Level Musical Data . PhD thesis, Darmstadt University\nof Technology, pages 54–60, 2004.\n[14] F. Kurth and M. M¨ uller. Efﬁcient index-based audio match-\ning.IEEE Trans. on Audio, Speech, and Language Process-\ning, 16(2):382–395, 2008.\n[15] S.B. Needleman and C.D. Wunsch. A general method appli-\ncable to the search for similarities in the amino acid sequence\nof two proteins. Journal of Molecular Biology , 48(3):443–\n453, 1970.\n[16] J. Serr` a, E. G´ omez, and P. Herrera. Audio cover song identi-\nﬁcation and similarity: background, approaches, evaluation,\nand beyond , volume 274 of Studies in Computational Intelli-\ngence , chapter 14, pages 307–332. 2010.\n[17] J. Serr` a, E. G´ omez, P. Herrera, and X. Serra. Chroma binary\nsimilarity and local alignment applied to cover song identi-\nﬁcation. IEEE Trans. on Audio, Speech and Language Pro-\ncessing , 16:1138–1151, 2008.\n[18] T.F. Smith and M.S. Waterman. Identiﬁcation of com-\nmon molecular subsequences. Journal of molecular biology,\n147(1):195–197, 1981."
    },
    {
        "title": "A Corpus-based Study of Rhythm Patterns.",
        "author": [
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414848",
        "url": "https://doi.org/10.5281/zenodo.1414848",
        "ee": "https://zenodo.org/records/1414848/files/MauchD12.pdf",
        "abstract": "We present a corpus-based study of musical rhythm, based on a collection of 4.8 million bar-length drum patterns extracted from 48,176 pieces of symbolic music. Ap- proaches to the analysis of rhythm in music information retrieval to date have focussed on low-level features for re- trieval or on the detection of tempo, beats and drums in audio recordings. Musicological approaches are usually concerned with the description or implementation of man- made music theories. In this paper, we present a quantita- tive bottom-up approach to the study of rhythm that relies upon well-understood statistical methods from natural lan- guage processing. We adapt these methods to our corpus of music, based on the realisation that—unlike words—bar- length drum patterns can be systematically decomposed into sub-patterns both in time and by instrument. We show that, in some respects, our rhythm corpus behaves like nat- ural language corpora, particularly in the sparsity of vo- cabulary. The same methods that detect word collocations allow us to quantify and rank idiomatic combinations of drum patterns. In other respects, our corpus has proper- ties absent from language corpora, in particular, the high amount of repetition and strong mutual information rates between drum instruments. Our findings may be of direct interest to musicians and musicologists, and can inform the design of ground truth corpora and computational models of musical rhythm.",
        "zenodo_id": 1414848,
        "dblp_key": "conf/ismir/MauchD12",
        "keywords": [
            "corpus-based study",
            "bar-length drum patterns",
            "4.8 million",
            "music information retrieval",
            "statistical methods",
            "natural language processing",
            "bottom-up approach",
            "realisation",
            "systematic decomposition",
            "idiomatic combinations"
        ],
        "content": "A CORPUS-BASED STUDY OF RHYTHM PATTERNS\nMatthias Mauch Simon Dixon\nCentre for Digital Music, Queen Mary University of London\nfmatthias.mauch, simon.dixong@eecs.qmul.ac.uk\nABSTRACT\nWe present a corpus-based study of musical rhythm, based\non a collection of 4.8 million bar-length drum patterns\nextracted from 48,176 pieces of symbolic music. Ap-\nproaches to the analysis of rhythm in music information\nretrieval to date have focussed on low-level features for re-\ntrieval or on the detection of tempo, beats and drums in\naudio recordings. Musicological approaches are usually\nconcerned with the description or implementation of man-\nmade music theories. In this paper, we present a quantita-\ntive bottom-up approach to the study of rhythm that relies\nupon well-understood statistical methods from natural lan-\nguage processing. We adapt these methods to our corpus of\nmusic, based on the realisation that—unlike words—bar-\nlength drum patterns can be systematically decomposed\ninto sub-patterns both in time and by instrument. We show\nthat, in some respects, our rhythm corpus behaves like nat-\nural language corpora, particularly in the sparsity of vo-\ncabulary. The same methods that detect word collocations\nallow us to quantify and rank idiomatic combinations of\ndrum patterns. In other respects, our corpus has proper-\nties absent from language corpora, in particular, the high\namount of repetition and strong mutual information rates\nbetween drum instruments. Our ﬁndings may be of direct\ninterest to musicians and musicologists, and can inform the\ndesign of ground truth corpora and computational models\nof musical rhythm.\n1. INTRODUCTION\nIn Western popular music and jazz, the main percussive\ninstrument is the drum kit, consisting of a collection of\ndrums and cymbals arranged around the drummer. Drum\nkits can contain a large range of different instruments. The\nbass drum (orkick drum) is usually the drum with the low-\nest frequency and is operated via a foot pedal. The snare\ndrum, the dominant back-beat instrument, has a higher-\npitched sound with additional noise components from the\nsnares spanned across its lower skin. The hi-hat is made\nfrom two cymbals facing each other, which the drummer\ncan open and close via a foot-pedal. The closed hi-hat has\na short, high-pitched sound, whereas the open hi-hat has a\nlonger sustain. Ride cymbals have a sustained high-pitched\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\nbdsdhhhoricrtotbhcpe(a) a pattern in4\n4meter\nbdsdhhhoricrtotbhcpe\n(b) pattern projected to the bass drum/snare drum space\nFigure 1: Example drum patterns in drum tab notation,\nwith musical time on the xaxis; strong lines are beats.\nDrums from bottom to top: bd – bass drums, sd – snare\ndrums, hh – hi-hat closed/pedal, ho – hi-hat open, ri – ride\ncymbals, cr – crash cymbals, to – toms, tb – tambourine,\nhc – hand clap, pe – other percussion.\nring, while the crash cymbals’ sound is usually more noise-\nlike. Tom-toms are drums of intermediate sizes between\nthe bass and snare drums. Hand-claps andtambourine are\noften used to provide additional colour, along with further\nvaried percussion instruments, which we do not discuss in\nthis study. The history and makeup of the modern drum kit\nis covered comprehensively elsewhere, e.g. [3].\nIn music information retrieval (MIR), most research on\nrhythmic features has concentrated on beats, meter and\ntempo. The tracking of beats establishes a temporal grid on\na piece of music, which is useful to anchor other descrip-\ntors of a musical piece in time. The timing of beats also\ndetermines the tempo, which correlates with the perceived\nspeed of the music [10]. The automatic identiﬁcation of\nmeter [9] is also based on beats, and provides additional\nrhythmic information. However, neither meter nor tempo,\nnor their combination, capture the temporal sequence of\nrhythmic events. Several audio features have been devel-\noped to include this temporal information [5, 17, 19] with\nconsiderable success, especially for clear-cut cases such as\nthe classiﬁcation of ballroom dances [4]. Being concerned\nwith good performance in retrieval tasks, these methods\nare deliberately agnostic to how the rhythmic signal was\noriginally created. On the other end of the spectrum liesthe automatic transcription of music, and that of drums in\nparticular. Drum transcription algorithms [7, 22, 23] usu-\nally neglect musical higher-level context such as meter and\nsimultaneous rhythm patterns. One possible exception is\nPaulus’sn-gram model of drum sequences [18] that in-\nforms transcription. The n-gram model demonstrates that\ncontext models can be useful to suppress errors, but it is\nalso quite obvious that modelling rhythm as sequences of\nhalf-beat length symbols is a strong simpliﬁcation that can-\nnot capture interactions of concurrent rhythms played on\nmultiple drums. A further simpliﬁcation present in most\ndrum transcription papers is the very small set of different\ndrums considered: bass drum, snare drum and hi-hat. In\nthis paper, we consider a much larger rhythm space, both\nin terms of temporal context and drum instrumentation.\nComparatively little work in MIR has quantitatively ex-\namined rhythms in symbolic data. While Muramaki’s work\non drum ﬁll detection [15] is concerned with analysis, most\nwork is focussed on improving music production, for ex-\nample by combination drum loops of suitable complexity\n[21]. The study of rhythm has a long tradition in musicol-\nogy, but only in recent decades has empirical music analy-\nsis found its way into the musicological tradition. Notable\ntools include the Humdrum Toolkit [8], jSymbolic [14] and\nmusic21 [2], which facilitate the processing of symbolic\nmusic, but do not directly examine the statistical properties\nof the corpus itself, nor provide tools as sophisticated as\nthose available for natural language processing. In the do-\nmain of harmony, some attempts have been made to anal-\nyse chord progressions with language models [13, 20].\nIn this paper, language models are employed to analyse\nthe statistical properties of a large corpus of drum parts, to\nreveal the degree of variety within and between pieces, and\nto discover interdependencies between different parts of\nthe drum kit. In the next section we describe our represen-\ntation of rhythm patterns, while in section 3 an overview\nof the data set, consisting of 48,176 MIDI ﬁles, is given.\nSection 4 provides the results of our analyses, and the ﬁnal\ntwo sections contain a brief discussion and conclusions.\n2. DRUM PATTERN DEFINITION\nIn order to build a corpus of drum patterns, we need to\nsegment the music into short chunks whose lengths corre-\nsponds to meaningful metrical units. Since we are deal-\ning with a symbolic representation which provides unam-\nbiguous onset times, the main effort required is to parse\nthe events according to the metrical structure, suppress-\ning performance-related information such as ﬂuctuations\nin tempo, timing, and dynamics, which—for the purposes\nof this study—we are not interested in. Instead, similar to\nlinguists building text corpora from stemmed words with\ngrammatical endings removed, we build reduced drum pat-\ntern models by applying ﬁve levels of abstraction.\nBar segmentation. The tracks are segmented into bars as\nencoded in the MIDI ﬁles. Each bar is a token, the funda-\nmental unit, similar to word tokens in language.\nDrum categorisation. We summarise the General MIDI\nstandard drums into 10 known drum categories (see\nFigure 1) and one unknown category.count portion\n4/4 4,305,516 90.3%\n3/4 188,297 3.9%\n2/4 114,068 2.3%\n6/8 53,681 1.1%\n12/8 19,575 0.4%\nother 84,830 1.7%\nTable 1: Distribution of time signatures in the corpus.\nTempo abstraction. We discard tempo information (but\nnot metrical structure).\nIntensity abstraction. We discard sound intensity infor-\nmation, i.e. MIDI velocity.\nQuantisation. We quantise the drum notes relative to the\nbeat, reducing the granularity to a grid of 12 equally spaced\ndivisions per beat span, and retain only their onset time.\nThe resulting representation contains approximately the\nsame information that would be found in traditional score\nnotation. After this “stemming” procedure, we charac-\nterise a drum pattern via the presence (or absence) of drum\nonsets for each beat, position within the beat, and drum\ncategory, as visualised in the example drum tab represen-\ntation shown in Figure 1a. Hence, a bar with Nbbeats\ncan be represented as a binary sequence of Nb\u000212\u000211\nbits. For the most frequent time signature,4\n4, the number\nof beats isNb= 4, and so the space of possible4\n4patterns\nallows 24\u000212\u000211\u001910159different patterns. Thus, despite\nﬁve abstraction steps, we have retained an extremely large\npattern space. Since the space is much larger than any data\nset, it is clear that large parts of the space will never ap-\npear in actual music. We show later that we can not only\nquantify the size of the space used in a given corpus, but\nalso make predictions about how much of the space will be\nused as the corpus grows.\nWe deﬁne drum pattern sub-spaces by discarding some\ndrums or metric positions. For example, if we restrict\nour attention to sub-patterns made of only bass and snare\ndrums, a large number of different full patterns with, say,\ndifferent use of the hi-hat would be mapped to the sub-\npattern shown in Figure 1b.\n3. DATA\nWe collected 72,283 unique MIDI ﬁles from the Internet.\nIn order to understand the nature of the resulting collec-\ntion, we drew a random sample of 100 songs and manually\nclassiﬁed them. The sample mainly contains pop/rock mu-\nsic (62 songs), ﬁlm music (10), jazz (9), classical (7) and\ncountry/folk music (6). Of the six remaining songs, ﬁve are\nof various genres and one was not decodable. A large pro-\nportion of the songs are good-quality renditions of popular\nrecordings.\nA study of the within-track interonset intervals (IOIs)\non the whole dataset reveals that many songs are already\nquantised; about a third of the songs (34%) contain >99%(a) All metrical positions\npredicted\n# types atPRswRloc\ndrums # types 20M tokens in % in % in %\nall 656798 1688906 6.23 73.5 33.4\nbd 46243 101230 0.45 91.1 64.2\nsd 62647 143525 0.62 90.5 67.5\nbd/sd 186688 454218 1.90 85.5 52.6\nhh/ho 76351 174590 0.79 91.3 69.4\ncmb 170344 415935 1.76 85.8 54.3\nto 29394 70500 0.30 95.3 85.9\nhc/tb/pe 84417 191712 0.88 94.4 81.9\nbd/sd/cmb 466962 1176552 4.76 77.6 38.5\n(b) Beats 1 and 2 only\npredicted\n# types atPRswRloc\ndrums # types 20M tokens in % in % in %\nall 342453 786850 3.30 82.1 48.0\nbd 7602 14788 0.07 94.9 76.8\nsd 14272 30612 0.14 94.7 79.5\nbd/sd 48493 106701 0.47 91.3 68.5\nhh/ho 21460 44131 0.20 94.5 79.1\ncmb 57287 124465 0.54 90.8 64.8\nto 7523 16782 0.07 97.2 92.1\nhc/tb/pe 32014 67845 0.31 96.1 86.9\nbd/sd/cmb 198699 454309 1.94 85.1 52.9\nTable 2: Sub-pattern statistics. Pis productivity (see\nSection 4.2), Rare repetition indices (Section 4.3).\nIOI-quantised events, while 60% still contain >75% IOI-\nquantised events. Our impression that the songs are usually\ncarefully crafted for authentic playback is reﬂected in the\nfact that 71% of the songs have varied velocities (less than\nhalf of the notes uses the most popular velocity), i.e. it is\nlikely that only few songs are MIDI exports from music\ntypesetting programs.\nIn order to limit the inﬂuence of abnormally long songs\nonly notes less than 20 minutes into any song are con-\nsidered. Very soft drum notes (velocity <20) are re-\nmoved. We exclude songs with empty drum tracks, and\nthose whose musical beat is likely to be out of sync with\nthe MIDI beat (i.e. where the frequency of on-beat drum\nnotes is<50% that of the most frequent quantisation).\nAfter decoding, the collection contains 4,765,947 bar\ntokens in 48,176 ﬁles, which corresponds to a mean of\naround 99 bars per song. The overwhelming majority, 90%\nof bars, is in4\n4time, with only a few other time signatures\nexceeding 1% of the corpus (see Table 1).\nThe terms typeandtoken are borrowed from natural lan-\nguage processing and will be used here as follows:\ntype: unique drum pattern (\u0019 unique word in language),\ntoken: drum pattern type instance.\nThe overall number of bar types in our database is\n656,798. The sub-pattern spaces retain the same number\n1 2 5 10 20\nfrequency r  in collectionVr       \n100010000100000Figure 2: Type frequencies Vrby token count rfor drum\npatterns (ﬁlled squares), drum pattern by song (ﬁlled trian-\ngles), the Brown language corpus (blank squares).\nof tokens, but can have dramatically fewer distinct pattern\ntypes, as Table 2 (column 2) shows.\n4. RESULTS\nThis section provides an overview and discussion of some\ninsights that can be gained from our corpus of drum\npatterns. More exhaustive information is available at\nhttp://isophonics.net/ndrum.\n4.1 Large Number of Rare Events\nAs with many natural language corpora, the distribution of\ntype counts and the frequencies of these type counts are ex-\ntremely skewed. Figure 2 shows a plot called frequency-of-\nfrequencies plot, in which the number of type occurrences\nrin the database is plotted against the number of types Vr\nthat occurrtimes. The ﬁgure shows three graphs: drum\npattern counts, song-wise drum pattern counts (one per\nsong in which the type occurs), and word type counts from\na corpus of American English, the Brown Corpus. The\nnumberV1of types that occur only once in the whole col-\nlection is greatest; tokens occurring twice already account\nfor much smaller fractions of the corpora, a phenomenon\noften referred to as large number of rare events (LNRE).\nThe log-log scale plot in Figure 2 illustrates an addi-\ntional property of the data: all distributions can be approx-\nimated by a straight line, a characteristic of “scale-free”\ndistributions. For a discussion of this phenomenon, see,\nfor example, [16]. While the full drum pattern count (ﬁlled\nsquares) resembles the word distribution in the Brown cor-\npus in slope (the absolute height reﬂects that the Brown\ncorpus has only 1M tokens), it is not as smooth as the\nBrown corpus’s. However, the unstable nature of the graph\nis not random; rather, the higher values at multiples of\n2 reﬂect the usual organisation of music in units of even\nmultiples of bars. As we would expect, then, counting the\nnumber of songs in which a type appears leads to a much\nsmoother graph (ﬁlled triangles) that is unaffected by rep-\netitions. We will return to song-wise counts in Section 4.3.4.2 Vocabulary Growth\nFor LNRE distributions we can estimate how fast the num-\nber of types (in our case: distinct drum patterns) is growing\nwith vocabulary size. A popular measure for that is pro-\nductivityP[1]. For a corpus of size NwithV1types that\noccur only once, productivity is calculated as\nP=V1=N: (1)\nThis measure is an indicator of the potential to generate\nnew patterns. The productivity of large pattern spaces is\ngenerally much higher than that of smaller sub-spaces. For\nexample, all productivity values in Table 2b, where the\nsub-patterns are constrained to the ﬁrst two beats of a bar,\nare far smaller than the respective ones in table 2a. More\ninterestingly, however, there are also large differences to\nbe found between single drums. For example, the produc-\ntivity of the snare drum as shown in table 2a is far greater\nthan that of the bass drum in the same table, suggesting\nthat snare drum patterns are used more creatively (most\nprobably due to the bass drum usually being operated by\none foot). In fact, assuming a Zipf-Mandelbrot model [6],\nwe can predict the vocabulary size as a function of corpus\nsize; Table 2 displays productivity values and the predicted\nnumber of tokens for a vocabulary size of 20 million.\n4.3 Repetition and Different Ranking Types\nSimply using the relative frequencies prfof pattern types\nis the standard way to measure word probabilities, but it\nis less informative in music because of the high amount of\nrepetition present. In our paper on chord progressions [12]\nwe suggest to use the proportion of songs a (chord) pattern\noccurs in, which we call pswhere. For example, count-\ning a token only once per song reduces the overall token\ncount from N= 4; 765;967 toNsw= 1; 264;139 for\nthe full pattern spaces. A softer way of reducing the in-\nﬂuence of repetition is motivated by the observation that\ndrum patterns in consecutive bars are often identical: one\ncan eliminate tokens that are exact repetitions of the im-\nmediately preceding token. This locally non-repeating set\nhasNloc= 3; 176;153 tokens, with relative frequencies\ndenoted byploc. We use the reduced token counts to deﬁne\nlocal and song-wise repetition indices:\nRloc= 1\u0000Nloc\nNandRsw= 1\u0000Nsw\nN: (2)\nTable 2 lists the repetition indices (in %) for different sub-\npattern corpora. Even the full patterns have a song-wise\nrepetition index Rswof 74%, meaning that only just more\nthan a quarter of the drum patterns per song are unique. A\nsimilar picture emerges when looking at local repetition,\nwhich accounts for Rloc\u001933% of all tokens. Repetition is\neven more dominant in smaller sub-patterns: Rsw= 90%\nof snare drum pattern tokens are repeated within a song,\nandRsw= 68% are repetitions of the preceding bar.\nIn Figure 3, we show the 10 most common bar-length\npatterns in the corpus. Empty patterns with different time\nsignatures occupy the 1st, 4th and 5th rank, while stan-\ndard rock patterns using only bass, snare and closed high-\nhat occupy the remaining ranks. A variation with a swing\nhigh-hat pattern appears at rank 9.1.\n2.\nbdsdhh\n3.\nbdsdhh\n4.\n5.\n6.\nbdsdhh\n7.\nbdsdhh\n8.\nbdsdhh\n9.\nbdsdhh\n10.\nbdsdhh\nFigure 3: Ranking by prfscore (Section 4.3).\nThe song-wise results are shown in Figure 4, with the\nempty patterns removed. Although some of the same pat-\nterns appear, the non-empty pattern which occurs in the\nmost songs is a crash cymbal and bass drum on the ﬁrst\nbeat of the bar, presumably at the end of a piece or sec-\ntion. Rank 3 and 7 have quarter note patterns often used\nfor “counting in” a song, while ranks 4 and 8 are the two\nsub-patterns of the rank 2 pattern—a single crash cymbal\nand a single bass drum respectively. Ranks 5 and 6 con-\ntain standard rock drum beats seen previously. The results\nwith local repetition removed (i.e. ranked by ploc) are sim-\nilar and are not shown here. Comprehensive rankings can\nbe found at http://isophonics.net/ndrum.\n4.4 Collocations and Typical Drum Patterns\nLinguists have long realised that interesting, idiomatic\nword combinations do not usually appear in the top ranks\nwhen sorted by frequency. Collocations—combinations of\ntwo words that occur more often than would be expected\nfrom their individual frequencies—are usually more inter-\nesting and meaningful. One strategy to discover colloca-\ntions is to consider two hypothetical models: H1, by which\nthe likelihood of one of the tokens to occur depends on the\nother, andH2, by which their occurrences are independent.\nOne can then calculate the likelihood ratio\nlog\u0015= logL(H 1)\nL(H 2)(3)\nof the two hypotheses for any pair of word types—or\nindeed drum pattern types. We follow Manning and2.\nbdsdhhhoricr\n3.\nbdsdhh\n4.\nbdsdhhhoricr\n5.\nbdsdhh\n6.\nbdsdhh\n7.\nbdsd\n8.\nbd\nFigure 4: Ranking by pswscore (Section 4.3).\n15.\nbdsdhh\n20.\nbdsd\n27.\nbdsdhhho\nFigure 5: Ranking by collocation score (Section 4.4).\nSch¨utze’s approach [11, Chapter 5], assuming binomial\ntype count distributions, and calculate log\u0015scores for\ncombinations of bass drum/snare drum patterns on the one\nhand and hi-hat (open and closed) patterns on the other.\nRanking by the collocation score (3) results in a list of\ntypical drum patterns that need not necessarily be frequent.\nFigure 5 shows some example of rarer patterns that never-\ntheless rank much higher than in the frequency rankings\ndiscussed in Section 4.3. For example, the typical6\n8pat-\ntern at rank 15 appears only at rank 99 in the raw frequency\nranking (and at ranks 59 and 389 when ranked by rlocand\nrsw, respectively). The3\n4pattern at rank 20, too, is much\nfurther down the frequency rankings (48, 115, 264), as is\nthe disco-style pattern at collocation rank 27 (35, 67, 171).\n4.5 Mutual Information\nThat the decomposition of drum patterns is meaningful can\nbe illustrated by the fact that the information ﬂow between\nthe sub-patterns across the corpus models musical relation-\nships between them. The entropy (in bits) of a discrete\nri\ntb\nhc\ncr\nto\npe\nhh/ho\nbd\nsdheightFigure 6: Hierarchical clustering of nine drum types by\nmutual information. The distance matrix is based on the\ninverted, normalised mutual information values (see text).\ndistributionXin with probabilities piis deﬁned as\nH(X) =\u0000NX\ni=1pilog2pi (4)\nwith the convention that if pi= 0, thenpilog2pi= 0.\nIt expresses how much information is needed in order to\nrepresent the distribution. While this is interesting in it-\nself, we are interested in how much information two drum\npattern sub-spaces XandYshare. This is what mutual\ninformation expresses. It is deﬁned as\nI(X;Y) =H(X) +H(Y)\u0000H(X;Y ): (5)\nTo normalise the measure we divide by the sum of the in-\ndividual entropies, and to turn the similarity into a measure\nof divergence we take the exponential of its negation:\nd(X;Y ) = exp\u001a\n\u0000I(X;Y)\nH(X) +H(Y)\u001b\n: (6)\nThis allows us to calculate pair-wise divergence values be-\ntween all drum types. The result is visualised in Figure 6\nas a binary tree obtained by hierarchical clustering with\nthe complete-linkage algorithm. The more information is\nshared between drums according to d, the closer they will\nappear on the tree. The algorithm has indeed recovered\naspects of the usage of the drum kit, with the drums that\nform the core of most rhythms in popular music—bass\ndrum, snare drum and hi-hat—grouped together on the\nright, loosely associated with the percussion instruments.\nWithin the remaining drums on the left hand side, the ride\ncymbals have their own branch, whereas the tambourine is\ngrouped with hand claps, and crash cymbals with tomtoms,\nwith each grouping suggesting high mutual information.\n5. DISCUSSION AND FUTURE WORK\nWe must be aware that ﬁndings made in the MIDI domain\nmay only partially be applicable to other music. Further-\nmore, the size of the database prohibits the manual veriﬁ-\ncation of every song. In addition to the measures described\nin Section 3, automatic sanity checks could further reduce\nthe noise in the data.\nWe expect that the outcomes of the present study will be\nvaluable to musicians and researchers, so we are interested\nin a rigorous evaluation of its usefulness. Several scenar-\nios are conceivable. For example, our system can easilybe extended to return a ranked list of pattern synonyms,\ni.e. patterns that are used in similar contexts as the query\npattern—a creative tool for drummers. A useful music in-\nformatics application could be to extend the promising n-\ngram technique for audio drum transcription proposed by\nPaulus, especially with models that “back off” [11, Chapter\n6] not only in time, but also in the sub-pattern instrument\nspaces presented in this paper.\n6. CONCLUSIONS\nWe have introduced a novel method of empirical research\non musical rhythm by considering bar-length drum pat-\nterns and treating them analogously to words in natural lan-\nguage processing. This paper has shown that the approach\nyields useful and interesting results because the palette of\ntools available from natural language processing can—to a\nlarge extent—be used in the musical domain, too.\nWe have found that the distributions of drum patterns\nresemble those of and English words, and have used this\nfact to predict different vocabulary growth behaviours in\nour musical corpus. V ocabulary growth predictions can be\nuseful to inform decisions on how much ground truth is\nneeded to cover a given proportion of unseen data.\nWe have discovered some properties that clearly distin-\nguish our data from language corpora, most prominently\nthe extremely high degree of repetition. A second, more\nsubtle, difference is that drum patterns can be decomposed\nin time and by instrument, yielding distributions with dif-\nferent characteristics.\nWe have proposed three simple ways of ranking drum\npatterns by raw frequency, repetition-reduced frequency,\nand song frequency. In order to identify not only fre-\nquent, but interesting drum pattern combinations, we have\napplied collocation ranking to our drum corpus. For\nmusicians, the pattern rankings, which can be found at\nhttp://isophonics.net/ndrum, may be the most interesting\naspect of this paper.\nFinally, by calculating the mutual information ﬂow be-\ntween sub-patterns pertaining to the individual drum cat-\negories, drum categories that are musically related cluster\ntogether.\nWe believe that the corpus-based study of rhythm as\nproposed in this paper is interesting not only to musi-\ncians. Musicologists and music informatics researchers\nmight ﬁnd it a valuable resource to obtain a quantitative\nview on rhythm and drum patterns.\n7. REFERENCES\n[1] R. H. Baayen. Quantitative aspects of morphological pro-\nductivity. In Geert Booij and Jaap Van Marle, editors, Year-\nbook of Morphology 1991, pages 109–149. Kluwer Aca-\ndemic Publishers, 1992.\n[2] M. S. Cuthbert. music21: A Toolkit for Computer-Aided Mu-\nsicology and Symbolic Music Data. Proc. of the 11th Int.\nConf. on Music Information Retrieval (ISMIR 2010), pages\n637–642, 2010.\n[3] M. Dean. The Drum: A History. Scarecrow Press, 2011.\n[4] S. Dixon, F. Gouyon, and G. Widmer. Towards characteri-\nsation of music via rhythmic patterns. Proc. of the 5th Int.Conf. on Music Information Retrieval (ISMIR 2004), pages\n509–516, 2004.\n[5] D. P. W. Ellis and J. Arroyo. Eigenrhythms: Drum pattern ba-\nsis sets for classiﬁcation and generation. Proc. of the 5th Int.\nConf. on Music Information Retrieval (ISMIR 2004), pages\n554–559, 2004.\n[6] S. Evert and M. Baroni. Testing the extrapolation quality of\nword frequency models. Proc. of Corpus Linguistics, 2005.\n[7] O. Gillet and G. Richard. Transcription and separation of\ndrum signals from polyphonic music. IEEE Trans. on Audio\nSpeech and Language Processing, 16(3):529–540, 2008.\n[8] D. Huron. Music Information Processing Using the Hum-\ndrum Toolkit: Concepts, Examples, and Lessons. Computer\nMusic Journal, 26(2):11–26, 2002.\n[9] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis of the\nMeter of Acoustic Musical Signals. IEEE Trans. on Audio,\nSpeech, and Language Processing, 14(1):342–355, 2006.\n[10] M. Levy. Improving Perceptual Tempo Estimation with\nCrowd-Sourced Annotations. Proc. of the 12th Int. Conf. on\nMusic Information Retrieval (ISMIR 2011), pages 317–322,\n2011.\n[11] C. D. Manning and H Sch ¨utze. Foundations of Natural Lan-\nguage Processing. MIT Press, 1999.\n[12] M. Mauch, S. Dixon, C. Harte, M. Casey, and B. Fields.\nDiscovering Chord Idioms through Beatles and Real Book\nSongs. Proc. of the 8th Int. Conf. on Music Information Re-\ntrieval (ISMIR 2007), 2007.\n[13] M. Mauch, D. M ¨ullensiefen, S. Dixon, and G. Wiggins. Can\nStatistical Language Models be Used for the Analysis of Har-\nmonic Progressions? Proc. of the 10th Int. Conf. on Music\nPerception and Cognition (ICMPC 2008), 2008.\n[14] C. McKay. Automatic Genre Classiﬁcation of MIDI Record-\nings. PhD thesis, McGill University, 2004.\n[15] Y . Murakami and M. Miura. Automatic detection system for\nﬁll-in from drum patterns employed in popular music. Proc.\nof the 10th Western Paciﬁc Acoustics Conference, 2009.\n[16] M. E. J. Newman. Power laws, Pareto distributions and Zipf’s\nlaw. Contemporary physics, 46(5):323–351, 2005.\n[17] E. Pampalk. Audio-Based Music Similarity and Retrieval :\nCombining a Spectral Similarity Model with Information Ex-\ntracted from Fluctuation Patterns. Proc. of the 7th Int. Conf.\non Music Information Retrieval (ISMIR 2006), 2006.\n[18] J. K. Paulus and A. P. Klapuri. Conventional and Periodic N-\ngrams in the Transcription of Drum Sequences. Int. Conf. on\nMultimedia and Expo (ICME 2003), 2003.\n[19] G. Peeters. Spectral and Temporal Periodicity Representa-\ntions of Rhythm for the Automatic Classiﬁcation of Music\nAudio Signal. IEEE Trans. on Audio Speech And Language\nProcessing, 19(5):1242–1252, 2011.\n[20] R. Scholz, V . Dantas, and G. Ramalho. Automating func-\ntional harmonic analysis: the Funchal system. Seventh IEEE\nInternational Symposium on Multimedia, 2005.\n[21] G. Sioros and C. Guedes. Complexity-Driven Recombination\nof MIDI Loops. Proc. of the 12th Int. Conf. on Music Infor-\nmation Retrieval (ISMIR 2011), pages 381–386, 2011.\n[22] K. Yoshii, M. Goto, and H. G. Okuno. Automatic Drum\nSound Description for Real-World Music Using Template\nAdaptation and Matching Methods. Proc. of the 5th Int. Conf.\non Music Information Retrieval, pages 184–191, 2004.\n[23] A. Zils, F. Pachet, O. Delerue, and F. Gouyon. Automatic ex-\ntraction of drum tracks from polyphonic music signals. Sec-\nond Int. Conf. on Web Delivering of Music, pages 179–183.\nIEEE Comput. Soc, 2002."
    },
    {
        "title": "Towards Time-resilient MIR Processes.",
        "author": [
            "Rudolf Mayer",
            "Andreas Rauber"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416310",
        "url": "https://doi.org/10.5281/zenodo.1416310",
        "ee": "https://zenodo.org/records/1416310/files/MayerR12.pdf",
        "abstract": "In experimental sciences, under which we may likely sub- sume most research areas in MIR, repeatability is one of the key cornerstones of validating research and measuring progress. Yet, due to the complexity of typical MIR exper- iments, ensuring the capability of re-running any experi- ment, achieving exactly identical outputs is challenging at best. Performance differences observed may be attributed to incomplete documentation of the process, slight vari- ations in data (preprocessing) or software libraries used, and others. Digital preservation aims at keeping digital objects authentically accessible and usable over long time spans. While traditionally focussed on individual objects, research is now moving towards the preservation of entire processes. In this paper we present the challenges of pre- serving a classical MIR process, i.e. music genre classi- fications, discuss the kinds of context information to be captured, as well as means to validate the re-execution of a preserved process.",
        "zenodo_id": 1416310,
        "dblp_key": "conf/ismir/MayerR12",
        "keywords": [
            "repeatability",
            "validation",
            "progress measurement",
            "complexity",
            "re-execution",
            "digital preservation",
            "authenticity",
            "context information",
            "preservation of processes",
            "music genre classifications"
        ],
        "content": "TOWARDS TIME-RESILIENT MIR PROCESSES\nRudolf Mayer and Andreas Rauber\nSecure Business Austria\nFavoritenstrasse 16, 1040 Vienna, Austria\nfmayer, rauberg@sba-research.at\nABSTRACT\nIn experimental sciences, under which we may likely sub-\nsume most research areas in MIR, repeatability is one of\nthe key cornerstones of validating research and measuring\nprogress. Yet, due to the complexity of typical MIR exper-\niments, ensuring the capability of re-running any experi-\nment, achieving exactly identical outputs is challenging at\nbest. Performance differences observed may be attributed\nto incomplete documentation of the process, slight vari-\nations in data (preprocessing) or software libraries used,\nand others. Digital preservation aims at keeping digital\nobjects authentically accessible and usable over long time\nspans. While traditionally focussed on individual objects,\nresearch is now moving towards the preservation of entire\nprocesses. In this paper we present the challenges of pre-\nserving a classical MIR process, i.e. music genre classi-\nﬁcations, discuss the kinds of context information to be\ncaptured, as well as means to validate the re-execution of a\npreserved process.\n1. INTRODUCTION\nIn many natural science disciplines, complex and data driven\nexperiments form the basis of research. Much of the re-\nsearch activities carried out in the domain of Music Infor-\nmation Retrieval can be attributed to this type of research.\nThose computationally intensive experiments trigger the\nneed for veriﬁcation of the results obtained. In many cases,\nhowever, only the publication as a ﬁnal result summarises\nthe entire scientiﬁc process, predominantly in the form of\nresults, with frequently (due to space restrictions or the\ncomplexity of the underlying process) only superﬁcial in-\nformation on the actual research and experiment process.\nIn general, the number of experimental studies in Music\nInformation Retrieval constitute a high number of MIR re-\nsearch, however, the comparability of the results is poor,\ndue to complex scenarios, user-dependent evaluation, and\nthe lack of data sharing. But even the re-evaluation and\nrepeatability of experiments is low, due to data or remote\nservices not being available, preprocessing not being docu-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.mented sufﬁciently, or code not running or libraries utilised\nhaving changed.\nA compact illustration of the experiment as it is com-\nmon in research papers is not sufﬁcient to trace the com-\nplete process of scientiﬁc research and all the sources that\ncontributed to a result. It more often than not does not pro-\nvide enough insight to allow for veriﬁcation of the results\nobtained. In many situations it is also not possible to re-\nengineer experiments reported on in the literature – impor-\ntant details such as which exact software stack and which\nversion of it were used, and which parameter settings were\napplied are often omitted or incomplete.\nOne step to mitigate this problem was the creation of\nbenchmark environments. These consist minimally of an-\nnotated ground truth data as well as evaluation measures\nand procedures, with MIREX being the most prominent\nsuch platform in the music IR community, relying on cen-\ntral evaluation. To facilitate decentralised evaluation, plat-\nforms such as those proposed by [1] and [8] have been pre-\nsented. However, none of these provide sufﬁcient docu-\nmentation of the process executed during the experiments,\nand therefore don’t allow for re-applying the process to\nnew (larger) data-sets. Publishing source the code of the al-\ngorithms used doesn’t fully alleviate this problem, as sub-\ntle details in the process conﬁguration (such as parameter\nsettings) play an important role.\nIn order to tackle this increasing complexity and the\norchestration of manifold services and systems, the con-\ncept of scientiﬁc workﬂows has received increasing atten-\ntion within the research community. E-Science projects\nproﬁt from the combination of automated processing steps\nin workﬂows in order to perform complex calculations and\ndata transformations. The advantage of workﬂows is their\ncapability of adding structure to a series of tasks. They\ncan be visualized as graph representations, where nodes\ndenote processes or tasks and edges denote information or\ndata ﬂows between the tasks. This adds a layer of abstrac-\ntion and helps to clarify interactions between tasks[3]. Dif-\nferent scientiﬁc workﬂow management systems (SWMS)\nexist that allow scientists to combine services and infras-\ntructure for their research. The most prominent examples\nof such systems are Taverna[6] and Kepler[4]. In the MIR\ndomain, M2K [2] provides a specialised workﬂow engine,\nthat allows users to combine certain MIR tasks in a se-\nquence. A similar initiative is the Networked Environment\nfor Music Analysis (NEMA) project [9], which aims at\nproviding an execution environment for evaluation of MIRsolutions.\nModelling a process in a workﬂow system alleviate many\nof the above mentioned shortcomings of insufﬁciently de-\ntailed experiments, as the exact sequence of processing\nsteps, the software used and the parameter settings become\nexplicit in the workﬂow deﬁnition language used. How-\never, while the repeatability of experiments is in princi-\nple enabled by such workﬂow management systems, many\nof todays data-intensive experiments depend on a num-\nber of services and aspects of the process beyond the con-\ntrol of the workﬂow system. These may include simple\naspects such as system updates – new libraries being de-\nployed may cause experimental results to differ. The prob-\nlems become even worse when considering external ser-\nvice such as web services. These changes are not under the\ncontrol of the researcher, and may happen at a system level\nbeyond the awareness of the individual researcher, such as\ne.g. a new library being installed as part of (automatic) sys-\ntem maintenance. This may lead to different results from\nthe workﬂow, or render the workﬂow not executable alto-\ngether. Preserving the repeatability of such a process in a\nchanging technological environment is thus a current and\nemerging topic in Digital Preservation research.\nDigital Preservation is a research discipline that tradi-\ntionally has focused on preserving mostly static digital ob-\njects, such as text or multimedia documents. Preserva-\ntion aims at keeping these digital objects accessible and\nusable over a long period of time, even when technolog-\nical change renders e.g. hardware or a speciﬁc operat-\ning system required unavailable, or ﬁle formats obsolete\nand thus not supported. More recently, digital preserva-\ntion has taken steps towards preserving more complex and\ndynamic digital objects, among them also complete pro-\ncesses. The aim is to make processes archivable and allow\nfor a later re-execution in a changed environment, while\nensuring authenticity in the process results. Digital preser-\nvation of business or E-Science processes requires captur-\ning the whole context of the process, including e.g. depen-\ndencies on other computing systems, the data consumed\nand generated, and more high-level information such as the\ngoals of the process.\nIn this paper, we will ﬁrst explore how experiments can\nbe made more repeatable, and will then examine what is\nneeded to preserve these processes over a longer period of\ntime. We do this along a case study of a musical genre\nclassiﬁcation experiment, where we highlight prototypical\naspects of digital preservation. The remainder of this paper\nis organised as follows. In Section 2, we describe the use\ncase process, for which we then outline aspects of process\npreservation in Section 3. Finally, we provide conclusions\nand an outlook on future work in Section 4.\n2. USE CASE: MUSICAL GENRE\nCLASSIFICATION\nAs an example, we consider a typical process in the MIR\nresearch community – musical genre classiﬁcation, i.e. cat-\negorisation of unknown music into one of a set of prede-\nﬁned categories. We also consider data and ground truth\nFigure 1: Musical genre classiﬁcation, including fetching\nof data, modelled in the Taverna workﬂow engine\nacquisition as part of the experiment, and assume that both\nare fetched from remote sources, e.g. a content provider\nsuch as the Free Music Archive1.\nTo simplify the implementation, we assume in this ex-\nample that the data source is a simple Apache directory\nlisting on a web-server, and the ground truth ﬁle is already\ncompiled and can be fetched from a different web resource\nvia HTTP. The experiment involves the following steps.\nFirst, the list of available music is fetched from the server,\nand each music ﬁle is downloaded from the server. For\nthis music data, the genre assignments is downloaded from\na different server. Then, a web-service is employed (via\nREST) to extract features from the audio ﬁles; the service\naccepts one ﬁle at the time. A typical example for such\na service could be the ones provided by The Echonest2\nNext, the features and the genre assignments are combined\ninto a ﬁle with the WEKA ARFF format This ﬁle and a\nset of parameters form the basis for learning a machine\nlearning model with WEKA. Finally, the classiﬁcation ac-\ncuracy, and a detailed description of the result, are obtained\nThese steps are usually carried out as a (more or less de-\nﬁned) sequence of calls to different programs via a Linux\nshell, also using some constructs built-in into this shell,\nsuch as loops and simple ﬁle and string processing.\nTo move towards more sustainable E-Science process,\nwe implement this process in the Taverna workﬂow en-\ngine [6]. Taverna is a system designed speciﬁcally to ex-\necute scientiﬁc workﬂows. It allows scientists to combine\nservices and infrastructure for modelling their workﬂows.\nServices can for example be remote web-services, invoked\nvia WSDL or REST, or local services, in the form of pre-\ndeﬁned scripts (e.g. for encoding binaries via Base64), or\nuser-deﬁned scripts. The latter are usually implemented by\nusing the Taverna-supported language beanshell, which is\nbased on the Java programming language.\nImplementing such a research workﬂow in a system like\nTaverna yields a complete and documented model of the\nexperiment process – each process step is deﬁned, as is the\n1http://freemusicarchive.org/\n2http://the.echonest.comsequence (or parallelism) of the steps. Further, Taverna re-\nquires the researcher to explicitly specify the data that is\ninput and output both of the whole process, as well as of\neach individual step. Thus, also parameter settings for spe-\nciﬁc software, such as the parameters for the classiﬁcation\nmodel or feature extraction, become explicit, either in the\nform of process input data, or in the script code.\nFigure 1 shows the generic process described above as\na speciﬁc implementation in the Taverna workﬂow engine.\nWe notice input parameters to the process such as the URL\nof the MP3 contents and the ground truth, and also an au-\nthentication voucher which is needed to authorise the use\nof the feature extraction service. The latter is a bit of infor-\nmation that is likely to be forgotten frequently in descrip-\ntions of this process, as it is rather a technical requirement\nthan an integral part of the scientiﬁc process transforma-\ntions. However, it is essential for allowing re-execution of\nthe process, and may help to identify potential licensing is-\nsues when wanting to preserve the process over longer pe-\nriods of time, requiring speciﬁc digital preservation mea-\nsures.\nThe ﬁst step is t fetch a list of available MP3s, before\neach ﬁle is downloaded individually. Before sending the\nbinary MP3 data to the web-service, it needs to be encoded\nvia base64 to allow for transport via HTTP. The feature\nextraction is then called via Taverna’s REST service inter-\nface, which requires the user to deﬁne an URL pattern for\ninvoking the service; parameters to the service become ex-\nplicit via this deﬁnition. The output of the web-service is in\ntext form. Taverna also allows using WSDL, in which case\nit can infer this information from the service description\ndirectly, and the output can be typed. Note that download-\ning and extraction are independent steps for each ﬁle, thus\nthese steps can and are automatically parallelised by Tav-\nerna. After a synchronisation point, i.e. when all MP3s are\nextracted, the features obtained for each ﬁle are merged,\ncombined with the groundtruth, and converted to WEKA\nARFF format. Finally, the classiﬁcation step is performed,\nand the accuracy measure, and a more detailed classiﬁca-\ntion report, are obtained as process outputs.\nImplementing for a workﬂow management system comes\nwith a certain effort, primarily to understanding the system\nand how process steps can be deﬁned. Another signiﬁcant\neffort can be needed to migrate existing scripts into the\nones required by the workﬂow engine, especially if certain\nfunctionality is not available in both scripting languages.\nA positive side-effect of this migration work is that the\nprocess, in principle, becomes independent from the origi-\nnal execution platform. The workﬂow system can in many\ncases act as a layer of abstraction, kind of like a virtual\nmachine, from the underlying operating system and shell\navailable there.\n2.1 Process veriﬁability with provenance data\nDuring an execution of the workﬂow, Taverna records so-\ncalled provenance data, i.e. information about the creation\nof the objects, on the data transformation happening dur-ing the experiment. Taverna stores this information in a\ndatabase, and allows to export it in the Open-Provenance\nModel (OPM) [7], or in the Janus format, and extension on\nthe OPM that describes more details.\nThe top section of Listing 1 shows an example of this\nprovenance data for the process output port ClassiﬁcationAc-\ncuracy. The ﬁrst RDF description element deﬁnes the out-\nput port and has a reference to the second RDF description\nelement, which contains the actual output value of 80.0, the\naccuracy measured in percent. The detailed classiﬁcation\nresult is then depicted in the bottom section of Listing 1.\nIt follows the same structure, i.e. the ﬁrst block deﬁning\nthe output port, and the second block containing the actual\nvalues, which in this case are a listing of the songs tested,\nand their predicted and actual values.\nSuch data is recorded for the input and output of each\nprocess step. It thus allows to trace the complete data ﬂow\nfrom the beginning of the process until the end, thus en-\nabling veriﬁcation of the results obtained. This is essen-\ntial for being able to verify system performance upon re-\nexecution, speciﬁcally when any component of the process\n(such as underlying hardware, operating systems, software\nversions, etc.) have changed.\n3. PROCESS PRESERV ATION\nWhile representing the process in the workﬂow engine in\nprinciple enables repeatability, and allows for tracing and\nthus veriﬁcation of the results, it still does not ensure the\nlongevity of the process. Our example musical genre clas-\nsiﬁcation process has several dependencies on software and\nservices that are not under direct control of the researcher.\nMost prominently, the audio feature extraction web-service\nis operated by a third party, where changes in the function-\nality, or even in the availability of the service, may not be\ncommunicated at all. These thus constitute possible points\nof failures that may cause a process execution at a later\nstage to yield different results, or not being executable at\nall any more.\nPreservation of workﬂows and processes has gained a\nlot of attention from researchers in the Digital Preserva-\ntion community recently. The goal of process preserva-\ntion is to allow re-executing the process at a later stage of\ntime, when a technological change in the environment of\nthe process has rendered the original instance of it unus-\nable. Digital preservation of business or E-Science pro-\ncesses requires capturing the whole context of the process,\nincluding e.g. different or evolved enabling technologies,\ndifferent system components on both hardware and soft-\nware levels, dependencies on other computing systems and\nservices operated by external providers, the data consumed\nand generated, and more high-level information such as the\ngoals of the process, different stakeholders and parties.\nTo enable digital preservation of business processes, it\nis therefore required to preserve the set of activities, pro-\ncesses and tools, which all together ensure continued ac-\ncess to the services and software which are necessary to\nreproduce the context within which information can be ac-\ncessed, properly rendered and validated.<r d f : D e s c r i p t i o n r d f : a b o u t =” fnsTavernag/ 2 0 1 0 / workflow /fidWFg/ p r o c e s s o r / M u s i c C l a s s i f i c a t i o n E x p e r i m e n t / o u t / C l a s s i f i c a t i o n A c c u r a c y” >\n<j a n u s : h a s v a l u e b i n d i n g r d f : r e s o u r c e =” fnsTavernag/ 2 0 1 1 / d a t a /fidDataGrpg/ r e f /fi d D a t a P o r t 0g”/>\n<r d f s : comment r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# s t r i n g ” > C l a s s i f i c a t i o n A c c u r a c y </ r d f s : comment >\n<j a n u s : i s p r o c e s s o r i n p u t r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# b o o l e a n” > f a l s e </ j a n u s : i s p r o c e s s o r i n p u t>\n<j a n u s : h a s p o r t o r d e r r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# l on g” > 0</ j a n u s : h a s p o r t o r d e r>\n<r d f : t y p e r d f : r e s o u r c e =” h t t p : / / p u r l . org / n e t / t a v e r n a / j a n u s # p o r t ”/ >\n</ r d f : D e s c r i p t i o n >\n<r d f : D e s c r i p t i o n r d f : a b o u t =” fnsTavernag/ 2 0 1 1 / d a t a /fidDataGrpg/ r e f /fi d D a t a P o r t 0g”>\n<r d f s : comment r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# s t r i n g ” > 8 0 . 0</ r d f s : comment >\n<j a n u s : h a s p o r t v a l u e o r d e r r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# l on g” > 1</ j a n u s : h a s p o r t v a l u e o r d e r>\n<j a n u s : h a s i t e r a t i o n r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# s t r i n g ” > [ ]</ j a n u s : h a s i t e r a t i o n >\n<r d f : t y p e r d f : r e s o u r c e =” h t t p : / / p u r l . org / n e t / t a v e r n a / j a n u s # p o r t v a l u e ”/ >\n</ r d f : D e s c r i p t i o n >\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n<r d f : D e s c r i p t i o n r d f : a b o u t =” fnsTavernag/ 2 0 1 0 / workflow /fidWFg/ p r o c e s s o r / M u s i c C l a s s i f i c a t i o n E x p e r i m e n t / o u t / D e t a i l e d C l a s s i f i c a t i o n R e s u l t s ” >\n<j a n u s : h a s v a l u e b i n d i n g r d f : r e s o u r c e =” fnsTavernag/ 2 0 1 1 / d a t a /fidDataGrpg/ r e f /fi d D a t a P o r t 1g”/>\n. . .\n</ r d f : D e s c r i p t i o n >\n<r d f : D e s c r i p t i o n r d f : a b o u t =” fnsTavernag/ 2 0 1 1 / d a t a /fidDataGrpg/ r e f /fi d D a t a P o r t 1g”>\n<r d f s : comment r d f : d a t a t y p e =” fnsW3g/ 2 0 0 1 / XMLSchema# s t r i n g ” >\n1 2 : Hip\u0000Hop 2 : Hip\u0000Hop 0 . 6 6 7 ( 3 . 3 5 9 4 6 1 )\n2 2 : Hip\u0000Hop 2 : Hip\u0000Hop 0 . 6 6 7 ( 3 . 2 9 4 6 8 7 )\n3 1 : C l a s s i c a 1 : C l a s s i c a 0 . 6 6 7 ( 2 . 0 3 2 6 8 7 )\n4 3 : J a z z 3 : J a z z 0 . 6 6 7 ( 2 . 5 3 6 8 4 9 )\n5 1 : C l a s s i c a 1 : C l a s s i c a 0 . 6 6 7 ( 1 . 3 1 7 2 7 )\n6 1 : C l a s s i c a 3 : J a z z + 0 . 6 6 7 ( 3 . 4 6 7 7 1 )\n7 3 : J a z z 1 : C l a s s i c a + 0 . 3 3 3 ( 2 . 1 5 9 7 6 4 )\n8 2 : Hip\u0000Hop 2 : Hip\u0000Hop 0 . 6 6 7 ( 3 . 1 2 7 6 4 5 )\n9 3 : J a z z 3 : J a z z 0 . 6 6 7 ( 3 . 0 1 0 5 6 3 )\n10 2 : Hip\u0000Hop 2 : Hip\u0000Hop 0 . 6 6 7 ( 4 . 6 3 1 3 1 6 )\n</ r d f s : comment >\n</ r d f : D e s c r i p t i o n >\nListing 1: Provenance data recorded by Taverna for the process outputs (cf. Figure 1). The ﬁrst RDF Description element deﬁnes the output Classiﬁca-\ntionAccuracy, the second element contains the actual value “80.0”. The third element deﬁnes the output DetailedClassiﬁcationResults, the fourth element\ncontains the actual value, one entry for each ﬁle tested, with the actual class and predicted class. Some identiﬁers have been abbreviated, marked by f...g\n(a)\n (b)\nFigure 2: Sections of the Context Model\nTo address these challenges, we have devised a context\nmodel to systematically capture aspects of a process that\nare essential for its preservation and veriﬁcation upon later\nre-execution. The model consists of approximately 240 el-\nements, structured in around 25 major groups. The model\nis implemented in the form of an ontology, which on the\none hand allows for the hierarchical categorisation of as-\npects, and on the other hand shall enable reasoning, e.g.\nover the possibility of certain preservation actions for a\nspeciﬁc process instance. The ontology is authored in the\nWeb Ontology Language (OWL). We developed a set of\nplug-ins for the Prot ´eg´e ontology editor to support easier\nworking with the model.\nThis context model corresponds to some degree to the\nrepresentation information network [5], modelling the re-\nlationships between an information object and its related\nobjects, be it documentation of the object, constituent parts\nand other information required to interpret the object. This\nis extended to understand the entire context within which a\nprocess, potentially including human actors, is executed,\nforming a graph of all constituent elements and, recur-sively, their representation information. Two sections of\nthis model are depicted in Figure 2. Each item represents\na class of aspects, for which a speciﬁc instance of the con-\ntext model then creates concrete members, which are then\nrelated to each other with properties.\nFigure 2(a) details aspects on software and speciﬁca-\ntions. Technical dependencies on software and operating\nsystems can be captured and described for example via\nCUDF (Common Upgradeability Description Format3) for\nsystems which are based on packages, i.e. where there is a\npackage universe (repositories) and a package manager ap-\nplication. Such an approach allows to capture the complete\nsoftware setup of a speciﬁc conﬁguration, which then can\nbe recreated. Related to the software installed, capturing\ninformation on the licences associated to them allows for\nverifying which preservation actions are permissible for a\nspeciﬁc scenario. Software and/or its requirements are for-\nmally described in speciﬁcation documents. Speciﬁc doc-\numents for a process are created as instances of the appro-\npriate class, and related to the software components they\ndescribe. Conﬁguration, also depicted in Figure 2(a), is\nanother important aspect, closely related to software (and\nhardware). Maybe even more than the speciﬁc version of a\nsoftware utilised might inﬂuence the process outcome, can\nthe speciﬁc conﬁguration applied alter the behaviour of an\noperating system or software component. Capturing this\nconﬁguration might not always be easy, but in systems that\nrely on packages for their software, these packages tend to\nprovide information about default locations for conﬁgura-\ntion ﬁles, which might be a start for capturing tools.\nAnother important aspect of the context model deals\nwith several types of data consumed and created by a pro-\n3http://www.mancoosi.org/cudf/cess, as seen in a section of Figure 2(b). We distinguish be-\ntween data that originates from hardware or software, and\nwhether this data is input to or output of the process, or\ncreated and consumed inside the process, i.e. output from\none process step and input for another. Capturing this data\nis an important aspect in verifying that a re-execution of a\nprocess yields the same results as the original process, as\nwe detailed in Section 2. It may be easily captured if the\nprocess is formally deﬁned in a workﬂow engine, and this\nengine provides provenance data, as it is the case with Tav-\nerna. In other cases, it may be more difﬁcult to obtain, e.g.\nby observing network trafﬁc or system library calls.\nOther aspects of the model cover for example human\nresources (including e.g. required qualiﬁcations for a cer-\ntain role), actors, or legal aspects such as data protection\nlaws. Location and time-based aspects need to be captured\nfor processes where synchronisation between activities is\nimportant. Further important aspects are documentation\nand speciﬁcations, on all different levels, from high-level\ndesign documents of the process, use-case speciﬁcations,\ndown to test documents, etc.\nWhile the model is very extensive, it should be noted\nthat a number of aspects can be ﬁlled automatically – es-\npecially if institutions have well-deﬁned and documented\nprocesses. Also, not all sections of the model are equally\nimportant for each type of process. Therefore, not every\naspect has to be described in most detail.\n3.1 Context of the MIR process\nWe modelled the scientiﬁc experiment in the above pre-\nsented context model. Figure 3 gives an overview on the\nconcrete instances and their relations identiﬁed as relevant\naspects of the process context.\nAs this experiment, as most experiments in the MIR do-\nmain, is a process mostly focusing on data processing, the\nmajority of the identiﬁed aspects are in the technical do-\nmain – software components, external systems such as the\nweb service to extract the numerical audio features from,\nor data exchanged and their format and speciﬁcation. How-\never, also goals and motivations are important aspects, as\nthey might heavily inﬂuence the process. As such, the mo-\ntivation for the providers of the external systems is rele-\nvant, as it might determine the future availability of these\nservices. Commercial systems might be more likely to sus-\ntain than services operated by a single person for free.\nAnother important aspect in this process are licences –\ndepending on which licence terms the components of our\nprocess are released under, different options of preserva-\ntion actions might be available or not. For closed-source,\nproprietary software, migration to a new execution plat-\nform might be prohibited.\nA central aspect in the scientiﬁc process is the AudioFea-\ntureExtractionService, i.e. the remote web-service that pro-\nvides the numeric representation for audio ﬁles. The ser-\nvice needs as input ﬁles encoded in the MP3 format (spec-\niﬁed by the ISO standard 11172-3). More speciﬁcally, as\nthey are binary ﬁles, they need to be further encoded with\nBase64, to allow for data exchange over the HTTP proto-col. The web-service further accepts a number of parame-\nters that control the exact information captured in the nu-\nmeric representation; they are speciﬁed in the AudioFea-\ntureExtractionSpeciﬁcation, which for example also cov-\ners a detailed information on how the extraction works.\nThe service requires an authorisation key. The operator\nof the web-service provides the service for free, but grants\nauthorisation keys that are non-transferable between dif-\nferent researchers. Finally, the feature extraction service\nprovides the numeric description as ASCII ﬁle, following\nthe SOMLib format speciﬁcation.\nAs a software component used locally, the WEKA ma-\nchine learning toolkit requires a Java Virtual Machine (JVM)\nplatform to execute. The JVM in turn is available for many\noperating systems, but has been speciﬁcally tested on a\nLinux distribution, Ubuntu “Oneiric” 11.04. WEKA re-\nquires as input a feature vector in the ARFF Format, and a\nset of parameters controlling the learning algorithm. These\nparameters are speciﬁed in the WEKA documentation. As\noutput result, the numeric performance metric “accuracy”\nis provided, as well as a textual, detailed description of the\nresult. WEKA is distributed under the terms of the open-\nsource GNU Public License (GPL) 2.0, which allows for\nsource code modiﬁcations.\nAfter this experimentation process, a subsequent pro-\ncess of result analysis and distillation is normally performed,\ntaking input from the experiment outcomes, and ﬁnally\nleading to a publication of the research in the form of e.g. a\nconference or journal paper. This, again, may be modelled\neither as a single information object (the paper) connected\nto the process, and thus to all data and processing steps that\nled to the results published, or as a more complex process\nin its own, speciﬁcally if a paper reports on meta-studies\nacross several experiment runs.\n3.2 Preservation Actions and Evaluation\nPreservation Actions are executed to regain or improve ac-\ncess to digital information. For process preservation, preser-\nvation actions could be cross compilation of software mod-\nules, to enable to run the process on a different platform,\nor code migration if the former is not (easily) possible.\nAlso the emulation of hardware or software utilised in the\nprocess might be a viable option. Further preservation ac-\ntions include the (ﬁle format) migration of speciﬁcations\nand documents. For external services such as web-service\ndigital preservation approaches still need to be developed.\nFor once, web-services should allow the user to query for\na version, to identify whether something has changed. To\nensure process continuity if a service has indeed changed\nor disappeared, re-implementing the service is only a vi-\nable option if the speciﬁcation is known. In other cases,\ncapturing provenance data as described in Section 2.1 al-\nlows to create mock-up service that can replay previously\nrecorded process executions.\nEvaluation of the process is enabled by comparing the\nprovenance data recorded during the original execution (cf.\nSection 2.1) with the one recorded from a modiﬁed pro-\ncess.Figure 3: Context Model of musical genre classiﬁcation process\n4. CONCLUSIONS\nThere is an urgent need to move towards more sustain-\nable process in the Music Information Retrieval domain.\nPrinciples of experimental science and traditions are well-\nestablished in other disciplines (biology, chemistry, crys-\ntallography). This is very complex to achieve in MIR,\nwhere legal issues associated with the data analysed are\na signiﬁcant obstacle, but more speciﬁcally, fast-changing\ntechnology has a huge impact. In this paper, we have pre-\nsented approaches from the Digital Preservation domain\nfor preserving processes, so that a later execution is en-\nabled. We discussed on the example of a typical musical\ngenre classiﬁcation process how this can be applied to MIR\ntasks. Future work will focus on an integration of digital\npreservation methods into benchmark environments such\nas the ones proposed by [1] and [8], and evaluation cam-\npaigns such as MIREX, forming research infrastructures\nfor MIR research.\n5. ACKNOWLEDGMENTS\nPart of this work was supported by the project TIMBUS,\npartially funded by the EU under the FP7 contract 269940.\n6. REFERENCES\n[1] Timothy G. Armstrong, Alistair Moffat, William Web-\nber, and Justin Zobel. Improvements that don’t add up:\nad-hoc retrieval results since 1998. In Proceedings of\nthe ACM Conference on Information and Knowledge\nManagement, CIKM ’09, New York, USA, 2009.\n[2] J. Stephen Downie, Joe Futrelle, and David K. Tcheng.\nThe international music information retrieval systems\nevaluation laboratory: Governance, access and secu-\nrity. In Proceedings of the International Conference on\nMusic Information Retrieval, Barcelona, Spain, 2004.[3] Juliana Freire, David Koop, Emanuele Santos, and\nCl´audio T. Silva. Provenance for computational tasks:\nA survey. Computing in Science and Engineering,\n10(3):11–21, May 2008.\n[4] Bertram Lud ¨ascher, Ilkay Altintas, Chad Berkley, Dan\nHiggins, Efrat Jaeger, Matthew Jones, Edward A. Lee,\nJing Tao, and Yang Zhao. Scientiﬁc workﬂow manage-\nment and the Kepler system. Concurrency and Compu-\ntation: Practice and Experience, 18(10):1039–1065,\n2006.\n[5] Yannis Marketakis and Yannis Tzitzikas. Dependency\nmanagement for digital preservation using semantic\nweb technologies. International Journal on Digital Li-\nbraries, 10:159–177, 2009.\n[6] Paolo Missier, Stian Soiland-Reyes, Stuart Owen, Wei\nTan, Alexandra Nenadic, Ian Dunlop, Alan Williams,\nTom Oinn, and Carole Goble. Taverna, reloaded.\nInProceedings of the 22nd international conference\non Scientiﬁc and Statistical Database Management,\nBerlin, Heidelberg, June 2010. Springer-Verlag.\n[7] Luc Moreau, Juliana Freire, Joe Futrelle, Robert E.\nMcgrath, Jim Myers, and Patrick Paulson. Provenance\nand annotation of data and processes. chapter The\nOpen Provenance Model: An Overview, pages 323–\n326. Springer-Verlag, Berlin, Heidelberg, 2008.\n[8] Juli ´an Urbano. Information retrieval meta-evaluation:\nChallenges and opportunities in the music domain. In\nProceedings of the International Society for Music In-\nformation Retrieval Conference, Miami, USA, 2011.\n[9] Kris West, Amit Kumar, Andrew Shirk, Guojun Zhu,\nJ. Stephen Downie, Andreas F. Ehmann, and Mert\nBay. The Networked Environment for Music Analysis\n(NEMA). In 6th World Congress on Services, pages\n314–317, Miami, USA, July 5-10 2010."
    },
    {
        "title": "Hypergraph Models of Playlist Dialects.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415618",
        "url": "https://doi.org/10.5281/zenodo.1415618",
        "ee": "https://zenodo.org/records/1415618/files/McFeeL12.pdf",
        "abstract": "Playlist generation is an important task in music informa- tion retrieval. While previous work has treated a playlist collection as an undifferentiated whole, we propose to build playlist models which are tuned to specific categories or dialects of playlists. Toward this end, we develop a general class of flexible and scalable playlist models based upon hypergraph random walks. To evaluate the proposed mod- els, we present a large corpus of categorically annotated, user-generated playlists. Experimental results indicate that category-specific models can provide substantial improve- ments in accuracy over global playlist models.",
        "zenodo_id": 1415618,
        "dblp_key": "conf/ismir/McFeeL12",
        "keywords": [
            "Playlist generation",
            "music information retrieval",
            "playlist models",
            "hypergraph random walks",
            "categorically annotated playlists",
            "user-generated playlists",
            "category-specific models",
            "global playlist models",
            "substantial improvements",
            "accuracy"
        ],
        "content": "HYPERGRAPH MODELS OF PLAYLIST DIALECTS\nBrian McFee\nComputer Science and Engineering\nUniversity of California, San DiegoGert Lanckriet\nElectrical and Computer Engineering\nUniversity of California, San Diego\nABSTRACT\nPlaylist generation is an important task in music informa-\ntion retrieval. While previous work has treated a playlist\ncollection as an undifferentiated whole, we propose to build\nplaylist models which are tuned to speciﬁc categories or\ndialects of playlists. Toward this end, we develop a general\nclass of ﬂexible and scalable playlist models based upon\nhypergraph random walks. To evaluate the proposed mod-\nels, we present a large corpus of categorically annotated,\nuser-generated playlists. Experimental results indicate that\ncategory-speciﬁc models can provide substantial improve-\nments in accuracy over global playlist models.\n1. INTRODUCTION\nPlaylist generation, the automated construction of sequences\nof songs, is a central component to online music delivery\nservices. Because users tend to consume music sequentially\nin listening sessions, the quality of a playlist generation\nalgorithm can signiﬁcantly impact user satisfaction.\nRecently, it has been proposed that playlist generation\nalgorithms may be best viewed as probabilistic models of\nsong sequences [11]. This viewpoint, borrowed from the\nstatistical natural language processing literature, enables\nthe automatic evaluation and optimization of a model by\ncomputing the likelihood of it generating examples of user-\ngenerated playlists. For this method to work, the practi-\ntioner must provide a large collection of example playlists,\nboth for model evaluation and parameter optimization.\nOf course, numerous subtleties and difﬁculties arise\nwhen working with user-generated playlist data. For ex-\nample, the data is often noisy, and the author’s intent may\nbe obscure. In extreme cases, users may compose playlists\nby randomly selecting songs from their libraries. More\ngenerally, different playlists may have different intended\nuses (e.g., road trip orparty mix ), thematic elements (break\nuporromantic), or simply contain songs only of speciﬁc\ngenres. While previous work treats the universe of user-\ngenerated playlists as a single language, building effective\nglobal models has proven to be difﬁcult [11].\nTo better understand the structure of playlists, we advo-\ncate a more subtle approach. Rather than viewing naturally\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.occurring playlists as a single language, we propose to\nmodel playlists as a collection of dialects, each of which\nmay exhibit its own particular structure. Toward this end,\nwe develop dialect-speciﬁc playlist models, and evaluate on\na large corpus of annotated, user-generated playlists.\nThe proposed approach raises several natural questions:\n\u000fIs it beneﬁcial to individually model playlist dialects?\n\u000fAre some dialects easier to model than others?\n\u000fWhich features are important for each dialect?\nAnswering these questions will hopefully provide valuable\ninsight into the underlying mechanics of playlist generation.\n1.1 Our contributions\nIn this work, our contributions are two-fold. First, we de-\nvelop a ﬂexible, scalable, and efﬁcient class of generative\nplaylist models based upon hypergraph random walks. Sec-\nond, we present a new, large-scale, categorically annotated\ncorpus of user-generated playlist data.\n2. HYPERGRAPH RANDOM WALKS\nOver the last decade, several researchers have proposed\nplaylist generation algorithms based upon random walks [9,\n11,12].1Random walk playlist models consist of a weighted\ngraphG= (X;E;w ), where the vertices Xrepresent the\nlibrary of songs, and the edges Eand weights wencode\npairwise afﬁnities between songs. A playlist is then gener-\nated by following a random trajectory through the graph,\nwhere transitions xt xt+1are sampled according to the\nweights on edges incident to xt.\nRandom walk models, while simple and efﬁcient, carry\ncertain practical limitations. It is often unclear how to deﬁne\nthe weights, especially when multiple sources of pairwise\nafﬁnity are available. Moreover, relying on pairwise inter-\nactions can severely limit the expressive power of these\nmodels (if each song has few neighbors), or scalability and\nprecision (if each song has many neighbors).\nTo overcome these limitations, we propose a new class of\nplaylist algorithms which allow for more ﬂexible afﬁnities\nbetween songs and sets of songs.\n2.1 The user model\nTo motivate our playlist generation algorithm, we propose a\nsimple model of user behavior. Rather than selecting songs\n1There are many approaches beyond random walk models; see [5,\nchapter 2] for a survey of recent work.YEAR_1977\nJazz\nAUDIO-5/16Figure 1 . An example random walk on a song hypergraph:\nvertices represent songs, and edges are subsets of songs.\nEach transition xt xt+1must lie within an edge.\ndirectly from the entire collection X, we assume that the\nuser ﬁrst narrows her selection to a subset e\u0012X (e.g., jazz\nsongs), from which a song x02eis chosen uniformly at\nrandom. For each subsequent transition xt xt+1, the user\nselects a subset containing the current song xt, and then\nselectsxt+1uniformly from that subset.\nThis user model is exactly characterized by a random\nwalk on a hypergraph. Hypergraphs generalize undirected\ngraphs by allowing an edge e2Eto be an arbitrary subset\nof the vertices, rather than a pair ( Figure 1 ). For example,\na hypergraph edge may be as general as jazz songs, or as\nspeciﬁc as funk songs from 1977. Edge weights can be\nused to encode the importance of a subset: for example,\na model of jazzplaylists would assign high weight to an\nedges containing jazzsongs.\nThis model has several practically beneﬁcial properties.\nFirst, it is efﬁcient and scalable, in that the only information\nnecessary to describe a song is its membership in the edge\nsets. Similarly, it naturally supports extension to new songs\nwithout having to signiﬁcantly alter the model parameters\n(edge weights). Second, the model can easily integrate dis-\nparate feature sources, such as audio descriptors, lyrics, tags,\netc, as long as they can be encoded as subsets. Moreover,\nthe model degrades gracefully if a song only has partial\nrepresentation (e.g., audio but no lyrics or tags). Finally,\nthe model is transparent, in that each transition can be ex-\nplained to the user simply in terms of the underlying edge\ntaken between songs. As we will see in Section 3 , these\nedges often have natural semantic descriptions.\n2.2 The playlist model\nTo formalize our model, let H= (X;E;w )denote a hy-\npergraph over vertices (songs) X, edgesE\u00122X, and\nnon-negative weights w2RjEj\n+. We assume that the song\nlibraryXand edge set Eare given, and our goal is to op-\ntimize the edge weights w. We denote by xe\nt:=1[xt2e]\nthe indicator that the song xtis contained in the edge e.\nBecause the selection of the next song xt+1depends only\non the previous song xtand edge weights w, the model is\na ﬁrst-order Markov process. The likelihood of a playlist\ns= (x 0 x 1 \u0001\u0001\u0001 xT)thus factors into likelihood of\nthe initial song, and each subsequent transition:\nP(x 0 x 1 \u0001\u0001\u0001 xTjw) =P(x 0jw)T\u00001Y\nt=0P(xt+1jxt;w):Given the edge weights w, the distribution over the initial\nsongx0can be characterized by marginalizing over edges:\nP(x 0jw) :=X\ne2EP(x 0je)P(ejw) =X\ne2Exe\nt\njejweP\nf2Ewf:\nSimilarly, the probability of a transition xt xt+1is deﬁned\nby marginalizing over edges incident to xt:\nP(xt+1jxt;w) :=X\ne2EP(xt+1je;xt)P(ejxt;w)\n=X\ne2E1[xt+16=xt]\u0001xe\nt+1\njej\u0000 1\u0001xe\ntweX\nf2Exf\ntwf:\nFinally, to promote sparsity among the edge weights and\nresolve scale-invariance in the model, we assume an IID\nexponential prior on edge weights wewith rate\u0015>0:\nP(w e) :=\u0015\u0001exp (\u0000\u0015w e)\u00011[we2R+]:\n2.3 Learning the weights\nGiven a training sample of playlists S\u001aX\u0003,2we would\nlike to ﬁnd the maximum a posteriori (MAP) estimate of w:\nw argmax\nw2RjEj\n+logP(wjS)\n= argmax\nw2RjEj\n+X\ns2SlogP(sjw) +X\ne2ElogP(w e): (1)\nThe MAP objective (1)is not concave, and it is generally\ndifﬁcult to ﬁnd a global optimum. Our implementation uses\nthe L-BFGS-B algorithm [2] to solve for w, and converges\nquite rapidly to a stationary point. Training typically takes\na matter of seconds, even for the large playlist collections\nand edge sets described in Section 3.\n3. DATA COLLECTION\nPrevious work on playlist modeling used the Art of the\nMix3(AotM) collection of Ellis, et al. [4]. The existing\nAotM dataset was collected in 2002, and consists of roughly\n29K playlists over 218K songs, provided as lists of plain-\ntext song and artist names. In this work, we expand and\nenrich this dataset into a new collection, which we denote\nas AotM-2011.4This section describes our data collection,\npre-processing, and feature extraction methodology.\n3.1 Playlists: Art of the Mix 2011\nTo expand the AotM playlist collection, we crawled the site\nfor all playlists, starting from the ﬁrst indexed playlist (1998-\n01-22) up to the most recent at the time of collection (2011-\n06-17), resulting in 101343 unique playlists. Each playlist\ncontains not only track and artist names, but a timestamp\nand categorical label (e.g., Road Trip orReggae).\n2X\u0003denotes the Kleene star operation.\n3http://www.artofthemix.org\n4http://cosmal.ucsd.edu/cal/projects/aotm2011/.To effectively model the playlist data, the plain-text song\nand artist names must be resolved into a common names-\npace. Following previous work, we use the Million Song\nDataset (MSD) as the underlying database [1, 11]. Rather\nthan rely on the Echo Nest text-search API to resolve song\nidentiﬁers, we instead implemented a full-text index of\nMSD song and artist names in Python with the Whoosh5\nlibrary. This allowed both high throughput and ﬁne-grained\ncontrol over accent-folding and spelling correction. Each\n(artist, song) pair in the raw playlist data was used as a\nquery to the index, and resolved to the corresponding MSD\nsong identiﬁer (if one was found). In total, 98359 songs\nwere matched to unique identiﬁers.\nBecause not every song in a raw playlist could be cor-\nrectly resolved, each playlist was broken into contiguous\nsegments of two or more matched song identiﬁers. Fi-\nnally, playlist segments were grouped according to category.\nTable 1 lists each of the 25 most popular categories by size.\n3.2 Edge features\nTo fully specify the playlist model, we must deﬁne the edges\nof the hypergraph. Because edges can be arbitrary subsets\nof songs, the model is able to seamlessly integrate disparate\nfeature modalities. We use the following collection of edge\nfeatures, which can be derived from MSD and its add-ons.\nAudio To encode low-level acoustic similarity, we ﬁrst\nmapped each song ito a vectorxi2R222using the\noptimized vector quantized Echo Nest Timbre (ENT)\ndescriptors provided by [1, 11]. Audio descriptors\nwere clustered via online k-means, and cluster assign-\nments were used to produce kdisjoint subsets. Re-\npeating this for k2f16; 64;256g provided multiple\noverlapping edges of varying degrees of granularity.\nAll 98K songs receive audio representations.\nCollaborative ﬁlter To capture high-level similarities due\nto user listening patterns, we construct edges from\nthe taste proﬁle data used in the MSD Challenge [10].\nWe used the Bayesian Personalized Ranking (BPR)\nalgorithm [6, 13] to factor the users-by-songs (1M-\nby-380K) feedback matrix into latent feature vectors\nxi2R32. The BPR regularization parameters were\nset to\u00151=\u00152= 10\u00004. Edges were constructed\nby cluster assignments following the procedure de-\nscribed above for audio features. 62272 songs (63%)\ncoincide with the taste proﬁle data.\nEra The era in which songs are released can play an impor-\ntant role in playlist composition [3, 8]. To model this,\nwe use the MSD meta-data to represent each song\nby its year and half-overlapping decades. For exam-\nple, the song Parliament - Flash Light maps to edges\nYEAR-1977 ,DECADE-1970 andDECADE-1975 .\n77884 songs (79%) were mapped to era descriptors.\nFamiliarity Previous studies have noted the importance of\nsong- or artist-familiarity when composing playlists [3,\n5http://packages.python.org/Whoosh/11]. We used the artist familiarity data provided\nwith MSD, which maps each song to the range [0;1]\n(0 being unfamiliar, 1 being very familiar). Edges\nwere constructed by estimating the 25th and 75th\npercentiles of familiarity, and mapping each song to\nLOW, MEDIUM, or HIGH familiarity.\nLyrics Previous studies have shown the importance of\nlyrics in playlist composition [8]. To compute lyrical\nsimilarity, we applied online latent Dirichlet allo-\ncation (LDA) [7] with k= 32 to the musiXmatch\nlyrics database.6We then constructed three sets of\n32 edges (one edge per topic): the ﬁrst matches each\nsong to its most probable topic, the second matches\neach song to its top three topics, and the third set to\nits top ﬁve topics. 53351 songs (56%) were found in\nthe musiXmatch data.\nSocial tags Previous work incorporated semantic informa-\ntion by using the total similarity between bag-of-tags\nvectors of songs to determine similarity [11]. Here,\nwe take a more ﬂexible approach, and model each\ntag separately. Using the Last.fm7tags for MSD,\nwe match each song to its top-10 most frequent tags.\nEach tag induces an edge (the songs assigned to that\ntag).880396 songs (82%) matched to tag edges.\nUniform shufﬂe Because the features described above can-\nnot model all possible transitions, we include a uniform\nedge that contains all songs. A transition through the\nuniform edge can be interpreted as a random restart\nof the playlist. The uniform shufﬂe also provides a\nstandard baseline for comparison.\nFeature conjunctions Some of the features described above\nmay be quite weak individually, but when combined,\nbecome highly descriptive. For example, the tag\nrock and era YEAR-1955 are both vague, but the\nconjunction of these two descriptors — rock-&-\n-YEAR-1955 — retains semantic interpretability,\nand is much more precise. We therefore augment\nthe above collection of edges with all pair-wise inter-\nsections of features. Note that this induces general\ncross-modal feature conjunctions, such as Lyrics\ntopic #4-&-Audio cluster #17 , resulting\nin an extremely rich set of song descriptors.\n4. EXPERIMENTS\nTo evaluate the proposed method, we randomly partitioned\neach of the top-25 categories listed in Table 1 into ten 75/25\ntrain/test splits. For each split, the train (test) sets are col-\nlected across categories to form a global train (test) set\nALL, which is used to train a global model. After ﬁtting a\n6http://labrosa.ee.columbia.edu/millionsong/\nmusixmatch\n7http://last.fm/\n8A similar tag-hypergraph model was proposed by Wang, et al. [14].Category Playlists Segments Songs Category Playlists Segments Songs\nMixed 41798 101163 64766 Sleep 675 1487 2957\nTheme 12813 31609 35862 Electronic Music 611 1131 2290\nRock-Pop 4935 13661 20364 Dance-House 526 1117 2375\nAlternating DJ 4334 10493 18083 Rhythm and Blues 432 1109 2255\nIndie 4528 10333 13678 Country 398 908 1756\nSingle Artist 3717 9044 17715 Cover 447 833 1384\nRomantic 2523 6269 8873 Hardcore 268 633 1602\nRoad Trip 1846 4817 8935 Rock 215 565 1866\nPunk 1167 3139 4936 Jazz 295 512 1089\nDepression 1128 2625 4794 Folk 241 463 1137\nBreak Up 1031 2512 4692 Reggae 183 403 831\nNarrative 964 2328 5475 Blues 165 373 892\nHip Hop 1070 1958 2505 Top-25 86310 209485 97411\nTable 1 . The distribution of the top 25 playlist categories in AotM-2011. Each playlist consists of one or more segments of\nat least two contiguous MSD songs. 948 songs do not appear within the top 25 categories, but are included in the model.\nFeature # Edges Feature # Edges\nAudio 204 Collaborative ﬁlter 93\nEra 56 Familiarity 3\nLyrics 82 Tags 201\nUniform 1 All features 640\nFeature conjunctions 6390\nTable 2. Summary of edges after pruning.\nmodel to each training set, we compute the average (length-\nnormalized) log-likelihood of the test set S0:\nL(S0jw) :=1\njS0jX\ns2S01\njsjlogP(sjw):\nFor comparison purposes, we report performance in terms\nof the relative gain over the uniform shufﬂe model wu(all\nweight assigned to the uniform edge):\nG(w) := 1\u0000L(S0jw)\nL(S0jwu):\nTo simplify the model and reduce over-ﬁtting effects, we\npruned all edges containing fewer than 384 (98359/256)\nsongs. Similarly, we pruned redundant conjunction edges\nthat overlapped by more than 50% with either of their con-\nstituent edges. Table 2 lists the number of edges retained\nafter pruning. On average, each song maps to 76:46\u000657:79\nedges, with a maximum of 218. In all experiments, we ﬁx\nthe prior parameter \u0015= 1.\n4.1 Experiment 1: Does dialect matter?\nIn the ﬁrst set of experiments, we compare the global model\nto category-speciﬁc models. Figure 2 illustrates the relative\ngain over uniform across all categories for four different\nmodel conﬁgurations: tags, tags with pairwise conjunctions,\nall features, and all features with conjunctions.\nSeveral interesting trends can be observed from Figure 2 .\nFirst, in all but two cases — Narrative andRock under the\nall features with conjunctions model — category-speciﬁc\nmodels perform at least as well as the global model, and\nare often substantially better. As should be expected, the\neffect is most pronounced for genre-speciﬁc categories that\nnaturally align with semantic tags (e.g., Hip Hop orPunk ).\nNote that the larger categories overlap more with ALL,\nleaving less room for improvement over the global model.Not surprisingly, the Mixed category appears to be difﬁcult\nto model with similarity-based features. The fact that it is\nthe single largest category ( Table 1 ) may explain some of\nthe difﬁculties observed in previous studies when using a\nglobal model [11]. Similarly, several other categories are\nquite broad (Theme, Narrative, Rock ), or may be inherently\ndifﬁcult (Alternating DJ, Mixed ).\nAlso of note are differences across model conﬁgurations.\nFeature conjunctions generally provide a modest improve-\nment, both in the global and category-speciﬁc models. Due\nto the large parameter space, some over-ﬁtting effects can\nbe observed in the smallest categories (Folk, Reggae, Blues).\nInterestingly, several categories beneﬁt substantially from\nthe inclusion of all features compared to only tags (e.g., Hip\nHop, Punk, Jazz).\n4.2 Experiment 2: Do transitions matter?\nGiven the ﬂexibility of the model, it is natural to question\nthe importance of modeling playlist continuity: could a\nmodel which ignores transition effects perform as well as\nthe random walk model? To test this, we split each playlist\ns= (x 0 x 1 \u0001\u0001\u0001 xT)into singletons s0= (x0),\u0001\u0001\u0001,\nsT= (xT). With this modiﬁed corpus, the model treats\neach song in a playlist as an independent draw from the\ninitial song distribution P(x 0jw). Consequently, a model\ntrained on this corpus can ﬁt global trends across playlists\nwithin a category, but cannot enforce local continuity.\nFigure 3 illustrates the relative gain for each category\nunder the stationary distribution with all features and con-\njunctions. The results are qualitatively similar for alternate\nmodel conﬁgurations. Compared to Figure 2 (bottom-right),\nthe results are substantially worse for most categories. In\nmany cases, the stationary model performs worse than the\nuniform shufﬂe. This reﬂects the importance of transition\neffects when modeling playlists, even when the corpus is\nconﬁned to genre-speciﬁc categories.\n4.3 Experiment 3: Which features matter?\nAs illustrated in Figure 2 , certain categories seem to beneﬁt\nsubstantially from the inclusion of non-tag features. To\ninvestigate this effect, Figure 4 illustrates the aggregated\nweight for each feature type under each of the category\nmodels. Note that weight is aggregated across feature con-ALL  \nMixed  \nTheme  \nRock−Pop  \nAlternating DJ  \nIndie  \nSingle Artist  \nRomantic  \nRoad Trip  \nPunk  \nDepression  \nBreak Up  \nNarrative  \nHip Hop  \nSleep  \nElectronic Music  \nDance−House  \nRhythm and Blues  \nCountry  \nCover  \nHardcore  \nRock  \nJazz  \nFolk  \nReggae  \nBlues  \n 0%  5% 10% 15% 20%\nGain over uniform: TAGS  \nGlobal  \nCategory  \nALL  \nMixed  \nTheme  \nRock−Pop  \nAlternating DJ  \nIndie  \nSingle Artist  \nRomantic  \nRoad Trip  \nPunk  \nDepression  \nBreak Up  \nNarrative  \nHip Hop  \nSleep  \nElectronic Music  \nDance−House  \nRhythm and Blues  \nCountry  \nCover  \nHardcore  \nRock  \nJazz  \nFolk  \nReggae  \nBlues  \n 0%  5% 10% 15% 20% 25%\nGain over uniform: ALL FEATURES  \nGlobal\nCategory  \nALL  \nMixed  \nTheme  \nRock−Pop  \nAlternating DJ  \nIndie  \nSingle Artist  \nRomantic  \nRoad Trip  \nPunk  \nDepression  \nBreak Up  \nNarrative  \nHip Hop  \nSleep  \nElectronic Music  \nDance−House  \nRhythm and Blues  \nCountry  \nCover  \nHardcore  \nRock  \nJazz  \nFolk  \nReggae  \nBlues  \n 0%  5% 10% 15% 20%\nGain over uniform: TAGS + CONJUNCTIONS  \nGlobal\nCategory  \nALL  \nMixed  \nTheme  \nRock−Pop  \nAlternating DJ  \nIndie  \nSingle Artist  \nRomantic  \nRoad Trip  \nPunk  \nDepression  \nBreak Up  \nNarrative  \nHip Hop  \nSleep  \nElectronic Music  \nDance−House  \nRhythm and Blues  \nCountry  \nCover  \nHardcore  \nRock  \nJazz  \nFolk  \nReggae  \nBlues  \n 0%  5% 10% 15% 20% 25%\nGain over uniform: ALL FEATURES + CONJUNCTIONS  \nGlobal\nCategory  Figure 2 . The median gain in log-likelihood over the uniform shufﬂe model, aggregated over ten random splits of the data.\nError bars span the 0.25–0.75 quantiles. Category-speciﬁc models generally outperform global models.\nALL  \nMixed  \nTheme  \nRock−Pop  \nAlternating DJ  \nIndie  \nSingle Artist  \nRomantic  \nRoad Trip  \nPunk  \nDepression  \nBreak Up  \nNarrative  \nHip Hop  \nSleep  \nElectronic Music  \nDance−House  \nRhythm and Blues  \nCountry  \nCover  \nHardcore  \nRock  \nJazz  \nFolk  \nReggae  \nBlues  \n−15%−10% −5%  0%  5% 10% 15% 20%\nGain over uniform: STATIONARY  \nGlobal\nCategory  \nFigure 3 . Log-likelihood gain over uniform with the sta-\ntionary model (all features and conjunctions). Ignoring\ntemporal structure signiﬁcantly degrades performance.\njunctions, so the weight for edge DECADE 1955-&-Rock\ncounts both for EraandTag.\nTags receive the most weight (64% on average) across all\ncategories. Audio features appear to be most useful in Hip\nHop, Jazz andBlues (43%–44%, compared to 26% average).\nThis is not surprising, given that these styles feature rela-\ntively distinctive instrumentation and production qualities.\nLyrical features receive the most weight in categories with\nsalient lyrical content (Folk, Cover, Narrative, Hardcore,\nBreak Up) and low weight in categories with little or highly\nvariable lyrical content (Electronic Music, Dance-House,\nJazz). Era and familiarity receive moderate weight (on aver-age, 22% and 15% respectively), but the majority (20% and\n14%) is due to conjunctions.\n4.4 Example playlists\nTable 3 illustrates samples drawn from category-speciﬁc\nfeature conjunction models. For generative purposes, the\nuniform edge was removed after training. The generated\nplaylists demonstrate both consistency within a single playlist\nand variety across playlists. Each transition in the playlist\nis explained by the corresponding (incoming) edge, which\nprovides transparency to the user: for example, Cole Porter\n- You’re the Top follows Django Rheinhardt - Brazil because\nboth songs belong to the conjunction edge AUDIO-3/16-\n-&-jazz, and share both high- and low-level similarity.\n5. CONCLUSION\nWe have demonstrated that playlist model performance can\nbe improved by treating speciﬁc categories of playlists in-\ndividually. While the simple models proposed here work\nwell in some situations, they are far from complete, and\nsuggest many directions for future work. The ﬁrst-order\nMarkov assumption is clearly a simpliﬁcation, given that\nusers often create playlists with long-term interactions and\nglobal thematic properties. Similarly, the uniform distribu-\ntion over songs within an edge set allows for an efﬁcient\nand scalable implementation, but allowing non-uniform dis-\ntributions could also be an avenue for future improvement.Audio CF EraFamiliarity Lyrics Tags UniformALL    Mixed    Theme    Rock−Pop    Alternating DJ    Indie    Single Artist    Romantic    Road Trip    Punk    Depression    Break Up    Narrative    Hip Hop    Sleep    Electronic Music    Dance−House    Rhythm and Blues    Country    Cover    Hardcore    Rock    Jazz    Folk    Reggae    Blues    Figure 4 . Distribution of learned edge weights for each playlist category. Weight is aggregated across feature conjunctions.\nCategory Edge Playlist\nHip HopAUDIO-149/256 Eminem - The Conspiracy (Freestyle)\nAUDIO-149/256 Busta Rhymes - Bounce\nDECADE-2000-&-rap Lil’ Kim (Featuring Sisqo) - How Many Licks?\nold school A Tribe Called Quest - Butter\nDECADE 1985-&-Hip-Hop Beastie Boys - Get It Together\nAUDIO-12/16 Big Daddy Kane - Raw [Edit]\nElectronic MusicAUDIO-11/16-&-downtempo Everything But The Girl - Blame\nDECADE 1990-&-trip-hop Massive Attack - Spying Glass\nAUDIO-11/16-&-electronica Bj¨ork - Hunter\nDECADE 2000-&-AUDIO-23/64 Four Tet - First Thing\nelectronica-&-experimental Squarepusher - Port Rhombus\nelectronica-&-experimental The Chemical Brothers - Left Right\nRhythm and Blues70s-&-soul Lyn Collins - Think\nAUDIO-14/16-&-funk Isaac Hayes - No Name Bar\nDECADE 1965-&-soul Michael Jackson - My Girl\nAUDIO-6/16-&-soul The Platters - Red Sails In The Sunset\nFAMILIARITY MED-&-60s The Impressions - People Get Ready\nsoul-&-oldies James & Bobby Purify - I’m Your Puppet\nJazzAUDIO-14/16-&-jazz Peter Cincotti - St Louis Blues\njazz Tony Bennett - The Very Thought Of You\nvocal jazz Louis Prima - Pennies From Heaven\njazz-&-instrumental Django Reinhardt - Brazil\nAUDIO-3/16-&-jazz Cole Porter - You’re The Top\njazz Doris Day - My Blue Heaven\nTable 3 . Example playlists generated by various dialect models. Edge denotes the incoming edge to the corresponding song,\nwhich for transitions, is shared by the previous song. Feature conjunctions are indicated by X-&-Y.\n6. ACKNOWLEDGMENTS\nThe authors acknowledge support from Qualcomm, Inc.,\neHarmony, Inc., Yahoo! Inc., Google, Inc., and NSF Grants\nCCF-0830535, IIS-1054960, and EIA-0303622.\n7. REFERENCES\n[1]T. Bertin-Mahieux, D.P.W. Ellis, B. Whitman, and\nP. Lamere. The million song dataset. In ISMIR, 2011.\n[2]R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited\nmemory algorithm for bound constrained optimization.\nSIAM J. Sci. Comput., 16(5):1190–1208, Sep. 1995.\n[3]S.J. Cunningham, D. Bainbridge, and A. Falconer.\n“More of an art than a science”: Supporting the cre-\nation of playlists and mixes. In ISMIR, 2006.\n[4]D. Ellis, B. Whitman, A. Berenzweig, and S. Lawrence.\nThe quest for ground truth in musical artist similarity.\nInISMIR, 2002.\n[5]B. Fields. Contextualize Your Listening: The Playlist\nas Recommendation Engine. PhD thesis, Goldsmiths,\nUniversity of London, April 2011.\n[6]Z. Gantner, S. Rendle, C. Freudenthaler, and L. Schmidt-\nThieme. MyMediaLite: A free recommender system\nlibrary. In RecSys, 2011.[7]M. Hoffman, D. Blei, and F. Bach. Online learning for\nlatent dirichlet allocation. In NIPS. 2010.\n[8]J.H. Lee. How similar is too similar?: Exploring users’\nperceptions of similarity in playlist evaluation. In IS-\nMIR, 2011.\n[9]B. Logan. Content-based playlist generation: ex-\nploratory experiments. In ISMIR, 2002.\n[10] B. McFee, T. Bertin-Mahieux, D.P.W. Ellis, and G.R.G.\nLanckriet. The million song dataset challenge. In Ad-\nMIRe, 2012.\n[11] B. McFee and G. R. G. Lanckriet. The natural language\nof playlists. In ISMIR, 2011.\n[12] R. Ragno, C. J. C. Burges, and C. Herley. Inferring\nsimilarity between music objects with application to\nplaylist generation. In 7th ACM SIGMM international\nworkshop on Multimedia information retrieval, 2005.\n[13] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-\nThieme. BPR: Bayesian personalized ranking from im-\nplicit feedback. In UAI, 2009.\n[14] F. Wang, X. Wang, B. Shao, T. Li, and M. Ogihara. Tag\nintegrated multi-label music style classiﬁcation with\nhypergraph. In ISMIR, 2009."
    },
    {
        "title": "Shortest Path Techniques for Annotation and Retrieval of Environmental Sounds.",
        "author": [
            "Brandon Mechtley",
            "Andreas Spanias",
            "Perry Cook"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416926",
        "url": "https://doi.org/10.5281/zenodo.1416926",
        "ee": "https://zenodo.org/records/1416926/files/MechtleySC12.pdf",
        "abstract": "Many techniques for text-based retrieval and automatic an- notation of music and sound effects rely on learning with explicit generalization, training individual classifiers for each tag. Non-parametric approaches, where queries are individually compared to training instances, can provide added flexibility, both in terms of robustness to shifts in database content and support for foreign queries, such as concepts not yet included in the database. In this paper, we build upon prior work in designing an ontological frame- work for annotation and retrieval of environmental sounds, where shortest paths are used to navigate a network con- taining edges that represent content-based similarity, se- mantic similarity, and user tagging data. We evaluate novel techniques for ordering query results using weights of both shortest paths and minimum cost paths of specified lengths, pruning outbound edges by nodes’ K nearest neighbors, and adjusting edge weights depending on type (acoustic, semantic, or user tagging). We evaluate these methods both through traditional cross-validation and through simulation of live systems containing a complete collection of sounds and tags but incomplete tagging data.",
        "zenodo_id": 1416926,
        "dblp_key": "conf/ismir/MechtleySC12",
        "keywords": [
            "explicit generalization",
            "learning with explicit generalization",
            "non-parametric approaches",
            "queries individually compared",
            "robustness to shifts",
            "foreign queries",
            "content-based similarity",
            "semantic similarity",
            "user tagging data",
            "ontological framework"
        ],
        "content": "SHORTEST PATH TECHNIQUES FOR ANNOTATION AND RETRIEVAL\nOF ENVIRONMENTAL SOUNDS\nBrandon Mechtley\nArizona State University\nComputer Science (SCIDSE)\nbmechtley@asu.eduPerry Cook\nPrinceton University\nComputer Science and Music\nprc@cs.princeton.eduAndreas Spanias\nArizona State University\nElectrical Engineering (SECEE)\nspanias@asu.edu\nABSTRACT\nMany techniques for text-based retrieval and automatic an-\nnotation of music and sound effects rely on learning with\nexplicit generalization, training individual classiﬁers for\neach tag. Non-parametric approaches, where queries are\nindividually compared to training instances, can provide\nadded ﬂexibility, both in terms of robustness to shifts in\ndatabase content and support for foreign queries, such as\nconcepts not yet included in the database. In this paper, we\nbuild upon prior work in designing an ontological frame-\nwork for annotation and retrieval of environmental sounds,\nwhere shortest paths are used to navigate a network con-\ntaining edges that represent content-based similarity, se-\nmantic similarity, and user tagging data. We evaluate novel\ntechniques for ordering query results using weights of both\nshortest paths and minimum cost paths of speciﬁed lengths,\npruning outbound edges by nodes’ K nearest neighbors,\nand adjusting edge weights depending on type (acoustic,\nsemantic, or user tagging). We evaluate these methods both\nthrough traditional cross-validation and through simulation\nof live systems containing a complete collection of sounds\nand tags but incomplete tagging data.\n1. INTRODUCTION\n1.1 Multiclass and non-parametric retrieval\nMany techniques for text-based retrieval or classiﬁcation\nof audio signals are parametric in nature, relying on ex-\nplicit generalization, where individual classiﬁers are cre-\nated for each label. For example, classiﬁcation systems\nhave been built for automatic record reviews [18], ono-\nmatopoetic labels [9], and genre [17], emotion [10], and in-\nstrumentation [5,7] identiﬁcation. These systems make use\nof techniques such as one-versus-all discrimination [18],\ntraining each label with a support vector machine (SVM)\nclassiﬁer [1, 9], and learning a separate gaussian mixture\nmodel (GMM) for each label [16, 18].\nThese multiclass methods beneﬁt from constant query\ntime complexity independent of the number of training in-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2012 International Society for Music Information Retrieval.stances, in that it is only necessary for each query (such as\na sound, in the case of annotation) to be measured against\neach classiﬁer. For speciﬁc, relatively stationary label do-\nmains, such as musical genres, this can be seen as a great\nbeneﬁt, especially when the number of sounds greatly ex-\nceeds the number of labels. However, there are many cases\nwhere non-parametric models can provide additional ﬂex-\nibility and robustness. One such case involves the pres-\nence of multiple types of information beyond acoustic fea-\nture vectors and annotations. For example, [19] and [13]\ndescribe methods where similarity between semantic con-\ncepts can assist in retrieval and annotation using tags not\nyet seen in training data. In large-scale systems with more\ncomplete tag sets, this may be less of a problem, but in live\ndatabases with incomplete tagging where no large-scale\ntraining database exists beyond user activity, as in the case\nof Freesound1, retrieval results can often come up empty.\nNon-parametric (also known as similarity- or instance-\nbased [6,11]) schemes compare each query to instances in\na live database rather than having distinct training and pro-\nduction / evaluation stages. For example, [2], [3], and [12]\nuse K-nearest-neighbors retrieval, where unlabeled sounds\nare annotated with tags belonging to their nearest neigh-\nbors in an acoustic feature space. [15] and [2] build two\nseparate hierarchical cluster models—one for retrieval and\none for annotation.\n1.2 Associative retrieval\nGraph-based techniques are often used for search in se-\nmantic and other associative networks. One technique that\nhas seen much use is spreading activation. In spreading\nactivation, an initial node (a query) is labeled with some\nweight, and this weight is spread to neighboring nodes with\nsome decay. Spreading activation has been used in infor-\nmation retrieval applications, where nodes correspond to\ndocuments and terms [4]. Shortest paths are also of inter-\nest in associative retrieval. [20] introduces a graph-based\nframework where sounds are connected to tags through\nuser activity and sounds are fully connected via acoustic\nsimilarity estimated by an HMM-based query-by-example\nalgorithm described in [21]. New queries are immediately\nconnected to other sounds or tags (either through acous-\ntic or semantic similarity via the WordNet::Similarity li-\nbrary [14]), and shortest path distances using all nodes are\n1Freesound: http://freesound.org/qt\ntM...t2t1\nsN...s2s1\n(a) Text-based retrievalqs\ntM...t2t1\nsN...s2s1\n(b) Annotation\nFigure 1: Two different possible query tasks with a sin-\ngle retrieval network. qsandqtrepresent a sound query\nand a tag query, respectively, and t1;t2;:::;tM2Tand\ns1;s2;:::;sN2Srepresent tag and sound nodes already\nin the database. The query node and the subset of nodes\nover which the user is querying is marked in bold, and on-\ndemand edge weights between the query and its respective\nclass of nodes (sounds or tags) are marked with dashed\nlines. Note that Sforms a clique.\nused to rank retrieval results.\nThe results of [19] demonstrate that including semantic\nsimilarity to account for tags foreign to the training set can\nassist in annotation and text-based retrieval both for a ran-\ndom subset from the Freesound library, where tags are as-\nsociated to sounds in a binary manner, and a smaller com-\nprehensive user study, where each sound is tagged by mul-\ntiple users. In this paper, we build upon this graph-based\ntechnique and perform a more in-depth study of its proper-\nties. Namely, we seek to a) evaluate the system using both\ntraditional cross-validation and simulations of real-world\nsystems with complete sound and tag sets but incomplete\ntagging data, b) demonstrate the effectiveness of adding\na shortest-path algorithm to any existing tag-based query\nsystem, regardless of the presence of acoustic and seman-\ntic similarity measures, c) improve shortest-path retrieval\nperformance by pruning network edges to nodes’ K near-\nest neighbors, and d) explore the impact of assigning dif-\nferent weights to the importance of acoustic, semantic, and\nuser-provided information.\n2. SHORTEST PATH RETRIEVAL\n2.1 Network structure\nFormally, the network structure for retrieval and annota-\ntion takes the form of a weighted, undirected graph, G=\n(V;E ), whereV=S[TandSandTrepresent sets of\nsound and tag nodes. The graph edges, E, can be parti-\ntioned into three disjoint subsets, ESS\u0012S\u0002S,EST\u0012\nS\u0002T, andETT\u0012T\u0002T, representing acoustic, user-\nprovided, and semantic information. The weighting func-\ntion is denoted by w:E!R+. This type of network\nstructure can be adapted to different domains, such as mu-\nsic or even text documents, but for the sake of this paper,\nwe focus on the task of retrieving and annotating envi-\nronmental sounds. We therefore assume that sounds take\nthe form of short audio clips representing individual sonic\nevents. For more discussion on the concept of a “soundevent,” see the related discussion in [21]. In the following\nsections, we will discuss how the weights for ESS,EST,\nandETTare calculated.\n2.1.1 Sound-to-sound weights (E SS)\nSound-to-sound weights can be computed by comparing\nthe acoustic content of each sound. For this task, we use\nthe Sirens library2. For detailed information of how sound\nsimilarity is computed in Sirens and an evaluation of its\nperformance, see [21]. A summary is as follows:\nSirens begins with acoustic feature extraction, where\nsix features are calculated on overlapping 40ms Hamming\nwindows hopped every 20ms. The feature trajectory for a\nsound ﬁle is given by Y(1:F )\n1:T , whereY(i)\ntis thei-th fea-\nture’s value at frame t.\nThe six features used by Sirens include loudness, the\ndB-scaled RMS level over time; temporal sparsity, the ra-\ntio ofl1and andl1norms calculated over all windowed\nRMS levels in a one-second interval; spectral sparsity, the\nratio ofl1andl1norms calculated over short-time Fourier\ntransform (STFT) magnitudes; spectral centroid, the bark-\nweighted average spectral content of a sound at any point\nin time; transient index, the l2norm of the difference of\nMel frequency cepstral coefﬁcients (MFCC) between con-\nsecutive frames; and harmonicity, a probabilistic measure\nof whether or not the signal comes from a harmonic source.\nThis feature set was developed for a broad range of envi-\nronmental sounds rather than any speciﬁc class of sounds\nand with a focus on ecological validity such that the fea-\ntures would best relate to qualities of human audition by\nusing perceptual scalings of spectral content [21].\nTo compare sounds, [21] describes a method of estimat-\ningL(si;sj) =\u0000logP(Y(1:F )\n1:T (si)j\u0015(1:F )(sj)), the log-\nlikelihood of the feature trajectory of sound sibeing gen-\nerated from the hidden Markov model (HMM), \u0015(1:F )(sj),\nbuilt to approximate the simple (i.e. constant, linear, or\nquadratic) feature trends of sound sj. For retrieval in our\nundirected graph, however, it is helpful to have a semi-\nmetric between sounds that is symmetric and nonnegative.\nIn [8], a semi-metric that holds these properties is given:\nw(si;sj) =L(si;si) +L(sj;sj)\u0000\nL(si;sj)\u0000L(sj;si): (1)\n2.1.2 Sound-to-tag weights (E ST)\nLettingUjSj\u0002jTjbe a votes matrix where Uijis equal to\nthe number of users who have tagged sound siwith tagtj,\nwe can compute the joint probability of siandtjas\nP(si;tj) =UijP\nk;lUkl(2)\nw(si;tj) =\u0000logP(si;tj): (3)\n2.1.3 Tag-to-tag weights (E TT)\nThe results from [13] and [19] demonstrate that seman-\ntic similarity (tag-to-tag edges) obtained from WordNet::-\n2Sirens (Segmentation, Indexing, and Retrieval of Environmental\nSounds): http://github.com/plant/sirensSimilarity [14] scores can be useful when performing text-\nbased retrieval queries using tags not in the database or an-\nnotating sounds with these foreign tags. In general, how-\never, it was found that including tag-to-tag links between\nin-network tags can hinder performance, so we have ex-\ncluded these links in this paper, as we are chieﬂy interested\nin studying the effects of different shortest-path retrieval\nstrategies rather than the source of the weights themselves.\n2.2 Shortest path retrieval\nGiven the structure of the graph, G(V;E )and its weights,\nw, as deﬁned in the previous section, we rank search results\naccording to their shortest path lengths from the query, q,\nto the target, t,in ascending order:\nw\u0003(q;t) = min\nP=hq;:::;tijPj\u00001X\ni=1w(Pi;Pi+1); (4)\n3. NETWORK MODIFICATIONS\n3.1 Depth-ordered retrieval\nShortest paths may sometimes hinder retrieval results in\ncases where they provide discursive paths that rely on nu-\nmerous relations. For example, if sound-to-tag weights are\ntrained with ground truth data obtained from user studies\nor extensive user activity, it would be desirable to only\nuse these direct paths and visit no other nodes rather than\nsecond-guessing users (who, for the purpose of evaluation,\nwe often assume are experts).\nIn these cases, we can form a list, L,of positive integers\nrepresenting desired path depths, with an optional ﬁnal ele-\nment,\u0003, representing shortest paths of any depth. Any tar-\ngets unconnected to the query will be returned at the end of\nthe list in random order. For example, L= (2;\u0003)will pri-\noritize minimum-cost direct edges between the query and\ntargets ﬁrst, only using shortest paths as a last resort in\nthe absence of direct edges. L= (2; 3;\u0003)will ﬁrst re-\nturn all direct edges, then shortest paths containing only\nthree nodes, and ﬁnally all shortest paths. For the case of\nL= (2; 3;:::), where depths are in monotonically increas-\ning order, this algorithm performs similarly to a breadth-\nﬁrst search. In Section 4.3, we will discuss the relative\nperformance of depth orderings.\n3.2 Edge pruning\nShortest-path retrieval can be quite computationally expen-\nsive. In the worst case, Dijkstra’s algorithm has O(jEj+\njVjlogjVj)time complexity. As our graph is quite well-\nconnected (for large numbers of sounds), we can assume\nthe complexity of performing a single query is O(jVj2),\nasjEj=O(jVj2)when the number of sounds greatly ex-\nceeds the number of tags. In these cases, it can be ben-\neﬁcial to limit search to only a node’s Knearest neigh-\nbors, giving a complexity of O(KjVj+jVjlogjVj) =\nO(jVjlogjVj).3In Section 4.4, we will discuss the ef-\n3Using spectral clustering to cluster sounds, as in [20], we can even\nimprove this to O(log jV j)complexity.fects ofKnearest neighbor pruning, where Gis converted\nto a directed graph with inbound and outbound edges iden-\ntical to the original undirected edges and all but the K\nlowest-weight outbound edges are removed.\n3.3 Weighting edge classes\nLastly, it should be noted that the ranking of search results\ncan be quite sensitive to variations in weighting between\nthe different classes of edges, ESS,EST, andETT, as\neach assumes a different probabilistic model. If one class\nhas particularly low weights, its edges may be used more\nfrequently than edges of other classes. In Section 4.5, we\nexamine the effects of setting class-speciﬁc weights, \rC:\nw\r(n1;n2) =\rCw(n1;n1)\n8(n 1;n2)2EC;8EC2fESS;EST;ETTg (5)\n4. RESULTS AND DISCUSSION\n4.1 Training data\nTo evaluate the text-based retrieval and annotation perfor-\nmance of our various modiﬁcations to the shortest-path re-\ntrieval method, we use two datasets that link sounds to con-\nceptual tags. The ﬁrst dataset, Soundwalks, is a collection\nof 178 2-60s sound events manually segmented from one\nof four outdoor recording sessions recorded at 44.1KHz.\nSound environments include light rail stops, a stadium dur-\ning a football game, a skatepark, and a walkway on a col-\nlege campus. To obtain tagging data, an online user study\nwas performed where each user was asked to provide sev-\neral semantic tags for 10 randomly selected sound events\nby freely typing terms separated by commas. Terms that\ncould not be found in the WordNet taxonomy were ig-\nnored. With a total of 90 responses, each sound was tagged\nan average of 4.62 times. In [19] and [13], only the most\npopular 88 tags were used for evaluation, but we have used\nthe entire set of 612 tags to more accurately study the sys-\ntem’s performance. The second dataset, Freesound, was\nobtained from user activity on the website Freesound.org.\n2046 sounds were randomly selected from the set of all\nsounds 2-60s in duration and containing at least one of any\nof the 50 most popular tags on the site. Each sound is asso-\nciated with 3-8 tags, and each of the 377 total tags used is\nassociated with at least 5 sounds. Note that on Freesound,\ntags are only associated to sounds in a binary manner, so\nallnonzero sound-to-tag weights are equal. Both datasets\nhave been used to test the performance of the system a)\nagainst a slightly more reliable ground truth, in the case\nSoundwalks, where each sound ﬁle has been tagged by 4-\n5 users, and b) against a larger collection of sounds, as in\nFreesound.\n4.2 Evaluation methodology\n4.2.1 Cross-validation versus incomplete tagging data\nFor multiclass retrieval, where classiﬁers are trained for\neach search term, evaluation procedures typically involve(a) Soundwalks Precision/Recall\n (b) Freesound Precision/Recall\nRetrieval Annotation\nL MAP MAROC MAP MAROC\n(2) 0.5505 0.7615 0.5920 0.7814\n(\u0003) 0.5751 0.7910 0.5230 0.7833\n(2;\u0003) 0.5673 0.7887 0.5964 0.7919\n(2;3;\u0003) 0.5678 0.7849 0.6590 0.8824\n(c) Soundwalks MAP/MAROCRetrieval Annotation\nL MAP MAROC MAP MAROC\n(2) 0.5200 0.7504 0.5237 0.7506\n(\u0003) 0.5594 0.8610 0.5628 0.8403\n(2;\u0003) 0.5618 0.8623 0.5618 0.8399\n(d) Freesound MAP/MAROC\nFigure 2: Performance metrics for text-based retrieval and annotation of sounds, respectively. Data is averaged across\nn= 50 trials with half the tagging data missing. Curves are labeled according to the order of path lengths used in sorting\nresults, where\u0003denotes all shortest paths. Limit is the absolute best performance possible with the dataset.\ncross-validation, where the set of sounds and their asso-\nciated tags are split into several (e.g. 10) random non-\noverlapping subsets, the classiﬁers are trained with only\none subset, and the remaining sounds are used as queries to\ntest the performance of the trained classiﬁers. With a suf-\nﬁciently large training dataset, performance results should\nconverge to give a picture of the expected performance in\na production setting.\nIn [19], [20], and [13], this technique was employed for\nshortest-path retrieval. For the cases of retrieval and an-\nnotation using sounds and tags not present in the training\ndata (thereby testing the usefulness of both acoustic and\nsemantic similarity), sounds and tags were split into 2 and\n5 subsets, respectively, each combination thereof (one of\n2\u00025 = 10 ) being used to build the network. For anno-\ntation, out-of-network sound queries were independently\nintroduced to the network by computing their similarity to\nall other sounds in the network, and out-of-network tags\nwere connected only to the in-network tags. Query perfor-\nmance was tested by querying each out-of-network sound\nagainst out-of-network tags. For retrieval, tag queries were\nconnected independently to other tags, and out-of-network\nsounds were connected only to in-network sounds.\nHowever, this method of cross-validation may not be\nentirely appropriate for shortest-path retrieval, as there is\nno distinct training phase (it is non-parametric). Rather\nthan having only sounds and tags as training data, acoustic\nand semantic similarities between training instances must\nbe considered. For this reason, we have chosen to imple-\nment a different evaluation strategy to compare techniques.\nIn this strategy, we simulate a database where the set of\nsounds and tags are complete (there are no cross-validation\nsplits), but only a random subset of the user tagging data\nis available. Speciﬁcally, for each association between asound and a tag (for which there may be many for a single\nsound-tag pair in the Soundwalks dataset), we remove it\nwith 50% probability. For annotation, every sound is used\nto query the entire set of tags, and for retrieval, every tag\nis used to query the entire set of sounds. Relevance re-\nsults are then averaged over each query and over 50-100\ntrials with different tagging data. This simulation is per-\nhaps more appropriate than the networks built for cross-\nvalidation, as we can examine how using shortest-path re-\ntrieval can help make up for sparse tagging data, which is\noftentimes present in online tagging systems.\n4.2.2 Performance measures\nEach query returns an ordered list of nodes (tags for an-\nnotation and sounds for retrieval), sorted by path length in\nascending order. An item in this list is said to be relevant\nif it is connected to the query at least once in the origi-\nnal user tagging data. Using this list of relevance for each\nitem returned, we can compute mean precision, the per-\ncentage of items returned that are relevant as more items\nare returned, and mean recall, the percentage of all rele-\nvant items that have been returned. Plotting precision as\na function of recall is a useful way of comparing different\nschemes. Additionally, one can compute summary statis-\ntics including mean average precision (MAP), the mean of\nprecision values at the points where each relevant item is\nreturned, and mean area under the receiver operator char-\nacteristic (MAROC), the integral of the curve produced by\nplotting the ratio of true positives versus false positives.\n4.3 Depth-ordered retrieval\nIn Figure 2, we examine the effects of a) using shortest\npaths versus retrieving items based only on their tags (as\nmost tag-based search strategies do) and b) using differ-5 10 15 20\nmax out-degree (K)020406080100MAP/MAROC %Retrieval\n5 10 15 20\nmax out-degree (K)020406080100MAP/MAROC %Annotation\nL=(2) L=(∗) L=(2,∗)Figure 3: Effects of pruning outbound edges to nodes’\nK nearest neighbors using the Soundwalks dataset. Both\nMAP (lower) and MAROC (upper) values are plotted as a\nfunction of K and averaged across n= 100 trials with half\nthe tagging data missing.\nent depth ordering strategies. L= (2) corresponds to\nthe case where only direct tag-to-tag links are used for\nretrieval (the baseline case, given no acoustic similarity\ninformation), L= (\u0003) represents the case of ranking re-\nsults based on the lengths of their shortest paths, and L=\n(2;\u0003)andL= (2; 3;\u0003)represent the cases of returning\nminimum 2- and 3-node paths before resorting to short-\nest paths. Results are shown for both the Freesound and\nSoundwalks datasets.Limit represents the theoretical up-\nper limit on performance imposed by the dataset, where the\nground truth user tagging data itself is used to order results,\nanalagous to UpperBnd from [16]. Acoustic links were\nincluded for the Soundwalks dataset but not the Freesound\ndataset, in order to study the effects of using shortest-path\nretrieval as a drop-in method in an existing system.\nFrom these plots, we can see that, in some cases, as\nin annotation on the Soundwalks dataset (Figure 2b), using\nshortest paths performs worse than the baseline case, likely\nbecause known sound-to-tag links are being circumvented\nin favor of paths that use acoustic similarity. However, L=\n(\u0003)seems to perform marginally better than L= (2) for\nthe case of retrieval. To account for this difference, we can\nsee that prioritizing direct links, as in L= (2;\u0003), performs\nbest.L= (2; 3;\u0003)is a special case, as it produces higher\nMAP/MAROC, corresponding to its better performance in\nthe last 75% of results, but it initially performs quite a bit\npoorer at annotation, which may be undesirable (if, say, we\nwere to annotate with only those tags that score highest).\nFor the Freesound dataset, for which we provided no\nsound-to-sound links, we can see that the L= (\u0003), and\noptionallyL= (2;\u0003), methods can assist in ordering the\nlast half of results. This improvement is likely because,\nfor annotation (and analagously for text-based retrieval),\na sound can be annotated with additional tags from those\nsounds it shares a few tags with. Of course, in some use\ncases, this increase in performance may not be worth the\nextra query time. Note that L= (2;3;\u0003)would behave the\nsame asL= (2;\u0003)in this case, as no sound-to-tag paths\nwith an odd number of nodes exist in a network containing\nno sound-to-sound edges.\n0.0 0.2 0.4 0.6 0.8 1.0\nγSS020406080100MAP/MAROC %Retrieval\n0.0 0.2 0.4 0.6 0.8 1.0\nγSS020406080100MAP/MAROC %Annotation\nL=(2) L=(∗) L=(2,3,∗) L=(2,∗)Figure 4: Effects of varying \rSS, the global weight mul-\ntiplier for sound-to-sound edges using the Soundwalks\ndataset. Both MAP (lower) and MAROC (upper) values\nare plotted as a function of \rSSand averaged over n= 50\ntrials with half tagging data missing.\n4.4 Pruning\nTo test the effects of limiting search to nodes’ K near-\nest neighbors, we ﬁrst constructed a network as described\nin 4.2.1 using the Soundwalks dataset, with sound-to-sound\nand sound-to-tag links, but with half the tagging data miss-\ning. ForK2f1; 2;:::; 20g, we then annotated with each\nsound and retrieved with each tag, testing relevance against\nthe original tagging data. For each value of K, we aver-\naged performance metrics over 50trials for a total of 1000\ntrials per query type. As shown in Figure 3, it is only\nwhenK < 10that signiﬁcant losses in MAP/MAROC\ncan be seen, suggesting that edge pruning can drastically\nimprove query time without having signiﬁcant effects on\nperformance, as 10\u001cjEj.\n4.5 Weighting edge classes\nFigure 4 demonstrates that, for the Soundwalks dataset,\nthere is a clear shift in performance at \rSS\u00190:2. For\n\rSS<0:2, acoustic weights are used as the primary source\nof similarity information at the expense of known tagging\ndata. For the case of annotation, there appears to be a slight\nincrease in MAROC for L= (\u0003) at this point. For L=\n(2;\u0003), there is a slight increase in performance for \rSS<\n0:2, which suggests that the system performs slightly bet-\nter when tagging data is used for direct links, but acoustic\nsimilarity, rather than cooccurrence of tags, is primarily re-\nlied on when no direct, 2-node links exist.\n5. CONCLUSIONS AND FUTURE WORK\nIn this paper, we have experimented with several modiﬁca-\ntions of a shortest-path retrieval algorithm, where acoustic\nsimilarities between sounds are used in conjunction with\nuser tagging data for the purpose of annotation and text-\nbased retrieval. Speciﬁcally, we have demonstrated that:\n1. giving priority to direct, 2-node paths before resort-\ning to shortest path lengths can greatly improve an-\nnotation and retrieval accuracy,2. pruning edges searched to nodes’ Knearest neigh-\nbors can reduce query complexity from O(jVj2)to\nO(jVjlogjVj)for values of Kas low as 10, and\n3. relative weighting between edge classes (sound-to-\nsound versus sound-to-tag) inﬂuences retrieval re-\nsults only slightly but indicate when certain types of\nsimilarity information are best used.\nWhile the query time of this approach will likely be slower\nthan similar parametric classiﬁcation approaches for any\ndatabase where the number of desired classiﬁers (tags) is\nmuch less than the number of sounds, this approach can\nstill be very useful for smaller datasets, where acoustic\nsimilarity between sounds and co-occurrence of tags can\nhelp make up for sparse tagging data, as shown in the re-\nsults for the Freesound dataset.\nIn [20], a method where spectral clustering is used to\ncreate cluster nodes that reduce the number of sound-to-\nsound edges is discussed. In addition to actually improv-\ning query accuracy, query complexity is greatly reduced.\nCombined with the methods of depth-ordered search and\npruning we have introduced in this paper, pre-processing\nsound-to-sound edges in this way could achieve time com-\nplexity as low as O(logjVj)during queries.\nAdditionally, [13] and [19] discuss the effects of the\npresence of tag-to-tag edges. While it was shown in [19]\nthat tag-to-tag edges between in-network tags tend to hin-\nder performance, using different edge class weights (\r SS\nand\rTT) and depth-ordered search could create a situation\nwhere tag-to-tag edges can improve results.\n6. ACKNOWLEDGEMENTS\nSpecial thanks are given to Gordon Wichern and our re-\nviewers for many helpful comments and the contributors\nto and developers of Freesound.org for providing a won-\nderful source of reusable, tagged sounds. This material is\nbased upon work supported by the National Science Foun-\ndation under grant NSF IGERT DGE-05-04647 and NSF\nNet Centric I/UCRC - ASU SenSIP Site Award 1035086.\n7. REFERENCES\n[1] Luke Barrington, Mehrdad Yazdani, Douglas Turnbull, and\nGert Lanckriet. Combining feature kernels for semantic mu-\nsic retrieval. In International Conference on Music Informa-\ntion Retrieval, pages 614–619, 2008.\n[2] P. Cano and M. Koppenberger. Automatic sound annotation.\nInIEEE Workshop on Machine Learning for Signal Process-\ning, pages 391–400, 2004.\n[3] P. Cano, M. Koppenberger, S. Le Groux, J. Ricard, P.Herrera,\nand N. Wack. Nearest-neighbor generic sound classiﬁcation\nwith a wordnet-based taxonomy. In The 116th AES Conven-\ntion, Berlin, Germany, 2004.\n[4] Fabio Crestani. Application of spreading activation tech-\nniques in information retrieval. Artiﬁcial Intelligence Review,\n11:453–482, 1997.\n[5] S. Essid, G. Richard, and B. David. Inferring efﬁcient hierar-\nchical taxonomies for music information retrieval tasks: Ap-\nplication to musical instruments. In International Conference\non Music Information Retrieval, pages 324–328, 2005.[6] M. Goto and K. Hirata. Recent studies on music information\nprocessing. Acoustical Science and Technology, 25(6), 24.\n[7] P. Herrera-Boyer, G. Peeters, and S. Dubnov. Automatic clas-\nsiﬁcation of musical instrument sounds. Journal of New Mu-\nsic Research, 32(1):3–21, 2003.\n[8] B. H. Huang and L. R. Rabiner. A probabilistic distance mea-\nsure for hidden markov models. AT&T Technical Journal,\n64(2):1251–1270, 1985.\n[9] S. Kim, S. Narayanan, and S. Sundaram. Acoustic topic\nmodel for audio information retrieval. In IEEE Workshop on\nApplications of Signal Processing to Audio and Acoustics,\npages 37–40, New Paltz, NY , 2009.\n[10] T. Li and M. Ogihara. Detecting emotion in music. In Pro-\nceedings of the International Symposium on Music Informa-\ntion Retrieval, pages 239–240, Baltimore, MD, 2003.\n[11] O. Celma M. Sordo, C. Laurier. Annotating music collec-\ntions: How content-based similarity helps to propagate labels.\nInInternational Conference on Music Information Retrieval,\nVienna, Austra, 2007.\n[12] E. Martinez, O. Celma, M. Sordo, B. de Jong, and X. Serra.\nExtending the folksonomies of freesound.org using content-\nbased audio analysis. In SMC, Porto, Portugal, 2009.\n[13] B. Mechtley, G. Wichern, H. Thornburg, and A. S. Spanias.\nCombining semantic, social, and acoustic similarity for re-\ntrieval of environmental sounds. In IEEE International Con-\nference on Acoustics, Speech, and Signal Processing, Dallas,\nTexas, 2010.\n[14] T. Pederson, S. Patwardhan, and J. Michelizzi. Word-\nnet::similarity: measuring the relatedness of concepts. In In-\nnovative Applications of Artiﬁcial Intelligence Conference,\npages 1024–1025, Cambridge, Massachusetts, 2004. AAAI\nPress.\n[15] M. Slaney. Semantic-audio retrieval. In IEEE International\nConference on Acoustics, Speech, and Signal Processing,\npages IV–1408–IV–1411, 2002.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Se-\nmantic annotation and retrieval of music and sound effects.\nIEEE Transactions on Audio, Speech, and Language Process-\ning, 16(2):467–476, February 2008.\n[17] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of\naudio signals. IEEE Transactions on Speech and Audio Pro-\ncessing, 10(5):293–302, 2002.\n[18] B. Whitman and D. Ellis. Automatic record reviews. In Inter-\nnational Conference on Music Information Retrieval, pages\n470–477, 2004.\n[19] G. Wichern, B. Mechtley, A. Fink, H. Thornburg, and\nA. Spanias. An ontological framework for retrieving en-\nvironmental sounds using semantics and acoustic content.\nEURASIP Journal on Audio, Speech, and Music Processing,\n2010.\n[20] G. Wichern, H. Thornburg, and A. Spanias. Unifying seman-\ntic and content-based approaches for retrieval of environmen-\ntal sounds. In IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics, pages 13–16, New Paltz, NY ,\n2009.\n[21] G. Wichern, J. Xue, H. Thornburg, B. Mechtley, and\nA. Spanias. Segmentation, indexing, and retrieval for envi-\nronmental and natural sounds. IEEE Transactions on Audio,\nSpeech and Language Processing, 18(3):688–707, 2010."
    },
    {
        "title": "A Geometric Language for Representing Structure in Polyphonic Music.",
        "author": [
            "David Meredith 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414736",
        "url": "https://doi.org/10.5281/zenodo.1414736",
        "ee": "https://zenodo.org/records/1414736/files/Meredith12.pdf",
        "abstract": "In 1981, Deutsch and Feroe proposed a formal language for representing melodic pitch structure that employed the powerful concept of hierarchically-related pitch alphabets. However, neither rhythmic structure nor pitch structure in polyphonic music can be adequately represented using this language. A new language is proposed here that incorpo- rates certain features of Deutsch and Feroe’s model but ex- tends and generalises it to allow for the representation of both rhythm and pitch structure in polyphonic music. The new language adopts a geometric approach in which a pas- sage of polyphonic music is represented as a set of multi- dimensional points, generated by performing transforma- tions on component patterns. The language introduces the concept of a periodic mask, a generalisation of Deutsch and Feroe’s notion of a pitch alphabet, that can be applied to any dimension of a geometric representation, allowing for both rhythms and pitch collections to be represented parsimoniously in a uniform way.",
        "zenodo_id": 1414736,
        "dblp_key": "conf/ismir/Meredith12",
        "keywords": [
            "1981",
            "Deutsch and Feroe",
            "formal language",
            "melodic pitch structure",
            "hierarchically-related pitch alphabets",
            "polyphonic music",
            "new language",
            "geometric approach",
            "multi-dimensional points",
            "transformations on component patterns"
        ],
        "content": "A GEOMETRIC LANGUAGE FOR REPRESENTING STRUCTURE IN\nPOLYPHONIC MUSIC\nDavid Meredith\nAalborg University, Denmark\ndave@create.aau.dk\nABSTRACT\nIn 1981, Deutsch and Feroe proposed a formal language\nfor representing melodic pitch structure that employed the\npowerful concept of hierarchically-related pitch alphabets.\nHowever, neither rhythmic structure nor pitch structure in\npolyphonic music can be adequately represented using this\nlanguage. A new language is proposed here that incorpo-\nrates certain features of Deutsch and Feroe’s model but ex-\ntends and generalises it to allow for the representation of\nboth rhythm and pitch structure in polyphonic music. The\nnew language adopts a geometric approach in which a pas-\nsage of polyphonic music is represented as a set of multi-\ndimensional points, generated by performing transforma-\ntions on component patterns. The language introduces the\nconcept of a periodic mask, a generalisation of Deutsch\nand Feroe’s notion of a pitch alphabet, that can be applied\nto any dimension of a geometric representation, allowing\nfor both rhythms and pitch collections to be represented\nparsimoniously in a uniform way.\n1. INTRODUCTION\nThe problem addressed here is that of designing a formal\ncoding language [9, p. 155] for precisely and parsimo-\nniously describing structure in polyphonic music. In gen-\neral, there are various ways in which a musical pattern can\nbe perceived to be constructed, and a music coding lan-\nguage should be capable of representing these different in-\nterpretations. Moreover, it should be possible to compare\nand evaluate encodings of the different ways of interpret-\ning a musical pattern. The various methods that have been\nproposed for carrying out such comparisons and evalua-\ntions fall into two categories: those based on the likelihood\nprinciple of preferring the most probable interpretations;\nand those based on the minimum principle of preferring\nthesimplest interpretations [9, p. 152]. Typically, statis-\ntical approaches to musical structure analysis (e.g., [8])\napply the likelihood principle, whereas approaches in the\ntradition of Gestalt psychology (e.g., [1]) apply the min-\nimum principle. Indeed, van der Helm and Leeuwenberg\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.[9, p. 153] suggest that the fundamental principle of Gestalt\npsychology, Koffka’s [2] law of Pr¨agnanz, which favours\nthe simplest and most stable interpretation, can be seen as\nan “ancestor” of the minimum principle.\nThe language proposed here is in the tradition of the\nGestalt-based coding languages for music proposed by Si-\nmon and Sumner [7], Restle [5] and Deutsch and Feroe [1].\nIt attempts to generalise and extend earlier languages by\nadopting a geometric approach, along the lines of that pro-\nposed by Meredith et al. [4]. A passage of polyphonic mu-\nsic is represented in the proposed language as a set of mul-\ntidimensional points, generated by performing geometric\ntransformations on component patterns. The language in-\ntroduces the concept of a periodic mask, a generalisation\nof Deutsch and Feroe’s notion of a pitch alphabet, that can\nbe applied to any dimension of a geometric representation,\nallowing for both rhythms and pitch collections to be rep-\nresented parsimoniously in a uniform way.\nA coding language of the type proposed here could be\nused in music information retrieval to allow for the discov-\nery of patterns in a database that relate to a query pattern\non a deeper structural level than the surface. For example,\ntwo patterns might be perceived to be related because they\nhave a similar structure but use different pitch alphabets\n(e.g., where a melody is repeated in a different mode) or\nbecause they have the same pitch interval structure but dif-\nferent rhythms. If the music is ﬁrst encoded in a way that\nrepresents these structures, then such relationships can be\nautomatically discovered more efﬁciently.\n2. BACKGROUND\nThe earliest coding language for music was proposed in\n1968 by Simon and Sumner [7]. Simon and Sumner rec-\nognized that music is multidimensional and aimed to allow\nfor the description of patterns in melody, harmony, rhythm\nand form. Their language treats each of these dimensions\nas an independent series of symbols, chosen from alpha-\nbets, for which a compact representation can be derived in\nterms of certain basic element operators, such as SAME\nand NEXT. Simon and Sumner deﬁned the notion of a\ncyclic alphabet and recognized the usual tonal scales and\nchords as “common” pitch alphabets that are “ordered sets\nalready deﬁned in the culture”.\nIn his experiments on serial pattern learning, Restle [5]\nfound that subjects are particularly good at identifying\n“runs” (e.g., (2 3 4 5)) and “trills” (e.g., (5 4 5 4)) andtend to use these to segment a series of symbols. Res-\ntle explained this by proposing that runs and trills allow\nfor particularly simple generative descriptions. He pre-\nsented an “E -Itheory” wherein a rule consists of a set E\nof “events” (roughly equivalent to an alphabet) and a set I\nof “intervals” (in the musical sense). Runs are then the set\nof products of E-Irule systems in which Icontains only\none interval. Restle’s language uses the sequence opera-\ntors M (for mirror), T (for transposition) and R (for repeat)\nfor producing compact descriptions of sequences. For ex-\nample, if the numbers 1 to 6 represent a row of 6 lights that\ncan either be on or off, then the sequence (1 2 1 2 2 3 2 3\n6 5 6 5 5 4 5 4) can be encoded as M(T(R(T(1)))). Restle\nrepresented structures in his language as hierarchical trees\nand presented an analysis of the theme of Bach’s ﬁrst Two-\npart Invention (BWV 772) which describes its tonal and\nmotivic structure.\nDeutsch and Feroe’s [1] model is in the tradition of the\nserial pattern languages proposed by Restle [5], Leeuwen-\nberg [3] and Simon [6]. The language can be used to\nencode arhythmic monophonic sequences of pitches (i.e.,\nneither polyphony nor rhythm can be encoded). Structures\nare deﬁned to be sequences of the elementary operators\n‘same’ (s), ‘next’ (n) and ‘predecessor’ (p), that operate\nover alphabets, which are linearly ordered sets of symbols.\nStructures are decoupled from alphabets. A sequence,\nfS;\u000bg, is the application of a structure Sto an alphabet, \u000b;\nand a “sequence of notes” (actually a sequence of pitches)\ncan be generated by applying a sequence to a reference\nelement. Compound sequences can be built up from se-\nquences by using the sequence operators, ‘prime’ (pr), ‘ret-\nrograde’ (ret), ‘inversion’ (inv) and ‘alternation’ (alt). Al-\nphabets can be deﬁned as sequences and ‘stacked’ hierar-\nchically. For example, the C major scale would be deﬁned\nas a subset of the chromatic scale (denoted by ‘Cr’), using\nthe expression C =ff(\u0003; 2n2;n;3n2;n); Crg; cgand the\nC major triad could be deﬁned relative to the C major scale\nby the expressionff(\u0003; 2n2;n3);Cg; 1g.\nThe music encoding language presented in the next sec-\ntion extends and generalises these notions of structures and\nalphabets by re-casting them in geometric terms.\n3. A GEOMETRIC MUSIC ENCODING\nLANGUAGE\nIn the language proposed here, a passage of music is rep-\nresented as a set of notes. A note,n, is an ordered pair,\nn=ht;pi, where t= t(n) is the onset time of the note\nin tatums and p= p(n) is the note’s MIDI note number.\nA note is therefore a point in note space which is the two-\ndimensional Euclidean integer lattice in which the xco-\nordinate represents time in tatums and the yco-ordinate\nrepresents pitch in terms of MIDI note number.\nAvector,v, is an ordered 4-tuple, v=ht;p;Mt;Mpi,\nwheret= t(v)is the time component ofv,p= p(v)is the\npitch component ofv,Mt=Mt(v)is the time mask se-\nquence ofvandMp=Mp(v)is the pitch mask sequence\nofv.t(v)andp(v)are integers. Mt(v)andMp(v)are\nmask sequences.m(m; i)\n1ifi=nilreturn nil\n2j o(m); k 0\n3ifi\u0015o(m)\n4 while j < i\n5 k k+ 1\n6 j j+ s(m)[(k\u00001) modjs(m)j]\n7else\n8 while j > i\n9 k k\u00001\n10 j j\u0000s(m)[k modjs(m)j]\n11ifj=ireturn k\n12return nil\nFigure 1. The function m(m;i ), wheremis a mask and i\nis an integer or nil.\nu(m; i)\n1ifi=nilreturn nil\n2j o(m); k 0\n3ifi\u00150\n4 while k < i\n5 k k+ 1\n6 j j+ s(m)[(k\u00001) modjs(m)j]\n7else\n8 while k > i\n9 k k\u00001\n10 j j\u0000s(m)[k modjs(m)j]\n11return j\nFigure 2. The function u(m;i), wheremis a mask and i\nis an integer or nil.\nAmask sequence, M, is an ordered set of masks,\nM=hm1;m2;:::;m ki. A mask,m, is an ordered pair,\nm=ho;si, whereo= o(m) is an integer called the off-\nsetof the mask and s= s(m) is an ordered set of integers\ncalled the structure of the mask. Each integer in a mask\nstructure is called an interval. Ifmis a mask and iis an in-\nteger, then the function m(m;i )(see Figure 1) returns the\nmasked value ofifor the mask m; and the function u(m;i)\n(see Figure 2) returns the unmasked value ofifor the mask\nm.\nFigure 3 illustrates how a mask is used to map a subset\nof the integers onto the complete set of integers. In the up-\nper part of Figure 3, the mask h3;h2;2;1;2;2;2;1iiis ap-\nplied. The mask deﬁnes a periodic repeated pattern of in-\ntervals on the number line such that successive elements in\nthe pattern are mapped onto successive integers. For exam-\nple, the mask offset, 3, is mapped onto 0. The next integer\nthat does not map onto nilis 5, which therefore maps onto\n1, and so on. This particular mask, h3;h2;2;1;2;2;2;1ii,\ncan be used to represent the E[ major scale; and the struc-\nture of this mask,h2;2;1;2;2;2;1i, can be used to repre-\nsent the class of all major scales.\nFigure 3 also illustrates that the output of one mask can\nbe given as input to another. Thus we can take the range\nof the maskh3;h2;2;1;2;2;2;1iiand apply the function\nin Figure 1 to these values with the mask h4;h2;2;3iito\ngive the result shown in the lower part of Figure 3. InFigure 3. Applying the mask sequence,\nhh3;h2;2;1;2;2;2;1ii;h4;h2;2;3iii.\n(a)\n(b)\n(c)\nFigure 4. (a) The right-hand part of the ﬁrst bar of\nChopin’s ´Etude, Op. 10, No. 5. (b) A plausible rhythmic\nreduction of (a). (c) A plausible rhythmic reduction of (b).\nthis particular case, the mask h4;h2;2;3iican be used to\nrepresent a dominant triad in a seven-note scale. Apply-\ning this to the output of the mask h3;h2;2;1;2;2;2;1ii\ntherefore gives a representation of the dominant triad in\nE[major—i.e., the B[ major triad. If the topmost number\nline in Figure 3 represents MIDI note number, then MIDI\npitch 10 maps onto 4 in the middle number line, represent-\ning the fact that 10 is the ﬁfth scale degree in E [major.\nThe number 4 in the middle line is then mapped onto 0 in\nthe lowest number line, representing the fact that this scale\ndegree is now the root of the dominant triad. Thus, the\ndominant triad in E[ major can be represented by the mask\nsequence,hh3;h2;2;1;2;2;2;1ii;h4;h2;2;3iii. This ex-\nample demonstrates that a mask sequence can be used to\nrepresent the notion of hierarchically related pitch alpha-\nbets proposed by Deutsch and Feroe [1]. For example, the\nmask sequencehh3;h2;2;1;2;2;2;1ii;h4;h2;2;3iii cor-\nresponds to Deutsch and Feroe’s alphabet,\nff(\u0003; 2n2;n3);ff(\u0003; 2n2;n;3n2;n); Crg;e[gg; 5g:\nHowever, unlike Deutsch and Feroe’s pitch alphabets,\nmask sequences can also be used to represent rhythmic and\nmetric structures. For example, the crotchet metric level in\na 4/4 bar in which the tatum is a semiquaver can be repre-\nsented by the mask sequence hh0;h2ii;h0;h2iii. Similarly,\nthe dotted crotchet metric level in a 6/8 bar where the tatum\nis a semiquaver can be represented by hh0;h2ii;h0;h3iii.\nFigure 4 (a) shows the right-hand part of the ﬁrst bar\nof Chopin’s ´Etude, Op. 10, No. 5. Figure 4 (b) and\n(c) show plausible rhythmic reductions of the surface in\nFigure 4 (a). If we use the triplet semiquaver as the tatum\nduration, then the rhythm of Figure 4 (b) can be repre-\nsented by the mask sequence hh0;h2iii; and the rhythm\nof Figure 4 (c) could be represented by the mask sequence\nhh0;h2ii;h0;h2;1;3iii (orhh0;h4;2;6iii).\nIfMis a mask sequence and iis an integer, then the\nfunction m(M;i )(see Figure 5) returns the masked value\nofifor the mask sequence M(i.e., the result of applyingm(M; i)\n1k i; j 0\n2while j <jMj\n3 k m(M[j ]; k)\n4 j j+ 1\n5return k\nFigure 5. The function m(M;i )whereMis a mask se-\nquence andiis an integer.\nu(M; i)\n1k i; j jMj\u0000 1\n2while j\u00150\n3 k u(M[j ]; k)\n4 j j\u00001\n5return k\nFigure 6. The function u(M;i)whereMis a mask se-\nquence andiis an integer.\neach of the masks in M, in turn, with an initial input value\nofi). Conversely, the function u(M;i)in Figure 6 returns\nthe unmasked value of ifor the mask sequence M.\nIfvis a vector, then Mt(v)andMp(v)together de-\nﬁne the space,M(v ) =hMt(v);Mp(v)i, in which vis\ndeﬁned. If a vector, v, is in note space, then the vec-\ntor can be written as an ordered pair, ht(v);p(v)i, with-\nout specifying the time and pitch mask sequences (e.g.,\nh2;3i=h2;3;hh0;h1iii;hh0;h1iiii). Given a note n1and\na vectorv, then we can translaten1byvto give a new\nnote,n2. In order to do this, n1must ﬁrst be mapped onto\na point,q1, in the spaceM(v ).q1is then translated by vin\nthe usual geometric way to another point, q2, inM(v ). Fi-\nnally,q2is mapped back onto a note, n2, in note space. If\nnis a note and vis a vector, then this process of translation\ncan be accomplished using the algorithm in Figure 7.\nNote that, if vis a vector in any space other than note\nspace, then there will not, in general, be a unique vector\nin note space to which vis equivalent. For example, if\nv=h1;1;hh0;h2;3iii;hh0;h3;2iii, then Figure 8 shows\nthat there are 4 distinct vectors in note space to which v\nmight be equivalent, depending on the note that is being\ntranslated. A consequence of this is that, in general, there is\nno unique vector in any space that is equivalent to the sum\nof two or more vectors that are in different spaces. The\nT(n; v )\n1x1 m(M t(v);t(n))\n2y1 m(M p(v);p(n))\n3x2 x1+ t(v)\n4y2 y1+ p(v )\n5t2 u(M t(v); x2)\n6p2 u(M p(v); y2)\n7returnht2; p2i\nFigure 7. The function T(n;v )wherenis a note and vis\na vector.Figure 8. Equivalent vectors in note space for the vector,\nv=h1;1;hh0;h2;3iii;hh0;h3;2iii.\nresultant vector in note space of applying two vectors in\nsuccession to a note depends on the note itself. This means\nthat vectors in the sense deﬁned here cannot be added to-\ngether in the normal way. Instead, if a note is to be trans-\nlated by the sum of two or more vectors, the vector sum\nhas to be explicitly stated.\nSince the sum of two or more vectors is not necessarily\nequal to a unique vector in any space, it must be considered\na different type of object from a vector. We therefore de-\nﬁne a special type of object called a vector sum to represent\na sum of vectors. A vector sum is an ordered set of vec-\ntors, since vector addition in the sense deﬁned here is not\ncommutative. If we want to denote the sum of the vectors\nv1;v2;:::v k, applied in that order, then we simply write\nv1+v2+:::+vk. Ifwis the vector sum v1+v2+:::+vk,\nthenjwj=k,w[j] =vj+1andw=Pk\ni=1vi. Ifw1and\nw2are vector sums such that w1=v1;1+v1;2+:::+v1;m\nandw2=v2;1+v2;2+:::+v2;n, thenw1+w2=\nv1;1+v1;2+:::+v1;m+v2;1+v2;2+:::+v2;n. Ifv\nis a vector, then w(v)returns the vector sum that contains\njustv. In other words, w(v)typecasts a vector to a vec-\ntor sum. Ifwis a vector sum, then we deﬁne w(w) =w.\nTo simplify the notation, we allow for vector sums to be\nadded to vectors without explicit typecasting. Thus if vis\na vector and wis a vector sum, then v+w= w(v ) +w\nandw+v=w+ w(v ).\nAs an example, suppose we have three mask sequences\nand two vectors as follows\nM0=hh0;h1iii ;\nM1=hh0;h2;2;1;2;2;2;1iii;\nM2=hh0;h2;2;1;2;2;2;1ii;h0;h2;2;3iii;\nv1=h1;1;M0;M1iand\nv2=h1;1;M0;M2i:T(n; w )\n1n2 n\n2fori 0tojwj\u00001\n3 n2 T(n2; w[i])\n4return n2\nFigure 9. The function T(n;w )wherenis a note and wis\na vector sum.\nS(V)\n1ifV=hireturnfg\n2w w(V[0])\n3W fwg\n4fori 1tojVj\u0000 1\n5 w w+V[i]\n6 W W[fwg\n7return W\nFigure 10 . Function for computing the equivalent vector\nsum set,W, forV, whereVis either a vector sequence or\na vector sum sequence.\nIf we now translate the note h0;0ibyv1, then we get the\nnoteh1;2i, that is T(h0; 0i;v 1) =h1;2i. However, we\ncannot then translate h1;2ibyv2becauseh1;2iis not in\nthe space in which v2is deﬁned. If nis a note and wis a\nvector sum, then the function T(n;w )in Figure 9 can be\nused to compute the note that results when nis translated\nbyw.T(h0; 0i;v 1+v2)is therefore undeﬁned, whereas\nT(h0; 0i;v 2+v1) =h2;5i, illustrating the fact that vector\naddition is not commutative in this context.\nAvector sequence is an ordered set of vectors and a\nvector sum sequence is an ordered set of vector sums. For\nany given vector sequence or vector sum sequence there\nexists an equivalent vector sum set which can be computed\nusing the function in Figure 10. Any run of kidentical\nvectors,v, in a vector sequence can be encoded as kv. For\nexample,hv1;3v2;v3i=hv1;v2;v2;v2;v3i. A run of k\nidentical vector sums, w, in a vector sum sequence can\nsimilarly be encoded as kw.\nIfVis a vector set, vector sum set, vector sequence or\nvector sum sequence, then the function W(V ), deﬁned in\nFigure 11 returns the vector sum set that is equivalent to V.\nW(V )\n1ifVis a vector sequence or vector sum sequence\n2 W S(V)\n3else ifVis a vector set\n4 W ;\n5 for each v2V\n6 W W[fw(v)g\n7else\n8 W V\n9return W\nFigure 11. Function for computing the equivalent vector\nsum set,W, forV, whereVis a vector sequence, a vector\nsum sequence, a vector set or a vector sum set.T(n;V)\n1W W(V )\n2N ;\n3for each w2W\n4 N N[fT(n; w )g\n5return N\nFigure 12. Function for translating a note, n, by a vec-\ntor set, vector sequence, vector sum set or vector sum se-\nquence,V.\nT(N;V)\n1N2 ;\n2for each n2N\n3 N2 N2[T(n;V)\n4return N2\nFigure 13. Function for translating a note set, N, by a\nvector set, vector sequence, vector sum set or vector sum\nsequence,V.\nA single note, n, or a set of notes, N, can be used to gen-\nerate a set of notes by translating it by a vector set, a vector\nsum set, a vector sequence or a vector sum sequence. Fig-\nures 12 and 13 show functions for carrying out these types\nof translation. A vector sum set therefore acts as a gener-\nalisation of Deutsch and Feroe’s concept of a “structure”.\nMoreover, the combination of a note and a vector sum set\nto generate a set of notes is a generalisation of Deutsch and\nFeroe’s notion of combining a structure with a reference\npitch to generate a sequence of pitches.\nThe function P(X), deﬁned in Figure 14 is a general-\nisation of Deutsch and Feroe’s “prime” sequence opera-\ntor, “pr”. The single argument, X, is an ordered set in\nwhich each element is a vector set, vector sum set, vector\nsequence or vector sum sequence. The ﬁrst step in P(X)\nis to convert each element X[i] into its equivalent vector\nsum set (lines 1–3). The zero vector sum is then included\nin each vector sum set, Y[i](lines 4–6). P(X)then re-\nturns a set containing a vector sum for each element of\nthen-ary Cartesian product of the vector sum sets in Y\n(lines 7–14). Note that if AandBare sequences such\nthatA=ha1;a2;:::;a miandB=hb1;b2;:::;b nithen\nA\bB=ha1;a2;:::;a m;b1;b2;:::;b ni.\nDeutsch and Feroe’s “ret” and “inv” sequence opera-\ntors correspond to reﬂection in the geometric language pro-\nposed here: “ret” corresponds to reﬂection in an axis par-\nallel to the pitch axis, while “inv” corresponds to reﬂection\nin an axis parallel to the time axis. The functions,\nRp(v) =ht(v);\u0000p(v );Mt(v);Mp(v)iand\nRt(v) =h\u0000t(v );p(v);Mt(v);Mp(v)i\nreﬂect vectors in the time axis and pitch axis, respectively.\nThe functions in Figure 15 can be used to reﬂect vector\nsums and the functions in Figure 16 can be used to reﬂect\nsequences or sets of vectors or vector sums. Note that\nthere is no necessity for a function analogous to Deutsch\nand Feroe’s “alt”, because the music is represented as aP(X)\n1Y hi\n2fori 0tojXj\u0000 1\n3Y Y\bhW(X[i])i\n4v0 h0; 0i\n5fori 0tojYj\u00001\n6Y[i] Y[i][fw(v0)g\n7W1 Y[0]\n8fori 1tojYj\u00001\n9 W2 ;\n10 for each w12W1\n11 for each w22Y[i]\n12 W2 W2[fw1+w2g\n13 W1 W2\n14return W1\nFigure 14. The function P(X)whereXis an ordered set\nin which each element is a vector set, vector sum set, vector\nsequence or vector sum sequence.\nRp(w)\n1w2 w(R p(w[0]))\n2fori 1tojwj\u00001\n3 w2 w2+ Rp(w[i])\n4return w2\nRt(w)\n1w2 w(R t(w[0]))\n2fori 1tojwj\u00001\n3 w2 w2+ Rt(w[i])\n4return w2\nFigure 15. Functions for reﬂection. wis a vector sum.\nsetof geometrical points rather than a sequence ofsym-\nbols. There is also no need for a scaling function to repre-\nsent augmentation or diminution, since this can be accom-\nplished using appropriate time mask sequences.\n4. EXAMPLES\nFigure 17 shows one of the examples used by Deutsch and\nFeroe [1, p. 504] to illustrate their model. This pattern can\nbe encoded as T(n 1;P(h3v1i;hv2i))where\nn1=h1;60i;\nv1=h1;1;M1;M2i;\nv2=h\u00001;\u00001i;\nM1=hh1;h2ii;h0;h3iii and\nM2=hh0;h2;2;1;2;2;2;1ii;h0;h2;2;3iii:\nGiven that the function, U(S 1;S2;:::S k)returns the\nunion of sets S1;S2;:::S k, then the pattern in Figure 4\ncan be encoded as follows:\nU(T(n 1;P(hv 1i;hv2i));T(n2;P(hv 3i));T(n3;P(h2v 1i;hv2i)))Rp(V)\n1W1 W(V )\n2W2 ;\n3for each w2W1\n4 W2 W2[fRp(w)g\n5return W2\nRt(V)\n1W1 W(V )\n2W2 ;\n3for each w2W1\n4 W2 W2[fRt(w)g\n5return W2\nFigure 16. Functions for reﬂection. Vis a vector set, vec-\ntor sum set, vector sequence or vector sum sequence.\nFigure 17. Example pattern used by Deutsch and Feroe [1,\np. 504].\nwhere\nn1=h0;90i;\nn2=h4;87i;\nn3=h6;85i;\nv1=h1;\u00001;M1;M2i;\nv2=h1;1;M3;M2i;\nv3=h1;1;M3;M4i;\nM1=hh0;h2iii ;\nM2=hm1;h0; s 1ii;\nM3=hh0;h1iii ;\nM4=hm1;h3; s 1ii;\nm1=h6;h2;2;1;2;2;2;1iiand\ns1=h2;2;3i:\nFigure 18 shows bars 320–322 from the ﬁrst movement\nof Beethoven’s Sonata in E[, Op. 7. This passage can be\nencoded as U(A;B )where\nA= T(n 1;P(h2v 1i;h5v2i));\nB= T(n 2;P(h17h1; 0ii)) ;\nn1=h0;65i;\nn2=h0;58i;\nv1=h0;1;M1;M2i;\nv2=h1;\u00001;M1;M2i;\nM1=h0;h3ii and\nM2=hh3;h2;2;1;2;2;2;1ii;h6;h2;2;3iii:\nFigure 18. Bars 320–322 of Beethoven’s Sonata in E[,\nOp. 7, 1st. mvt.5. CONCLUSIONS AND FUTURE WORK\nThis paper has introduced a geometric coding language\nfor describing musical structure that extends Deutsch and\nFeroe’s [1] model and recasts it in geometrical terms, al-\nlowing rhythmic and pitch structure in polyphonic music to\nbe expressed as transformations on sets of points. The lan-\nguage introduces the concepts of masks, mask sequences\nand masked spaces which generalise Deutsch and Feroe’s\nnotion of hierarchical alphabets to the time dimension, al-\nlowing rhythms and pitch collections to be represented par-\nsimoniously in a uniform way. A Java implementation of\nthe language and some extended encoding examples are\nfreely available online.1\nThe primary design goal of the language described here\nis that it should allow for the formulation of minimal-\nlength descriptions of musical works. There are many\nways in which the language could be developed further.\nFor example, decoupling vector co-ordinate values from\nspaces could permit repetitions of vector co-ordinate value\npatterns in different spaces to be represented more parsi-\nmoniously. There are also cases where structure might be\nexpressed more compactly if pitch class were decoupled\nfrom pitch height. Such decoupling of information types\nand other potentially useful modiﬁcations will be explored\nin the near future. Longer-term goals include\n\u000fto develop algorithms for automatically inferring en-\ncodings from note sets,\n\u000fto develop appropriate measures for description\nlength and\n\u000fto explore the relationship between such an encod-\ning language and the way that musical structure is\nrepresented cognitively and neurologically.\n6. REFERENCES\n[1] D. Deutsch and J. Feroe. The internal representation of pitch se-\nquences in tonal music. Psychol. Rev., 88(6):503–522, 1981.\n[2] K. Koffka. Principles of Gestalt Psychology. Harcourt Brace, New\nYork, 1935.\n[3] E. L. J. Leeuwenberg. A perceptual coding language for visual and\nauditory patterns. Am. J. Psychol., 84(3):307–349, 1971.\n[4] D. Meredith, K. Lemstr ¨om, and G. A. Wiggins. Algorithms for dis-\ncovering repeated patterns in multidimensional representations of\npolyphonic music. J. New Music Res., 31(4):321–345, 2002.\n[5] F. Restle. Theory of serial pattern learning: Structural trees. Psychol.\nRev., 77(6):481–495, 1970.\n[6] H. A. Simon. Complexity and the representation of patterned se-\nquences of symbols. Psychol. Rev., 79(5):369–382, 1972.\n[7] H. A. Simon and R. K. Sumner. Pattern in music. In B. Kleinmuntz,\neditor, Formal representation of human judgment. Wiley, New York,\n1968.\n[8] D. Temperley. Music and Probability. MIT Press, Cambridge, MA.,\n2007.\n[9] P. A. van der Helm and E. L. J. Leeuwenberg. Accessibility: A cri-\nterion for regularity and hierarchy in visual pattern codes. J. Math.\nPsychol., 35:151–213, 1991.\n1Java code available at http://tinyurl.com/bl7rfgd, longer\nexamples available at http://tinyurl.com/blqkgmq."
    },
    {
        "title": "Learning to Embed Songs and Tags for Playlist Prediction.",
        "author": [
            "Joshua L. Moore",
            "Shuo Chen 0008",
            "Thorsten Joachims",
            "Douglas R. Turnbull"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416966",
        "url": "https://doi.org/10.5281/zenodo.1416966",
        "ee": "https://zenodo.org/records/1416966/files/MooreCJT12.pdf",
        "abstract": "Automatically generated playlists have become an impor- tant medium for accessing and exploring large collections of music. In this paper, we present a probabilistic model for generating coherent playlists by embedding songs and social tags in a unified metric space. We show how the embedding can be learned from example playlists, pro- viding the metric space with a probabilistic meaning for song/song, song/tag, and tag/tag distances. This enables at least three types of inference. First, our models can gener- ate new playlists, outperforming conventional n-gram mod- els in terms of predictive likelihood by orders of magni- tude. Second, the learned tag embeddings provide a gener- alizing representation for embedding new songs, allowing it to create playlists even for songs it has never observed in training. Third, we show that the embedding space pro- vides an effective metric for matching songs to natural- language queries, even if tags for a large fraction of the songs are missing.",
        "zenodo_id": 1416966,
        "dblp_key": "conf/ismir/MooreCJT12",
        "keywords": [
            "automatically generated playlists",
            "accessing and exploring large collections of music",
            "provides a unified metric space",
            "provides a probabilistic meaning",
            "generates new playlists",
            "conventional n-gram models",
            "generates playlists even for songs it has never observed",
            "provides a generalizing representation",
            "provides an effective metric",
            "matches songs to natural-language queries"
        ],
        "content": "LEARNING TO EMBED SONGS AND TAGS FOR PLAYLIST PREDICTION\nJoshua L. Moore, Shuo Chen, Thorsten Joachims\nCornell University, Dept. of Computer Science\nfjlmo|shuochen|tjg@cs.cornell.eduDouglas Turnbull\nIthaca College, Dept. of Computer Science\ndturnbull@ithaca.edu\nABSTRACT\nAutomatically generated playlists have become an impor-\ntant medium for accessing and exploring large collections\nof music. In this paper, we present a probabilistic model\nfor generating coherent playlists by embedding songs and\nsocial tags in a uniﬁed metric space. We show how the\nembedding can be learned from example playlists, pro-\nviding the metric space with a probabilistic meaning for\nsong/song, song/tag, and tag/tag distances. This enables at\nleast three types of inference. First, our models can gener-\nate new playlists, outperforming conventional n-gram mod-\nels in terms of predictive likelihood by orders of magni-\ntude. Second, the learned tag embeddings provide a gener-\nalizing representation for embedding new songs, allowing\nit to create playlists even for songs it has never observed in\ntraining. Third, we show that the embedding space pro-\nvides an effective metric for matching songs to natural-\nlanguage queries, even if tags for a large fraction of the\nsongs are missing.\n1. INTRODUCTION\nMusic consumers can store thousands of songs on their\ncomputer or smart phone. In addition, cloud-based ser-\nvices like Rhapsody or Spotify give instant and on-demand\naccess to millions of songs. While these technologies pro-\nvide powerful new ways to access music, they can also\noverwhelm users by giving them too much choice [15].\nThis has created substantial interest in automatic playlist\nalgorithms that can help consumers explore large collec-\ntions of music. Companies like Apple and Pandora have\ndeveloped successful commercial playlist algorithms, but\nrelatively little is known about how these algorithms work\nand how well they perform in rigorous evaluations. Com-\nparably little scholarly work has been done on automated\nmethods for playlist generation (e.g., [1,4,10,12,14]), and\nthe results to date indicate that it is far from trivial to oper-\nationally deﬁne what makes a playlist coherent.\nMost approaches to automatic playlist creation rely on\ncomputing some notion of music similarity between pairs\nof songs. Numerous similarity functions have been pro-\nposed and are often based on the analysis of audio con-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.tent [9,13], social tag information [8], web document min-\ning [6], preference-based ratings data [11], or some com-\nbination of these data sources. Given a music similarity\nalgorithm, a playlist is created by ﬁnding the most similar\nsongs to a given seed song or set of seed songs.\nIn this paper, we explore the idea of learning a playlist\nmodel that does not require an external similarity mea-\nsure and that is trained directly on the data of interest,\nnamely historical playlists. In particular, we extend the\nLogistic Markov Embedding (LME) [3] approach to prob-\nabilistic sequence modeling to incorporate social tags, uni-\nfying song and tag embeddings in a single Euclidean space.\nThis provides a probabilistically well-founded and con-\nstructive way to compute meaningful distances between\npairs of songs, pairs of tags, and songs and tags. We show\nthat this joint embedding is useful not only for probabilis-\ntically sound playlist generation, but also for a variety of\nother music information retrieval tasks such as corpus vi-\nsualization, automatic tagging, and keyword-based music\nretrieval.\nAn efﬁcient C implementation, a demo, and data are\navailable at http://lme.joachims.org.\n2. RELATED WORK\nAutomatically generated playlists are a key component in\nseveral commercial systems. For example, Pandora relies\non content-based music analysis by human experts [16]\nwhile Apple iTunes Genius relies on preference ratings and\ncollaborative ﬁltering [2]. What is not known is the mech-\nanism by which the playlist algorithms are used to order\nthe set of relevant songs, nor is it known how well these\nplaylist algorithms perform in rigorous evaluations.\nIn the scholarly literature, two recent papers address the\ntopic of playlist prediction. First, Maillet et al. [10] formu-\nlate the playlist ordering problem as a supervised binary\nclassiﬁcation problem that is trained discriminatively. Pos-\nitive examples are pairs of songs that appeared in this order\nin the training playlists, and negative examples are pairs of\nsongs selected at random which do not appear together in\norder in historical data. Second, McFee and Lanckriet [12]\ntake a generative approach by modeling historical playlists\nas a Markov chain. That is, the probability of the next\nsong in a playlist is determined only by acoustic and/or\nsocial-tag similarly to the current song. Our approach is\nsubstantially different from both [10] and [12], since we\ndo not require any acoustic or semantic information about\nthe songs.While relatively little work has been done on explic-\nitly modeling playlists, considerably more research has fo-\ncused on embedding songs (or artists) into a similarity-\nbased music space (e.g., [4, 9, 14, 18].) For example, Platt\net al. use semantic tags to learn a Gaussian process kernel\nfunction between pairs of songs [14]. More recently, We-\nston et al. learn an embedding over a joint semantic space\nof audio features, tags and artists by optimizing perfor-\nmance metrics for various music retrieval tasks [18]. Our\napproach, however, differs substantially from these exist-\ning methods, since it explicitly models the sequential na-\nture of playlists in the embedding. Recently and indepen-\ndently, [1] also proposed a sequential embedding model.\nHowever, their model does not include tags.\nModeling playlists as a Markov chain connects to a large\nbody of work on sequence modeling in natural language\nprocessing (NLP) and speech recognition. Smoothed n-\ngram models (see e.g. [5]) are the most commonly used\nmethod in language modeling, and we will compare against\nsuch models in our experiments.\n3. PROBABILISTIC EMBEDDING OF PLAYLISTS\nOur goal is to estimate a generative model of coherent\nplaylists, which will enable us to efﬁciently sample new\nplaylists. More formally, given a collection S=fs1;:::;sjSjg\nof songssi, we would like to estimate the distribution Pr(p)\nof coherent playlists p= (p[1];:::;p[kp]). Each element p[i]\nof a playlist refers to one song from S.\nA natural approach is to model playlists as a Markov\nchain, where the probability of a playlist p= (p[1];:::;p[kp])\nis decomposed into the product of transition probabilities\nPr(p[i]jp[i\u00001])between adjacent songs p[i\u00001]andp[i].\nPr(p) =kpY\ni=1Pr(p[i]jp[i\u00001]) (1)\nFor ease of notation, we assume that p[0]is a dedicated start\nsymbol. Such bi-gram (or, more generally, n-gram) models\nhave been widely used in language modeling for speech\nrecognition and machine translation with great success [5].\nIn these applications, the O(jSjn)transition probabilities\nPr(p[i]jp[i\u00001])are estimated from a large corpus of text\nusing sophisticated smoothing methods.\nWhile such n-gram approaches can be applied to playlist\nprediction in principle, there are fundamental differences\nbetween playlists and language. First, playlists are less\nconstrained than language, so that transition probabilities\nbetween songs are closer to uniform. This means that we\nneed a substantially larger training corpus to observe all of\nthe (relatively) high-probability transitions even once. Sec-\nond, and in contrast to this, we have orders of magnitude\nless playlist data to train from than we have written text.\nTo overcome these problems, we propose a Markov-\nchain sequence model that produces a generalizing repre-\nsentation of songs, song sequences, and social tags. Un-\nlike n-gram models that treat words as atomic units with-\nout metric relationships between each other, our approach\nseeks to model coherent playlists as paths through a latentspace. In particular, songs are embedded as points in this\nspace so that Euclidean distance between songs reﬂects the\ntransition probabilities. Similarly, each social tag is repre-\nsented as a point in this space, summarizing the average\nlocation of songs with that tag. The key learning prob-\nlem is to determine the location of each song and tag using\nexisting playlists as training data. Once songs and tags\nare embedded, our model can assign meaningful transition\nprobabilities even to those transitions that were not seen in\nthe training data, and it can also reason about tagged songs\nthat were never seen before.\nIn the following we start by reviewing the basic LME\nmodel of Pr(p), and then extend this model to incorporate\nsocial tags.\n3.1 Embedding Model for Songs\nThe basic LME model [3] represents each song sas a sin-\ngle vectorX(s)ind-dimensional Euclidean space M. The\nkey assumption of our model is that the transition prob-\nabilities Pr(p[i]jp[i\u00001])are related to the Euclidean dis-\ntancejjX(p[i])\u0000X(p[i\u00001])jj2betweenp[i\u00001]andp[i]in\nMthrough the following logistic model:\nPr(p[i]jp[i\u00001]) =e\u0000jjX(p[i])\u0000X(p[i\u00001])jj2\n2\nPjSj\nj=1e\u0000jjX(sj)\u0000X(p[i\u00001])jj2\n2(2)\nThis is illustrated in the ﬁgure to the right, showing that\ntransitioning from sto a nearby point s0is more likely\ns'' s' \ns \nthan transitioning to a point s00that\nis further away. We will typically\nabbreviate the partition function\nin the denominator as Z(p[i\u00001]),\nand the distancejjX(s)\u0000X(s0)jj2\nbetween two songs in embedding\nspace as \u0001(s;s0)for brevity. Us-\ning a Markov model with this tran-\nsition distribution, we can now deﬁne the probability of an\nentire playlist of a given length kas\nPr(p) =kpY\ni=1Pr(p[i]jp[i\u00001]) =kpY\ni=1e\u0000\u0001(p[i];p[i\u00001])2\nZ(p[i\u00001]):(3)\nThe LME seeks to discover an embedding of the songs into\nthis latent space which causes “good” playlists to have high\nprobability of being generated by this process. This is in-\nspired by collaborative ﬁltering methods such as [7], which\nsimilarly embed users and items into a latent space to pre-\ndict users’ ratings of items. However, our approach dif-\nfers from these methods in that we wish to predict paths\nthrough the space, as opposed to independent item ratings.\nIn order to learn the embedding of songs, we use a sam-\npleD= (p 1;:::;pn)of existing playlists as training data\nand take a maximum a posteriori (MAP) approach to learn-\ning. Denoting with Xthe matrix of embedding vectors for\nall songs in the collection S, this leads to the following\ntraining problem\nX= argmax\nX2<jSj\u0002dY\np2DkpY\ni=1e\u0000\u0001(p[i];p[i\u00001])2\nZ(p[i\u00001])\u0001jSjY\ni=1e\u0000\u0015jjX (si)jj2\n2;(4)where we also added a zero-mean Normal prior as regu-\nlarizer to control overﬁtting (see term after the dot). The\nparameter\u0015controls how heavily the embedding is regu-\nlarized. While the optimization problem is not concave, we\nhave already shown in [3] how to efﬁciently and robustly\nﬁnd good optima using a stochastic gradient approach.\n3.2 Embedding Model for Songs and Tags\nThe previous model is very general in that it does not re-\nquire any features that describe songs. However, this is\nalso a shortcoming, since it may ignore available informa-\ntion. We therefore now extend the LME to include tags as\nprior information. The new model will provide reasonable\nembeddings even for songs it was not trained on, and it will\ndeﬁne a uniﬁed metric space for music retrieval based on\nquery tags.\nThe key idea behind the new model is that the tags T(s)\nof songsinform the prior distribution of its embedding\nlocationX(s). In particular, each tag tis associated with\na Normal distribution N(M(t);1\n2\u0015Id)with meanM(t).\nHere,Idis thedbydidentity matrix and we will see soon\nthat\u0015again behaves like a regularization parameter. For a\nsong with multiple tags, we model the prior distribution of\nits embedding as the average of the Normal distribution of\nits tagsT(s), while keeping the variance constant.\nPr(X (s)jT (s)) =N0\n@1\njT(s)jX\nt2T(s)M(t);1\n2\u0015Id1\nA (5)\nNote that this deﬁnition of Pr(X (s)jT (s)) nicely gen-\neralizes the regularizer in (4), which corresponds to an\n“uninformed” Normal prior Pr(X (s)) =N(0;1\n2\u0015Id)cen-\ntered at the origin of the embedding space. The tag-based\nTag 1: Pop Music \nTag 2: 1980’s  \nTag 3: Male vocals Actual position  \nof “Billie Jean”  \nprior distribution is illus-\ntrated in the ﬁgure to the\nright. In this example,\nthe song “Billie Jean” has\nthe three tags “pop mu-\nsic”, “male vocals” and\n“1980s”. Each tag has a\nmeanM(t)as depicted, and\nPr(X (s)jT (s)) is centered\nat the average of the tag means, providing the prior for the\nembedding of “Billie Jean”. Without any training data, the\nmost likely location is the center of the prior, but with more\nobserved training data the embedding may move further\naway as necessary.\nLetMbe the matrix of all tag means, we obtain the\nfollowing maximum a posteriori estimate for the tag-based\nLME analogous to the basic LME model:\n(X;M ) = argmax\nX;MPr(DjX)\u0001Pr(XjM) (6)\n= argmax\nX;MY\np2DkpY\ni=1e\u0000\u0001(p[i];p[i\u00001])2\nZ(p[i\u00001])\u0001jSjY\ni=1e\u0000\u0015jjX (s)\u0000P\nt2T(s)M(t)\njT(s)jjj2\n2\nNote that we now optimize jointly over the song locations\nX(s)and tag locations M(t). In this way, the tag-basedyessmall yesbig\nAppearance Threshold 20 5\nNum of Songs 3,168 9,775\nNum of Train Trans 134,431 172,510\nNum of Test Trans 1,191,279 1,602,079\nTable 1: Statistics of the playlists datasets.\nLME model yields a meaningful probabilistic interpreta-\ntion of distances not only among songs, but also among\nsongs and tags. The following experiments exploit this for\nlocating new songs and for tag-based music retrieval.\n4. EXPERIMENTS\nThe playlists and tag data we used for our experiments are\nrespectively crawled from Yes.com andLast.fm.\nYes.com is a website that provides radio playlists from\nhundreds of radio stations in the United States. By using\nthe web based API1, one can retrieve the playlist record of\na speciﬁed station for the last 7 days. We collected as many\nplaylists as possible by specifying all possible genres and\ngetting playlists from all possible stations. The collection\nlasted from December 2010 to May 2011. This lead to\na dataset of 75,262 songs and 2,840,553 transitions. To\nget datasets of various sizes, we pruned the raw data so\nthat only the songs with a number of appearances above\na certain threshold are kept. We then divide the pruned\nset into a training set and a testing set, making sure that\neach song has appeared at least once in the training set.\nWe report results for two datasets, namely yessmall and\nyesbig, whose basic statistics are shown in Table 1.\nLast.fm provides tag information for songs, artists and\nalbums that is contributed by its millions of users. For each\nof the songs in our playlists dataset, we query the Last.fm\nAPI2for the name of the artist and the song, retrieving the\ntop tags. We then prune the tag set by only keeping the top\n250 tags with the most appearances across songs. Note that\nLast.fm did not provide any tags for about 20% of songs.\nUnless noted otherwise, experiments use the follow-\ning setup. Any model (either the LME or the base-\nline model) is ﬁrst trained on the training set and then\ntested on the test set. We evaluate test performance us-\ning average log-likelihood as our metric. It is deﬁned as\nlog(Pr(D test))=N test, whereNtestis the number of transi-\ntions in test set.\n4.1 What does the embedding space look like?\nBefore starting the quantitative evaluation of our method,\nwe ﬁrst want to give a qualitative impression of the embed-\ndings it produces. Figure 1 shows the two-dimensional em-\nbedding of songs and tags according to (6) for the yessmall\ndataset. The top 50 genre tags are labeled, and the lighter\npoints represent songs.\nOverall, the embedding matches our intuition of what a\nsemantic music space should look like. The most salient\n1http://api.yes.com\n2http://www.last.fm/api-6 -4 -2 0 2 4 6\n-4-202468\nrock\npopalternative\nclassic rockalternative rockhard rock\ndancepop rock\nsinger-songwriter\ncountryoldies\neasy listeningsoft rockmetal\nindie\nchilloutballadsoundtracksoul\nrnb\nacousticheavy metal\ntop 40rock n roll\nlivehip-hop\nmodern countrygrunge\nprogressive rock\nchristian hip hopindie rock\nrap\nbluespunk\nelectronicr&balternative metal\nchristian rockrock and roll\nblues rockemo\nfunkjazzpop-rockmelancholicpost-grunge\nfolkballads90s rockFigure 1: 2D embedding for yessmall. The top 50 genre\ntags are labeled; lighter points represent songs.\n-9-8-7-6-5\n 2  5 10  25  50 100Avg. log likelihood\nd 2  5 10  25  50 100\ndLME\nUniform\nUnigram\nBigram\nFigure 2: Log-likelihood on the test set for the LME and\nthe baselines on yessmall (left) and yesbig(right).\nobservation is that the embedding of songs does not uni-\nformly cover the space, but forms clusters as expected.\nThe location of the tags provides interesting insight into\nthe semantics of these clusters. Note that semantically syn-\nonymous tags are typically close in embedding space (e.g.\n“christian rock” and “christian”, “metal rock” and “heavy\nmetal”). Furthermore, location in embedding space gen-\nerally interpolates smoothly between related genres (e.g.\n“rock” and “metal”). Note that some tags lie outside the\nsupport of the song distribution. The reason for this is\ntwofold. First, we will see below that a higher-dimensional\nembedding is necessary to accurately represent the data.\nSecond, many tags are rarely used in isolation, so that some\ntags may often simply modify the average prior for songs.\nTo evaluate our method and the embeddings it produces\nmore objectively and in higher dimensions, we now turn to\nquantitative experiments.\n4.2 How does the LME compare to n-gram models?\nOur ﬁrst quantitive experiment explores how the general-\nization accuracy of the LME compares to that of traditional\nn-gram models from natural language processing (NLP).\nThe simplest NLP model is the Unigram Model, where\n-9-8-7-6-5-4-3\n 0  2  4  6  8  10 0 0.2 0.4 0.6 0.8 1Avg. log likelihood\nFraction of transitions\nFreq. of transitions in training setLME log-likelihood\nBigram log-likelihood\nFraction of transitionsFigure 3: Log-likelihood on testing transitions with re-\nspect to their frequencies in the training set for yessmall.\nthe next song is sampled independently of the previous\nsongs. The probability p(si)of each song siis estimated\nfrom the training set as p(si) =niP\njnj, whereniis the\nnumber of appearances of si.\nTheBigram Model conditions the probability of the\nnext song on the previous song similar to our LME model.\nHowever, the transition probabilities p(sjjsi)of each song\npair are estimated separately, not in a generalizing model\nas in the LME. To address the the issue of data sparsity\nwhen estimating p(sjjsi), we use Witten-Bell smoothing\n(see [5]) as commonly done in language modeling.\nAs a reference, we also report the results for the Uni-\nform Model, where each song has equal probability 1=jSj.\nFigure 2 compares the log-likelihood on the test set of\nthe basic LME model to that of the baselines. The x-axis\nshows the dimensionality dof the embedding space. For\nthe sake of simplicity and brevity, we only report the re-\nsults for the model from Section 3.1 trained without reg-\nularization (i.e. \u0015= 0). Over the full range of dthe\nLME outperforms the baselines by at least two orders of\nmagnitude in terms of likelihood. While the likelihoods on\nthe big dataset are lower as expected (i.e. there are more\nsongs to choose from), the relative gain of the LME over\nthe baselines is even larger for yesbig.\nThe tag-based model from Section 3.2 performs com-\nparably to the results in Figure 2. For datasets with less\ntraining data per song, however, we ﬁnd that the tag-based\nmodel is preferable. We explore the most extreme case,\nnamely songs without any training data, in Section 4.4.\nAmong the conventional sequence models, the bigram\nmodel performs best on yessmall. However, it fails to beat\nthe unigram model on yesbig(which contains roughly 3\ntimes the number of songs), since it cannot reliably es-\ntimate the huge number of parameters it entails. Note\nthat the number of parameters in the bigram model scales\nquadratically with the number of songs, while it scales only\nlinearly in the LME model. The following section analyzes\nin more detail where the conventional bigram model fails,\nwhile the LME shows no signs of overﬁtting.\n4.3 Where does the LME win over the n-gram model?\nWe now analyze why the LME beats the conventional bi-\ngram model. In particular, we explore to what extent-7-6.8-6.6-6.4-6.2-6-5.8-5.6\n 0.0001  0.001  0.01  0.1  1  10  100  1000Avg. log likelihood\nλd = 2\nd = 5\nd = 10\nd = 25\nUniformFigure 4: Log-likelihood of predicting transitions for new\nsongs for different dand\u0015.\nthe generalization performance of the methods depends on\nwhether (and how often) a test transition was observed in\nthe training set. The ability to produce reasonable prob-\nability estimates even for transitions that were never ob-\nserved is important, since even in yessmall about 64% of\ntest transitions were not at all observed in our training set.\nFor both the LME and the bigram model, the lines in\nFigure 3 show the log-likelihood of the test transitions con-\nditioned on how often that transition was observed in the\ntraining set of yessmall. The bar graph illustrates what\npercentage of test transitions had that given number of oc-\ncurences in the training set (i.e. 64% for zero). It can\nbe seen that the LME performs comparably to the bigram\nmodel for transitions that were seen in the training set at\nleast once, but it performs substantially better on previ-\nously unseen transitions. This is a key advantage of the\ngeneralizing representation that the LME provides, since it\nprovides an informed way of assigning transition probabil-\nities to all pairs of songs.\n4.4 Can the tag model coldstart new songs?\nAny playlist generator will encounter new songs it has not\nbeen trained on. Fortunately, it is easy to impute an embed-\nding for new songs in our tag-based LME model. Given a\nnew songswith tagsT(s), the most likely embedding lo-\ncation according our probabilistic model is\nX(s) =1\njT(s)jX\nt2T(s)M(t): (7)\nTo evaluate performance on new songs, we take the yessmall\ndataset and randomly withhold a subset of 30% (951) of\nthe songs which have at least one tag each. We test on\nthese songs and train the tag-based LME on the remaining\nsongs. In particular, we test on transitions from training to\ntest songs, having our model predict based on the imputed\ntest-song location which one of the 951 songs was played.\nThe only valid baseline for this experiment is the uni-\nform model, since we have no history for the testing songs.\nThe results are shown in Figure 4 for various dimension-\nalities and regularization parameters \u0015. Over all parameter\nsettings, the LME outperforms the baseline substantially.\nComparing Figure 4 with Figure 2, the gain over uniform\nfor new songs is still roughly half of that for songs that\nAll\nGenre\nEmotion\nMusical\nYear\nOther0.450.50\n.550.60.650.70.750.80.850.9AUC\nAll\nGenre\nEmotion\nMusical\nYear\nOther00.050.10\n.150.20.250.30.350.40.45\nRandom\nFrequency\nLME\nPrec@\n10Figure 5: Average AUC (left) and precision at 10 (right)\nacross tag categories for random and frequency baselines\nand LME. Error bars indicate +/- 1 standard error.\nthe LME was trained on. This demonstrates that the em-\nbedding of the tags captures a substantial amount of the\nplaylist semantics, generalizing well even for new songs.\n4.5 Can the embedding space be used for retrieval?\nAs already demonstrated in the previous section, a pow-\nerful property of our model is that it results in a similar-\nity metric that uniﬁes tags and songs – namely, the Eu-\nclidean distance of the corresponding points in the embed-\nding. This leads to a natural method for retrieval of songs\nbased on query tags: rank songs by their Euclidian dis-\ntance to the query tag(s). Note that this method can retrieve\nsongs even though they are not manually tagged with any\nof the query tags.\nTo evaluate the effectiveness of the embedding space\nfor retrieval, we now evaluate how well untagged songs\ncan be retrieved using queries that consist of a single tag.\nThe experiment is set up as follows. We pooled the train\nand test partitions of the yessmall dataset and then ran-\ndomly split all songs with at least one tag into 5 parti-\ntions. Following a 5-fold cross-validation setup, we re-\nmoved the tags from the songs in one of the partitions,\ntrained the tag-based LME on the now untagged songs plus\nthe tagged songs from the other 4 partitions, and then com-\nputed the query-tag rankings over the untagged songs. For\neach query tag, we computed the average (over folds) ROC\nArea (AUC) and Precision@10.\nFigure 5 shows the results for the LME and for two\nbaselines: a random ranking of all held-out songs and a\nranking of the held-out songs in order of decreasing fre-\nquency of appearance in the data set. We separated (by\nhand) each of the 250 query tags into one of ﬁve categories:\ngenre tags (91 tags like rock, hip hop, etc.), emotion tags\n(35 tags: sad, happy, dark, upbeat etc.), musical and in-\nstrumental tags (23 tags: male vocalist, guitar, major key\ntonality...), years and decades (17 tags), and other tags (84\ntags including awesome, loved, catchy, and favorites). For\nbrevity, we only report results for a model with dimension\n25 and\u0015= 10. However, similar to the results in Figure 4,\nwe ﬁnd that the exact choice of these parameters is not cru-\ncial. For example, the best unregularized model was never\nmore than 4 percentage points worse in AUC than the best\nregularized model (though naturally for higher dimensionsregularization becomes more important).\nOur method signiﬁcantly and substantially outperforms\nboth baselines in every category. Matching our intuition,\nit does the best for genre queries, with an AUC of nearly\n0.85 and Precision@10 of about 37%. The emotion and\nmusical categories prove the most difﬁcult, while the year\nand other categories are the easiest after genre.\nNote that the performance values reported in Figure 5\nare extremely conservative estimates of the actual retrieval\nquality of our method. This is for three reasons: First,\nsocial tags can be noisy since they result from ad-hoc la-\nbeling practices by non-experts [17]. Second, we made\nno attempt to identify lexicographically similar tags as the\nsame. For example, consider the following ranking that\nour method produces for the tag-query “male vocals”, with\na relevant subset of the tags given for each song:\nDaughtry - Home: male vocalists, male vocalist, male\nAllen - Live Like We’re Dying: male vocalists, male vocalist\nThe Fray - How To Save A Life: male vocalists, male vocalist\nAerosmith - Rag Doll: male vocalist, malesinger\nLifehouse - Hanging By A Moment: male vocalists,\nmale vocalist, male vocals\nHere, all ﬁve songs are clearly relevant to the query, but\nonly the last song was considered relevant for the purposes\nof our experiments. Third, we only test our method on\nsongs for which no tags at all were seen during training.\nFor these reasons, it is important to keep in mind that the\nresults we report are strict lower bounds on the actual re-\ntrieval performance of our method.\n5. CONCLUSIONS\nWe presented a method for learning to predict playlists\nthrough an embedding of songs and tags in Euclidian space.\nThe method not only provides a well-founded probabilis-\ntic model for playlist generation, it also produces a dis-\ntance metric with a probabilistic meaning for song/song,\nsong/tag, and tag/tag distances. We show that the method\nsubstantially outperforms conventional sequence models\nfrom NLP, that it can sensibly impute the location of previ-\nously unseen songs, and that its distance metric is effective\nfor music retrieval even of untagged songs.\nThe ﬂexibility of the LME approach provides exciting\nopportunities for future work, since the model leaves open\nthe possibility of more complex representations of songs.\nFor example, instead of representing each song as a single\nX(s), one can use two embedding vectors U(s)andV(s)\nto model the beginning and ending of a song respectively.\nThis allows modeling that the ending of song sis com-\npatible with the beginning of song s0, but that the reverse\nmay not be the case. Another interesting direction for fu-\nture work is the modeling of long-range dependencies in\nplaylists. Such long-range dependencies could capture the\namount of redundancy/repetition that a user may seek, ver-\nsus how much a playlist provides variety and explores new\nmusic.\nThis research was supported in part by NSF Awards IIS-\n1217686, IIS-0812091 and IIS-0905467.6. REFERENCES\n[1] N. Aizenberg, Y . Koren, and O. Somekh. Build your\nown music recommender by modeling internet radio\nstreams. In WWW, 2012.\n[2] L. Barrington, R. Oda, and G. Lanckriet. Smarter than\ngenius? human evaluation of music recommender sys-\ntems. ISMIR, 2009.\n[3] Shuo Chen, J. L. Moore, D. Turnbull, and T. Joachims.\nPlaylist prediction via metric embedding. In SIGKDD,\n2012.\n[4] D. F. Gleich, L. Zhukov, M. Rasmussen, and K. Lang.\nThe World of Music: SDP embedding of high dimen-\nsional data. In Information Visualization 2005, 2005.\n[5] D. Jurafsky and J.H. Martin. Speech and language pro-\ncessing, 2008.\n[6] P. Knees, T. Pohle, M. Schedl, D. Schnitzer, and\nK. Seyerlehner. A document-centered approach to a\nnatural language music search engine. In ECIR, 2008.\n[7] Y . Koren, R. M. Bell, and C. V olinsky. Matrix fac-\ntorization techniques for recommender systems. IEEE\nComputer, 42(8):30–37, 2009.\n[8] M. Levy and M. Sandler. A semantic space for music\nderived from social tags. In ISMIR, 2007.\n[9] B. Logan. Content-based playlist generation: ex-\nploratory ex- periments. ISMIR, 2002.\n[10] F. Maillet, D. Eck, G. Desjardins, and P. Lamere. Steer-\nable playlist generation by learning song similarity\nfrom radio station playlists. In ISMIR, 2009.\n[11] B. McFee, L. Barrington, and G. Lanckriet. Learning\ncontent similarity for music recommendation. IEEE\nTASLP, 2012.\n[12] B. McFee and G. R. G. Lanckriet. The natural language\nof playlists. In ISMIR, 2011.\n[13] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrieval. PhD thesis, TU Wien, Vienna, Austria, 2006.\n[14] J. C. Platt. Fast embedding of sparse music similarity\ngraphs. In NIPS, 2003.\n[15] B. Schwartz. The Paradox of Choice: Why More is\nLess. Ecco, 2003.\n[16] D. Tingle, Y . Kim, and D. Turnbull. Exploring auto-\nmatic music annotation with “acoustically-objective”\ntags. In ICMR, 2010.\n[17] D. Turnbull, L. Barrington, and G. Lanckriet. Five ap-\nproaches to collecting tags for music. In ISMIR, 2008.\n[18] J. Weston, S. Bengio, and P. Hamel. Multi-tasking with\njoint semantic spaces for large-scale music annotation\nand retrieval. Journal of New Music Research, 2011."
    },
    {
        "title": "A Scape Plot Representation for Visualizing Repetitive Structures of Music Recordings.",
        "author": [
            "Meinard Müller",
            "Nanzhu Jiang"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416866",
        "url": "https://doi.org/10.5281/zenodo.1416866",
        "ee": "https://zenodo.org/records/1416866/files/MullerJ12.pdf",
        "abstract": "The development of automated methods for revealing the repetitive structure of a given music recording is of cen- tral importance in music information retrieval. In this pa- per, we present a novel scape plot representation that al- lows for visualizing repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. In a scape plot, each point corresponds to an audio segment identified by its center and length. As our main contri- bution, we assign to each point a color value so that two segment properties become apparent. Firstly, we use the lightness component of the color to indicate the repetitive- ness of the encoded segment, where we revert to a recently introduced fitness measure. Secondly, we use the hue com- ponent of the color to reveal the relations between different segments. To this end, we introduce a novel grouping pro- cedure that automatically maps related segments to similar hue values. By discussing a number of popular and classi- cal music examples, we illustrate the potential and visual appeal of our representation and also indicate limitations.",
        "zenodo_id": 1416866,
        "dblp_key": "conf/ismir/MullerJ12",
        "keywords": [
            "automated methods",
            "repetitive structure",
            "music recording",
            "scape plot",
            "visualizing",
            "hierarchical",
            "compact",
            "intuitive",
            "color value",
            "segment properties"
        ],
        "content": "ASCAPE PLOT REPRESENTATIONFORVISUALIZING\nREPETITIVEST R\nUCTURES OF MUSICRECORDINGS\nMeinard M ¨uller\nBonn UniversityandMPIInformatik\nmeinard@mpi-inf.mpg.deNanzhu Jiang\nSaarland Universityand MPIInformatik\nnjiang@mpi-inf.mpg.de\nABSTRACT\nThe development of automated methods for revealing the\nrepetitive structure of a given music recording is of cen-\ntral importance in music information retrieval. In this pa-\nper, we present a novel scape plot representation that al-\nlowsforvisualizingrepetitivestructuresoftheentiremusic\nrecordingin a hierarchical,compact, and intuitive way. In\na scape plot, each point corresponds to an audio segment\nidentiﬁed by its center and length. As our main contri-\nbution, we assign to each point a color value so that two\nsegment properties become apparent. Firstly, we use the\nlightnesscomponentofthecolortoindicatetherepetitive-\nnessoftheencodedsegment,wherewereverttoarecently\nintroducedﬁtnessmeasure. Secondly,weusethehuecom-\nponentofthecolortorevealtherelationsbetweendifferent\nsegments. To thisend,we introducea novelgroupingpro-\ncedurethatautomaticallymapsrelatedsegmentstosimilar\nhuevalues. By discussinga numberof popularandclassi-\ncal music examples, we illustrate the potential and visual\nappealofourrepresentationandalsoindicatelimitations.\n1. INTRODUCTION\nThe musical form describes a piece of music in terms of\nmusical partssuch as intro, chorus, and verse of a popular\nsongortheﬁrstandsecondthemeofaclassicalwork. Such\nmusicalpartsaretypicallyrepeatedseveraltimesthrough-\noutthepieceandevokeinthelistenerthefeelingoffamil-\niarity. One major goal of audio structure analysis is to au-\ntomatically derive the musical form directly from a given\nmusic recording. To this end, most procedures divide the\nmusicrecordingintorepeatingtemporalsegmentsandthen\ngroup these segments according to musically meaningful\ncategories[13].\nFindingtherepetitivestructureofamusicrecordinghas\nbeenacentralandwell-studiedtaskwithinthewideareaof\naudiostructureanalysis,see,e.g.,[2,5,7,8,11,12]andthe\noverview articles [3,13]. Even though most of these ap-\nproaches work well when repetitions largely agree, struc-\ntureanalysisbecomesa hardandevenill-posedtaskwhen\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2012 International Society for MusicInformation Retrieval.audio segments that refer to the same musical part reveal\npronounced musical variations. One way to circumvent\nsuch problems is to only visualize structural elements and\ntheir relations without explicitly extracting them. For ex-\nample, in [4] self similarity matrices are used to visualize\noverallstructuralpatternsor,in [15],repeatingandrelated\nelementsareindicatedbyarcdiagrams.\nIn this paper, we contribute to this line of research by\nintroducing a novel representation that reveals the hierar-\nchical repetitive structure of a given music recording. In-\nspired by the work by Sapp [14], we use the concept of a\n2D scape plot, where each point represents an audio seg-\nmentbymeansofitscenterandlength. Asourmaincontri-\nbution, we describe an automated procedurefor assigning\nto each point a color value such that the repetitive struc-\nture of the music recordingbecomesapparent. On the one\nhand, we use the lightness component of the color to in-\ndicate the repetitiveness of the respective segment. This\nrepetitiveness is expressed in terms of a ﬁtness measure\nas recently introduced by M¨ uller et al. [10]. On the other\nhand, we use the hue componentof the color to reveal the\nrelations across different segments, where we introduce a\nfunction that maps related segments to similar hue values.\nAs a result, one obtains a hierarchical structure visualiza-\ntionoftheunderlyingmusicrecordingreferredto structure\nscapeplot ,seeFigure4gforanexample. Wehopethatthis\nrepresentation not only visually appeals to the reader, but\nalso brings valuable and even surprising insights into the\nstructuralpropertiesofa recording.\nThe remainderof this paper is organizedas follows. In\nSection 2, we review the underlying ﬁtness measure and\ndescribe the corresponding ﬁtness scape plot. Then, in\nSection 3, we introduce our structure scape plot represen-\ntation whichis based ona noveldistancemeasure to com-\nparedifferentsegmentsaswell as onanefﬁcientgrouping\nandcoloringprocedure. Based onanumberofexplicitex-\namples,wediscussbeneﬁtsandlimitationsofourstructure\nvisualization in Section 4 and conclude with Section 5 by\nindicatingfuturework.\n2. FITNESS SCAPEPLOT\nIn this section, we summarize the construction of the ﬁt-\nness measure(Section 2.1)andthen introducethe concept\nofa ﬁtnessscapeplot(Section2.2).(a)\n0 50 100 150 200A1A2B1B2CA3B3B4\n  \n0 50 100 150 200050100150200\n00.050.10.150.20.250.30.350.4\n(c)(d)(e)(b)Segment length (sec)\nSegment center(sec)\n  \n0 50 100 150 200050100150200\n−200.51(c)Time(sec)\nTime(sec)\n  \n0 50 100 150 200050100150200\n−200.51(d)\nTime(sec)Time(sec)\n  \n0 50 100 150 200050100150200\n−200.51(e)\nTime(sec)Time(sec)\nFigure 1: Various representations for an Ormandy record-\ning of Brahms’ Hungarian Dance No. 5. (a)Musical form\nA1A2B1B2CA3B3B4.(b)Fitness scape plot. The remaining\nsubﬁgures show the SSM with optimal path families for various\nsegments α(horizontal axis) and induced segment families (ver-\ntical axis). (c)α= [68:89] (thumbnail, maximal ﬁtness, corre-\nsponding to B2).(d)α= [131:150] (corresponding to A3).(e)\nα= [131:196] (corresponding to A3B3B4).\n2.1 FitnessMeasure\nLet[1 :N] ={1,2,...N}denotethe(sampled)timeaxis\nof a given music recording. Then a segment is a subset\nα= [s:t]⊆[1 :N]speciﬁed by its starting point sand\nitsendpoint t. Let|α|:=t−s+1denotethelengthofseg-\nmentα. In[10],aﬁtnessmeasurehasbeenintroducedthat\nassigns to each audio segment αa ﬁtness value ϕ(α)∈R\nwhich simultaneously captures two aspects. Firstly, it in-\ndicates how well the given segment explains other related\nsegments. Secondly, it indicates how much of the overall\nmusic recording is covered by all these related segments.\nIn the computation of the ﬁtness measure, an enhanced\nself-similarity matrix (SSM) is computed from the music\nrecordingbasedonchroma-basedaudiofeatures. Itiswell\nknown that each path of the SSM (a stripe of high score\nrunningparalleltothemaindiagonal)revealsthesimilarity\nof two segments (given by the two projectionsof the path\nonto the vertical axis and horizontal axis), see [13]. The\nmain idea of [10] is to compute for each audio segment\nαa so-called optimal path family overαthat simultane-\nously revealsthe relations between αand all other similar\nsegments. By projecting such optimal path family to the\nvertical axis, we get the corresponding induced segment\nfamily, where each element of this family deﬁnes a seg-\nmentsimilarto α.\nAs an example, we consider a recording of the Hun-\ngarian Dance No. 5 by Johannes Brahms, which has the\nmusical form A1A2B1B2CA 3B3B4, see Figure 1a. Fig-\nure 1c shows an optimal path family (cyan stripes) for theB2-segment α= [68:89] (horizontal axis) as well as the\ninduced segment family (vertical axis). The induced seg-\nmentation consists of four segments corresponding to the\nfouroccurrencesofthe B-partin this recording. Note that\nrepeating segments may be played in different tempi. For\nexample, the B2-part is played much faster than the B1-\npart. Similarly, Figure 1d shows the optimal path family\nfor the segment α= [131:150] (correspondingto the A3-\npart)andtheinducedsegmentation(consistingofthethree\nA-part segments). Finally, Figure 1e reveals that, for the\nlongsegment α= [131:196] (correspondingto A3B3B4),\nthereexistsasimilarsegment(correspondingto A2B1B2).\nThe ﬁtness value of a given segment is derived from\nthe corresponding optimal path family and the values of\nthe underlyingSSM. Intuitively, one considers the overall\nscore accumulated by the path family and the total length\ncoveredbytheinducedsegmentation. Afterasuitablenor-\nmalization, the ﬁtness is deﬁned as the harmonic mean of\nofcoverageandscore. Forfurtherdetails,wereferto[10].\n2.2 ScapePlotRepresentation\nWe now describehow a compactﬁtness representationfor\ntheentiremusicrecordingcanbeobtainedshowingtheﬁt-\nnessϕ(α)forallpossiblesegments α. Notethateachseg-\nmentα= [s:t]isspeciﬁedbyitscenter c(α) := (s+t)/2\nand its length |α|. Using the center as horizontal coordi-\nnate and the length as vertical coordinate, each segment\ncan be represented as a point in some triangular represen-\ntationalsoreferredtoas scapeplot . Suchscapeplotswere\noriginal introduced by Sapp [14] to represent harmony in\nmusical scores in a hierarchical way. In our context, we\ndeﬁne a scape plot Φby setting Φ(c(α),|α|) :=ϕ(α)\nfor segment α. Figure 1b shows a visualization of the\nﬁtness scape plot for our Brahms example, where the ﬁt-\nness is represented by a lightness grayscale ranging from\nwhite (ﬁtnessis zero)to black(ﬁtnessishigh). Thepoints\ncorresponding to the three segments discussed above are\nmarked within the scape plot by small circles. For exam-\nple, the segment α= [68:89] (corresponding to B2) has\nthe scape plot coordinates c(α) = 78.5(horizontal axis)\nand|α|= 22(vertical axis). Actually, this segment has\nthehighestﬁtnessamongall possiblesegmentsandisalso\nreferredtoas thumbnail [10].\nThe ﬁtness scape plot represents the repetitiveness of\neach segment in a compact and hierarchical form. For ex-\nample,inourBrahmsexample,therepeatingsegmentscor-\nresponding to the A-parts and B-parts are reﬂected by lo-\ncal maxima in the scape plot. Also the repetitions of the\nsuperordinate segments corresponding to ABBare cap-\ntured by the plot. However,so far, the visualization of the\nﬁtnessscapeplotdoesnotrevealtherelations acrossdiffer-\nentsegments. Inotherwords,nothingissaidaboutgroups\nof pairwise similar segment corresponding to the various\nmusicalparts.3. STRUCTURESCAPEPLOT\nActually, th e\ngrouping information is implicitly encoded\nby the optimal path families underlying the ﬁtness mea-\nsure. Tomaketheserelationsmoreexplicit,wenowextend\nthegrayscaleoftheﬁtnessscapeplotbyacolorcomponent\nthat reﬂects the cross-segment relations. Based on the in-\nducedsegmentations,weﬁrstintroduceadistancemeasure\nthat allows for comparing two arbitrary segments (Sec-\ntion 3.1). Then the objective is to map similar segments\ntosimilarcolorsanddissimilarsegmentstodistinctcolors.\nIn the following, we proceed in several steps including a\ncolormappingstep (Section3.2),a pointsamplingandin-\nterpolationstep(Section3.3),andacolorcombinationstep\n(Section3.4). Theoverallpipelineofourprocedureisalso\nillustratedbyFigure4.\n3.1 SegmentDistanceMeasure\nRecall from Section 2.1 that for a given segment αthere\nis an optimal path family along with an induced segment\nfamily, where each segment of this family is similar to α.\nLetA={α1,α2,...,α K}denote the induced segment\nfamily of α, then the segments αk,k∈[1 :K], can be\nthoughtofasthe(approximate)repetitionsof α. Notethat,\nbydeﬁnition,overlapsbetweenrepetitionsarenotallowed,\nsee [10].\nNow,letαandβbetwoarbitrarysegments. Intuitively,\nwe consider these two segmentsto be close if they are ap-\nproximately repetitions of each other (or at least if some\nrepetitions of αandβhave a substantial overlap), other-\nwiseαandβare considered to be far apart. More pre-\ncisely, let A={α1,...,α K}andB={β1,...,β L}be\nthe respective induced segment families. Then, we deﬁne\nthedistance δ(α,β)betweenαandβtobe\nδ(α,β) := 1−max\nk∈[1:K],ℓ∈[1:L]|αk∩βℓ|\n|αk∪βℓ|,(1)\nseealsoFi g\nure2foranillustration. Inotherwords,thedis-\ntance is obtained by subtracting the maximal overlap(rel-\native to the union)overall repetitionsof αandβfromthe\nvalue1. For example, the B1-segment and B2-segment\nfor the Brahms recording have a small distance (close to\nzero) since the induced segment families more or less co-\nincide(consistingofthefour B-partsegments). Incontrast\ntheB1-segmentand the A1-segment have a large distance\n(close to one) since none of their repetitions have a sub-\nstantial overlap.\n3.2 ColorMapping\nBased onthe distancemeasure δ, we nowintroducea pro-\ncedure for mapping the scape plot points (segments) to\ncolor values in such a way that distance relations are pre-\nserved. To this end, we ﬁrst need to specify a suitable\ncolor space. Because of its perceptual relevance, we re-\nvert to the HSL model, which is a cylindric parametriza-\ntionoftheRGBcolorspace[6]. Heretheanglecoordinate\nH∈[0,360](given in degrees) refers to the hue, the co-\nordinateS∈[0,1]to the saturation, and the coordinate0 20 40 60 80 100 120 140 160 180 200\n0 20 40 60 80 100 120 140 160 180 200\n0 20 40 60 80 100 120 140 160 180 200\n0 20 40 60 80 100 120 140 160 180 200\nTime(sec)α\nβ\nα1α2 α3α4\nβ1 β2(a)\n(b)\n(c)\n(d)\nFigu\nre 2:Illustration of the computation of the distance mea-\nsuresδ(α,β)used to compare two segments α(shown in (a))\nandβ(shown in (b)). The respective induced segment families\nare shown in (c)and(d), respectively. The black box indicates\nthe union and the red box the overlap of the twosegments which\nare used tocompute distance value δ(α,β).\nL\n01\nH0◦120◦240◦360◦\nFigure\n3:Cylindric HSL(hue, saturation, lightness) color repre-\nsentation. The ﬁgure shows only the outside surface of the cylin-\nder corresponding tothe saturation S= 1.\nL∈[0,1](with0being black and 1being white) to the\nlightnessofthecolor. Toobtain“full”saturatedcolors,we\nﬁx the parameter S= 1. Figure 3 shows the color space\nforS= 1spanned by the coordinates HandL. Note that\nthe hue angle coordinates H= 0andH= 360encode\nthe same color (by deﬁnition this is the color “red”). In\nthe following, we reserve the lightness coordinate to rep-\nresent the ﬁtness value and only use the hue coordinate to\nrepresentthecross-segmentrelationships.\nTheproblemofmappingthescapeplotpointstothehue\ncolor coordinates (which topologically corresponds to the\nunit circle) in a distance preservingway can be seen as an\ninstanceof multidimensionalscaling (MDS),see[1]. Gen-\nerally,MDS refersto a family ofrelated techniqueswhich\nallow for mapping a set of points with pairwise distance\nvalues onto a low-dimensional Euclidean space (often di-\nmension 2or3for visualization purposes) such that the\ndistancesbetween the originalpointsare approximatedby\ntheEuclideandistancesofthemappedpoints.\nIn the following, we use basic MDS techniquesto map\nthe scape plot points onto the unit circle (representing the\nhue color space). Let Mdenote the number of scape plot\npoints to be considered in the mapping, see Figure 4b.\nFirst, we compute an M×M-distance matrix ∆by com-\nparing the Mpoints in a pairwise fashion using δ. Next,\nwe perform a principal component analysis (PCA) of ∆\nand consider the two eigenvectors corresponding to the\ntwo largest eigenvalues. The columns of ∆(which are\nindexed by the Mscape plot points) are then projected\nontothetwo-dimensionalEuclideanspacedeﬁnedbythese\ntwo eigenvectors,see Figure 4c. Using PCA, the variance\nacross the mapped column vectors is maximized. There-\nfore, scape plot points that have a distinct distance distri-  \n0 50 100 150 200050100150200\n  \n0 50 100 150 200050100150200−8−6−4−202468−8−6−4−202468\n0 50 100 150 2000950100150200\n0 50 100 150 2000950100150200\nSegment center(sec)Segment length (sec)\nSegment center(sec)Segment length (sec)Segment center(sec)Segment length (sec)\nSegment center(sec)Segment length (sec)\n0 50 100 150 200A1A2B1B2CA3B3B4\nTime(sec)(a)\n(b\n)(c)\n(d)(e)\n(f)(g)\nF\nigure 4:Illustration of the pipeline for computing the structure scape plot for Brahms. (a)Fitness sc a pe plot.(b)Fitness scape plot\nwithsampledanchor points. (c)Anchor pointsprojectedontothe ﬁrsttwoprincipal components. (d)Anchor points projectedtotheunit\ncirclecoloredwiththe resultinghue value. (e)Hue-colored anchor points. (f)Hue-colored scape plotusinginterpolationtechniques. (g)\nStructure scape plot combining ﬁtness(lightness) andcross-segment relation(hue) information.\nbution to the other points (encoded by its respective col-\numn vectors) are likely to be mapped to different regions\ninthe2Dspace,see[1]fordetails. Furthermore,asshown\ninFigure4c,theprojectedpointsareusuallydistributedin\na circular fashion (even though this is not guaranteed and\ncruciallydependsonthedistancedistributionsoftheorigi-\nnalpoints). Finally,wenormalizetheprojectedpointswith\nrespect to the Euclidean norm to obtain points on the unit\ncircle,whichyieldsangleparametersthatareassociatedto\nhue values, see Figure 4d. Figure 4e shows the original\nscapeplotpointscoloredwiththederivedhuevalues.\n3.3 Samplingand Interpolation\nUsing all scape plot points in the described color map-\nping procedure may be problematic because of two rea-\nsons. Firstly, using a largenumber Mof pointswouldnot\nonlymakethecomputationofthe M×Mdistancematrix\n∆butalsoofthesubsequentPCAratherexpensive. There-\nfore, the number Mof used points should be kept small.\nSecondly, using all scape plot points may over-represent\nsegmentsofshortlengthsthatarelocatedinthe lowerpart\nof the triangular scape plot. As a result, the distance re-\nlations of the short segments may dominate the selection\nof the eigenvectors obtained in the PCA step. Therefore,\nwe onlychoose a suitable subset of scape plot points, also\nreferredtoas anchorpoints ,andthentransfertheobtained\nhue color information to the other points using interpola-\ntiontechniques.\nNote that scape plot points of higher ﬁtness are struc-\nturally more relevant than scape plot points of lower ﬁt-\nness. Therefore, in the anchor point selection step, we\nsample the scape plot by taking the ﬁtness into account.\nTothisend,weuseagreedyprocedurethatconsistsoftwo\nsteps. Firstly, we select the scape plot point of maximal\nﬁtness as an anchor point. Secondly, around this anchorpoint, we specify a neighborhood of size ρ >0and set\ntheﬁtnessvaluesofall pointsinthisneighborhoodtozero\nexcluding them for the subsequent procedure. The role of\nthe neighborhoodis to avoid a samplingthat is locally too\ndense. Thisprocedureis repeateduntil eitherall of the re-\nmaining scape plot points have a ﬁtness of zero, or until\na speciﬁed maximal number of points M0is reached, see\nalso Figure4b.\nSometimes the ﬁtness values of short segments are\nrather “noisy.” This may also have musical reasons since\nsuch segments often correspond to highly repetitive frag-\nments like a short riff or a single chord of dominant har-\nmony. Therefore, it is often beneﬁcial to exclude such\nshort segments in the anchor point selection by only con-\nsidering scape plot points whose length coordinate lies\nabove a certain lower bound λ >0. The inﬂuence of the\nparameters M0,ρ,andλontheresultingnumberofanchor\npointsMisdiscussedin Section4.\nThe color mapping as described in Section 3.2 is now\nappliedonlytotheanchorpoints. Inthenextstep,thecolor\ninformationis transferredto arbitraryscape plot pointsby\nsimply interpolating color values of the nearest neighbor-\nhoodanchorpoints. However,since the huevaluesliveon\na unitcircle(ratherthanin thetwo-dimensionalEuclidean\nspace), one needs to use spherical interpolation instead of\nlinear interpolation. Figure 4f shows the interpolation re-\nsult obtainedfromtheanchorpointsofFigure4e.\n3.4 ColorCombination\nSo far, we have derived two scape plot visualizations:\none indicating the repetitive properties (ﬁtness value rep-\nresentedbylightness,see Figure4a)andtheotherindicat-\ningthecross-segmentrelations(representedbyhuecolors,\nsee Figure4f). We nowcombinethisinformationwithina\nsingle scape plot representation, which we also refer to as0 50 100 150 200 25001350100150200250\n0 50 100 150 200 250A1B1A2B2A3C D1D2A4B3A5(a)\n0 50 100 150 200 25001950100150200250\n0 50 100 150 200 250I V1B1V2V3B2V4O(b)\n0 20 40 60 80 10004102030405060708090100\n0 20 40 60 80 100A1B1B2 A2\nTime(sec)(c)\n0 50 100 15001050100150\n0 50 100 150I V1V2B1V3V4B2V5O\nTime(sec)(d)\nFi\ngure 5: Structure scape plots and structure annotations for\nrecordings of various pieces. (a)Chopin Mazurka Op. 17 No. 3.\n(b)Beatles song “While My Guitar Gently Weeps.” (c)Chopin\nMazurka Op. 33 No.3. (d)Beatles song “YouCan’t DoThat.”\nstructure scape plot . To this end, we ﬁrst linearly map the\nﬁtness values onto the lightness parameter space [0,1]of\ntheHSLmodelsuchthat L= 1(white)correspondstothe\nﬁtness value 0andL= 0(black) to the maximal ﬁtness\nvalue occurring in the ﬁtness scape plot. Furthermore, by\nrotating the hue parameter space (unit circle) we normal-\nize the color assignment such that the thumbnail (ﬁtness-\nmaximizingscape plot point)is mappedto the color “red”\n(angleH= 0). Finally, for each scape plot point we use\nthe saturation S= 1, the computed lightness L, and the\nnormalizedhueangle Hto obtaina singlecolorvalue.\nFigure 4g shows the ﬁnal result of the structure scape\nplot for our Brahms example. Note that the four B-part\nsegments(repetitionsofthe B2-thumbnail)arerepresented\nby red, the three A-part segments by blue, and the super-\nordinate two ABB-part segments by green. Furthermore,\nthevisualizationrevealssomesubstructuresofthe A-parts,\neach actually consisting of two (approximate) repetitions.\nFinally, note that smaller segments within the C-part are\nassigned to the color violet. Since the C-part contains\nmanyfragmentssharing the same harmony,ourprocedure\nhascapturedsomerepetitivenessalso inthismiddlepart.\n4. EXAMPLES ANDDISCUSSION\nIn this section, we indicate the potential and some limi-\ntations of our visualization procedure by discussing rep-\nresentative examples. In our experiments, we used audio\nrecordings considering popular music as well as classical\nmusic. On the onehand,we employedthe dataset consist-\ningofrecordingsofthe 12studioalbumsby“TheBeatles”\nusing the structure annotations as described by [9]. On\nthe other hand, we used the complete Rubinstein (1966)\nrecordings of the 49Mazurkas composed by Fr´ ed´ ericChopin, where we manually generated some structure an-\nnotations for each piece. Note that these annotations are\nnot needed to generate the structure scape plots, but are\nonly used to compare our visualizationswith some sort of\nground truth. As mentioned in the introduction, the pur-\npose of the scape plot visualizations is to yield a compact\nandintuitiverepresentationwithoutthenecessityofexplic-\nitly extractingthestructure.\nAs for the parameter settings, we choose M0,ρ, and\nλin a relative fashion depending on the duration of the\nrespective music recording. In particular, we determined\nthe upperbound M0and the neighborhoodparameter ρto\nresult in a number Mof anchor points ranging between\n200and250for each recording. Furthermore, the lower\nboundλwas set to correspondto 5-7%of the recording’s\ntotal duration. Figure 5 shows structure scape plots for\nsome representative music recordings. For example, Fig-\nure 5a shows the scape plot for a Rubinstein performance\nof Chopin’s Mazurka Op. 17 No. 3. The ﬁve A-part seg-\nments,whichalsocomprisethethumbnail,arerepresented\nby red. Furthermore, the three B-part segments are in-\ndicated by a lighter orange color, and the superordinate\nABA-part segments are represented by green. Also sub-\nstructures of the A-part segments are visible: indeed each\nA-part consists of two similar subparts. Interestingly, the\nsegmentscorrespondingto the C- andthetwo D-partsare\nall represented by pink. Actually this is musically mean-\ningful, since each of the two repeating D-parts is only a\nslight extensionofthe C-part.\nFigure5bvisualizesthestructurescapeplotfortheBea-\ntles song “While My Guitar Gently Weeps.” Also in this\nexample, the structure scape plot nicely reﬂects the over-\nall musical form. Each of the four verse segments ( V-\npart) consists of two (approximately) repeating subparts,\nsayV=WW. Actually,theintroalsocorrespondstosuch\na subpart ( I=W) and the outro corresponds to three of\nthese subparts( O=WWW), whichalsoexplainsthered\ncoloring of these segments. Furthermore, the color blue\ncorresponds to WWW-segments and the color green to\nVBV-segments.\nThe structure scape plot of a recording of the Mazurka\nOp. 33 No. 3 is shown in Figure 5c, which indicates a\nnumber of substructures not reﬂected in the structure an-\nnotation (see both Aparts). Finally, Figure 5d correctly\nreproduces the overall structure of the Beatles song “You\nCan’t Do That.” Only the V4-segment has not been cap-\ntured well. Actually, V4corresponds to an instrumental\nsection with some vocal interjections,whichmake the V4-\nsegment spectrally quite different to the other four V-part\nsegments.\nNext, we discuss some limitations and problems that\nmayoccurinourvisualizationapproach. Asanillustrating\nexample, we consider the Beatles song “Hello Goodbye.”\nFigure6bshowsthestructurescapeplotusingourstandard\nparametersetting asdescribedabove. Theredcolorcorre-\nspondstothefour VR-partsegments,whichalsocomprise\nthe thumbail. However, the individual V-part and R-part\nsegments are all represented by green and are not distin-−6 −3 0 3 6−6−3036(a)\n0 50 100 150 20001420406080100120140160180200\n0 50 100 150 200V1R1V2R2V3R3V4R4F S(b)\n−6 −3 0 3 6−6−3036(c)\n0 50 100 150 20001020406080100120140160180200\n0 50 100 150 200V1R1V2R2V3R3V4R4F S\nTime(sec)(d)\nF\nigure 6: Anchor points projected onto the ﬁrst two principal\ncomponents(left)andresultingstructurescapeplot(right)forthe\nBeatles song “Hello Goodbye.” (a)/(b)Usingλ= 14seconds.\n(c)/(d)Usingλ= 10seconds.\nguishable. The reason for this is that the lower bound for\nthe anchor pointswas set to λ= 14seconds, which is too\nhigh to capture the ﬁner structures. By decreasingthis pa-\nrameter to λ= 10seconds, V-part and R-part segments\nare separated, see Figure 6d. As this example shows, the\nchoice of the parameter λmay have a signiﬁcant impact\non the ﬁnal visualization. The Beatles example also indi-\ncates a second problem that may arise in our color map-\npingprocedure. Usually,theanchorpointsprojectedtothe\ntwo principal components are homogeneously distributed\nalong the unit circle as in our Brahms example, see Fig-\nure4c. Therefore,projectingthese pointstothe unitcircle\n(toyieldthedesiredhuevalues)doesnotdestroytoomuch\noftheneighborhoodrelations. However,intheBeatlesex-\nample, the projected anchor points are rather scattered in\nthe two-dimensional Euclidean space with some outliers\nas indicatedbythe boxedandcircledpointsshownin Fig-\nure 6a. Therefore, projecting these points onto the unit\ncircle may result in the same hue value for anchor points\nthatareactuallyfarapart. Thisexplains,whythesubstruc-\ntures within the S-part are mapped to the same color as\nsubstructuresofthe VR-part,see Figure6b.\n5. FUTUREWORK\nThese problems indicate some future research directions.\nPossible improvements of the color mapping step may be\nachieved by applying more involved generalized multidi-\nmensional scaling techniques which directly map the an-\nchor pointsto a smooth manifold(in our case the unit cir-\ncle). Also, the one-dimensional hue color space may not\nsufﬁcetosuitablecapturemoreintricatecross-segmentre-\nlations. Here, a more ﬂexible usage of the color space or\nan extension to 3D scape plot representationsmay help tobetter represent more complex structures. So far, we have\nonly given a qualitative evaluation to demonstrate the po-\ntential of ourtechniques. In this context, user studies may\nbenecessarytobetterunderstandtheactualuserneedsand\nthe applicability of our concepts. Besides introducing a\nnovelsegmentdistancefunctionas well as a groupingand\ncoloringprocedure,themaincontributionofthispaperwas\nto introduce the concept of a structure scape plot for visu-\nalizing repetitive structures of music recordings. We hope\nthat our visualization is not only aesthetically appealing,\nbut also may allow a user to explore and browse musical\nstructuresinnovelways.\nAcknowledgments: This work has been supported by the Clus-\nter of Excellence on Multimodal Computing and Interaction at\nSaarland University and the German Research Foundation (DFG\nMU 2686/5-1).\n6. REFERENCES\n[1] Ingwer Borg and Patrick J. F. Groenen. Modern Multidimensional\nScaling Theory and Applications. Springer, 2005.\n[2] Matthew Cooper and Jonathan Foote. Summarizing popular music\nvia structural similarity analysis. In Proc. IEEE Workshop on Ap-\nplications of Signal Processing to Audio and Acoustics (WASPAA) ,\npages 127–130, New Paltz, NY, USA,2003.\n[3] Roger B. Dannenberg and Masataka Goto. Music structure analy-\nsis from acoustic signals. In David Havelock, Sonoko Kuwano, and\nMichaelVorl¨ ander, editors, HandbookofSignalProcessinginAcous-\ntics, volume1,pages 305–331. Springer, New York,NY,USA,2008.\n[4] Jonathan Foote. Visualizing music and audio using self-similarity. In\nProc. ACM International Conference on Multimedia , pages 77–80,\nOrlando, FL,USA,1999.\n[5] Masataka Goto.Achorus section detection methodformusicalaudio\nsignals and its application to a music listening station. IEEE Trans.\nAudio, Speech and Language Processing , 14(5):1783–1794, 2006.\n[6] Allan Hanbury. Constructing cylindrical coordinate colour spaces.\nPattern Recognition Letters , 29:494–500, 2008.\n[7] Mark Levy, Mark Sandler, and Michael A. Casey. Extraction of\nhigh-level musical structure from audio data and its application to\nthumbnail generation. In Proc. IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) , pages 13–16,\nToulouse, France, 2006.\n[8] Namunu C. Maddage. Automatic structure detection for popular mu-\nsic.IEEEMultimedia , 13(1):65–77, 2006.\n[9] Matthias Mauch,ChrisCannam,Matthew E.P.Davies,SimonDixon,\nChristopher Harte, Sefki Kolozali, Dan Tidhar, and Mark Sandler.\nOMRAS2 metadata project 2009. In Late Breaking Demo, Interna-\ntional Conference on Music Information Retrieval (ISMIR) , Kobe,\nJapan, 2009.\n[10] Meinard M¨ uller, Peter Grosche, and Nanzhu Jiang. A segment-based\nﬁtness measure for capturing repetitive structures of music record-\nings. InProc. 12th International Conference on Music Information\nRetrieval (ISMIR) , pages 615–620, Miami, FL,USA,2011.\n[11] Meinard M¨ uller and Frank Kurth. Towards structural analysis of au-\ndio recordings in the presence of musical variations. EURASIPJour-\nnal on Advances in Signal Processing , 2007(1), 2007.\n[12] Jouni Paulus and Anssi P. Klapuri. Music structure analysis using a\nprobabilistic ﬁtness measure and a greedy search algorithm. IEEE\nTrans. Audio, Speech, and Language Processing , 17(6):1159–1170,\n2009.\n[13] Jouni Paulus, Meinard M¨ uller, and Anssi P. Klapuri. Audio-based\nmusic structure analysis. In Proc. 11th International Conference on\nMusic Information Retrieval (ISMIR) , pages 625–636, Utrecht, The\nNetherlands, 2010.\n[14] Craig Stuart Sapp. Harmonic visualizations of tonal music. In Proc.\nInternational Computer Music Conference (ICMC) , pages 423–430,\n2001.\n[15] Ho-Hsiang Wu and Juan P. Bello. Audio-based music visualization\nfor music structure analysis. In Proceedings of Sound and Music\nComputing Conference (SMC) , Barcelona, Spain, 2010."
    },
    {
        "title": "A Cross-version Approach for Stabilizing Tempo-based Novelty Detection.",
        "author": [
            "Meinard Müller",
            "Thomas Prätzlich",
            "Jonathan Driedger"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417753",
        "url": "https://doi.org/10.5281/zenodo.1417753",
        "ee": "https://zenodo.org/records/1417753/files/MullerPD12.pdf",
        "abstract": "The task of novelty detection with the objective of detect- ing changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of mu- sic, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempo- based novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individ- ual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be con- sistent across different performances. This hypothesis is supported by our experiments in the context of music struc- ture analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings.",
        "zenodo_id": 1417753,
        "dblp_key": "conf/ismir/MullerPD12",
        "keywords": [
            "novelty detection",
            "musical properties",
            "structural properties",
            "performance style",
            "tempo-related properties",
            "individual performance",
            "common musical time axis",
            "musically relevant points",
            "cross-version fusion curves",
            "better results"
        ],
        "content": "ACROSS-VERSION APPROACH FORSTABILIZING\nTEMPO-BASED NO V\nELTYDETECTION\nMeinard M ¨uller1,3, Thomas Pr ¨atzlich2,3, and JonathanDriedger1,2\n1BonnUniversity,2Saarland University,3MPIInformatik\n{meinard,tpratzli }@mpi-inf.mpg.de, driedger@iai.uni-bonn.de\nABSTRACT\nThe task of novelty detection with the objective of detect-\ningchangesregardingmusicalpropertiessuchasharmony,\ndynamics, timbre, or tempo is of fundamental importance\nwhen analyzing structural properties of music recordings.\nBut for a speciﬁc audio version of a given piece of mu-\nsic, the noveltydetection result may also crucially depend\non the individual performance style of the musician. This\nparticularlyholdstrue fortempo-relatedproperties,which\nmayvarysigniﬁcantlyacrossdifferentperformancesofthe\nsame piece of music. In this paper, we show that tempo-\nbasednoveltydetectioncanbestabilizedandimprovedby\nsimultaneously analyzing a set of different performances.\nWe ﬁrst warpthe version-dependentnoveltycurvesontoa\ncommonmusical time axis, and thencombinethe individ-\nualcurvestoproduceasinglefusioncurve. Ourhypothesis\nis that musically relevantpointsof noveltytend to be con-\nsistent across different performances. This hypothesis is\nsupportedbyourexperimentsinthecontextofmusicstruc-\nture analysis, where the cross-version fusion curvesyield,\non average, better results than the noveltycurvesobtained\nfromindividualrecordings.\n1. INTRODUCTION\nMusic is highly structured data. Structure in music arises\nfrom repetitions, contrasts and homogeneity in musical\naspects such as melody, dynamics, harmony, timbre or\ntempo [12]. The extraction of the musical structure from\naudio recordingsis an important task in the ﬁeld of music\ninformation retrieval. It consists of a segmentation prob-\nlem, wherethegoal isto ﬁndthe boundariesthat markthe\ntransitions between two structural parts, and a musically\nmeaningful labeling (e.g. chorus, verse, ﬁrst theme, sec-\nond theme) of the segments, see [3,12] for an overview.\nIn many cases, segment boundariesare accompanied by a\nchange in instrumentation, dynamics, harmony, tempo, or\nsome other characteristics. The task of novelty detection\nistospecifypointswithinagivenaudiorecordingwherea\nhumanlistenerwouldrecognizesuchachange[6,9,14,15].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2012 International Society for MusicInformation Retrieval.Such points of novelty are not only of musical relevance,\nbut also allow for speeding up further music analysis\ntasks[11].\nInthispaper,wepresentageneralapproachforstabiliz-\ningnovelty-basedsegmentationtechniques. Following[6],\nweﬁrstconverttheaudiosignalintoasuitablefeaturerep-\nresentation, compute a self distance matrix, and derive a\nnovelty curve by detecting 2D corner points in this ma-\ntrix. The choice of features (e.g. MFCCs, chroma fea-\ntures,tempogramfeatures)dependsonthemusicalaspects\n(e.g. timbre, harmony, tempo) of interest [9]. In the fol-\nlowing, we consider the aspect of tempo using the cyclic\ntempogram features as proposed in [7] as an illustrative\nexample. Particularly in classical music, there often exist\nmanydifferentrecordingsforagivenpieceofmusic. Even\nthough all recordings follow the same musical score, two\ndistinct versions may differ signiﬁcantly in performance\naspects regardingtempo, dynamics, or timbre. This is the\nreasonwhynoveltydetectionresultsoftenvaryacrossdif-\nferentaudioversions.\nThemaincontributionofthispaperis toapplythenov-\nelty detection simultaneously to a set of different perfor-\nmances of a given piece. To this end, using a score-based\nMIDI reference,we convertthephysicaltime axis(in sec-\nonds) of all version-dependentnovelty curves into a com-\nmon musical time-axis (in measures) . Then we combine\ntheindividualcurvesintoacross-versionfusioncurve,see\nFigure 1 foran overview. Assuming that the musically in-\nteresting points of novelty are consistent across the differ-\nent versions, we expect the fusion curve to be more stable\nand musically meaningful than the individual curves. Ap-\nplyingourcross-versionnoveltydetectionapproachforlo-\ncating segment boundariesin music structure analysis, we\nshow that the fusion curves yield, on average, better re-\nsultsthantheversion-dependentnoveltycurvesofindivid-\nualrecordings. Thiseffectbecomesmoreprominent,when\nthereisahighperformancevarianceacrosstherecordings,\nwhichistypicallythe casefortheaspectoftempo.\nCross-version strategies have previously been applied\nfor other music analysis tasks. For example, multiple per-\nformancesareusedin[1]tosupporttempotracking,in[10]\nto stablize chord labeling, and in [8] to detect critical pas-\nsages in a piece of music that are prone to beat tracking\nerrors.\nThe remainder of this paper is organized as follows.\nIn Section 2, we describe the various steps of our cross-\nversion novelty detection procedure. Then, in Section 3,  \n0 10 20 30 40 50 60 70 80 9010203040\n00.010.020.030.04\n  \n10 20 30 40 50 6010203040\n00.010.020.030.04\n10 20 30 40 50 6000.0050.010.015\n0.8 9 17 25 33 41 4953 60.9A1A1 B1 A2 B1 A2 C1D1A3\n  Version\nTimein secondsVersion\nTime inmeasures\nTime inmeasures\nTime inmeasures(a)\n(b)\n(c)\n(d)\n(e)\nFigure 1: Overview of the cross-version novelty detection\npipeline for Chopin’s Mazurka Op. 7 No. 4. (a)Waveforms\nof several performances. (b)Individual novelty curves (color-\ncoded) for 43performances. Eachrow of the matrixcorresponds\nto one novelty curve. (c)Individual novelty curves warped to\na common musical time axis (in measures). (d)Fusion novelty\ncurve.(e)Annotated structure andsegment boundaries.\nwe give a detailed quantitative evaluation of our proce-\ndure within a structure analysis scenario for Chopin’s Pi-\nanoMazurkas. Furthermore,wecriticallyassesstheresults\nby a musically informeddiscussion of concrete examples.\nFinally,weconcludewithSection4indicatingfuturework.\n2. CROSS-VERSIONNOVELTY DETECTION\nIn this section, we describe the pipeline for our cross-\nversion approach to novelty detection. For the purpose\nof illustration, we concentrate on the musical aspect of\ntempo using a cyclic tempogram feature representation\n(Section 2.1). As for the novelty detection, we follow a\nstandard procedure based on 2D corner detection in self\ndistances matrices (Section 2.2). Applying music syn-\nchronizationtechniques,weshowhowtowarpthenovelty\ncurvesontoa version-independentmusicaltime axis(Sec-\ntion 2.3). Finally, we describe how to merge the novelty\ncurves based on a late-fusion strategy (Section 2.4). This\npipelineisalsoillustratedbyFigure1.\n2.1 CyclicTempogramFeatures\nIn a ﬁrst step, the given audio recording is transformed\ninto a suitable feature representationthat capturesthe mu-\nsical aspects of interest. As an example, we consider the\ncase of tempo-based novelty detection, even though ourcross-version approach is applicable to any kind of fea-\nture representation. In the following, we revert to cyclic\ntempogram features as introduced in [7]. These features\nconstitutearobustmid-levelrepresentationencodinglocal\ntempo information. In a ﬁrst step, we capture changes in\nthe signal’s energy and spectrum [2] and then apply win-\ndowed autocorrelation methods [4]. Afterwards, the lag-\naxis is converted into a tempo axis speciﬁed in beats per\nminute (BPM), yielding a tempogram as shown in Fig-\nure 2c. Forming tempo equivalence classes by binning\ntempithatdifferbyapoweroftwoandquantizingtheval-\nuesofthe resultingcyclictempogramyieldsanevenmore\nrobust feature representation,see Figure 2d. In our exper-\niments we use a feature resolution of 5 Hz (ﬁve feature\nvectorspersecond)andafeaturedimensionof 10(tenfea-\nture values per vector). A free MATLAB implementation\nof these features is part of the tempogram toolbox.1For\nfurtherdetailswe referto [7].\n2.2 NoveltyCurve\nLetX= (x1,...,x N)denote the resulting feature se-\nquence. To compute a novelty curve from this sequence,\nwe employ a standard approach introduced by Foote [6].\nTo this end, an N×Nself distance matrix D(n,m) :=\nd(xn,xm)iscomputedusingthelocaldistancefunction\nd(xn,xm) = 1−exp/parenleftbigg/angbracketleftxn,xm/angbracketright\n/bardblxn/bardbl/bardblxm/bardbl−1/parenrightbigg\n,\nfor1≤n,m≤N. Then,Dis\nanalyzed by correlat-\ning a kernel along its main diagonal. The kernel consists\nof anM×Mmatrix (with M < N) which has a 2×2\ncheckerboard-likestructureweightedby a Gaussian radial\nfunction. This yields a novelty curve, the peaks of which\nindicate changes in the musical aspect represented by the\nfeature type (in our case, tempo changes), see Figure 2e.\nWe further process the novelty curve by subtracting a lo-\ncal average, see Figure 2f. In our experiments, a value M\ncorresponding to 7 seconds has turned out to be suitable,\nsee Section3.2andFigure3forafurtherdiscussionofthe\nparameter M.\n2.3 Time AxisConversion\nThe computed novelty curve depends on the performance\ncharacteristicsoftheunderlyingmusicrecording. Tomake\nnovelty curves comparable across different recordings of\nthesamepieceofmusic,weconverttheversion-dependent\nphysical time axis (in seconds) to a version-independent\nmusical time axis (in measures). To this end, we assume\nthat we are given a score-like MIDI version of the piece\nwithexplicitbeatandmeasurepositions. Then,foragiven\nmusic recording, we apply music synchronization tech-\nniques to automatically align the MIDI version with the\naudio version.2The alignment result allows for transfer-\nring the beat and measure positionsspeciﬁed by the MIDI\n1www.mpi-inf.mpg.de/resources/MIR/tempogramtoolbox\n2In our implementation, we revert to the high-resolution music syn-\nchronization approach described in [5].Allegro ma non troppo Poco più vivo 29\n43\n43\n54 56 58 60 62 64\n  \n54 56 58 60 62 6450100150200250300350400\n0.050.10.150.20.250.30.35\n  \n54 56 58 60 62 6411.11.211.331.51.661.81\n0.20.40.60.8\n54 56 58 60 62 6400.020.040.060.08\n54 56 58 60 62 6400.020.040.060.08\n54 56 58 60 62 6400.020.040.060.08\n29 30 31 32 33 34 35 36 3700.020.040.060.08\n29 30 31 32 33 34 35 36 3700.020.040.060.08\n  \n29 30 31 32 33 34 35 36 3700.02Time insecondsTempoin BPM\nTime insecondsRelative tempo\nTime inseconds\nTime inseconds\nTime inseconds\nTime inseconds\nTimein measures\nTimein measures\nTimein measures(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure2: NoveltydetectionforarecordingofChopin’sMazurka\nOp. 68 No. 3. (a)Measures 29-36.(b)Waveform. (c)Tem-\npogram. (d)Quantized cyclic tempogram. (e)Novelty curve\n(solid line) and its local average curve (dashed line). (f)Post-\nprocessed novelty curve. (g)Time axis conversion. (h)Resam-\npled noveltycurve. (i)Color-coded representation of (h).\nversion to the corresponding time positions in the audio\nversion. Based on this information,we locally stretch and\ncontract the time axisof the noveltycurvecomputedfrom\nthe recordingto obtaina musical time axis, see Figure2g.\nFinally, we interpolate and resample the novelty curve to\nobtainonevalueforeachbeatpositionofthe pieceofmu-\nsic, see Figure2handFigure2i.2.4 Fusion NoveltyCurve\nBeing based on the same musical time axis, one can now\ndirectly compare novelty curves from different perfor-\nmances of the same piece of music. As an example, Fig-\nure 1b shows the original novelty curves (in some color-\ncoded form) for 43different performances of Chopin’s\nMazurka Op. 7 No. 4. No correlationsacross the different\nperformances are visible. After the time axis conversion,\nasshowninFigure1c,strongcorrelationsbetweenthedif-\nferent novelty curves become evident. For example, there\nis a tempo change at measure 52for basically all perfor-\nmances.\nTo fuse the information across all novelty curves, we\nbasically compute the average of the novelty curves. To\nbecome more robust to outliers, we ﬁrst remove the 20%\nsmallest and largest novelty values for each beat position\namongallperformances,andthencomputethe fusionnov-\nelty curve by taking the beat-wise arithmetic mean of the\nremaining values. The crucial observation is that a fusion\nnoveltycurverevealsalocalmaximum(peak)atthosepo-\nsitions where a large number of individual novelty curves\nalso possess a local maximum. In other words, the fusion\nnoveltycurveexpressestheconsistenciesinthepeakstruc-\nturesacrossthevariousrecordings,see alsoFigure1d.\n3. EXPERIMENTS\nEven though there are often signiﬁcant differences in the\nway musicians interpret a piece of music, tempo changes\nare not arbitrary and there are musical reasonsfor a speed\nuporslowdown. Ourhypothesisisthatthetempochanges\nthatcanbeobservedacrossalargenumberofdifferentper-\nformances are of particular musical importance. There-\nfore, we conjecture that peaks of the fusion novelty curve\nare more relevant than the peaks of the individual novelty\ncurves. To investigate our hypothesis, we have conducted\nvarious experiments on a dataset consisting of Chopin’s\nMazurkas (Section 3.1). Our quantitative evaluation in\nthe context of music structure analysis (Section 3.2) as\nwell as a discussion of various representative examples\n(Section 3.3) demonstratethat cross-versionfusion curves\nyield,onaverage,betterresultsthanthenoveltycurvesob-\ntainedfromindividualrecordings.\n3.1 Datasetand Annotations\nWe conduct our experiments on a Mazurka dataset,\nwhich consists of 2792recorded performances for the 49\nMazurkasbyFr´ ed´ ericChopin. Theserecordingswerecol-\nlected in the Mazurka Project3and have been previously\nused, e.g., for the purpose of performance analysis [13].\nFor each of the 49Mazurkas, there are on average 57dif-\nferent recordings (ranging from the early stages of music\nrecordinguntiltoday),aswellasaMIDIﬁlethatrepresents\nthe piece in an uninterpretedsymbolic form. In particular,\nmeasureandbeatpositionsareknownintheMIDIﬁle.\nThe Chopin Mazurkas are short piano compositions\nwith a 3/4 time signature. These pieces have a relatively\n3mazurka.org.ukclear musical structure, where certain parts are repeated\nmore or less i n\nthe same way. We have manually anno-\ntated each score-like MIDI ﬁle according to its musical\nstructure. Onaveragethisleadsto 9.4segmentboundaries\nper Mazurka (disregarding segment boundaries at the be-\nginning and end of the piece) and an average duration of\n11.9measures per musical part, see also Table 1 for more\ndetails.\n3.2 QuantitativeEvaluation\nAs is the case for romantic piano music, most Mazurka\nperformancesrevealnumerouslocaltempochangeswhich\noften indicate transitions of musical importance. Many of\nthese transitions occur near segment boundaries between\nmusicalparts,whereonecanoftenobservetempochanges.\nEven though not all segment boundariesare characterized\nthis way, we use them for a ﬁrst quantitative evaluation to\nindicate the behavior of our cross-version fusion novelty\ncurves. Let Bdenotethe set of segmentboundaries(spec-\niﬁed inmusicalbeats)foragivenMazurka.\nFor a novelty curve (with time axis given in musical\nbeats), we perform some peak picking to determine a set\nPof relevant peak positions. Here a position is consid-\nered relevant if the novelty curve assumes at this position\na global maximum over a window of length λcentered at\nthe correspondingposition. In our experiments, the value\nλ= 19 beats has turned out to be meaningful, see also\nFigure 3. A peak position in Pis considered to be true\nif there is a segment boundary in Bin aδ-neighborhood,\notherwiseitisconsideredtobe false. Thisallowstodeﬁne\na precision(P), recall(R),andF-measure(F)fortheset P\nrelative to B. In our experiments,we choose δ= 3 beats\ncorresponding to a musical measure. In our evaluation,\nwe furtherignoredall boundariesand all peaks in the ﬁrst\nfour and last four measuresof a piece of music. The main\nreason for excluding these measures is that many of the\nrecordingsstart and end with non-musicalcontentsuch as\nsilence or applause, which leads to spurious peaks at the\npositions where the music starts or ends. Also, synchro-\nnizationerrorstypicallyoccurin theseregions.\nBeforeweinvestigatetheroleofthevariousparameters,\nwe ﬁrst look at the results for a ﬁxed parameter setting as\nindicatedbyTable1. Tobetterunderstandtheeffectofthe\ncross-version approach, we computed P/R/F-measures in\ntwo different ways. First, for a given Mazurka, we com-\nputedindividualP/R/F-measuresforeachperformanceus-\ning the version-dependent novelty curves and then aver-\naged over all performances to obtain averaged individ-\nual P/R/F-measures. Secondly, we computed these mea-\nsuresfromthefusionnoveltycurvetoobtaincross-version\nP/R/F-measures. Table 1 shows the resulting averaged in-\ndividualaswellascross-versionP/R/F-measuresforall of\nthe49Mazurkas. Furthermore,thelastrowofthetablein-\ndicates the overall values averaged over all Mazurkas. As\nthe main result, one can see that the overall F-measure\nobtained from individual novelty curves is F= 0.39,\nwhereas the overall F-measure obtained from the fusion\nnovelty curves is F= 0.52. In other words, the tempo-Ind.-Version Cross-Version\nPiece #P #M #B P R F P R F\nM06-1 49 112 7 0.29 0.60 0.39 0.50 1.00 0.67\nM06-2 51 96 9 0.42 0.63 0.50 0.47 0.89 0.62\nM06-3 47 98 12 0.23 0.31 0.26 0.13 0.18 0.15\nM06-4 46 40 9 0.65 0.40 0.49 0.83 0.63 0.71\nM07-1 55 104 9 0.30 0.52 0.38 0.44 0.89 0.59\nM07-2 51 120 14 0.40 0.45 0.42 0.36 0.36 0.36\nM07-3 65 105 11 0.31 0.44 0.36 0.47 0.64 0.54\nM07-4 43 60 7 0.51 0.60 0.55 0.75 0.86 0.80\nM07-5 46 20 12 0.61 0.35 0.44 1.00 0.60 0.75\nM17-1 52 100 10 0.25 0.35 0.29 0.07 0.10 0.08\nM17-2 55 68 3 0.21 0.65 0.32 0.25 1.00 0.40\nM17-3 51 168 10 0.26 0.64 0.37 0.42 1.00 0.59\nM17-4 93 132 10 0.23 0.49 0.31 0.35 0.67 0.46\nM24-1 61 96 10 0.30 0.40 0.34 0.46 0.60 0.52\nM24-2 66 120 16 0.38 0.48 0.42 0.50 0.64 0.56\nM24-3 55 79 6 0.26 0.46 0.33 0.45 0.83 0.59\nM24-4 76 186 20 0.42 0.59 0.49 0.65 0.85 0.74\nM30-1 50 53 4 0.34 0.57 0.42 0.50 0.75 0.60\nM30-2 60 64 7 0.48 0.60 0.53 0.78 1.00 0.88\nM30-3 63 111 10 0.30 0.45 0.36 0.50 0.60 0.55\nM30-4 65 139 14 0.34 0.51 0.40 0.47 0.62 0.53\nM33-1 55 48 4 0.43 0.62 0.51 0.50 0.75 0.60\nM33-2 70 143 16 0.34 0.47 0.39 0.33 0.44 0.38\nM33-3 50 48 3 0.28 0.61 0.38 0.33 1.00 0.50\nM33-4 74 224 19 0.24 0.40 0.30 0.27 0.37 0.31\nM41-1 56 139 14 0.26 0.39 0.32 0.19 0.29 0.23\nM41-2 63 68 7 0.43 0.57 0.49 0.75 0.86 0.80\nM41-3 40 78 13 0.44 0.45 0.44 0.57 0.73 0.64\nM41-4 45 74 9 0.42 0.58 0.48 0.62 0.89 0.73\nM50-1 49 104 6 0.18 0.48 0.26 0.20 0.50 0.29\nM50-2 58 127 10 0.31 0.56 0.40 0.56 0.90 0.69\nM50-3 74 208 10 0.18 0.57 0.27 0.21 0.70 0.33\nM56-1 42 204 13 0.18 0.41 0.25 0.11 0.23 0.15\nM56-2 53 92 8 0.31 0.60 0.41 0.54 1.00 0.70\nM56-3 57 220 15 0.23 0.51 0.32 0.30 0.67 0.42\nM59-1 63 142 9 0.19 0.42 0.26 0.17 0.44 0.25\nM59-2 63 111 4 0.10 0.41 0.16 0.20 0.75 0.32\nM59-3 66 154 11 0.20 0.44 0.28 0.30 0.55 0.39\nM63-1 46 102 9 0.26 0.46 0.33 0.27 0.44 0.33\nM63-2 65 56 4 0.33 0.61 0.43 0.38 0.75 0.50\nM63-3 88 76 8 0.41 0.55 0.47 0.78 0.88 0.82\nM67-1 44 60 7 0.23 0.31 0.26 0.14 0.17 0.15\nM67-2 41 72 6 0.33 0.54 0.41 0.45 0.83 0.59\nM67-3 46 56 3 0.30 0.75 0.43 0.43 1.00 0.60\nM67-4 59 112 6 0.26 0.71 0.38 0.40 1.00 0.57\nM68-1 46 84 11 0.53 0.63 0.57 0.50 0.50 0.50\nM68-2 65 84 11 0.49 0.54 0.51 0.77 0.91 0.83\nM68-3 51 60 9 0.61 0.55 0.57 0.78 0.78 0.78\nM68-4 63 63 7 0.35 0.37 0.36 0.47 0.58 0.52\n∅ 57.0 103.7 9.4 0.33 0.51 0.39 0.45 0.69 0.52\nTable 1: Overview of the Mazurka dataset and precision (P), re-\ncall (R), an d\nF-measures (F) for two different settings. The ﬁrst\nfourcolumnsspecifytheMazurka(e.g. M06-1referstoMazurka\nOp. 6 No. 1), the number of performances (#P), the number of\nmeasures (#M), and the number of annotated segment bound-\naries (#B). The next three columns show the average individ-\nual P/R/F-measures obtained from individual performances and\nthe last three columns show cross-version P/R/F-measures ob-\ntained from the fusion novelty curves. The used parameters are:\nM∼7 seconds ,λ= 19 beats ,δ= 3 beats .\nbased novelty detection can indeed be improved when si-\nmultaneouslyanalyzinga set ofdifferentperformances.\nIn the next experiments, we investigate the role of the\nkernel size parameter M(see Section 2.2) and the neigh-\nborhood parameter λused in the peak picking. Figure 3\nshows the cross-versionP/R/F-measures averagedoverall\n49Mazurkasforvariouscombinationsof Mandλ. Gener-\nally,whenincreasing λ,theprecisionincreases(Figure3a)\nand the recall decreases (Figure 3b). This is not surpris-\ning, since an increase in λimposes stricter conditions on\nthe peak picking (and the set of relevant peaks becomes\nsmaller). The remainingpeaks tend to be true (increase in\nprecision), while fewer segment boundaries in Bare de-\ntected (decrease in recall). The kernel size parameter M\nhas a minor inﬂuence on the ﬁnal results. Only for large\nvalues of λ, smaller kernel sizes tend to be favorable. As  \n711151923273135393\n5\n7\n0.20.30.40.5\n  \n711151923273135393\n5\n7\n0.50.60.70.80.9\n  \n711151923273135393\n5\n7\n0.350.40.450.5Minseconds\nλinbeats\nMinseconds\nλinbeatsMinseconds\nλinbeats(a) (b)\n(c)\nFigure 3: Average cross-version P/R/F-measures for different\nparameter settings. (a)Average precision values. (b)Average\nrecall values. (c)Average F-measure values. The red circle indi-\ncates the parameter settingusedinTable 1.\n  \n10 20 30 40 50 601020304050\n00.010.020.030.040.05\n10 20 30 40 50 6000.0020.0040.006\n1 9 172125 33374145 53 60.9A1A1 A2 B1B2 A3 CD1D2 A4 A2Version\nTime inmeasures\nTime inmeasures\nTime inmeasures(a)\n(b)\n(c)\nFigure 4: Novelty curves for M68-3.(a)Individual novelty\ncurves (color-coded, musical time axis)for 51performances. (b)\nFusion novelty curve. True peaks are indicated by green discs\nand false peaks by red crosses. The gray areas at the beginning\nand end are left out in the evaluation. The thin gray curve indi-\ncates the peak pickingcondition introduced bythe neighborhood\nparameter λ.(c)Annotated structure and segment boundaries.\nforourmainexperiments,wefavoredcomparativelylarger\nkernel sizes (resulting in smoother novelty curves) and a\nsmallerλ(beinglessrestrictiveinthepeakpicking)choos-\ningM∼7 seconds andλ= 19 beats . However, as\nalsoindicatedbyFigure3c,thespeciﬁcparamtersettingis\nnotofcrucialimportanceandslightlychangingthesettings\nyieldssimilar experimentalresults.\n3.3 QualitativeEvaluation\nFor some Mazurkas this improvement is signiﬁcant. For\nexample, for the Mazurka Op. 7 No. 4 shown in Figure 1,\nthe F-measure increases from F= 0.55(individual) to\nF= 0.80(cross-version). Also for the Mazurka Op. 68\nNo. 3 (Figure 4) the cross-version fusion approach stabi-\nlizes the tempo-based novelty detection improving the F-\nmeasure from F= 0.57(individual) to F= 0.78(cross-\nversion).\nHowever,thereisalsoanumberofMazurkaswhereone\nhas rather low P/R/F-measures—for the individual curves\nas well as for the fusion novelty curves. For example,\nfor the Mazurka Op. 56 No. 1 shown in Figure 5, the F-\n  \n20 40 60 80100 120 140 160 180 20010203040\n00.0050.010.0150.020.0250.03\n20 40 60 80100 120 140 160 180 20000.0020.004\n1 23 45617781 103119135143 165181189197204.9A1A1 A2 B1B2C1A3 B3B4C2A4 D1E1E2F1Version\nTime inmeasures\nTime inmeasures\nTime inmeasures(a)\n(b)\n(c)\nFigure 5: Novelty curves for M56-1as in Figure 4. (a)Indi-\nvidual novelty curves for 42performances. (b)Fusion novelty\ncurve.(c)Annotated structure andsegment boundaries.\n20 40 60 80 100 12000.0020.004\n0.68.616.624.632.640.548.656.56573818997104.6112.6120.9A1A1A2A1A2B1A3B1A3C1D1C2D1C2A1A2\n29 30 31 32 33 343\npoco rall.43\n43\n29 30 31 32 33 34 3500.0020.004aTempoTime inmeasu r\nes\nTime inmeasures\nTime inmeasures(a)\n(b)\n(c)\n(d)\nFigure6: Detailedexampleon M07-2.(a)Fusionnoveltycurve.\n(b)Annotated structure and segment boundaries. (c)Score ex-\ncerpt of measures 29-34.(d)Fusion novelty curve excerpt of\nmeasures 29-34.\nmeasure even decreases from F= 0.25(individual) to\nF= 0.15(cross-version). Inthispiece,theannotatedseg-\nmentsareratherlongincomparisontotheotherMazurkas.\nListeningtotheperformancesrevealsthateach A-partcon-\nsists of several phrases, which are shaped by most pi-\nanistsusingacharacteristictempoprogressionwithaslow\ndown and speed up at phrase boundaries. These tempo\nchangeslead to a large numberof consistent peaks, which\narenotreﬂectedbyourstructureannotations(eventhough\nthe peaks are musically meaningful) and sometimes also\nnot capturedby our peak picking(λ being too restrictive).\nAlso,intheotherpartsthereareanumberoffalsepositive\npeaksoflessmusicalsigniﬁcance. AsthisMazurkashows,\nannotated segment boundaries do not need to go along\nwith tempo changes and, vice versa, musically meaning-\nful tempo changes may also occur within musical parts.\nTherefore, our quantitative evaluation within the structure\nanalysis context, even though indicating meaningful gen-\neraltendencies,is anoversimpliﬁcation.\nWe now discuss some further typical examples where\nthe fusion novelty curve reveals musically relevant tempo\nchanges that do not concur with segment boundaries. Let\nuslookatthefusionnoveltycurveforMazurkaOp.7No.2\nasshowninFigure6a. Hereonecannoticestrongpeaksin10 20 30 40 50 60 70 80 9000.0050.01\n1513 29 37 45 53 61 77 92.9AAB1 C1 D DE1 E2 F C2\n29 30 31 32 33 34 3536\n3 343\n43\n29 30 31 32 33 34 35 36 3700.0050.01Time inmeasures\nTime inmeasu r\nes\nTime inmeasures(a)\n(b)\n(c)\n(d)\nFigure7: Detailedexampleon M56-2.(a)Fusionnoveltycurve.\n(b)Annotated structure and segment boundaries. (c)Score ex-\ncerpt of measures 29-36.(d)Fusion novelty curve excerpt of\nmeasures 29-36.\ntheA2-parts and A3-parts located roughly two measures\nbefore segment boundaries, so that these peaks are con-\nsidered false positives in our evaluation. Looking at the\nscore of the piece revealsthat there is actuallya tempoin-\nstruction a Tempojust two measures before the respective\nsegments boundaries, see Figure 6c. Most pianists realize\nthis instruction by speeding up their performances, which\nleadstothemusicallyrelevantpeakscapturedbyourcross-\nversion novelty curve. As another example, let us look at\nthefusionnoveltycurveofMazurkaOp.56No.2,seeFig-\nure 7. Here, two of the false peak positions in the fusion\nnoveltycurvearelocatedinthemiddleofthetwo D-parts,\nsee Figure7a. Amanualinvestigationshowedthateachof\nthe eight-measure D-parts consists of two repeating four-\nmeasure phrases. This substructure is not reﬂected by our\nstructure annotations. The pianists, however, shape the\nphrases by a pronounced tempo change. Furthermore, in\nthe middle of the C-parts and the F-part, Figure 7 also\nshows some false positive peaks of no musical relevance.\nHere,animprovedpeakpickingmayremedythisproblem.\n4. CONCLUSIONS\nIn this paper, we introduced a cross-version approach for\nnovelty detection capturing consistencies across different\nperformancesof a piece. Applying this concept to tempo-\nrelatedaudiofeatures,we showedthat the resultingfusion\nnoveltycurvesperformbetterinrevealingmusicallymean-\ningful points of novelty than the individual curves. In the\nfuture, we plan to conduct similar experiments using dif-\nferent audio features that reﬂect not only tempo, but also\nharmony,timbre,anddynamics. Also,thedescribedcross-\nversion approachis generic in the sense that it can also be\nappliedtoothermusicanalysistasksbeyondnoveltydetec-\ntion. Astabilizationeffecthasalsobeenreportedforchord\nlabeling and beat tracking, and we plan to apply this con-\ncept to general structure analysis. Finally, we discussed\nthat our evaluation of the novelty detection results based\non segment boundaries indicates interesting general ten-\ndencies, but also constitutes an oversimpliﬁcation. Here,future work must address the evaluation problem by in-\ncludingmoremusicologicalknowledge,e.g.bylookingat\nexpected tempo changes in the score, annotated by musi-\ncally trainedexperts. On the otherhand,ourcross-version\napproachmightnot onlybe usedforthe taskof audioseg-\nmentation,butmayalsoaidasaperformanceanalysistool\nformusicologists.\nAcknowledgments: This work has been supported by the Clus-\nter of Excellence on Multimodal Computing and Interaction at\nSaarland University and the German Research Foundation (DFG\nMU 2686/5-1).\n5. REFERENCES\n[1] Andreas Arzt, Johannes Kepler, and Gerhard Widmer. Simple tempo\nmodelsforreal-timemusictracking.In Proceedings oftheSoundand\nMusic Computing Conference (SMC) , 2010.\n[2] Juan Pablo Bello, Laurent Daudet, Samer Abdallah, Chris Duxbury,\nMike Davies, and Mark B. Sandler. A tutorial on onset detection in\nmusic signals. IEEE Transactions on Speech and Audio Processing ,\n13(5):1035–1047, 2005.\n[3] Roger B. Dannenberg and Masataka Goto. Music structure analy-\nsis from acoustic signals. In David Havelock, Sonoko Kuwano, and\nMichaelVorl¨ ander, editors, HandbookofSignalProcessinginAcous-\ntics, volume1,pages 305–331. Springer, New York,NY,USA,2008.\n[4] Daniel P.W. Ellis. Beat tracking by dynamic programming. Journal\nofNew Music Research , 36(1):51–60, 2007.\n[5] SebastianEwert,MeinardM¨ uller,andPeterGrosche.Highresolution\naudiosynchronization usingchromaonsetfeatures.In Proceedingsof\nthe IEEEInternational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP),pages 1869–1872, Taipei, Taiwan, 2009.\n[6] Jonathan Foote. Automatic audio segmentation using a measure of\naudio novelty. In Proceedings of the IEEE International Conference\non Multimedia and Expo (ICME) , pages 452–455, New York, NY,\nUSA,2000.\n[7] Peter Grosche, Meinard M¨ uller, and Frank Kurth. Cyclic tempogram\n– amid-level tempo representation for music signals. In Proceedings\nof IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP),pages 5522–5525,Dallas, Texas, USA,2010.\n[8] Peter Grosche, Meinard M¨ uller, and Craig Stuart Sapp. What makes\nbeat tracking difﬁcult? A case study on Chopin Mazurkas. In Pro-\nceedings of the 11th International Conference on Music Information\nRetrieval (ISMIR) , pages 649–654, Utrecht, TheNetherlands, 2010.\n[9] Kristoffer Jensen. Multiple scale music segmentation using rhythm,\ntimbre, and harmony. EURASIP Journal on Advances in Signal Pro-\ncessing, 2007(1):11 pages, 2007.\n[10] Verena Konz, Meinard M¨ uller, and Sebastian Ewert. A multi-\nperspective evaluation framework for chord recognition. In Proceed-\nings of the International Society for Music Information Retrieval\nConference (ISMIR) ,pages 9–14, Utrecht, TheNetherlands, 2010.\n[11] Jouni Paulus and Anssi P. Klapuri. Music structure analysis us-\ning a probabilistic ﬁtness measure and a greedy search algorithm.\nIEEE Transactions on Audio, Speech, and Language Processing ,\n17(6):1159–1170, 2009.\n[12] Jouni Paulus, Meinard M¨ uller, and Anssi P. Klapuri. Audio-based\nmusic structure analysis. In Proceedings of the 11th International\nConference onMusicInformation Retrieval (ISMIR) ,pages625–636,\nUtrecht, TheNetherlands, 2010.\n[13] Craig Stuart Sapp. Comparative analysis of multiple musical perfor-\nmances.In Proceedings oftheInternational Conference onMusicIn-\nformation Retrieval (ISMIR) , pages 497–500, Vienna, Austria, 2007.\n[14] Douglas Turnbull, Gert Lanckriet, Elias Pampalk, and Masataka\nGoto.Asupervised approach fordetecting boundaries inmusicusing\ndifference features and boosting. In Proceedings of the International\nConference on Music Information Retrieval (ISMIR) , pages 51–54,\nVienna, Austria, 2007.\n[15] George Tzanetakis and Perry Cook. Multifeature audio segmentation\nfor browsing and annotation. In Proceedings of the IEEE Workshop\non Applications of Signal Processing to Audio and Acoustics (WAS-\nPAA),pages 103–106, New Platz, NY, USA,1999."
    },
    {
        "title": "Learning Sparse Feature Representations for Music Annotation and Retrieval.",
        "author": [
            "Juhan Nam",
            "Jorge Herrera",
            "Malcolm Slaney",
            "Julius O. Smith III"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415202",
        "url": "https://doi.org/10.5281/zenodo.1415202",
        "ee": "https://zenodo.org/records/1415202/files/NamHSS12.pdf",
        "abstract": "We present a data-processing pipeline based on sparse feature learning and describe its applications to music an- notation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are hand- crafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features auto- matically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning al- gorithms to music data, in particular, focusing on a high- dimensional sparse-feature representation. Our experi- ments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and re- trieval systems.",
        "zenodo_id": 1415202,
        "dblp_key": "conf/ismir/NamHSS12",
        "keywords": [
            "sparse feature learning",
            "systemic approach",
            "high-dimensional sparse-feature representation",
            "unsupervised algorithms",
            "content-based music annotation",
            "state-of-the-art music annotation and retrieval systems",
            "audio processing",
            "MFCC",
            "music retrieval",
            "music annotation"
        ],
        "content": "LEARNING SPARSE FEATURE REPRESENTATIONS FOR MUSIC\nANNOTATION AND RETRIEV AL\nJuhan Nam\nCCRMA\nStanford University\njuhan@ccrma.stanford.eduJorge Herrera\nCCRMA\nStanford University\njorgeh@ccrma.stanford.eduMalcolm Slaney\nYahoo! Research\nStanford University\nmalcolm@ieee.eduJulius Smith\nCCRMA\nStanford University\njos@ccrma.stanford.edu\nABSTRACT\nWe present a data-processing pipeline based on sparse\nfeature learning and describe its applications to music an-\nnotation and retrieval. Content-based music annotation\nand retrieval systems process audio starting with features.\nWhile commonly used features, such as MFCC, are hand-\ncrafted to extract characteristics of the audio in a succinct\nway, there is increasing interest in learning features auto-\nmatically from data using unsupervised algorithms. We\ndescribe a systemic approach applying feature-learning al-\ngorithms to music data, in particular, focusing on a high-\ndimensional sparse-feature representation. Our experi-\nments show that, using only a linear classiﬁer, the newly\nlearned features produce results on the CAL500 dataset\ncomparable to state-of-the-art music annotation and re-\ntrieval systems.\n1. INTRODUCTION\nAutomatic music annotation (a.k.a. music tagging) and re-\ntrieval are hot topics in the MIR community, as large col-\nlections of music are increasingly available. Therefore,\ntasks such as music discovery have become progressively\nharder for humans without the help of computers. Exten-\nsive research has been done on these topics [20], [11], [8]\n[5]. Also, different datasets have become standards to train\nand evaluate these automatic systems [19], [12].\nTraining for most automatic systems use audio\ncontent—in the form of audio features—as the input data.\nTraditionally well-known audio features, such as MFCC,\nchroma and spectral centroid, are used to train algorithms\nto perform the annotation and retrieval tasks. These “hand-\ncrafted” features usually capture partial auditory character-\nistics in a highly condensed form, ignoring many details\nof the input data. While such engineered features have\nproven to be valuable, there is increasing interest in ﬁnd-\ning a better feature representation by learning from data in\nan unsupervised manner. Unsupervised learning is usually\nconducted either by mapping the input data into a high-\ndimensional sparse space or by means of deep learning.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.In this paper, we apply high-dimensional sparse feature-\nlearning to short-term audio spectrograms and construct\nsong-level features for music annotation and retrieval.\nIn summary, the contributions of this paper are as fol-\nlows:\n\u000fWe propose a data preprocessing method to make\nfeature-learning algorithms more effective.\n\u000fWe demonstrate that the feature-learning algorithms\ncapture rich local timbral patterns of music, useful\nfor discrimination.\n\u000fWe show that song-level features constructed from\nthe local features achieve results comparable to state-\nof-art algorithms on the CAL500 dataset using only\na linear classiﬁer, and furthermore outperform them\nwith a nonlinear classiﬁer.\n1.1 Recent Work\nLee et. al. proposed a Convolutional Deep Belief Network\n(CDBN) and applied it to the audio spectrogram for music\ngenre and artist classiﬁcation [14]. Dieleman et. al. also\nemployed the CDBN but on engineered features (EchoN-\nest chroma and timbre features) for artist, genre and key\nrecognition tasks [6]. Our approach is similar to these sys-\ntems in that the input data is taken from multiple audio\nframes as an image patch and max-pooling is performed\nfor scalable feature-learning. However, we perform feature\nlearning with a high-dimensional single-layer network and\nthe max-pooling separately after learning the features [2].\nWhile this can limit the representational power, it allows\nfaster and simpler training of the learning algorithms.\nHenaff et. al. applied a sparse coding algorithm to a\nsingle frame of constant-Q transform spectrogram and ag-\ngregated them into a segment-level feature for music genre\nclassiﬁcation [10]. Likewise, Schlter et. al. compared\nRestricted Boltzmann Machine (RBM), mean-covariance\nRBM and DBN on similarity-based music classiﬁcation\n[17]. Our approach is also similar to these pipelines. How-\never, in our work we provide deeper insight on the learned\nfeatures by showing how they are semantically relevant.\nIn addition, we investigate the effect of sparsity and max-\npooling on the performance.\nFinally, Hamel et. al. showed that simply PCA-whitened\nspectrogram can provide good performance by combining\ndifferent types of temporal pooling [9]. Our approach isWaveformAutomatic\nGain ControlTime-Freq.\nRepresentationPCA \nWhiteningFeature \nLearning \nAlgorithmMax-\nPoolingAggregation Classiﬁer\nAmplitude\nCompression\nPreprocessingFeature \nRepresentationMultiple\nFrames\nFigure 1: Data processing pipeline for feature representation. This takes an waveform as input and produces a song-level\nfeature for the classiﬁer.\nquite different from this work because we encode the PCA-\nwhitened spectrogram into a high-dimensional sparse space\nand extract features from it.\n2. DATA PROCESSING PIPELINE\nWe perform the music annotation and retrieval tasks using\nthe data processing pipeline shown in Figure 1. Each block\nin the pipeline is described in this section.\n2.1 Preprocessing\nData preprocessing is a very important step to make fea-\ntures invariant to input scale and to reduce dimensionality.\nWe perform several steps of the preprocessing.\n2.1.1 Automatic Gain Control\nMusical signals are dynamic by nature and each song ﬁle\nin a dataset has different overall volume due to different\nrecording conditions. Thus, we ﬁrst apply Automatic Gain\nControl (AGC) to normalize the local energy. In particular,\nwe employ time-frequency AGC using Ellis’ method [7].\nThe AGC ﬁrst maps FFT magnitude to a small number of\nsub-bands, computes amplitude envelopes for each band,\nand uses them to create a time-frequency magnitude en-\nvelope over a linear-frequency scale. Then, it divides the\noriginal spectrogram by this time-frequency envelope. As\na result, the AGC equalizes input signals so they have uni-\nformly distributed spectra over frequency bins.\n2.1.2 Time-frequency Representation\nA time-frequency representation is an indispensable pro-\ncessing step for musical signals, which are characterized\nprimarily by harmonic or non-harmonic elements. There\nare many choices of time-frequency representations, each\none having different time/frequency resolutions and/or per-\nceptual mappings. In this paper, we chose a mel-frequency\nspectrogram.\nOur initial experiments—based on a spectrogram—\nshowed that using multiple consecutive frames as an in-\nput unit for learning algorithms (which is analogous to tak-\ning a patch from an image) signiﬁcantly improves perfor-\nmance over using single frames. However, the FFT size\nused for musical signals is usually large and thus con-\ncatenating multiple frames yields a very high-dimensional\nvector requiring expensive computation for learning algo-\nrithms. Using a moderate number of mel-frequency bins,\ninstead of the straight FFT, preserves the audio content\nwell enough, while signiﬁcantly reducing the input di-\nmension. We chose 128 mel-frequency bins, followingHamel’s work [9], and will present results for various num-\nbers of frames below.\n2.1.3 Magnitude Compression\nWe compress the magnitude using an approximated log\nscale, log10(1 +CjX(t;f)j), wherejX(t;f)jis the mel-\nfrequency spectrogram and Ccontrols the degree of com-\npression [15]. In general, the linear magnitude of each bin\nhas an exponential distribution. Scaling with a log function\ngives the magnitude a more Gaussian distribution. This en-\nables the magnitude to be well-ﬁtted with the ensuing PCA\nwhitening, which has an implicit Gaussian assumption.\n2.1.4 Multiple Frames\nAs previously discussed, we take multiple frames as an\ninput unit for feature learning. This approach was used\nin the convolutional feature-learning algorithm [14]. In\nthat work, however, the multiple frames are taken over the\nPCA-whitening space where PCA is performed on single\nframes. In our case, we apply the PCA to multiple consec-\nutive frames for the reasons explained next.\n2.1.5 PCA Whitening\nPCA whitening is often used as a preprocessing step for\nindependent component analysis or other learning algo-\nrithms that capture high-order dependency. It removes pair-\nwise correlation in the input data domain and, as a result,\nreduces the data dimensionality. Note that the PCA cap-\ntures short-term temporal correlation as well because it is\nperformed on multiple frames (after vectorizing them).\n2.2 Feature Representation\nAt this point, the input has been processed in a highly re-\nconstructible way so that the underlying structure of the\ndata can be discovered via feature-learning algorithms. In\nthis section, we describe how such algorithms reveal the\nunderlying structure.\n2.2.1 Feature Learning Algorithm\nWe compare three feature-learning algorithms to encode\nthe preprocessed data into high-dimensional feature vec-\ntors: K-means clustering, Sparse Coding and Sparse Re-\nstricted Boltzmann Machine.\nK-means Clustering: K-means clustering learns Kcen-\ntroids from the input data and assigns the membership of\na given input to one of the Kcentroids. In the representa-\ntional point of view, this can be seen as a linear approxima-\ntion to the input vectors, x\u0019Ds, whereDis a dictionary(centroids) and sis an extremely sparse vector that has all\nzeros but a single “1” that corresponds to the assigned cen-\ntroid. We use the encoded vector, s, as learned features.\nSparse Coding (SC): Sparse coding is an algorithm to rep-\nresent input data as a sparse linear combination of elements\nin a dictionary. The dictionary is learned using the L1-\npenalized sparse coding formulation. In our experiments,\nwe optimize\nmin\nD;s(i)X\ni\r\r\rDs(i)\u0000x(i)\r\r\r2\n2+\u0015\r\r\rs(i)\r\r\r\n1\nsubject to\r\r\rD(j)\r\r\r2\n2= 1;8j(1)\nusing alternating minimization over the sparse codes s(i),\nand the dictionary D[3]. We use the absolute value of the\nsparse code s, as learned features.\nSparse Restricted Boltzmann Machine (sparse RBM):\nThe Restricted Boltzmann Machine is a bipartite undirected\ngraphical model that consists of visible nodes xand hidden\nnodes h[18]. The visible nodes represent input vectors and\nthe hidden nodes represent the features learned by training\nthe RBM. The joint probability for the hidden and visi-\nble nodes is deﬁned in Eq. 2 when the visible notes are\nreal-valued Gaussian units and the hidden notes are binary\nunits. The RBM has symmetric connections between the\ntwo layers denoted by a weight matrix W, but no connec-\ntions within hidden nodes or visible nodes. This particu-\nlar conﬁguration makes it easy to compute the conditional\nprobability distributions, when nodes in either layer is ob-\nserved (Eq. 3 and 4 ).\n\u0000logP(x;h)/E(x;h) =\n1\n2\u001b2xTx\u00001\n\u001b2\u0000\ncTx+bTh+hTWx\u0001\n(2)\np(hjjx) =sigmoid(1\n\u001b2(bj+wT\njx)) (3)\np(xijh) =N((ci+wT\nih);\u001b2); (4)\nwhere\u001b2is a scaling factor, bandcare bias terms, and\nWis a weight matrix. The parameters are estimated by\nmaximizing the log-likelihood of the visible nodes. This\nis performed by block Gibbs sampling between two lay-\ners, particularly, using contrastive-divergence learning rule\nwhich involves only a single step of Gibbs sampling.\nWe further regularize this model with sparsity by encour-\naging each hidden unit to have a pre-determined expected\nactivation using a regularization penalty:\n\u0015X\nj(\u001a\u00001\nm(mX\nk=1E[hjjxk]))2; (5)\nwherefx1;:::;xmgis the training set and \u001adetermines the\ntarget sparsity of the hidden unit activations [13].\nSimilar to K-means clustering and SC, we can interpret Eq.\n4 as approximating input vectors, x, with a linear combi-\nnation of elements from dictionary W. That is, x\u0019Wh(ignoring the bias term, c). The advantage of RBM over\nthe two algorithms is that the RBM has an explicit encod-\ning scheme, h=sigmoid(1\n\u001b2(b+WTx)from Eq. 3. This\nenables much faster computation of learned features than\nSC.\n2.2.2 Pooling and Aggregation\nA song is a very long sequence of data. There are many\nways to summarize the data over the entire song. A typi-\ncal approach to construct a long-term feature is aggregat-\ning short-term features by computing statistical summaries\nover the whole song. However, summarizing all short-term\nfeature over a song dilutes their local discriminative char-\nacteristics. Instead, we pool relevant features over smaller\nsegments and then aggregate them by averaging over all\nthe segments in a song.\nSince the learned feature vectors are generally sparse\nand high-dimensional, we performed max-pooling over seg-\nments of the song. Max-pooling is an operation that takes\nthe maximum value at each dimension over a pooled area.\nThis is often used in the setting of convolutional neural\nnetworks to make features invariant to local transforma-\ntion. In our experiments, it is used to reduce the smoothing\neffect of the averaging. In Section 4 we discuss how the\npooling size is determined.\n2.3 Classiﬁcation\nMusic annotation is a multi-labeling problem. We tackle\nthis by using multiple binary classiﬁers, each predicting\nthe presence of an annotation word. The binary classiﬁer\nalso returns the distance from the decision boundary given\na song-level feature. We used the distance as a conﬁdence\nmeasure of relevance between a query word and a song for\nmusic retrieval.\n2.3.1 Linear SVM\nWe use a linear SVM as a reference classiﬁer to evaluate\nthe song-level feature vectors learned by different settings\nof feature representation. We trained the linear SVM by\nminimizing the hinge loss given training data. By combin-\ning the hinge loss for multiple SVMs as a single objective,\nwe trained them simultaneously, avoiding individual cross-\nvalidation for each SVM and thereby saving computation\ntime [16].\n2.3.2 Neural Network\nWe also applied a neural network to improve classiﬁcation\nperformance. For simple evaluation, we used a single hid-\nden layer. However, instead of the cross-entropy, which is\nusually used as a cost function for a neural network, we\nemployed the hinge loss from the linear SVM above, so\nthat the penalty term is consistent between classiﬁers. That\nway, performance difference can be attributed only to the\ninclusion of the hidden layer.mel-frequenc\ny\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120\nW\nide-band energy\nwith strong\nhigh-frequency\ncontent\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120Harmonic\npatterns\nwith strong pitchness\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120Extremely low-freq.\nenergy and several\nwideband and\ntransient patterns\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120Low-frequency\ncontent with\nharmonic patterns\nElectronica\n20406080100120\nRock\n20406080100120\nCalming/Soothing\n20406080100120\nExciting/Thrilling\n20406080100120\nPiano\n20406080100120\nDrumMachine\n20406080100120\nSleeping\n20406080100120\nWakingup\n20406080100120\nRapping\n20406080100120\nScreaming\n20406080100120Non-harmonic and\ntransient patterns\nFigure 2: Top 20 most active feature bases (dictionary elements) for ﬁve different tags: Rock, Piano, Electronica, Sleeping\nandExciting/Thrilling. Note that all the features come from the same learned dictionary (mel-frequency spectrogram and\nsparse RBM with 1024 hidden units and 0.01 sparsity), but different types of music use different feature bases.\n3. EXPERIMENTS\n3.1 Dataset\nWe evaluated our proposed feature representation on the\nCAL500 dataset [19]. This dataset contains 502 western\nsongs, each of which was manually annotated with one or\nmore tags out of 174 possibilities, grouped in 6 categories:\nMood, Genre, Instrument, Song, Usage, and Vocal. In our\nexperiments, we considered only 97 tags with at least 30\nexample songs, to be able to compare with results reported\nelsewhere [20], [4], [8] [5]. In order to apply the full path\nof our pipeline, we obtained MP3 ﬁles of the 502 songs\nand used the decoded waveforms.\n3.2 Preprocessing Parameters\nWe ﬁrst resampled the waveform data to 22.05kHz and ap-\nplied the AGC using 10 sub-bands and attack/delay smooth-\ning the envelope on each band. We computed an FFT with\na 46ms Hann window and 50% overlap, which produces a\n513 dimensional vector (up to half the sampling rate) for\neach frame. We then converted it to a mel-frequency spec-\ntrogram with 128 bins. In the magnitude compression, C\nwas set to 10 (see section 2.1.3). For PCA whitening and\nfeature learning steps, we sampled 100000 data examples,\napproximately 200 examples at random positions within\neach song. Each example is selected as a 128\u0002n(n=1, 2,\n4, 6, 8 and 10) patch from the mel-frequency spectrogram.\nUsing PCA whitening, we reduced the dimensionality of\nthe examples to 49, 80, 141, 202, 263 and 323 for each n\nby retaining 90% of the variance. Before the whitening,\nwe added 0.01 to the variance for regularization.\n3.3 Feature Representation Parameters\nWe used dictionary size (or hidden layer size) and sparsity\n(when applicable) as the primary feature-learning param-\neters. The dictionary size was varied over 128, 256, 512\nand 1024. The sparsity parameter was set to \u001a= 0.01, 0.02,\n0.03, 0.05, 0.07 and 0.1 for sparse RBM and \u0015= 0.5, 1.0,\n1.5 and 2.0 for sparse coding. Max-pooling was performed\nover segments of length 0.1, 0.25, 0.5, 1, 2, 4, 8, 16, 32 and\n64 seconds.3.4 MFCC\nWe also evaluated MFCC as a “hand-crafted” feature in or-\nder to compare it to our proposed feature representation.\nInstead of using the MFCC provided from the CAL500\ndataset, we computed our own MFCC to match parame-\nters as close as possible to the proposed feature. We used\nthe same AGC and FFT parameters but 40 bins for mel-\nfrequency spectrogram and then applied log and DCT. In\naddition, we formed a 39-dimensional feature vector by\ncombining its delta and double delta and normalized it by\nmaking the MFCC have zero mean and unit variance. The\nMFCC was also fed into either the classiﬁer directly or the\nfeature-learning step.\n3.5 Classiﬁer Parameters\nWe ﬁrst subtracted the mean and divided by the standard\ndeviation of each song-level feature in the training set and\nthen trained the classiﬁers with the features and hard an-\nnotation using 5-fold cross-validation. In the neural net-\nwork, since the classiﬁer is not our main concern, we sim-\nply ﬁxed the hidden layer size to 512. After training, we\nadjusted the distance from the decision boundary using the\ndiversity factor of 1.25, following the heuristic in [11].\n4. EV ALUATION AND DISCUSSION\n4.1 Annotation and Retrieval Performance Metrics\nThe annotation task was evaluated using Precision, Recall\nand F-score, following previous work. Precision and Re-\ncall were computed based on the methods described by\nTurnbull [20]. The F-score was computed by ﬁrst calcu-\nlating individual F-scores for each tag and then averag-\ning the individual F-scores, similarly to what was done by\nEllis [8]. It should be noted that averaging individual F-\nscores tends to generate lower average F-score than com-\nputing the F-score from mean precision and recall values.\nAs for the retrieval, we used the area under the receiver\noperating characteristic curve (AROC), mean average pre-\ncision (MAP) and top-10 precision (P10) [8].1 2 3 4 5 6 7 8 9 100.260.270.280.290.3\nNumber of FramesF−Score\n1 2 3 4 5 6 7 8 9 100.710.720.730.740.75\nAROC\n  \nAROC\nF−ScoreFigure 3: Effect of number of frames (Sparse RBM with\n1024 hidden units)\n0.1 0.25 0.5 1 2 4 8 16 32 640.260.2650.270.2750.280.2850.290.295\nMax−pooling [sec]F−score\n  \n0.01\n0.02\n0.03\n0.05\n0.07\n0.1Sparsity\nFigure 4: Effect of sparsity and max-pooling (Sparse\nRBM with 1024 hidden units)\n4.2 Visualization\nFigure 2 shows most active top-20 feature bases learned on\nthe CAL500 for each tag. They are vividly distinguished\nby different timbral patterns, such as harmonic/non-\nharmonic, wide/narrow band, strong low/high-frequency\ncontent and steady/transient ones. This indicates the fea-\nture learning algorithm effectively maps input data to high-\ndimensional sparse feature vectors such that the feature\nvectors (hidden units in RBM) are “selectively” activated\nby given music.\n4.3 Results and Discussion\nWe discuss the effect of parameters in the pipeline on the\nannotation and retrieval performance.\n4.3.1 Number of Frames\nFigure 3 plots F-score and AROC for different number of\nframes (patch size) taken from the mel-frequency spec-\ntrogram. It shows that the performance signiﬁcantly in-\ncreases between 1 and 4 frames and then saturates beyond\n4 frames. It is interesting that the best results are achieved\nat 6 frames (about 0.16 second long). We think this is re-\nlated to the representational power of the algorithm. That\nis, when the number of frames is small, the algorithm is\ncapable of capturing the variation of input data. However,\nas the number of frames grows, the algorithm becomes in-\ncapable of representing the exponentially increasing varia-\ntion, in particular, temporal variation.Annotation Retrieval\nData+Algorithm Prec. Recall F-score AROC MAP P10\nWith AGC\nMFCC only 0.399 0.223 0.242 0.713 0.446 0.467\nMFCC+K-means 0.446 0.240 0.270 0.732 0.471 0.492\nMFCC+SC 0.437 0.232 0.260 0.713 0.452 0.476\nMFCC+SRBM 0.441 0.235 0.263 0.725 0.463 0.485\nMel-Spec+K-means 0.467 0.252 0.287 0.740 0.488 0.520\nMel-Spec+SC 0.468 0.252 0.286 0.734 0.482 0.507\nMel-Spec+SRBM 0.479 0.257 0.289 0.741 0.489 0.513\nWithout AGC\nMFCC only 0.399 0.222 0.239 0.712 0.444 0.460\nMFCC+K-means 0.438 0.237 0.267 0.727 0.465 0.489\nMel-Spec+SRBM 0.449 0.244 0.274 0.727 0.477 0.503\nTable 1: Comparison of the performance for different in-\nput data and feature learning algorithms. These results are\nall based on a linear SVM.\n4.3.2 Sparsity and max-pooling size\nFigure 4 plots F-score for a set of sparsity values and max-\npooling sizes. It shows a clear trend that higher accuracy is\nachieved when the feature vectors are sparse (around 0.02)\nand max-pooled over segments of about 16 seconds.1\nThese results indicate that the best discriminative power\nin song-level classiﬁcation is achieved by capturing only\na few important features over both timbral and temporal\ndomains.\n4.3.3 Input Data, Algorithms and AGC\nTable 1 compares the best results on features learned on\ndifferent types of input data and feature learning algorithms.\nAs shown, the mel-frequency spectrogram signiﬁcantly out-\nperforms MFCC regardless of the algorithms. Among the\nfeature learning algorithms, K-means and sparse RBM gen-\nerally perform better than SC. In addition, the results show\nthat the AGC signiﬁcantly improves both annotation and\nretrieval performance, regardless of the input features.\n4.3.4 Comparison to state-of-the-art algorithms\nTable 2 compares our best results to those of state-of-the-\nart algorithms. They all use MFCC features as input data\nand represent them either using a Gaussian Mixture Model\n(GMM), as a bag of frames [20], or Dynamic Texture Mix-\nture (DTM) [4]. They have progressively improved their\nperformance by building on the previous systems, such\nas, in Bag of Systems (BoS) [8] or Decision Fusion (DF)\ndecision-fusion. However, our best system trained with a\nlinear SVM shows comparable results. In addition, with\nnonlinear neural-network classiﬁcation, our system outper-\nforms the prior algorithms in F-score and all retrieval met-\nrics.\n1We found that the average length of songs on the CAL500 dataset\nis approximately 250 seconds, which suggests that aggregating about 16\n(\u0019250/16) max-pooled feature vectors over an entire song is an optimal\nchoice.Annotation Retrieval\nMethods Prec. Recall F-score AROC MAP P10\nHEM-GMM [20] 0.374 0.205 0.213 0.686 0.417 0.425\nHEM-DTM [4] 0.446 0.217 0.264 0.708 0.446 0.460\nBoS-DTM-GMM-LR [8] 0.434 0.272 0.281 0.748 0.493 0.508\nDF-GMM-DTM [5] 0.484 0.230 0.291 0.730 0.470 0.487\nDF-GMM-BST-DTM [5] 0.456 0.217 0.270 0.731 0.475 0.496\nProposed methods\nMel-Spec+SRBM+SVM 0.479 0.257 0.289 0.741 0.489 0.513\nMel-Spec+SRBM+NN 0.473 0.258 0.292 0.754 0.503 0.527\nTable 2: Performance comparison: state-of-the-art (top)\nand proposed methods (bottom).\n5. CONCLUSION AND FUTURE WORK\nWe have presented a sparse feature representation method\nbased on unsupervised feature-learning. This method was\nable to effectively capture many timbral patterns of mu-\nsic from minimally pre-processed data. Using a simple\nlinear classiﬁer, our method achieved results comparable\nto state-of-the-art algorithms for music annotation and re-\ntrieval tasks on the CAL500 dataset. Furthermore, our sys-\ntem outperformed them with a non-linear classiﬁer.\nTo ensure the discriminative power of our proposed fea-\nture representation method, we need to evaluate it on larger\ndatasets, such as, the Million Song Dataset [1] or Magnata-\ngatune [12] and also for different classiﬁcation tasks.\n6. REFERENCES\n[1] T. Bertin-Mahieux, D. Ellis, B. Whitman, and\nP. Lamere. The million song dataset. In ISMIR, 2011.\n[2] A. Coates, H. Lee, and A. Ng. An analysis of\nsingle-layer networks in unsupervised feature\nlearning. Journal of Machine Learning Research,\n2011.\n[3] A. Coates and A. Ng. The importance of encoding\nversus training with sparse coding and vector\nquantization. In ICML, 2011.\n[4] E. Coviello, A. Chan, and G. Lanckriet. Time series\nmodels for semantic music annotation. IEEE\nTransactions on Audio, Speech, and Language\nProcessing, 2011.\n[5] E. Coviello, R. Miotto, and G. Lanckriet. Combining\ncontent-based auto-taggers with decision-fusion. In\nISMIR, 2011.\n[6] S. Dieleman, P. Brakel, and B. Schrauwen.\nAudio-based music classiﬁcation with a pretrained\nconvolutional network. In ISMIR, 2011.\n[7] D. Ellis. Time-frequency automatic gain control. web\nresource, available, http://labrosa.ee.\ncolumbia.edu/matlab/tf_agc/, 2010.[8] K. Ellis, E. Coviello, and G. Lanckriet. Semantic\nannotation and retrieval of music using a bag of\nsystems representation. In ISMIR, 2011.\n[9] P. Hamel, S. Lemieux, Y . Bengio, and D. Eck.\nTemporal pooling and multiscale learning for\nautomatic annotation and ranking of music audio. In\nISMIR, 2011.\n[10] H. Henaff, K. Jarrett, K. Kavukcuoglu, and Y . LeCun.\nUnsupervised learning of sparse features for scalable\naudio classiﬁcation. In ISMIR, 2011.\n[11] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A\nsimple probabilistic model for tagging music. In\nISMIR, 2009.\n[12] E. Law and L. Ahn. Input-agreement: a new\nmechanism for collecting data using human\ncomputation games. In Proc. Intl. Conf. on Human\nfactors in computing systems, CHI. ACM, 2009.\n[13] H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief\nnet model for visual area v2. Advances in Neural\nInformation Processing Systems, 2007.\n[14] H. Lee, P. Pham Y . Largman, and A.Y . Ng.\nUnsupervised feature learning for audio classiﬁcation\nusing convolutional deep belief networks. Advances in\nNeural Information Processing Systems, 2009.\n[15] M. M ¨uller, D. Ellis, A. Klapuri, and G. Richard.\nSignal processing for music analysis. IEEE Journal on\nSelected Topics in Signal Processing, 2011.\n[16] J. Nam, J. Ngiam, H. Lee, and M. Slaney. A\nclassiﬁcation-based polyphonic piano transcription\napproach using learned feature representation. In\nISMIR, 2011.\n[17] J. Schlter and C. Osendorfer. Music Similarity\nEstimation with the Mean-Covariance Restricted\nBoltzmann Machine. In ICMLA, 2011.\n[18] P. Smolensky. Information processing in dynamical\nsystems:Foundation of harmony theory. MIT Press,\nCambridge, 1986.\n[19] D. Turnbull, L. Barrington, D. Torres, and\nG. Lanckriet. Towards musical query-by-semantic\ndescription using the CAL500 data set. In ACM\nSpecial Interest Group on Information Retrieval\nConference, 2007.\n[20] D. Turnbull, L. Barrington, D. Torres, and\nG. Lanckriet. Semantic annotation and retrieval of\nmusic and sound effects. IEEE Transactions on Audio,\nSpeech, and Language Processing, 2008."
    },
    {
        "title": "Association Mining of Folk Music Genres and Toponyms.",
        "author": [
            "Kerstin Neubarth",
            "Izaro Goienetxea",
            "Colin Johnson",
            "Darrell Conklin"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417643",
        "url": "https://doi.org/10.5281/zenodo.1417643",
        "ee": "https://zenodo.org/records/1417643/files/NeubarthGJC12.pdf",
        "abstract": "This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genre– region associations have been widely studied in folk mu- sic research but have been neglected in music information retrieval. We present a method of association rule min- ing with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre–region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered.",
        "zenodo_id": 1417643,
        "dblp_key": "conf/ismir/NeubarthGJC12",
        "keywords": [
            "association rule mining",
            "genre",
            "region ontology",
            "folk music",
            "genre–region associations",
            "musicological motivation",
            "corpus",
            "rules",
            "rule sets",
            "music information retrieval"
        ],
        "content": "ASSOCIATION MINING OF FOLK MUSIC GENRES AND TOPONYMS\nKerstin Neubarth1;2Izaro Goienetxea3Colin G. Johnson2Darrell Conklin3;4\n1Canterbury Christ Church University, Canterbury, United Kingdom\n2School of Computing, University of Kent, Canterbury, United Kingdom\n3Department of Computer Science and Artiﬁcial Intelligence,\nUniversity of the Basque Country UPV/EHU, San Sebasti ´an, Spain\n4IKERBASQUE, Basque Foundation for Science, Bilbao, Spain\nABSTRACT\nThis paper demonstrates how association rule mining can\nbe applied to discover relations between two ontologies\nof folk music: a genre and a region ontology. Genre–\nregion associations have been widely studied in folk mu-\nsic research but have been neglected in music information\nretrieval. We present a method of association rule min-\ning with constraints consisting of rule templates and rule\nevaluation measures to identify different, musicologically\nmotivated, categories of genre–region associations. The\nmethod is applied to a corpus of 1902 Basque folk tunes,\nand several interesting rules and rule sets are discovered.\n1. INTRODUCTION\nIn recent years music information retrieval (MIR) research\nhas increasingly turned towards folk and ethnic music and\nits contexts [6, 19], and perspectives for collaboration be-\ntween MIR and ethnomusicology have been outlined [22,\n23]. While musicologists consider interactions between\nfolk music genres, their geographical distribution and mu-\nsical characteristics [12, 18], MIR research has mainly fo-\ncused on geographically organised folk music corpora [10,\n11, 21]. A recent study on Cretan folk music extracts dis-\ntinctive melodic interval patterns for genre and for area\nclasses, but does not link genres and areas, although the\nidea of conjunctive genre–area classes is mentioned [5]. In\nthis paper we analyse relations between genres and regions\nin a collection of Basque folk music, through association\nrule mining.\nAssociation rule mining has been developed extensively\nin the wider ﬁeld of knowledge discovery and data mining,\nbut has seen only limited attention in MIR (e.g. [4]). Work\non cultural heritage, not restricted to but also covering mu-\nsic, has used association rule mining to populate a heritage\nontology with new relations between concepts: annota-\ntions of heritage objects were mined and discovered associ-\nations proposed to a domain expert, who categorised them\nas subclass or associative relations [13]. Our research goes\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.beyond these studies by applying association rule mining\nto suggest different, labelled, categories of associations.\n2. TASK DESCRIPTION\nThe folk music collection Cancionero Vasco was origi-\nnally compiled by the musicologist Padre Donostia for a\nmusical heritage competition in 1912 [18, article ‘Basque\nMusic’]. It has been digitised and curated by Fundaci ´on\nEuskomedia in collaboration with musicologists at Fun-\ndaci´on Eresbil, under the auspices of the Basque Studies\nSociety.1The digitised collection contains 1902 Basque\nfolk songs and dances. The examples are annotated with\ngenre information and the location (toponym) where they\nwere collected. The annotation vocabulary is deﬁned in\ntwo ontologies: a geographical ontology of provinces, mu-\nnicipalities and towns or villages, and an ontology of hier-\narchically organised genres. The aim of applying associa-\ntion rule mining to this collection is to discover patterns of\ngenres and regions co-occurring in the annotations, which\nsuggest that certain genres are particularly associated with\ncertain regions and vice versa.\nA common challenge in applications of association rule\nmining is to manage potentially large numbers of discov-\nered association rules and identify those rules which are\ninteresting to a user [14]. Genre–region relations feature\nprominently in folk music research, and the musicologi-\ncal interest goes beyond simple mappings: In a qualitative\nanalysis we examined surveys of traditional music in 25\nEuropean countries [18], extracted statements linking gen-\nres and regions, grouped them according to similar mean-\ning and for each group suggest a shared interpretation to\nfacilitate translation into association rules (Table 1). As\nthese association categories are based on recurring obser-\nvations in musicological reference articles, they are con-\nsidered relevant to users of folk music collections. The\ndata mining task then is not only to discover associations\nbetween the two ontologies of genres and regions, but to\ndistinguish associations of the categories listed in Table 1.\n3. DATA AND METHODS\nThis section describes the data representation and the data\nmining method for discovering genre–region associations\nin the Cancionero Vasco. In order to identify association\n1http://www.euskomedia.org/cancioneroCategory Example observation Interpretation\nPresent “survivals of calendar ritual and wedding music are\nfound in the Opole area”genre present in region\nAbsent “the [two-part] form does not exist at all in Kosovo and\nMetohija”genre not present in region\nLocal “localized dances include the corridinho (Algarve)” genre present in exactly one region\nMainly “krakowiak dances [...] are found mainly in Mało-\npolska”genre over-represented in region with\nrespect to other regions\nDominant “the polka is the most popular dance in these regions” genre over-represented in region with\nrespect to other genres\nTypical “the dance-song seguidillas is typical of New Castile” genre over-represented in region with\nrespect to other regions and genres\nHardly “the klarino style is hardly found at all on the islands” genre under-represented in region with\nrespect to other regions\nRare “Genres [...] such as the epic are quite rare in central\nEuropean repertories, whereas genres [...] such as the\nballad are quite common.”genre under-represented in region with\nrespect to other genres\nTable 1. Categories of genre–region associations, with example quotations from the New Grove [18].\nrules of the different categories summarised in Table 1,\nthese categories are translated into association constraints\nconsisting of rule templates and rule evaluation measures.\n3.1 Ontologies\nThe annotations mined in this study are standardised and\nstructured in two ontologies: a genre and a geographical\nontology, formalised in description logic (DL) [3, 9].\nThe ontology of folk music genres consists of two parts:\na set of statements GvG0deﬁnes the genres and their\nsubsumption relations, e.g.\nwork songsvlife-cycle songs\nlife-cycle songsvgenre\nstate that work songs are subsumed by, i.e. more speciﬁc\nthan, life-cycle songs and that life-cycle songs are a genre.\nThe second part of the ontology is a set of assertions G(e)\nwhereGis a genre concept and ea folk tune example:\nthis part formalises the genre annotations of the examples\nin the Cancionero Vasco, using 31 genres. Examples are\nasserted with their most speciﬁc known genre annotation,\nwhich is not necessarily the lowest level of the subsump-\ntion hierarchy. The genre counts used in the association\nrule mining (Section 3.2) can be derived from the ontology\nby querying for the examples instantiating a genre; here\nthe inference capabilities of DL allow to count examples\nnot only for the directly asserted genre but also for more\ngeneral, subsuming genres. Out of the 1902 examples in\nthe corpus 341 examples are without a genre annotation.\nThe geographical ontology covers Euskal Herria, the\nBasque speaking areas in North-East Spain and South-West\nFrance. The ontology is organised into the three levels\nof provinces (7 toponyms), municipalities (681 toponyms)and towns or villages (2280 toponyms). Locations are for-\nmalised as instances in DL, assigned to levels, e.g.\nprovince(Lapurdi) and municipality (Azkaine):\nIn DL the geographical relationships are deﬁned in terms\nof spatial containment roles, e.g.\ncontains(Lapurdi; Azkaine)\nasserts that Lapurdi (a province) contains Azkaine (a mu-\nnicipality). Each folk tune example eis asserted with the\nregionRin which it was collected: collected(R;e). As\nwith the genre ontology, assertions can be made at any\nlevel of the hierarchy and higher-level counts are inferred\nbased on the transitivity of the containment relation. Out\nof the 1902 folk tunes in the Cancionero Vasco 272 tunes\nare without a toponym annotation.\nThe original annotation terms for genres are Spanish or\nBasque. For the presentation in this paper we give English\ntranslations for genres. As toponyms we use the Basque,\nrather than Spanish or French, names.\n3.2 Association Rule Mining\nAssociation rules are rules of the form a!bwith an an-\ntecedent item set aand a consequent item set b(a\\b=;)\n[20]. A rule a!bwith conﬁdence cstates thatc%of the\ndata records containing items aalso contain items b. Rule\ntemplates [14] deﬁne the form of a rule and specify which\nitems can occur in the antecedent and consequent. In this\nstudy we mine for rules with one item in the antecedent\nand one item in the consequent. Here an item can be a\ngenre (denoted Gin the rule templates), a region (denoted\nR) or the complement of a genre or region (denoted Gand\nRrespectively).a a\nbnab nb\u0000nabnb\nbnab=na\u0000nabnb\u0000nabnb=n\u0000nb\nna na=n\u0000nan\nFigure 1. Contingency table for a rule a!b.\nThe rule templates determine how the genre and region\nof a candidate association are mapped onto a contingency\ntable from which a rule evaluation measure can be calcu-\nlated [15]. Figure 1 presents a 2\u00022contingency table for\nan association rule a!b, whereais the antecedent item,\nbthe consequent item, nathe number of folk tunes anno-\ntated witha,nbthe number of folk tunes annotated with\nb,nabthe number of folk tunes annotated with both aand\nb, andnthe total number of folk tunes in the corpus. The\nnotationadenotes the complement of item a. For exam-\nple, with the rule template G!R,narefers to the genre\ncount andnbrefers to the region count; nabis the number\nof tunes instantiating both the genre and the region.\nThe evaluation measures most commonly used in as-\nsociation rule mining are support (frequency of the co-\noccurrence) and conﬁdence (conditional probability of the\nco-occurrence given the antecedent). These two measures,\nhowever, are not sufﬁcient to distinguish all association\ncategories of Table 1, e.g. Typical against Mainly andDom-\ninant. We thus considered further existing measures and\ntheir properties (e.g. [8, 16]). Measures for each category\nwere selected in two steps. First, we deﬁned the require-\nments for each category, given its interpretation (Table 1),\nbased on established measure properties:\n\u000fAsymmetric vs symmetric measures: For all categories\nexcept Present andTypical the measure should be asym-\nmetric, i.e. distinguish between a!bandb!a.\n\u000fIncreasing function with number of examples: The mea-\nsures for Mainly, Dominant, Hardly andRare should in-\ncrease withnabfor ﬁxedna, while the measure for Typ-\nicalshould increase with nabfor bothnaandnbﬁxed.\n\u000fDecreasing function with number of counter-examples:\nThe measures for Mainly, Dominant, Hardly andRare\nshould decrease with increasing nab, and thusna, for\nﬁxednab, while the measure for Typical should decrease\nwith both increasing nabandnab, and thus both naand\nnb, for ﬁxednab.\n\u000fSensitivity vs. insensitivity to sample size: As the mea-\nsures are used to capture the relationship between aand\nb, they should be insensitive to changes in naband thus\nto changes in nfor ﬁxednab,naandnb.\nSecond, we determined measures that match the require-\nments. Where more than one measure meets the category\ncriteria, a measure is preferred in which variations of the\nmeasure value can easily be related to values in the contin-\ngency table [16]. Table 2 lists the resulting constraints for\neach association category.Category Template Measure\nPresent G\u0000R support\nAbsent G!R conﬁdence (c = 1)\nLocal G!R conﬁdence (c = 1)\nMainly G!R conﬁdence\nDominant R!G conﬁdence\nTypical G\u0000R Jaccard\nHardly G!R Sebag-Schoenauer\nRare R!G Sebag-Schoenauer\nTable 2. Constraints for the association categories.\nMeasure Deﬁnition\nsupport s=nab\nconﬁdence c=nab=na\nJaccard J=nab=(n a+nb\u0000nab)\nSebag-Schoenauer S=nab=nab\nTable 3. Deﬁnitions of the rule evaluation measures.\nThe deﬁnitions of the measures are given in Table 3.\nTo ensure invariance with changes in n, absolute rather\nthan relative support is applied for Present. For the cate-\ngories describing under-representation (Hardly andRare),\nthe measure of Sebag-Schoenauer was found to discrimi-\nnate better than conﬁdence: conﬁdence accepts most infre-\nquent pairs, while Sebag-Schoenauer accepts pairs that are\nless frequent than comparison pairs.\nDuring the mining, each candidate pair of a genre and\na region is evaluated against the category constraints, i.e.\nthe genre, region and pair counts are mapped onto na,nb\nandnabaccording to the template, and the measure value\nis calculated. Pairs are tested for all categories, and asso-\nciations can be assigned to more than one category (e.g.\nPresent andMainly).\n4. RESULTS\nTable 4 lists selected highly ranked rules for all categories.\nThep-values in Table 4 are calculated according to Fisher’s\nexact test with left tail for Absent, Hardly andRare and\nright tail for all other categories [7]. The p-value mea-\nsures the probability of ﬁnding at most (left tail) or at least\n(right tail) the number of co-occurrences given by the pair\ncount, under the conditions of the genre and region counts.\nTo account for multiple comparisons, i.e. testing multiple\nhypotheses on the same data, results are marked for both\nsigniﬁcance level \u000band the Bonferroni-corrected signiﬁ-\ncance level\f; it should be noted, though, that the Bonfer-\nroni correction is highly conservative. Rules above the sig-\nniﬁcance level are not necessarily rejected as uninteresting\n(see also [16]), rather the p-values provide additional in-\nformation for interpreting discovered rules, with respect to\nthe distribution of genres and regions in the total corpus.Genre (count) Region (count) Pair count Template Measure p-value\nPresent\nlife-cycle songs (477) Nafarroa (897) 259 G\u0000R s = 259 0.00089++\nArtaxuriketak (38) Nafarroa (897) 30 G\u0000R s = 30 4.3e-05\u0003\u0003\nAbsent\ndances (495) Hazparne (23) 0 G!R c = 1 0.00093++\nsacred songs (301) Araba (27) 0 G!R c = 1 0.00922\u0003\nLocal\nsmugglers’ songs (1) Lapurdi (383) 1 G!R c = 1 0.02321+\nsmugglers’ songs (1) Azkaine (61) 1 G!R c = 1 0.03207+\nMainly\nArtaxuriketak (38) Nafarroa (897) 30 G!R c = 0:79 4.3e-05\u0003\u0003\nmoral songs (11) Nafarroa (897) 8 G!R c = 0:73 0.04470+\nDominant\ndances (495) Araba (27) 24 R!G c = 0:89 7.8e-12\u0003\u0003\ndances (495) Eugi (15) 13 R!G c = 0:87 1.4e-06\u0003\u0003\nTypical\nCarlism songs (1) Biriatu (3) 1 G\u0000R J = 0:33 0.00158\u0003\nHardly\ndances (495) Atharratze (23) 1 G!R S = 494 0.00857\nlife-cycle songs (477) Bizkaia (21) 2G!R S = 237:5 0.07232\nRare\nArtaxuriketak (38) Lapurdi (383) 1 R!G S = 382 0.01112+\nmoral songs (11) Nafarroa (897) 8 R!G S = 111:13 0.98935\n\u0003signiﬁcance level \u000b= 0:01,\u0003\u0003with Bonferroni correction \f= 0:0002\n+signiﬁcance level \u000b= 0:05,++with Bonferroni correction \f= 0:001\nTable 4. Examples of discovered association rules.\nIn all cases of Local, genres are represented by only\none example in the corpus and thus are necessarily identi-\nﬁed as local. The occurrence of the smugglers’ song in La-\npurdi could be linked to the site of Lapurdi, stretching from\nthe coast inland across the Pyrenees between Spain and\nFrance: in fact, the example in the Cancionero Vasco was\ncollected more speciﬁcally in the municipality of Azkaine,\nin the Larrun area known to have been used by smugglers;\nin nearby Sara the “smugglers’ race”, celebrated in August,\nhas become part of 20th-century folklore [17].\n5. DISCUSSION\nThe interest of this study lies in discovering genre–region\nassociations in the Cancionero Vasco which are potentially\ninteresting to users who browse or analyse the collection.\nThe association categories deﬁned in this paper provide\nadditional semantics for rules as compared to traditionalassociation rule mining, and can help organise and under-\nstand the folk music corpus. Multiple labels can further\nspecify an association (Example 1) and even capture dif-\nferent aspects of the same genre–region pair (Example 2):\nExample 1: Not only are the corn-harvesting songs Artax-\nuriketak present in Nafarroa, the Artaxuriketak examples\nin the Cancionero Vasco are mainly from Nafarroa.\nExample 2: Within the Cancionero Vasco moral songs are\nrare in Nafarroa, i.e. under-represented with respect to\nother genres in Nafarroa (8 out of 897 instances), but are\nover-represented in Nafarroa with respect to their occur-\nrence in other regions, i.e. they occur mainly in Nafarroa\n(8 out of 11 instances).\nThep-value is a symmetric measure, i.e. it does not\ndistinguish between e.g. rule G!Rand ruleR!G.\nUsing different rule templates with conﬁdence or Sebag-\nSchoenauer as evaluation measure, on the other hand, al-Region (count) Pair count Category Measure p-value\nAraba (27) 0 Absent c= 1 0.57771\nBizkaia (21) 0 Absent c= 1 0.65307\nGipuzkoa (175) 4 Present s= 4 0.46864\nLapurdi (383) 1 Present s= 1 0.9964\nRare S= 382 0.01112\nNafarroa-Beherea (47) 1 Present s= 1 0.61722\nNafarroa (897) 30 Present s= 30 4.3e-05\nMainlyc= 0:79 4.3e-05\nZuberoa (80) 2 Present s= 2 0.48504\nTable 5. Group of rules for the genre Artaxuriketak (genre count 38).\nlows one to identify the asymmetric cases of over-repre-\nsentation (Mainly vs.Dominant ) or under-representation\n(Hardly vs.Rare), which correspond to different observa-\ntions in folk music surveys.\nExample 3: Artaxuriketak occur mainly in Nafarroa as\ncompared to other regions (79% of the Artaxuriketak in-\nstances in the Cancionero Vasco are from Nafarroa). On\nthe other hand, dances are dominant in Araba with respect\nto other genres in the same region (89% of examples from\nAraba are dances).\nThe association categories can facilitate the analysis of\ngroups of discovered rules. Folk music collections are of-\nten organised according to genres or regions [12], and folk\nmusic surveys may review genres against an underlying\ngeographical classiﬁcation [18]. For example, a summary\nof folk-dance in Finland states: “Although most polska\nmelodies have been collected in Pohjanmaa, the dance was\nknown throughout the country except in the far north and\nKarelia.” [18]\nExample 4: The geographical distribution of Artaxurike-\ntak at the level of provinces can be described in a simi-\nlar way (Table 5): While most Artaxuriketak in the corpus\nwere collected in Nafarroa, the genre is also known in the\nother provinces except – within the Cancionero Vasco – in\nAraba and Bizkaia. Given the large number of instances\nfor Nafarroa (897 instances, 47% of the corpus) it is not\nsurprising that most Artaxuriketak were collected in Nafar-\nroa, but the high proportion (nearly 80% of Artaxuriketak)\nis statistically signiﬁcant. It is interesting to note that tra-\nditionally farmers in Nafarroa have cultivated corn (maize)\nwhile the chief crops in Araba have been cereals as well\nas fruit, wine and olives [2]. For Lapurdi, surveys in the\n1980s reported that nearly half of the area was dedicated to\nwine (45%), followed by other crops (40%), woods (10%)\nand urban or uncultivated areas (5%) [1].\nOther folk music surveys follow a mainly geographical\norganisation. For example, the New Grove article on tradi-\ntional music in Croatia is structured according to regions;\nthe following statement is taken from the section on West-\nern Croatia: “This region is characterised by the tanac and\nbalun dances. [...] Other dances, like the polka and thevalcer, are also performed.” [18]\nExample 5: Folk music in the province of Araba, accord-\ning to the Cancionero Vasco, is dominated by dances (89%\nof the Araba examples are dances, p-value = 7:8e-12). Re-\nlated rules, not shown in Table 4, indicate that of the other\ngenres only life-cycle songs were also collected in Araba\n(pair count 3, Hardly,S= 158 ,p-value = 0:06411), and\nmore particularly within life-cycle songs the subgroup of\nwork songs (Present, s= 3,p-value = 0:02580).\n6. CONCLUSIONS\nIn this paper we have shown how association rule mining\ncan be used to discover relevant relations between genres\nand geographical locations of folk tunes, more speciﬁcally\nhow association rule mining with rule templates and eval-\nuation measures can be used to identify different, musi-\ncologically motivated, categories of genre–region associa-\ntions. The method was applied to a collection of Basque\nfolk music, and example associations discussed in the con-\ntext of folk music research.\nThis research represents an original contribution to MIR\nboth in terms of retrieval task (discovering associations be-\ntween folk music genres and their geographical distribu-\ntion) and method (systematically combining rule templates\nand evaluation measures to distinguish different associa-\ntion categories). As a case study of interdisciplinary col-\nlaboration between MIR and musicology it demonstrates\nhow musicology can inform the task deﬁnition, method\ndesign and discussion of results; the examples illustrate\nhow MIR can both support musicological observations and\nstimulate further analysis.\nOur work can be extended in several ways. The mining\nresults can support more traditional information retrieval\nof music, like browsing and searching of music collections\nguided by association categories or by groups of associa-\ntion rules. Classiﬁcation using association rules could be\nexplored to suggest genre and toponym annotations for un-\nlabelled tunes. The method could also be adapted to search\nfor associations between annotations and music content\nclasses. In addition the approach could be applied to data\nin other MIR areas such as user tagging, folksonomies and\nmusic recommendation.7. ACKNOWLEDGEMENTS\nWe thank Fundaci ´on Euskomedia and Fundaci ´on Eresbil\nfor providing the Cancionero Vasco for study. This re-\nsearch was partly supported by a grant An´alisis Computa-\ncional de la M ´usica Folcl ´orica Vasca (2011-2012) from the\nDiputaci ´on Foral de Gipuzkoa, Spain.\n8. REFERENCES\n[1] Enciclopedia Au ˜namendi. Fondo Bernardo Estor ´es\nLasa. http://www.euskomedia.org/aunamendi.\n[2] Encyclopaedia Britannica Online. academic edition,\n2012. http://original.search.eb.com/.\n[3] F. Baader, D. Calvanese, D. McGuiness, D. Nardi, and\nP. F. Patel-Schneider, editors. The Description Logic\nHandbook: Theory, Implementation and Applications.\nCambridge University Press, Cambridge, 2003.\n[4] D. Conklin. Melodic analysis with segment classes.\nMachine Learning, 65(2-3):349–360, 2006.\n[5] D. Conklin and C. Anagnostopoulou. Comparative pat-\ntern analysis of Cretan folk songs. Journal of New Mu-\nsic Research, 40(2):119–125, 2011.\n[6] O. Cornelis, M. Lesaffre, D. Moelants, and M. Leman.\nAccess to ethnic music: Advances and perspectives in\ncontent-based music information retrieval. Signal Pro-\ncessing, 90:1008–1031, 2010.\n[7] S. Falcon and R. Gentleman. Hypergeometric testing\nused for gene set enrichment analysis. In F. Hahne,\nW. Huber, R. Gentleman, and S. Falcon, editors, Bio-\nconductor Case Studies, pages 207–220. Springer,\n2008.\n[8] L. Geng and H. J. Hamilton. Interestingness measures\nfor data mining: a survey. ACM Computing Surveys ,\n38(3):1–32, 2006.\n[9] I. Goienetxea, J. Arrieta, I. Bag ¨u´es, A. Cuesta,\nP. Lei ˜nena, and D. Conklin. Ontologies for rep-\nresentation of folk song metadata. Technical Re-\nport EHU-KZAA-TR-2012-01, Department of Com-\nputer Science and Artiﬁcial Intelligence, Univer-\nsity of the Basque Country UPV/EHU, 2012.\nhttp://hdl.handle.net/10810/8053.\n[10] R. Hillewaere, B. Manderick, and D. Conklin. Global\nfeature versus event models for folk song classiﬁca-\ntion. In Proceedings of the 10th International Society\nfor Music Information Retrieval Conference (ISMIR\n2009), pages 729–733, Kobe, Japan, 2009.\n[11] Z. Juh ´asz. A systematic comparison of different Euro-\npean folk music traditions using self-organizing maps.\nJournal of New Music Research, 35(2):95–112, 2006.\n[12] I. J. Katz. The traditional folk music of Spain: explo-\nrations and perspectives. Yearbook of the International\nFolk Music Council, 6:64–85, 1974.[13] T. Kauppinen, H. Kuittinen, K. Sepp ¨ala, J. Tuominen,\nand E. Hyv ¨onen. Extending an ontology by analyzing\nannotation co-occurences in a semantic cultural her-\nitage portal. In Proceedings of the ASWC 2008 Work-\nshop on Collective Intelligence, 3rd Asian Semantic\nWeb Conference (ASCW 2008), pages 1–6, Bangkok,\nThailand, 2008.\n[14] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivo-\nnen, and A. I. Verkamo. Finding interesting rules from\nlarge sets of discovered association rules. In Proceed-\nings of the 3rd International Conference on Infor-\nmation and Knowledge Management, pages 401–407,\nGaithersburg, Maryland, 1994.\n[15] N. Lavra ˇc, P. Flach, and B. Zupan. Rule evaluation\nmeasures: a unifying view. In Proceedings of the 9th\nInternational Workshop on Inductive Logic Program-\nming (ILP-99), pages 174–185, Bled, Slovenia, 1999.\n[16] P. Lenca, P. Meyer, B. Vaillant, and S. Lallich. On se-\nlecting interestingness measures for association rules:\nuser oriented description and multiple criteria deci-\nsion aid. European Journal for Operational Research,\n184(2):610–626, 2008.\n[17] J. A. Perales D ´ıaz. Fronteras y contrabando en el Piri-\nneo Occidental. Zainak. Cuadernos de Antropolog ´ıa-\nEtnograf ´ıa, 17:127–136, 1998.\n[18] S. Sadie, editor. New Grove Dictionary of Music and\nMusicians. Macmillan, London, 2001.\n[19] X. Serra. A multicultural approach in music informa-\ntion research. In Proceedings of the 12th International\nSociety for Music Information Retrieval Conference\n(ISMIR 2011), pages 151–156, Miami, Florida, USA,\n2011.\n[20] R. Srikant, Q. Vu, and R. Agrawal. Mining associa-\ntion rules with item constraints. In Proceedings of the\n3rd International Conference on Knowledge Discovery\nand Data Mining (KDD-97), pages 67–73, Newport\nBeach, CA, USA, 1997.\n[21] J. Taminau, R. Hillewaere, S. Meganck, D. Conklin,\nA. Now ´e, and B. Manderick. Descriptive subgroup\nmining of folk music. In 2nd International Workshop\non Machine Learning and Music (MML 2009), Bled,\nSlovenia, 2009.\n[22] G. Tzanetakis, A. Kapur, W. Schloss, and M. Wright.\nComputational ethnomusicology. Journal of Interdisci-\nplinary Music Studies, 1(2):1–24, 2007.\n[23] P. van Kranenburg, J. Garbers, A. V olk, F. Wiering,\nL. Grijp, and R. C. Veltkamp. Collaborative perspec-\ntives for folk song research and music information re-\ntrieval: The indispensable role of computational mu-\nsicology. Journal of Interdisciplinary Music Studies,\n4(1):17–43, 2010."
    },
    {
        "title": "Using Hyper-genre Training to Explore Genre Information for Automatic Chord Estimation.",
        "author": [
            "Yizhao Ni",
            "Matt McVicar",
            "Raúl Santos-Rodriguez",
            "Tijl De Bie"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415932",
        "url": "https://doi.org/10.5281/zenodo.1415932",
        "ee": "https://zenodo.org/records/1415932/files/NiMSB12.pdf",
        "abstract": "Recently a large amount of new chord annotations have been made available. This raises hopes for further devel- opment in automatic chord estimation. While more data seems to imply better performance, a major challenge how- ever, is the wide variety of genres covered by these new data. As a result, the genre-independent training scheme as is common today is bound to fail. In this paper we in- vestigate various options for exploring genre information for chord estimation, while also maximally exploiting the full dataset. More specifically, we propose a hyper-genre training scheme in which each genre cluster has its own pa- rameters, tied together by hyper parameters as a Bayesian prior. The results are promising, showing significant im- provements over other prevailing training schemes.",
        "zenodo_id": 1415932,
        "dblp_key": "conf/ismir/NiMSB12",
        "keywords": [
            "new chord annotations",
            "automatic chord estimation",
            "wide variety of genres",
            "genre-independent training",
            "major challenge",
            "genre information",
            "hyper-genre training",
            "Bayesian prior",
            "promising results",
            "significant improvements"
        ],
        "content": "USING HYPER-GENRE TRAINING TO EXPLORE GENRE\nINFORMATION FOR AUTOMATIC\nCHORD ESTIMATION\nYizhao Ni, Matt Mcvicar, Ra ´ul Santos-Rodr ´ıguez and Tijl De Bie\nIntelligent Systems Laboratory\nUniversity of Bristol, U. K.\n{enxyn, matt.mcvicar, enrsr, tijl.debie }@bristol.ac.uk\nABSTRACT\nRecently a large amount of new chord annotations have\nbeen made available. This raises hopes for further devel-\nopment in automatic chord estimation. While more data\nseems to imply better performance, a major challenge how-\never, is the wide variety of genres covered by these new\ndata. As a result, the genre-independent training scheme\nas is common today is bound to fail. In this paper we in-\nvestigate various options for exploring genre information\nfor chord estimation, while also maximally exploiting the\nfull dataset. More speciﬁcally, we propose a hyper-genre\ntraining scheme in which each genre cluster has its own pa-\nrameters, tied together by hyper parameters as a Bayesian\nprior. The results are promising, showing signiﬁcant im-\nprovements over other prevailing training schemes.\n1. INTRODUCTION\nIdentifying musical chords from audio recordings is a chal-\nlenging task and has recently attracted the interest of many\nresearchers in the music information retrieval (MIR) ﬁeld.\nThe general approach of automatic chord estimation (ACE)\ninvolves two stages: the extraction of spectral features such\nas chromagram from audio; and the estimation of chords\nbased on these features, via e.g. Hidden Markov Models\n(HMMs). In the past few years, while developing chroma\nextraction techniques has become a fruitful topic [2, 7–9],\nresearchers have also explored a variety of musical factors\nsuch as key [5, 6, 10] and bassline [7, 9] that are related to\nchord progressions to build up richer ACE systems.\nNevertheless, one issue that has cramped the develop-\nment in automatic chord estimation is the limited amount\nof the data available. Since most of the studies so far\nwere carried out on a collection of The Beatles, Queen\nand Zweieck songs (i.e. the MIREX dataset), it is becom-\ning increasingly probable that the existing ACE researches\nare overﬁtting this dataset. Recently a large amount of\nnew chord annotations have been released by the structural\nanalysis of large amounts of music information (SALAMI)\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2012 International Society for Music Information Retrieval.project [14], raising hopes for further development of the\nACE systems. A major challenge it also brings however, is\nthe wide variety of genres covered by the data.\nThis paper is devoted to the study of the new dataset.\nDistinct from feature extraction and decoding research, we\ninvestigate various training schemes for exploring genre\ninformation to aid automatic chord estimation. We begin\nby giving an overview of the ACE task.\n1.1 Automatic Chord Estimation\nLetx= [x 1; : : : ; x s; : : : ; x S]be a mono audio signal with\nxsindicating the value of the s-th sample. In ACE re-\nsearch, the signal is usually converted into a 12-dimensional\nrepresentation of the harmonic content, with one such vec-\ntor for each time frame. This vector is known as a chroma\n[2] vector, and it is intended to reﬂect the distribution of\nsalience over the 12pitch classes. The chroma vectors\nfor the audio signal xare then gathered as the columns\nof a matrix X∈Rd\u0002T, with Tdenoting the number of\nframes and d= 12. In the target domain the chord an-\nnotations are denoted by c∈ A1\u0002T, with Arepresent-\ning the chord alphabet. To model the relationship between\nobserved variables Xtand hidden variables ct, a standard\nHMM [13] with the parameter set \u0002 ={\nPi∈RjAj;Pt∈\nRjAj\u0002jAj;Pe}\nis commonly used. Pi,Ptdenote the ini-\ntialization and the transition probabilities respectively, and\nthe emission probability for chord ctis frequently mod-\nelled as a single Gaussian\npe(Xt|ct) =Xt∼ N(\u0016ct;\u0006ct) (1)\nwith the distribution parameters {\u0016c;\u0006c}c2A. Under this\nframework, the joint probability of the feature vectors X\nand the corresponding chord sequence cis of the form\nP(X;c|\u0002) = pi(c1)T∏\nt=2pt(ct|ct\u00001)T∏\nt=1pe(Xt|ct):(2)\nGiven the optimal parameters \u0002\u0003, the ACE task is equiva-\nlent to ﬁnding c\u0003that maximizes the joint probability c\u0003=\narg max\n\u0016cP(X;\u0016c|\u0002\u0003), which can be done efﬁciently by the\nViterbi algorithm [13].\nBy restricting our interest to a standard HMM, the train-\ning scheme as we deﬁne it is a strategy to derive the optimal\nparameters \u0002\u0003from the training data. Given Naudio clips\nfor which the chromagrams X={Xn∈Rd\u0002Tn}N\nn=1andFigure 1. Genre distribution of the MIREX and the SALAMI\ndatasets. The MIREX dataset is dominated by Rock and Pop\ngenres, whereas the SALAMI dataset has a much wider variety of genres such as Country and Blues/Soul.\nFigure 2 . Training schemes for automatic chord estima-\ntion: universal\ntraining (left), genre-speciﬁc training (mid-\ndle) and the proposed hyper-genre training (right).\nthe chord annotations C={cn∈ A1\u0002T n}N\nn=1are both\navailable, the prevailing scheme is the universal training\n(denoted by UN, cf. left block in Fig. 2) that derives one\n\u0002\u0003from all available data {X;C}.\n1.2 Why a new training scheme?\nThe effectiveness of UN-training on the MIREX dataset\nhas now been established. Since this collection is highly\ngenre-biased and only small variations exist (cf. Fig. 1),\nUN-training can make full use of the data without con-\nfounding chord characteristics. However, the SALAMI\ndata has a much wider variety of genres1(cf. Fig. 1), this\ncasts doubts on the effectiveness of UN-training for two\nreasons: ﬁrst of all, when reducing the chords to an al-\nphabet such as triads (cf. Fig. 3), the variety of chords is\nnoticeably different between genres. For instance, Rock\nand Country genres mainly use simple chords (e.g. major),\nwhereas Jazz tends to uses more complex ones (e.g. ma-\njor 7th). This subsequently incurs a large variation on the\nchromas of the same chord among different genres (cf. Fig.4).\nIn addition, chord progressions of different genres can vary\ndramatically [11]. Neither of these can be solved by UN-\ntraining, since the scheme ignores the connection between\nmusical genre and chord variety and progression.\n1The genre information was obtained with thanks from http://\nwww.last.fm/ andhttp://www.wikipedia.org/.A potential solution to\nthese issues is to apply a more\ncomplex model such as a Mixture of Gaussians (MOG) to\nEqn. (1), but this risks the probability functions of different\nchords being confused [e.g. E:dim = (E; G; Bb) may also\nyield high probability in an MOG model for C:maj due to\nthe modelling of C:7= (C; E; G; Bb )]. One can also re-\nspect this musical factor with a more rigorous approach:\ntraining a different \u0002for each genre, an example of which\nis the genre-speciﬁc training (denoted by GS, cf. middle\nblock in Fig.2) presented in [5]. However, this method of-\nten suffers from the problems caused by data sparseness,\nbecause it can not maximally exploit the full dataset (see\nthe discussion in Sec. 3.1).\nAs an alternative, we develop a new training scheme\nwhich we call hyper-genre training (denoted by HG, cf.\nright block in Fig. 2). Instead of using independent param-\neters for each genre as it is in GS-training, HG-training\nconstructs a hierarchical probabilistic model and connect\nthe genres using hyper parameters. This framework is sim-\nilar to a Hierarchical Dirichelet Process (HDP) [15], which\nhas been applied to music similarity measurement [12] and\ntimbral similarity estimation [4] in the MIR domain. The\nmain difference here is that the proposed approach uses a\nwell-deﬁned cluster structure on the basis of musical knowl-\nedge, hence avoiding the massive sampling and uncertain\nclustering process in HDP.\nThe rest of the paper is organized as follows. In Sec. 2\nwe apply the proposed HG-training to the standard HMM\nand detail the corresponding parameter estimation. We\nthen evaluate the approach and compare it with the other\ntwo training schemes in Sec. 3. Finally the conclusions\nand future work are drawn in Sec. 4.\n2. HYPER-GENRE HMM\nTo take into account the fact that songs belong to different\ngenres, the data is divided into Kclusters according to the\ngenre information. Suppose the k-th cluster contains nk\nsongs and∑K\nk=1nk=N, we then denote the collection of\nchromagrams and chord annotations for cluster kasXk=\n{Xn∈Rd\u0002Tn}nk\nn=1, andCk={cn∈ A1\u0002T n}nk\nn=1.\nIn general, a UN-HMM trains one \u0002on all availableFigure 3 . Comparison of chord varieties of chord C be-\ntween dif\nferent genres. Each ﬁgure includes the chords\nthat can be reduced to the same triad. The percentages of\nthese chords in a cluster then suggest the genre-speciﬁc\nchord variety. Note that in this ﬁgure the genres appeared\nin the SALAMI dataset have been grouped manually and\nformed 11genre clusters.\nFigure 4. Chroma features for all occurrences of C:min-\nlike chords\n(cf. middle plot in Fig. 3) for the Groove and\nthe Hard rock genres. To aid visualization, the 12dimen-\nsional feature space has been reduced to 2using Principal\nComponent Analysis. We found that in the Groove clus-\nter, there were many more complex variants (e.g. C:min7),\nwhilst in Hard rock simple chords such as C:min were\nmore common. Owing to this, there is a large variation\nbetween their chroma features.data{X;C}; while a GS-HMM has a set of parameters\n\u0002={\u0002k}K\nk=1, each of which is trained on the cluster\nexamples {Xk;Ck}. The HG-HMM also has a set of genre-\nspeciﬁc parameters \u0016\u0002={\u0016\u0002k= (\u0016Pk\ni;\u0016Pk\nt;\u0016Pk\ne)}K\nk=1, but it\nties them together by hyper parameters as a Bayesian prior.\nOne implementation of the HG-HMM is depicted in\nFig. 5, in which the genre clusters are connected via a hy-\nper parameter set \u00020={\u00160∈Rd\u0002jAj;\u00060∈Rd\u0002d\u0002jAj;\n\u000b0∈RjAj;\f0∈RjAj\u0002jAj;k0∈NK; m0}to propagate\ninformation. Under this framework, the parameter esti-\nmates of \u0016\u0002are computed as follows.\n2.1 Parameter Estimation\nFor the emission probability \u0016Pk\neof cluster k, the hyper link\n(dash line in Fig. 5) is equivalent to applying a conjugate\nprior to the distribution parameters {\u0016c\nk;\u0006c\nk}c2A:\n\u0006c\nk∼ W (\u0006c\n0; m0)\n\u0016c\nk|\u0006c\nk∼ N(\u0016c\n0;1\nkc\n0\u0006c\nk)∀c∈ A; (3)\nwhere Wdenotes the Wishart distribution [1]. The Bayesian\nupdate of the\nemission probability then becomes\n\u0016pk\ne(Xt|c) = Xt∼ T(m0+mc\nk−d+ 1;\n\u0016\u0016c\nk;kc+ 1\nkc(m0+mc\nk−d+ 1)\u0016\u0006c\nk):(4)\nIn (4)Tdenotes the multivariate Student-t distribution with\nthe following\nparameters\nmc\nk= #(c t=c),∀ct∈ Ck;\nkc=kc\n0+mc\nk;\n\u0016\u0016c\nk=kc\n0\u0016c\n0+mc\nk\u0016∗c\nk\nkc\n0+mc\nk;\n\u0016\u0006c\nk=\u0006c\n0+mc\nk\u0006\u0003c\nk+kc\n0mc\nk\nkc∥\u0016\u0003c\nk−\u0016c\n0∥2;(5)\nwhere #indicates ‘the number of’ and {\u0016\u0003c\nk;\u0006\u0003c\nk}c2Aare\nthe maximum likelihood (ML)\nestimations of the parame-\nters{\u0016c\nk;\u0006c\nk}c2Ausing the cluster examples {Xk;Ck}.\nSimilarly, the hyper priors applied to the initialization\nand the transition parameters of cluster kare given by\nPk\ni|\u000b0={pk\ni(c)|c∈ A} ∼ Dir(|A|;\u000b0);\nPk\nt|\f0={pk\nt(c|\u0016c)|c∈ A} ∼ Dir(|A| ;\f\u0016c\n0);∀\u0016c;(6)\nwhere Diris the Dirichlet distribution. The Bayesian up-\ndate of these probabilities are then computed by: ∀c1; ct\u00001;\nct∈ Ck\n\u0016pk\ni(c) =#(c1=c)+\u000bc\n0∑\nc′∈A#(c1=c′)+∑\nc′∈A\u000bc′\n0;\n\u0016pk\nt(c|\u0016c) =#(c t=c&ct−1=\u0016c)+\f\u0016c;c\n0∑\nc′∈A#(ct=c′&ct−1=\u0016c)+∑\nc′∈A\f\u0016c;c′\n0:(7)\nNote that if a non-informative prior is used (i.e. \u000bc\n0= 1\nand\f\u0016c;c\n0=\n1), the Bayesian update (7) is the ML esti-\nmations of the initialization and the transition parameters\ntrained on the cluster examples. This is equivalent to using\n{Pk\ni;Pk\nt}K\nk=1as used in the GS-HMM.\nEqns. (5) and (7) provide the insight of the hyper pa-\nrameters: they reﬂect our prior belief about the genre clus-\nter parameters. Hence when few data are available to esti-\nmate the parameters of cluster k, the HG-HMM can beneﬁtFigure 5 . The implementation (dash-dot box) of the hyper-genre HMM\nfor the SALAMI dataset. Ideally each genre\nshould be regarded as a cluster, but in practice this is difﬁcult to achieve with limited data. In order to assure a reasonable\ncluster size, we grouped the genres and created 11genre-related clusters. These clusters are then connected via the hyper\nparameters so as to share information. The number of songs in a genre (or cluster) is shown in the bracket.\nfrom the hyper parameter set. The clusters (e.g. Rock) can\nalso share information with related ones (e.g. Blues), while\nretaining their intrinsic chord varieties and progressions.\nIdeally the set \u00020should has several subsets, one for each\ngroup of related genres. However, restricted by the data\navailable we had to follow the suggestions in [4, 12] and\nused a single \u00020that reﬂects the distribution of the whole\ndataset instead: that is, (\u00160;\u00060)are set to the mean and\nthe covariance matrices of all training data respectively;\n\u000b0;\f0counts for all chord initializations and transitions,\nm0=dandkc\n0= #(c t=c),∀ct∈ C.\n2.2 Decoding\nGiven the updated parameters {\u0016\u0002k}K\nk=1, the decoding pro-\ncess is given by\nc\u0003= arg max\n\u0016cP(X;\u0016c|\u0016\u0002k)\n= arg max\n\u0016c\u0016pk\ni(c1)T∏\nt=2\u0016pk\nt(ct|ct\u00001)T∏\nt=1\u0016pk\ne(Xt|ct)\n(8)\nThe decoder (8) requires the cluster label kof the test\nexample. This requirement can be easily waived by using\nthe maximum likelihood inference as suggested in [5]:\n{c\u0003; k\u0003}= arg max\n\u0016c;kP(X;\u0016c|\u0016\u0002k): (9)3. EXPERIMENTS\nHere we describe the main experiments conducted. The\ndataset investigated is the SALAMI data, which contains\n522songs along with the ground truth chord annotations2.\nIn the experiments, we restrict the ACE system to a stan-\ndard HMM with the loudness based chromagram [9]. In\norder to capture intrinsic chord varieties and progressions\nbetween genres but retaining a controllable complexity, we\nrestricted ourselves to an alphabet of triads3, with 73uni-\nque chords in total. To evaluate the proposed approach,\nwe randomly split 2=3of songs from each genre cluster to\nform the training set, while the remaining 1=3were used\nfor testing. The frame-based chord estimation accuracy is\nused as the evaluation metric and in total 102train-test runs\nwere done to assess variance4.\n3.1 Comparison of different training schemes\nThere are three training schemes we can apply to the emis-\nsion parameters: universal training (UN), genre-speciﬁc\ntraining (GS) and hyper-genre training (HG). Similarly, th-\ney can be applied to the initialization and the transition pa-\nrameters, resulting in 3×3 = 9 combinations. Suppose the\n2The readers are referred to the Appendix for our process of ex-\ntracting\nthe SALAMI chord annotations. These annotations are avail-\nable online at https://patterns.enm.bris.ac.uk/files/\nSALAMI_522_chord_annotations.zip .\n3Chord types: maj, min, dim, sus2, sus4, aug and N.\n4That is, each song in the dataset would be tested 34times.cluster labels are known, in this subsection we compared\nthese combinations using the decoder (8).\nTable. 1 presents the overall performances for each com-\nbination, from which we observed a consistent improve-\nment of the hyper-genre training over the other schemes.\nIn particular, the HG-GS combination achieves the best\nperformance, amounting to 9:3% and14:1% reductions\non the error rate compared with the universal (UN-UN)\nand the genre-speciﬁc (GS-GS) trainings respectively. Al-\nthough applying a transition prior learnt from all the data\nstill yields better results than the other schemes, it is worse\nthan merely using a non-informative one. We postulate this\nis because a transition prior learnt from all examples is a\nmixed chord progression of all genres, applying it would\ninevitably confound some typical progressions such as “I\n(tonic) - V (dominant) - IV (subdominant)” commonly seen\nin the Blues genre. This suggests that further improvement\nmight be gained by applying musical knowledge based tran-\nsition priors on different genres, which can be obtained\nfrom e.g. the synthetic MIDI data used in [5].\nE\\T UN GS HG\nUN 63.61 ±1:31 64:39±1:29 64:3±1:30\nGS 60:61 ±1:60 61.56 ±1:68 61:05 ±1:67\nHG 65:93 ±1:17 66:98±1:21 66:47 ±1:19\nTable 1. Performances [%]of different scheme combina-\ntions on the SALAMI dataset\n(best result in bold). The\nvertical axis shows the schemes applied to the emission\nparameters and the horizontal axis shows that to the ini-\ntialization/transition parameters. The improvement of the\nHG-HMM with non-informative priors (HG-GS) is signif-\nicant at a level <10\u000034over the performances of the other\nscheme combinations under a paired t-test.\nFigure 6 further depicts the performances of different\ntraining schemes on each genre cluster. For GS-training,\nthe performance gradually increases when more examples\nare available for a cluster. The only exception is Groove,\npossibly due to the fact that the complex chords in this\ncluster are difﬁcult to estimate. Limited by the amount\nof the data available for each cluster, GS-training is gener-\nally inferior to UN-training. This problem was not experi-\nenced or explored in [5] (Sec. 4.3), since their experiments\nwere based on synthetical MIDI data such that the GS-\nHMM had sufﬁcient examples to train the parameters. Al-\nternatively, HG-training is slightly worse than UN-training\nwhen the data is very limited, by means of sharing in-\nformation from the hyper parameters. For other clusters\nsuch as Rock, HG-training allows it to obtain information\nfrom related clusters such as Blues/Soul, while retaining\nthe genre-speciﬁc chord variety and progression. This ben-\neﬁt makes it outperform UN-training and improve the per-\nformance by an absolute 4:4%.\n3.2 Bypassing the genres\nIn practice the genre information of the test data is un-\nknown, hence there is no choice but to use a genre-independent\nFigure 6 . Performances of the universal (UN-UN), the\ngenre-speciﬁc (GS-GS), the\nhyper-genre (HG-HG) and the\ncombined (HG-GS) training schemes on each cluster. The\nclusters from left to right are sorted by the number of ex-\namples in the clusters.\nmodel (e.g. UN-training); or increase the model complex-\nity and infer the genre as well. In this subsection we inves-\ntigate how much we can gain by inferring the genre with\nmaximum likelihood technique (9). The experiment setup\nwas the same as that in Sec. 3.1, and we compared the fol-\nlowing models with the ones presented in Tab. 1:\n1)The GS-HMM (using the schemes GS-GS) with the\ndecoder (9). This is the model suggested in [5] (Sec. 4.3).\n2)The HG-HMM (using HG-GS) with the decoder (9).\nIn addition to chord estimation accuracies, the genre\nprediction accuracies of the models are also evaluated.\nTab. 2 shows the results and we observed a mild de-\ncrease in performance when compared with the same mod-\nels using genre information in Tab. 1. This reﬂects the ben-\neﬁt obtained from the genre information used. However,\nthe performances of using ML inference are very close to\nthat of using genre information directly, suggesting that\nthis technique is reliable to use when the genre for a test\nexample is unknown.\nIt is worth pointing out that the genre prediction accu-\nracies are very low for both models, probably for two rea-\nsons. Firstly, since the numbers of examples in different\nclusters are highly imbalanced, some clusters might not\nhave enough data to train the parameters (e.g. Folk and\nJazz). In this case, a test example from that cluster could\nbe better decoded by other cluster models. Although ap-\nplying the hyper parameters can ease this problem and im-\nprove the chord estimation accuracy, it makes the cluster\nmodels closer to the hyper prior and inevitably confounds\ntheir intrinsic chord varieties. This consequently worsens\nthe genre prediction accuracies.\nAnother potential explanation is that since the genre\nclusters are highly overlapped, it is very difﬁcult to rigor-\nously classify an example into just one cluster. This seems\nto suggest using a more ﬂexible forest structure (e.g. the\ngenre Country Rock should be connected to both Country\nand Rock clusters) instead of the directed tree depicted in\nFig. 5, which will be investigated in our future work.GS-GS HG-GS\nC-Acc 61:27±1:50 66:83 ±1:27\nG-Acc 16:05±2:98 10:52 ±1:81\nTable 2. Performances [%]of different models on the\nSALAMI dataset (best result\nin bold). C-Acc denotes the\nframe-based chord estimation accuracy and G-Acc is the\ngenre prediction accuracy.\n4. CONCLUSIONS AND FUTURE WORK\nIn this paper, we proposed a new training scheme – hyper-\ngenre training for automatic chord estimation, capable of\ntesting on multiple varied genres. The principle is to con-\nstruct a hierarchical probabilistic model and connect the\ngenre clusters using hyper parameters. Compared with\nthe prevailing universal training scheme, HG-training is\nable to retain chord variety and progression characteris-\ntics of musical styles. Compared with genre-speciﬁc train-\ning, HG-training can beneﬁt from the hyper prior and it\nresolves the problem of data sparseness often encountered\nin real world data. Both beneﬁts have been veriﬁed in\nour experiments on a large and varied chord annotation\ndataset, where HG-training achieved signiﬁcant improve-\nments over the other two schemes.\nFor future work, we aim to improve the hierarchical\nstructure of the proposed approach. This can be done by\nemploying a more ﬂexible forest structure instead of the\ndirected tree graph. An alternative direction of research\nis to learn such hierarchical structure from the data auto-\nmatically, which might lead to a more robust and powerful\nACE system. Finally, we are also interested in how incor-\nporating musical knowledge based transition priors may\nimprove chord estimation accuracy.\n5. APPENDIX: EXTRACTING CHORD\nANNOTATIONS FOR THE SALAMI DATASET\nIn this appendix we summarize how we extracted the ground\ntruths from the SALAMI chord annotation ﬁles.\nThere are two processes: obtaining the chord labels and\ninferring the durations. For the former, we followed the\ndescription in [14] and parsed the chord labels with the C.\nHarte’s chord parser [3]. There were several exceptions,\nwhich we revised manually. A more difﬁcult task is to\nextract chord durations. The SALAMI chord annotation\nﬁles do not offer time stamps for each chord (as used in\nthe MIREX annotation ﬁles). Instead, time instances are\ngiven over multiple bars, where each bar contains one or\nmore chord. Our assumption is that the bars between two\ntime stamps would have equal durations (if they are not in\nthe same meter, then the durations are adjusted according\nto the meter). Under this assumption we extracted chord\ndurations from the annotation ﬁles.\nThe original SALAMI dataset contains 649songs, from\nwhich we found 21songs having ambiguous tuning. Addi-\ntionally there are some duplications and cover songs. For\nour experiments, we removed the 21songs and the dupli-cate and cover songs so that each unique song only ap-\npeared once in the dataset. After this process we obtained\na set of 522songs.\n6. REFERENCES\n[1] C. M. Bishop. Pattern Recognition and Machine\nLearning . Springer, 2006.\n[2] T. Fujishima. Real time chord recognition of musical\nsound: a system using common lisp music. In Proc. of\nICMC, pages 464–467, 1999.\n[3] C. Harte, M. Sandler, and S. Abdallah. Symbolic repre-\nsentation of musical chords: a proposed syntax for text\nannotations. In Proc. of ISMIR, pages 66–71, 2005.\n[4] M. Hoffman. Probabilistic graphical models for the\nanalysis and synthesis of music audio. PhD thesis,\nPrinceton University, 2010.\n[5] K. Lee. A system for acoustic chord transcription and\nkey extraction from audio using hidden Markov mod-\nels trained on synthesized audio . PhD thesis, Stanford\nUniversity, 2008.\n[6] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent hmms\ntrained on synthesized audio. The IEEE Transactions\non Audio, Speech and Language Processing , 2008.\n[7] M. Mauch. Automatic chord transcription from audio\nusing computational models of musical context . PhD\nthesis, Queen Mary University of London, 2010.\n[8] M. M ¨uller and S. Ewert. Towards timbre-invariant au-\ndio features for harmony-based music. IEEE Trans.\nAudio, Speech, Lang. Process. , 18(3):649–662, 2010.\n[9] Y. Ni, M. Mcvicar, R. Santos-Rodriguez, and T. De\nBie. An end-to-end machine learning system for har-\nmonic analysis of music. IEEE Trans. Audio, Speech,\nLang. Process. , 20(5), 2012.\n[10] K. Noland and M. Sandler. Key estimation using a hid-\nden Markov model. In Proc. of ISMIR , 2006.\n[11] W. Piston. Harmony. Norton, New York, 1978.\n[12] Y. Qi, J. Paisley, and L. Carin. Music analysis using\nhidden markov mixture models. IEEE Transactions on\nSignal Processing , 55(11):5209–5224, 2007.\n[13] L. R. Rabiner. A tutorial on hidden Markov models and\nselected application in speech recognition. In Proc. of\nthe IEEE, 1989.\n[14] J. Smith, J. Burgoyne, I. Fujinaga, D. Roure, and\nJ. Downie. Design and creation of a large-scale\ndatabase of structural annotations. In Proc. of ISMIR,\n2011.\n[15] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical\ndirichlet processes. Journal of the American Statistical\nAssociation, 101(476):1566–1581, 2007."
    },
    {
        "title": "Compressing Music Recordings into Audio Summaries.",
        "author": [
            "Oriol Nieto",
            "Eric J. Humphrey",
            "Juan Pablo Bello"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415802",
        "url": "https://doi.org/10.5281/zenodo.1415802",
        "ee": "https://zenodo.org/records/1415802/files/NietoHB12.pdf",
        "abstract": "We present a criterion to generate audible summaries of music recordings that optimally explain a given track with mutually disjoint segments of itself. We represent audio as sequences of beat-synchronous harmonic features and use an exhaustive search to identify the best summary. To demonstrate the merit of this approach, we evaluate the cri- terion and show consistency across a collection of multiple recordings of different works. Finally, we present a fast algorithm that approximates the exhaustive search and al- lows us to automatically learn the hyperparameters of the algorithm for a given track.",
        "zenodo_id": 1415802,
        "dblp_key": "conf/ismir/NietoHB12",
        "keywords": [
            "beat-synchronous",
            "harmonic features",
            "audio sequences",
            "exhaustive search",
            "audible summaries",
            "mutually disjoint segments",
            "optimally explain",
            "collection of multiple recordings",
            "hyperparameters",
            "fast algorithm"
        ],
        "content": "COMPRESSING MUSIC RECORDINGS INTO AUDIO SUMMARIES\nOriol Nieto\nNew York University\noriol@nyu.eduEric J. Humphrey\nNew York University\nejhumphrey@nyu.eduJuan Pablo Bello\nNew York University\njpbello@nyu.edu\nABSTRACT\nWe present a criterion to generate audible summaries of\nmusic recordings that optimally explain a given track with\nmutually disjoint segments of itself. We represent audio\nas sequences of beat-synchronous harmonic features and\nuse an exhaustive search to identify the best summary. To\ndemonstrate the merit of this approach, we evaluate the cri-\nterion and show consistency across a collection of multiple\nrecordings of different works. Finally, we present a fast\nalgorithm that approximates the exhaustive search and al-\nlows us to automatically learn the hyperparameters of the\nalgorithm for a given track.\n1. INTRODUCTION\nOne of the classic motivations in the ﬁeld of music infor-\nmatics is facilitating the navigation of massive digital mu-\nsic collections by human users. Research in this area aims\nto develop computational methods of organizing and re-\ntrieving music recordings —tracks— in the spirit of reduc-\ning the amount of effort necessary to ﬁnd desired content.\nUltimately, the user must listen to any unfamiliar track to\nvalidate the search results, making the process consider-\nably time consuming.\nIn digital music storefronts and other kinds of large col-\nlections, the traditional solution is to represent a full track\nwith a single, identiﬁable excerpt. Known as audio thumb-\nnailing, much effort has been invested into the develop-\nment of automatic systems to these ends; for a partial re-\nview, we refer to [1, 2, 6, 7]. For some popular music that\nis highly repetitive in nature, these methods perform well\nin identifying useful thumbnails. Regardless, representing\na full track with a single excerpt presents one unavoidable\ndeﬁciency: the deﬁning characteristics of a track are rarely\nconcentrated in one speciﬁc section.\nRecognizing this shortcoming, we motivate an alterna-\ntive approach to classical thumbnailing that instead creates\na short, listenable audio summary, capturing both the most\nunique and representative parts of a track. Speciﬁcally, this\npaper presents a novel audio summary criterion and an efﬁ-\ncient method of automatically generating these summaries\nfrom real music recordings. The criterion is maximal for\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.the set of segments that best explain the overall track while\nsimultaneously exhibiting minimal overlap between them.\nVia examples and an experimental study we show how this\nmeasure yields good audio summaries. Furthermore, we\nshow that it is possible to automatically select the optimal\nnumber and length of the selected subsequences speciﬁc to\na given recording.\nThe remainder of this paper is organized as follows:\nSection 2 addresses the topic of feature representation. Sec-\ntion 3 deﬁnes the music summary criterion and showcases\nthe measure in practice. Section 4 details a heuristic ap-\nproximation to the exhaustive evaluation over the free pa-\nrameters. Section 5 presents a systematic evaluation of the\nfeature representation, heuristic solution and effect of au-\ntomatically learning hyperparameters. Finally, we discuss\nour conclusions and observations for future work in Sec-\ntion 6.\n2. FEATURE REPRESENTATION\nThe goal of developing an appropriate representation is to\ncapture the information relevant to a given task while dis-\ncarding unnecessary attributes. With this in mind, we de-\nscribe the method of transforming time-domain audio sig-\nnals into beat-synchronous sequences of harmonic features\nfrom which audio summaries can be identiﬁed.\n2.1 Beat-Synchronicity\nAs a preprocessing stage, a recording is ﬁrst analyzed by\na beat tracking algorithm adapted from [3] for subsequent\nbeat-synchronous feature extraction. In the interest of mit-\nigating octave errors and producing consistent feature se-\nquences across a variety of content, we impose constraints\non the range of possible tempi the system can track. This is\nachieved by the following modiﬁcation: periodicity anal-\nysis of the novelty function \u0001nis computed at N log 2\nspaced frequencies per octave over the range [1 : 8] Hz,\nproducing the tempogram Tas deﬁned in [3]. This time-\nfrequency representation is then wrapped to a single tempo\noctave ofNbins and the most likely tempo path is ex-\ntracted via the Viterbi decoder. In lieu of static transition\nprobabilities, the transition probability matrix ptrans is de-\nﬁned as an identity matrix Iof rankNconvolved with a\n1-D, 0-mean Gaussian window N, where the standard de-\nviation\u001bnis parameterized by the relative amplitude of the\nmaximum tempogram as a function of time n, as follows:ptrans[n] =IN\u0003N\u0012\n\u0016= 0;\u001b n=max(jT [n]j)\n\u0016jT[n]j\u0013\n(1)\nThis has the desirable effect of allowing the tempo es-\ntimator to adapt when the pulse strength is high, but resist\nchange when the tempo becomes ambiguous. To ﬁnd the\nbest tempo octave to unwrap the path into, we analyzed\na histogram of the chord durations contained in publicly\navailable chord annotations1. Having found that approxi-\nmately 95% of the chord durations are greater than 0.5 sec-\nonds in duration, we select 2Hz as a natural upper bound\nand map the optimal path through the single octave tem-\npogram into the range of 60-120 BPM. At this stage, the\nremainder of the implementation follows the reference al-\ngorithm.\n2.2 Harmonic Representations\nConventional approaches to harmonic analysis tasks in mu-\nsic informatics are predominantly built upon the use of\nchroma features, and we continue that tradition here. We\nalso explore the use of tonal centroids, or Tonnetz features,\nas a mid-level harmonic representation. Introduced for the\npurpose of detecting harmonic change by Harte et al [4],\nthe intuition for this decision is motivated as follows. First,\ntypical distance metrics fail to capture musical signiﬁcance\nbetween chroma vectors. In a pitch class representation,\nfor example, the L2distance between a C major triad and\na C] triad is equal to the distance between either triad and\nthe notes B, B[, and A. Additionally, chroma behaves like\na mass function and it is not immediately apparent how to\nbest measure the distance between these vectors. A Ton-\nnetz representation, however, provides a geometric inter-\npretation of pitch collections where distance is better de-\nﬁned as a musical and an Euclidean sense.\nTo compute both harmonic feature variants, we apply\nthe constant-Q transform to a frame of audio over the range\nof 110–1760 Hz with 12 bins per octave, producing a pitch\nvectorX. The length of the analysis window is determined\nby the longest ﬁlter, and is set to 0.45 seconds. Inspired\nby [5], a modiﬁed pitch vector Yis produced by standard-\nizing the log-coefﬁcients log(\u0015X)and half-wave rectify-\ning the result. The \u0015scale factor is heuristically set to\n1000, but values within an order of magnitude in either\ndirection produce similar results. Chroma features are de-\nrived fromYby wrapping onto a single octave and scaling\nby theL2norm, and Tonnetz features are computed iden-\ntically to the method presented in [4].\n2.3 Feature Quantization\nIt is computationally advantageous to quantize the feature\nspace into a ﬁnite number of discrete values. We perform\nvector quantization by clustering the feature space via K-\nmeans and replacing each feature vector by its cluster’s\ncentroid. The pairwise distances between centroids are\nprecomputed to accelerate distance calculations between\n1https://github.com/tmc323/Chord-Annotationssymbolic feature sequences (see Section 3). Though larger\nvalues ofKmore faithfully reproduce the original features,\nthis might result in an intractable process due to computa-\ntion limitations as we see in subsection 3.3.\n3. DEFINING AN AUDIO SUMMARY CRITERION\nStructure and repetition are fundamental characteristics of\na musical work, and an audio summary should retain the\nminimum number of distinct parts that are necessary to\ndescribe it. Therefore, a good summary criterion actu-\nally synthesizes two opposing notions: we seek to lose as\nlittle information as possible, while avoiding overlap be-\ntween chosen segments. A summary is deﬁned as the set\n\u0000 = [\rN\n1;:::;\rN\nP]ofP,N-length subsequences that max-\nimizes a function \u0002over a feature sequence Sof length\nM, where9ms.t.sN\nm=\rN\ni;m2[1 :M],sN\nm2S, and\ni2[1 :P].\n3.1 Compression Measure\nThe goal of describing a sequence in terms of itself with a\nminimal loss of information is fundamentally a data com-\npression problem. Building upon this idea, we deﬁne a\ncompression measure C(\u0000jS) that quantiﬁes the extent to\nwhich \u0000explains a given S, deﬁned as follows:\nC(\u0000jS) = 1\u00001\nPJPX\ni=1JX\nm=1jj\rN\ni;sN\nmjj2 (2)\nThis measure can be interpreted as a normalized, convo-\nlutive Euclidean distance, such that there are J=M\u0000N+\n1element-wise comparisons between a given N-length sub-\nsequence\rN\niand allJ N -length subsequences sN\nm2S.\nAll distances, taken directly from the precomputed pair-\nwise matrix discussed in Subsection 2.3, are then averaged\nover theJrotations and Psubsequences in \u0000. Intuitively,\nthe compression measure equals 1when \u0000 =Sand0when\n\u00006\u0012S.\n3.2 Disjoint Information Measure\nIn addition to determining how well \u0000describes S, it is\nnecessary to also measure the amount of information shared\nbetween each pair of subsequences in a set. Conversely, a\ndisjoint information measure I(\u0000)seeks to quantify the\nuniqueness of each subsequence in \u0000relative to the rest,\ndeﬁned as follows:\nI(\u0000) =0\n@PY\ni=1PY\nj=i+1Dmin(\u001e(\rN\ni);\u001e(\rN\nj))1\nA2\nP(P\u00001)\n(3)\nWe achieve shift-invariance by mapping a sequence of\nfeatures\rN\nito a sequence of shingles\u001aK\niwith lengthK=\nN\u0000L+1where a shingle is deﬁned as the stacking of Lad-\njacent feature frames into a single feature vector. The func-\ntion\u001ereturns the shingled version of a subsequence. A\nmodiﬁed Euclidean distance function Dminthen measures\nthe intersection between sequences of shingles, returning\nthe average minimum distance between the uthshingle inFigure 1. Search space for C,Iand\u0002(left, middle, and right respectively) for P= 2 subsequences in the ﬁrst half of a\nperformance of the Mazurka Op. 30 No. 2. Black lines split part A and B. Circles mark the maximum value. Each position\nin the matrices correspond to a 8-beat subsequence.\n\u001aK\niand allvshingles in a different subsequence \u001aK\nj, de-\nﬁned as follows:\nDmin(\u001aK\ni;\u001aK\nj) = KX\nu=1minv(\u001ai[u]\u0000\u001aj[v])2!1=2\n(4)\nThere are two important subtleties that must be observed\nwhen calculating this measure. First, distances between\nshingles are deﬁned by the element-wise L2norm based\non the same pairwise distance matrix as before. Addition-\nally,I(\u0000)is a geometric mean and only produces large\nvalues when all pairwise distances are also large; any small\ndistance in the product forces the overall measure toward\nzero.\n3.3 Criterion Deﬁnition and Calculation\nHaving established measures of compression and disjoint\ninformation for some \u0000, we capture both of these traits by\ndeﬁning a single criterion \u0002as follows:\n\u0002(C;I) =2CI\nC+I(5)\nNoting thatCandIare constrained on the interval [0,1]\nand converge to one when optimal, computing the criterion\nas a harmonic mean enforces the behavior that its value is\nonly large when both measures are as well.\nIt is worthwhile at this point to make the observation\nthat this criterion can —at least theoretically— be evalu-\nated at every unique combination of subsequences \u0000over\nan entire sequence S. The output of this exhaustive cal-\nculation is a Pdimensional tensor where each axis is of\nlengthJ, and the best summary is given simply by the\nargmax of the resulting data structure. From here on-\nward, we use optimal criterion \u0002max to refer to the ab-\nsolute maximum of this tensor, as would be found through\na na¨ıve, exhaustive search of the space. Note that for large\nJandPhowever, evaluating every cell in this tensor be-\ncomes computationally intractable and efﬁcient approxi-\nmations are necessary (see Section 4).3.4 Case Example\nHere we illustrate the behavior of the audio summary cri-\nterion by analyzing by the ﬁrst half of Fr ´ed´eric Chopin’s\nMazurka Op. 30 No. 2, which exhibits a well-deﬁned AB\nstructure. For the sake of demonstration, we select a sub-\nsequence length of N= 8 and deﬁneP= 2 such that\nan exhaustive evaluation of \u0002produces aJ\u0002Jmatrix.\nThe result of computing C,Iand\u0002over all pairs of sub-\nsequences is shown in Figure 1.\nThe compression measure Cis shown in the left-most\nmatrix of Figure 1. This measure quantiﬁes the extent to\nwhich a set \u0000explains the overall track independent of\nany correlation between subsequences. The optimal Cin\nthis matrix corresponds to the two subsequences at beat in-\ndices (48, 59) in the B-B quadrant. These subsequences\ncorrespond to repetitions of the same part, making the in-\nformation in \u0000redundant.\nThe center matrix in Figure 1 corresponds to the disjoint\ninformation measure I. This measure captures the degree\nof uniqueness between subsequences in \u0000. It is clear from\nthe plot that the measure behaves as expected: repeated\nsubsequences in the same section (in quadrants A-AorB-B)\nproduce signiﬁcantly lower values of Ithan subsequence\npairs in A-B, where the highest Iis found.\nFinally, the previous two matrices combine to yield a\nthird, the criterion \u0002. In the example the maximum value\nofCcorresponds to repetitions of the same part, thus mak-\ningIto be small and forcing the overall \u0002to also be small.\nSimilarly, the position of the maximum value of Iat the\nboundary between AandBresults in a lowCvalue, again\nproducing a smaller \u0002. In this example, \u0002is maximized\nby the combination of subsequences in A,Bthat best bal-\nance the two criteria by capturing the midsections of each\npart.\n4. APPROXIMATING THE OPTIMAL SOLUTION\nAs mentioned in the previous section, na ¨ıve calculation\nof the optimal criterion can, in certain scenarios, becomecomputationally inefﬁcient, impractical, or worse. More\nspeciﬁcally, an exhaustive evaluation and parallel search\nof the full \u0002tensor of size (J=2)Pwould result in an algo-\nrithm of complexity O((JN logJ)P). In this section, we\npresent a heuristic approach that approximates the optimal\nsolution using a much faster implementation.\n4.1 Heuristic Search Algorithm\nThe main idea behind the fast approach is to assume that\nthe most relevant parts of a song will most likely be uni-\nformly spread across time. The pseudocode is found in Al-\ngorithm 1. The method EquallySpaced() initializes all P\nsubsequences into equally spaced time indices and stores\nthem in the array \u0007. We then iterate over the Psubse-\nquences, ﬁxing all of them except the Pibeing processed.\nWe use a sliding window, operating over the region be-\ntween the endpoint of the previous subsequence and the\nstart of the next one, to ﬁnd the best local music criterion\n\u0012by calling the function ComputeCriterion(). At every\niteration we check if the sliding window is within the cor-\nrect bounds with the method CheckBounds(), and if it is,\nwe update the best index \u001din\u0007. Finally, the summary \u0000\nis obtained by concatenating the subsequences at the time\nindices in \u0007. This operation is done inside the method Get-\nSubseqsFromTimeIdxs().\nAlgorithm 1 Heuristic Approach\nRequire: S=fs1;:::;s Mg;P;N\nEnsure: \u0000 =f\rN\n1;:::;\rN\nPg\n\u0007 EquallySpaced(S;P;N )\nfori= 1!Pdo\n\u0012 0\nforj= 1!Mdo\nifCheckBounds(\u0007) then\n\u0002 ComputeCriterion(S; \u0007;N;P )\nif\u0002>\u0012 then\n\u0012 \u0002;\u001d j\nend if\n\u0007[i] j\nend if\nend for\n\u0007[i] \u001d\nend for\n\u0000 GetSubseqsFromTimeIdxs(S; \u0007)\nreturn \u0000\nThe complexity in time of this algorithm is O(PMJ ),\nwhich makes it linear with respect to P. This approach im-\nproves the efﬁciency dramatically and let us explore differ-\nent hyperparameter values of PandN, as we will see in\nsubsection 5.4.\n5. EV ALUATION\nWe now proceed to evaluate multiple facets of the audio\nsummary criterion. We begin by reviewing the dataset\nused for evaluation before presenting three different exper-\niments.5.1 Methodology\nIn our experimentation, we use a collection of solo pi-\nano music compiled by the Mazurka Project2, comprised\nof 2914 tracks corresponding to different recorded perfor-\nmances of 49 Mazurkas. For clarity, we use piece orwork\nwhen referring to a Mazurka, and reserve track orperfor-\nmance to describe an instance of the work as audio. The\nmotivation for using this dataset is to leverage the sev-\neral performances of a single work to measure the con-\nsistency of our criterion. Additionally, this collection con-\ntains 301 tracks with human-annotated, ground truth beat\ntimes, which allows us evaluate the impact of beat track-\ning on various dimensions of performance. It also provides\nthe added beneﬁt that Chopin’s Mazurkas are notoriously\ndifﬁcult to beat-track via computational means [3].\n5.2 Parameter Sweep & Selection\nIn the interest of selecting a feature space with which to\nproceed, an experiment is designed to sweep across the\nrange of free parameters to identify the optimal conﬁgu-\nration. There are three questions to address: Is automatic\nbeat tracking sufﬁcient? Do chroma and Tonnetz features\nperform equivalently, or is one preferable? Does perfor-\nmance vary signiﬁcantly as a function of codebook size?\nThese three decisions can be resolved by observing how\nthe optimal criterion behaves across various performances\nof the same work, comparing between ground truth and es-\ntimated beat annotations. Intuitively, a satisfactory audio\nsummary of the same piece would persist across recorded\nversions, so the summaries themselves should be substan-\ntially similar.\nFor those 301 recordings with ground truth beat annota-\ntions, we stratify the tracks into ﬁve folds for cross valida-\ntion such that all but one are used to train the quantizer and\nthe remaining hold-out is reserved as a test set. Sweeping\nacross the two beat annotation sources (ground truth, auto-\nmatic), two harmonic representations (chroma, Tonnetz),\nand three codebook sizes (50, 100, 200) produces 12 pos-\nsible feature space conﬁgurations (see Table 1). Summary\nsets\u0000are identiﬁed by exhaustively computing \u0002maxover\nall possible combinations of subsequences, where segment\nlengthNand number Pare ﬁxed at 16 and 4, respectively.\nAdditionally, a stride parameter of N=2, analogous to a hop\nsize in frame based audio processing, is applied to make\nthe exhaustive search more computationally tractable.\nTo measure the degree to which summaries of the same\nwork (intra-class distance) are closer than those from other,\ndissimilar works (inter-class distance), the pairwise dis-\ntances between summaries of tracks in each fold are com-\nputed and the values are treated as empirical distributions\nof these two classes. The Fisher ratio, deﬁned by (6),\nprovides an estimate of the separation between intra- and\ninter-class summary distances.\nFratio =\u0016intra\u0000\u0016inter\n\u001b2\nintra +\u001b2\ninter(6)\n2http://www.mazurka.org.ukk GT-C GT-T A-C A-T\n50 3.64 3.97 2.71 3.89\n100 3.84 4.29 2.68 4.20\n200 4.09 4.74 2.87 4.45\nTable 1. Parameter Sweep. GT: Ground Truth, A: Auto-\nmatic, C: Chromagram, T: Tonnetz\nIntuitively, higher values of Fratio indicate distinct, well-\nlocalized distributions where ‘similar’ items cluster togeth-\ner, and translates to more consistency across performances.\nTable 1 shows the results of sweeping free parameters in\nthe feature space. There are a few important observations\nto make about these results. First, a Tonnetz representa-\ntion produces consistently better results than chroma fea-\ntures. Additionally, Tonnetz features computed from au-\ntomatically extracted beat times only marginally trail their\nground truth equivalent. Finally, the codebook size Khas\na non-trivial impact on performance and is positively cor-\nrelated. Therefore, we can conclude that Tonnetz-features\ncomputed with a beat tracking front-end are the best choice\ngoing forward, and that the parameter Kshould be large\nand ultimately based on practical limitations of the imple-\nmentation.\n5.3 Heuristic Approximation\nWe evaluate the performance of the heuristic approach by\ncomparing the summaries it produces with the optimal so-\nlution obtained through exhaustive computation. A second\ncomparison is made with the expected performance a ran-\ndom algorithm, obtained by averaging across all results ob-\nserved in the course of computing \u0002max. This establishes\nthe upper (optimal) and lower (random) bounds of perfor-\nmance and allows us to determine where on this continuum\nour heuristic solution lives. We measure the discrepancy\nbetween the optimal \u0002max, random \u0002rand, and heuristic\n\u0002heur solutions by computing the averaged Mean-Squared\nError (MSE) across all tracks in the full dataset. To account\nfor local variance resulting for a given track, we normalize\nthe range of \u0002such that \u0002max= 1 and\u0002min= 0. The\nnormalized MSE can be expressed formally as follows:\nMSE(\u0002) =1\nSSX\ni(1\u0000\u0002i)2(7)\nHere, a normalized \u0002max is always equals 1, \u0002rep-\nresents a vector of normalized criteria obtained by some\nsearch strategy, and Sis the number of songs in the Mazurka\ndata set.\nSetting the hyperparameters to P= 4 andN= 16 ,\nthe MSE of the random baseline is approximately 21%,\nwhereas our heuristic approximation is nearly two orders\nof magnitude better, achieving a MSE of slightly over 1%.\nIt is evident from this contrast that the heuristic search\nvery closely approximates the results of exhaustive com-\nputation, signiﬁcantly outperforming the random baseline.\nTherefore we offer the preliminary conclusion that the heu-ristic approach is a sufﬁcient approximation, allowing a\nmore thorough exploration over the space of hyperparam-\neters.\n5.4 Automatically Selecting Hyperparameters\nHaving gained the efﬁciency to perform a search across\nhyperparameters PandN, we can compute \u0002max for dif-\nferent combinations and deﬁne the maximum over this set\nas the optimal summary. In this experiment we explore 9\npairs ofP2[2 : 5] andN2[16 : 64] (constraining\nNto powers of two), avoiding (P;N )combinations such\nas(5;64)or(2;16)that would produce summaries that\nare too long or short, respectively. These ranges incorpo-\nrate prior musical knowledge, as there are typically a small\nnumber of distinct parts in a work and meter is predomi-\nnantly binary. It is worthwhile to mention though the best\nchoice ofPandNis signal-dependent and that, in reality,\nthere is no universally optimal combination for all music.\nIn light of this, the combination of PandNthat yields\n\u0002maxfor a given track provides another statistic that should\npersist across multiple performances of the same work, as\nstructure and meter are generally invariant to interpreta-\ntion. We evaluate the criterion further by measuring con-\nsistency of the optimal (P;N )pair using the entire Mazurka\ndataset, and provide qualitative examples of the observed\nbehavior.\n5.4.1 Quantitative Evaluation\nA consistency distribution resulting from a sweep across\ncombinations of PandNis given in Figure 2. The x-\naxis represents the proportion of performances for a given\nMazurka that produces the most frequent (P;N )pair at\n\u0002max, where a value of 1 indicates complete agreement\nand 0 complete disagreement. The y-axis represents the\nnumber of works that produce a given consistency value,\nand there are 49 in total.\nAs illustrated by the plot, there is very high consistency\n(\u001590%) for more than half of the data set, resulting in\nan average consistency of 87%. This shows that our cri-\nterion is able to capture high-level information about the\nstructure of a work across various performances, validat-\ning its capacity to produce informative audio summaries.\nDespite a high average overall, it is of special interest to\nqualitatively analyze the Mazurkas that yield different op-\ntimal conﬁgurations of the hyperparameters.\n5.4.2 Qualitative Evaluation\nImportantly, Figure 2 fails to capture is the degree of con-\ntrast between \u0002max and values for other combinations of\nPandN. Upon closer inspection, we ﬁnd that the structure\nof some works is not clearly deﬁned leading to multiple,\nequally reasonable interpretations. This manifests explic-\nitly in the data, leading to more than one (P;N )with large\n\u0002values. One such instance of multiple interpretations\noccurs for Op. 7 No. 2. The form of this work is ABCA,\nbut– depending on performance – parts BandCcan be\ninterpreted as one longer part, resulting in an ABA struc-\nture. Consequently, 62% of these performances producedFigure 2. Evaluating consistency across different perfor-\nmances of the same song for the entire Mazurka data-set\na\u0002max forP= 2, while 31% of performances occurred\natP= 3.\nThe other primary cause of inconsistency is due to tempo\nmodulations and the resulting errors and artifacts caused\nby the beat tracker. An example of this is Op. 41 No. 1,\nproducing the lowest consistency ratio of 49%. Here we\nobserve a lack of well-deﬁned onsets and liberal rhythmic\ninterpretations, both within and between performances. This\ncauses the beat tracker to behave erratically, producing mis-\naligned feature sequences that ultimately yield \u0002max val-\nues for different pairs of (P;N ).\nAlternatively, Op. 24 No. 3, which exhibits a clear ABA\nstructure and a more stable tempo, achieves 100% consis-\ntency forP= 2andN= 32. The more noteworthy obser-\nvation though is that this particular piece is in a ternary me-\nter. Therefore a better summary would likely be obtained\nwithNbeing a power of 3, and exploring other values of\nNcould potentially improve consistency.\n6. DISCUSSION & CONCLUSIONS\nWe have presented a novel audio summary criterion and\nestablished the merit of this approach through data-driven\nevaluation and qualitative inspection. We have illustrated\nhow our criterion consistently produces informative sum-\nmaries that capture both meaningful harmonic and high-\nlevel structural information. Finally, we have presented a\nheuristic approach capable of producing audio summaries\nthat closely approximates the absolute maximum.\nComplementary to the main body of work itself, the\nunexpected observation that Tonnetz features deﬁnitively\nyield better results warrants discussion. One possible ex-\nplanation, as Tonnetz features live in a continuous-valued\ngeometric space, is that any beat estimation errors result\nin a smooth interpolation of the feature space. Chroma\nfeatures, acting as a time-varying probability distribution,\ncannot resolve timing errors in the same way. As a result,\na beat tracker does not need to be perfect to be useful given\na suitable feature representation.As part of future work, we identify the potential of au-\ndio summaries to be used for various application where\nthe data needs to be time normalized. More related to this\nwork, the next logical step would be to explore the use of\nvariable length subsequences to generate summaries. Fi-\nnally, several example summaries are made available on-\nline3.\n7. ACKNOWLEDGEMENTS\nThis work was supported by the Caja Madrid Foundation\nand the National Science Foundation under grant IIS-0844654,\nand the authors would like to sincerely thank them.\n8. REFERENCES\n[1] Mark Bartsch and Gregory Wakeﬁeld. Audio thumb-\nnailing of popular music using chroma-based represen-\ntations. IEEE Transactions on Multimedia, 7(1):96–\n104, 2005.\n[2] Matthew Cooper and Jonathan Foote. Summarizing\npopular music via structural similarity analysis. In\nProc. of the IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics, pages 127–130.\nIEEE, 2003.\n[3] Peter Grosche and Meinard Muller. Extracting pre-\ndominant local pulse information from music record-\nings. Ieee Transactions On Audio Speech And Lan-\nguage Processing, 19(6):1688–1701, 2011.\n[4] Christopher Harte, Mark Sandler, and Martin Gasser.\nDetecting harmonic change in musical audio. In Pro-\nceedings of the 1st ACM workshop on Audio and mu-\nsic computing multimedia AMCMM 06, volume C,\npage 21. ACM Press, 2006.\n[5] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁcult\nchords. In Proc. of the International Society of Mu-\nsic Information Retrieval, volume 11, pages 135–140,\n2010.\n[6] Geoffroy Peeters and Xavier Rodet. Toward Automatic\nMusic Audio Summary Generation from Signal Anal-\nysis. In Proc. of the International Society of Music In-\nformation Retrieval, 2002.\n[7] Xi Shao, NC Maddage, and Changsheng Xu. Auto-\nmatic Music Summarization Based On Music Structure\nAnalysis. In Proc. of the IEEE International Confer-\nence on Acoustics Speech and Signal Processing, pages\n1169–1172. IEEE, 2005.\n3https://ﬁles.nyu.edu/onc202/public/ismir2012"
    },
    {
        "title": "Inferring Chord Sequence Meanings via Lyrics: Process and Evaluation.",
        "author": [
            "Tom O&apos;Hara",
            "Nico Schüler",
            "Yijuan Lu",
            "Dan E. Tamir"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417975",
        "url": "https://doi.org/10.5281/zenodo.1417975",
        "ee": "https://zenodo.org/records/1417975/files/OHaraSLT12.pdf",
        "abstract": "We improve upon our simple approach for learning the “associational meaning” of chord sequences from lyrics based on contingency statistics induced over a set of lyrics with chord annotations. Specifically, we refine this pro- cess by using word alignment tools developed for statis- tical machine translation, and we also use a much larger set of chord annotations. In addition, objective evaluation measures are included. Thus, this work validates a novel application of lexicon induction techniques over parallel corpora to a domain outside of natural language learning. To confirm the associations commonly attributed to major versus minor chords (i.e., happy and sad, respectively), we compare the inferred word associations against synonyms reflecting this dichotomy. To evaluate meanings associated with chord sequences, we check how often tagged chords occur in songs labeled with the same overall meaning.",
        "zenodo_id": 1417975,
        "dblp_key": "conf/ismir/OHaraSLT12",
        "keywords": [
            "associational meaning",
            "contingency statistics",
            "word alignment",
            "statistical machine translation",
            "lexicon induction",
            "parallel corpora",
            "objective evaluation measures",
            "synonyms",
            "dichotomy",
            "chord sequences"
        ],
        "content": "INFERRING CHORD SEQUENCE MEANINGS VIA LYRICS:\nPROCESS AND EVALUATION\nTom O’Hara1, Nico Sch ¨uler2, Yijuan Lu1,andDan E. Tamir1\n1Dept. of Computer Science and2School of Music, Texas State University, San Marcos, TX 78666\n{tomohara, nico.schuler, yl12, dan.tamir}@txstate.edu\nABSTRACT\nWe improve upon our simple approach for learning the\n“associational meaning” of chord sequences from lyrics\nbased on contingency statistics induced over a set of lyrics\nwith chord annotations. Speciﬁcally, we reﬁne this pro-\ncess by using word alignment tools developed for statis-\ntical machine translation, and we also use a much larger\nset of chord annotations. In addition, objective evaluation\nmeasures are included. Thus, this work validates a novel\napplication of lexicon induction techniques over parallel\ncorpora to a domain outside of natural language learning.\nTo conﬁrm the associations commonly attributed to major\nversus minor chords (i.e., happy and sad, respectively), we\ncompare the inferred word associations against synonyms\nreﬂecting this dichotomy. To evaluate meanings associated\nwith chord sequences, we check how often tagged chords\noccur in songs labeled with the same overall meaning.\n1. INTRODUCTION\nChords are the foundation of western music, providing the\nharmony for music and also inﬂuencing the melody (given\nclose relation to musical keys). Chords are not simply three\nor more notes simultaneously played but also involve pre-\ncise relationships among the notes. For example, the notes\nin a major chord consist of the root (lowest frequency),\na note a third above the root (i.e., two whole steps), and\na note a ﬁfth above the root (e.g., three whole steps and\na half). An example would be the CMaj chord, which\nconsists of the notes C,EandG. Likewise, chord se-\nquences generally have precise deﬁnitions. For example,\nthe popular 12-Bar Blues Progression commonly uses the\nfollowing scheme: /angbracketleftI, I, I, I, IV, IV, I, I, V, IV, I, I/angbracketright, where\nRoman numerals refer to chord intervals [16]. In the key\nofC, this would be as follows: /angbracketleftC, C, C, C, F, F, C, C,\nG, F, C, C/angbracketright. Given such precise relationships to musical\nintervals, meanings typically attached to chord sequences\nare unlikely to be completely arbitrary. This paper demon-\nstrates how to learn the associational meaning [8] of chord\nsequences (e.g., in terms of word associations).\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2012 International Society for Music Information Retrieval.Parallel text corpora were developed primarily to serve\nmultilingual populations but have proved invaluable for in-\nducing lexicons for machine translation [2, 5]. Similarly, a\ntype of resource intended for musicians can be exploited\nto associate meaning with music. Guitarists learning new\nsongs often rely upon tablature notation (“tabs”) provided\nby others to show the ﬁnger placement for a song measure\nby measure. Tabs often include lyrics, enabling note se-\nquences to be associated with words. They also might in-\ndicate chords as an aid to learning the sequence (as is often\ndone in scores for folk songs). In some cases, the chord an-\nnotations for lyrics are sufﬁcient for playing certain songs\n(e.g., accompaniment by guitar strumming).\nThere are several web sites with large collections of tabs\nand chord annotations for songs (e.g., about 250,000 via\nwww.chordie.com). These build upon earlier Usenet-based\nguitar forums (e.g., alt.guitar.tab). Such repositories pro-\nvide a practical means to implement unsupervised learn-\ning of the meaning of chord sequences from lyrics. As\nthese resources are willingly maintained by thousands of\nguitarists and other musicians, a system based on them can\nbe readily kept current. This paper investigates how to uti-\nlized such resources for associating meaning with chords.\nA motivation for this work comes from the context of\nsongwriting. Given lyrics one has written, the challenge is\nto come up with the structure of the accompaniment, such\nas chord sequences that might be strummed and/or a series\nof notes to be played at various points of the song. Al-\nthough the main consideration is in composing music that\nsounds good when played, it is often desirable for the mu-\nsic to convey a mood that complements the lyrics. The ap-\nproach used here could be used to suggest chord sequences\nthat might convey moods suitable for a particular set of\nlyrics. It could also aid in the reverse direction to aid song-\nwriters who proceed from melody to lyrics, but this would\nrequire elaborate natural language generation support [7]\nto produce coherent lyrics. This is a follow-up to our previ-\nous work [15], which presents a simple approach for learn-\ning the meaning of chord sequences from associated lyrics.\nThere, co-occurrence statistics are maintained over chord\nsequences and meaning tokens to determine signiﬁcant as-\nsociations. To improve the associations between chords\nand lyrics, we use tools developed for machine transla-\ntion, which rely upon word alignments discovered in par-\nallel corpora. This is not to suggest that learning mean-\ning from chords is simply a matter of “translating” chord\nsequences into text. Our hypothesis is simply that wordassociations over a large collection of lyrics with chord an-\nnotations provide an effective basis for chord meanings.\nGiven the relatively small number of chords used in prac-\ntice compared to words, this is a many-to-few type of as-\nsociation (i.e., course-grained). Text categorization can be\nused to produce more constrained associations, as done in\nour previous work [15], which is more suitable for music\nrecommendation.\nWe ﬁrst discuss related work (§2). Subsequent sections\nprovide details on the methodology (§3), an overview of\nthe data (§4), and experimentation results (§5). We con-\nclude with a summary and directions for future work (§6).\n2. BACKGROUND\nThere has been a variety of work in music information re-\ntrieval on learning the meaning of music. Most approaches\nhave used supervised classiﬁcation in which user tags serve\nas ground truth for machine learning algorithms. A few\nhave inferred the labels based on existing resources. The\napproaches differ mainly on the types of features used.\nWhitman and Ellis [20] combine audio features based on\nsignal processing with features based on signiﬁcant terms\nextracted from reviews for the album in question, thus an\nunsupervised approach relying only upon metadata about\nsongs (e.g., author and title). Turnbull et al. [19] use sim-\nilar types of audio features, but they incorporate tagged\ndata describing the song in terms of genre, instrumentality,\nmood, and other attributes. Hu et al. [6] combine word-\nlevel lyrics and audio features, using tags derived from\nsocial media, ﬁltered based on degree of affect, and then\nrevised by humans (i.e., partly supervised). McKay et al.\n[11] combine class-level lyric features (e.g., part of speech\nfrequencies and readability level) with ones extracted from\nuser tags from social media, speciﬁcally via Last.fm.1They\nalso include features for general term co-occurrence via\nweb searches for the task of genre classiﬁcation.\nThere has been other recent work in analyzing symbolic\nchord annotations. Macrae and Dixon [9] extract online\nchord annotations and show how they can be ranked ac-\ncording to sequence similarity to help ﬁlter bad annota-\ntions. McVicar et al. [13] use chord sequences from on-\nline sources to augment the task of chord recognition from\naudio via Hidden Markov Models (HMM’s). Barthet et\nal. [1] extract chord annotations to augment a guitar tutor\nprogram (e.g., to illustrate chord ﬁngering).\nLastly, there are a few approaches addressing the re-\nlations between lyrics and audio, rather than using them\nas separate features. Torres et al. [18] use a correlation-\nbased approach referred to as Canonical Correlation Anal-\nysis(CCA) to associate lyrics with audio features. Under\nthe CCA methodology, songs are represented in two fea-\nture spaces: a semantic annotation feature space and an\naudio feature space. For each space, the CCA identiﬁes\na one dimensional projection that maximizes the correla-\ntion between the projected data. The identiﬁed projections\nare used to construct and reﬁne a musically meaningful vo-\n1See http://www.last.fm.Overall process\n1. Obtain large collection of lyrics with chord annotations\n2. Extract lyrics proper with annotations from dataset\n3. Convert into tab-delimited chord annotation data format\n4. Determine best chord-word associations\nSimple approach\n4a. Fill contingency table: chord(s)/word co-occurrences\n4b. Determine signiﬁcant chord(s)/word associations\nPreferred approach\n4a. Invoke GIZA to produce chord(s)/word alignments\n4b. Filter extraneous alignments\nFigure 1. Process in learning meanings for chord se-\nquences. The meanings are via individual words; and,\nchord(s) is a single chord or a four-chord sequence.\ncabulary applied to assigning meaning to music. In addi-\ntion, they present an approach to infer the projections un-\nder the assumption that the vector spaces are sparse. More\nrecently, McVicar et al. [12] apply CCA to assess the cor-\nrelation between lyrics and audio features as a part of an\nunsupervised system for quantifying mood. The system\nexploits a special dictionary on affect, speciﬁcally with\nratings for valence (e.g., ‘pleased’ vs. ‘frustrated’) and\narousal (e.g., ‘excited’ vs. ‘sleepy’). Both approaches deal\nwith meaning at the song level, but we address the issue\nof assigning meaning to smaller units. Furthermore, rather\nthan audio features, we assign meanings to musical units\nmore commonly used in music theory (e.g., chord progres-\nsions), making the results more accessible to musicians.\nParallel corpora are vital for machine translation. Gale\nand Church [5] show how translation lexicons can be in-\nduced via co-occurrence statistics over contingency tables\nderived from such corpora. Parallel corpora have also been\nexploited to develop statistical machine translation systems,\nfollowing pioneering work by IBM [2]. This incorporates\nsophisticated statistical models to account not only for co-\noccurrence, but also word order and degree to which align-\nment with multiple words are allowed (i.e., “fertility”, which\ncan account for phrasal alignments). Och and Ney [14]\nshow that these models outperform other approaches for\nalignment (using GIZA, their implementation of them).\n3. METHODOLOGY\nFigure 1 lists the steps involved in the overall process\nfor learning the meaning of chord sequences. First, a web-\nsite for guitar instruction is downloaded to obtain a large\nsample of lyrics with chord annotations. The resulting data\nthen is passed through a ﬁlter to remove extraneous text\nassociated with the lyrics (e.g., transcriber notes). Next,\nthe data is converted into a tabular format reﬂecting the\nchord/lyrics correspondences.\nThere are two approaches for obtaining the chord/wordAlternating lines:\nC F\nThey’re gonna put me in the movies\nC G\nThey’re gonna make a big star out of me\nC\nWe’ll make a film about a man that’s sad\nF\nand lonely\nG7 C\nAnd all I have to do is act naturally\nIn-line chords:\n[C] They’re gonna put me in the [F] movies\n[C] They’re gonna make a big star out of [G]\nme\nWe’ll [C] make a film about a man that’s sad\nand [F] lonely\nAnd [G7] all I have to do is act\n[C] naturally\nFigure 2. Chord annotation sample. Lyrics are from\n“Act Naturally” by Johnny Russell, with chord annotations\nfor the song as recorded by Buck Owens.\nassociations. In the simple approach, the data is converted\ninto contingency tables from which co-occurrence statis-\ntics [10] are computed (e.g., Dice and mutual informa-\ntion). In the preferred approach (i.e., current NLP “best\npractice”), the data is formatted as a parallel corpus ﬁle\nand fed into a statistical word alignment system, such as\nGIZA. Afterwards, extraneous alignments are ﬁltered.\n3.1 Lyric Chord Annotation Data\nThe most critical resource required is a large set of lyrics\nwith chord annotations. These annotations are often spec-\niﬁed with alternative lines for chords and for the lyrics.\nThey can also be speciﬁed with chords in-line with the\nlyrics. Figure 2 shows some examples of both formats.\nThe popular website Chordie is used to obtain the data.2\nThe website is crawled, and all the songs in the chord.pere\ndirectory are extracted (other directories are for user song-\nbooks, etc.). There are over 65,000 ﬁles, but preprocessing\ncomplications reduces this to about 10,000 usable songs.\nAfter all processing, over 2 million distinct chord annota-\ntions are obtained. The chord annotation data is used as is\n(e.g., without normalization into key of C). We are working\non transposing into the key of C, but we have run into key\ndetection issues with the standard approach using key pro-\nﬁles [17]: presumably, that relies upon support from notes\nin the melody (omitted from chord annotations).\nAfter the chord-annotated lyrics are downloaded, post-\nprocessing is needed to ensure that user commentary and\nother additional material are not included. This is based\non a series of regular expressions.3The lyrics are all con-\nverted into a tabular format that more directly reﬂects the\n2See www.chordie.com; this was crawled in September of 2011.\n3The Perl code for reproducing the experiments is available at\nwww.cs.txstate.edu/%7Eto17/chord-meaning-from-lyrics.C They’re gonna put me in the\nF movies <l>\nC They’re gonna make a big star out of\nG me <l> We’ll\nC make a film about a man that’s sad and\nF lonely <l> And\nG7 all I have to do is act\nC naturally <l> <v>\nFigure 3. Sample chord annotations extracted from\nlyrics. Each chord instance in ﬁgure 2 has a separate line.\nContingency Table Cells\nX\\Y + -\n+ XY X¬Y\n- ¬XY ¬X¬YG versus ‘ﬁlm’\n+ -\n+14 231,223\n-85 1,557,047\nTable 1. Contingency tables. The left shows the general\ncase, and the right shows the data for chord G and ‘ﬁlm’.\nline-level alignment of chords and the corresponding text.\nSpeciﬁcally, this uses a tab-separated format with the cur-\nrent chord name along with words from the lyrics for which\nthe chord applies. There will be a separate line for each\nchord change in the song. Figure 3 illustrates this format.\nThis shows that special tokens are also included to indicate\nthe end of the line and verse.\n3.2 Chord Sequence Token Co-occurrence\nAs mentioned above, the simple approach to deriving word\nassociations is based on co-occurrence statistics. Several\nmetrics have been proposed to measure this [4]; for exam-\nple,Chi Square analysis determines the extent to which co-\noccurrence counts differs from that due to chance (e.g., dif-\nference of joint probability from the product of the marginals).\nGiven the tabular representation of the chord annota-\ntions with lyrics words, the next stage is to compute the co-\noccurrence statistics. This ﬁrst tabulates the contingency\ntable entry for each pair of chord and target token, as il-\nlustrated in table 1. (Alternatively, chord sequences can\nbe of length four, as discussed later. These are tabulated\nusing a sliding window over the chord annotations, as in\nn-gram analysis.) This table shows that the chord G co-\noccurred with the word ‘ﬁlm’ 14 times, out of the 231,237\ntotal instances for G. The word itself had 99 occurrences,\nand there were 1,557,047 instances where neither the word\n‘ﬁlm’ nor the chord G occurred. Next, a variety of co-\noccurrence metrics are derived using these tabulations, in-\ncluding Dice, Jaccard, mutual information, Chi square, and\nG2log likelihood [4, 10]. These are deﬁned as shown in\nﬁgure 4.\n3.3 Alignment via GIZA\nUsing the IBM models [2] for word alignment has been\nshown to outperform simple co-occurrence metrics [14].\nFor this, we use the GIZA toolkit (speciﬁcally GIZA++\nversion 2). Given its development for machine translation,Dice(X, Y ) =2×P(X= 1, Y = 1)\nP(X= 1) + P(Y= 1)Jaccard (X, Y ) =f(X= 1, Y = 1)\nf(X= 1, Y = 1) + f(X= 1, Y = 0) + f(X= 0, Y = 1)\nMI(X, Y ) =log2P(X= 1, Y = 1)\nP(X= 1)×P(Y= 1)AvgMI (X, Y ) =P\nxP\nyP(X=x, Y =y)×log2(P(X=x, Y =y))\nP(X=x)×P(Y=y)\nχ2(X, Y ) =P\ni,jP(obs[ij ]−exp[ij ])2\nexp[ij ]G2(X, Y ) = 2∗P\ni,jexp[ij ]×log(obs[ij ]\nexp[ij ])\nDice(G, film) = 0.000121; Jaccard (G, film ) = 0.000061; MI(G, film ) = 0.129199\nAvgMI (G, film ) = 0.0000001; X2(G, film ) = 0.129045; G2(G, film ) = 0.125770\nFigure 4. Common co-occurrence metrics. Using the counts shown in table 1, these statistics can be directly computed,\nresulting in the values shown for the chord Gand word ‘ﬁlm’.\nC F They’re gonna put me in the movies<l>\nC G They’re gonna make a big star ... me<l>\nC F We’ll make a film about a man that’s \\\\\nsad and lonely<l>\nF G7 C And all I have ... act naturally<l>\nFigure 5. Alternative chord annotations extracted from\nlyrics. Chords for same verse line in ﬁgure 3 are together.\nGIZA requires the speciﬁcation of the source and target\nlanguages. Most work in statistical MT treats English as\nthe source language and another language like French as\nthe target. For the experiments discussed here, the chords\nare treated as the source and the target the words (mostly\nEnglish). In our case, running the tool with the reverse di-\nrection produces negligible differences. In addition, GIZA\nnormally includes a preprocessing stage that groups tokens\nin classes based on similar usages. However, that stage is\nomitted here because there is no context with which to de-\ntermine the classes.\nIBM Model 1, the simplest one in GIZA, follows: [2]\nPθ(t, a |s) = Pθ(l|s)P θ(a|l, s )Pθ(t|a, l, s )\nwhere sis the source language, tis the target language, l\nis target sentence length, ais the alignment, and θare the\noverall parameters. The alignments are hidden and esti-\nmated via an HMM.\nPrior to using GIZA, each column is put into separate\nﬁles. Then, the toolkit preprocessing utilities convert them\ninto a combined sentence ﬁle. (To avoid problems, lines\nthat are too long or that contain garbage are discarded us-\ning the toolkit’s utility to clean the input ﬁles.) GIZA only\nrelies upon line correspondence in the two ﬁles when es-\ntablishing alignments. Figure 5 shows how the input might\nbe formatted. In addition, an optional step is used to group\nchords on the same line into sequences of four chords (e.g.,\nCDCD), which are treated as individual tokens in the\nalignment.\n4. OVERVIEW OF DATA\nBefore discussing the experiments, we present characteri-\nzations of the data involved. Naturally, lyrics are different\nfrom general English. Table 2 illustrates some differences\nin relative frequency for the top words. Comparing the two\nword frequency listings, we can see some peculiarities withGeneral Lyrics General Lyrics\nWord Freq Word Freq Word Freq Word Freq\nthe .057 i .033 with .007 is .005\nand .028 the .028 as .006 all .005\nof .027 a .028 at .005 for .005\nto .026 you .024 this .005 we .005\na .023 and .018 they .005 can .004\nin .019 to .017 be .005 but .004\nthat .013 in .011 are .005 so .004\ni .011 it .010 have .005 don .004\nit .010 me .010 we .005 re .004\nis .010 my .009 but .005 ll .004\nfor .009 of .008 his .005 d .004\nyou .008 on .007 from .004 love .004\nwas .008 that .007 not .004 no .004\nhe .007 your .006 n’t .004 she .004\non .007 be .005\nTable 2. Top words in corpus. General word frequencies\nbased\non Corpus of Contemporary American English [3],\nand word frequencies for lyrics based on Chordie.\nrespect to lyrics, such as the most common word being ‘I’\nrather than ‘the’ and that ‘you’ moves up to the top 5. The\nword ‘love’ moves up in rank dramatically (271 to 27), and\nthe word ‘your’ moves up a bit as well (from 69 to 14).\nFrequency information for common chords and for chord\nsequences is shown in table 3. This illustrates that the ma-\njor chords dominate the others, accounting for 64% of to-\ntal occurrences. The Bchord is an oddball, occurring less\nfrequently than both of the minor chords AmandBm, as\nwell as being just a little more frequent than its minor. Note\nthat the top of the sequence listing is skewed towards ma-\njor chords; minor chords do occur in about half of the se-\nquence types.\n5. EXPERIMENTS\nTwo separate groups of experiments are performed. We\nﬁrst present an evaluation of the meanings attached to in-\ndividual chords, using the common happy-versus-sad at-\ntribution regarding major versus minor chords. We also\nevaluate arbitrary chord sequences, using external annota-\ntions for songs meanings. External song-level annotationsSingle Sequence Single Sequence\nCh. Freq Seq. Freq Ch. Freq Seq. Freq\nG .154 CGCG .005 G7 .010 CFCF .003\nC .124 GCGC .005 D7 .008 DCGD .003\nD .124 EEEE .004 A7 .008 GCGD .002\nA .094 DGDG .004 E7 .007 DAGD .002\nE .068 GDGD .003 Gm .006 GCDG .002\nF .061 GDCG .003 Eb .006 AGDA .002\nAm .053 EAEA .003 Em7 .006 AEDA .002\nEm .047 DADA .003 Am7 .005 DGCG .002\nB .026 ADAD .003 Cm .005 GDAG .002\nBm .022 AEAE .003 B7 .005 CGDG .002\nDm .019 CGDC .003 C7 .005 DAED .002\nBb .015 FCFC .003 Cadd9 .004 GDEmC .002\nTable 3. Chord frequency. This shows the frequency of\nchords and sequences (i.e., 4-grams) in Chordie.\nhappy : happy, blessed, blissful, bright, golden, halcyon,\nprosperous, laughing, riant, cheerful, contented, content,\nglad, elated, euphoric, felicitous, joyful, joyous, felicitous,\nfortunate, glad, willing, well, chosen, felicitous\nsad:, sad, bittersweet, doleful, mournful, heavyhearted,\nmelancholy, melancholic, pensive, wistful, tragic, tragical,\ntragicomic, tragicomical, sorrowful, deplorable, distress-\ning, lamentable, pitiful, sorry, bad\nFigure 6. Synonyms for happy & sad. Via WordNet 2.1.\nare used in order to keep the evaluation objective, as there\nis no available resource with segment-level annotations.\n5.1 Results for individual chords\nThe ﬁrst evaluation covers the meaning attached to individ-\nual chords, such as that Cmaj is ‘bright’ whereas Cmin\nis ‘somber’). To conﬁrm the typical associations attributed\nto major versus minor chords (i.e., happy and sad, respec-\ntively), we compare the inferred word associations with\nsynonyms reﬂecting this dichotomy. Figure 6 shows the\nsynonyms for ‘happy’ and ‘sad’ from WordNet.4The idea\nis to check the most common chord associated with each\nof these synonym sets, seeing how often a major chord is\nchosen for a happy word versus a minor chord for a sad\nword.\nSpeciﬁcally, we tabulate the average metric assigned to\ntrue and false positives for major versus minor chords. Fig-\nure 7 summarizes the result. For major chords, synonyms\nfor ‘happy’ are assigned an average score of 81.1 (using\nX2), whereas synonyms for ‘sad’ are assigned an average\nscore of 39.2. Likewise, for minor chords, synonyms for\n‘sad’ have an average score of 77.7, compared to 62.1 for\n‘happy‘. As a baseline, a random value was used in place\nof the co-occurrence metric. As shown in the ﬁgure, there\nare much fewer true positives for the major chords (e.g.,\naverage scores for good versus bad nearly the same).\n4See http://wordnet.princeton.edu.Total\n186 cases with score 12541.236 (avg 67.426)\nMajor\ngood: 81 with 6573.275 (avg 81.152) (A,contented)\nbad: 35 with 1326.313 (avg 37.895) (C,wistful)\nbaseline: average scores 52.4 and 51.4, respectively\nMinor\ngood: 19 with 1476.133 (avg 77.691) (Bm,tragic)\nbad: 51 with 3165.515 (avg 62.069) (Am,bright)\nbaseline: average scores 32.9 and 43.7, respectively\nFigure 7. Evaluation of individual chord meanings.\nThis tests how well the metric decides whether synonyms\nfor ‘happy’ (‘sad’) should go with a major (minor) chord.\n5.2 Results for chord sequences\nTo evaluate the performance in learning chord sequence\nmeaning, we compare the output against the Mood Tag\nDataset (MTD) prepared by Hu et al. [6].5Table 4 lists\nthe meaning categories used in the MTD, along with the\nwords used to deﬁne the categories. For example, category\nG11 is for sincerity and is deﬁned in terms of ‘earnest’ and\n‘heartfelt’. This data set only provides song-level annota-\ntions, so we count how often the inferred chord sequence\nmeanings match the song-level meanings for all the songs\nincorporating the chord sequence. For example, if a par-\nticular song contains 10 distinct chord sequences, and if\nsix of the sequences were labeled with the meaning cate-\ngory corresponding to the song annotation, then the score\nfor the song would be 0.6. As the MTD categories are de-\nﬁned in terms of words, we check for word overlap from\nthe top words associated with a chord sequence with those\nfrom the meaning category. Although a lenient measure,\nthe word-chord alignment process being evaluated has the\nhandicap of dealing with over 10,000 meaning categories\n(i.e., all lyric words).\nTo test against the MTD, we just need the chord anno-\ntations for each of the songs covered. The annotations are\nfor speciﬁc combinations of artist and album, so the songs\nare downloaded individually via the web interface to en-\nsure the right version is used (if available). Out of 3,470\nsongs that are annotated, only 2,160 chord annotation ﬁles\nwere obtained. Songs can be labeled with more than one\ncategory. If so, when verifying whether a chord is a match,\nwe check the associated word for membership in any of the\nlists. The results are promising when using GIZA for the\nalignment using special tokens for chord sequences. The\nresulting alignment shows high precision, speciﬁcally at\n89.5% (1,779 chord sequences out of 1,987). However,\nthis comes at the expense of recall, with no suggestions\nfor many of the chord sequences. In comparison, using\naverage mutual information yields about 70,000 more tag-\ngings, but the precision drops to 20%. The baseline for\nthis is 25.9%, which is the relative frequency for the most\ncommon category (G12).\n5This dataset was used in MIREX-2011. See www.music-\nir.org/mirex/wiki/2011:Audio TagClassiﬁcation.Label Freq Examples\nG12 .259 calm, comfort, quiet, ... tranquility\nG15 .182 sad, sadness, unhappy, ..., sad song\nG5 .115 happy, happiness, ..., mood: happy\nG32 .095 romantic, romantic music\nG2 .084 upbeat, gleeful, ...\nG16 .073 depressed, blue, dark, ... gloomy\nG28 .039 anger, angry, choleric, ...\nG17 .028 grief, heartbreak, ... sorrowful\nG14 .022 dreamy\nG6 .022 cheerful, cheer up, ... sunny\nG8 .018 brooding, contemplative, ... wistful\nG29 .018 aggression, aggressive\nG25 .012 angst, anxiety, ... nervous\nG9 .009 conﬁdent, encouraging, ... optimistic\nG7 .007 desire, hope, hopeful, ...\nG11 .006 earnest, heartfelt\nG31 .006 pessimism, cynical, pessimistic, ...\nG1 .005 excitement, exciting, exhilarating, ...\nTable 4. Mood Tag Dataset. Categories for MTD along\nwith sample words used to deﬁne them. Freq gives rela-\ntive frequency, out of 6,490 total assignments.\n6. CONCLUSION\nThis paper has demonstrated how to learn the meaning of\nchord sequences from lyrics annotated with chords. Two\nseparate approaches have been illustrated. The simple ap-\nproach uses co-occurrence statistics derived from contin-\ngency tables. The preferred approach uses word alignment\ntools designed for statistical machine translation.\nFor future work, we will look into additional aspects of\nmusic as features for modeling meaning (e.g., tempo and\nnote sequences). In addition, as this approach could be\nused to suggest chord sequences that convey moods suit-\nable for a particular set of lyrics, future work will investi-\ngate its use as a songwriting aid; in fact, this was the origi-\nnal motivation for the research.\nBy using resources intended for guitarists, the current\nwork is more suitable for popular music than other types\n(e.g., classical). A long-term research goal is to develop\na framework for learning similar associations from scores\nthat include lyrics (e.g., operas). Other long-term aspects\nto be addressed include getting access to more data and\nintegrating audio analysis into the process. In principle,\nvoice recognition over lyrics could ameliorate sparse data\nproblem, provided that the natural noise in songs can be\nsufﬁciently ﬁltered.\nAcknowledgments Hupahu Ballard helped validate our experi-\nments. Scott Heftler provided useful tips on the processing. Hu\nXiao provided access to the MTD. Douglas Eck suggested the\nextension using speech recognition over audio.\n7. REFERENCES\n[1] M. Barthet, A. Anglade, G. Fazekas, S. Kolozali, and\nR. Macrae. Music recommendation for music learning: Hott-\ntabs, a multimedia guitar tutor. In Proc. Workshop on MusicRecommendation and Discovery (WOMRAD-2011), pages 7–\n13, 2011.\n[2] P. F. Brown, V. J. Pietra, S. A. D. Pietra, and R. L. Mercer. The\nmathematics of statistical machine translation: Parameter es-\ntimation. Computational Linguistics, 19:263–311, 1993.\n[3] M. Davies. The 385+ million word corpus of contemporary\nAmerican English (1990-2008+): Design, architecture, and\nlinguistic insights. International Journal of Corpus Linguis-\ntics, 14:159–90, 2009.\n[4] T. Dunning. Accurate methods for the statistics of surprise\nand coincidence. Computational Linguistics, 19(1):61–74,\n1993.\n[5] W. A. Gale and K. W. Church. Identifying word correspon-\ndences in parallel texts. In Fourth DARPA Workshop on\nSpeech and Natural Language, pages 152–157, 1991.\n[6] X. Hu, J. S. Downie, and A. F. Ehman. Lyric text mining\nin music mood classiﬁcation. In Proc. ISMIR, pages 411–6,\n2009.\n[7] D. Jurafsky and J. Martin. Speech and Language Processing.\nPrentice Hall, Upper Saddle River, New Jersey, 2000.\n[8] G. Leech. Semantics. Middlesex, Penguin Books, 1974.\n[9] R. Macrae and S. Dixon. Guitar tab mining, analysis and\nranking. In Proc. ISMIR, Miami, Florida., 2011.\n[10] C. D. Manning and H. Sch ¨utze. Foundations of Statistical\nNatural Language Processing. MIT Press, Cambridge, Mas-\nsachusetts, 1999.\n[11] C. McKay, J. A. Burgoyne, et al. Evaluating the genre classi-\nﬁcation performance of lyrical features relative to audio, sym-\nbolic and cultural features. In Proc. ISMIR, 2010.\n[12] M. McVicar, T. Freeman, and T. D. Bie. Mining the correla-\ntion between lyrical and audio features and the emergence of\nmood. In Proceedings ISMIR, 2011.\n[13] M. McVicar, Y. Ni, R. Santos-Rodriguez, and T. D. Bie.\nLeveraging noisy online databases for use in chord recogni-\ntion. In Proc. 12th International Society on Music Informa-\ntion Retrieval (ISMIR), 2011.\n[14] F. J. Och and H. Ney. A systematic comparison of vari-\nous statistical alignment models. Computational Linguistics,\n29(1):19–51, 2003.\n[15] T. O’Hara. Inferring meaning of chord sequences from lyrics.\nInProc. Workshop on Music Recommendation and Discovery\n(WOMRAD-2011), pages 40–43, 2011.\n[16] C. Schmidt-Jones and R. Jones, editors. Under-\nstanding Basic Music Theory. Connexions, 2007.\nhttp://cnx.org/content/col10363/latest.\n[17] D. Temperley. A Bayesian approach to key-ﬁnding. In\nC. Anagnostopoulou, M. Ferrand, and A. Smaill, editors,\nMusic and Artiﬁcial Intelligence, pages 195–206. Berlin,\nSpringer-Verlag, 2002.\n[18] D. Torres, D. Turnbull, L. Barrington, and G. Lanckriet. Iden-\ntifying words that are musically meaningful. In Proc. ISMIR,\nSeptember 2007.\n[19] D. Turnbull, L. Barrington, et al. Semantic annotation and\nretrieval of music and sound effects. IEEE TASLP, 16 (2),\n2008.\n[20] B. Whitman and D. Ellis. Automatic record reviews. In Proc.\nISMIR, 2004."
    },
    {
        "title": "Characterization of Embellishments in Ney Performances of Makam Music in Turkey.",
        "author": [
            "Tan Hakan Özaslan",
            "Xavier Serra",
            "Josep Lluís Arcos"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416008",
        "url": "https://doi.org/10.5281/zenodo.1416008",
        "ee": "https://zenodo.org/records/1416008/files/OzaslanSA12.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416008,
        "dblp_key": "conf/ismir/OzaslanSA12"
    },
    {
        "title": "Reuse, Remix, Repeat: the Workflows of MIR.",
        "author": [
            "Kevin R. Page",
            "Benjamin Fields",
            "David De Roure",
            "Tim Crawford",
            "J. Stephen Downie"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417299",
        "url": "https://doi.org/10.5281/zenodo.1417299",
        "ee": "https://zenodo.org/records/1417299/files/PageFRCD12.pdf",
        "abstract": "Many solutions for the reuse and remixing of MIR meth- ods and the tools implementing them have been introduced over recent years. Proposals for achieving the necessary interoperability have ranged from shared software libraries and interfaces, through common frameworks and portals, to standardised file formats and metadata. Each proposal shares the desire to reuse and combine repurposable com- ponents into assemblies (or “workflows”) that can be used in novel and possibly more ambitious ways. Reuse and remixing also have great implications for the process of MIR research. The encapsulation of any algorithm and its operation – including inputs, parameters, and outputs – is fundamental to the repeatability and reproducibility of any experiment. This is desirable both for the open and reliable evaluation of algorithms (e.g. in MIREX) and for the ad- vancement of MIR by building more effectively upon prior research. At present there is no clear best practice widely adopted throughout the community. Should this be consid- ered a failure? Are there limits to interoperability unique to MIR, and how might they be overcome? In this paper we assess contemporary MIR solutions to these issues, align- ing them with the emerging notion of Research Objects for reproducible research in other domains, and propose their adoption as a route to reuse in MIR.",
        "zenodo_id": 1417299,
        "dblp_key": "conf/ismir/PageFRCD12",
        "keywords": [
            "Reuse",
            "Remixing",
            "Interoperability",
            "Shared Software Libraries",
            "Common Frameworks",
            "Standardized File Formats",
            "Metadata",
            "Reproducibility",
            "MIR Research",
            "Research Objects"
        ],
        "content": "REUSE, REMIX, REPEAT: THE WORKFLOWS OF MIR\nKevin R. Page1Ben Fields2,3David De Roure1Tim Crawford3J. Stephen Downie4\n1Oxford e-Research Centre, University of Oxford2Musicmetric (Semetric Ltd.)\n3Department of Computing, Goldsmiths, University of London\n4Graduate School of Library and Information Sciences, University of Illinois\nABSTRACT\nMany solutions for the reuse and remixing of MIR meth-\nods and the tools implementing them have been introduced\nover recent years. Proposals for achieving the necessary\ninteroperability have ranged from shared software libraries\nand interfaces, through common frameworks and portals,\nto standardised ﬁle formats and metadata. Each proposal\nshares the desire to reuse and combine repurposable com-\nponents into assemblies (or “workﬂows”) that can be used\nin novel and possibly more ambitious ways. Reuse and\nremixing also have great implications for the process of\nMIR research. The encapsulation of any algorithm and its\noperation – including inputs, parameters, and outputs – is\nfundamental to the repeatability and reproducibility of any\nexperiment. This is desirable both for the open and reliable\nevaluation of algorithms (e.g. in MIREX) and for the ad-\nvancement of MIR by building more effectively upon prior\nresearch. At present there is no clear best practice widely\nadopted throughout the community. Should this be consid-\nered a failure? Are there limits to interoperability unique to\nMIR, and how might they be overcome? In this paper we\nassess contemporary MIR solutions to these issues, align-\ning them with the emerging notion of Research Objects for\nreproducible research in other domains, and propose their\nadoption as a route to reuse in MIR.\n1. INTRODUCTION\nThe integration of tools for Music Information Retrieval\n(MIR) into a “complete system” has been repeatedly iden-\ntiﬁed as a key – if not the grand – challenge [5, 6] for our\ncommunity. This stems from the predominance of tools\nthat are designed to solve a speciﬁc task, often developed in\ndifferent frameworks, and usually with incompatible for-\nmats for input, output, and parameters. Production of any\nmore sophisticated application that combines several tech-\nniques therefore requires either a full reimplementation and\ncombination of the constituent algorithms, potentially with-\nout source code or a sufﬁcient published description of the\nmethod, or development of a mechanism through which\nthe original tools can be reused or interoperate.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\n© 2012 International Society for Music Information Retrieval.The beneﬁts of the latter approach appear multiple and\ndesirable, that is to:\n1.realise any number of “complete systems” assembled\nfrom building block components; specialised versions of\nour tools for different music-related end-user communities.\n2.“stand on the shoulders of giants” and advance research\nby building upon and reusing prior methods and results.\n3.optimise systems through reuse of data, as well as func-\ntionality, at points of interoperability, e.g. to reuse already\ncalculated features.\n4.build distributed systems [12] through reuse of network\nexposed interoperability.\n5.reuse the mechanisms of interoperability for the pur-\nposes of transparent comparability in evaluation systems\nsuch as those undertaking MIREX.\nYet despite the steady production of frameworks and\ntoolkits over many years a de facto standard has failed to\nemerge. In this paper we assess reuse through considera-\ntion of MIR research as a data intensive scientiﬁc method,\nand assess how a selection of MIR tools might meet the\nrequirements of scientiﬁc workﬂow systems. As such it is\nnot a study of MIR capabilities or algorithms, but rather\nof the cogs and levers that together enable MIR systems to\noperate – of the effectiveness of our research processes and\nthe scalability of MIR methods and data.\n2. CHARACTERISING WORKFLOWS AND\nREUSE\nTo characterise reuse we draw on experience from the sci-\nentiﬁc workﬂow systems – tools that assist the composi-\ntion and execution of computational or data manipulation\nsteps. As a key tool for overcoming the issues of scale and\nusability associated with ad-hoc scripting when applied to\ndata-driven science, Gil [9] identiﬁes three requirements\nfor assisted workﬂow composition: workﬂows described\natdifferent levels of abstraction to support varying de-\ngrees of reuse and repeatability; expressive descriptions\nof workﬂow components describing data input and out-\nput, constraints on interactions between components (in-\nteroperability), and relationships between alternate com-\nponents; and ﬂexible workﬂow composition mechanisms\nto assist the user in construction of complete executable\nﬂows. The principles of reuse and the deployment of sci-\nentiﬁc workﬂow systems go hand-in-hand: adherence to\nthe latter encourages structured system design and interop-\nerability, providing the principled framework within which\nthe metadata and provenance required to support the for-mer can be gathered.\nBechhofer et al. [1] go on to introduce seven character-\nistics required to satisfy reuse of the data and method that\ncomprise an experimental workﬂow, capturing the motiva-\ntions raised in the previous section through the notion of\nResearch Objects: (i)reuse or redeployment as a whole or\nsingle “black box” entity; (ii)repurposable elements that\ncan be reused independently of the whole; (iii)sufﬁcient\ninformation describing data and method that the study is\nrepeatable; (iv)the repeating of an experiment to repli-\ncate a result, bringing with it the need for comparability;\n(v)replayable examination of provenance of data and re-\nsults (how they came to be); (vi)referencable and retriev-\nable versions to support unambiguous citation of results;\n(vii)revealable provenance for auditing the integrity of the\ndigitally captured data and method.\n3. REUSABILITY OF MIR SYSTEMS\nTo inform our discussion of reuse within MIR we have\nstudied many of the tools used across the community, ex-\namining publications, software documentation, and source\ncode during our evaluation. There is a wide spectrum of\npurpose and architecture between these systems and as such\ndirect implementation-level comparison becomes unwieldy\nand uninformative; rather, we make our judgement within\nthe context characterised in Section 2, i.e. primarily with\nregard to reusability, workﬂow, and for interoperability.\nWe perform our comparison through the identiﬁcation\nof what we have termed realised abstractions, summaris-\ning these for ten systems in Table 1 with further points of\ndiscussion within this Section.\nA realised abstraction can take several forms: for a soft-\nware library this might be a function or class deﬁnition,\nfor a service a remote-procedure call or ﬁle serialization,\nor on the semantic web an ontology; but it must be, in\nsome sense, a tangible resource that might be repurposed\nor called upon with or by other MIR software components.\nA realised abstraction is not synonymous with functional-\nity implemented by the software: a framework or toolset\nmight provide functionality in a manner completely practi-\ncal and appropriate for its own use cases, but which is not\nrecognised as a realised abstraction because we have been\nunable to identify a principled abstraction of the function-\nality that could be reused or that is suitable for interoper-\nability. Neither is the study intended to be comprehensive\n– it is an illustrative sample of typical practice from across\nthe community.\n3.1 Implementation and scope\nThere is signiﬁcant variety in the interaction by which a re-\nsearcher or developer will reuse the provided functionality\nof the tools and systems in Table 1.\nTheimplementation environment andlanguage have\na strong bearing on this. libXtract [3], for example, is a\nportable C library with Python and Java bindings provid-\ning feature extraction primitives, but requiring a developer\nto write the enacting skeleton of the software. jMIR [13]provides an extensible suite of components written in Java,\nwhile MIR Toolbox [11] and supporting toolboxes (Signal\nProcessing, Auditory, Netlab and SOM) are written for the\nhigh-level MATLAB numerical computing environment.\nChucK [22] is a programming language and environment\nusing a time-based concurrent model designed with com-\nputer music in mind.\nSome software provides a framework in which devel-\nopers can structure reuse and extensions of existing code.\nMarsyas (C++ with Ruby, Python, and Java bindings) pro-\nvides a comprehensive architecture for creating, manag-\ning, and visualising dataﬂows of audio, signal processing,\nand machine learning [20, 21]. sMIRk provides a toolkit\nof reusable functions for ChucK [8]. Once a developer\nhas written a V AMP plugin (in C/C++; Python bindings\navailable) it can be hosted and executed within the Sonic\nAnnotator and Sonic Visualiser applications [4] – one such\nplugin exposes functionality from libXtract. The NEMA\nsystem [23] provides a language agnostic environment lim-\nited only by the Operating System and architecture of the\nunderlying (virtual) machines: its framework uses the Me-\nandre workﬂow system for distribution and execution of\nvirtually any MIR algorithm (typically written using one of\nthe other tools described here) and a Java-based data model\nfor exchanging and consolidating inputs and outputs.\nScope of systems also varies, often depending on\nwhether a general or specialised approach has been taken,\nand if it is operated as a stand-alone platform or in con-\njunction with other tools. Weka [10], for example, is a gen-\neral purpose Java-based data-mining and machine learning\ntoolset favoured within the MIR community for its experi-\nmentation environment and range of classiﬁers. AudioDB\n[18], on the other hand, is a specialised piece of database\ninfrastructure for content-based similarity searches that re-\nlies upon the import of features extracted by other tools.\n3.2 Reusable Method\nAt a basic level any piece of software with source code (or\nindeed machine code) can be considered reusable, along\nwith the methods it embodies. In this study, we require\nmore explicit recognition and encoding of concepts. In\nthe ﬁrst section of Table 1 we look for such realised ab-\nstractions representing MIR methods that are reusable and\nrepurposable (and, for novel solutions, potentially refer-\nencable). Even when not developed for a workﬂow sys-\ntem we have also tried to identify the key characteristics of\nworkﬂow components: different levels of abstraction, and\nexplicit description of input, output, parameters, and inter-\noperability. These are, of course, the same attributes that\nenable reuse at the level of a software library or develop-\nment framework and which typically emerge from a prin-\ncipled software engineering effort to recognise the realised\nabstractions and encourage their reuse through implemen-\ntation of, for example, a documented API.\nReusable MIR methods can be broadly grouped into\nthree categories: signal processing derived feature extrac-\ntion, within which we subdivide more deterministic signal\nfeatures from less clearly deﬁned music features; metricAudioDB1\nChucK\n& sMIRk\njMIR2\nlibXtract\nM2K\n(inc. D2K)\nMarsyas\nMatlab &\ntoolboxes3\nNEMA/\nMeandre\nV AMP4\nWeka\nMETHOD\nSignal Feature Extraction\nBasic signal \u000f \u000f \u000f \u000f \u000f \u000f \u000f\nBasic maths \u000f \u000f \u000f \u000f \u000f\nBasic ﬁlters \u000f \u000f \u000f \u000f \u000f \u000f\nEnvelopes and windowing \u000f \u000f \u000f \u000f \u000f\nSpectral distribution \u000f \u000f \u000f \u000f \u000f \u000f\nError rate \u000f \u000f\nPower \u000f \u000f \u000f \u000f \u000f\nTransforms \u000f \u000f \u000f \u000f\nLinear Predictive Coding \u000f \u000f\nMFCC \u000f \u000f \u000f \u000f \u000f\nMusic Feature Extraction\nPitch \u000f \u000f \u000f \u000f\nBeat \u000f \u000f \u000f\nCorrelation and Distance\nCorrelation \u000f \u000f \u000f\nDistance \u000f \u000f \u000f\nDimensional reducers \u000f \u000f\nClassiﬁcation\nPredictive modelling \u000f \u000f \u000f \u000f\nRegression \u000f \u000f \u000f\nClustering \u000f \u000f \u000f \u000f\nAssociation Rule Learning \u000f \u000f\nWORKFLOW\nComponents \u000e \u000e \u000f \u000e \u000e \u000f \u000f\nWorkﬂows \u000e \u000e \u000f \u000e \u000e \u000f \u000f \u000f\nDATA EXCHANGE\nAbstract Signal \u000f \u000f \u000f \u000f \u000f\nSignal (values) \u000f \u000f \u000f \u000f \u000f\nAudio (playback, I/O) \u000f \u000f \u000f \u000f \u000f\nAbstract Feature \u000f \u000f \u000f \u000f \u000f\nFeature (values) \u000f \u000f \u000f \u000f \u000f \u000f\nEvents / scheduling \u000f \u000f \u000f\nAbstract Classiﬁer \u000f \u000f \u000f \u000f\nClassiﬁcation (values) \u000f \u000f \u000f\nAggregation (signal, feature) \u000f \u000e \u000f \u000f \u000f\nAnnotation \u000f \u000f \u000f\n\u000ecaveat described in Section 3.1including fftextract tool and AudioDB API library.2including jAudio, ACE, and ACE XML.\n3including MIR toolbox, Signal Processing Toolbox, Auditory toolbox, Netlab toolbox, SOM toolbox.\n4distribution including example plugins and Sonic Annotator.\nTable 1: Presence of Realised Abstractions in MIR systems and tools.\nbased correlation and distance measures; and machine-\nlearning based classiﬁcation, which broadly includes any\nmethod taking as input features or distances and outputting\nitem groupings. Coverage of these methods through re-\nalised abstractions varies widely between systems and is\noften a reﬂection of the intended scope and specialism of\nthe tool: few have comprehensive coverage beyond a core\ncompetency, while others present no specialisation and rely\non the ecosystem provided by their framework for methodimplementation, e.g. NEMA hosting of standalone algo-\nrithms, V AMP use of plugins, and toolboxes in Matlab. In\nthese latter cases it also highlights a limitation of the sur-\nvey, since including only a subset of extensions creates an\nartiﬁcial limit on methods unrepresentative of the tool’s ca-\npabilities.\nThis highlights an opportunity for interoperable and re-\nplaceable workﬂow components when considering MIR sys-\ntems as a single ecosystem, and starts to identify the group-ing of methods for which expressive descriptions (Section\n2) would be required to effect this process (a more compre-\nhensive taxonomy of features, without the ﬁlter of realised\nabstractions, can be found in [15]).\n3.3 Workﬂow\nThe second section of Table 1 appraises realised abstrac-\ntions for the constituent parts of scientiﬁc workﬂow sys-\ntems: the structure of workﬂows themselves, and the en-\ncapsulation of reusable components within them.\nSeveral of the surveyed systems adopt a workﬂow ap-\nproach in spirit: the dataﬂow and patching model at the\ncore of Marsyas, and ACE (jMIR) Coordinator and Exper-\nimenter, provide facilities for chaining and adapting func-\ntionality but are strongly tied to their respective environ-\nments and do not easily generalise (Marsyas, for example,\nis tied to a synchronous tick model). MIRtoolbox follows a\nuser centric procedural model with abstractions well suited\nto the MATLAB environment, but reﬂecting the process\na (human) MIR researcher performs, rather than one that\nmight map cleanly to a (machine-driven) workﬂow system.\nOthers tools embody more explicit examples of work-\nﬂow technique: M2K [7] and NEMA build upon exist-\ning general purpose workﬂow environments (D2K and Me-\nandre respectively) and their graphical management inter-\nfaces. However, with the exception of a genre classiﬁca-\ntion proof of concept, NEMA has not made use of work-\nﬂow components to encode a deconstructed method at the\nlevel described in the previous subsection, rather it utilises\nthe distribution and scheduling features of the workﬂow\nsystems when performing the MIREX evaluation. V AMP,\na system designed for MIR but offering many traditional\nworkﬂow system features, uses hosts such as Sonic An-\nnotator which provide a ﬂexible and extensible environ-\nment in which to compose and execute workﬂows consist-\ning of V AMP plugin components. sMIRk and ChucK are\nalso strongly workﬂow oriented, with their pervasive time-\ncentric concurrent model providing ample illustration of\nhow workﬂows can be applied across radically different\napproaches.\n3.4 Data Exchange\nRealised abstractions of speciﬁc methods and workﬂow el-\nements can identify reuse within the bounds of a common\nenvironment (e.g. particular toolkit or software library).\nFor reuse to occur between systems there must also be a\nmechanism for a mapping of method and workﬂow be-\ntween systems, performed through some process of data\nexchange. To move beyond ad-hoc workﬂows components\nmust be sufﬁciently described to support workﬂow com-\nposition. We have identiﬁed these higher-level concepts\nin the third section of Table 1 and, as in previous sub-\nsections, marked systems in which a realised abstraction\ncorrelates with the concept. The presence of a realised ab-\nstraction does not indicate an implementation of data ex-\nchange, merely that, within the software design, there is an\nexplicit abstraction of the concept which could, in theory,\nform a basis for interoperability.For Signals, Features, and Classiﬁers we highlight the\nneed to represent both the abstract concept – required for\nﬂexible workﬂow composition and the provision of generic\nmechanisms for referencable and revealable reuse – and\nthe values associated with an instance of that concept (sig-\nnal input, feature data, classiﬁer results) for repeatable and\nreplayable reuse. The conceptual recognition of events and\nscheduling is also necessary for exchange of the temporal\nsemantics often used in MIR applications. Aggregation of\nresources – be it collections of audio for analysis, com-\nputed features, or classiﬁed results – is a common require-\nment for scientiﬁc workﬂows systems and critical to sys-\ntems interoperability, reuse (of data and results), and eval-\nuation (including repeatability) in MIR. A particular facet\nof music, included here due to its common occurrence, is\nthe explicit notion of exchange or playback of audio data.\nThe level at which the abstraction is found reﬂects the\ndiffering scope of the systems: for signal libXtract uses\nnamed pointers to data structures, whereas ChucK includes\na sample primitive, and V AMP uses the Signal class from\nthe Music Ontology [17]; for feature values Marsyas writes\nout from (the somewhat overloaded) realvec, jMIR deﬁnes\na DataSet class, ChucK uses the (timesliced) unablob, while\nV AMP applies the Audio Features ontology. In all cases\nthere is, if not a full model, a principled abstraction to-\nwards one.\nAbstractions used for interoperability through serialisa-\ntion of data to either ﬁle and network are a relevant sub-\ntopic. Serialisation can raise a number of requirements\ndistinct from those considered purely for information mod-\nelling, including the reduction of parsing and transmission\n(size) overheads and the incorporation of mechanisms for\nefﬁcient error checking. Several of the systems reviewed\ndeploy abstractions designed with serialisation in mind, in-\ncluding ACE XML [14], the WEKA Attribute Relationship\nFile Format (ARFF), and to a lesser extent the Audio Fea-\ntures Ontology used by V AMP. That these serialisations\nmay not be optimal for data exchange beyond serialisation\nreinforces the need for varying levels of abstraction (Sec-\ntion 2) when building workﬂow systems – it is unlikely that\na single abstraction will be appropriate for all operations.\n4. REFLECTION\n4.1 Reusable MIR: success or failure?\nA superﬁcial glance over Table 1 might highlight a signif-\nicant level of duplication between MIR systems with an\nassociated failure of reuse. This is not a failure. It is the\nmark of a strong and vibrant community that can support\nmultiple toolkits catering to different preferences in devel-\nopment and deployment. There is no automatic beneﬁt –\nnor apparent desire – to “standardise” on a single platform,\ntoolkit, or programming language; indeed the rich variety\nof sophisticated software tailored to MIR speciﬁc problems\nindicates, if anything, the exact opposite.\nSuch a view would also overlook the successful soft-\nware reuse exempliﬁed in our study by libXtract, where a\nsmall well designed library with multi-language bindingshas been reused by tools such as ChucK and V AMP. But\nmore signiﬁcantly, this would be a mischaracterisation of\nreuse which, as we have explored, goes beyond the rede-\nployment and compatibility of source code.\n4.2 Adoption of reuse\nWhile our study has shown that no single MIR system pro-\nvides comprehensive coverage across all notions of reuse,\nit also raises plentiful opportunities for systems that share\ncommon concepts to use these as a basis for abstraction\nand interoperability. Yet ISMIR proceedings indicate lit-\ntle cross-fertilization of most systems beyond the “home”\nlab and close collaborators. An explanation for this dis-\ncrepancy might be the difference between the potential for\nreuse and the overhead of actual implementation: while we\nhave highlighted the points at which there is conceptual\nalignment between systems, any implemented interoper-\nability through the surveyed tools would require adoption\nof a software library, toolkit, or service, and the associated\ncosts of building that interface.\nAt the level of an individual researcher selecting a tool,\ninteroperability does not automatically follow reuse. The\nprevalence of Matlab – 52% of MIREX submissions in\n2011 – demonstrates the preference for a familiar envi-\nronment with a large body of basic methods, despite the\nlack of wider interoperability. Conversely, the authors of\nM2K believe the choice of Java was an unpopular one that\nlimited uptake even through the system provided a work-\nﬂow creation environment. In both cases the provision of\ninteroperability, or the lack thereof, has not provided a suf-\nﬁcient motivation to override other preferences.\nOne approach, then, might be to lower interoperability\noverheads by switching from an “all or nothing” adoption\nmodel to something more akin to “pick and choose”: se-\nlectively implementing interoperability where the beneﬁts\nare clear and well scoped. Scientiﬁc workﬂow approaches\ncan provide the principled framework to assist such con-\nversion, exempliﬁed at a technical level by the deployment\nof NEMA to run the MIREX evaluations: whilst wedded to\na single implementation, the complexity of interoperability\nhas been reduced to a single data abstraction appropriately\nselected and scoped for the evaluation and presentation of\ntask results.\nAnother promising and ﬂexible approach to reuse is the\nadoption of an agnostic modelling substrate upon which\nMIR speciﬁc abstractions can be developed. A prominent\nexample of this is the use of RDF and other Semantic Web\ntechnologies in Sonic Annotator, V AMP plugins, and the\nthe tools and ontologies they interoperate with and through.\nThe use of a modelling layer that bridges into domains be-\nyond MIR brings further beneﬁts: the common model and\ndistribution mechanism afforded by RDF and Linked Data\ncan enable reuse and exchange of related data beyond that\nproduced and consumed by the MIR system alone [16].\nThe uptake of Linked Data in industry and academia,\nincluding the scientiﬁc workﬂow and publishing commu-\nnities, provides an opportunity to reuse and adapt tools\nand software developed elsewhere for similar purpose – al-though the burden and utility of adding compatible layers\nto MIR tools should not be overlooked. Nor, given its im-\nportance, should we ignore the task of selecting and scop-\ning the appropriate level of abstraction for a model; it is not\na panacea in itself, as evidenced by the lengthy gestation of\nstandardised models such as MPEG-7.\n4.3 Workﬂow centric research\nWe have presented our review of reuse within MIR through\nthe lens of requirements originating in the scientiﬁc work-\nﬂow community. We have seen that workﬂow systems are\nexplicitly used as the basis for several MIR frameworks,\nand implicitly as an approach in others, however in both\ncases they are primarily employed for the distribution and\nscheduling of “black box” workﬂow components.\nThe increase in data driven science and the associated\nintroduction of scientiﬁc workﬂow systems has led to a re-\nﬂection on the nature of scientiﬁc method and its dissemi-\nnation in a digital world – the question of how we can open\nthese “black boxes”. The principles for reuse outlined ear-\nlier in Section 3 are also the deﬁning characteristics of a\nResearch Object [1] – a semantically rich principled ag-\ngregation of resources bringing together the essential in-\nformation relating to an experiment or investigation. This\nincludes not only the data used, the methods employed to\nproduce and analyse that data, but also the people involved\nin the investigation.\nIn our study of contemporary MIR systems we have\nsurveyed for the principles of reuse, repurposing, and re-\npeatability. While providing a foundation for data-driven\nresearch, it is when they are supplemented to encompass\nreplication, replay, referencing and revealability that we\nsee how the method and provenance captured by Workﬂow-\ncentric Research Objects [2] can radically enhance the re-\nsearch environment and process.\nBy identifying realised abstractions for method, work-\nﬂow, and data exchange in MIR systems we have demon-\nstrated that the underlying conditions for Research Objects\nin MIR are already present: one can easily imagine a future\nin which MIREX entries are developed, submitted, evalu-\nated and published as Research Objects.\n5. CONCLUSIONS\nInteroperability has not been – and should not be – achieved\nthrough the adoption of a single portal, toolkit, or program-\nming language. Plurality of systems and the different ap-\nproaches they embody is as important in avoiding skewed\nresearch and results as the plurality of datasets.\nMIR embodies a process of digital research. While work-\nﬂows provide a platform for principled reuse, they are also\nthe building blocks for Research Objects, and through these\nthe opportunity to conduct our research in new transparent,\nreusable, repurposable, and repeatable ways. In this paper\nwe have demonstrated MIR is well positioned to take ad-\nvantage of these approaches.\nWorkﬂows and Research Objects can provide a frame-\nwork, but as a community we must deﬁne the levels ofreuse and interoperability we wish to achieve through them.\nThis does not imply a single level of abstraction nor an as-\nsociated single level of modularised software, but multiple\nmodels appropriate to each task at hand. As the survey\nin this paper has shown, the basis for these encapsulations\nalready exists at different levels within MIR systems.\nAdopting a “pick and choose” approach to reuse, the\nidentiﬁcation of boundary objects [19] – points of shared\nunderstanding through standardised method and transla-\ntion between viewpoints – may prove helpful. So too can\nMIREX as a process through which the community must\nreach consensus regarding tasks and output – and where\nthe beneﬁts of reuse might be most keenly felt. In this con-\ntext we suggest a ﬁrst step should be taken at the data level:\ndescribing and exchanging input, output, and parameters\nusing community agreed vocabularies encoded in RDF.\n6. ACKNOWLEDGEMENTS\nThis work was carried out through the EPSRC funded e-Research South\nplatform grant (Grant No. EP/F05811X/1) and the Structural Analysis of\nLarge Amounts of Musical Information project funded by the JISC Digi-\ntisation and e-Content programme and the National Science Foundation\n(Grant Nos. IIS 10-42727 and IIS 09-39253). We are also grateful for the\nhelpful comments from reviewers of earlier drafts of the paper.\n7. REFERENCES\n[1] S. Bechhofer, I. Buchan, D. De Roure, P. Missier,\net al. Why linked data is not enough for scientists.\nFuture Generation Computer Systems, In press/online.\nhttp://dx.doi.org/10.1016/j.future.2011.08.004.\n[2] K. Belhajjame, O. Corcho, D. Garijo, J. Zhao, et al.\nWorkﬂow-centric research objects: First class citizens\nin scholarly discourse. In Proc. Workshop on the Se-\nmantic Publishing (SePublica), pages 1–12, 2012.\n[3] J. Bullock. Libxtract: A lightweight library for au-\ndio feature extraction. In Proc. International Computer\nMusic Conference, pages 25–28, 2007.\n[4] C. Cannam, C. Landone, M. Sandler, and J.P. Bello.\nThe sonic visualiser: A visualisation platform for se-\nmantic descriptors from musical signals. In Proc. 7th\nInternational Conference on Music Information Re-\ntrieval, pages 324–327, 2006.\n[5] M.A. Casey, R. Veltkamp, M. Goto, M. Leman, et al.\nContent-based music information retrieval: current di-\nrections and future challenges. Proc. IEEE, 96(4):668–\n696, 2008.\n[6] J.S. Downie, D. Byrd, and T. Crawford. Ten years of\nISMIR: Reﬂections on challenges and opportunities. In\nProc. 10th International Society for Music Information\nRetrieval Conference, pages 13–18, 2009.\n[7] J.S. Downie, A.F. Ehmann, and X. Hu. Music-to-\nknowledge (M2K): a prototyping and evaluation en-\nvironment for music digital library research. In Proc.\n5th ACM/IEEE Joint Conference on Digital Libraries,\npages 376–376, 2005.\n[8] R. Fiebrink, G. Wang, and P. Cook. Support for MIR\nprototyping and real-time applications in the chuckprogramming language. In Proc. 9th International\nConference of Music Information Retrieval, pages\n153–158, 2008.\n[9] Y . Gil. Workﬂow composition: Semantic representa-\ntions for ﬂexible automation. Workﬂows for e-Science,\npages 244–257, 2007.\n[10] M. Hall, E. Frank, G. Holmes, B. Pfahringer, et al.\nThe WEKA data mining software: an update. ACM\nSIGKDD Explorations Newsletter, 11(1):10–18, 2009.\n[11] O. Lartillot and P. Toiviainen. MIR in Matlab (II): A\ntoolbox for musical feature extraction from audio. In\nProc. 8th International Society of Music Information\nRetrieval Conference, pages 127–130, 2007.\n[12] D. McEnnis, C. McKay, and I. Fujinaga. Overview of\nOMEN. In Proc. International Conference on Music\nInformation Retrieval, pages 7–12, 2006.\n[13] C. McKay. Automatic music classiﬁcation with jMIR.\nPhD thesis, McGill University, 2010.\n[14] C. McKay, J.A. Burgoyne, J. Thompson, and I. Fuji-\nnaga. Using ACE XML 2.0 to store and share feature,\ninstance and class data for musical classiﬁcation. In\nProc. International Society for Music Information Re-\ntrieval Conference, pages 303–8, 2009.\n[15] D. Mitrovi ´c, M. Zeppelzauer, and C. Breiteneder. Fea-\ntures for content-based audio retrieval. Advances in\nComputers, 78:71–150, 2010.\n[16] K. R. Page, B. Fields, B .J. Nagel, G. O’Neill, et al. Se-\nmantics for music analysis through linked data: How\ncountry is my country? In Proc. IEEE Sixth Interna-\ntional Conference on e-Science, pages 41–48, 2010.\n[17] Y . Raimond, S. Abdallah, M. Sandler, and F. Giasson.\nThe music ontology. In Proc. International Conference\non Music Information Retrieval, pages 417–422, 2007.\n[18] C. Rhodes, T. Crawford, M. Casey, and M. d’Inverno.\nInvestigating music collections at different scales with\naudiodb. Journal of New Music Research, 39(4):337–\n348, 2010.\n[19] S.L. Star and J.R. Griesemer. Institutional ecology,\ntranslations and boundary objects: Amateurs and pro-\nfessionals in Berkeley’s Museum of Vertebrate Zool-\nogy, 1907-39. Social studies of science, 19(3):387–\n420, 1989.\n[20] G. Tzanetakis and P. Cook. Marsyas: A framework for\naudio analysis. Organised sound, 4(3):169–175, 1999.\n[21] G. Tzanetakis, L.G. Martins, L.F. Teixeira,\nC. Castillo, R. Jones, and M. Lagrange. Interop-\nerability and the marsyas 0.2 runtime. In Proc.\nInternational Computer Music Conference, 2008.\nhttp://hdl.handle.net/2027/spo.bbp2372.2008.149.\n[22] Ge Wang. The ChucK Audio Programming Language\nA Strongly-timed and On-the-ﬂy Environ/mentality.\nPhD thesis, Princeton University, 2008.\n[23] K. West, A. Kumar, A. Shirk, G. Zhu, et al. The net-\nworked environment for music analysis (NEMA). In\nProc. 6th IEEE World Congress on Services, pages\n314–317, 2010."
    },
    {
        "title": "Music Structure Analysis by Ridge Regression of Beat-synchronous Audio Features.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417059",
        "url": "https://doi.org/10.5281/zenodo.1417059",
        "ee": "https://zenodo.org/records/1417059/files/PanagakisK12.pdf",
        "abstract": "A novel unsupervised method for automatic music struc- ture analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations are em- ployed in order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vec- tors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate ridge regression problem, resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing the structure of the music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity ma- trix. In the same context, the combination of multiple au- dio features is investigated as well. The proposed method is referred to as ridge regression-based music structure anal- ysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset.",
        "zenodo_id": 1417059,
        "dblp_key": "conf/ismir/PanagakisK12",
        "keywords": [
            "unsupervised",
            "automatic",
            "music",
            "structure",
            "analysis",
            "audio",
            "features",
            "mel-frequency",
            "cepstral",
            "coefficients"
        ],
        "content": "MUSIC STRUCTURE ANALYSIS BY RIDGE REGRESSION OF\nBEAT-SYNCHRONOUS\nAUDIO FEATURES\nYannis Panagakis and Constantine Kotropoulos\nDepartment of Informatics\nAristotle University of Thessaloniki\nBox 451 Thessaloniki, GR-54124, Greece\n{panagakis,costas}@aiia.csd.auth.gr\nABSTRACT\nA novel unsupervised method for automatic music struc-\nture analysis is proposed. Three types of audio features,\nnamely the mel-frequency cepstral coefﬁcients, the chroma\nfeatures, and the auditory temporal modulations are em-\nployed in order to form beat-synchronous feature sequences\nmodeling the audio signal. Assume that the feature vec-\ntors from each segment lie in a subspace and the song as a\nwhole occupies the union of several subspaces. Then any\nfeature vector can be represented as a linear combination\nof the feature vectors stemming from the same subspace.\nThe coefﬁcients of such a linear combination are found by\nsolving an appropriate ridge regression problem, resulting\nto the ridge representation (RR) of the audio features. The\nRR yields an afﬁnity matrix with nonzero within-subspace\nafﬁnities and zero between-subspace ones, revealing the\nstructure of the music recording. The segmentation of the\nfeature sequence into music segments is found by applying\nthe normalized cuts algorithm to the RR-based afﬁnity ma-\ntrix. In the same context, the combination of multiple au-\ndio features is investigated as well. The proposed method\nis referred to as ridge regression-based music structure anal-\nysis (RRMSA). State-of-the-art performance is reported for\nthe RRMSA by conducting experiments on the manually\nannotated Beatles benchmark dataset.\n1. INTRODUCTION\nThe structural description of a music piece at the time scale\nof segments, such as intro, verse, chorus, bridge, etc. is re-\nferred to as the musical form of the piece [15]. Its deriva-\ntion from the audio signal is a core task in music thumb-\nnailing and summarization, chord transcription [10], learn-\ning of music semantics and music annotation [1], song seg-\nment retrieval [1], or remixing [6].\nHuman listeners analyze and segment music into mean-\ningful parts by detecting the structural boundaries between\nthe segments thanks to the perceived changes in timbre,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc⃝2012 International Society for Music Information Retrieval.tonality, and rhythm over the music piece. Music struc-\nture analysis extracts low-level feature sequences from the\naudio signal in order to model the timbral, melodic, and\nrhythmic content [15]. The segmentation of the feature\nsequences into structural parts is performed by employ-\ning methods based on either repetition, homogeneity, or\nnovelty [1, 6, 7, 12, 14, 15, 17] to analyze a recurrence plot\nor a self-similarity distance matrix. For a comprehensive\nreview on automatic music structure analysis systems the\ninterested reader is referred to [4, 15] (and the references\ntherein).\nIn this paper, a novel method for music structure anal-\nysis is proposed, which differs signiﬁcantly from the pre-\nvious methods. In particular, three types of audio features,\nnamely the mel-frequency cepstral coefﬁcients (MFCCs),\ntheChroma features, and the auditory temporal modula-\ntions (ATMs) are employed in order to form beat-synchro-\nnous feature sequences modeling the timbral, tonal, and\nrhythmic content of the music signal. It is reasonable to\nassume that due to the timbral, tonal, and rhythmic ho-\nmogeneity within the music segments, the audio features\nextracted from a speciﬁc music segment are highly cor-\nrelated and thus linearly dependent. Therefore, there is\na linear subspace that spans the beat-synchronous audio\nfeatures for each speciﬁc music segment implying that the\nsequence of feature vectors extracted from the whole mu-\nsic recording will lie in a union of as many linear sub-\nspaces as the music segments of this recording are. Ac-\ncordingly, a feature vector extracted at the time scale of\na beat can be represented as a linear combination of the\nfeature vectors stemming from the subspace it belongs to.\nFormally, one solves an appropriate inverse problem in or-\nder to obtain the representation of each feature vector with\nrespect to a dictionary, which is constructed by all the other\nfeature vectors as atoms (i.e., column vectors). Here, it\nis proved that the joint ridge representation (RR) of the\nfeatures drawn from a union of independent linear sub-\nspaces exhibits nonzero within-subspace afﬁnities and zero\nbetween-subspace afﬁnities. That is, a ridge regression1\nproblem is solved, which admits a unique and closed-form\nsolution. The segmentation of the feature sequence into\nmusic segments is revealed by applying the normalized\ncuts spectral clustering algorithm [16] to the RR-based afﬁn-\n1Ridge regression is also known as Tikhonov regularization.Figure 1 . Each music recording is modeled by three audio features, namely\nthe MFCCs, the Chroma features, and the\nATMs resulting to three beat-synchronous feature matrices. The RR is derived for each feature matrix and three afﬁnity\nmatrices are obtained as described in Section 3. A cross-feature afﬁnity matrix is obtained by linearly combining the afﬁnity\nmatrices obtained for the individual features. The segmentation of the music recording into music segments is obtained by\napplying the normalized cuts spectral clustering algorithm to the cross-feature RR-based afﬁnity matrix.\nity matrix. Provided that music segments can seldom be\nrevealed efﬁciently by resorting to a single feature, multi-\nple features are extracted from each music recording and\nthe cross-feature information is utilized in order to obtain\na reliable music segmentation. To this end, a cross-feature\nRR-based afﬁnity matrix is constructed by linearly com-\nbining the RR-based afﬁnity matrices obtained for each\nindividual feature. Again, the segmentation of the fea-\nture sequence into music segments is obtained by applying\nthe normalized cuts to the cross-feature RR-based afﬁn-\nity matrix. The proposed method is referred to as ridge\nregression-based music structure analysis (RRMSA) and\nit is outlined in Fig. 1.\nThe performance of the RRMSA is assessed by con-\nducting experiments on the manually annotated Beatles da-\ntaset. The RRMSA is demonstrated to yield a state-of-the-\nart performance.\nThe remainder of the paper is as follows. In Section 2,\nthe audio features employed are brieﬂy described. The\nRRMSA is detailed in Section 3. Dataset, evaluation met-\nrics, and experimental results are presented in Section 4.\nConclusions are drawn in Section 5.\n2. AUDIO FEATURE REPRESENTATION\nThe variations between different music segments are cap-\ntured by extracting three audio features from each monau-\nral music recording sampled at 22.05-kHz. In particular,\nthe MFCCs, the Chroma features, and the ATMs are em-\nployed.\n1)The MFCCs encode the timbral properties of the mu-\nsic signal by parameterizing the rough shape of spectral\nenvelope. Following [14], the MFCC calculation employs\nframes of duration 92.9ms with a hop size of 46.45 msand a 42-band ﬁlter bank. The correlation between the fre-\nquency bands is reduced by applying the discrete cosine\ntransform along the log-energies of the bands. The zeroth\norder coefﬁcient is discarded yielding a sequence of 12-\ndimensional MFCCs vectors.\n2)The Chroma features are able to characterize the har-\nmonic content of the music signal by projecting the entire\nspectrum onto 12bins representing the 12distinct semi-\ntones (or chroma) of a musical octave. They are calculated\nby employing 92.9 ms frames with a hope size of 23.22\nms as follows. First, the salience of different fundamental\nfrequencies in the range 80−640Hz is calculated. The\nlinear frequency scale is transformed into a musical one by\nselecting the maximum salience value in each frequency\nrange corresponding to one semitone. Finally, the octave\nequivalence classes are summed over the whole pitch range\nto yield a sequence of 12-dimensional chroma vectors.\n3)The ATMs carry important time-varying information\nof the audio signal [11]. They are obtained by modeling the\npath of human auditory processing as a two-stage process.\nIn the ﬁrst stage, which models the early auditory system,\nthe acoustic signal is converted into a time-frequency dis-\ntribution along a logarithmic frequency axis, the so-called\nauditory spectrogram . The early auditory system is mod-\neled by Lyons’ passive ear model [9] employing 96fre-\nquency channels ranging from 62Hz to 11kHz. The au-\nditory spectrogram is then downsampled along the time\naxis in order to obtain 10feature vectors between two suc-\ncessive beats. The underlying temporal modulations of\nthe music signal are derived by applying a biorthogonal\nwavelet ﬁlter along each temporal row of the auditory spec-\ntrogram, where its mean has been previously subtracted,\nfor8discrete rates r∈ {2, 4,8,16,32,64,128,256}Hz\nranging from slow to fast temporal rates. Thus, the entireauditory spectrogram is modeled by a three-dimensional\nrepresentation of frequency, rate, and time which is then\nunfolded2along the time-mode in order to obtain a se-\nquence of 96×8 = 728 -dimensional ATMs.\nPostprocessing. Sequences of beat-synchronous feature\nvectors are obtained by averaging the feature vectors over\nthe beat frames. The latter are found by using the beat\ntracking algorithm described in [5]. Each row of the beat-\nsynchronous feature matrix is ﬁltered by applying an aver-\nage ﬁlter of length 8. Finally, each feature vector under-\ngoes a normalization in order to have zero-mean and unit\nℓ2norm.\n3. MUSIC STRUCTURE ANALYSIS BASED ON\nRIDGE REGRESSION\nIn this section, the RRMSA is detailed. Let a given mu-\nsic recording of Kmusic segments be represented by a\nsequence of Nbeat-synchronous audio feature vectors of\nsized, i.e., X= [x1|x2|. . .|xN]∈Rd×N. The per-\nceived timbral, tonal, and rhythmic homogeneity within\na music segment implies that the audio features extracted\nfrom this music segment are highly correlated, exhibiting\nlinear dependence. This motivated us to assume that beat-\nsynchronous feature vectors belonging to the same mu-\nsic segment live into the same subspace. Therefore, if\na music recording consists of Kmusic segments, the se-\nquence of Nbeat-synchronous audio feature vectors (i.e.,\nthe columns of X) are drawn from a union of Kindepen-\ndent linear subspaces of unknown dimensions. Thus, each\nfeature vector can be represented as a linear combination\nof feature vectors drawn from the same subspace. That is,\nX=XZ, where Z∈RN×Nis the representation matrix,\nwhich contains the linear combination coefﬁcients in its\ncolumns3. Clearly, zij= 0, ifxiandxjlie on different\nsubspaces and nonzero otherwise.\nSuch a representation matrix Zcan be found by solving\na least-squares problem regularized by the Frobenius norm\n(denoted by ∥.∥F), the so-called ridge regression problem:\nargmin\nZ∥X−X Z∥2\nF+λ∥Z∥2\nF. (1)\nThe unique solution of the unconstrained convex problem\n(1) is referred to as ridge representation (RR) matrix and it\nis given in closed-form by:\nZ= (XTX+λI)−1XTX. (2)\nTechnically, the desired property of the RR matrix to ad-\nmit nonzero entries for within-subspace afﬁnities and zero\nentries for between-subspace afﬁnities is enforced by the\nregularization term λ∥Z∥2\nFin (1) as proved in Theorem 1,\nwhich is a consequence of Lemma 1. This result indicates\nthat if the data follow subspace structures (i.e., come from\n2The tensor unfolding can be implemented in Matlab by em-\nploying the tenmat function of\nthe MATLAB Tensor Tool-\nbox available at: http://csmr.ca.sandia.gov/ ˜tgkolda/\nTensorToolbox/.\n3Due to the assumptions stated at the beginning of Section 3, the ma-\ntrixXdoes not has full column rank. Therefore, X=XZ does not\nadmit the identity matrix as solution.a union of independent subspaces), the correct identiﬁca-\ntion of the subspaces can be obtained accurately, fast, and\nin closed form by solving (1) without imposing sparsity or\nother constraints on the data model.\nLemma 1 [2]. For any four matrices B,C,D, and Fof\ncompatible dimensions,\n\r\r\r\r[B C\nD F]\r\r\r\r2\nF≥\r\r\r\r[B 0\n0 F]\r\r\r\r2\nF=∥B∥2\nF+∥F∥2\nF.\n(3)\nTheorem 1. Assume the columns of X(i.e., the fea-\nture vectors) are drawn from a union of Klinear indepen-\ndent subspaces of unknown dimensions. Without loss of\ngenerality, one may decompose Xas[X1|X2|. . .|XK]∈\nRd×N, where the columns of Xk∈Rd×Nk,k= 1,2, . . . , K\ncorrespond to the Nkfeature vectors originating from the\nkth subspace. The minimizer of (1) is block-diagonal.\nThe proof can be found in the Appendix.\nLetZ=U\u0006VTbe the singular value decomposition\n(SVD) of Z. Set ~U=U(\u0006)1\n2andM=~U~UT. A RR-\nbased nonnegative symmetric afﬁnity matrix W∈RN×N\n+\nhas\nelements [8]:\nwij=m2\nij. (4)\nThe segmentation of the columns of XintoKclusters (i.e.,\nmusic segments) is performed by employing the normal-\nized cuts [16] onto the RR-based afﬁnity matrix W.\nSince the music segments cannot be accurately derived\nby resorting to one feature, cross-feature information is ex-\npected to produce a more reliable music segmentation. Let\nWm,Wc, andWabe the RR-based afﬁnity matrix ob-\ntained, when the MFCCs, the Chroma, and the ATMs are\nemployed, respectively. A cross-feature RR-based afﬁnity\nmatrixWcf∈RN×N\n+ can be constructed by:\nWcf= 1/3(W m+Wc+Wa), (5)\nor any other combination of the individual afﬁnity matri-\nces. The segmentation of the music recording can be ob-\ntained by applying the normalized cuts [16] to the cross-\nfeature RR-based afﬁnity matrix Wcf.\nIn general, the number of segments Kin a music record-\ning is unknown and thus it is reasonable to be estimated. To\nthis end, the soft-thresholding approach is employed [8].\nThat is, the estimated number of segments \u0016Kis found by:\n\u0016K=N−int(N∑\ni=1f\u001c(σi)). (6)\nThe function int (.)returns the nearest integer of a real num-\nber,{σi}N\ni=1denotes the set of the singular values of the\nLaplacian matrix derived by the corresponding afﬁnity ma-\ntrix, and f\u001cis the soft-thresholding operator deﬁned as\nf\u001c(σ) = 1 ifσ≥τand log2(1 +\u001b2\n\u001c2), otherwise. Clearly,\nthe threshold τ∈(0,1).4. EXPERIMENTAL EVALUATION\n4.1 Dataset, Evaluation Procedure,and Evaluation\nMetrics\nBeatles dataset4: The dataset consists of 180songs by\nThe Beatles. The songs are annotated by the musicol-\nogist Alan W. Pollack. Segmentation time stamps were\ninserted at Universitat Pompeu Fabra (UPF). Each music\nrecording contains on average 10segments from 5unique\nclasses [17].\nThe structure segmentation is obtained by applying the\nRRMSA to each individual feature sequence as well as to\nall possible feature combinations. In Fig. 2, sample RR-\nbased afﬁnity matrices are depicted. Two sets of experi-\nments were conducted on the Beatles dataset. First, follow-\ning the experimental setup employed in [1,3,6,7,12,14,17],\nthe number of clusters (i.e., segments) Kwas kept constant\nand equal to 4. Second, for each music recording, the num-\nber of segments was estimated by (6). The optimal values\nof the various parameters (i.e., λ,τ) were determined by a\ngrid search over 10randomly selected music recordings of\nthe dataset.\nIn order to compare fairly the RRMSA with the state-\nof-the-art music structure analysis methods, the segment\nlabels are evaluated by employing the pairwise F-measure\nas in [3,6,7,12,14,17]. The pairwise F-measure is a stan-\ndard evaluation metric for clustering algorithms. It is de-\nﬁned as the harmonic mean of the pairwise precision and\nrecall. The segmentation results and the reference segmen-\ntation (i.e., the ground truth) are handled at the time scale\nof beats. Let FAbe the set of identically labeled pairs of\nbeats in a recording according to the music structure analy-\nsis algorithm and FHbe the set of identically labeled pairs\nin the human reference segmentation. The pairwise pre-\ncision, PP, the pairwise recall, PR, and the pairwise F-\nmeasure, PF, are deﬁned as:\nPP=|FA∩FH|\n|FA|, (7)\nPR=|FA∩FH|\n|FH|, (8)\nPF= 2·PP·PR\nPP+PR, (9)\nwhere |.|denotes the set cardinality.\n4.2 Experimental Results\nThe segment-type\nlabeling performance of the RRMSA on\nthe Beatles dataset is summarized in Table 1 for a ﬁxed\nnumber of segments (i.e., K= 4) as in [3, 6, 7, 12, 14,\n17]. By inspecting Table 1, one can see that the ATMs are\nmore suitable for music structure analysis than the MFCCs\nand the Chroma features. Furthermore, the latter two fea-\ntures lead to an undesirable over-segmentation of the mu-\nsic recordings. Similar ﬁndings were reported in [12]. The\nbest result reported for segment-type labeling on the Bea-\ntles dataset is obtained here, when the RR-based afﬁnity\n4http://www.dtic.upf.edu/ perfe/annotations/sections/license.htmlmatrices of the MFCCs and the ATMs are combined. In-\nterestingly to note that by employing cross-feature afﬁnity\nmatrices the average number of segments approaches 10\n(i.e., the actual average number of segments according to\nthe ground-truth), although no constraints have been en-\nforced during clustering. In addition to the very promising\nperformance of the RRMSA with respect to PF, it is worth\nmentioning that the RRMSA is fast. The average CPU time\nfor the calculation of the RR-based afﬁnity matrix is 0.858\nCPU seconds.\nFeatures Parameters PF Segments\nMFCCs (λ= 0.3) 0.54 37.1\nChroma (λ= 0.1) 0.57 36.7\nATMs (λ= 0.1) 0.61 6.1\nMFCCs & Chroma (λ= 0.3, 0.1) 0.55 20.6\nMFCCs & ATMs (λ= 0.3, 0.1) 0.63 7.1\nChroma & ATMs (λ= 0.1, 0.1) 0.60 8.1\nMFCCs & Chroma & ATMs (λ= 0.3, 0.1,0.1) 0.61 8.8\nTable 1. Segment-type labeling performance of the\nRRMSA on the Beatles dataset\nwith ﬁxed K= 4.\nThe best result obtained by the RRMSA on the Beatles\ndataset with respect to PF(i.e., 0.63) outperforms the re-\nsults obtained by the majority of the state-of-the-art music\nsegmentation methods listed in Table 2 on the same dataset\npreviously. The results were rounded down to the nearest\nsecond decimal digit. It is seen that the RRMSA admits\nthe highest PFwhen the MFCCs and the ATMs are com-\nbined. Similarly, MFCCs combined with Chroma yielded\nthe top PFin [3] and [6]. Similar conclusions were drawn\nin [13], when multiple audio features were combined. It\nis worth mentioning that the RRMSA does not involve any\npostprocessing based on music knowledge, such as elimi-\nnating too short segments or restricting the segment length\nto improve the accuracy of music segmentation. This is not\nthe case for the methods in [7] and [14]. Furthermore, the\nRRMSA involves only one parameter in contrast to meth-\nods [3, 7, 14, 17], where the tuning of multiple parameters\nis needed.\nThe segment-type labeling performance of the RRMSA\non the Beatles dataset, when Kis estimated by (6), is re-\nported in Table 3. Again, the use of the ATMs for music\nrepresentation makes the RRMSA to achieve better per-\nformance than that when either the MFCCs or the Chroma\nfeatures are used. By combining the ATMs and the MFCCs,\nthePFfor the RRMSA reaches 0.60. In this case, the es-\ntimated average number of segments equals the actual av-\nRefer ence Features PF\n[3] MFCCs & Chroma 0.63\n[6] MFCCs & Chroma 0.62\n[17] Chroma 0.60\n[14] MFCCs 0.60\n[12] ATMs 0.59\nMethod in [7] as evaluated in [14] MPEG-7 0.58\nTable 2 . Segment-type labeling performance on the Beat-\nles dataset obtained by\nstate-of-the-art methods with ﬁxed\nK= 4.Figure 2. RR-based afﬁnity and self-distance matrices of beat-synchronous feature\nvectors extracted from the Anna (Go\nto Him) by The Beatles when employing the MFCCs (a) and (d), the Chroma features (b) and (e), or the ATMs (c) and (f).\nThe negative image of the afﬁnity matrices is depicted. It is obvious that RR-based afﬁnity matrices provide more clear and\nnoise-free structural information than the self-distance matrices for all features.\nerage number of segments according to the ground-truth\n(i.e., 10). This result indicates that it is possible to perform\na robust unsupervised music structure analysis in a fully\nautomatic setting.\nFurther details related to the estimation of Kby em-\nploying various audio features and their combinations are\nshown in Table 4. The absolute error is deﬁned as |\u0016K−\nKg|, where Kgis the actual number of segments based\non the ground-truth. The prediction rate refers to the ra-\ntio of the number of music recordings where the number\nof segments was predicted correctly over the total num-\nber of music recordings in the dataset. If we consider the\nvalue \u0016K=Kg±1as the correct number of predicted seg-\nments, then we obtain the Proximal Prediction Rate (PPR)\n(i.e., the last column in Table 4). The results presented in\nTable 4 indicate that the combination of the MFCCs and\nthe ATMs yields the lowest absolute error, resulting to the\nhighest prediction rate and thus the highest segmentation\naccuracy.\nFeatures Parameters PF Segments\nMFCCs (λ= 0.3, τ = 0.7) 0.54 24.9\nChroma (λ= 0.1, τ = 0.64) 0.48 26.6\nATMs (λ= 0.1, τ = 0.64) 0.59 6.4\nMFCCs + Chroma (λ= 0.3, 0.1, τ = 0.7) 0.59 12.2\nMFCCs & ATMs (λ= 0.3, 0.1, τ = 0.23) 0.60 10.0\nChroma & ATMs (λ= 0.3, 0.1, τ = 0.27) 0.56 12.8\nMFCCs & Chroma & ATMs (λ= 0.3, 0.1, τ = 0.33) 0.53 20.0\nTable 3. Segment-type labeling performance of the\nRRMSA on the Beatles\ndataset for automatically estimated\nK.Features Absolute Error Prediction Rate (%) PPR (%)\nMFCCs 1.24 25.26 65.59\nChroma 1.88 15.59 42.47\nATMs 1.72 18.81 52.68\nMFCCs & Chroma 1.88 15.60 42.47\nMFCCs & ATMs 1.15 30.10 73.11\nChroma& ATMs 1.43 22.58 61.82\nMFCCs & Chroma & ATMs 1.22 26.34 67.20\nTable 4 . Accuracy of the estimation of the number of se\ng-\nments, K, on the Beatles dataset.\n5. CONCLUSIONS\nIn this paper, a robust and fast method for music structure\nanalysis (i.e., the RRMSA) has been proposed. In particu-\nlar, the ridge regression representation of the MFCCs, the\nChroma, and the ATMs have been used to derive afﬁnity\nmatrices, where the normalized cuts algorithm has been\napplied to obtain the music structure. Among the three\nfeatures, the ATMs and the MFCCs have been proved the\nmost powerful. By linearly combining the RR-based afﬁn-\nity matrices of the MFCCs and the ATMs and applying\nnext the normalized cuts, state-of-the-art performance on\nthe Beatles dataset has been reported for a ﬁxed number\nof segments. Furthermore, an accurate method to estimate\nthe number of segments in each music recording has been\ndeveloped, enabling a fully automatic unsupervised music\nstructure analysis.APPENDIX: PROOF OF THEOREM 1\nLet us denote by {S1,S2, . . .\n,SK}, a collection of K\nindependent subspaces. The direct sum of a collection of\nKsubspaces is denoted by ⊕K\nk=1Sk. LetZbe the unique\nminimizer of (1) and Dbe a block-diagonal matrix with\nelements dij=zij, ifxiandxjbelong to the same sub-\nspace (i.e., music segment here), and dij= 0 otherwise.\nWe can deﬁne Q=Z−D. Without loss of general-\nity let us suppose that xjbelongs to the ith subspace, i.e.,\nxj= [XZ] j∈Si. We can write Qas the sum of two ma-\ntricesQ1andQ2whose supports are on disjoint subsets of\nindices, such that [XQ 1]j∈Siand[XQ 2]j∈ ⊕K\nk̸=iSk.\nWe show that Q2=0. For the sake of contradiction, we\nassume that Q2̸=0. Since Z=D+Q1+Q2, we have\nxj= [XZ ]j= [X(D +Q1)]j+ [XQ 2]j. Since xj∈Si\nand[X(D +Q1)]j∈Si, by the independence of sub-\nspaces,Si∩⊕K\nk̸=iSk={0}, we should have [XQ 2]j=0.\nBut[XQ 2]j=0implies, xj= [XZ ]j= [X(D +Q1)]j\nand hence D+Q1is feasible solution of (1). By the fact\nthat the supports of Q1andQ2are disjoint subsets of in-\ndices and Lemma 1, ∥D+Q1∥2\nF≤ ∥D+Q1+Q2∥2\nF=\n∥Z∥2\nF. That is D+Q1, is a feasible solution of (1) at-\ntaining a smaller Frobenius norm than ∥Z∥2\nF, which con-\ntradicts the optimality of Z. Thus, Q2=0, meaning that\nonly the blocks that correspond to vectors in the true sub-\nspaces are nonzero.\nAcknowledgements\nThis research has been co-ﬁnanced by the European Union\n(European Social Fund - ESF) and Greek national funds\nthrough the Operational Program “Education and Lifelong\nLearning” of the National Strategic Reference Framework\n(NSRF) - Research Funding Program: Heraclitus II. In-\nvesting in Knowledge Society through the European Social\nFund.\n6. REFERENCES\n[1] L. Barrington, A. Chan, and G. Lanckriet. Model-\ning music as a dynamic texture. IEEE Trans. Au-\ndio, Speech, and Language Processing , 18(3):602–\n612, 2010.\n[2] R. Bhatia and F. Kittaneh. Norm inequalities for par-\ntitioned operators and an application. Math. Ann.,\n287(1):719–726, 1990.\n[3] R. Chen and L. Ming. Music structural segmenta-\ntion by combining harmonic and timbral information.\nInProc. 12th Int. Conf. Music Information Retrieval ,\npages 477–482, Miami, USA, 2011.\n[4] R. B. Dannenberg and M. Goto. Music structure analy-\nsis from acoustic signals. In D. Havelock, S. Kuwano,\nand M. Vorl ¨ander, editors, Handbook of Signal Pro-\ncessing in Acoustics , pages 305–331. Springer, New\nYork, N.Y., USA, 2008.[5] D. Ellis. Beat tracking by dynamic programming. J.\nNew Music Research , 36(1):51–60, 2007.\n[6] F. Kaiser and T. Sikora. Music structure discovery in\npopular music using non-negative matrix factorization.\nInProc. 11th Int. Conf. Music Information Retrieval ,\npages 429–434, Utrecht, The Netherlands, 2010.\n[7] M. Levy and M. Sandler. Structural segmentation of\nmusical audio by constrained clustering. IEEE Trans.\nAudio, Speech, and Language Processing , 16(2):318–\n326, 2008.\n[8] G. Liu, Z. Lin, S. Yan, J. Sun, and Y. Ma. Robust re-\ncovery of subspace structures by low-rank representa-\ntion. IEEE Trans. Pattern Analysis and Machine Intel-\nligence, 2011. arXiv:1010.2955v4 (preprint).\n[9] R. Lyon. A computational model of ﬁltering, detection,\nand compression in the cochlea. In IEEE Int. Conf.\nAcoustics, Speech, and Signal Processing , pages 1282–\n1285, Paris, France, 1982.\n[10] M. Mauch, K. Noland, and S. Dixon. Using musi-\ncal structure to enhance automatic chord transcription.\nInProc. 10th Int. Conf. Music Information Retrieval ,\npages 231–236, Kobe, Japan, 2009.\n[11] Y. Panagakis, C. Kotropoulos, and G. R. Arce. Non-\nnegative multilinear principal component analysis of\nauditory temporal modulations for music genre clas-\nsiﬁcation. IEEE Trans. Audio, Speech, and Language\nTechnology , 18(3):576–588, 2010.\n[12] Y. Panagakis, C. Kotropoulos, and G. R. Arce. l1-graph\nbased music structure analysis. In Proc. 12th Int. Conf.\nMusic Information Retrieval , pages 495–500, Miami,\nUSA, 2011.\n[13] J. Paulus and A. Klapuri. Acoustic features for music\npiece structure analysis. In Proc. 11th Int. Conf. Digital\nAudio Effects , pages 309–312, Espoo, Finland, 2008.\n[14] J. Paulus and A. Klapuri. Music structure analysis us-\ning a probabilistic ﬁtness measure and a greedy search\nalgorithm. IEEE Trans. Audio, Speech, and Language\nProcessing , 17(6):1159–1170, 2009.\n[15] J. Paulus, M. M ¨uller, and A. Klapuri. Audio-based mu-\nsic structure analysis. In Proc. 11th Int. Conf. Music\nInformation Retrieval, pages 625–636, Utrecht, The\nNetherlands, 2010.\n[16] J. Shi and J. Malik. Normalized cuts and image seg-\nmentation. IEEE Trans. Pattern Analysis and Machine\nIntelligence , 22(8):888–905, 2000.\n[17] R. Weiss and J. Bello. Identifying repeated patterns\nin music using sparse convolutive non-negative matrix\nfactorization. In Proc. 11th Int. Conf. Music Informa-\ntion Retrieval, pages 123–128, Utrecht, The Nether-\nlands, 2010."
    },
    {
        "title": "Modeling Chord and Key Structure with Markov Logic.",
        "author": [
            "Hélène Papadopoulos",
            "George Tzanetakis"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416724",
        "url": "https://doi.org/10.5281/zenodo.1416724",
        "ee": "https://zenodo.org/records/1416724/files/PapadopoulosT12.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416724,
        "dblp_key": "conf/ismir/PapadopoulosT12"
    },
    {
        "title": "A Multimedia Search and Navigation Prototype, Including Music and Video-clips.",
        "author": [
            "Geoffroy Peeters",
            "Frédéric Cornu",
            "Christophe Charbuillet",
            "Damien Tardieu",
            "Juan José Burred",
            "Marie Vian",
            "Valérie Botherel",
            "Jean-Bernard Rault",
            "Jean-Philippe Cabanal"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417761",
        "url": "https://doi.org/10.5281/zenodo.1417761",
        "ee": "https://zenodo.org/records/1417761/files/PeetersCCTBVBRC12.pdf",
        "abstract": "Moving music indexing technologies developed in a re- search lab to their integration and use in the context of a third-party search and navigation engine that indexes music files, archives of TV music programs and video- clips, involves a set of choices and works that we re- late here. First one has to choose technologies that per- form well, which are scalable (in terms of computation time of extraction and item comparison for search-by- similarity), and which are not sensitive to media quality (being able to process equally music files or audio tracks from video archives). These technologies must be applied to estimate tags chosen to be understandable and useful for users (the specific genre and mood tags or other content- descriptions). For training the related technologies, rele- vant and reliable annotated corpus must be created. For using them, relevant user-scenarios must be created and friendly Graphical User-Interface designed. In this paper, we share the experience we had in a recent project on inte- grating six state-of-the-art music-indexing technologies in a multimedia search and navigation prototype.",
        "zenodo_id": 1417761,
        "dblp_key": "conf/ismir/PeetersCCTBVBRC12",
        "keywords": [
            "Music indexing",
            "Search by similarity",
            "Media quality",
            "User-friendly interface",
            "Annotated corpus",
            "Training technologies",
            "Scalability",
            "Relevance",
            "Reliability",
            "Multimedia search"
        ],
        "content": "A MULTIMEDIA SEARCH AND NA VIGATION PROTOTYPE, INCLUDING\nMUSIC AND VIDEO-CLIPS\nG. Peeters, F. Cornu\nCh. Charbuillet, D. Tardieu, J.J. Burred\nSTMS IRCAM-CNRS-UPMC\nfpeeters,cornug@ircam.frM. Vian1, V . Botherel2\nJ.-B. Rault2and J.-Ph. Cabanal2\n1Bertin Technologies,2Orange-Labs\njeanphilippe.cabanal@orange.com\nABSTRACT\nMoving music indexing technologies developed in a re-\nsearch lab to their integration and use in the context of\na third-party search and navigation engine that indexes\nmusic ﬁles, archives of TV music programs and video-\nclips, involves a set of choices and works that we re-\nlate here. First one has to choose technologies that per-\nform well, which are scalable (in terms of computation\ntime of extraction and item comparison for search-by-\nsimilarity), and which are not sensitive to media quality\n(being able to process equally music ﬁles or audio tracks\nfrom video archives). These technologies must be applied\nto estimate tags chosen to be understandable and useful for\nusers (the speciﬁc genre and mood tags or other content-\ndescriptions). For training the related technologies, rele-\nvant and reliable annotated corpus must be created. For\nusing them, relevant user-scenarios must be created and\nfriendly Graphical User-Interface designed. In this paper,\nwe share the experience we had in a recent project on inte-\ngrating six state-of-the-art music-indexing technologies in\na multimedia search and navigation prototype.\n1. INTRODUCTION\nThe objective of the MSSE project (Multimedia Search\nServices for European Portals) is to develop a multimedia\nsearch and navigation prototype, which gives access to sev-\neral types of contents (catch-up TV , archives, videos, mu-\nsic) and which illustrates the beneﬁts of advanced audio-\nvideo analysis technologies. The prototype is organized\naround three use-cases:\n\u000fSearching recently broadcasted TV programs\n(“Catch-up TV”); navigating inside the videos by\nchapters or keywords.\n\u000fSearching video extracts on a speciﬁc topic related\nto recent news and culture; browsing in relevant\ntranslated foreign videos and in public TV archives.\n\u000fSearching and exploring music pieces with the help\nof tags, music structure, summaries and similarity.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.The prototype is based on video indexing, speech recog-\nnition and music indexing technologies. In this paper, we\ndescribe the works performed for the music indexing tech-\nnologies. Those have to deal with three types of content:\n\u000fA music collection\n\u000fA collection of video clips from the W9 TV channel\n\u000fA collection of video archives of music programs\nfrom the INA1collection. (only the audio track of\nthe video is processed by our indexing modules).\nIn this paper, we propose to review the technologies\nintegrated into this search and navigation prototype, why\nthey were chosen and how they were developed and inte-\ngrated as well as the corresponding user-evaluations and\nGUI developed. We believe that sharing the experience of\nthis work could provide a good example of integration of\nresearch modules in a real application scenario.\nWhile many papers have been published on the inde-\npendent elements this paper deals with (content-based, se-\nmantic tags, corpus creation, GUI, user-tests), few of them\ndeal with all these elements as a whole to create a sys-\ntem. Among exceptions are the works made for the Mu-\nsic Browser [1], FM4-Soundpark [2], Musicream [3], Mu-\nsicBox [4] or PlaySOM [5]. Our work differs from the\nprevious in the number of integrated technologies, the in-\ntegration into a whole video and music search engine ac-\ncessible through a web-browser and the simplicity of the\nGUI.\n2. OVERALL DESIGN PROCESS\nFigure 1 represents the various elements of work (and in-\nteraction/dependency between them), needed for integrat-\ning the music technologies in the prototype.\nThe starting point is a set of requirements from the\nthird-party developer and its users2\u0000in terms of func-\ntionalities (such as searching-by/ ﬁltering-by tags, search-\nby-similarity or summarization) and \u0000in terms of types of\ncontent-description (genre, mood, instrumentation).\nFrom this, a set of potential technologies are studied in\nterms of performances and scalability3. Candidate tech-\nnologies are tested over the years in internal benchmark-\ning or in public ones such as MIREX. For example, from\n1French National Audio Visual Archiving Institution\n2During the project, 1 or 2 user tests per year were performed, each\ncorresponding to a new version of the prototype (see part 6).\n3By scalability we mean computation time of content-extraction and\nof items comparison for search-by-similarity.Existing \nTechnologies\nPerformances ? Scalability ?\nSelection\nUsers/ Application \nRequirementsFunctionalities ?Types of Content \nDescription ?Creation of \nAnnotated Corpus GUI/HMI designTechnology \n(code optimization)\nTraining\nQuality ? Integration\nUser Testingreliability ?MIREXFigure 1. Interaction/dependency between the various el-\nements of work needed integrating music technologies.\nour tests in MIREX between 2008-2011, it appears that us-\ning Universal Background Model (UBM) to model audio\nfeatures [6] has many advantages over other techniques:\nit achieves performances among the best for both auto-\ntagging [7] and similarity tasks [8]; it allows to share the\nsame front-end for both tasks; it allows easy scalability in\nthe case of similarity (items comparison remains in an Eu-\nclidean space). Therefore, we chose UBM for these tasks.\nIn parallel, the design of the GUI starts. Since the GUI\ndirectly infers on the usability of the functionalities, its de-\nsign is mainly driven by those. It is also driven by the\noutputs of user-tests and by extra outputs that technologies\ncan provide without extra-costs. For example, when com-\nputing audio summaries, music structures are estimated as\nan internal step. Therefore, it can easily be integrated to\nprovide new functionalities (display interactive player).\nIn a latter stage, annotated corpora need to be created\nfor each of the requested content description (genre, mood,\ninstrumentation). This part forms a close feedback loop\nbetween:\u0000annotation of a corpus, \u0000measuring the re-\nliability of the annotations (this can highlight the fact that\nsome required concepts may appear unclear), \u0000redeﬁn-\ning the types of content with the third-party. After sev-\neral iterations, this loop-process leads to a much clearer\nset of content-description concepts (the speciﬁc deﬁnition\nof genre, mood, instrumentation) and more accurate anno-\ntated corpora (their speciﬁc use for music tracks).\nThese annotated corpora are then used to the train the\ncorresponding technologies and optimization is performed\nto reduce computation time, disk access and memory load.\nThe resulting prototype is then submitted to global user\ntests (testing both functionalities, the GUI and the underly-\ning technologies to achieve the functionalities). The whole\nprocess is then started again (once a year in our project).\n3. TECHNOLOGIES INTEGRATED\nResulting from the process explained in part 2, six different\nmusic-content-based technologies have been selected:\u000fauto-tagging based on training (for genre, mood, in-\nstrumentation tags and singing segmentation),\n\u000ftwo technologies for auto-tagging based on dedi-\ncated models (for tempo and key/mode tags),\n\u000fsearch by similarity (for music recommendation),\n\u000fmusic structure (for interactive browsing),\n\u000faudio summary creation (for content preview).\nThese modules are either applied to mp3 ﬁles or to the au-\ndio part of video archives or clips. The inter-connections\nbetween the various modules are indicated in Figure 2. It\nshould be noted that the ﬁrst ﬁve technologies were evalu-\nated very positively in the recent MIREX-11 evaluations.\n3.1 Audio feature extraction\nIn order to decrease the total computation time, auto-\ntagging based on training and search-by-similarity are\nbased on the same audio features front-end. The audio fea-\ntures front-end is described in Figure 2 and corresponds to\nthe proposals made in [8], [7] or [9]. It is based on two\nmodeling techniques coming from speech processing:\n\u000fUniversal Background Model (UBM) [6] [10]. The\naim of this technique is to represent the “world” of fea-\ntures using a GMM and then deform4this “world” to rep-\nresent a new feature vector. The resulting representation is\nthe concatenation of the adapted \u0016-vectors of the GMM,\nthe size of which depends on the dimensionality Dof the\ninitial feature vectors and the number mof mixtures used\nfor the GMM. These concatenated-vectors are denoted by\n“Super-Vectors” (SV) in the following.\n\u000fMultivariate Auto-Regressive Model (MAR) [11].\nAs for the mono-dimensional AR-model, the goal is to rep-\nresent the dependency of the values of a signal over time\nby an all-pole ﬁlter of order K5. In the case of the MAR,\nwe consider the dependencies in time and between the var-\niousDdimensions of the feature vectors. The results of\nthis is a matrix of coefﬁcients \u000bk;d.\nThe input to these two modeling techniques is a fea-\nture set made of 13 Mel Frequency Cepstral Coefﬁcients\nand 4 Spectral Flatness Measure coefﬁcients, extracted us-\ning a 40 ms Blackman window with a 20ms hop size.\nFrom those, two modeled feature sets are computed:\n(1) Super-Vector of MFCC/SFM, which we denote by\nSV(mfcc/sfm), (2) MAR of MFCC/SFM, which we denote\nby MAR(mfcc/sfm). The two modelings are performed us-\ning\u0000either the whole set of features inside a track (in\ncase of search-by-similarity and global auto-tagging) \u0000or\nthe set of feature inside successive windows of 2s duration\n(in case of segmentation, such as singing voice location).\nIn each case, the UBM has been previously trained on a\nrepresentative database. This training is the most time-\nconsuming part but needs only to be performed once. The\nUBM conﬁguration is a set of m= 64 (for search-by-\nsimilarity) or m= 32 (for auto-tagging) mixtures, each\nwith a diagonal covariance matrix. The order of the MAR\nmodel isK= 4.\n4Deforming means here adapting the \u0016-vectors of the GMM using an\nExpectation Maximization algorithm.\n5s(n) =PK\nk=1\u000bks(n\u0000k) +\u000fwheresis a signal,ndiscrete time,\n\u000fa residual.Feature Extraction\nMFCC (13) / SFM (4)\nUCS/MCS \nNormalisation\nSearch-by-\nsimilaritySVM classiﬁers\nThreshold \ndecisions + \nexclusive rule\nAuto-taggingTempo \ndetection\nSegmentationKey/Mode \ndetectionFeature Extraction\nMusic \nStructureAudio \nSumm.UBM \nm=64/32UBM\nMAR\nK=4\nSV(mfcc/sfm) MAR(mfcc/sfm)AudioFigure 2. Modules used for music content extraction.\nAuto-tagging based on training\nCategories Conﬁguration Tags\nGenre single-label Classical, Other Genres, NA\nOther Genres multi label Pop/Rock, Blues, Elec-\ntronica, Metal/Punk,\nReggae, Jazz, Rap,\nSoul/Funk, Rhythm &\nBlues, Latin/Bossa\nMood single label Happy, Sad, NA\nsingle label Dynamic, Calm, NA\nsingle label Romantic, NA\nInstrumenta-\ntionmulti label Brass, String, Piano, Elec-\ntronic, Acoustic\nDrum Kit single label No drum, Electronic,\nPop/Rock, Hard/Metal\nGuitar single label No guitar, Acoustic Guitar,\nElectric Guitar\nLive Studio segmentation\n+ single labelLive, Studio\nSinging segmentation Singing voice\nTable 1. Categories, conﬁgurations and tags of the various\nclassiﬁers used for the Auto-tagging modules\n3.2 Search by similarity\nAs explained in [8], the main goal of using UBM and\nMAR for modeling the features (instead of the usual\nMFCC/GMM with EMD Kullback-Leibler divergence) is\nto remain in an Euclidean space. In the case of search\nby similarity, it therefore allows the use of standard tech-\nniques to decrease the search time in the database. In order\nto avoid hubs and orphans, various techniques have also\nbeen proposed. We have used the UCS-norm (UBM Cen-\ntered Spherical normalization) and the MCS-norm (Mean\nCentered Spherical) proposed in [8]. Both techniques con-\nsist in projecting the features vector on a unit sphere (ei-\nther centered on the mean of the UBM, or the mean of\nthe database). After this, each track of the database sees\nthe rest of the database with the same point of view (unit\nsphere). Using those, the similarity between two tracks is\nsimply the correlation of their vectors. Two similarity ma-\ntrices, corresponding to the two feature sets are then com-\nputed and combined linearly (late-fusion).3.3 Auto-tagging based on training\nAuto-tagging based on training aims at providing the tags\nindicated in Table 1. Tags can be exclusive (such as “dy-\nnamic” and “calm”) or inclusive (“pop/rock” and “elec-\ntronica”). In our system, all problems are solved using\nmulti-label classiﬁers in a one-against-all strategy (true\nversus false class). For this, all problems are decomposed\nas set of binary SVM classiﬁers (with an RBF-kernel, \u001b=1)\n[12] [7]. The input to the classiﬁers is the concatenation of\nSV(mfcc/sfm) and MAR(mfcc/sfm) (early fusion).\nGlobal Classiﬁers: Music genre classiﬁer is a set of\n11 binary classiﬁers (one for each genre) trained and eval-\nuated independently. A given track tis said to belong to\na tag-classcif the afﬁnity-output ac(t)of the correspond-\ning SVM classiﬁer is above a threshold Ac. The estima-\ntion of each threshold Acis based on the Recall/Precision\ncurve obtained on a training set. Considering that the es-\ntimated tags are to be used as search criteria, it was de-\ncided to favor Precision over Recall: we chose the lowest\nAcleading to a Precision greater than 0.8. In terms of us-\nability, we also decided to make “classical music” mutu-\nally exclusive to the “other genres” (see Table 1). For a\ngiven track t, if bothaclass(t)and several aother(t)are\nabove their respective threshold, the choice is based on\nthe maximum between aclass(t)andmax(a other(t)). In\ncase max(a other(t)) is selected, the corresponding sub-\ngenres above their respective thresholds are returned. The\nsame process is applied for the 5 mood classiﬁers. In this\ncase, the mutually exclusive classes are “happy” / “sad”\nand “calm” / “dynamic”. The auto-tagging module also re-\nturns three view-points related to the instrumentation of\nthe track: (1) a global instrumentation based on dominant\ninstruments (brass, string, piano, electronic instruments,\nacoustic instruments), (2) a detailed description of the per-\ncussive part (electronic drum, pop/rock drum, hard/metal\ndrum) (3) a detailed description of the guitar part (acoustic\nguitar, electric guitar).\nSegmentation: The segmentation is obtained by detect-\ning class-changes over time. For this, the same system as\npresented above is used, but the UBM/MAR models are\napplied to the set of features inside a succession of win-\ndows of 2s duration (hop size of 1s). Each 2s features is\nthen classiﬁed using SVM classiﬁers. This segmentation\nis used to provide singing/non-singing segmentation over\ntime. In order to avoid spurious class transitions over time,\na 3rd-order median ﬁlter is applied to the estimated classes\nover time before segmentation. This segmentation is to be\nused to display singing segments in the interface.\nWe also use this segmentation to perform the “live/studio”\nauto-tagging. In our case, “live” is deﬁned as the pres-\nence of “applauses, whistling . . . ” of audience in a bar,\nconcert-hall, stadium. Since those do not occur over the\nwhole time-duration of the track (usually at the beginning,\nending or during a break), the decision is based on frame-\nclassiﬁcation. We use a minimum threshold of 26s frames\nbeing classiﬁed as “live” for the track to be classiﬁed as\n“live”. A similar approach has been used in [13].\nEach tag has also an associated “reliability” deﬁned in\nthe interval [0;1](low/high reliability). For this, the afﬁn-ity of each SVM is passed through a sigmoid and centered\non its respective threshold. This reliability is to be used by\nthe GUI for sorting the list of results.\n3.4 Auto-tagging based on dedicated algorithms\nFor each track, we also estimate its global tempo in beats-\nper-minute. Note that this estimation does not rely on the\nset of UBM/MAR features but on a dedicated algorithm.\nWe have used the algorithm proposed in [14]. We also as-\nsign a “reliability” to this estimated tempo. For this we\nused the “periodicity” features proposed by [15] (measure-\nments of the amount of periodicity in the track).\nWe also estimate the global (most dominant over time)\nkey/mode among a set of 24 key/mode classes (C Maj, C\nmin . . . B Maj, B min). We have used the algorithm pro-\nposed in [16]. The “reliability” of the output is here es-\ntimated as the distance between the most-likely key/mode\nand the second most likely.\n3.5 Music Structure and Summary\nThis module aims at providing two functionalities: (1) to\ndisplay a map of the temporal organization of the track\n(music structure) which allows user to interact with it (skip\nforward/backward by parts) [17], (2) to provide a meaning-\nful preview of the track content (music audio summary).\nThe estimation of the music structure and of the audio\nsummary are based on the same front-end. This front-\nend combines the three similarity matrices corresponding\nto MFCC, Spectral-Contrast and Spectral-Valley [18] mea-\nsures and Chroma/Pitch-Class-Proﬁle (see [19] for details).\nMusic Structure Estimation: For robustness reasons,\nthe structure is estimated using a “state” approach. For\nthis, a segmentation of the similarity matrix is ﬁrst per-\nformed using a “checker-board” kernel [20]. The segments\nobtained are then grouped using a constrained hierarchical\nagglomerative clustering. The distance used for this clus-\ntering is a linear combination of \u0000the distance between the\naverage values inside the two segments (centroid linkage)\n\u0000the smallest possible distance between one of the diago-\nnals they may contain (sequence approach) \u0000a constraint\nto minimize the departure of the duration of the merged\nsegments from the average segment durations.\nMusic Audio Summary Generation: The technique\nused for the summary creation is based on an extension of\nthe summary score of [21]. In this extension, the method\nof [21] is iteratively applied to the combined matrix of\n[19]. At each iteration, the two time corridors in the self-\nsimilarity-matrix corresponding to the previously chosen\naudio extract are canceled to avoid further re-uses. To gen-\nerate the ﬁnal audio signal, the selected segments are con-\ncatenated using a Downbeat Synchronous OverLap-Add\n(DSOLA) techniques.\n4. ANNOTATED CORPORA FOR TRAINING\n4.1 Corpus creation for the UBM training\nSince both auto-tagging and search-by-similarity modules\nrely on Super-Vectors, the corresponding UBM needs tobe trained in advance. The training of which needs to take\ninto account the various types of contents (various genres\nand various audio qualities) that the system will need to\ndeal with. For this, a large database of audio ﬁles has been\nused: including clean mp3 ﬁles at various bit-rates and au-\ndio tracks of TV archives.\n4.2 Annotated corpora for the auto-tagging problems\nFor the auto-tagging modules, statistical models (SVMs\nand related thresholds) need to be trained for each tag\n(genre, mood, instrumentation, singing, live). We explain\nhere the data used for the training. For the creation of the\nlist of genres, several attempts have been made: \u0000from\na purely acoustic deﬁnition of genres (pop-rock synthed,\npoprock hard, electronica ambient, electronica beat . . . )\nwhich guarantees a close proximity to content-based es-\ntimation algorithms but may be difﬁcult to understand by\nusers\u0000to a purely application oriented deﬁnition. The ﬁ-\nnal list indicated in Table 1 is the results of a feedback-loop\nbetween the two. The training-set has then been obtained\nby selecting tracks among a large music collection consid-\nered as prototypical of the chosen genres. By prototypi-\ncal, we mean tracks representative of the exact genre and\nnot cross-over between several genres. For the other tags\n(mood, instrumentation), 4000 tracks have been manu-\nally annotated by two individual professional annotators.\nOnly labels for which the annotators agreed on the ma-\njority of the tracks are considered. For these labels, only\ntracks for which both annotators agreed have been selected\nfor the training. This process lead to the ﬁve moods and\nthree view-points on instrumentation indicated in Table 1.\n“Live” classiﬁer has been trained on a dedicated training-\nset made of the concatenation of all possible audience\nnoise derived from real recording. The singing segment\nclassiﬁer has been trained using the Jamendo corpus [22].\n5. GRAPHICAL USER INTERFACE\nThe GUI is the central element that allows user to interact\nwith the prototype and to test the proposed use-cases. Its\ndesign is crucial since a bad GUI can hinder a good tech-\nnology or a good use-case. Its design must follow a close\nuser-feedback loop (see part 6). The current GUI (see Fig-\nure 3) is organized in three main panels: the interactive-\nplayer (top), the current play-list (left), the various tag-\nclouds (right).\nTheplayer panel displays the classical editorial meta-\ndata (track-title, artist-name, album-title) and the cover. A\nlarge horizontal time-line displays the estimated structure\nof the tracks. In this, parts with similar content are in-\ndicated by rectangle with similar colors. The user can\nbrowse through parts by directly clicking on the corre-\nsponding colored rectangle. The time-line also indicates\nthe segments used for the audio summary by highlighting\nthe corresponding parts (independently of the color). Once\nselected (using the play-list panel), a track automatically\nstarts playing in the player either in full-duration or in au-\ndio summary mode. This choice is based on user prefer-\nences. A search-by-text panel is placed on the top of theTAG CLOUDS CURRENT PLAYLISTINTERACTIVE PLAYER\nFigure 3. GUI of the Music Interface of the prototype\ninterface. It allows either full-database search or search\nover restricted criteria (title, artist, album).\nThe play-list panel indicates the currently selected\ntracks which correspond either to \u0000the results of a search-\nby-text, a search/ﬁltering using the tag-clouds or a search-\nby-similarity,\u0000or a previously stored play-list6. For each\ntrack, the estimated tags (genre, mood, instrumentation)\nare also indicated. The musical note icon next to each track\nallows performing search-by-similarity.\nThe tag clouds panel indicates the various viewpoints\non the content: genre, mood, and instrumentation. The\ntags that are currently active in the ﬁltering are indicated\nby highlighted colors. Next to each tag-name is indicated\nthe number of corresponding items. It should be noted that\nthe tags inside a cloud are not mutually exclusive.\n6. USER TESTS\nWe deﬁne user experience as “the combination between the\nquality of the technology, the functionality based on it and\nthe way to present it on Human Machine Interface”. Dur-\ning the project, 1 or 2 user tests per year were performed,\neach corresponding to a new version of the prototype.\nMany of the outputs of user-tests relate to the usability\nof the GUI: naming of the ﬁelds, their spatial organization,\nlayout of the tag-clouds . . . This is of course essential; es-\npecially considering that the music search part is only one\npart of the whole search engine (which also includes TV\nand Web-Video search) and the presentation of the various\nsearch engines must be as much as possible homogeneous.\nUser-tests are performed using two methods.\n6The playlist, tempo and key/mode functionalities are not discussed\nhere since their are currently subject to modiﬁcations of the GUI.6.1 Qualitative tests\nThe ﬁrst method consists in performing qualitative tests.\nQualitative tests have three focuses: (1) to asses the us-\nability of HMI (2) to asses users’ judgment of functional-\nities (3) to imagine with users new use cases and maybe\nnew functionalities based on the music technology. For\nthis, users were asked to perform various scenario: “use\nthe search engine to create a music play list of a speciﬁc\nmood”, “to discover new music” . . . This is followed by in-\nterviews, which allows highlighting problem in the usabil-\nity of the GUI, collecting judgments of functionalities (au-\ndio summary, genre, mood and similarity are found highly\nrelevant while music structure displaying less relevant).\nThis has also allowed highlighting missing functionalities.\nDisplaying singing segments was one of those.\n6.2 Quantitative tests: the case of audio summary\nThe second method consists in performing quantitative\ntests to compare several variations of a technology. An\nexample of this are the ”audio summary” user-tests.\nFor the creation of the summary, a set of user tests have\nbeen performed in order to select the best summary strat-\negy. For this we have compared four different types of\nsummary:\u0000a 30s extract at the beginning of the ﬁle, \u0000a\nrandom 30s extract, \u0000the most representative 30s extract\n(denoted by 1x30), \u0000a downbeat-synchronous concatena-\ntion of the three most representative 10s extracts (denoted\nby 3x10) [17]. 24 users had to listen to tracks of music\nthey knew (7 tracks) and music they didn’t know (6 tracks).\nHalf of the songs were in their native language (French),\nthe other half in English. They were then asked the ques-\ntions - “which technique better summarized the track” (for\nmusic they knew), - “which technique is the most informa-\ntive of its content” (for music they didn’t know). In both\ncases, the 3x10 summary was judged better.\nA quantitative evaluation has also been performed to\ncompare the 1x30 and 3x10 summary. Over a 160-tracks\ndatabase, we have measured the number of tracks for\nwhich each technique allowed to include the track title in\nthe summary (the track title is considered here as the most\nmemorable part of the track). The 3x10 summary achieved\n95% correct location, while the 1x30 achieved 90%.\nA user evaluation of the acoustical quality of the multi-\nparts (3x10) summary has also been performed. We have\ncompared four conﬁgurations of the audio construction:\n\u0000complete DSOLA \u0000partial DSOLA (the loudness of the\naudio decreases during the transitions between parts to\nhighlight them),\u0000DSOLA with sound insertion (a proto-\ntypical sound is introduced at each transition to highlight\nthem),\u0000partial DSOLA with visual feedback. This exper-\niment highlighted the fact that in some cases (especially\nRap music), the complete DSOLA leads to an audio that\nsounds exactly like a real track. However, users prefer to\nfeel a separation between the three 10-second parts to avoid\nhaving the feeling of listening to a new mix from a DJ. We\nalso decided to add a visual presentation to increase the\nunderstanding of this summary functionality. This visual\npresentation consists in 3 highlighted segments of the com-plete music timeline corresponding to the three 10-second\nparts of the summary. The play cursor “jumps” from part\nto part. With these choices and modiﬁcations, user experi-\nence of the summary was improved.\n7. INTEGRATION\nThe back-ofﬁce of the prototype is based on a Service Ori-\nented Architecture (SOA). This kind of architecture is ﬂex-\nible and particularly adapted for the integration of numer-\nous and distant technologies. The main elements of this ar-\nchitecture are:\u0000Metadata collectors, which collect meta-\ndata coming from content providers (TV Programs, INA\narchives, Web videos, music); \u0000Technological modules,\naccessible as Web services (e.g. speech to text, named\nentities extraction, music analysis); \u0000An XML transverse\nmetadata base, which stores all metadata coming from col-\nlectors and technological modules; \u0000An ESB (Enterprise\nService Bus), which connects the metadata collectors, the\ntechnological modules and the metadata base; \u0000A speciﬁc\nXML “pivot” format for all metadata manipulated by the\nESB and the XML database. The search engine indexes\nare fed by the XML database through a metadata exporter.\nThe search engine is directly connected to the application.\n8. CONCLUSION\nIn this paper, we wanted to share our experience on inte-\ngrating music-content indexing technologies, as developed\nin a research lab, into a third-part search and navigation\nengine. For this, we provided a panorama of the various\nelements of works implied and how they interact.\nThe lessons we learned from this experience is that\nthis integration involves much more than good signal pro-\ncessing and machine learning technologies, which are of\ncourse essential. A side from the technical constraints (ro-\nbustness, scalability), many of the works to be performed\nrelate to make these technologies usable. This involves\nﬁrst proposing useful and understandable tags for users\nand creating the related annotated corpus to train the algo-\nrithms. This also involves tuning and modifying technolo-\ngies: to favor precision over recall; or to provide reliability\nfor all estimations (which is difﬁcult for descriptions such\nas tempo or key). User tests allows to highlight new chal-\nlenges, such as the need for a list containing only the simi-\nlar items and not just a ranked-list from the most to the less\nsimilar items; or the fact that some innovative technologies\nmay be found too specialized for users (music structure).\nWe hope the information provided here would help the re-\nsearch community when trying to move from research ap-\nplications to third-party applications.\nAckowledgements: J.J. Burred is now with Audion-\namix. This work was supported by the “Quaero” Program\nfunded by Oseo French State agency for innovation.\n9. REFERENCES\n[1] F. Pachet, J. Aucouturier, A. La Burthe, A. Zils, and A. Beurive, “The\ncuidado music browser: an end-to-end electronic music distribution\nsystem,” Multimedia Tools and Applications, vol. 30, no. 3, pp. 331–\n349, 2006.[2] M. Gasser and A. Flexer, “Fm4 soundpark: Audio-based music rec-\nommendation in everyday use,” in Proc. of the 6th Sound and Music\nComputing Conference (SMC 2009), Porto, Portugal, 2009.\n[3] M. Goto and T. Goto, “Musicream: New music playback interface for\nstreaming, sticking, sorting, and recalling musical pieces,” in Pro-\nceedings of the 6th International Conference on Music Information\nRetrieval, pp. 404–411, 2005.\n[4] A. Lillie, MusicBox: Navigating the space of your music. PhD thesis,\nMassachusetts Institute of Technology, 2007.\n[5] P. Knees, M. Schedl, T. Pohle, and G. Widmer, “Exploring music\ncollections in virtual landscapes,” Multimedia, IEEE, vol. 14, no. 3,\npp. 46–54, 2007.\n[6] D. Reynolds, T. Quatieri, and R. Dunn, “Speaker veriﬁcation using\nadapted gaussian mixture models,” Digital signal processing, vol. 10,\nno. 1-3, pp. 19–41, 2000.\n[7] D. Tardieu, C. Charbuillet, F. Cornu, and G. Peeters, “Mirex-2011\nsingle-label and multi-label classiﬁcation tasks: Ircamclassiﬁca-\ntion2011 submission,” in MIREX Extended Abstract, (Miami, USA),\n2011.\n[8] C. Charbuillet, D. Tardieu, and G. Peeters, “Gmm supervector for\ncontent based music similarity,” in Proc. of DAFX, (Paris, France),\npp. 425–428, September 2011.\n[9] C. Charbuillet, D. Tardieu, F. Cornu, and G. Peeters, “2011 ircam au-\ndio music similarity system 1,” in MIREX Extended Abstract, (Miami,\nUSA), October 2011.\n[10] C. Cao and M. Li, “Thinkit’s submissions for mirex2009 audio mu-\nsic classiﬁcation and similarity tasks,” in MIREX Extended Abstract,\n(Kobe, Japan), 2009.\n[11] F. Bimbot, L. Mathan, A. De Lima, and G. Chollet, “Standard and\ntarget driven ar-vector models for speech analysis and speaker recog-\nnition,” in Proc. of IEEE ICASSP, vol. 2, pp. 5–8, 1992.\n[12] J.-J. Burred and G. Peeters, “An adaptive system for music classiﬁca-\ntion and tagging,” in Proc. of LSAS (Int. Workshop on Learning the\nSemantics of Audio Signals), (Graz, Austria), 2009.\n[13] F. Fuhrmann and P. Herrera, “Quantifying the relevance of locally\nextracted information for musical instrument recognition from entire\npieces of music,” in Proc. of ISMIR, (Miami, USA), 2011.\n[14] G. Peeters, “Template-based estimation of time-varying tempo,”\nEURASIP Journal on Applied Signal Processing, vol. 2007, no. 1,\npp. 158–158, 2007. doi:10.1155/2007/67215.\n[15] G. Peeters, “Spectral and temporal periodicity representations of\nrhythm for the automatic classiﬁcation of music audio signal,” IEEE\nTrans. on Audio, Speech and Language Processing, vol. 19, pp. 1242–\n1252, July 2011.\n[16] G. Peeters, “Chroma-based estimation of musical key from audio-\nsignal analysis,” in Proc. of ISMIR, (Victoria, Canada), pp. 115–120,\n2006.\n[17] G. Peeters, A. Laburthe, and X. Rodet, “Toward automatic music au-\ndio summary generation from signal analysis,” in Proc. of ISMIR,\n(Paris, France), pp. 94–100, 2002.\n[18] D. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H. Cai, “Music type\nclassiﬁcation by spectral contrast,” in Proc. of IEEE ICME (Interna-\ntional Conference on Multimedia and Expo), (Lausanne Switzerland),\n2002.\n[19] G. Peeters, “Sequence representation of music structure using higher-\norder similarity matrix and maximum-likelihood approach,” in Proc.\nof ISMIR, (Vienna, Austria), 2007.\n[20] J. Foote, “Automatic audio segmentation using a measure of audio\nnovelty,” in Proc. of IEEE ICME (International Conference on Mul-\ntimedia and Expo), (New York City, NY , USA), pp. 452–455, 2000.\n[21] M. Cooper and J. Foote, “Automatic music summarization via simi-\nlarity analysis,” in Proc. of ISMIR, (Paris, France), pp. 81–85, 2002.\n[22] M. Ramona, G. Richard, and B. David, “V ocal detection in music\nwith support vector machines,” in Proc. of IEEE ICASSP, pp. 1885–\n1888, 2008."
    },
    {
        "title": "Towards a (Better) Definition of the Description of Annotated MIR Corpora.",
        "author": [
            "Geoffroy Peeters",
            "Karën Fort"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417871",
        "url": "https://doi.org/10.5281/zenodo.1417871",
        "ee": "https://zenodo.org/records/1417871/files/PeetersF12.pdf",
        "abstract": "Today, annotated MIR corpora are provided by various re- search labs or companies, each one using its own annota- tion methodology, concept definitions, and formats. This is not an issue as such. However, the lack of descriptions of the methodology used—how the corpus was actually an- notated, and by whom—and of the annotated concepts, i.e. what is actually described, is a problem with respect to the sustainability, usability, and sharing of the corpora. Ex- perience shows that it is essential to define precisely how annotations are supplied and described. We propose here a survey and consolidation report on the nature of the an- notated corpora used and shared in MIR, with proposals for the axis against which corpora can be described so to enable effective comparison and the inherent influence this has on tasks performed using them.",
        "zenodo_id": 1417871,
        "dblp_key": "conf/ismir/PeetersF12",
        "keywords": [
            "corpora",
            "annotated",
            "research",
            "labs",
            "companies",
            "methodology",
            "descriptions",
            "sustainability",
            "usability",
            "sharing"
        ],
        "content": "TOWARDS A (BETTER) DEFINITION OF THE DESCRIPTION OF\nANNOTATED MIR CORPORA\nGeoffroy Peeters\nSTMS IRCAM-CNRS-UPMC\nParis, FranceKar¨en Fort\nINIST-CNRS & Universit ´e Paris 13,\nSorbonne Paris Cit ´e, LIPN\nNancy, France\nABSTRACT\nToday, annotated MIR corpora are provided by various re-\nsearch labs or companies, each one using its own annota-\ntion methodology, concept deﬁnitions, and formats. This is\nnot an issue as such. However, the lack of descriptions of\nthe methodology used—how the corpus was actually an-\nnotated, and by whom—and of the annotated concepts, i.e.\nwhat is actually described, is a problem with respect to the\nsustainability, usability, and sharing of the corpora. Ex-\nperience shows that it is essential to deﬁne precisely how\nannotations are supplied and described. We propose here\na survey and consolidation report on the nature of the an-\nnotated corpora used and shared in MIR, with proposals\nfor the axis against which corpora can be described so to\nenable effective comparison and the inherent inﬂuence this\nhas on tasks performed using them.\n1. INTRODUCTION\nThe use of annotated data usually corresponds to increas-\ning performances in a ﬁeld of research, as has been seen\nin the cases of speech and language processing. The ac-\ncessibility of novel annotated data usually corresponds to\nthe initiation of a number of research activities in a ﬁeld.\nThis is the case of music genre, chord recognition, and mu-\nsic structure in music information retrieval (MIR). For this\nreason, annotated data can be considered to be a major is-\nsue in MIR. In MIR, there is currently no dedicated in-\nstitution responsible for providing music corpora compa-\nrable to ELRA1or LDC2in the speech and natural lan-\nguage processing (NLP) community. Instead, corpora are\nprovided individually by various research labs and compa-\nnies. While recent years have seen a large increase in cor-\npora creation initiatives (e.g. Isophonic, SALAMI3, Bill-\nboard, and Quæro), each research lab or company uses its\nown annotation methodology, concepts deﬁnition, and for-\nmat. This is not a problem in and of itself, but the lack\n1http://www.elra.info/\n2http://www.ldc.upenn.edu/\n3Structural Analysis of Large Amounts of Music Information\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.of descriptions of the methodology used, i.e. how the cor-\npus was actually annotated, or of the concepts annotated,\ni.e. what is actually described, presents problems with re-\nspect to the sustainability, usability, and sharing of cor-\npora. Therefore, it is essential to deﬁne exactly what and\nhow annotations of MIR corpora should be supplied and\ndescribed. We propose here an avenue to improve this\nsituation by deﬁning a methodology for describing MIR\ncorpora and the implicit or explicit assumptions made dur-\ning their creation. It should be noted that similar initia-\ntives have been taken in the speech and NLP community\nto favor sharing and exchange of corpora (see for exam-\nple [1], [2] [3]) leading to descriptions close to the one\nproposed here.\n2. DEFINING AN ANNOTATED MIR CORPUS\nIn the following, by annotated corpora, we mean “musical\naudio data with annotations”. Such corpora can be used\nfor research purposes to derive knowledge or train systems,\nor for benchmarking and evaluation projects, both internal\nand public, as in MIREX4. Creating an annotated MIR\ncorpus involves:\n(A) choosing or creating a set of audio items (denoted by\n“raw corpus” in the following),\n(B) creating and/or attaching related annotations, and\n(C) documenting and storing the results to ensure sus-\ntainability and sharing.\nWhile these points may seem obvious, each of them\ninvolves making choices that in the aggregate will deﬁne\nwhat exactly the corpus is about, what use it is for, and\nwhat the underlying assumptions behind it are. In the fol-\nlowing, we provide insights about the choices that must be\nexplicitly or implicitly made for each of these points, and\nthe implications of those choices. Figure 1 summarizes the\nvarious aspects of the proposed description.\n2.1 (A) Raw Corpus\nIn the case of “audio MIR,” the annotations describe au-\ndio items5, which we denote here by the term “raw cor-\npus”, as opposed to the “annotated corpus”. The choice of\nthese audio items deﬁnes the domain, or musical area, for\nwhich the results derived from the annotations—results\n4MIREX: Music Information Retrieval Evaluation eXchange\n5Whether the annotations are distributed with or without the audio\nitems, the following remains true.(A) Raw Corpus (audio)Meta-data\n(B) Annotations\n(B22) Annotation Rules(B21) Concepts Deﬁnition\n(A1) Raw Corpus Deﬁnition\n(B32) V alidation/Reliability(C1) Identiﬁer\n(B4) Annotation T ools(C2)  Storage\n(B31) Annotators(A2) Diffusion\n(a11) Synthetic (a12) Created (a13) Sampled\nPopularity Uniformitem stream\n(b11) Synthetic\n(b13) Experiment\n(b14) Crowdsourcing(b12) Aggregation\n(b15) Manual(B1) Origin\n(b14) Games With A Purpose\nAccessibility Content(C) Documenting and StoringFigure 1. Decomposing the creation of a MIR annotated corpus into the tasks and sub-tasks involved.\nfrom an experiment, training, or evaluation—are valid. For\nexample, results derived from the music genre deﬁned for\nthe Tzanetakis test-set [4] do not generalize to the Million-\nSong test-set. The choice of audio items also determines\nthe domain for which the concepts deﬁned by the anno-\ntations are valid. This is speciﬁc to the way annotation is\nperformed in the MIR ﬁeld: while in other domains, con-\ncepts are ﬁrst deﬁned, and then used for the annotation of\nitems, in MIR, the concepts are (in most cases) deﬁned by\nthe annotations themselves. For example, “music genre”\nis not deﬁned textually, but rather is deﬁned by its appli-\ncation to a speciﬁc test-set, such as Tzanetakis or Million\nSong. The same is true for “chords,” whose meaning may\ndiffer in the context of the data to which it is applied: in\nthe Beatles [5], it refers to guitar chords, but when applied\nto Billboard songs, it is a reduction of the overall harmonic\ncontent. Because of this, special care must be taken when\nselecting audio items for an annotated MIR corpus.\nIt is clear that results obtained from experiments on (i)\nsynthesized MIDI ﬁles, (ii) audio recorded for the purposes\nof an experiment, or (iii) audio as sold by online music\nservices, do not have the same impact. Note that this in no\nway means that one is better than the others.\nTo help describe the choices made so far in MIR, we\npropose to distinguish between three categories:\n\u000fa11 - artiﬁcial audio items made speciﬁcally for the\npurpose of representing an annotation\n\u000fa12 - real audio items recorded speciﬁcally for the\npurpose of the creation of a corpus\n\u000fa13 - real audio items sampled from the real world\n2.1.1 (a11) Corpus of Synthetic Items\nThis kind of corpus is speciﬁc to the music research com-\nmunity, based on the assumption that, within certain limits,\nrendering a MIDI ﬁle can create a music audio signal. Ex-\namples of this are [6] for the creation of a multi-pitch esti-\nmation corpus, [7] for the case of chords, and the MIREX\ncorpus used for key estimation.\nThere are several advantages to this approach. It allows\n(i) having close to perfect annotations very easily, since au-dio can be partly considered to be a direct instantiation of\nthe annotation; (ii) having full control over the audio ren-\ndering process, such as testing the inﬂuence of instrument\nchanges, reverberation or delay; and (iii) rapidly creating\na large corpus. Its major drawback is the lack of realism\ndue to (i) the absence of interpretation by musicians; (ii)\nthe absence of realism due to sound variations, propaga-\ntion, and capture by microphone; and (iii) the absence of\n“production” as made in recording studios.\n2.1.2 (a12) Corpus of Created Real Items\nThe second trend consists in creating speciﬁc audio items\nfor the purpose of research. The ﬁrst corpora prepared in\nthis way were built for “instrument samples” research—\nMcGill [8] and Studio-On-Line [9]. In this case, the\nannotation—pitch, instrument name, and playing mode—\nis added during the recording session. Corpora for multi-\npitch, source separation, and event recognition have also\nbeen created, such as the ENST Drum database [10], con-\ntaining audio and video, and the MAP database [11], using\na Yamaha disklavier for automatic pitch annotations. The\nmost well-known and used corpus in MIR research is such\na data set—the RWC corpus [12, 13].\nThe advantages of this approach are that it allows (i)\ncomplete speciﬁcation of the underlying content property,\n(ii) easy creation of the annotations at the same time as the\noccurrence, and (iii) distribution of the corpus with no re-\nstrictions, as the creator of the corpus usually owns the au-\ndio copyright. The main drawback of this is again the lack\nof realism of the resulting audio items—e.g., RWC is a\nvery valuable resource but does not sound like iTunes mu-\nsic. This is partly due to the recording conditions that a lab\ncan afford—expensive compressors and enhancers remain\nin the big production studios. Also, the music composition\nused is often prototypical. All of this frequently creates a\nbias when using these corpora outside of the context of the\nexperiment for which they were built.2.1.3 (a13) Corpus of Sampled Real Items\nThe last trend corresponds to what has long been known\nas “using a private music collection”. These could not be\nshared, mostly due to copyright issues. Today, because of\nthe possibility of referencing audio items by ID (CD ref-\nerence or Musicbrainz/ EchoNest/ Amazon/ 7-Digital ID),\nthere is a major trend toward these corpora. The main ad-\nvantage of this type of corpus is that it represents exactly\nthe music people listen to or buy, with artistic interpretation\nand professional sound production. It also allows the eval-\nuation of concepts that are well-established in the literature\nfor their applicability to everyday music (see the case of\nthe “chord” concept). The major drawback of this type of\ncorpus is the cost of the annotations, which involve either\nhuman annotation (by dedicated people or by crowdsourc-\ning) or data aggregation (for example, aggregating guitar-\ntab collections or music-recommendation sites).\nHowever, underlying a corpus created by sampling the\nreal world lies a major question: how was the sampling\ndone? For which reasons or purposes were the speciﬁc\nmusic tracks selected? This is actually rarely described,\nwith the exception of [14], which provides an in-depth de-\nscription of the sampling for the Billboard corpus. We dis-\ntinguish here between four trends:\nSpeciﬁc-content sampling: The sampling is done in\norder to highlight speciﬁc content characteristics. An ex-\nample of this is the corpus proposed by [15] for music\nstructure. It consists of a selection of tracks from Euro-\nvision (the European Song Contest), i.e. pop songs with\na typical pop structure. Another is the corpus proposed\nby [5] for chord annotation, which consists primarily of a\nselection of tracks from The Beatles, essentially made of\nguitar chords. While this perfectly ﬁts the purpose of their\nannotations, care must be taken with respect to the validity\nof the concepts (e.g. the speciﬁc deﬁnition of structure or\nchords) outside of the context of these corpora.\nPopularity-oriented sampling: The sampling is done\naccording to what people were or are reading, listening to,\nor watching the most. An example is given in [14], in\nwhich the sampling is performed based on the Billboard\ncharts. However, in this case, some music genres might be\nover-represented.\nUniform sampling: The sampling is done in a uniform\nway according to a description grid. The dimensions of this\ngrid, as in our project, may represent music genre/style,\nyear, or country6. In each resulting cell of the grid, the\nmost popular audio items are selected. In this case, some\nmusic styles can be over-represented.\nAccessibility-oriented sampling: The last trend con-\nsists in selecting items because they are freely available\n(e.g. Magnatune and Internet Archive), without any other\nconsiderations.\n2.1.4 (A2) Type of Media Diffusion\nApart from the choice of the sampling process, the type of\nmedia diffusion also needs to be decided during the process\nof corpus creation. Corpora can represent isolated music\n6These meta-data can be provided, e.g. by AMG. It should be noted,\nhowever, that the source of meta-data can create a bias.tracks, but may also include items as diverse as music in-\nside a TV/Radio audio stream (as in the corpus of [16]),\nthe audio part of a video clip or User-Generated-Content\nvideos, a live recording, a bootleg, or a DJ-remix. This im-\nplies different audio qualities, and also the possible pres-\nence of interfering sounds such as speech, applause, and\nthe ambient atmosphere of live performances.\n2.1.5 Deﬁnition of the Media Coding Properties\nFinally, the audio properties also have to be described, in\nterms of such variables as frequency bandwidth; the pres-\nence of drops, noise, pops, hisses or clicks (due for exam-\nple to media trans-coding from vinyl); and the number of\nchannels—mono, stereo, or multi-channel.\n2.2 (B) Attaching Annotations to Audio Items\nAlthough this is probably the most important aspect of an\nannotated corpus, it is often the one that is least described\n(except if the annotations were the subject of a dedicated\npublication, as in the case of the results of a listening ex-\nperiment [17]). The main points to detail are the following:\n\u000fWhere do the annotations come from?\n\u000fWhat do they represent? How are they deﬁned?\n\u000fWhat is their reliability?\n2.2.1 (B1) Origin of the Annotations\nThe central question is the origin of the annotations. We\ndistinguish here between four different cases:\nAutomatic annotations\n\u000f(b11): The annotations are obtained by the synthesis\nparameters [6] (a11), as scores given during the recording\nprocess or analysis of the individual tracks of the record-\nings [13] (a12). In this case, the generative process of the\nmusic deﬁnes both the labels used for the annotation and\nthe annotation itself. Its reliability is very high.\n\u000f(b12) The annotations are obtained by aggregation\nof diverse extant content. Examples of this are the Million\nSong Test-Set [18] and the use of Guitar-Tab in [19]. In this\ncase, each annotation and its deﬁnition and reliability are\ndeﬁned by its provider: Last-FM data are obtained through\ncrowdsourcing, Echo-Nest data are algorithm estimations,\nand MusicXMatch contains ofﬁcial lyrics.\nManual annotations:\n\u000f(b13): The annotations are the results of an experi-\nment. In this case, the deﬁnition of the annotation is pro-\nvided by the guidelines of the experiment. The reliability\nof the annotation is derived from the experimental results,\neither in a summarized form (e.g. two major peaks of the\ntempo histogram in [17]) or from the whole set of annota-\ntions, letting the user decide the way to summarize it (e.g.\nperception of tempo and speed in the case of [20]).\n\u000f(b14): Crowdsourcing, in particular Games With A\nPurpose (GWAP). In this case, annotations are obtained\nusing various game processes [21–24]. The labels used for\nthe annotation are either determined before the game, pro-\nviding an existing frame of reference; or determined by the\nusers during the game, allowing free input. In both cases,\nthe deﬁnitions of the labels are not provided (although theymay be inferred by another gamer’s choices), but rather are\ndeﬁned by the use that gamers make of them. In this con-\ntext, when a reliability measure of the annotation is pro-\nposed, it is usually derived from the number of occurrences\nof a label [24].\n\u000f(b15): Traditional manual human annotations. Ex-\namples of these are [5, 25, 26].\n(b13), (b14) and (b15) are the most interesting for us\nhere, since they involve thinking about the deﬁnition of the\nannotation concepts and the techniques for performing the\nannotations and measuring their reliability. Manual anno-\ntation is (very) costly, so the annotation process should en-\nsure quality and reusability. In the ﬁeld of natural language\nprocessing, the authors of [27] show that “corpora that are\ncarefully annotated with respect to structural and linguistic\ncharacteristics and distributed in standard formats are more\nwidely used than corpora that are not”.\n2.2.2 (B2) Deﬁnitions\n(B21) Concepts: The term annotation refers both to the\nprocess of adding a note or a label to a ﬂow of data\n(such as audio music, speech, text or video) and to the\nresult of this process—the notes themselves, anchored in\nthe source ﬂow. The annotations are all the more useful\nto the extent that they are designed for a speciﬁc appli-\ncation [28]. Depending on the ﬁnal application, the la-\nbels may not carry the same semantics. The semantics\nmay even be completely different—for example, annotat-\ning football matches with the intent of producing an auto-\nmatic summary [29] is very different from annotating foot-\nball matches for purposes of linguistic analysis. In speech\nand natural language processing, saying that we may ﬁnd\nas many annotation models as there are annotation projects\nis not too far from reality. In MIR, it seems that the same\nconcepts are always used, with different meanings that are\nsometimes only implicit.\nIn the case of manual human annotations, the concepts\nto be annotated must be deﬁned. The absence of deﬁni-\ntion is clearly a problem for a set of tasks in MIR (beat7,\nchord8, and structure9, to name just a few). Recently,\nefforts have been made to clarify the concepts being anno-\ntated through dedicated papers or through the on-line avail-\nability of so-called “annotation guides” [15,26]. Those ef-\nforts should be encouraged. It must be noted that the use\nof annotation guidelines has been considered part of “best\npractices” in speech and natural language processing for\nsome time, following the trend in this direction in corpus\nlinguistics [28].\n(B22) Rules: Beyond the deﬁnition of the concepts be-\ning annotated, the annotations are performed using a set of\nrules. This set of rules should also be described. For exam-\n7Given that beat is mostly a perceptual concept, what is the metrical\nlevel being annotated?\n8In the case of chord annotations, what is the deﬁnition of chords? Are\nwe considering the perceived chord of the background accompaniment?\nDo we also consider the lead vocal? Are the chords derived from the\nguitar part?\n9The case of music structure is even less deﬁned. What is a chorus? A\nsegment? Why could a segment not be further divided into sub-segments\nor grouped into meta-segments? Considering this, the proposal made in\n[30] to store the various possible annotations is worth mentioning.ple, what is the temporal precision used for segment anno-\ntations? Which type of dictionary was used for the labels?\nAre there equivalences between labels? To exemplify the\ndifference between concept andrules used to annotate this\nconcept, consider an experiment in a recent project to an-\nnotate beat/tempo. Two different rules were used. The\nﬁrst was to do annotation of beats and then infer the tempo\ncurve from that; the second was to adjust a tempo curve\nso as to align a sequencer grid to an audio track, and then\ninfer beat positions. The two methods describe the same\nconcept, but lead to different results (data not shown).\n2.2.3 (B3) Actors and Quality\n(B31) Who are the annotators? Annotators may be stu-\ndents or researchers, creating a corpus that will directly ﬁt\ntheir research, with the model of their algorithm in mind\nwhile annotating; musicians, with a strong ability to ap-\nply the concepts with respect to detailed musical structure,\nsometimes losing sight of overall perception; or everyday\npeople. This choice inﬂuences the way the annotation is\nperformed.\n(B32) Reliability of the annotation? Although they\nare considered to be able to generate “gold standards“, hu-\nmans are not perfect annotators. The deﬁnitions of the con-\ncepts to be annotated might not have been deﬁned clearly,\nthey may not ﬁt the content of a given audio ﬁle, there\nmight be several plausible possibilities for a particular an-\nnotation, or the annotator may lose concentration. The\nquestion of the reliability of the annotation is therefore\nanother major issue. For this reason, it is common prac-\ntice to do cross-validation of the annotations. This can be\ndone by applying either or both of two scenarios. In the\nﬁrst scenario, an annotated track is validated or corrected\nby a second annotator. In the second scenario, the same\ntrack is annotated independently by at least two annotators.\nThe resulting annotations are then compared by comput-\ning the inter-annotator agreement (using the Kappa coefﬁ-\ncient [31] or other measures10). A decision is then made\nwhether the annotation is sufﬁciently reliable, or whether\nit should be redone using the same deﬁnitions and rules,\nor whether the deﬁnitions or rules should be modiﬁed.\nIn speech and natural language processing, computing the\nintra-annotator agreement (agreement of an annotator with\nhim/herself as the project progresses) is also considered\nto be good practice and allows the detection of potential\nissues with the annotators [33]. This is already done in\nsound perception experiments, and could be extended to\nannotation projects.\nOverall, the methodology used should be documented\nand detailed. In speech and natural language processing,\nthe typical methodology includes early evaluation of the\nannotation guidelines using inter-annotator agreement, the\nupdate of these guidelines with the help of the annotators’\nfeedback, regular checking, continuous use of inter- and\n10It must be noted, however, that the resulting coefﬁcient of agreement\n(Cohen’s Kappa or others) is far from being wholly sufﬁcient as a met-\nric when used in isolation, and should be accompanied by details of the\nchoices that were made to compute it. In this respect, the contingency ta-\nble provides more interesting information about the annotation reliability\nthan the inter-annotator agreement itself [32].intra-annotator agreement and/or precision measures (go-\ning so far as the so-called “agile annotation” [34]), and a\nﬁnal evaluation of the resource that has been produced.\n2.2.4 (B4) Annotation Tools\nCaution is necessary in selecting the appropriate annota-\ntion tool, as the limitations of the tool will impact the an-\nnotation model. For example, there may be relations that\nare impossible to annotate, or the interface may contain a\nfeature that is difﬁcult to access and hence seldom used.\n2.3 (C) Documenting and Storing the Results to\nEnsure Sustainability and Sharing\nCorpus sharing and distribution does not simply require\nputting all of the audio data and annotations into an archive\nﬁle. From our point of view, it implies providing infor-\nmation on all of the above-mentioned points (A* and B*).\nWe provide here some additional recommendations for im-\nproving the distribution process.\n2.3.1 (C1) Corpus Identiﬁcation\nCurrently, most corpora in MIR research or MIREX bench-\nmarking have no identiﬁer (except RWC, Isophonic, or\nMillion Song Test-Set). They are referred to as “the corpus\nused in the publication of [reference]”. A unique identiﬁer\nshould be assigned to each corpus, including versioning of\nthe annotations and annotation guidelines (see [35]). This\ncould take the form of a simple URI (example of this would\nbe corpus:MIR:qmul:2004:beatles:chords:version1.0) or\nused the more elaborated V ocabulary of Interlinked\nDatasets11. This would solve some ambiguity issues, such\nas when a corpus is updated over time (for example the\nSALAMI corpus), or when one set of annotations is re-\nvised by another lab and later included in a new corpus\n(for example the structure annotations of The Beatles).\n2.3.2 (C2) Storage of the Created Annotations\nAnnotations must be sustainable. We therefore recom-\nmend that the storage of the data make their semantics\nexplicit. Up to this point in time, many annotations of\nlocal-in-time concepts such as beat, chord, and structure\nwere done in formats where the semantics is implicit in the\ncorpus. In particular, the so-called “.csv” or “.lab” formats\n(one row for time, one row for labels) would not be sustain-\nable outside of the context of a given corpus12. RDF (as\nused by QMUL [5]) or XML (as used by [30]) seem good\nchoices. For the later, the MPEG-7 xml shema [36] already\nproposes a full-range of description with inherent semantic\nand the possibility to deﬁne new semantics using Classiﬁ-\ncation Schemes (CS). Whatever choice, the deﬁnition of\nand the reference to a controlled list of labels is necessary.\nIt also allow to deﬁne the width of the description-space13.\nProviding a precise reference to the audio items being\ndescribed is also crucial. Considering that recent anno-\ntated corpora were distributed without audio media, this\n11http://www.w3.org/TR/void/\n12Consider the question of how a user will interpret the “1” label ten\nyears from now.\n13This would for example make it possible to decide whether a C-Maj\nchord is really a C-Maj or a reduction of a C-Maj7+9 chord.is clearly a major issue. Several linkage mechanisms be-\ntween annotations and audio media have been proposed\nso far: CD reference (as in Isophonic), Musicbrainz ID\n(as in the Million Song) or the EchoNest ID, Amazon ID,\n7-Digital ID. The reference should also allow referencing\ntime inside the ﬁles. The example of the alignment prob-\nlem of the Beatles annotations to the various possible audio\ninstances is notorious in the MIR community. Inclusion in\nthe annotation of time-stamped identiﬁcation, such as is\nprovided by audio-ID techniques, would help.\n(C1) Corpus ID: corpus:MIR:AIST:RWC:2006:version1.0\n(A) Raw Corpus\n(A1) Deﬁnition: (a12) created real items; 315 tracks created for\nthe speciﬁc purpose of having a copyright-free test-set for MIR\nresearch representative of the various genres, styles, instrumenta-\ntion, vocal types (see [12] for details)\n(A2) Type of media diffusion: full tracks stereo high-quality\n(B) Annotations\n(B1) Origin: (b11) synthetic—obtained during creation and\n(b15) manual annotations\n(B21) Concepts deﬁnition: only deﬁned by the annotation rules\n(B22) Annotation rules: - Standard MIDI Files (SMF) tran-\nscribed by ear, - Lyrics of songs obtained during creation, - Beat/\ndownbeat annotated using metronome clicks of recording and\nmanual editing, - Melody line annotated using fundamental fre-\nquency estimation on the melody track and manual editing, - Cho-\nrus sections method is not indicated, - Audio synchronized MIDI\nFiles using the annotated beat positions\n(B31) Annotators: a music college graduate with absolute pitch\n(B32) Validation/ reliability: not indicated\n(B4) Annotation tools: “Music Scene Labeling Editor”\n(C) Documents and Storing\n(C2) Audio identiﬁer and storage: RWC-speciﬁc audio iden-\ntiﬁers, audio ﬁles are available through audio CDs, annotations\navailable through archive ﬁles in CSV format\n(C1) Corpus ID: corpus:MIR:LastFM:Tempo:2011:version1.0\n(A) Raw Corpus\n(A1) Deﬁnition: (a13) sampled real items. Sampling method:\nsomehow uniform—4006 tracks chosen “essentially at random”\namong several thousands\n(A2) Type of media diffusion: 30s extract of music items\n(B) Annotations\n(B1) Origin: (b13) Experiment and (b14) Crowd-Sourcing\n(B21) Concepts deﬁnition: the concepts are deﬁned by the re-\nsults of the experiments, itself deﬁned by the instructions pro-\nvided to the annotators: ”tap along to each except”, ”describe\nits speed on a three point scale”, compare two tracks in terms of\nspeed.\n(B22) Annotation rules: deﬁned by the experiment protocol\n(see [20] for details)\n(B31) Annotators: 2141 users of Last-FM (not all tracks are\nannotated by all the annotators)\n(B32) Validation/ reliability: for each track, all the annotations\nare provided, it is let to the user of the corpus to compute inter-\nannotator agreement\n(B4) Annotation tools: Web-interface\n(C) Documents and Storing\n(C1) Audio identiﬁer and storage: no audio identiﬁers are\nprovided (except the artist, album and track name); annotations\ndistributed as an archive ﬁle accessible through an URL, ﬁles in\nTSV format (Tab Separated Values File).\nTable 1. Application of the proposed description to the\ncorpus of [12, 13] and [20]3. EXAMPLES OF DESCRIPTIONS\nAs examples of the application of the proposed description,\nwe illustrate in Table 1 its use for the (short) description of\ntwo corpora [12,13] and [20]. It should be noted that these\ndescriptions are solely based on the information provided\nwith the distributed corpora and the respective publications\nand should ideally be complemented and corrected by the\nrespective authors themselves. Based on this, a compara-\ntive table of the corpora can easily be made14.\n4. CONCLUSION\nMIR should beneﬁt from the “best practices” that have\nbeen evolving for decades in the speech and natural lan-\nguage processing communities. Among these practices,\nwe attempt here to provide insights into the choices cur-\nrently made when creating a MIR annotated corpus, their\nimplications, and the resulting necessity to better describe\nthem when distributing an annotated corpus. We presented\nthem in the form of a numbered list—A*, B*, C*—to high-\nlight the fact that all of these choices must be described.\nConsidering the importance that the distribution of anno-\ntated corpora will have to the development of MIR re-\nsearch, we hope that providing this list will facilitate the\nsharing and re-use of annotated corpora.\nAckowledgements: This work was party supported by the\nQuaero Program funded by Oseo French State agency for innova-\ntion and by the MIReS project funded by EU-FP7-ICT-2011.1.5-\n287711. Many thanks to Kevin B. Cohen for proof-reading.\n5. REFERENCES\n[1] F. Schiel, C. Draxler, et al., “Production and validation of speech\ncorpora,” Bavarian Archive for Speech Signals. Munchen: Bastard\nVerlag, 2003.\n[2] A.-J. Li and Z.-g. Yin, “Standardization of speech corpus,” Data Sci-\nence Journal, vol. 6, no. 0, pp. 806–812, 2007.\n[3] M. Wynne, ed., Developing Linguistic Corpora: a Guide to Good\nPractice. Oxford: Oxbow Books, 2005.\n[4] G. Tzanetakis and P. Cook, “Musical genre classiﬁcation of audio\nsignals,” IEEE Trans. on Speech and Audio Processing, vol. 10, no. 5,\npp. 293–302, 2002.\n[5] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte, S. Klozali,\nD. Tidhar, and M. Sandler, “OMRAS2 metadata project 2009,” in\nProc. of ISMIR (Late-Breaking News), (Kobe, Japan), 2009.\n[6] C. Yeh, N. Bogaards, and A. Roebel, “Synthesized polyphonic music\ndatabase with veriﬁable ground truth for multiple f0 estimation,” in\nProc. of ISMIR, pp. 393–398, 2007.\n[7] T. Fujishima, “Realtime chord recognition of musical sound: a sys-\ntem using common lisp music,” in Proc. of ICMC, (Beijing, China),\npp. 464–467, 1999.\n[8] F. Opolko and J. Wapnick, “McGill university master samples CD-\nROM for SampleCell VOLUME 1,” 1991.\n[9] G. Ballet, R. Borghesi, P. Hoffman, and F. L ´evy, “Studio online 3.0:\nAn internet “killer application” for remote access to ircam sounds\nand processing tools,” in Proc. of JIM, (France), 1999.\n[10] O. Gillet and G. Richard, “Enst-drums: an extensive audio-visual\ndatabase for drum signals processing,” in Proc. of ISMIR, pp. 156–\n159, 2006.\n[11] V . Emiya, R. Badeau, and B. David, “Multipitch estimation of pi-\nano sounds using a new probabilistic spectral smoothness princi-\nple,” IEEE Transactions on Audio, Speech, and Language Process-\ning, vol. 18, no. 6, pp. 1643–1654, 2010.\n14It could not be included due to space constraints.[12] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, “Rwc music\ndatabase: Popular, classical, and jazz music databases,” in Proc. of\nISMIR, (Paris, France), pp. 287–288, 2002.\n[13] M. Goto, “Aist annotation for the rwc music database,” in Proc. of\nISMIR, (Victoria, Canada), pp. 359–360, 2006.\n[14] J. A. Burgoyne, J. Wild, and I. Fujinaga, “An expert ground-truth set\nfor audio chord recognition and music analysis,” in Proc. of ISMIR,\n(Miami, USA), 2011.\n[15] F. Bimbot, E. Deruty, S. Gabriel, and E. Vincent, “Methodology\nand resources for the structural segmentation of music pieces into\nautonomous and comparable blocks,” in Proc. of ISMIR, (Miami,\nUSA), 2011.\n[16] M. Ramona, S. Fenet, R. Blouet, H. Bredin, T. Fillon, and G. Peeters,\n“A public audio identiﬁcation evaluation framework for broadcast\nmonitoring,” Journal of Experimental and Theoretical Artiﬁcial In-\ntelligence (Special Issue on Event Recognition), 2012.\n[17] D. Moelants and M. F. McKinney, “Tempo perception and musi-\ncal content: What makes a piece slow, fast, or temporally ambigu-\nous?,” in International Conference on Music Perception and Cogni-\ntion, Evanston, IL, 2004.\n[18] T. Bertin-Mahieux, D. Ellis, B. Whitman, and P. Lamere, “The mil-\nlion song dataset,” in Proc. of ISMIR, (Miami, USA), 2011.\n[19] M. McVicar and T. De Bie, “Enhancing chord recognition accuracy\nusing web resources,” in Proc. of the 3rd international workshop on\nMachine learning and music, pp. 41–44, ACM, 2010.\n[20] M. Levy, “Improving perceptual tempo estimation with crowd-\nsourced annotations,” in Proc. of ISMIR, (Miami, USA), 2011.\n[21] Y . E. Kim, E. Schmidt, and L. Emelle, “Moodswings: A collaborative\ngame for music mood label collection,” in Proc. of the International\nSymposium on Music Information Retrieval, pp. 231–236, 2008.\n[22] D. Turnbull, R. Liu, L. Barrington, and G. Lanckriet, “A game-based\napproach for collecting semantic annotations of music,” in Proc. of\nISMIR, (Vienna, Austria), 2007.\n[23] E. L. M. Law, L. v. Ahn, R. Dannenberg, and M. Crawford,\n“TagATune: A game for music and sound annotation,” in Proc. of\nISMIR, (Vienna, Austria), 2007.\n[24] M. I. Mandel and D. P. W. Ellis, “A web-based game for collect-\ning music metadata,” in Journal of New Music Research , vol. 37,\npp. 151–165, Taylor & Francis, 2008.\n[25] C. Harte, M. Sandler, S. Abdallah, and E. Gomez, “Symbolic repre-\nsentation of musical chords: A proposed syntax for text annotations,”\ninProc. of ISMIR, (London, UK), pp. 66–71, 2005.\n[26] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. De Roure, and J. S.\nDownie, “Design and creation of a large-scale database of structural\nannotations,” in Proc. of ISMIR, (Miami, USA), 2011.\n[27] K. B. Cohen, L. Fox, P. V . Ogren, and L. Hunter, “Corpus design for\nbiomedical natural language processing,” in Proc. of the ACL-ISMB\nworkshop on linking biological literature, ontologies and databases:\nmining biological semantics, pp. 38–45, 2005.\n[28] G. Leech, Developing Linguistic Corpora: a Guide to Good Prac-\ntice, ch. Adding Linguistic Annotation, pp. 17–29. Oxford: Oxbow\nBooks, 2005.\n[29] K. Fort and V . Claveau, “Annotating football matches: Inﬂuence of\nthe source medium on manual annotation,” in Proc. of LREC, (Istan-\nbul, Turkey), May 2012.\n[30] E. Peiszer, T. Lidy, and A. Rauber, “Automatic audio segmentation:\nSegment boundary and structure detection in popular music,” in Proc.\nof LSAS (Learning the Semantics of Audio Signals), (Paris, France),\n2008.\n[31] J. Cohen, “A coefﬁcient of agreement for nominal scales,” Educa-\ntional and Psychological Measurement, vol. 20, no. 1, pp. 37–46,\n1960.\n[32] K. Fort, C. Franc ¸ois, O. Galibert, and M. Ghribi, “Analyzing the im-\npact of prevalence on the evaluation of a manual annotation cam-\npaign,” in Proc. of LREC, (Istanbul, Turkey), May 2012.\n[33] U. Gut and P. S. Bayerl, “Measuring the reliability of manual annota-\ntions of speech corpora,” in Proc. of Speech Prosody, (Nara, Japan),\npp. 565–568, 2004.\n[34] H. V oormann and U. Gut, “Agile corpus creation,” Corpus Linguis-\ntics and Linguistic Theory, vol. 4(2), pp. 235–251, 2008.\n[35] D. Kaplan, R. Iida, and T. Tokunaga, “Annotation process manage-\nment revisited,” in Proc. of LREC, pp. 365 – 366, May 2010.\n[36] MPEG-7, “Information technology - multimedia content description\ninterface - part 5: Multimedia description scheme,” 2002."
    },
    {
        "title": "Tracking Melodic Patterns in Flamenco Singing by Analyzing Polyphonic Music Recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Francisco Gómez 0001",
            "Sergio Oramas",
            "José Miguel Díaz-Báñez",
            "Joaquín Mora",
            "Francisco Escobar-Borrego",
            "Emilia Gómez",
            "Justin Salamon"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415560",
        "url": "https://doi.org/10.5281/zenodo.1415560",
        "ee": "https://zenodo.org/records/1415560/files/PikrakisGODMEGS12.pdf",
        "abstract": "The purpose of this paper is to present an algorithmic pi- peline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal pitch se- quences are extracted from the audio recordings by means of a predominant fundamental frequency estimation tech- nique; second, instances of the patterns are detected di- rectly in the pitch sequences by means of a dynamic pro- gramming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the fla- menco fandango style was performed. To this end, a num- ber of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandan- gos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strat- egy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper.",
        "zenodo_id": 1415560,
        "dblp_key": "conf/ismir/PikrakisGODMEGS12",
        "keywords": [
            "algorithmic",
            "melodic",
            "pattern",
            "audio",
            "files",
            "vocal",
            "pitch",
            "sequences",
            "fandango",
            "style"
        ],
        "content": "TRACKING MELODICPATTERNSINFLAMENCO SINGING BY\nANALYZING POLYPHONI\nC MUSICRECORDINGS\nA.Pikrakis\nUniversityofPiraeus, Greece\npikrakis@unipi.grF.G´omez, S. Oramas\nPolytechnicUniversityofMadrid,Spain\n{fmartin,soramasm }@eui.upm.es\nJ.M. D. B ´a˜nez, J. Mora,F. Escobar\nUniversityofSevilla,Spain\n{dbanez, mora, fescobar }@us.esE. G´omez,J. Salamon\nUniversitatPompeu Fabra, Spain\n{emilia.gomez,justin.salamon} @upf.edu\nABSTRACT\nThe purpose of this paper is to present an algorithmic pi-\npeline for melodic pattern detection in audio ﬁles. Our\nmethodfollowsatwo-stageapproach: ﬁrst,vocalpitchse-\nquencesare extractedfromthe audiorecordingsbymeans\nof a predominant fundamental frequency estimation tech-\nnique; second, instances of the patterns are detected di-\nrectly in the pitch sequences by means of a dynamic pro-\ngramming algorithm which is robust to pitch estimation\nerrors. In order to test the proposed method, an analysis\nof characteristic melodic patternsin the context of the ﬂa-\nmencofandangostyle was performed. To thisend,a num-\nber of such patterns were deﬁned in symbolic format by\nﬂamencoexpertsandwerelaterdetectedinmusiccorpora,\nwhich were composed of un-segmented audio recordings\ntaken from two fandango styles, namely Valverde fandan-\ngos and Huelva capital fandangos. These two styles are\nrepresentativeofthefandangotraditionandalsodifferwith\nrespect to their musical characteristics. Finally, the strat-\negy in the evaluation of the algorithm performance was\ndiscussed by ﬂamenco experts and their conclusions are\npresentedin thispaper.\n1. INTRODUCTION\n1.1 Motivationand context\nThe study of characteristic melodic patterns is relevant to\nthe musical style and this is especially true in the case of\noral traditions that exhibit a strong melodic nature. Fla-\nmenco music is an oral tradition where voice is an es-\nsential element. Hence, melody is a predominant feature\nand many styles in ﬂamenco music can be characterized\nin melodic terms. However, in ﬂamenco music the pro-\nblem of characterizing styles via melodic patterns has so\nfar received very little attention. In this paper, we study\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom useis granted without fee provided that copies are\nnotmadeordistributed forproﬁtorcommercialadvantageandthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2012 International Society for MusicInformation Retrieval.characteristic melodic patterns, i.e., melodic patterns that\nmakea givenstylerecognizable.\nIn general, it is possible to adopt two main approaches\nto the study of characteristic melodic patterns. According\nto the ﬁrst approach, music is analysed to discover cha-\nracteristic melodic patterns [2] (distinctive patterns in the\nterminologyof[2]);see,forexample,[3]forapracticalap-\nplicationofthisapproachtoﬁndingcharacteristicpatterns\nin Brahms’ string quartet in C minor. Typically, the de-\ntected patternsare assessed by musicologiststo determine\nhowmeaningfultheyare. Therefore,thistypeofapproach\nis essentially an inductive method. The second approach\nis in a certain sense complementary to the ﬁrst one: spe-\nciﬁc melodic patterns, which are known or are hypothe-\nsized to be characteristic, are tracked in the music stream.\nThe results of this type of method allow musicologists to\nstudy important aspects of the givenmusical style, e.g., to\nconﬁrm existing musical hypotheses. The techniques to\ncarry out such tracking operations vary greatly depending\non the application context, the adopted music representa-\ntion (symbolic or audio), the musical style and the avail-\nable corpora. This type of approach can be termed as de-\nductive.\nInthispaper,we adoptedthe secondapproach. Speciﬁ-\ncally,certaincharacteristicmelodicpatternswerecarefully\nselectedbyagroupofﬂamencoexpertsandweresearched\nin a corpus of ﬂamenco songs that belong to the style of\nfandango. Tracking patterns in ﬂamenco music is a chal-\nlengingtaskforanumberofreasons. Firstofall,ﬂamenco\nmusic is usually only available as raw audio recordings\nwithout any accompanyingmetadata. Secondly, ﬂamenco\nmusic uses intervals smaller than a half-tone and is not\nstrict with tuning. Furthermore, due to improvisation, a\ngiven abstract melodic pattern can be sung in many dif-\nferent ways, sometimes undergoing dramatic transforma-\ntions, and still be considered the same pattern within the\nﬂamenco style. These facts obviously increase the com-\nplexityofthe melodysearchoperationanddemandforin-\ncreasedrobustness.\nPreliminaryworkondetectingornamentationinﬂamen-\nco music was carried out in [6], where a number of pre-\ndeﬁned ornaments were adapted from classical music and\nwerelookedupinaﬂamencocorpusof ton´asstyles. In[9]a melodic study of ﬂamenco a cappella singing styles was\nperformed.\n1.2 G\noals\nTwo main goals were established for this work: the ﬁrst\nonewasoftechnicalnature-transcriptionofmusicandlo-\ncationofmelodicpatterns-,andthesecondoneofmusico-\nlogical nature -the study of certain characteristic patterns\noftheValverdefandangostyle.\nFrom an algorithmic perspective, two major problems\nhad to be addressed. The ﬁrst problem was related to the\ntranscriptionofmusic,sinceﬂamencoisanoralmusictra-\ndition and transcriptions are meagre. In addition, our cor-\npus consisted of audio recordingsthat containedboth gui-\ntar and voice and predominant melody (pitch) estimation\nwas appliedin orderto extract the singing voice. The out-\nputofthisprocessingstagewasasetofpitchcontoursrep-\nresenting the vocal lines in the recordings. Note that even\nthoughweuseastate-of-the-artalgorithm,theseslineswill\nstill contain estimation errors, and our algorithm must be\nable to cope with them. The second problem was related\nto the fact that the patterns to be detected were speciﬁed\nby ﬂamencoexpertsin an abstract (symbolic)way andwe\nhad to locate the characteristic patternsdirectly on the ex-\ntracted pitch sequences. To this end, we developeda trac-\nkingalgorithmthatoperatesonaby-examplebasisandex-\ntendsthecontext-dependentdynamictimewarpingscheme\n[10],whichwasoriginallyproposedforpre-segmenteddata\nin thecontextofwindinstruments.\nMusicologicallyspeaking,thegoalwastoexaminecer-\ntain melodic patterns as to being characteristic of the Val-\nverde fandango style. Those patterns were speciﬁed in a\nsymbolic, abstract way and were detected in the corpus.\nBoththepatternitselfanditslocationwereimportantfrom\na musicological point of view. The tracking results were\nreviewed and assessed by a number of ﬂamenco experts.\nThe assessment was carried out with respect to a varying\nsimilarity threshold that served as means to ﬁlter the re-\nsults returned by the algorithm. In general, the subjective\nevaluation of the results (experts’ opinion) was consistent\nwith thealgorithmicoutput.\n2. THEFANDANGOSTYLE\nFandango is one of the most fundamental styles in ﬂa-\nmenco music. In Andalusia, there are two main regions\nwherefandangohasmarkedmusicalcharacteristics: Malaga\n(verdialesfandangos)andHuelva(Huelvafandangos).\nVerdiales fandangos are traditional folk cantesrelated\nto dance and a particular sort of gathering. The singing\nstyle ismelismaticandﬂowingat thesametime [1].\nHuelva fandangos are usually sung in accompaniment\nwith a guitar. The oldest references about Huelva fan-\ndangos date back to the second half of the XIX century.\nAt present, Huelva fandangos are the most popular ones\nand display a great number of variants. They can be clas-\nsiﬁed based on the following criteria: (1) Geographical\norigin: from the mountains (Encinasola), from And´ evalo(Alosno), from the capital (Huelva capital fandango); (2)\nTempo: fast (Cala˜ nas), medium (Santa Barbara), or slow\n(valientes from Alosno); (3) Origin of tradition: village\n(Valverde), or personal, i.e., fandangos that are attributed\nto importantsingers(Rebollo and otherimportantsingers,\nfor example). More information on the different styles of\nfandangocanbefoundin[7].\nFrom a musicological perspective, all fandangos have\na common formal and harmonic structure which is com-\nposed of an instrumental refrain in ﬂamenco mode (major\nPhrygian) and a sung verse or coplain major mode. The\ninterpretation of fandangos can be closer to the folkloric\nstyle, or to the ﬂamenco style, with predominant melis-\nmas and greater freedom in terms of rhythm. The reader\nmay refer to [5] for further information on their musical\ndescription.\nThe study of the fandangos of Huelva is of particular\ninterest for the following reasons: (1) Identiﬁcation of the\nmusical processes that contribute to the evolution of folk\nstylestoﬂamencostyles;(2)Deﬁnitionofstylesaccording\ntotheirmelodicsimilarity;(3)Identiﬁcationofthemusical\nvariablesthatdeﬁneeachstyle;thisincludesthediscovery\nofmelodicandharmonicpatterns.\n3. THECHARACTERISTIC PATTERNS OF\nFANDANGOSTYLES\nPatterns heardin the exposition(the initial presentationof\nthethematicmaterial)arefundamentaltorecognizingfan-\ndango styles. The main patterns identiﬁed in the Valverde\nfandangostyleareshowninFigure1(chordsshowninFig-\nure 1 are playedbythe guitar; pitchesarenotatedas inter-\nvals from the root) . These patterns are named as follows:\nexp-1,exp-2,exp-4, andexp-6. The number in the name\nof the pattern refers to the phrase in which it occursin the\npiece.\nPatternexp-1is composed of a turn-like ﬁgure around\nthe tonic. Pattern exp-2basically goes up by a perfect\nﬁfth. First,themelodyinsistsontheBﬂat,makesaminor-\nsecondmordent-likemovement,and thenrises with a leap\nof a perfect fourth. Pattern exp-4is a fall from the tonic\nto the fourth degree by conjunct degrees followed by an\nascending leap of a fourth. Pattern exp-6is a movement\nfrom B ﬂat to the tonic. Again, the B ﬂat is repeated,then\nit goes down by a half-tone and raises to the tonic with\nan ascending minor third. The rhythmic grouping of the\nmelodic cell is ternary (three eighth notes for B ﬂat and\nthreeeighthnotesforA).\nAgain, notice that this is a symbolic description of the\nactual patterns heard in the audio ﬁles. Any of these pat-\nterns may undergo substantial changes in terms of dura-\ntion, sometimes even in pitch, not to mention timbre and\notherexpressivefeatures.\n3.1 The CorpusofFandango\nThe corpus of our study was provided by Centro andaluz\ndeﬂamencodelaJuntadeAndaluc ´ıa,anofﬁcialinstitution\nwhose mission is the preservation of the cultural heritageFigure1. CharacteristicpatternsintheValverdefandango\nstyle.\nofﬂamen\ncomusic. Thisinstitutionpossessesaround1200\nfandangos, from which 241 were selected. The selection\nwas based on the following four criteria: (1) Audio ﬁles\nmust contain guitar and voice; (2) Audio ﬁles are of ac-\nceptablerecordingqualitytopermitautomaticprocessing;\n(3)FandangosmustbeinterpretedbysingersfromHuelva\nor acknowledged singing masters; (4) The time span of\nthe recordings must be broad and in our case it covers six\ndecades,from1950to 2009.\nThe corpus was gathered for the purposes of a larger\nproject that aims at investigating fandango in depth. The\nsample under study is broadly representativeof styles and\ntendencies over time. The current paper is an attempt to\nstudy 60 fandangos in total (30 Valverde fandangos and\n30 Huelva capital fandangos). In this experimental setup\nwe excluded Valientes of Huelva fandangos, Valientes de\nAlosnofandangos,Cala˜ nasfandangos,andAlmonasterfan-\ndangos. AllrecordingswereavailableinPCM(wav)single-\nchannelformat,witha16bit-depthpersampleand44kHz\nsamplingrate.\n4. COMPUTATIONALMETHOD\n4.1 AudioFeatureExtraction\nAsmentionedearlier,writtenscoresinﬂamencomusicare\nscattered and scant. This can be explained to some ex-\ntent bythefact that ﬂamencomusicis basedonoraltrans-\nmission. Issues related to the most appropriate transcrip-\ntionmethodhavebeenquitecontroversialinthecontextof\nthe ﬂamenco community. Some authors, like Hurtadoand\nHurtado [8], are in favour of Western notation, whereasothers propose different methods, e.g., Donnier [4], who\nadvocates the use of plainchant neumes. In view of this\ncontroversy,we adopteda more technical approachthat is\nbasedonaudiofeatureextraction.\nWenowdescribehowtheaudiofeatureextractionalgo-\nrithmoperates. Ourgoalwastoextractthevocallineinan\nappropriate, musically meaningful format that would also\nserveasinputtothepatterndetectionalgorithm. Theaudio\nfeature extractionstage was mainlybased on predominant\nmelody(fundamentalfrequency,fromnowon F0)estima-\ntion from polyphonic signals. For this, we used the state-\nof-theartalgorithmproposedbySalamonandG´ omez[11].\nTheir algorithm is composed of four blocks. First, they\nextract spectral peaks from the signal by taking the local\nmaxima of the short-time Fourier transform. Next, those\npeaksareusedtocomputeasaliencefunctionrepresenting\npitchsalience overtime. Then,peaksofthesalience func-\ntionaregroupedovertime toformpitchcontours. Finally,\nthe characteristics of the pitch contours are used to ﬁlter\nout non-melodic contours, and the melody F0sequence\nis selected from the remaining contours by taking the fre-\nquency of the most salient contour in each frame. Further\ndetailscanbefoundin [11].\n4.2 PatternRecognitionMethod\nThepatterndetectionmethodusedinthispaperbuildsupon\nthe “Context-Dependent Dynamic Time Warping” algo-\nrithm (CDDTW) [10]. While standard dynamic time war-\nping schemes assume that each feature in the feature se-\nquence is uncorrelated with its neighboring ones (i.e. its\ncontext), CDDTW allows for grouping neighboring fea-\ntures (i.e. forming feature segments) in order to exploit\npossible underlying mutual dependence. This can be use-\nfulinthecase ofnoisypitchsequences,becauseitpermits\ncanceling out several types of pitch estimation errors, in-\ncluding pitch halving or doublingerrors and intervals that\nare broken into a sequence of subintervals. Furthermore,\nin the case of melismatic music, the CDDTW algorithm\nis capable of smoothing variations due to the improvisa-\ntional style of singers or instrument players. For a more\ndetailed study of the CDDTW algorithm, the reader is re-\nferredto [10].\nA drawback of CDDTW is that does not take into ac-\ncount the duration of music notes and focuses exclusively\non pitch intervals. Furthermore, CDDTW was originally\nproposedforisolatedmusicalpatterns(pre-segmenteddata).\nThe term isolated refers to the fact that the pattern that is\nmatchedagainsta prototypehasbeenpreviouslyextracted\nfrom its context by means of an appropriate segmentation\nprocedure, which can be a limitation in some real-world\nscenarios,liketheonewearestudyinginthispaper. There-\nfore, we propose here an extension to the CDDTW algo-\nrithm,that:\n•Removes the need to segment the data prior to the\napplication of the matching algorithm. This means\nthat the prototype (in our case the time-pitch repre-\nsentationoftheMIDIpattern)isdetecteddirectlyin\nthepitchsequenceoftheaudiostreamwithoutpriorsegmentation, i.e. the pitch sequence that was ex-\ntractedfromth\nefandango.\n•Takes into account the note durations in the formu-\nlationofthelocalsimilaritymeasure.\n•Permitstosearchforapatterniteratively,whichmeans\nthatmultipleinstancesofthepatterncanbedetected,\noneperiteration.\nAdetaileddescriptionoftheextensionofthealgorithm\nis beyond the scope of this paper. Instead, we present the\nbasic steps:\nStep 1: The MIDI pattern to be detected is ﬁrst con-\nvertedto atime-pitchrepresentation\nP={[f1,t1]T,[f2,t2]T,...,[fJ,tJ]T},\nwherefiis the frequency of the i-th MIDI note, mea-\nsuredincents(assumingthatthereferencefrequencyis55\nHz) andtiis the respective note duration (in seconds), for\na MIDIpatternof Jnotes.\nStep2: Similarly,thepitchsequenceoftheaudiorecord-\ningisconvertedtotheabovetime-pitchrepresentation,\nR={[r1,tr1]T,[r2,tr2]T,...,[rI,trI]T},\nwhereriis a pitch value (in cents) and triis alwaysequal\ntotheshort-termstepofthefeatureextractionstage( 10ms\nin our case), for an audio recording of Inotes. In other\nwords, even if two successive pitch values are equal, they\nare still treated as two successive events, each of which\nhas a duration equal to the short-term step of the feature\nextraction stage. This approach was adopted to increase\ntheﬂexibilityofthedynamictimewarpingtechniqueatthe\nexpense of increased computational complexity. For the\nsakeofuniformityofrepresentation,eachtimeintervalthat\ncorresponds to a pause or to a non-vocal part is inserted\nas a zero-frequencynote and is assigned a respective time\nduration.\nStep 3: Sequences RandPare placed on the vertical\nand horizontal axis of a similarity grid, respectively. The\nCDDTW algorithm is then applied on this grid, but, this\ntime,thecosttoreachnode (i,j)fromanallowableprede-\ncessor, say (i−k,j−1), dependsbothonthe pitchinter-\nvals and the respective note durations. More speciﬁcally,\nthe interpretation of the transition (i−k,j−1)→(i,j)\nis that the pitch intervals in the MIDI pattern and audio\nrecording are equal to fj−fj−1andri−rk−1, respec-\ntively. Note that on the y-axis, the pitch interval only de-\npends on the end nodes of the transition and not on any\nintermediate pitch values, hence the ability to cancel out\nany intermediate pitch tracking phenomena. In the same\nspirit,thetimedurationthathaselapsedonthe x−axisand\ny−axis is equal to tjand/summationtexti\ni−k+1trk, respectively. It is\nworth noticing that we do not permit omitting notes from\ntheMIDIpattern,andthereforeanyallowablepredecessor\nof(i,j)must reside in column j−1. The pitch intervals\nand respective durations are fed to the similarity functionof Eq. (1), that yields a score, S(i−k,j−1)→(i,j), for the\ntransition (i−k,j−1)→(i,j), i.e.,\nS(i−k,j−1)→(i,j)= 1−f(/summationtexti\ni−k+1trk\ntj)\n−g(ri−rk−1,fj−fj−1)(1)\nwhere\nf(x) =\n\n(1−x)1.1,1≤x≤4\n1.51.1(1−x)1.1,1\n3≤x <1\n3−6x, 0< x <1\n3\n∞, otherwise\nand\ng(x1,x2) =/braceleftbigg(\n1−x1\nx2)0.7,if0.98≤x1\nx2≤1.02\n∞,otherwise\nThe\ninterpretation of this function is that it penalizes ex-\ncessivetimewarpinganddoesnottoleratemuchdeviation\nin terms of pitch intervals. More speciﬁcally, f(x)is a\npiecewise function that operateson the basis that duration\nratios are not penalized uniformly and that any ratio out-\nside the interval [1\n3,1)should receive a stronger penalty.\nSimilarly, function g(x)implies that, taking the music in-\ntervalofthe MIDIpatternasreference,the respectivesum\nof intervals of the audio recording exhibits at most a 2%\ndeviation. The scalars involved in the formulae of f(x)\nandg(x)are the result of ﬁne-tuning with respect to the\ncorpusunderstudy. Thecomputationofthetransitioncost\nisrepeatedforeveryallowablepredecessorof (i,j). Inthe\nend, one of the predecessors is selected to be the winner\nby examining the sum of the similarity that has been gen-\nerated by the transition with the accumulated similarity at\nthepredecessor.\nStep 4: After the accumulated cost has been computed\nfor all nodes in the grid, the maximum accumulated cost\nis selected and normalized and, if it exceeds a predeﬁned\nthreshold,astandardbacktrackingprocedurerevealswhich\npartoftheaudiorecordinghasbeenmatchedwiththepro-\ntotype;otherwise,thealgorithmterminates.\nStep 5: All nodes in the best path are marked as stop-\nnodes, i.e. forbidden nodes and Steps 1-4 are repeated in\nordertodetectasecondoccurrenceoftheprototypeandso\non,dependingonhowmanypatterns(atmost)theuserhas\nrequestedtobe detected.\n5. EVALUATION\n5.1 Methodology\nFour different exposition patterns were deﬁned by the ex-\nperts, whichare distinctiveofthe Valverdestyle. The Val-\nverdefandangohas6expositionphrasesineach copla(sung\nverse), where 1, 3 and 5 are usually the same pattern, and\n2, 4 and6 have differentpatternseach. Therefore,4expo-\nsition patterns(1,2, 4, and6) werechosento be put to thetest. Again, we insist that these patterns are abstract rep-\nresentationso\nftheactualpatternsheardintheaudiorecor-\ndings. Ouralgorithmwasthenruntolocatethosefourpat-\nterns in the corpus of Valverde fandangos and Huelva ca-\npital fandangos. Therefore, our ground-truth in this study\nconsistsofallthemelodicpatternsplustheirspeciﬁcloca-\ntions. For example, exposition pattern 1 has to be located\n90 times, as it occurs three times in each of the 30 pieces\nthat make up the corpusof the Valverdefandangos. If this\npattern is found elsewhere (not in the exposition phrase),\nthen it will be considered as a true negative. Once the re-\nsultsoftheexperimentswereobtained,theyweremanually\nchecked by the ﬂamenco experts, both in terms of pattern\noccurrenceandrespectiveposition.\n5.2 Results\nResults are summarized in Tables 1 and 2 with respect\nto the similarity threshold,whichis a user-controlledvari-\nable. Oncethethresholdissettoaspeciﬁcvalue,thealgo-\nrithm ﬁlters out any patterns whose similarity score does\nnot exceed the threshold. In our study, we experimented\nwith values of the similarity threshold ranging from 30%\nto80%. In Table 1, Testands for the total number of ex-\npected occurrences of each pattern in the corpus of Val-\nverdefandangos(basedonthegroundtruththatisprovided\nbythemusicologicalknowledge), Tfisthetotalnumberof\ndetected instances (both true and false), Tpis the number\nof true positives, Fpis the number of false positives, and\nPrec.,Rec.andFare the values of precision, recall and\ntheF-measure, respectively. In Table 2, we focus on the\ncorpusofHuelvafandangos.\nFigure 2 shows the average F-measure (over all pat-\nterns) as a functionof similarity threshold. The maximum\nvalueisobtainedatthreshold 50%.\nFigure 2. Average F-measure (ove r all patterns) with re-\nspect tothesimilaritythreshold.\nNext, we attempt to detect the Valverde patterns in the\nHuelva collection. Hence, one would expect that it would\nbe otiose to reproducecomputationslike those in Table 1,\nasthetotalexpectednumberofoccurrenceswouldbezero.\nHowever, Table 2 summarizes the detection results in the\ncorpus of the Huelva capital fandangos for the four expo-Valverdefandangos\nSim.TeTfTpFpPrec. Rec.FExp-130%903832684% 36%0.5\n40%903632489% 36%0.5\n50%903130197% 33%0.49\n60%902524196% 27%0.41\n70%9015150100% 17%0.28\n80%90660100% 7%0.12Exp-230%3013130100% 43%0.6\n40%3013130100% 43%0.6\n50%3013130100% 43%0.6\n60%3013130100% 43%0.6\n70%30770100% 23%0.37\n80%30110100% 3%0.06Exp-430%3027111641% 37%0.38\n40%3026111542% 37%0.39\n50%3021111052% 37%0.43\n60%30169756% 30%0.39\n70%30116554.5%20%0.29\n80%30330100% 10%0.18Exp-630%3034142041% 47%0.43\n40%3031141745% 47%0.46\n50%3027131448% 43%0.45\n60%301510566.6%33%0.44\n70%30880100% 27%0.42\n80%30330100% 10%0.18\nTable 1. ExperimentalresultsforValverdefandangos.\nsitionpatter\nnsunderstudyandwemakeanattempttopro-\nvideaninterpretationofthedetectedoccurrences.\nHuelvacapitalfandangos\n30%40%50%60%70%80%\nExp-1 741100\nExp-2 100000\nExp-4 2927231784\nExp-6 2722191795\nTable 2. Experimental results for the Huelva capital fan-\ndangos.\nOveral\nl,fromaquantitativepointofview,thealgorithm\nhasexhibitedareasonablygoodperformanceinﬁndingthe\npatterns in the melody, despite the problems posed by the\npolyphonicsource, the highly melismatic content, and the\nnote-durationvariation. Regardingperformancemeasures,\non the one hand, precision is quite high, but, on the other\nhand, recall is low. Most of the values of F-measure are\naround0.3−0.45,withafewisolatedexceptions. Inother\nwords,thealgorithmiscapableofdetectingwell localized\noccurrencesof the patterns,but fails to locate a signiﬁcant\nnumber of occurrences. The best performance of the F-\nmeasureoccurswitha thresholdof 50%.\nFromaqualitativepointofview,wemakethefollowing\nremarks.\nExp-1: This pattern is the expositionof the ﬁrst phrase\nofthefandango. Interestinglyenough,notonlydoestheal-gorithmdetectthepatterncorrectlyintheﬁrstphraseofthe\nValverde fanda\nngo, but also in other phrases, as expected.\nIndeed, it identiﬁes the pattern as a leit-motiv throughout\nthe piece. This pattern was detected only a few times by\nthealgorithminthe Huelvacapitalfandangos.\nExp-2: This is the pattern of the second exposition ph-\nrase in Valverde fandangos. This is the musical passage\nwith the amplest tessitura. The algorithm detects it with\nhighprecisionintheValverdecorpus(evenforasimilarity\nthresholdequalto 30%),andveryfewmatchesareencoun-\nteredinthe Huelvacapitalfandangos.\nExp-4: In the Valverde corpus, for a threshold equal\nto80%, the algorithm only detects the pattern in cantes\nsung by women who have received music training in ﬂa-\nmenco clubs in Huelva. These clubs are called pe˜nas ﬂa-\nmencasand organizesinging lessons. Women from pe˜nas\nare trained to follow very standard models of singing and\ntherefore do not contribute to music innovation like other\nfandango performerse.g., Toronjo or Rengel). For a 70%\nsimilarity threshold (and below), the pattern is also de-\ntectedin thevoicesofwell-knownfandangosingers.\nIntheHuelvacapitalfandangocorpusthispatternisfre-\nquentlydetectedbythealgorithminthetransitionbetween\nphrases. Note that we can state that the pattern is there,\nmoreorless blurredorstretched,butit ispresent,so these\narenotconsideredtobefalse positives.\nExp-6: Thispatternisusedtopreparetheﬁnalcadence\nof the last phrase. In the Valverde corpus, irrespective of\nthe similarity level, the algorithm returns correct results,\nalthough as stated above, many occurrences fail to be de-\ntected. IntheHuelvacapitalcorpusandwhenthethreshold\nislow,thealgorithmdetectsthepatternintheﬁrst,themid-\ndle and the ﬁnal section. When the threshold is raised to\n80%,it isonlylocatedintheﬁnal cadence.\n6. CONCLUSIONS\nIn this paper we presented an algorithmic pipeline to per-\nform melodic pattern detection in audio ﬁles. The overall\nperformanceofourmethoddependsbothonthequalityof\nthe extracted melody and the precision of the tracking al-\ngorithm. Ingeneral,thesystem’sperformance,in termsof\nprecision and recall of detected patterns, was measured to\nbe satisfactory, despite the great amount of melismas and\nthe high tempo deviation. From a musicological perspec-\ntive, we carried out a study of fandango styles by means\nofanalyzingarchetypalmelodicpatterns. Asalreadymen-\ntioned, written scores are not in general available for ﬂa-\nmenco music. Therefore, our approach was to design a\nsystem that operateddirectly on raw audio recordingsand\ncircumventedthe need for a transcriptionstage. In the fu-\nture,ourstudycouldbeextendedtootherHuelvafandango\nstyles. A more ambitious goal would be to carry out the\nanalysis for the whole corpus of fandango music. Also,\nothermusicalfeaturescouldbetakenintoaccountandthus\nperform a more general analysis, i.e., embrace more than\nwhatmelodicdescriptorscanoffer.7. ACKNOWLEDGEMENTS\nThe authors would like to thank the Centro Andaluz del\nFlamenco, Junta de Andaluc ´ıa1for providing the music\ncollection. ThisworkhasbeenpartiallyfundedbyAGAUR\n(mobility grant), the COFLA project2(P09 - TIC - 4840\nProyecto de Excelencia, Junta de Andaluc´ ıa) and the Pro-\ngrama de Formaci´ on del Profesorado Universitario of the\nMinisteriodeEducaci´ ondeEspa˜ na.\n8. REFERENCES\n[1] M. A. Berlanga. Bailes de candil andaluces y ﬁesta\nde verdiales. Otra visi ´on de los fandangos . Colecci´ on\nmonograf´ ıas.Diputaci´ ondeM´ alaga,2000.\n[2] D.Conklin.Discoveryofdistinctivepatternsinmusic.\nIntelligentDataAnalysis ,14(5):547–554,2010.\n[3] D. Conklin. Distinctive patterns in the ﬁrst movement\nofBrahms’stringquartetinCminor. JournalofMath-\nematicsandMusic , 4(2):85–92,2010.\n[4] P. Donnier. Flamenco: elementospara la transcripci´ on\ndel cante y la guitarra. In Proceedings of the II-\nIrd Congress of the Spanish EthnomusicologySociety,\n1997.\n[5] Lola Fern´ andez. Flamenco Music Theory. Acordes\nConcert,Madrid,Spain,2004.\n[6] F. G´ omez, A. Pikrakis, J. Mora, J.M. D´ ıaz-B´ a˜ nez,\nE. G´ omez, and F. Escobar. Automatic detection of\nornamentation in ﬂamenco. In Fourth International\nWorkshop on Machine Learning and Music MML ,\nNIPSConference,December2011.\n[7] M. G´ omez(director). Rito y geograf´ ıa del cante ﬂa-\nmenco II . Videodisco. Madrid: C´ ırculo Digital, D.L.,\n2005.\n[8] A. Hurtado Torres and D. Hurtado Torres. La voz\nde la tierra, estudio y transcripci ´on de los cantes\ncampesinosenlasprovinciasdeJa ´enyC´ordoba.Cen-\ntroAndaluzdeFlamenco,Jerez,Spain,2002.\n[9] J.Mora,F.G´ omez,E.G´ omez,F.EscobarBorrego,and\nJ. M D´ ıaz B´ a˜ nez. Characterization and melodic simi-\nlarity of a cappella ﬂamencocantes. In Proceedingsof\nISMIR, pages 9–13, Utrecht School of Music, August\n2010.\n[10] A.Pikrakis,S.Theodoridis,andD.Kamarotos.Recog-\nnition of isolated musical patterns using context de-\npendentdynamictime warping. IEEE Transactionson\nSpeechandAudioProcessing ,11(3):175–183,2003.\n[11] J. Salamon and E. G´ omez. Melody Extraction from\nPolyphonic Music Signals using Pitch Contour Char-\nacteristics. IEEE Transactions on Audio, Speech and\nLanguageProcessing ,20(6):1759-1770,Aug.2012.\n1http://www.juntadeandalucia.es/cultura/centroandaluzﬂamenco/\n2http://mtg.upf.edu/research/projects/coﬂa"
    },
    {
        "title": "Breathy or Resonant - A Controlled and Curated Dataset for Phonation Mode Detection in Singing.",
        "author": [
            "Polina Proutskova",
            "Christophe Rhodes",
            "Geraint A. Wiggins",
            "Tim Crawford"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415872",
        "url": "https://doi.org/10.5281/zenodo.1415872",
        "ee": "https://zenodo.org/records/1415872/files/ProutskovaRWC12.pdf",
        "abstract": "This paper presents a new reference dataset of sus- tained, sung vowels with attached labels indicating the phonation mode. The dataset is intended for training com- putational models for automated phonation mode detec- tion. Four phonation modes are distinguished by Johan Sun- dberg  [15]:  breathy,  neutral,  flow  (or  resonant)  and pressed. The presented dataset consists of ca. 700 record- ings of nine vowels from several languages, sung at vari- ous pitches in various phonation modes. The recorded sounds were produced by one female singer under con- trolled conditions, following recommendations by voice acoustics researchers. While datasets on phonation modes in speech exist, such resources for singing are not available. Our dataset closes this gap and offers researchers in various discip- lines a reference and a training set. It will be made avail- able online under Creative Commons license. Also, the format of the dataset is extensible. Further content addi- tions and future support for the dataset are planned.",
        "zenodo_id": 1415872,
        "dblp_key": "conf/ismir/ProutskovaRWC12",
        "keywords": [
            "sustained",
            "sung",
            "vowels",
            "phonation",
            "mode",
            "dataset",
            "phonation",
            "mode",
            "detection",
            "Creative Commons"
        ],
        "content": "BREATHY OR RESONANT  – A CONTROLLED AND CUR-\nATED DATASET FOR PHONATION MODE DETECTION IN \nSINGING \nPolina Proutskova Christophe Rhodes Geraint Wiggins Tim Crawford\nGoldsmiths, University of \nLondon\nproutskova@\ngooglemail.comGoldsmiths, University of \nLondon\nc.rhodes@gold.ac.ukQueen Mary, Univer-\nsity of London\ngeraint.wiggins\n@eecs.qmul.ac.ukGoldsmiths, Univer-\nsity of London\nt.crawford\n@gold.ac.uk\nABSTRACT\nThis paper presents a new reference dataset of sus-\ntained, sung vowels with attached labels indicating the \nphonation mode. The dataset is intended for training com-\nputational models for automated phonation mode detec-\ntion. \nFour phonation modes are distinguished by Johan Sun-\ndberg  [15]:  breathy,  neutral,  flow  (or  resonant)  and \npressed. The presented dataset consists of ca. 700 record-\nings of nine vowels from several languages, sung at vari-\nous pitches in various phonation modes. The recorded \nsounds were produced by one female singer under con-\ntrolled conditions, following recommendations by voice \nacoustics researchers. \nWhile datasets on phonation modes in speech exist, \nsuch resources for singing are not available. Our dataset \ncloses this gap and offers researchers in various discip-\nlines a reference and a training set. It will be made avail-\nable online under Creative Commons license. Also, the \nformat of the dataset is extensible. Further content addi-\ntions and future support for the dataset are planned.\n1. MOTIV ATION: NARROW, WIDE, BREATHY, \nRESONANT SINGING IN V ARIOUS \nDISCIPLINES\nPhonation modes play an important role in singing: they \nare an essential characteristic of a singing style – all mu-\nsical traditions have cultural preferences for the use of \none or more phonation modes; they are used as a means \nfor  expressive  performance;  they  can  be  indicative  of \nvoice disorders; subtle changes in phonation mode pro-\nduction are used routinely by singing teachers to determ-\nine the progress of a student.\nJohan Sundberg in his seminal work “The Science Of \nThe  Singing  V oice”  identifies  four  different phonation \nmodes in singing: breathy, neutral, flow (called resonant \nby other authors) and pressed [15]. To illustrate the differ-\nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2012 International Society for Music Information Retrieval \nences between phonation modes let us bring some well-\nknown examples.\nBreathy vocalisation is used skillfully by jazz and pop-\nular music singers to express qualities like sweetness or \nsexuality: think of Marilyn Monroe's most famous per-\nformances like “I wanna be loved by you”1 or “Happy \nbirthday Mr President”2; or listen to Ella Fitzgerald's and \nLouis Armstrong's “Dream a little dream of me”3. This \nmode of vocal production can easily be distinguished by human listeners from the flow phonation mode, such as a \nresonant,  vibrating  vocalising  by  Liza  Minelli  on  her \n“New York, New York”\n4; and from the pressed phonation, \ne.g. the tense, forceful voice of James Brown in “I feel good”\n5.\nAt the same time, Liza Minelli can be quite forceful; \nElla Fitzgerald's vocals are very dominant but light and \neconomical most of the time, because flow phonation is \nnatural for her voice. All the singers mentioned above use \nflow phonation, and all of them have their personal pref-\nerences for varying phonation and using these variations \nas  stylistic  markers  or  as  their  individual  expressive \ndevices.\n In Muslim countries of the Old High Culture a call for \nprayer can be heard five times a day from the minarets of \nthe mosques. Call for prayer is an art form and Muezzins \nare  experts  in  using  squeezed,  narrow  sound  (pressed \nphonation) in their singing which makes their perform-\nances very expressive. Their vocal technique is very dif-\nferent from a rounded, flying performance of a Western \nEuropean Gregorian chant (neutral phonation) and is at \nthe same time distant from the brassy, resonant Greek or \nRussian liturgic singing (flow phonation).6\n1http://www.youtube.com/watch?v=MLU0jndUGg4   \n2http://www.youtube.com/watch?v=k4SLSlSmW74   \n3http://www.youtube.com/watch?v=j6TmogXhOZ8   \n4http://www.youtube.com/watch?v=rgusCINe260   \n5http://www.youtube.com/watch?v=XgDrJ5Z2rKw   \n6Here, again, situation with employed phonation modes \ncan be quite ambiguous. For example, Greek Byzantine \nsingers  use  somewhat pressed,  nasalized  phonation  on higher pitches quite a lot. In fact, a Byzantine singer with BREATHY OR RESONANT  – A CONTROLLED AND CUR-\nATED DATASET FOR PHONATION MODE DETECTION IN \nSINGING \nPolina Proutskova Christophe Rhodes Geraint Wiggins Tim Crawford\nGoldsmiths, University of \nLondon\nproutskova@\ngooglemail.comGoldsmiths, University of \nLondon\nc.rhodes@gold.ac.ukQueen Mary, Univer-\nsity of London\ngeraint.wiggins\n@eecs.qmul.ac.ukGoldsmiths, Univer-\nsity of London\nt.crawford\n@gold.ac.uk\nABSTRACT\nThis paper presents a new reference dataset of sus-\ntained, sung vowels with attached labels indicating the \nphonation mode. The dataset is intended for training com-\nputational models for automated phonation mode detec-\ntion. \nFour phonation modes are distinguished by Johan Sun-\ndberg  [15]:  breathy,  neutral,  flow  (or  resonant)  and \npressed. The presented dataset consists of ca. 700 record-\nings of nine vowels from several languages, sung at vari-\nous pitches in various phonation modes. The recorded \nsounds were produced by one female singer under con-\ntrolled conditions, following recommendations by voice \nacoustics researchers. \nWhile datasets on phonation modes in speech exist, \nsuch resources for singing are not available. Our dataset \ncloses this gap and offers researchers in various discip-\nlines a reference and a training set. It will be made avail-\nable online under Creative Commons license. Also, the \nformat of the dataset is extensible. Further content addi-\ntions and future support for the dataset are planned.\n1. MOTIV ATION: NARROW, WIDE, BREATHY, \nRESONANT SINGING IN V ARIOUS \nDISCIPLINES\nPhonation modes play an important role in singing: they \nare an essential characteristic of a singing style – all mu-\nsical traditions have cultural preferences for the use of \none or more phonation modes; they are used as a means \nfor  expressive  performance;  they  can  be  indicative  of \nvoice disorders; subtle changes in phonation mode pro-\nduction are used routinely by singing teachers to determ-\nine the progress of a student.\nJohan Sundberg in his seminal work “The Science Of \nThe  Singing  V oice”  identifies  four  different phonation \nmodes in singing: breathy, neutral, flow (called resonant \nby other authors) and pressed [15]. To illustrate the differ-\nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2012 International Society for Music Information Retrieval \nences between phonation modes let us bring some well-\nknown examples.\nBreathy vocalisation is used skillfully by jazz and pop-\nular music singers to express qualities like sweetness or \nsexuality: think of Marilyn Monroe's most famous per-\nformances like “I wanna be loved by you”1 or “Happy \nbirthday Mr President”2; or listen to Ella Fitzgerald's and \nLouis Armstrong's “Dream a little dream of me”3. This \nmode of vocal production can easily be distinguished by human listeners from the flow phonation mode, such as a \nresonant,  vibrating  vocalising  by  Liza  Minelli  on  her \n“New York, New York”\n4; and from the pressed phonation, \ne.g. the tense, forceful voice of James Brown in “I feel good”\n5.\nAt the same time, Liza Minelli can be quite forceful; \nElla Fitzgerald's vocals are very dominant but light and \neconomical most of the time, because flow phonation is \nnatural for her voice. All the singers mentioned above use \nflow phonation, and all of them have their personal pref-\nerences for varying phonation and using these variations \nas  stylistic  markers  or  as  their  individual  expressive \ndevices.\n In Muslim countries of the Old High Culture a call for \nprayer can be heard five times a day from the minarets of \nthe mosques. Call for prayer is an art form and Muezzins \nare  experts  in  using  squeezed,  narrow  sound  (pressed \nphonation) in their singing which makes their perform-\nances very expressive. Their vocal technique is very dif-\nferent from a rounded, flying performance of a Western \nEuropean Gregorian chant (neutral phonation) and is at \nthe same time distant from the brassy, resonant Greek or \nRussian liturgic singing (flow phonation).6\n1http://www.youtube.com/watch?v=MLU0jndUGg4   \n2http://www.youtube.com/watch?v=k4SLSlSmW74   \n3http://www.youtube.com/watch?v=j6TmogXhOZ8   \n4http://www.youtube.com/watch?v=rgusCINe260   \n5http://www.youtube.com/watch?v=XgDrJ5Z2rKw   \n6Here, again, situation with employed phonation modes \ncan be quite ambiguous. For example, Greek Byzantine \nsingers  use  somewhat pressed,  nasalized  phonation  on higher pitches quite a lot. In fact, a Byzantine singer with BREATHY OR RESONANT  – A CONTROLLED AND CUR-\nATED DATASET FOR PHONATION MODE DETECTION IN \nSINGING \nPolina Proutskova Christophe Rhodes Geraint Wiggins Tim Crawford\nGoldsmiths, University of \nLondon\nproutskova@\ngooglemail.comGoldsmiths, University of \nLondon\nc.rhodes@gold.ac.ukQueen Mary, Univer-\nsity of London\ngeraint.wiggins\n@eecs.qmul.ac.ukGoldsmiths, Univer-\nsity of London\nt.crawford\n@gold.ac.uk\nABSTRACT\nThis paper presents a new reference dataset of sus-\ntained, sung vowels with attached labels indicating the \nphonation mode. The dataset is intended for training com-\nputational models for automated phonation mode detec-\ntion. \nFour phonation modes are distinguished by Johan Sun-\ndberg  [15]:  breathy,  neutral,  flow  (or  resonant)  and \npressed. The presented dataset consists of ca. 700 record-\nings of nine vowels from several languages, sung at vari-\nous pitches in various phonation modes. The recorded \nsounds were produced by one female singer under con-\ntrolled conditions, following recommendations by voice \nacoustics researchers. \nWhile datasets on phonation modes in speech exist, \nsuch resources for singing are not available. Our dataset \ncloses this gap and offers researchers in various discip-\nlines a reference and a training set. It will be made avail-\nable online under Creative Commons license. Also, the \nformat of the dataset is extensible. Further content addi-\ntions and future support for the dataset are planned.\n1. MOTIV ATION: NARROW, WIDE, BREATHY, \nRESONANT SINGING IN V ARIOUS \nDISCIPLINES\nPhonation modes play an important role in singing: they \nare an essential characteristic of a singing style – all mu-\nsical traditions have cultural preferences for the use of \none or more phonation modes; they are used as a means \nfor  expressive  performance;  they  can  be  indicative  of \nvoice disorders; subtle changes in phonation mode pro-\nduction are used routinely by singing teachers to determ-\nine the progress of a student.\nJohan Sundberg in his seminal work “The Science Of \nThe  Singing  V oice”  identifies  four  different phonation \nmodes in singing: breathy, neutral, flow (called resonant \nby other authors) and pressed [15]. To illustrate the differ-\nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2012 International Society for Music Information Retrieval \nences between phonation modes let us bring some well-\nknown examples.\nBreathy vocalisation is used skillfully by jazz and pop-\nular music singers to express qualities like sweetness or \nsexuality: think of Marilyn Monroe's most famous per-\nformances like “I wanna be loved by you”1 or “Happy \nbirthday Mr President”2; or listen to Ella Fitzgerald's and \nLouis Armstrong's “Dream a little dream of me”3. This \nmode of vocal production can easily be distinguished by human listeners from the flow phonation mode, such as a \nresonant,  vibrating  vocalising  by  Liza  Minelli  on  her \n“New York, New York”\n4; and from the pressed phonation, \ne.g. the tense, forceful voice of James Brown in “I feel good”\n5.\nAt the same time, Liza Minelli can be quite forceful; \nElla Fitzgerald's vocals are very dominant but light and \neconomical most of the time, because flow phonation is \nnatural for her voice. All the singers mentioned above use \nflow phonation, and all of them have their personal pref-\nerences for varying phonation and using these variations \nas  stylistic  markers  or  as  their  individual  expressive \ndevices.\n In Muslim countries of the Old High Culture a call for \nprayer can be heard five times a day from the minarets of \nthe mosques. Call for prayer is an art form and Muezzins \nare  experts  in  using  squeezed,  narrow  sound  (pressed \nphonation) in their singing which makes their perform-\nances very expressive. Their vocal technique is very dif-\nferent from a rounded, flying performance of a Western \nEuropean Gregorian chant (neutral phonation) and is at \nthe same time distant from the brassy, resonant Greek or \nRussian liturgic singing (flow phonation).6\n1http://www.youtube.com/watch?v=MLU0jndUGg4   \n2http://www.youtube.com/watch?v=k4SLSlSmW74   \n3http://www.youtube.com/watch?v=j6TmogXhOZ8   \n4http://www.youtube.com/watch?v=rgusCINe260   \n5http://www.youtube.com/watch?v=XgDrJ5Z2rKw   \n6Here, again, situation with employed phonation modes \ncan be quite ambiguous. For example, Greek Byzantine \nsingers  use  somewhat pressed,  nasalized  phonation  on higher pitches quite a lot. In fact, a Byzantine singer with While  the  term  phonation  mode  is  borrowed  from \nvoice acoustics, the differentiation between breathy and \npressed voices, between tense and open singing is opera-\ntional in many voice-related research areas: ethnomusico-\nlogy,  singing  education,  medical  research  (phoniatrics, \nvocology) as well as in linguistics (phonetics). This is \nhow an ethnomusicologist Alan Lomax describes the dif-\nference between narrow and wide vocalisation:\n“The  measure  concerns  the  contrast  between  the \nvoices which sound mellow, relaxed and richly resonant \n(we call this  wide ) and the voices which sound tense, \npinched and restricted in resonance (which we call nar-\nrow). Many singing styles can be characterized as having \none or the other; in some rare cases both may occur; and many  ways  of  vocalizing  (like  everyday  American \nspeech) are neutral in width – these we call mid, singers \nwith a “speech” tone.” [10, p. 125]. Lomax then gives a number of examples: narrow singing from Indonesia and \nThailand; wide, open singing from Eastern Europe; the \nmid mode form the US and from Ireland. \nThese examples demonstrate that breathy, pressed or \nresonant singing production can be representative of a \nsinging style or even a music culture. While each voice is \ndifferent and two singers never sing the same way, every \nmusical tradition displays cultural preferences for the use \nof particular phonation mode(s), which are imposed on \nthe singers performing in this tradition. In many cases a \nsingle phonation mode is encouraged: for example bari-\ntone singers in Western classical music are trained to sing \nin flow phonation and move through their singing career \nusing just this phonation mode. In contrast, in classical \nOttoman tradition a singer was expected to operate in all \nfour phonation modes.\nApart from being a cultural characteristic, breathy or \ntense vocalisation can be indicative of vocal disorders: \nhypofunction and hyperfunction of the glottis [5]. Their \ndiagnostics and treatment are a prime concern in the dis-\nciplines of vocology (voice habilitation) and phoniatrics \n(in case of functional or anatomic pathologies) [13]. \nWhile  in  some  examples  even  a  less  experienced \nlistener can easily distinguish between various phonation \nmodes, in other cases this distinction can be very subtle \nand requires training and expertise to be identified cor-\nrectly.  Lomax  refers  to  his  narrow  vs  wide  singing \ndescriptor (he calls this descriptor vocal width or vocal \ntension) as an “emotionally loaded quality” and thus ex-\nplains why some people have difficulties in rating it [10]. \nV ocal width is one of 36 descriptors of the Cantometrics \nsystem  –  a  global  parametrisation  of  world's  singing \nstyles. Lomax and his Cantometrics team manually rated \nmore than 5000 recordings of singing from around the \na higher range often cannot be distinguished from an Ar-\nabic singer of a similar range in terms of their phonation mode  usage.  Also,  some  Gregorian  chant  interpreters such as Ensemble Organum deliberately use flow phona-tion in their performance.world. Of all 36 descriptors vocal width appeared to be \nthe hardest to rate consistently: the inter-rater consensus \nscores for this descriptor are the lowest (see [10], p. 168). \nVictor Grauer, the co-inventor of Cantometrics, admitted \nin  personal  communication  (February  2011)  that  this \ndescriptor is the most difficult to rate.\nNaturally, voice therapists are experts in vocal produc-\ntion and could serve as experts for manual rating of phon-\nation modes. Although, in practice their  work is often \nmore tailored to the  needs of speech professionals. In \nsinging it's singing teachers/educators who have the deep-\nest operational knowledge of all the issues related to vo-\ncal  production  and  in  particular  to  phonation  modes. \nMost  singing  students  display  various  kinds  of  voice \nhypo- and/or hyperfunction during the stages of their pro-\ngress. The students' perception mechanisms are usually \nnot sufficient for self-control (in absence of any visual or \nany reliable auditory indicators). It is thus the task of the \nteacher to identify and to correct the subtlest dysfunction \non the spot, over and over again, until the student has \ngained the bodily controls needed to regulate the voice \nsource function on an automatic level. \n2. PHONATION MODES IN VOICE ACOUSTICS – \nPREVIOUS WORK \nDue to complications in terminology of narrow vs. wide singing within and across disciplines as well as to the \nsubjectivity of the distinction between phonation modes, \nwe turn to voice acoustics for objective definitions and \nphysically measurable effects. The four phonation modes \nintroduced  by  Sundberg  [15]:  breathy,  neutral,  flow \n(called resonant by other authors) and pressed are vocal \nproduction qualities resulting from the voice source (the \nvibrating vocal folds). In particular they are closely re-\nlated to glottal resistance which is defined as the quotient \nof subglottal pressure to glottal airflow. A low subglottal \npressure combined with a high glottal flow results in a \nbreathy phonation. Pressed phonation arises when a high \nsubglottal pressure is accompanied by a low glottal flow. \nThe neutral mode lies between these two extremes. The \nflow  phonation  is  characterised  by  a  lower  subglottal \npressure than in pressed mode and also by a lower adduc-\ntion force on the vocal folds. It is an economical voice \nproduction mode, because it uses much less effort than in \npressed mode gaining a similar sound level, which can be \nsignificantly higher than in a neutral mode. At the same \ntime the flow phonation allows various resonances of the \nvocal tract to be used most effectively, while the pressed \nphonation tends to restrict some of them.\nGiven the above, the four phonation modes are not dis-\ncrete states of vocal production but are rather areas in a \ncontinuum space which can be distinguished on the psy-\nchoacoustic level. This continuum is not fully ordered: \nwhile breathy and pressed modes represent its two ex-\ntremes, there are endless states between them, and flow \nphonation is a sweet spot in that continuum which optim-ises psychoacoustic values such as loudness and overtone \nrichness. As we know from singing and teaching practice, \na singer is usually capable of using one or more localities \nof this space.\nWhile the phonation mode of a singing fragment can \nonly be identified subjectively in a psychoacoustic exper-\niment, the glottal wave - the signal produced by the voice \nsource - can be measured during singing by means of a \nlaryngograph  (electroglottograph),  a  non-invasive  tool \nwhich sends a small current through the larynx and re-\ncords the changes in resistance [5, 12]. Figure 1 shows \ntypical graphs of the glottal wave in all phonation modes.\nElectroglottography can be used to measure the glottal \nwave during the singing process. If, in contrast, audio re-\ncordings of previous events are studied, this technique is \nnot applicable. \nTo analyse the sound production of the voice source in \na recording the technique called inverse filtering is often \nused: the resonances of the vocal tract are estimated from \nthe original signal and a filter is constructed to eliminate \nthem [2, 4, 8, 19]. Applying this filter to the original sig-\nnal results in an estimation of the glottal wave.\nFigure  1.  Typical  graphs of  the  glottal  wave  pulse \nfunctions in various phonation modes (from [15], p. 85, used  with  permission  of  Northern  Illinois  University \nPress)\nA number  of  publications dedicated  to detection  of \npressed  and  breathy  phonation  modes  employed \ndescriptors derived from the glottal wave such as amp-\nlitude  quotient  (AQ),  normalised  amplitude  quotient \n(NAQ) and the difference between the first two harmon-\nics (H1-H2) [3, 9, 11, 16, 19]. \nUnfortunately, all of them rely on internal datasets for \ntheir experiments which are neither well documented and controlled nor are they available to other researchers for \nbenchmarking or new studies. \nThere have been single attempts to determine domin-\nant phonation modes or typical values of glottal wave \ndescriptors  for  various  singing  styles.  For  example \nThalén  and Sundberg [18] studied Western classical mu-\nsic, pop, jazz and blues, and in a later publication Zang-ger Borch and Sundberg [1] looked at rock, pop, soul and \nSwedish dance. Both these studies worked with record-\nings by just one singer. As a starting point both studies \nwere  certainly  instructive.  Unfortunately  it  is  virtually \nimpossible to make generalisations about a musical style \nbased on samples from just one singer. At the same time \nthe methodology suggested in these papers doesn't scale \nto batch processing applications. The datasets were not \nmade available to other researchers.\nIn order to address high-level semantic questions about \nmusic like the relationship between a singing style and \nphonation modes using MIR, new approaches have to be \ndeveloped that would allow processing of large, real-life \ndata collections. One of the main obstacles for such a de-\nvelopment is a lack of reference and training datasets that \nare well documented and supported and are available to \nall researchers. \nIn this paper we present a new dataset that will close \nthis gap and will be a first step in the development of new \nscalable  methods  for  study  of  phonation  modes  in \nsinging. While we also start with recordings by only one \nsinger, the format of the dataset is extensible (see Section \n3.4). We plan to add further recordings in future as de-\nscribed in Section 4. Contributions by other researchers \nwill also be welcome.\n3. THE DATASET\n3.1 The recordings\nThe dataset consists of ca. 700 WAV files. Each file \ncontains a single recording of a sustained sung vowel. \nRecordings are of 750 sec length on average. We recom-\nmend to use 300 ms around the middle of the samples for \nanalysis - here we can guarantee a relative stability in \npitch,  intensity,  phonation  and  articulation  (beginnings \nand ends of the samples can be less stable). \nSound examples Symbols \nused in the labels\n[a:] /a/ - low front unrounded sound, like in English father , German Rat \nor in Russian там A\n[e:] /e/  -  high-mid  front  unrounded vowel, like in English get, German \nEsel, Russian местоE\n[i:] /i/ - high front unrounded, like in I\nEnglish free, German Genie, Rus-\nsian вид\n[o:] /o/ - high-mid back rounded, like \nin  German  rot,  Russian  кот, \nsomewhat  similar  to  English \ncaughtO\n[ø:] High-mid  front  rounded  vowel, like German /ö/ in schönOE\n[u:] /u/  -  high  back  rounded,  like  in \nEnglish boot, German Fuß, Russi-\nan пл угU\n[y:] High  front  rounded  sound,  like \nGerman or Turkish /ü/, e.g. in Ger-man müdeUE\n[ɨ:] High  central  unrounded  vowel, Russian /ы/ like in ты, similar to \nEnglish rosesY\n[ɛ:] Low-mid  front  unrounded,  Ger-\nman /ä/ like in  Ähre, Russian /э/ \nlike in этот, similar to [æ] in Eng-lish cat AE\nTable 1. The vowels represented in the dataset. \nPitches Modes\nA3 - G4 Breathy, neutral, flow, pressed\nG4# - C5 Breathy, neutral, pressed\nC5# - G5 Breathy, neutral\nTable 2.  This table indicates which phonation modes are \nrepresented for particular pitches in the dataset.\nThe vowel sounds represented on the recordings are \nlisted in Table 1. These sounds were sung on all pitches \non a semitone scale from A3 to G5, in every phonation \nmode given in Table 2.\n3.2 The singer\nAll the recordings were produced by one female sing-\ner.  This excludes  any  variation  that  would  necessarily \narise between singers, which is useful particularly at the \ninitial stages of classification model training and testing.\nThe singer was professionally trained, with expertise \nin Western popular and in Russian traditional singing and \na profound experience in a number of other music tradi-\ntions. \nThe singer's vocal range is approximately D3 – C6, \nwith the working range being usually limited to G3 – F5. \nAt both extreme ends of the range, phonation became un-\nreliable and they were not included into the dataset. The \nsinger's break between the modal and the head (falsetto) \nregister is around E5, thus the surrounding pitches(D5# to F5#) can also be less reliable. Still we decided to include \nvocalisation in the head register into the dataset to make \nit more representative, thus all pitches up to G5 were in-\ncluded.\nIn the head register the singer was unable to produce \npressed sounds, thus the pressed phonation mode is only \nrepresented up to the upper end of the modal register (see \nTable 2). Why this is the case seems to be an unsolved \nproblem. While this seems to be common among singers \nof various traditions in Europe and the Near East, it is un-\nclear whether in other cultures (e.g. in some East Asian \ntraditions) the singers are in fact capable of producing \npressed vocalisation in their head register. This observa-\ntion leads to the question whether the ability to use partic-\nular phonation modes on particular pitches is innate or \nontogenetic (culturally constructed).\nAlso, flow phonation could only be produced in the \nchest voice – up to A4 (recordings up to G4 retained for \nthe  dataset). Above  A4 it becomes impossible  to sing \nmost vowels in the flow mode; at the same time, the neut-\nral mode in the middle and head voice partly gains the \nqualities of the flow mode, such as intensity and richness \nin overtones, though it is very different from the chesty \nflow phonation. The singer reported from her experience \nof  teaching  Russian  traditional  singing,  which  heavily \nuses the flow mode, that this limit is typical for female \nsingers, though some exceptional performers are capable \nof producing the flow phonation at as high as C5.\nIn the lower range, at G3 and below, the opposite is the \ncase: the neutral phonation becomes more and more sim-\nilar to the flow mode – for this reason recordings below \nA3 were excluded from the dataset.\nThe singer apparently had more difficulties with some \nvowels  than  with  others  in  particular  modes.  For  ex-\nample, high front sounds like [i:] and [y:] proved to be \nharder to achieve in flow phonation.\n3.3 Recording conditions\nThe recordings were  made with Olympus LS10 linear \nPCM digital recorder. We chose 96 kHz sampling rate \nand 24 bits bit resolution in compliance with the recom-\nmendations for acoustic analysis and archiving by the In-\nternational  Association  of  Sound-  and  Audiovisual \nArchives (IASA TC-04) [6].\nThe built-in high-sensitivity, low-noise stereo micro-\nphone of Olympus LS10 is a combination of two micro-\nphone heads positioned at an 90° angle. It has an overall \nfrequency  response  20  –  44000  Hz.  In  the  frequency \nrange of 150 -3000 Hz it displays a flat frequency re-\nsponse of  ±2dB and in the range up to 20 kHz the re-\nsponse is ±5dB. \nThe lowest fundamental frequency recorded was 220 \nHz (A3) which is about ten times higher than the micro-phone's low frequency response limit. This guarantees the \nflat phase response and preserves the exact shape of the waveform – a necessary condition for applications such \nas inverse filtering [17]. \n The highest frequencies perceived by the human ear \nare about 20 kHz which is within the microphone's flat \nresponse range and is way below the half of the upper \nlimit of the microphone's frequency response. See [17] \nfor detailed instructions on the choice and positioning of \nthe microphone.\nThe recorder and the microphone were positioned ho-\nrizontally at the level of the singer's mouth, at the dis-\ntance of 50 cm as recommended by the manufacturer for \nbest voice capturing. \nThe recording session took place in a quiet room en-\nvironment. The requirement of a signal-to-noise ratio of \nat least 15 dB has been adhered to [17]. \n3.4 The labels\nThe metadata is stored in a table of a relational database, \nsee Table 3. This way of organising metadata is advant-\nageous, because it can be easily extended by further fields \nand is scalable for unlimited number of entries and rela-\ntionships.  For  example  if  we  add  recordings  by  other \nsingers, a new field indicating the singer will be intro-\nduced to the table.\nLabels were provided by the singer. They mark the \npitch, the vowel and the phonation mode the singer inten-\nded to reproduce. We also performed a listening test, ex-\ncluding all recordings where phonation was ambiguous or \nof a poor quality. \nMetadata \nfieldsID File Pitch V owel Phona-\ntion modeVersion\nexample 212 212.wav C5# AE breathy 2\nTable 3.  Metadata fields of the dataset. For vowel sym-\nbols please consult Table 1. Version is optional, it is only \nused when several recordings of the same vowel at the \nsame pitch in the same phonation mode were recorded. \nCurrently simple numeric IDs are used. In future a use of \ncontent derived IDs is planned. \n3.5 Dataset availability and license\nThe dataset will be made available for download under \nCreative Commons CC BY-NC-SA license. This license \nallows free sharing of the dataset as well as altering it or \nbuilding new work based upon it. There are following \nconditions for the use of the dataset according to this li-\ncense:\nattribution – reference the creators \nno commercial use  share  alike  –  if  you  alter,  transform  or  build upon it, you may distribute the result only under \nthe same license. \n4. FUTURE WORK  \nOur dataset is a first step in creating reference and \ntraining collections for the study of phonation mode use in singing. There are several directions in which this data-\nset can be improved and extended:\n1. To further improve recording quality for the particu-\nlar task of glottal wave estimation a professional micro-\nphone  specifically  designated  for  voice  measurements \nshould be used for the production of the recordings (LS2-\ntype microphones as specified by IEC 61094-1 and ANSI \nS1.15-1997 standards). For other experiment designs, a \ndataset with varying recording quality and recording con-\nditions could be useful.\n2. To enhance the reliability of the labels, they can be \nverified by independent experts, ideally by singing teach-\ners representing various music cultures.\n3. Electroglottograph can be used to measure the glot-\ntal  wave  at  the  singer's  glottis  during  recording.  This \nwould allow more objective judgements about the phona-\ntion mode. These measurements would also provide an \nexcellent reference for glottal wave estimations on new, \nunseen data.\n4. Alternatively, if an exact measurement of the glottal \nairflow  is  required,  an  airflow  mask  developed  by \nRothenberg can be used, which also has an advantage of \nthe low frequency limit of 0 Hz [14].\n5. The scope of the dataset can be generalised by in-\ncluding recordings of other singers, male, female as well \nas children. This would introduce inter-performer vari-\nation, which needs to be studied and is necessary to con-\nstruct  real-life  classifiers.  It  is  important  that  singers \nfrom different musical traditions are represented, because \nthe ability to utilise various phonation modes can vary \ngreatly across cultures. Ideally, a representative dataset \nwith recordings from all around the globe could be com-\npiled,  which  would  allow  to  study  the  distribution  of \nphonation mode use in singing among humans.\n6. Another way of generalisation, in particular in view \nof practical tasks of automatic phonation mode detection, \nwould be to introduce recordings by groups of singers, \nfrom small groups to large choirs. Also, recordings where \nsingers are accompanied by musical instruments could be \nincluded. \n5. CONCLUSIONSPhonation mode is an important characteristic of singing, \nplaying a vital role in many singing-related disciplines. It \nremains under-researched, one of the reasons being the \nlack of reference and training data. The dataset presented \nhere closes this gap. It is aimed at MIR researchers who \nwish to develop automated methods for phonation mode \ndetection in singing. \n6.REFERENCES\n[1] Borch, D. Z. and Sundberg, J. (2011). Some phonat-\nory and resonatory characteristics of the rock, pop, \nsoul, and swedish dance band styles of singing. J \nVoice , 25(5):532–7.\n[2] Drugman, T., Bozkurt, B., and Dutoit, T. (2012). A \ncomparative study of glottal source estimation tech-niques. Computer Speech and Language , 26:20–34.\n[3] Drugman, T., Dubuisson, T., Moinet, A., D’Aless-\nandro, N., and Dutoit, T. (2008). Glottal source es-timation robustness. In Proc. of the IEEE Interna-\ntional Conference on Signal Processing and Multi-media Applications (SIGMAP08) .\n[4] Fritzell, B. (1992). Inverse filtering. Journal of \nVoice , 6(2):111–114.\n[5] Froeschels, E. (1943). Hygiene of the voice. Arch  \nOtolaryngol. , 38(2):122–130.\n[6] Gudnason, J., Mark R.P. Thomas, D. P. E., and \nNaylor, P. A. (2012). Data-driven voice source waveform analysis and synthesis. Speech Commu-\nnication , 54:199–211.\n[7] Howard, D. M. (2010). Electrolaryngographically \nrevealed aspects of the voice source in singing. Lo-\ngopedics Phoniatrics Vocology , 35(2):81–89.\n[8] (2009). Guidelines on the Production and Preserva-\ntion of Digital Audio Objects: Standards, Recom-mended Practices and Strategies (IASA-TC 04) . \nIASA (International Association for Sound- and Au-diovisual Archives) Technical Committee, 2 edition.\n[9] Lehto, L., Airas, M., Björkner, E., Sundberg, J., and \nAlku, P. (2007). Comparison of two inverse filtering methods in parameterization of the glottal closing phase characteristics in different phonation types. J \nVoice , 21(2):138–50.\n[10] Lomax, A. (1977). Cantometrics: A Method of Mu-\nsical Anthropology (audio-cassettes and handbook) . \nBerkeley: University of California Media Extension Center.\n[11] Orr, R., Cranen, B., de Jong, F., d’Alessandro, C., \nand Scherer, K. (2003). An investigation of the parameters derived from the inverse filtering of flow and microphone signals. In Voice Quality: Func-\ntions, Analysis and Synthesis (VOQUAL ’03) . Taal-\nwetenschap Otorhinolaryngology.\n[12] Pulakka, H. (2005). Analysis of human voice pro-\nduction using inverse filtering, high-speed imaging, and electroglottography. Master’s thesis, HEL-SINKI UNIVERSITY OF TECHNOLOGY, Depart-ment of Computer Science and Engineering.\n[13] Ramig, L. O. and Verdolini, K. (1998). Journal of \nspeech, language, and hearing research. Journal of \nSpeech, Language, and Hearing Research , 41:101–\n116.\n[14] Rothenberg, M. (1973). A new inverse-filtering \ntechnique for deriving the glottal air flow waveform during voicing. The Journal of the Acoustical Soci-\nety of America , 53:1632–1645.\n[15] Sundberg, J. (1987). The science of the singing \nvoice . Illinois University Press.\n[16] Sundberg, J., Thalén, M., Alku, P., and Vilkman, E. \n(2004). Estimating perceived phonatory pressedness in singing from flow glottograms. J Voice , \n18(1):56–62.\n[17] Svec, J. G. and Granqvist, S. (2010). Guidelines for \nselecting microphones for human voice production research. American Journal of Speech-Language \nPathology , 19:356–368.\n[18] Thalén, M. and Sundberg, J. (2001). Describing dif-\nferent styles of singing: a comparison of a female singer’s voice source in \"classical\", \"pop\", \"jazz\" and \"blues\". Logoped Phoniatr Vocol , 26(2):82–93.\n[19] Walker, J. and Murphy, P. (2007). A review of glot-\ntal waveform analysis. In PROGRESS IN NONLIN-\nEAR SPEECH PROCESSING , volume 4391 of Lec-\nture Notes in Computer Science , pages 1–21. \nSpringer."
    },
    {
        "title": "Separating Presentation and Content in MEI.",
        "author": [
            "Laurent Pugin",
            "Johannes Kepper",
            "Perry Roland",
            "Maja Hartwig",
            "Andrew Hankinson"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416978",
        "url": "https://doi.org/10.5281/zenodo.1416978",
        "ee": "https://zenodo.org/records/1416978/files/PuginKRHH12.pdf",
        "abstract": "Common Western music notation is traditionally orga- nized on staves that can be grouped into systems. When multiple systems appear on a page, they are arranged from the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation doc- uments for printing requires this arrangement to be cap- tured. However, in the music notation model proposed by the Music Encoding Initiative (MEI), the hierarchy of the XML sub-tree representing the music emphasizes the content rather than the layout. Since systems and pages do not coincide with the musical content, they are encod- ed in a secondary hierarchy that contains very limited information. In this paper, we present a complementary solution for augmenting the level of detail of the layout of musical documents; that is, the layout information can be encoded in a separate sub-tree with cross-references to other elements holding the musical content. The major advantage of the proposed solution is that it enables mul- tiple layout descriptions, each describing a different vis- ual instantiation of the same musical content.",
        "zenodo_id": 1416978,
        "dblp_key": "conf/ismir/PuginKRHH12",
        "keywords": [
            "staves",
            "systems",
            "pages",
            "layout",
            "encoding",
            "MEI",
            "hierarchy",
            "content",
            "visual instantiation",
            "musical content"
        ],
        "content": "SEPARATING PRESENTATION AND CONTENT IN MEI  \nLaurent Pugin Johannes Kepper  Perry Roland \nSwiss RISM / Fribourg University \nlaurent.pugin@rism -ch.org  Edirom \nkepper@edirom.de  University of Virginia \npdr4h@eservices.virginia.edu  \n \nMaja Hartwig  \nAndrew Hankinson \nMcGill University, Schulich School of Music  \nandrew.hankinson@mail.mcgill.ca  Edirom \nmaja.hartwig@gmx.de  \nABSTRACT  \nCommon W estern music notation is traditionally orga-\nnized on staves that can be grouped into systems. When \nmultiple systems appear on a page,  they are arranged \nfrom the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation do c-\numents for printing requires this arrangement to be cap-tured. However, in the music not ation model proposed \nby the Music Encoding Initiative (MEI), the hierarchy of \nthe XML sub -tree representing the music emphasizes the \ncontent rather than the layout. Since systems and pages \ndo not coincide with the musical content, they are encod-\ned in a secondary hierarchy that contains very l imited \ninformation. In this paper, we present a complementary \nsolution for augmenting the level of detail of the layout \nof musical documents; that is, the layout info rmation can \nbe encoded in a separate sub -tree with cross -references \nto other el ements hold ing the musical content. The major \nadvantage of the proposed solution is that it enables mu l-\ntiple layout descriptions, each describing a different vi s-\nual instantiation of the same musical content. \n1. INTRODUCTION  \nCommon W estern music notation is a system made  up of \nstructured symbols organized upon a group of horizontal lines, commonly called a “staff”, which acts as a bi -\ndimensional reference system. The horizontal axis repr e-\nsents time while the vertical axis indicates pitch. Staves can be grouped into system s, where the systems contain \nconcurrent streams of musical events aligned vertically \nand where each staff encompasses a defined pitch range. \nSystems are arranged across as many pages as necessary to accommodate the musical content. When multiple sy s-\ntems ap pear on a page, multiple systems are arranged \nfrom the top of the page to the bottom, similar to par a-\ngraphs in a text document.  \nNumerous schemes have been develope d for encoding music notation [8 ]. Over the last decade, XML has been \nincreasingly used for defining encoding schemes, for e x-\nample, in the MusicXML\n1 interchange format [2]  and the \nIEEE15992 standard [6]. More recently, with a major r e-\nlease in 2010 and with the upcoming 2012 release, the music notation model proposed by the Music Encoding \nInitiati ve\n3 (MEI) has begun to take a leading role. Deve l-\noped by a community of scholars, it acts as an extens ible \nmusic document encoding framework that can be custom-ized for specific needs [5].\n1 2 3 \nFor XML encoding schemes, such as MEI, that aim to \ntake into ac count the graphical context of the notation, \nthe organization of the notation into staves, systems, and \npages often needs to be captured. Whereas a page -based \napproach will have the page at the top of the XML hiera r-\nchy, a content -based approach will place an element with \nsemantic meaning at the top of the hierarchy, relegating the visual appearance to a secondary role. Music notation itself is obviously multi -hierarchical, and both approaches \nreflect valid perspectives. However, a basic principle of XML des ign is that it requires a single hierarchy to b e-\ncome the primary ordering mechanism of the music nota-tion description. Other hierarchies inherent in  music not a-\ntion may then be implemented using alternativ e tech-\nniques such as standoff markup.  \nCurrently, MEI  emphasizes the logical content of the \nnotation. For example, in the case of CMN, it employs measures at the top of the hierarchy. Pages and systems are captured using the same milestone technique that TEI \noffers; that is, page and system breaks are repres ented by \nthe empty elements <pb/> and <sb/> respectively. It is \nfairly easy to convert between measure -based and page -\nbased hierarchies using XSLT stylesheets, analogous to \nMusicXML’s conversion between time -based and part -\nbased file organization. However,  there are additional \ncomplicating factors in the case of MEI. For example, when multiple sources are described within a single e n-\ncoding, which is a significant design goal of MEI, the \nsources do not necessarily agree with regard to page and system breaks.  Furthermore, they might use a different \n                                                             \n1 <http:// www.makemusic.com/musicxml > \n2 <http:// www.mx.dico.unimi.it > \n3 <http:// www.music -encoding.org >  \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.   \n© 2012 International Society for Music Information Retrieval    \n \nscore order or map instruments to staves differently. Even \nthe number of instruments or staves may differ between multiple sources. Although MEI is currently capable of dealing with these circumstances, the markup i s often \nverbose, repetitive, and difficult to comprehend  quickly. \nIn this paper, we present a complementary module for MEI that provides for more detailed capture of layout i n-\nformation and better separation of musical content and visual presentation. The n ext section describes the objec-\ntives pursued, followed by a section on related work. We \nthen present the module we developed for MEI and co n-\nclude the paper with remarks on future work. \n2. OBJECTIVES  \nThere are at least two use -cases that  would benefit from \na clearer separation of layout -related info rmation and the \nmusical content as propos ed in this paper. The first use -\ncase is when precise descriptions in the encoding of e x-\nisting source materials are required. A typical ex ample is \nthe use of MEI as an output of and archival format for \noptical music recogn ition (OMR) software applications \n[4]. In such a use, it is necessary to be able to record the \nexact position of the elements on the page. In OMR tra n-\nscriptions, each note, each music symbol, but also each \nstaff and each system requires its coord inate to be stored \nin the MEI encoding. Diplomatic transcriptions with e x-\nact coordinates are not only useful as interchange and training data for adaptive OMR software applications, \nbut they can also be used in digital edition environments \nfor producing transcription image ove rlays. A diplomatic \ntranscription can be shown directly on top of the original source, either for highlighting a particular aspect of the \nsource or simply for facilitating its rea dability. Examples \nalready exist for text editions [7], and a similar approach \nfor music could very well be envi saged with MEI.  \nSuch a model would also serve the second use- case, \nwhich is the preparation of different renditions from the same musical material, the typical case  being an edition \nof the full score and, in parallel, an edition of the pe r-\nformers’ parts. While it is relatively easy to extract parts from a score encoded in MEI, there will always be cases where human intervention will be r equired to finalize the \nlayout  of the parts, whatever the automatic layout cap a-\nbilities of the rendering sof tware application used. The \nmodifications can include additional dynamic markings, \nlyrics, directives and similar musical information encoded \nin nearby staves. The ideal solution  is to encode only the \nlayout modifications applied to the parts so that additio n-\nal changes to the score would automatically be reflected \nin the parts. This means that a <note> element for which \nonly the stem direction is changed in the layout need not \nbe duplicated. Features like this already exist in some \nmusic notation software applications, such as in Sibel i-\nus©, which includes a so- called Dynamic Parts™ func-\ntionality. However, they are not designed to handle mul-\ntiple sources. Having an option to record this type of la y-out information in an optimized manner would certainly be valuable.  \n2.1 Requirements  \nIn order optimally  to increase the level of detail of the \ndocuments encoded in MEI, it is necessary to achieve a solution that will not overload the logical su b-tree that \nholds the musical co ntent. When mingled with notation \ncontent, page and sy stem milestone markers complicate \nthe encoding of content. Ad ding more detailed layout \ninformation, such as page size, results in further compli-\ncation.  \nThe solution shoul d avoid overlapping hierarchy prob-\nlems whenever possible. Page breaks and system breaks \nembedded in the content sub -tree represent a non -\nconcurrent hierarchy. Multiple sources requiring different presentation exacerbate the problem by creating multiple instances of non -concurrent hierarchies. \nFurthermore, it is important for the solution to limit \nstrictly the amount of duplicated data in the enco ding. For \nexample, in the case of “score and parts” ed itions, when \nthe data for the parts duplicates that of the score, the \nscore data and the parts data may become desynchr o-\nnized. Howe ver, since the duration of a note in the parts \nshould be the same as in the score, employing a reference \nsystem eliminates this po ssibility.  \nThe proposed solution should not require th e user to \nchoose between content -based or page -based approaches \nbut should supplement the current content -focused repr e-\nsentation of MEI instead. With that in mind it becomes \nclear that the use of this additional layout information has to be optional – for users, but also for applications. This \nmeans that applications unaware of this proposal may safely ignore it, that the additional information provided by this proposal must leave the musical content sub -tree \nuntouched as much as possible, and that links added be-tween the content and the layout elements must  not pr e-\nclude the encoding and decoding of the musical content on its own.  \n3. RELATED WORK \nFor some aspects, t he problem describe d above is similar \nto what is achieved by OMR software application s such \nas Ph otoscore© that extend MusicXML in order to store \nexact positioning information. However, it is done in a \nnon standard way and is application dependent. There are \nalso several standard existing encoding strate gies and \nformats that seem to be relevant to the problem d escribed \nabove. The following section will intr oduce them  briefly \nand discuss their applicability for describing multiple \nrendit ions or  sources of the same musical co ntent. \n3.1 TEI Encoding Model for Genetic Editions  \nThe aim of providing  a detailed model of  how content is \nlaid out on the page is similar to the goal pursued by the \nTEI Workgroup on Genetic Editions [ 3]. Their model   \n \nessentially follows a document -centric approach, as o p-\nposed to the traditional text -centric a pproach for TEI. \nThe alternat ive hierarchy of this model privileges the \ndocument and is organized as follows:  \n \n• Document  \n• Writing surface (page, double page, folium, etc.)  \n• Zone \n• Text, lines or tables  \n \nThe model is designed for encoding complex cases of \nmanuscripts in various stages of cr eation. It s purpose is  to \ntrace and encod e their genesis. In that regard, it is diffe r-\nent from what  we hope to achieve for MEI because this \nmodel aims principally for a chron ological  ordering of \nzones in one document rather than  transcribing or defi n-\ning th e layout of multiple documents sep arately.  \nFurthermore, the TEI encoding model for genetic ed i-\ntions is an alternativ e model for e ncoding a document. It \nis not designed to be applied on top of an existing, tradi-\ntional TEI encoding. Links are not maintained between \nthe textual content of the document and separately enco d-\ned layout information. For this reason, some TEI projects \nadopt a cumbersome double -encoding approach, with one \nencoding for the representation of the source text(s) and a \nsecond encoding for the doc umentary edition [1 ], which it \nwould be desir able to avoid .  \n3.2 XSL:FO \nOne of our design goals is to offer a method that pr o-\nvides a description of how the content of an encoding \nshould be presented. This mechanism must be c apable of \ndescribing differen t rendering outputs of the same musi-\ncal content. XSL:FO (eXtensible Stylesheet Language: Formatting Objects) appears to be useful in this co ntext \nas it allows a set of rules  to be specified  for the transfo r-\nmation of the content of an e ncoded document using  a \ndefined page layout. For this purpose, it uses templates \nwhich are instantiated as often as necessary during pr o-\ncessing, until the entire content is rendered. Using \nXSL:FO <block> elements for systems, staves, and la y-\ners, the general layout of pages con taining music not a-\ntion can be described. XSL:FO can define the ma rgins of \nthe page, padding between and size of systems and staves, and so on.  \nDespite its initial promise, because XSL:FO is co ntent-\nagnostic  it cannot be used to adjust the layout in r espons e \nto the content as required by music notation. For instance, in opera or other equally large scores, it is quite common \nthat only the  staves of the active voices or i nstruments be \npresent.  This leads to vari ation in the size and content of \nsystems that is  only achievable in XSL:FO by providing a \nlarge number of separate templates for each distinct case. Additionally, these templates need to be called  explicitly  \nby the user, so that a fully automatic rendering of the con-\ntent is no longer possible. Furthe rmore, XSL:FO does not provide mechanisms for capturing the coordinate info r-\nmation necessary for diplomatic transcription of the \nsources. For these reasons, a template -driven lan guage \nsuch as XSL:FO is not suitable for the description of co n-\ntent-dependent layout.  \n3.3 Scalable Ve ctor Graphics  \nInstead of using templates for laying out pages, a d e-\nscription of the already laid -out pages could be another \npossibility. A legitimate approach for this would be to \nuse Scalable Vector Graphics (SVG) markup to describe \nindividual pages. There are already processors that gen-\nerate SVG output from MEI markup. The problem with using SVG, however, is that it makes it nearly imposs i-\nble to maintain a connection to the logical content. B e-\ncause the SVG markup represents the graphical p rimi-\ntives of music notation (lines, note head shapes, etc.) and not the semantic information, changes in the content r e-\nquire the prim itives to be recalculated. For example, the \nSVG markup for the representation of a beam would be made up of filled parallel ogram shapes, one for each \nbeam line, with their size and position on the page. \nChanging the pitch of a single note within the <beam> \nelement in the MEI data would  require the size and pos i-\ntion of all the graphical components of the beam to be recomputed. Since SVG describes already- processed d a-\nta, it is inappropriate for storing layout i nformation in a \nflexible way despite its utility as an output format.  \n4. THE MEI LAYOUT MODUL E \nAs mentioned above, XSL:FO offers general instructions \non how to process data, whereas SVG is more appropr i-\nate for already -processed data. The ideal solution for \nMEI lies between the two: a description of what is in a source, or what should appear  on every page  in a re n-\ndered edition, without duplicating the content and with-\nout requi ring additional processing of the data.  \n4.1 General organization  \nA solution to this problem is to store the layout info r-\nmation in a dedicated sub -tree separate from the musical \ncontent. The sub -tree is represented by a <layoutGrp> \nelement within the <music> element. It may contain an \narbitrary number of <layout> elements, each of them de-\nscribing a different visualization of the same musical \ncontent.  \nFor example, for the case illustrated in Figure 1 with \ntwo sources A and B, the musical content of both sources will be encoded following the traditional approach of \nMEI, in a single hierarchy with <app> elements for e n-\ncoding their differences. At the same time, each source \nwill be described further by its own layout sub -tree, if \nneed be  in parallel with its related f acsimile.  \n   \n \n \nFigure 1. An example of two sources as organized with \nthe layout module in MEI. While they share the same \nmusical content, each layout is described in its own sub -\ntree. \nThe <layout> element is expected to have a @ type at-\ntribute for indicating whether it is intended for “transcri p-\ntion” or “rendering”. The <layout> element contains a sequence of <page> elements, each with page -level \nmetadata and nesting <system>, <laidoutStaff> and <laidoutLayer> child elements that can precisely repre-\nsent how each of them is positioned on the page. The h i-\nerarchy can be summarized as follows: \n \n• layoutGrp  \n• layout (‘transcription’ or ‘rendering’)  \n• page \n• system  \n• laido utStaff  \n• laido utLayer  \n At the lowest level, the <laidoutLayer> element co n-\ntains a list of <laidoutElement>  children . Each  \n<laidoutElement> acts as a generic container that can r e-\nfer to any element within the corresponding <layer> el e-\nment in the musical content sub -tree. \nThe <system>, <laidoutStaff>, <laidoutLayer> and \n<laidoutEleme nt> elements all have attributes for storing  \ntheir coordinate position (@lrx , @lry, @ulx and @uly) in \n“transcriptional” layouts.  \n4.2 Referencing system  \nThe links that are esta blished between the layout and the \nelements in the musical content sub -tree are a keystone \nof the module. Every <page> and <system> in the layout sub-tree is linked to its related <pg> and <sb> elements \nin the musical content sub -tree. In order to limit the mo d-\nifications of the musical content sub -tree as much as po s-\nsible, the links operat e deliberately from the layout to-wards the content, and not the reverse. Each <page> e l-\nement is ex pected to have a @pbrefs attribute with the \nlist of XML IDs of <pb> elements in the musical content sub-tree to which it applies, as illustrated in Figure 2. \nSimilarly, <system> elements have a @sbrefs attribute containing a list of <sb> elements. Therefore, the correct insertion of <pb> and <sb> elements is the only change \nto the logical tree required for this pr oposal.  \nFor the <laidoutStaff> and <laidoutLayer > elements, \nthe link with the musical content sub -tree is established \nusing a @staff attribute that refers to the @n attribute of a <staff> element in the musical content sub- tree. Fina lly, \n<laidoutElement> elements have a @target attribute for referencing elements in the musical content sub -tree.\n \n!<music> \n  <facsimile source=\"A\"> \n    <!-- facsimile for source A --> \n  </facsimile>   <facsimile source=\"B\"> \n    <!-- facsimile for source B --> \n  </facsimile>   <layoutGrp> \n    <layout source=\"A\" type=\"transcription\"> \n      <page pbrefs=\"pb-A-1\"> \n        <!-- the page layout in source A --> \n      </page>     </layout> \n    <layout source=\"B\" type=\"transcription\"> \n      <page pbrefs=\"pb-B-1\">         <!-- the page layout in source B --> \n      </page> \n    </layout> \n  </layoutGrp> \n  <body>     <mdiv> \n      <score> \n        <scoreDef barplace=\"mensur\" key.sig=\"0\">           <staffGrp> \n            <staffDef clef.shape=\"C\" clef.line=\"3\"/> \n          </staffGrp> \n        </scoreDef> \n        <section>           <staff n=\"1\"> \n            <layer n=\"1\"> \n              <pb xml:id=\"pb-A-1\" source=\"A\"/> \n              <pb xml:id=\"pb-B-1\" source=\"B\"/> \n              <sb xml:id=\"sb-A-1-1\" source=\"A\"/>               <sb xml:id=\"sb-B-1-1\" source=\"B\"/> \n              <!-- the musical content in A and B --> \n            </layer>           </staff> \n        </section> \n      </score> \n    </mdiv> \n  </body> </music> \n \nFigure 2. The scaffold encoding for the example given \nin Figure 1. The <pb> elements in the score are refe r-\nenced from the <page> elements in the layout. \n4.3 Overlapping  hierarchies  \nAs we have seen, a fundamental reason why it is adva n-\ntageous to keep the layout information in a separate sub -\ntree is because the layout represents a distinct hiera rchy \nthat might overlap with the content hierarchy. A typ ical \ncase is when a system break occurs in the middle of a \nmeasure. In such a situation, the same system is indica t-\ned in MEI by several <sb> elements, one in every la yer \nwhere the system break occurs. The encoding in Fi gure 3 \ngives an example for such a case with a fictitious system \nbreak introduced in the middle of measure num ber five. \nIn practice, this system break could be present in one or \nmore sources, or it could be desired in a specific rende r-  \n \ning. Notice that there are two <sb> elements, one for \neach layer.  \nAs illustrated in Figure 4, in the corresponding layout \nsub-tree of this example, the second system references the \ntwo new <sb> elements via its @sbrefs attribute.  \n!\n \n<measure n=\"5\" xml:id=\"m5\"> \n  <staff n=\"1\" xml:id=\"m5s1\"> \n    <layer n=\"1\" xml:id=\"m5s1l1\"> \n      <beam>         <note xml:id=\"m5s1e1\" pname=\"g\" oct=\"5\" dur=\"16\"/> \n        <note xml:id=\"m5s1e2\" pname=\"f\" oct=\"5\" dur=\"16\"/> \n        <note xml:id=\"m5s1e3\" pname=\"d\" oct=\"6\" dur=\"16\"/>         <note xml:id=\"m5s1e4\" pname=\"c\" oct=\"6\" dur=\"16\"/> \n      </beam> \n      <sb xml:id=\"sb-X-2-1\" source=\"X\"/> \n      <beam> \n        <note xml:id=\"m5s1e5\" pname=\"b\" oct=\"5\" dur=\"16\"/>         <note xml:id=\"m5s1e6\" pname=\"a\" oct=\"5\" dur=\"16\"/> \n        <note xml:id=\"m5s1e7\" pname=\"g\" oct=\"5\" dur=\"16\"/> \n        <note xml:id=\"m5s1e8\" pname=\"f\" oct=\"5\" dur=\"16\"/> \n      </beam> \n    </layer>   </staff> \n  <staff n=\"2\" xml:id=\"m5s2\"> \n    <layer n=\"1\" xml:id=\"m5s2l1\">       <note xml:id=\"m5s2e1\" pname=\"d\" oct=\"4\" dur=\"4\"/> \n      <sb xml:id=\"sb-X-2-2\" source=\"X\"/> \n      <rest xml:id=\"m5s2e2\" dur=\"8\" dots=\"1\"/> \n      <note xml:id=\"m5s2e3\" pname=\"b\" oct=\"3\" dur=\"16\"/> \n    </layer>   </staff> \n  <slur staff=\"1\" startid=\"#m5s1e1\" endid=\"#m5s1e2\"/> \n</measure> <sb xml:id=\"sb-Y-2-1\" source=Y\"/> \n \n \nFigure 3. The customary encoding of measure 5 with an \nadditional internal <sb> . The beginning of the new  sys-\ntem is represented by two <sb> elements  in the co ntent \nsub-tree. \n!<page n=\"1\"> \n  <system n=\"1\"> \n    <laidOutStaff staff=\"1\"> \n      <laidOutLayer>         <!-- previous measures --> \n        <!-- first half of measure 5 --> \n        <!-- musical content up to the sb --> \n      </laidOutLayer> \n    </laidOutStaff>                  <laidOutStaff staff=\"2\"> \n      <laidOutLayer> \n        <!-- previous measures -->         <!-- first half of measure 5 --> \n        <!-- musical content up to the sb --> \n      </laidOutLayer> \n    </laidOutStaff>              \n  </system>   <system n=\"2\" sbrefs=\"sb-X-2-1 sb-X-2-2\"> \n    <laidOutStaff staff=\"1\"> \n      <laidOutLayer>         <!-- second half of measure 5 --> \n        <!-- musical content from the sb --> \n        <!-- next measures --> \n      </laidOutLayer> \n    </laidOutStaff>       <laidOutStaff staff=\"2\"> \n      <laidOutLayer> \n        <!-- second half of measure 5 -->         <!-- musical content from the sb --> \n        <!-- next measures --> \n      </laidOutLayer> \n    </laidOutStaff>   \n  </system> </page> \n \nFigure 4. The proposed encoded layout for the example \ngiven in Figure 3. The second <system> contains refe r-\nences to the two <sb> elements .  4.4 Content selection \nImplicitly, we expect the <laidoutLayer> element to i n-\nclude all elements contained in the corresponding <la y-\ner> element of the musical content sub -tree. This means \nthat in the example illustrated by Figures 3 and 4, the \ncontent of the measu re will be rendered implicitly up to \nthe <sb> element for the system that ends in the middle \nof measure five and from the <sb> element for the next \nsystem. The use of <laidoutElement> for each element is optional for rendering layouts, but it is required for tran-\nscription la youts because in that case, we need to be able \nto store the coordinate positions of the elements.  \nIn some cases, however, a more granular way of s e-\nlecting content might be required. For example, it might be ne cessary to hide an element i n a specific layout. For \nthis purpose, the <laidoutElement> element has an \n@ignore attribute.  \nThe selection of content can also be performed at the \n<laidoutStaff> level. For example, a layout for only one staff in the score will have only a single <laidout Staff> \nelement in each <system> element. Implicitly, all the ot h-\ner staves  will not be included in that layout. Similarly, it \nis possible to change the order of the staves in a specific layout just by modifying the order of the <laidoutStaff> \nelements.  \n4.5 Text ual and layout variants  \nIn MEI, all variants are traditionally encoded with <app> and <rdg> elements in the music content sub -tree. In \nsome cases, however, variants do not necessarily repr e-\nsent a textual difference between the sources because the \nmusical c ontent represented by the notation is identical. \nIn Figure 5, we can see two examples of the beginning of Beethoven’s “Waldstein” sonata. The right hand is wri t-\nten on the lower staff in the autograph and on the upper \nstaff in the edition of Breitkopf & Här tel. Traditionally, \nthis difference could be regarded as a variant in music \ncritical editing even though the musical content is actua l-\nly the same. However, it would not be possible to “hear” \nthe difference between the two versions.  \n \n \nAutograph manuscript  \n \n \nLeipzig, Breitkopf & Härtel, (Serie 16, Plate B.144)  \nFigure 5. The beginning of Beethoven’s  “Waldstein ” so-\nnata No. 21.  The right hand is written on the lo wer staff \nin the manuscript and on the upper one in the ed ition.   \n \nThe layout module is designed in such a way that it is \npossible to encode purely presentational differences b e-\ntween sources at the layout level. In the <laidoutStaff> \nelement, following a content selection method as de-\nscribed above, it is possible to retri eve content from a n-\nother staff of the musical content sub -tree. In our exa mple, \nthis means that the lower <laidoutStaff> in the la yout of \nthe manuscript would pull the content from the first staff, \nassuming that the music content is encoded as in the ed i-\ntion. Even at its current experimental stage, this practice \ncould represent  a significant  conceptual change in critical \nediting. The layout encoding itself becomes the way to \nrepresent layout variants, reserving the traditional <app> \nand <rdg> elements in the musical content sub -tree for \ntextual di fferences.  \n5. CONCLUSION AND FUTURE WORK  \nWe believe that the proposed solution is a novel met hod \nof encoding MEI documents  because it creates a  separ a-\ntion between the content of music notation and its possi-ble realizat ions. Multiple realizations of the same musi-\ncal content can be stored in parallel, each with its own \nspecific layout information. The layout information can \nalso provide additional functionality. It can be used for \nspeci fying how the content appears in an already -\nexisting source, but it can also be used for specifying how the content must  be rendered when creating a new \nedition. The first use is particularly interesting for OMR \nsoftware applications and for producing image overlays for displaying a transcri ption directly on top of the fa c-\nsimile image of the original source. The second use is particularly convenient for storing refined layout info r-\nmation for the parts of an encoded full score. The pr o-\nposed module lays the basis for a new way of organizing the information contained in an existing MEI encoding.  \nThis approach, however, also raises an interesting \nquestion regarding the line between content and present a-\ntion in music notation that we hope will receive more at-tention. There is clearly no fixed border  because in music \nnotation layout is a constituent  component. The proposed \nlayout module does not attempt to define an absolute boundary, but is intended to be flexible. In practice, the \nmore varied the layout of the sources and the more d e-\ntailed the layout information recorded, the less it will be \ndesirable to keep layout information in the musical con-\ntent sub -tree as has been MEI practice so far.  With these \nchanges, the musical content sub -tree may become a \nmore abstract representation of the music.  \nThe next step will be to finalize the module in prepara-\ntion for the next official release of MEI, including prepar-\ning guidelines for its usage. We also expect to have to add more features at the <laidoutElement> level depen d-\ning on thorough testing. For example , it would be logical \nto expect the module to handle the  transposition of  in-\nstruments when generating parts . Because creating an encoding with multiple layouts in \na general purpose XML editor will be unmanageable, tools that implement at least some of the features of this \nnew module will be a high priority. Currently, the module \nis being implemented in the Aruspix  software application, \nwhich will be used for prototyping and providing some \nmore actual examples.  \nThe authors believe that this proposal has grea t pote n-\ntial to enhance MEI’s interoperabi lity, to accelerate its \nfurther adoption  by the scholarly community, and thus to \nreinforce its leading role in the digital humanities.  \n5.1 Availability  \nThe module is available in  the incubator of the MEI pro-\nject.\n1 It ne eds to be com piled with the Roma processor \n[5]. \n6. REFERENCES  \n[1] G. Brüning, K. Henzel, and D. Pravida: “Rationale \nof multiple encoding in the genetic Faust edition,” \nJournal of the Text Encoding Initiative , \n[Forthcoming].  \n[2] M. Good  and G. Actor : “Using M usicXML for file \ninterchange,” Proceedings  of the 3rd International \nConference on WEB Delivering of Music,  p. 153, \n2003.  \n[3] F. Jannidis  (chair): “An encoding model for genetic \neditions,” http://www.tei- c.org/Activities/Council/ \nWorking/tcw19.html 2011.  \n[4] A. Hanki nson, L . Pugin, and I. Fujinaga:  “An \ninterchange format for optical music recognition \napplications,”  Proceedings of the 11 th International \nConference on Music Information Retrieval , pp. 51–\n56, 2010 . \n[5] A. Hankin son, P. Roland, and I. Fujinaga:  “The \nmusic encoding in itiative as a document -encoding \nframework,”  Proceedings of the 12th International \nConference on Music Information Retrieval , pp. \n293– 298, 2011.  \n[6] L. A. Ludovico: “ Key con cepts of the IEEE 1599 \nstandard ,” Proceedings of the IEEE CS Conference : \nThe Use of Symb ols to Represent Music and \nMultimedia Objects , p. 15– 26, 2008.  \n[7] D. Oberhelman:  “Quijote In teractivo (Interactive \nQuixote) ,” Reference Reviews, Vol. 25 No.  5, pp.  \n33–34, 2011. \n[8] E. Selfridge- Field: Beyond MIDI: The Handbook of \nMusical C odes, MIT Press, Cambrid ge MA, 1997.  \n                                                             \n1 <http://code.google.com/p/mei -incubator/source/browse/ >"
    },
    {
        "title": "Music/Voice Separation Using the Similarity Matrix.",
        "author": [
            "Zafar Rafii",
            "Bryan Pardo"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417631",
        "url": "https://doi.org/10.5281/zenodo.1417631",
        "ee": "https://zenodo.org/records/1417631/files/RafiiP12.pdf",
        "abstract": "Repetition is a fundamental element in generating and per- ceiving structure in music. Recent work has applied this principle to separate the musical background from the vo- cal foreground in a mixture, by simply extracting the un- derlying repeating structure. While existing methods are effective, they depend on an assumption of periodically re- peating patterns. In this work, we generalize the repetition- based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating el- ements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a simi- larity matrix can overall improve on the separation perfor- mance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally effi- cient.",
        "zenodo_id": 1417631,
        "dblp_key": "conf/ismir/RafiiP12",
        "keywords": [
            "repetition",
            "source separation",
            "music",
            "voice",
            "mixtures",
            "underlying repeating structure",
            "periodicities",
            "fast-varying repeating structures",
            "isolated repeating elements",
            "similarity matrix"
        ],
        "content": "MUSIC/VOICE SEPARATION USING THE SIMILARITY MATRIX\nZafar RAFII\nNorthwestern University\nEECS Department\nEvanston, IL, USA\nzafarrafii@u.northwestern.eduBryan PARDO\nNorthwestern University\nEECS Department\nEvanston, IL, USA\npardo@northwestern.edu\nABSTRACT\nRepetition is a fundamental element in generating and per-\nceiving structure in music. Recent work has applied this\nprinciple to separate the musical background from the vo-\ncal foreground in a mixture, by simply extracting the un-\nderlying repeating structure. While existing methods are\neffective, they depend on an assumption of periodically re-\npeating patterns. In this work, we generalize the repetition-\nbased source separation approach to handle cases where\nrepetitions also happen intermittently or without a ﬁxed\nperiod, thus allowing the processing of music pieces with\nfast-varying repeating structures and isolated repeating el-\nements. Instead of looking for periodicities, the proposed\nmethod uses a similarity matrix to identify the repeating\nelements. It then calculates a repeating spectrogram model\nusing the median and extracts the repeating patterns using\na time-frequency masking. Evaluation on a data set of 14\nfull-track real-world pop songs showed that use of a simi-\nlarity matrix can overall improve on the separation perfor-\nmance compared with a previous repetition-based source\nseparation method, and a recent competitive music/voice\nseparation method, while still being computationally efﬁ-\ncient.\n1. INTRODUCTION\nA system that can efﬁciently separate a song into fore-\nground (e.g. the soloist or voice) and background (the mu-\nsical accompaniment) components would be of great inter-\nest for a wide range of applications. These applications in-\nclude instrument/vocalist identiﬁcation, music/voice tran-\nscription, melody extraction, audio remixing, and karaoke.\nWhile there are many approaches that have been applied\nto this problem (see Section 2), one promising approach\nis to use analysis of the repeating structure in the audio.\nMany musical pieces are characterized by an underlying\nrepeating structure (e.g. drum loop or 4-measure vamp)\nover which varying elements are superimposed. This is\nespecially true for pop songs where a singer often overlays\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.varying vocals on a repeating accompaniment.\nRecent work (see Section 2) has exploited repetition to\nseparate the repeating musical background from the non-\nrepeating vocal foreground. This work has relied on the\nassumption that there is a global or a local period of repeti-\ntion in the musical background. Should repeated elements\nbe present (e.g. reuse of the same chord voicing in the\npiano) but performed in a way that is not obviously peri-\nodic (e.g. occasional chordal piano “ﬁlls” at the appropri-\nate moments), existing repetition-based approaches fail.\nIn this work, we generalize the repetition-based source\nseparation approach to handle cases where repetitions also\nhappen intermittently or without a ﬁxed period. Instead of\nlooking for periodicities, the proposed method identiﬁes\nrepeating elements by looking for similarities, by means\nof a similarity matrix. Once identiﬁed, median ﬁltering\nis then performed on the repeating elements to calculate a\nrepeating spectrogram model for the background. A time-\nfrequency mask can ﬁnally be derived to extract the repeat-\ning patterns (see Section 3). This allows the processing\nof music pieces with fast-varying repeating structures and\nisolated repeating elements, without the need to identify\nperiods of the repeating structure beforehand.\nThe rest of this paper is organized as follows. Section\n2 describes the related work. Section 3 introduces the pro-\nposed method. Section 4 presents an evaluation of the pro-\nposed method on a data set of 14 full-track real-world pop\nsongs, against a previous repetition-based source separa-\ntion method, and a recent competitive music/voice separa-\ntion method. Section 5 concludes this article.\n2. RELATED WORK\nThere have been a number of approaches applied to the\nproblem of separating the foreground (typically the voice)\nfrom the background. In stereo recordings, panning infor-\nmation (e.g. the vocalist is typically panned to the middle)\ncan be applied. Such approach fails when the vocalist is\nnot center-panned (e.g. many Beatles recordings). Cross-\nchannel timing and amplitude differences can be applied in\nmore complex frameworks, such as in the Degenerate Un-\nmixing Estimation Technique (DUET) [8]. This approach\nis difﬁcult to apply to pop music, due to the reverberant\neffects added, as well as the violation of the sparsity as-\nsumption for music mixtures.\nOther music/voice separation methods focus on mod-eling either the music signal, by generally training an ac-\ncompaniment model from the non-vocal segments [6, 11],\nor the vocal signal, by generally extracting the predomi-\nnant pitch contour [7, 9], or both signals via hybrid mod-\nels [1, 13]. Most of these methods require a training phase\non audio with labeled vocal/non-vocal segments.\nRecently, a relatively simple approach has also been\nproposed for music/voice separation. The method is based\non a median ﬁltering of the mixture spectrogram at dif-\nferent frequency resolutions, in such a way that the har-\nmonic and percussive elements of the accompaniment can\nbe smoothed out, leaving out the vocals [3].\nAnother recent and promising approach is to apply anal-\nysis of the repeating structure in the audio to extract the re-\npeating musical background from the non-repeating vocal\nforeground. In this work, we focus on this approach.\nThe ﬁrst method to explicitly use repetition to sepa-\nrate the musical background from the vocal foreground\nis the REpeating Pattern Extraction Technique (REPET)\n[12]. The method seeks to identify a global period for the\nrepeating structure, so that it can build a model of the re-\npeating background. This model is then used to construct\na time-frequency mask to separate the repeating musical\nbackground from the non-repeating vocal foreground.\nThe original REPET method can be successfully ap-\nplied for music/voice separation on short excerpts (e.g. 10\nsecond verse) [12]. For complete music pieces, the repeat-\ning background is likely to vary over time (e.g. verse fol-\nlowed by chorus). An extended version of REPET was\ntherefore later introduced to handle variations in the repeat-\ning structure [10]. Rather than ﬁnding a global period, the\nmethod tracks local periods of the repeating structure. In\nboth cases, the algorithm needs to identify periods of the\nrepeating structure, as both methods assume periodically\nrepeating patterns.\nIn this work, we propose to generalize the repetition-\nbased source separation approach to handle cases where\nrepetitions also happen intermittently or without a ﬁxed pe-\nriod, by using a similarity matrix.\n3. PROPOSED METHOD\n3.1 Similarity Matrix\nThe similarity matrix is a two-dimensional representation\nwhere each point (a; b) measures the (dis)similarity be-\ntween any two elements aandbof a given sequence. Since\nrepetition/similarity is what makes the structure in music, a\nsimilarity matrix calculated from an audio signal can help\nto reveal the musical structure that underlies it [4].\nGiven a single-channel mixture signal x, we ﬁrst cal-\nculate its Short-Time Fourier Transform (STFT) X, using\nhalf-overlapping Hamming windows of Nsamples length.\nWe then derive the magnitude spectrogram Vby taking\nthe absolute value of the elements of X, after discarding\nthe symmetric part, while keeping the DC component.\nWe then deﬁne the similarity matrix Sas the matrix\nmultiplication between transposed VandV, after normal-\nization of the columns of Vby their Euclidean norm. Inother words, each point (ja; jb)inSmeasures the cosine\nsimilarity between the time frames jaandjbof the mixture\nspectrogram V. The calculation of the similarity matrix S\nis shown in Equation 1.\nS(ja; jb) =Pn\ni=1V(i; ja)V(i; jb)pPn\ni=1V(i; ja)2pPn\ni=1V(i; jb)2\nwhere n=N=2 + 1 = # frequency channels\n8ja; jb2[1; m ]where m=# time frames(1)\n3.2 Repeating Elements\nOnce the similarity matrix Sis calculated, we use it to\nidentify the repeating elements in the mixture spectrogram\nV. For all the frames jinV, we look for the frames that\nare the most similar to the given frame jand save them in\na vector of indices Jj. Assuming that the non-repeating\nforeground (\u0019 voice) is sparse and varied compared to the\nrepeating background (\u0019 music) - a reasonable assumption\nfor voice in music, the repeating elements revealed by the\nsimilarity matrix should be those that form the underlying\nrepeating structure (\u0019 music). The use of a similarity ma-\ntrix actually allows us to identify repeating elements that\ndo not necessarily happen in a periodic fashion.\nWe add the following constraint parameters to the algo-\nrithm. To limit the number of repeating frames considered\nsimilar to the given frame j, we deﬁne k, the maximum\nallowed number of repeating frames. We deﬁne t, the min-\nimum allowed threshold for the similarity between a re-\npeating frame and the given frame (t 2[0;1]). Consecu-\ntive frames can exhibit high similarity without representing\nnew instances of the same structural element, since frame\nduration is unrelated to the duration of musical elements.\nWe therefore deﬁne d, the minimum allowed (time) dis-\ntance between two consecutive repeating frames deemed\nto be similar enough to indicate a repeating element.\n3.3 Repeating Model\nOnce the repeating elements have been identiﬁed for all the\nframes jin the mixture spectrogram Vthrough their cor-\nresponding vectors of indices Jj, we use them to derive a\nrepeating spectrogram model Wfor the background. For\nall the frames jinV, we derive the corresponding frame\njinWby taking the median of the corresponding repeat-\ning frames whose indices are given by vector Jj, for every\nfrequency channel. The calculation of the repeating spec-\ntrogram model Wis shown in Equation 2.\nW(i; j) =median\nl2[1;k ]\b\nV(i; Jj(l)\t\nwhere Jj= [j1: : : jk] =indices of repeating frames\nwhere k=maximum number of repeating frames\n8i2[1; n] = frequency channel index\n8j2[1; m ] =time frame index(2)\nThe rationale is that, assuming that the non-repeating\nforeground (\u0019 voice) has a sparse time-frequency repre-\nsentation compare to the time-frequency representation of\nthe repeating background (\u0019 music), time-frequency binsFigure 1. Derivation of the repeating spectrogram model\nW: (1) compute the similarity matrix Sfrom the mixture\nspectrogram Vusing the cosine similarity measure; (2) for\nall frames jinV, identify the kframes j1: : : jkthat are\nthe most similar to frame jusing S; (3) derive frame jof\nthe repeating spectrogram model Wby taking the median\nof the kframes j1: : : jkofV, for every frequency channel.\nwith little deviations between repeating frames would con-\nstitute a repeating pattern and would be captured by the\nmedian. Accordingly, time-frequency bins with large de-\nviations between repeating frames would constitute a non-\nrepeating pattern and would be removed by the median.\nThe derivation of the repeating spectrogram model Wfrom\nthe mixture spectrogram Vusing the similarity matrix Sis\nillustrated in Figure 1.\n3.4 Time-frequency Mask\nOnce the repeating spectrogram model Wis calculated,\nwe use it to derive a time-frequency mask M. But ﬁrst,\nwe need to create a reﬁned repeating spectrogram model\nW0for the background, by taking the minimum between\nWandV, for every time-frequency bin. Indeed, as noted\nin [10], if we assume that the non-negative mixture spec-\ntrogram Vis the sum of a non-negative repeating spec-\ntrogram Wand a non-negative non-repeating spectrogram\nV\u0000W, then time-frequency bins in Wcan at most have\nthe same value as the corresponding time-frequency bins\ninV. In other words, we want W\u0014V, for every time-\nfrequency bin; hence the use of the minimum function.\nWe then derive a time-frequency mask Mby normaliz-\ningW0byV, for every time-frequency bin. The rationale\nis that time-frequency bins that are likely to constitute a\nrepeating pattern in Vwill have values near 1 in Mand\nwill be weighted toward the repeating background (\u0019 mu-sic). Accordingly, time-frequency bins that are unlikely to\nconstitute a repeating pattern in Vwill have values near 0\ninMand will be weighted toward the non-repeating fore-\nground (\u0019 voice). The calculation of the time-frequency\nmask Mis shown in Equation 3.\nW0(i; j) = min\u0000\nW(i; j); V(i; j)\u0001\nM(i; j) =W0(i; j)\nV(i; j)withM(i; j)2[0;1]\n8i2[1; n] = frequency channel index\n8j2[1; m ] =time frame index(3)\nThe time-frequency mask Mis then symmetrized and\napplied to the STFT Xof the mixture signal x. The es-\ntimated music signal is ﬁnally obtained by inverting the\nresulting STFT into the time domain. The estimated voice\nsignal is obtained by simply subtracting the music signal\nfrom the mixture signal.\n4. EVALUATION\n4.1 Competitive Methods & Data Set\nWe label the proposed method, based on the use of a sim-\nilarity matrix, Proposed. We compare separation perfor-\nmance of Proposed with two competitive music/voice sep-\naration methods on a data set of 14 full-track pop songs.\nThe ﬁrst competitive method is an extension of the orig-\ninal REPET algorithm to handle variations in the underly-\ning repeating structure [10]. We refer to this method as\nREPET+. The method ﬁrst tracks local periods of the un-\nderlying repeating structure using a beat spectrogram, then\nmodels local estimates of the repeating background using\nthe median, and ﬁnally extracts the repeating patterns from\nthe mixture using a time-frequency mask. For the com-\nparison, we used the separation results of REPET+ with\nsoft time-frequency masking and high-pass ﬁltering with a\ncutoff frequency of 100 Hz on the voice estimates, as pub-\nlished in [10].\nThe second competitive method is the Multipass Me-\ndian Filtering-based Separation (MMFS), another recently\nproposed simple and novel approach for music/voice sepa-\nration [3]. The method is based on a median ﬁltering of the\nmixture spectrogram at different frequency resolutions, in\nsuch a way that the harmonic and percussive elements of\nthe accompaniment can be smoothed out, leaving out the\nvocals. For the comparison, we used the separation results\nof the best version of MMFS out of the four proposed ver-\nsions, with high-pass ﬁltering with a cutoff frequency of\n100 Hz on the voice estimates, as published in [3].\nThe data set consists of 14 full-track real-world pop\nsongs, in the form of split stereo WA VE ﬁles sampled at\n44.1 kHz, with the accompaniment and vocals on the left\nand right channels, respectively. These 14 stereo sources\nwere created from recordings released by the band The\nBeach Boys, where some of the accompaniments and vo-\ncals were made available as split stereo tracks1and sepa-\nrated tracks2. This data set was used in [10] for the evalu-\n1Good Vibrations: Thirty Years of The Beach Boys, 1993\n2The Pet Sounds Sessions, 1997ation of REPET+ against the best version of MMFS.\nFollowing the framework adopted in [3] and [10], we\nthen used those 14 stereo sources to create three data sets\nof 14 mono mixtures, by mixing the channels at three dif-\nferent voice-to-music ratios: -6 dB (music is louder), 0 dB\n(same original level), and 6 dB (voice is louder). Note that\nwe are using the exact same data set as in [10], however\nit is not the exact same data set that was used in [3]. The\nauthors of [3] did not mention which tracks they used for\ntheir experiments and also unlike them, but as in [10], we\nprocess the full tracks without segmenting them before-\nhand, since Proposed can handle long recordings, and this\nwithout memory or computational constraints.\n4.2 Algorithm Parameters & Separation Measures\nWe calculated the STFT of each mixture for each of the\nthree mixture sets (-6, 0, and 6 dB) using half-overlapping\nHamming windows of N= 2048 samples length, corre-\nsponding to a duration of 46.4 milliseconds at a sampling\nfrequency of 44.1 kHz. We then processed each mixture\nusing Proposed. The parameters were ﬁxed as follows:\nmaximum number of repeating frames k= 100, minimum\nthreshold for the similarity between a repeating frame and\nthe given frame t= 0, and minimum distance between two\nconsecutive repeating frames d= 1 second. Pilot experi-\nments showed that those parameters lead to overall good\nseparation results. For the comparison, we also applied a\nhigh-pass ﬁltering with a cutoff frequency of 100 Hz on\nthe voice estimates. This means that all the energy un-\nder 100 Hz in the voice estimates is transferred to the cor-\nresponding music estimates. The rational is that singing\nvoice rarely happen below 100 Hz.\nWe measured separation performance by employing the\nBSS EVAL toolbox [2]. The toolbox proposes a set of now\nwidely adopted measures that intend to quantify the quality\nof the separation between a source and its corresponding\nestimate: Source-to-Distortion Ratio (SDR), Sources-to-\nInterferences Ratio (SIR), and Sources-to-Artifacts Ratio\n(SAR). Following the framework adopted in [3] and [10],\nwe measured SDR, SIR, and SAR on segments of 45 sec-\nond length from the music and voice estimates. Higher val-\nues of SDR, SIR, and SAR suggest better separation per-\nformance. We chose to use those measures because they\nare widely known and used, and also because they have\nbeen shown to be well correlated with human assessments\nof signal quality [5].\n4.3 Comparative Results & Statistical Analysis\nFigures 2, 3, and 4 show the separation performance using\nthe SDR, SIR, and SAR, respectively, in dB, for the mu-\nsic component (top row) and the voice component (bottom\nrow), at voice-to-music mixing ratio of -6 dB (left column),\n0 dB (middle column), and 6 dB (right column). In each\ncolumn, from left to right, the ﬁrst results correspond to\nthe best version of MMFS (MMFS ), where the means of\nthe distributions are represented as crosses (the standard\ndeviations were not reported in [3]). The second resultscorrespond to the extension of REPET for varying repeat-\ning structures (REPET+), where the means and standard\ndeviations of the distributions are represented as error bars.\nThe third results correspond to the proposed method with\nsimilarity matrix (Proposed ), where the means and stan-\ndard deviations of the distributions are represented as error\nbars. The mean values are displayed. Higher values are\nbetter.\nFigure 2. Separation performance using the SDR in dB,\nfor the music component (top row) and the voice com-\nponent (bottom row), at voice-to-music mixing ratio of\n-6 dB (left column), 0 dB (middle column), and 6 dB\n(right column), using the best version of MMFS (MMFS )\n(means represented as crosses), the extension of REPET\nfor varying repeating structures (REPET+), and the pro-\nposed method with similarity matrix (Proposed ) (means\nand standard deviations represented as error bars). Mean\nvalues are displayed. Higher values are better.\nWe compared the three different methods including a\nhigh-pass ﬁltering with a cutoff frequency of 100 Hz on the\nvoice estimates, because such post-processing of the esti-\nmates typically helps to produce better separation results.\nFor our proposed method, the high-pass ﬁltering increased\nSDR and SIR, for both the music and voice estimates. SAR\nhowever increased only for the music estimates. This is\nprobably due to the fact that, although improving the sep-\naration performance overall, using a high-pass ﬁltering on\nthe voice estimates creates “holes” in their time-frequency\nrepresentation, which tend to increase the separation arti-\nfacts, hence the decrease of SAR for the voice estimates.\nAs we can see in Figures 2, 3, and 4, as the voice-\nto-music ratio gets larger, SDR, SAR, and SIR get lower\nfor the music estimates and larger for the voice estimates,\nand vice versa. This is an intuitive result also observed\nforMMFS andREPET+. Indeed, as the voice compo-\nnent gets louder compared to the music component, it thenFigure 3. Separation performance using the SIR in dB.\nbecomes easier to extract the voice component, and ac-\ncordingly harder to extract the music component, and vice\nversa. A multiple comparison test showed that those re-\nsults were statistically signiﬁcant in each case. We used an\nAnalysis of Variance (ANOV A) when the compared distri-\nbutions were all normal, and a Kruskal-Wallis test when\nat least one of the compared distributions was not normal.\nWe used a Jarque-Bera normality test to determine if a dis-\ntribution was normal or not.\nFigure 4. Separation performance using the SAR in dB.\nAs we can see in Figures 2, 3, and 4, compared with\nMMFS, Proposed gave overall better SDR for both the mu-\nsic and voice estimates, better SIR for the music estimates,and better SAR for the voice estimates, and this for all the\nthree voice-to-music ratios. A one-sample t-test comparing\nthe means of the distributions of Proposed with the means\nofMMFS (since the only values provided in [3] were the\nmeans) showed that those results were statistically signif-\nicant in each case, except for the SDR at voice-to-music\nratio of -6 dB, where the improvement of Proposed com-\npared with MMFS was not signiﬁcant for the music esti-\nmates, and a decrease, although not signiﬁcant, was ob-\nserved for the voice estimates. This suggest that, compared\nwith MMFS, Proposed has globally better separation per-\nformance, particularly it is better at removing the “vocal”\ninterferences from the accompaniment, and at limiting the\nseparation artifacts in the voice estimates.\nAs we can also see in Figures 2, 3, and 4, for the music\nestimates, compared with REPET+, Proposed gave over-\nall better SDR and SAR for all the three voice-to-music\nratios, and better SIR at voice-to-music ratio of 6 dB. A\nmultiple comparison test showed that those results were\nstatistically signiﬁcant in each case, except for the SDR\nat voice-to-music ratio of -6 dB where the improvement of\nProposed compared with REPET+ was not signiﬁcant. For\nthe voice estimates, compared with REPET+, Proposed\ngave overall better SAR for all the three voice-to-music\nratios, and better SDR and SIR at voice-to-music ratio of -\n6 dB. A multiple comparison test showed that those results\nwere statistically signiﬁcant in each case, except for the\nSAR where the improvement of Proposed compared with\nREPET+ was only signiﬁcant at voice-to-music ratio of -\n6 dB. We used ANOV A when the compared distributions\nwere all normal, and a Kruskal-Wallis test when at least\none of the compared distributions was not normal. This\nsuggest that, compared with REPET+, Proposed has glob-\nally better separation performance for a component (mu-\nsic or voice), as the given component becomes softer com-\npared with the other one.\nThe average computation time of Proposed over all the\nmixtures and all of the three mixture sets (-6, 0, and 6 dB)\nwas 0.563 second for 1 second of mixture, when imple-\nmented in Matlab on a PC with Intel(R) Core(TM)2 Quad\nCPU of 2.66 GHz and 6 GB of RAM. In other words,\nProposed can perform music/voice separation of a mix-\nture audio in half the time of the playback of the audio,\nfor recordings of the length of a typical pop song. This is\nencouraging, since Proposed builds a similarity matrix that\nisO(n2), where nis the length of the audio ﬁle. As a point\nof comparison, the average computation time for REPET+\nfor the exact same data set was 1.1830 second for 1 second\nof mixture [10].\n5. CONCLUSION\nIn this work, we have proposed a generalization of the RE-\npeating Pattern Extraction Technique (REPET) method for\nthe task of music/voice separation, based on the calculation\nof a similarity matrix. The REPET approach is based on\nthe separation of a musical background from a vocal fore-\nground, by extraction of the underlying repeating structure.\nThe basic idea is to identify elements that exhibit similar-ity, and compare them to repeating models derived from\nthem to extract the repeating patterns.\nUnlike the previous REPET methods that assume peri-\nodically repeating patterns, the proposed method with sim-\nilarity matrix generalizes to repeating structures where rep-\netitions can also happen intermittently or without a ﬁxed\nperiod, therefore allowing the processing of music pieces\nwith fast-varying repeating structures and isolated repeat-\ning elements, without the need to identify periods of the\nunderlying repeating structure beforehand.\nEvaluation on a data set of 14 full-track real-world pop\nsongs showed that the proposed generalization of REPET\nwith similarity matrix can overall improve on the separa-\ntion performance compared with the extension of REPET\nfor varying repeating structures, and another recent com-\npetitive music/voice separation method based on median\nﬁltering, while still being computationally efﬁcient. Given\nthe SDR, which can be understood as a measure of the\noverall quality of the separation, our evaluation showed\nthat when the results between the proposed method and the\ncompetitive methods were statistically signiﬁcant, the pro-\nposed method gave higher results, and this compared with\nboth the competitive methods.\nThe proposed generalization of REPET is only based on\na similarity matrix. In other words, it does not depend on\nparticular features, does not rely on complex frameworks,\nand does not need prior training. Because it is only based\non self-similarity, it has the advantage of being simple, fast,\nblind, and therefore completely and easily automatable.\n6. ACKNOWLEDGMENTS\nThe authors would like to thank Antoine Liutkus, Roland\nBadeau, Ga ¨el Richard, and their colleagues from Telecom\nParisTech for the fruitful discussions. This work was sup-\nported by NSF grant number IIS-0812314.\n7. REFERENCES\n[1] Jean-Louis Durrieu, Bertrand David, and Ga ¨el\nRichard. A musically motivated mid-level representa-\ntion for pitch estimation and musical audio source sep-\naration. IEEE Journal on Selected Topics on Signal\nProcessing, 5(6):1180–1191, October 2011.\n[2] Cedric F ´evotte, R ´emi Gribonval, and Emmanuel Vin-\ncent. BSS EV AL toolbox user guide. Technical Re-\nport 1706, IRISA, Rennes, France, April 2005.\nhttp://www.irisa.fr/metiss/bss eval/.\n[3] Derry FitzGerald and Mikel Gainza. Single channel vo-\ncal separation using median ﬁltering and factorisation\ntechniques. ISAST Transactions on Electronic and Sig-\nnal Processing, 4(1):62–73, 2010.\n[4] Jonathan Foote. Visualizing music and audio using\nself-similarity. In 7th ACM International Conference\non Multimedia (Part 1), pages 77–80, Orlando, FL,\nUSA, October 30-November 0 1999.[5] Brendan Fox, Andrew Sabin, Bryan Pardo, and Alec\nZopf. Modeling perceptual similarity of audio signals\nfor blind source separation evaluation. In 7th Interna-\ntional Conference on Independent Component Anal-\nysis, pages 454–461, London, UK, September 09-12\n2007.\n[6] Jinyu Han and Ching-Wei Chen. Improving melody\nextraction using probabilistic latent component anal-\nysis. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing, Prague, Czech Repub-\nlic, May 22-27 2011.\n[7] Chao-Ling Hsu and Jyh-Shing Roger Jang. On the\nimprovement of singing voice separation for monau-\nral recordings using the MIR-1K dataset. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n18(2):310–319, February 2010.\n[8] Alexander Jourjine, Scott Rickard, and ¨Ozg¨ur Yil-\nmaz. Blind separation of disjoint orthogonal signals:\nDemixing n sources from 2 mixtures. In IEEE Inter-\nnational Conference on Acoustics, Speech, and Sig-\nnal Processing, volume 5, pages 2985–2988, Istanbul,\nTurkey, June 5-9 2000.\n[9] Yipeng Li and DeLiang Wang. Separation of singing\nvoice from music accompaniment for monaural record-\nings. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 15(4):1475–1487, May 2007.\n[10] Antoine Liutkus, Zafar Raﬁi, Roland Badeau, Bryan\nPardo, and Ga ¨el Richard. Adaptive ﬁltering for mu-\nsic/voice separation exploiting the repeating musical\nstructure. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing, Kyoto, Japan,\nMarch 25-30 2012.\n[11] Alexey Ozerov, Pierrick Philippe, Fr ´ed´eric Bimbot,\nand R ´emi Gribonval. Adaptation of Bayesian models\nfor single-channel source separation and its applica-\ntion to voice/music separation in popular songs. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing, 15(5):1564–1578, July 2007.\n[12] Zafar Raﬁi and Bryan Pardo. A simple music/voice\nseparation system based on the extraction of the re-\npeating musical structure. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\nPrague, Czech Republic, May 22-27 2011.\n[13] Tuomas Virtanen, Annamaria Mesaros, and Matti\nRyyn ¨anen. Combining pitch-based inference and non-\nnegative spectrogram factorization in separating vo-\ncals from polyphonic music. In ISCA Tutorial and Re-\nsearch Workshop on Statistical and Perceptual Audi-\ntion, pages 17–20, Brisbane, Australia, 21 September\n2008."
    },
    {
        "title": "Facilitating Comprehensive Benchmarking Experiments on the Million Song Dataset.",
        "author": [
            "Alexander Schindler",
            "Rudolf Mayer",
            "Andreas Rauber"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417521",
        "url": "https://doi.org/10.5281/zenodo.1417521",
        "ee": "https://zenodo.org/records/1417521/files/RauberSM12.pdf",
        "abstract": "The Million Song Dataset (MSD), a collection of one million music pieces, enables a new era of research of Mu- sic Information Retrieval methods for large-scale applica- tions. It comes as a collection of meta-data such as the song names, artists and albums, together with a set of fea- tures extracted with the The Echo Nest services, such as loudness, tempo, and MFCC-like features. There is, however, no easily obtainable download for the audio files. Furthermore, labels for supervised machine learning tasks are missing. Researchers thus are currently restricted on working solely with these features provided, limiting the usefulness of MSD. We therefore present in this paper a more comprehensive set of data based on the MSD, allowing its broader use as benchmark collection. Specifically, we provide a wide and growing collection of other well-known features in the MIR domain, as well as ground truth data with a set of recommended training/test splits. We obtained these features from audio samples provided by 7digital.com, and metadata from the All Music Guide. While copyright prevents re-distribution of the audio snip- pets per se, the features as well as metadata are publicly available on our website for benchmarking evaluations. In this paper we describe the pre-processing and cleansing steps applied, as well as feature sets and tools made avail- able, together with first baseline classification results.",
        "zenodo_id": 1417521,
        "dblp_key": "conf/ismir/RauberSM12",
        "keywords": [
            "Music Information Retrieval",
            "large-scale applications",
            "meta-data",
            "features extracted",
            "audio files",
            "supervised machine learning",
            "ground truth data",
            "benchmark collection",
            "7digital.com",
            "All Music Guide"
        ],
        "content": "FACILITATING COMPREHENSIVE BENCHMARKING EXPERIMENTS\nON THE MILLION SONG DATASET\nInstitute of Software Technology and Interactive Systems, Vienna University of Technology\nfschindler,mayer,rauberg@ifs.tuwien.ac.at\nABSTRACT\nThe Million Song Dataset (MSD), a collection of one\nmillion music pieces, enables a new era of research of Mu-\nsic Information Retrieval methods for large-scale applica-\ntions. It comes as a collection of meta-data such as the\nsong names, artists and albums, together with a set of fea-\ntures extracted with the The Echo Nest services, such as\nloudness, tempo, and MFCC-like features.\nThere is, however, no easily obtainable download for\nthe audio ﬁles. Furthermore, labels for supervised machine\nlearning tasks are missing. Researchers thus are currently\nrestricted on working solely with these features provided,\nlimiting the usefulness of MSD. We therefore present in\nthis paper a more comprehensive set of data based on the\nMSD, allowing its broader use as benchmark collection.\nSpeciﬁcally, we provide a wide and growing collection of\nother well-known features in the MIR domain, as well as\nground truth data with a set of recommended training/test\nsplits.\nWe obtained these features from audio samples provided\nby 7digital.com, and metadata from the All Music Guide.\nWhile copyright prevents re-distribution of the audio snip-\npets per se, the features as well as metadata are publicly\navailable on our website for benchmarking evaluations. In\nthis paper we describe the pre-processing and cleansing\nsteps applied, as well as feature sets and tools made avail-\nable, together with ﬁrst baseline classiﬁcation results.\n1. INTRODUCTION\nMusic Information Retrieval (MIR) research has histori-\ncally struggled with issues of publicly available benchmark\ndatasets that would allow for evaluation and comparison\nof methods and algorithms on the same data base. Most\nof these issues stem from the commercial interest in mu-\nsic by record labels, and therefore imposed rigid copyright\nissues, that prevent researchers from sharing their music\ncollections with others. Subsequently, only a limited num-\nber of data sets has risen to a pseudo benchmark level, i.e.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.where most of the researchers in the ﬁeld have access to\nthe same collection.\nAnother reason identiﬁed as a major challenge for pro-\nviding access to research data in general is the lack of\nesteem and valuation of these kind of activities. While\npreparing, maintaining and providing access to massive\ndata collections requires signiﬁcant investments in terms\nof system maintenance and data (pre-)processing, it is con-\nsidered administrative rather than research work (in spite\nof several even research-afﬁne challenges emerging dur-\ning such activities), and thus does not gain acceptance in\nclassical research-oriented publication venues. Such lack\nof career rewards is one of the many factors, next to legal\nlimitations and lack of expertise, limiting sharing of re-\nsearch data [7]. Several initiatives have been started in the\nResearch Infrastructures area to mitigate this problem and\nfoster collaborative research. These areas span across vir-\ntually all topical areas, from Astronomy, via meteorology,\nchemistry to humanities1.\nA recent effort in the MIR domain has lead to the com-\npilation of the Million Song Dataset [3] (MSD). It provides\na database of meta-data for a collection of one million\nsongs, such as the song name, artists and album. In ad-\ndition, a number of descriptive features extracted with the\nservices from The Echo Nest2are provided. These fea-\ntures include tempo, loudness, timings of fade-in and fade-\nout, and MFCC-like features for a number of segments.\nMoreover, a range of other meta-data has been published\nrecently, such as song lyrics (for a subset of the collection),\nor tags associated to the songs from Last.fm3.\nThe MSD enables researchers to test algorithms on a\nlarge-scale collection, thus allowing to test them on more\nreal-world like environments. However, there are no easily\nobtainable audio ﬁles available for this dataset, and there-\nfore, researchers are practically restricted to benchmarking\non algorithms that work on top of features, such as recom-\nmendation of classiﬁcation, but can not easily develop new\nor test existing feature sets on this dataset. The availabil-\nity of just one feature set also does not allow an evaluation\nacross multiple feature sets. As previous studies showed,\nhowever, there is no single best feature set, but their per-\nformance depends very much on the dataset and the task.\nWe therefore aim to alleviate these restrictions by pro-\n1http://ec.europa.eu/research/infrastructures/\n2http://the.echonest.com\n3http://www.last.fm\nAlexander Schindler, Rudolf Mayer, and Andreas Rauberviding a range of features extracted for the Million Song\nDataset, such as MFCCs and a set of low-level features ex-\ntracted with the jAudio feature extraction software as well\nas the Marsyas framework [13], and the Rhythm Patterns\nand derived feature sets [9]. To this end, we ﬁrst obtained\naudio samples for the MSD by using the content provider\n7digital.\nA second shortcoming of the MSD is that it does not,\nat the moment, contain a mapping to categorisation such\nas genres. Thus, experimental evaluations such as musical\ngenre classiﬁcation, a popular task in MIR research, are\nnot possible. We therefore further propose a partitioning\nof the dataset into a set of genres obtained from the All\nMusic Guide4. Speciﬁcally, we created a partitioning on\ntwo levels of detail, with 13 top-level-genres and 25 sub-\ngenres, and propose a number of splits for training and test\nsets, with different ﬁlters, allowing several tasks for evalu-\nation.\nBoth the feature sets and the partitioning into genres are\navailable from our website5. The features are stored in\nthe WEKA Attribute-Relation File Format (ARFF) [14],\nwith one attribute being the unique identiﬁer of the song in\nthe MSD. We further provide a set of scripts to match the\nfeatures with the genre mapping so that they can be used\nfor classiﬁcation experiments.\nThe remainder of this paper is structured as follows.\nSection 2 introduces the dataset and the properties of the\naudio samples, while Section 3 describes the sets of fea-\ntures extracted from them. Section 4 gives details on the\ngenre assignment obtained, and in Section 5, we describe\nbenchmark partitions and how we aim to facilitate excha-\nnge between researchers. Finally, we provide conclusions\nin Section 6\n2. THE DATASET\nThe Million Song Dataset contains in the meta-data a uni-\nque identiﬁer for an audio sample at the content provider\n7digital6. For some songs no sample could be downloaded,\nas the identiﬁer was unknown to 7digital. We thus obtained\na total of 994,960 audio samples, i.e. a coverage of 99.5%\nof the dataset; the list of missing audio samples is available\non the website. This points to an important issue related to\nthe use of external on-line resources for scientiﬁc experi-\nmentation. Especially when the provider is not genuinely\ninterested in the actual research performed, there is little\nmotivation to maintain the data accessible in unmodiﬁed\nmanners, and is thus susceptible to changes and removal.\nThus, maintaining a copy of a ﬁxed set of data is essential\nin benchmarking to allow the evaluation of newly devel-\noped feature sets, and for acoustic evaluation of the results.\nIn total, the audio ﬁles account for approximately 625\ngigabyte of data. The audio samples do not adhere to a\ncommon encoding quality scheme, i.e. they differ in length\nand quality provided. Figure 1 shows a plot of the sample\nlengths; please note that the scale is logarithmic. It can\n4http://allmusic.com\n5http://www.ifs.tuwien.ac.at/mir/msd/\n6http://www.7digital.com\nFigure 1: Distribution of sample length\nbe observed that there are two peaks at sample lengths of\n30 and 60 seconds with 366,130 and 596,630 samples, re-\nspectively, for a total of 96,76% of all the samples. These\nshorter snippets normally contain a section in the middle of\nthe song. Many other well-known collections in the MIR\ndomain as well contain only 30 second snippets, and fea-\nture extraction algorithms normally deal with this.\nTable 1: Audio properties of 7Digital Samples\nSamplerate\n22 768,710 77,26%\n44 226,169 22,73%\nother 81 0,01%\nBitrate\n128 646,120 64,94%\n64 343,344 34,51%\nother (VBR) 5,494 0,55%\nChannels\nMono 6,342 0.64%\nStereo 150,779 15.15%\nJoint stereo / dual channel 837,839 84.21%\nTable 1 gives an overview on the audio quality of the\nsamples. The majority, more than three quarters, of the\naudio snippets have a sample rate of 22khz, the rest has a\nsample rate of 44khz (with the exception of 81 songs, of\nwhich the majority have 24 and 16khz). Regarding the bi-\ntrate, approximately two-thirds of the songs are encoded\nwith 128kbit, the majority of the rest with 64kbit; only\nabout half a percent of the songs come with higher (192\nor 320kbps) or variable bitrates (anywhere between 32 and\n275kpbs). Almost all samples are provided in some form\nof stereo encoding (stereo, joint stereo or dual channels) –\nonly 0.6% of them have only one channel. As these charac-\nteristics, speciﬁcally the sample rate, may have signiﬁcant\nimpact on the performance of the data analysis algorithms,\nwe must consider these for stratiﬁcation purpose when de-\nsigning the benchmark splits.\n3. FEATURE SETS\nWe extracted a wide range of audio features from the sam-\nples provided, namely features provided by the jAudio fea-\nture extraction software (which is a part of the jMIR pack-Table 2: Overview on features extracted from the MSD samples. Dim. denotes the dimensionality, Deriv. derivatives\ncomputed from the base features\n# Feature Set Extractor Dim Deriv.\n1 MFCCs [12] MARSAYS 52\n2 Chroma [6] MARSAYS 48\n3 timbral [13] MARSAYS 124\n4 MFCCs [12] jAudio 26 156\n5 Low-level spectral features [11] (Spectral Centroid, Spectral Rolloff Point, Spectral Flux,\nCompactness, and Spectral Variability, Root Mean Square, Zero Crossings, and Fraction of Low Energy Win-\ndows)jAudio 16 96\n6 Method of Moments [11] jAudio 10 60\n7 Area Method of Moments [11] jAudio 20 120\n8 Linear Predictive Coding [11] jAudio 20 120\n9 Rhythm Patterns [9] rpextract 1440\n10 Statistical Spectrum Descriptors [9] rpextract 168\n11 Rhythm Histograms [9] rpextract 60\n12 Modulation Frequency Variance Descriptor [10] rpextract 420\n13 Temporal Statistical Spectrum Descriptors [10] rpextract 1176\n14 Temporal Rhythm Histograms [10] rpextract 420\nage [11]), the MARSYAS feature extractor [13], and the\nRhythm Patterns family of feature sets [9]. An overview\non these features is given in Table 2.\nThe jAudio software provides a range of 28 features as-\nsociated with both the frequency and time domains. It in-\ncludes several intermediate-level musical features, mainly\nrelated to rhythm, as well as lower-level signal processing-\noriented features. It also provides an implementation of\nMFCC features [12], using 13 coefﬁcients. jAudio com-\nputes in general mean and standard deviations over the\nsequence of frames, and provides for most measures also\nderivatives, i.e. additional statistical moments over the ba-\nsic measures. For the extraction, we utilised jAudio as bun-\ndled in the jMIR 2.4 release7.\nA very popular audio feature extraction system is MAR-\nSAYS, one of the ﬁrst comprehensive software packages to\nbe available to MIR researchers. A very popular set from\nthis audio extractor is the so-called “timbral” set, which is\ncomposed of 13 MFCC coefﬁcients, and the twelve chroma\nfeatures and the average and minimum chroma value, and\nthe four low-level features zero crossings, and rolloff, ﬂux\nand centroid of the spectrum. For these 31 values, four sta-\ntistical moments are computed, resulting in a 124 dimen-\nsional vector. For the extraction, we utilised MARSYAS\nversion 0.4.58.\nThe Rhythm Patterns and related features sets are ex-\ntracted from a spectral representation, partitioned into seg-\nments of 6 sec. Features are extracted segment-wise, and\nthen aggregated for a piece of music computing the median\n(Rhythm Patterns, Rhythm Histograms) or mean (Statisti-\ncal Spectrum Descriptors, Modulation Frequency Variance\nDescriptor) from features of multiple segments. For the ex-\ntraction, we employed the Matlab-based implementation,\nversion 0.64119.\n7available from http://jmir.sourceforge.net/\n8available from http://sourceforge.net/projects/\nmarsyas/\n9available from http://www.ifs.tuwien.ac.at/mir/It is intentional that we provide two different versions\nof the MFCCs features, as this will allow for interesting in-\nsights in how these implementations differ on various MIR\ntasks.\n3.1 Publication of Feature Sets\nAll features described above are available on our website\nfor download, encoded in the WEKA Attribute-Relation\nFile Format (ARFF) [14]. The features are available un-\nder the Creative Commons Attribution-NonCommercial-\nShareAlike 2.0 Generic License10.\nTo allow high ﬂexibility when using them, we provide\none ARFF ﬁle for each type of features; these can then be\ncombined in any particular way when performing exper-\nimental evaluations. A set of scripts is provided as well\non the website for this. In total, the feature ﬁles account\nfor approximately 40 gigabyte of uncompressed text ﬁles.\nThe feature ﬁles contain the numeric values for each fea-\nture, and additionally the unique identiﬁer assigned in the\nMSD. This way, it is possible to generate various feature\nﬁles with different ground truth assignments; we again pro-\nvide scripts for this. The proposed assignment into genres\nfor genre classiﬁcation tasks is described in Section 4.\nFurther feature sets to be extracted and provided will\ninclude e.g. MIRtoolbox [8] or M2k [5].\n4. ALLMUSIC DATASETS\nThe All Music Guide (AMG) [4] was initiated by an archiv-\nist in 1991 and emerged 1995 from its book form into a\ndatabase which can be accessed through the popular com-\nmercial Web page allmusic.com. The Web page offers a\nwide range of music information, including album reviews,\nartist biographies, discographies well as classiﬁcation of\ndownloads.html\n10http://creativecommons.org/licenses/by-nc-sa/\n2.0/albums according to genres, styles, moods and themes. In-\nformation is provided and curated by experts.\nGenre information is very coarse, provided as a single\ntag for each album. Further the two main categories Pop\nand Rock are combined into a single genre ’Pop/Rock’.\nAdditionally to genre labels, style tags are provided al-\nlowing for a more speciﬁc classiﬁcation. Usually multi-\nple style tags are applied for each album, but unfortunately\nno weighting scheme can be identiﬁed and in many cases\nonly one tag is provided. Style tags also tend to be even\nmore generic than genre labels. Especially non-American\nmusic is frequently tagged with labels describing country\nor region as well as the language of the performing artist.\nInstrumentation, situational descriptions (e.g. Christmas,\nHalloween, Holiday, etc.) as well as confessional or gen-\nder attributes (e.g. Christian, Jewish, Female, etc.) are also\nprovided. Unfortunately these meta-descriptive attributes\nare not used as isolated synonyms, but are concatenated\nwith conventional style information (e.g. Japanese Rock,\nChristian Punk, Classic Female Blues, etc.).\nAllmusic.com assembles styles to meta-styles which can\nbe interpreted as sub genres used to diversify the major\ngenre labels. Meta-styles are not distinctive and are used\noverlapping in many meta-styles (e.g. Indie Electronic is\ncontained in the meta-styles Indie Rock, Indie Pop andAl-\nternative/Indie Rock ).\n4.1 Data Collection\nData was collected automatically from Allmusic.com us-\ning direct string matching to query for artist-release com-\nbinations. From the resulting Album Web page genre and\nstyle tags were collected.\nWe were able to collect 21 genre labels for 62,257 al-\nbums which initially provided genre tags for 433,714 tracks.\nStyle tags were extracted attributing only 42,970 albums\nresulting in 307,790 labeled tracks. An average of 3.25\ntags out of a total of 905 styles were applied to each al-\nbum, but 5,742 releases were only tagged with a single\nstyle label. The most popular genre with 32,696 tagged\nalbums, was Pop/Rock - this is 10% more as the sum of\nall other genres. Referring to tracks the difference rises to\n30%. Further, the granularity of Rock is very scarce, in-\ncluding Heavy Metal, Punk, etc. A similar predominating\nposition of this genre as well as was also reported by [2].\nThe most popular style tag is Alternative/Indie Rock ap-\nplied to 12,739 albums, which is more than twice as much\nas the second popular style Alternative Pop/Rock. About\n120 tags describe the country of the performing artist or the\nlanguage of the interpretation - the most common among\nthem is Italian Music which has been applied to 610 al-\nbums.\n4.2 Allmusic Genre Dataset\nThe Allmusic Genre Dataset is provided as an unoptimized\nexpert annotated ground truth dataset for music genre clas-\nsiﬁcation. We provide two partitions of this set. The MSD\nAllmusic Genre Dataset (MAGD) assembles all collected\ngenres including generic and small classes.Table 3: MSD Allmusic Genre Dataset (MAGD) - up-\nper part represents the MSD Allmusic Top Genre Dataset\n(Top-MAGD)\nGenre Name Number of Songs\nPop/Rock 238,786\nElectronic 41,075\nRap 20,939\nJazz 17,836\nLatin 17,590\nR&B 14,335\nInternational 14,242\nCountry 11,772\nReggae 6,946\nBlues 6,836\nV ocal 6,195\nFolk 5,865\nNew Age 4,010\nReligious 8814\nComedy/Spoken 2067\nStage 1614\nEasy Listening 1545\nAvant-Garde 1014\nClassical 556\nChildrens 477\nHoliday 200\nTotal 422,714\nThe second partition - MSD Allmusic Top Genre Data-\nset (Top-MAGD) - consists of 13 genres - the 10 major gen-\nres of Allmusic.com (Pop/Rock, Jazz, R&B, Rap, Country,\nBlues, Electronic, Latin, Reggae, International) including\nthe three additional genres V ocal, Folk, New Age (see Ta-\nble 3). Generic genres as well as classes with less than 1%\nof the number of tracks of the biggest class Pop/Rock are\nremoved. Due to the low number of tracks, the Classical\ngenre is also removed from the Top Genre dataset.\n4.3 Allmusic Style Dataset\nThe Allmusic Style Dataset attempts to more distinctively\nseparate the collected data into different sub-genres, al-\nleviating predominating classes. For the compilation of\nthe dataset genre labels were omitted and solely style tags\nwere used. In a ﬁrst step metastyle description as presented\non the Allmusic.com Web site were used to map multiple\nstyle tags to a single genre name - in this case we used the\nmetastyle name. This simple aggregation approach gen-\nerated a total of 210 genre labels many of them highly\ngeneric or hierarchical specializing (e.g. Electric Blues\nandElectric Chicago Blues. The MSD Allmuisc Metastyle\nDataset - Multiclass (MAMD) was derived from these 210\nresulting metaclasses. Each track was matched to one or\nmore metaclasses according to its style tags. In a second\nstep we removed from the initial set of 905 style tags all\nconfessional, situational and language speciﬁc entries. Re-\ngional tags were discarded if they do not refer to a speciﬁc\ntraditional cultural music style (e.g. African Folk ). Pop-\nular music attributed with regional information was dis-Table 4: The MSD Allmusic Style Dataset (MASD)\nGenre Name Number of Songs\nBig Band 3,115\nBlues Contemporary 6,874\nCountry Traditional 11,164\nDance 15,114\nElectronica 10,987\nExperimental 12,139\nFolk International 9,849\nGospel 6,974\nGrunge Emo 6,256\nHip Hop Rap 16,100\nJazz Classic 10,024\nMetal Alternative 14,009\nMetal Death 9,851\nMetal Heavy 10,784\nPop Contemporary 13,624\nPop Indie 18,138\nPop Latin 7,699\nPunk 9,610\nReggae 5,232\nRnB Soul 6,238\nRock Alternative 12,717\nRock College 16,575\nRock Contemporary 16,530\nRock Hard 13,276\nRock Neo Psychedelia 11,057\nTotal 273,936\ncarded due to extensive genre overlaps (e.g. Italian Pop\nranges from Hip-Hop to Hard-Rock). Finally, we succes-\nsively merged these genres into general descriptive classes\nuntil we ﬁnalized the dataset into the MSD Allmusic Style\nDataset (MASD) presented in Table 5. For completeness\nwe also provide the MSD Allmuisc Style Dataset - Multi-\nclass (Multi-MASD). This set contains the pure track-style\nmapping as collected from Allmusic.com.\n5. BENCHMARK PARTITIONS\nInﬂuenced by the tremendous experience in the text clas-\nsiﬁcation domain, speciﬁcally with the landmark Reuters-\n21578 corpus, we provide a number of benchmark parti-\ntions that researcher can use in their future studies, in order\nto facilitate repeatability of experiments with the MSD be-\nyond x-fold cross validation. We also encourage and pro-\nvide a platform for exchange of results obtained and new\npartitions created via our website.\nWe provide the following categories of splits:\n\u000fSplits with all the tow ground truth assignments into\ngenre and style classes, described in Section 4.\n\u000fSplits with just the majority classes from these two\nground truth assignments.\n\u000fSplits considering the sample rate of the ﬁles, i.e.\nonly the 22khz samples, only the 44khz samples, and\na set with all audio ﬁles.Table 5: Classiﬁcation results on MSD Allmusic Guide\nStyle dataset (MASD), 66% training set split\nDataset NB SVM k-NN DT RF\nMFCC (4) 15.04 20.61 24.13 14.21 18.90\nSpectral (5) 14.03 17.91 13.84 12.81 17.21\nSpectral Derivates (5) 11.69 21.98 16.14 14.09 19.03\nMethodOfMoments (6) 13.26 16.42 12.77 11.57 14.80\nLPC (8) 13.41 17.92 15.94 11.97 16.19\nSSD (10) 13.76 27.41 27.07 15.06 20.06\nRH (11) 12.38 17.23 12.46 10.30 13.41\nIn particular, we provide the following size partitions:\n\u000f“Traditional” splits into training and test sets, with\n90%, 80%, 66% and 50% size of the training set, ap-\nplying stratiﬁcation of the sampling to ensure having\nthe same percentage of training data per class, which\nis important for minority classes.\n\u000fA split with a ﬁxed number of training samples, eq-\nually sized for each class, with 2,000 and 1,000 sam-\nples per class for the genre and style data sets, re-\nspectively. This excludes minority classes with less\nthan the required number of samples.\nFinally, we apply stratiﬁcation on other criteria than just\nthe ground truth class, namely:\n\u000fSplits into training and test sets with an artist ﬁlter,\ni.e. avoiding to have the same artist in both the train-\ning and test set; both stratiﬁed and non-stratiﬁed sets\nare provided\n\u000fAs above, but with an album ﬁlter, i.e. no songs from\nthe same album appear in both training and test set,\nto account for more immediate production effects\n\u000fAs above, but with a time ﬁlter, i.e. for each genre\nusing the earlier songs in the training set, and the\nlater releases in the test set.\nFull details on the results for predictions for the differ-\nent tasks outlined above are available on our website. In\nthis paper, we discuss the results of a musical genre clas-\nsiﬁcation experiment on the MSD Allmusic Guide Style\nDataset (MASD) with a frequently-used 2/3 training and\n1/3 test set split.\nTable 5 shows classiﬁcation accuracies obtained with\nﬁve different classiﬁers using the WEKA Machine Learn-\ning Toolkit [14], version 3.6.6. Speciﬁcally, we employed\nNa¨ıve Bayes, Support Vector Machines (polynomial ker-\nnel with exponent 1), k-nearest Neighbours with k=1, a\nJ48 Decision Tree, and Random Forests, with the default\nsettings. Due to space limitations, we selected the most\ninteresting of the feature sets. The number in parentheses\nafter the feature set name corresponds to the number given\nin Table 2. Bold print indicates the best, italics the second\nbest result per feature set (column-wise).\nFor this classiﬁcation task, we have 25 categories, for\nwhich the biggest “Pop Indie” accounts for 6.60% of the\nsongs, which is thus the lowest baseline for our classiﬁers.\nIt can be noted from the results that the jMIR MFFC fea-\ntures provide the best results on the Na ¨ıve Bayes classiﬁer,followed by the jMIR low-level spectral features. How-\never, all results on this classiﬁer are just roughly twice as\ngood as the baseline identiﬁed above, and low in abso-\nlute terms. Better results have been achieved with Support\nVector Machines and k-NN classiﬁers, on both the Statisti-\ncal Spectrum Descriptors achieve more than 27% accuracy.\nAlso on the other two classiﬁers, Random Forests and De-\ncision Trees, the SSD feature set is the best, followed by\neither the derivatives of the jMIR spectral features, or the\njMIR MFFC implementation.\n6. CONCLUSION AND FUTURE WORK\nBenchmarking is an important aspect in experimental sci-\nences – results reported by individual research groups need\nto be comparable. Important aspects of these are common\nplatforms to exchange these results, and datasets that can\nbe easily shared among researchers, together with a set\nof deﬁned tasks. The MIR community has traditionally\nsuffered from only few (and small) data collections be-\ning available, also complicated by stringent copyright laws\non music. Recently, the publication of the Million Song\nDataset has aimed at alleviate these issues. The dataset\ncomes with associated metadata and a basic set of features\nextracted from the audio. Other modalities such as lyrics\nhave subsequently been provided for (parts of the) collec-\ntion.\nTo increase the usefulness of the dataset, we presented\na wide range of other features extracted from the audio sig-\nnals, and enabled musical genre classiﬁcation tasks by pro-\nviding a ground-truth annotation to a signiﬁcant part of the\ndataset. To foster exchange between different researchers,\nwe deﬁned a number of tasks by providing standardised\nsplits between training and test data.\nOur goal is to create a collaborative research environ-\nment for sharing data and adding new features (by inviting\nother researchers to submit their algorithms), also for other\ndata sets besides the MSD. We will extend the features pro-\nvided also by features for each short segment of the audio\nanalysed, similar to the Echonest features currently avail-\nable for the MSD, which will allow for time-based analysis\nover a song. The platform shall also allow sharing of re-\nsults. This is an important aspect in experimental research,\nas researchers normally know well how to tune their own\nalgorithms and to optimise parameters to achieve better re-\nsults – but when they utilise other algorithms for compari-\nson, we often simply apply the default parameter settings,\nwhich does not create a realistic baseline Such a collabo-\nrative platform will allow fairer comparisons, relieving re-\nsearchers from the need to run all permutations of feature\nextractions and settings, and will enable moving towards\nevaluation platforms as described in [1].\n7. REFERENCES\n[1] Timothy G. Armstrong, Alistair Moffat, William Web-\nber, and Justin Zobel. Improvements that don’t add\nup: ad-hoc retrieval results since 1998. In Proc. of the\nACM Conf. on Information and Knowledge Manage-\nment, CIKM ’09, New York, USA, 2009.[2] James Bergstra, Re Lacoste, and Douglas Eck. Predict-\ning genre labels for artists using freedb. In Proc. of the\nInt. Conf. on Music Information Retrieval, 2006.\n[3] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProc. of the Int. Conf. on Music Information Re-\ntrieval, 2011.\n[4] Dave Datta. Managing metadata. In Proc. of the Int.\nConf. on Music Information Retrieval, Paris, France,\nOctober 2002.\n[5] J. Stephen Downie, Joe Futrelle, and David K. Tcheng.\nThe international music information retrieval systems\nevaluation laboratory: Governance, access and secu-\nrity. In Proc. of the Int. Conf. on Music Information\nRetrieval, Barcelona, Spain, October 10-14 2004.\n[6] M. Goto. A chorus section detection method for musi-\ncal audio signals and its application to a music listen-\ning station. IEEE Trans. on Audio, Speech & Language\nProcessing, 14(5):1783–1794, 2006.\n[7] Michel Jubb, Alma Swan, and Sheridan Brown. To\nshare or not to share. Publication and Quality Assur-\nance of Research Data Outputs. Technical report, Re-\nsearch Information Network, 2008.\n[8] Olivier Lartillot, Petri Toiviainen, and Tuomas Eerola.\nA matlab toolbox for music information retrieval.\npages 261–268. 2008.\n[9] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiﬁcation. In Proc. of the Int. Conf.\non Music Information Retrieval, pages 34–41, London,\nUK, September 11-15 2005.\n[10] Thomas Lidy, Carlos N. Silla, Olmo Cornelis, Fabien\nGouyon, Andreas Rauber, Celso A. A. Kaestner, and\nAlessandro L. Koerich. On the suitability of state-of-\nthe-art music information retrieval methods for ana-\nlyzing, categorizing, structuring and accessing non-\nwestern and ethnic music collections. Signal Process-\ning, 90(4):1032–1048, 2010.\n[11] Cory Mckay. Automatic Music Classiﬁcation with\njMIR. PhD thesis, McGill University, Canada, 2010.\n[12] Lawrence Rabiner and Biing-Hwang Juang. Funda-\nmentals of Speech Recognition. Prentice Hall, 1993.\n[13] George Tzanetakis and Perry Cook. Marsyas: a frame-\nwork for audio analysis. Organized Sound, 4(3):169–\n175, December 1999.\n[14] Ian H. Witten, Eibe Frank, Len Trigg, Mark Hall, Geof-\nfrey Holmes, and Sally Jo Cunningham. Weka: Practi-\ncal Machine Learning Tools and Techniques with Java\nImplementations, 1999."
    },
    {
        "title": "Decoding Tempo and Timing Variations in Music Recordings from Beat Annotations.",
        "author": [
            "Andrew Robertson"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416806",
        "url": "https://doi.org/10.5281/zenodo.1416806",
        "ee": "https://zenodo.org/records/1416806/files/Robertson12.pdf",
        "abstract": "This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing vari- ations due to expressively timed events, phase shifts and er- rors in the annotation times. These deviations tend to prop- agate into the tempo graph and so tempo analysis meth- ods tend to average over recent inter-beat intervals. How- ever, whilst this minimises the effect such timing devia- tions have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the opti- mal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing.",
        "zenodo_id": 1416806,
        "dblp_key": "conf/ismir/Robertson12",
        "keywords": [
            "tempo",
            "inter-beat interval",
            "expressively timed events",
            "phase shifts",
            "annotations",
            "tempo graph",
            "tempo analysis methods",
            "tempo path",
            "cost function",
            "tempo change"
        ],
        "content": "DECODING TEMPO AND TIMING VARIATIONS IN MUSIC\nRECORDINGS FROM BEAT ANNOTATIONS\nAndrew Robertson\nSchool of Electronic Engineering and Computer Science\nandrew.robertson@eecs.qmul.ac.uk\nABSTRACT\nThis paper addresses the problem of determining tempo\nand timing data from a list of beat annotations. Whilst\nan approximation to the tempo can be calculated from the\ninter-beat interval, the annotations also include timing vari-\nations due to expressively timed events, phase shifts and er-\nrors in the annotation times. These deviations tend to prop-\nagate into the tempo graph and so tempo analysis meth-\nods tend to average over recent inter-beat intervals. How-\never, whilst this minimises the effect such timing devia-\ntions have on the local tempo estimate, it also obscures the\nexpressive timing devices used by the performer. Here we\npropose a more formal method for calculation of the opti-\nmal tempo path through use of an appropriate cost function\nthat incorporates tempo change, phase shift and expressive\ntiming.\n1. INTRODUCTION\nMusicologists are interested in how individual perform-\ners convey musical expression, which can manifest it-\nself through control of dynamics, instrumental timbre and\nthrough tempo and timing variation. Honing [9] describes\nperformed rhythm as consisting of three aspects: the rhyth-\nmic pattern, the tempo or speed of the performed pattern,\nand expressive timing deviations. Whilst the tempo can\nbe understood as the rate at which beats occur, the onset\ntime of a note is also dependent upon deviation from strict\nmetrical time. Indeed, deviation from the score is a crucial\naspect of musical performance and these variations have\nbeen found to be systematic [15]. Vercoe [16] characterises\nthe relationship between score and performance “as if the\nmusical score acts as a carrier signal for other things we\nprefer to process”.\nGouyon and Dixon [8] present the difﬁculty of\nanalysing performance data in that “the two dimensions\nof tempo and timing are projected onto the single axis of\ntime”. At the extreme, any tempo change can be repre-\nsented as a sequence of timing changes and vice-versa.\nOne simple way to represent tempo is to use the instanta-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\nFigure 1. Four time lines illustrating the difference be-\ntween the different timing variations (after Gouyon and\nDixon [8]).\nneous inter-beat interval, but in doing so all expressive tim-\ning information has also been included. Desain and Hon-\ning [6] criticise the use of such “tempo curves” as mean-\ningful representations of timing, arguing that expressive\nfeatures, such as rubato, do not scale linearly with tempo,\nand that timing must be understood in relation to musical\nstructure and phrasing. Whilst a simple moving average\ncan help to smooth this estimate, these other timing devia-\ntions remain hidden within the tempo data and there is no\nexplanation how these two aspects of timing might relate\nto each other.\nDespite these difﬁculties, it is still possible to attribute\nvalues to the tempo as it changes over throughout a piece,\nalbeit with some inherent uncertainty. In rock and pop\nmusic, where the tempo is often approximately steady,\nbeat trackers are successfully used to track tempo changes\nand when evaluated do so relatively well when compared\nwith human listeners performing the same task [13]. The\nPerformance Worm [7] provides real-time visualisation of\ntempo and dynamics by clustering inter-onset intervals.\nFor scored music, M ¨uller et al. [14] generate a tempo graph\nby aligning a “neutral” MIDI ﬁle, in which the tempo is\nconstant, to the audio recording through the matching of\nchroma-onset features [14]. The tempo graph is calculated\nby using using windowing techniques to compute the aver-\nage tempo in each local region.\n1.1 Timing variations\nOur model makes use of a framework provided by Gouyon\nand Dixon [8], who enumerate three types of timing vari-\nation: expressively timed events, local tempo variation or\nphase shift, and global tempo variation or tempo change.\nFigure 1 shows their illustration of each of these types for asingle late event where the preceding events are a series of\nregularly spaced events. It should be noted that at the point\nin time where the event happens, it is unknown which type\nof timing variation has occurred.\nIn the case of expressively timed events, the event hap-\npens early or (in this case) late, but subsequent events are\nunaffected with respect to timing. The displacement oc-\ncurs merely for the expressively timed event, but the un-\nderlying sequence is constantly spaced. In a local tempo\nchange (or phase shift), there is a displacement for both\nthe event and all subsequent events. So whilst the time be-\ntween events, the underlying tempo, remains constant, the\nphase shift represents a variation in the interval. The global\ntempo change (or tempo variation) occurs when there is a\nchange to the interval duration which continues in all sub-\nsequent intervals. This would be heard as a slowing down\nor speeding up of the events.\n2. METHODOLOGY\nIntuition might suggest that once the beat times in a record-\ning are annotated that the instantaneous tempo is thereby\nknown directly. We can calculate the tempo at annotation i\nat timetifrom the beat period ti\u0000ti\u00001. If these annotation\ntimes are in milliseconds, the tempo in beats per minute\n(BPM) is 60000=(ti\u0000ti\u00001). However, in practice, such\na tempo graph is often jagged and then requires smooth-\ning to extract what is taken to be the underlying tempo.\nThe reason for this is the conﬂation of tempo and timing\n(phase) variations, described above. Thus a local tempo\nchange or phase shift will be represented by ﬁrst a global\ntempo change in one direction and then a reverse change\nin the other. The smoothing process discards information\nabout expressively timed events and phase shifts, as there\nis no explicit interpretation of the annotations in terms of\npotential timing variations.\nHere, we propose a formal solution to this problem,\nwhich calculates the optimal timing variations according\nto a set of associated cost functions designed to penalise\ntempo change, expressive timing and phase shifts. By cal-\nculating the accumulated cost across a multitude of tem-\nporal locations (phases) and tempi, we can use the well-\nknown dynamic programming technique to then trace the\nsolution with least cost back through the song.\n2.1 Input: Annotated beat times\nFor simplicity, we describe here how the method works for\nannotations at the beat level. However, the input can also\nbe annotations at the note level, in which case the input\ncontains both the event time and the quantised event loca-\ntion in beats and bars.\nWe shall assume that exact beat annotations exist for the\naudio recording. These take the form of a list of beat times,\nin seconds, and may have been generated either algorithmi-\ncally or by hand. One program allowing the creation of an-\nnotated audio data is Sonic Visualiser [2] [1], designed to\nprovide visualisation of audio analysis features using the\nV AMP plugin format. One such plugin is a beat track-\nxxxx\ncost \nof t 2annotation time\nannotation index0 1 2 3Figure 2. Illustration of the costs incurred by a sequence\nof isochronous pulses (circles) relative to the sequences of\nannotated beatsfx1;x2;:::g. The cost for the pulse at an-\nnotation time t2is illustrated as the distance between the\npulse and the annotation time.\ning algorithm, based on work by Davies and Plumbley [5],\nwhich automatically labels the beats. It is also possible\nto manipulate these annotations, so that in cases where the\nbeat is not exactly correct, it may be pushed earlier or later.\nSonic Visualiser also supports the creation of hand anno-\ntated audio data by tapping the keyboard or use of a MIDI\ninterface. This data may then be exported as a text ﬁle.\nThe analysis proceeds on the assumption that the an-\nnotations indicate where the event actually occurred. The\nact of tapping along by hand, and use of the algorithmic\nbeat tracker described above, will actually smooth the data\nby placing the beat towards the general trend rather than\nwhere each note onset happens. Thus if one wants to ex-\ntract information about precise timing variations via this\nmethod, it would be advisable to then edit by hand so that\nthe beat annotations are as close as possible to where the\nonsets occur in the audio. In the literature, a correspond-\ning difference can also be found between predictive beat\ntrackers which place beats causally and therefore smooth\nthe output, and descriptive ones which place the beat after\nanalysing the whole ﬁle and provide the ground truth of\nwhere the beat occurred [8].\n2.2 Timing Transition costs\nGiven a set of annotated beat times as input, ft0;t1;t2;::g,\nwe evaluate a total cost for each possible path of tempo and\ntiming variations.\nLet us deﬁne a beat path as consisting of a sequence of\nevent timesf\u00120;\u00121;\u00122;:::g, each with an associated beat\nperiod of\u001cims. These event times deﬁne the underlying\nbeat and involve transitions in tempo and phase which in-\ncur costs. We can express each point on the path as a pair\nconsisting of a phase location (the time of the event) and atempo (as a beat period). Thus the point on the path cor-\nresponding to annotated time tiis(\u0012i;\u001ci). Now we shall\ndeﬁne the cost for this path and for the possible timing\nvariations within it.\nFirstly, the annotated beat time timight be expressively\ntimed relative to this beat path, and the cost incurred is\nj\u0012i\u0000tij, equivalent to the time difference in ms. In Figure\n2, we see the cost of an annotated path relative to a series\nof isochronous pulses. The cost is simply the sum of the\nerror between the two.\nSecondly, the path may involve a phase shift or local\ntempo change. A tempo and phase pair (\u0012i\u00001;\u001ci\u00001)at an-\nnotation time ti\u00001naturally implies that the next point on\nthe path at annotation time tiwill be at (\u0012i\u00001+\u001ci\u00001;\u001ci\u00001).\nHowever, there may occur a phase shift of xms, so that the\nnext phase\u0012iis in fact\u0012i\u00001+\u001ci\u00001+x. Then an additional\ncost is incurred of \u000bx, where\u000bis a parameter set by hand.\nThirdly, the path may involve a tempo change. Suppose\nwe change from tempo of period \u001ci\u00001to a tempo of \u001ci\u00001+\nx, making this the next tempo \u001ci, we incur a cost of \fx.\nThe predicted point for such a transition would also have\na phase location of \u0012i\u00001+\u001ci\u00001+x, although in this case\ndue to the change in tempo.\nTo reﬂect the fact that we wish to penalise phase shifts\nand tempo changes, we set \u000band\fby hand to values\ngreater than 1. We have chosen \u000bto be 1.4, and \fto be 1.8\nin practice although there is no deﬁnitive ‘correct’ value.\n2.3 Updating the cost matrix\nLet us deﬁne the cost matrix \u0000ito be all possible pairs of\ntempo and phase values, each with an associated cost. For\neach point (\u0012;\u001c)in\u0000i, we must consider all the possible\ntransitions from points in \u0000i\u00001.\nSupposing there was no change in tempo or phase, then\na point (\u0012;\u001c)in\u0000i\u00001naturally suggests the next beat loca-\ntion at time tito be\u0012+\u001cwith the beat period remaining\n\u001cms. We will employ dynamic programming to choose\nthe minimum cost so far incurred on a path to (\u0012;\u001c)in\n\u0000i. This is done by working out the respective costs for all\nphase shifts and tempo changes to our new point and then\nchoosing the minimum.\nObserve that the point (\u0012;\u001c)in\u0000ican be reached from\n(\u0012\u0000x\u0000y;\u001c\u0000y)in\u0000i\u00001by a tempo transition of yms\n(from\u001c\u0000yto\u001c) and a subsequent phase shift of xms,\nfrom the predicted event time of \u0012\u0000xto\u0012. These incur\ncosts of\fyand\u000bxrespectively. We also need to compute\nthe additional cost for the point, which is given by j\u0012\u0000tij,\nthe discrepancy between the location of the beat and the\nannotation time. Then our full update equation is\n\u0000i(\u0012;\u001c) =minx;yf\u0000i\u00001(\u0012\u0000x\u0000y;\u001c\u0000y)+\u000bx +\fyg+j\u0012\u0000tij:\n(1)\n2.4 Backwards Path calculation\nHaving calculated the cost matrix \u0000ifor each annotated\nbeat times, ti, we ﬁnd the minimum point in the ﬁnal\nmatrix and the corresponding backwards path. Thus, wechoose\n(\u0012N;\u001cN) =min\u0012;\u001c\u0000N(\u0012;\u001c) (2)\nThen we iterate back to ﬁnd each previous point in the ma-\ntrix that was chosen by Equation 1. This gives the com-\nplete path through the annotated beat times with the lowest\ncost for our parameters \u000band\f. This path can be seen as\nthe optimal explanation of the sequence of annotated beat\ntimes as a combination of tempo changes, phase shifts and\nexpressively timed events.\n2.5 Computational considerations\nFor our tempo analysis to be reasonably quick, we made\nuse of some simpliﬁcations to reduce the computation\ntime. By considering only those phase locations within\noccur within a ﬁxed range either side of the beat annota-\ntion, we can discard points in the cost matrix which would\nalmost certainly never occur. Similarly the tempo range\nwas determined to be a ﬁxed range either side of the inter-\nval between the two most recent annotations. These two\nranges can be set by hand, depending on the nature of the\npiece.\nAlso, our data has a ﬁxed temporal resolution. By\nchoosing integers to represent note onset times, we have\nthereby chosen to use a precision of 1 ms for the resolu-\ntion of both phase and beat period. However, by changing\nthis to 2 ms or higher, the computation time can be reduced\nto a few seconds for a whole song without any signiﬁcant\ndegradation of the output.\n3. PERFORMANCE ANALYSIS\nThe resulting tempo path is signiﬁcantly more helpful\nwhen seeking to understand the global tempo changes in\na performance than simply plotting the inter-beat intervals.\nWe visualise the data using a standard tempo curve which\nplots the graph of tempo, or beat period, against the beat\nannotation index, i.e. plotting \u001ci. Expressive timing infor-\nmation can be shown by placing a dot above or below this\npoint (i;\u001ci), whereby if the beat annotation occurs xms\nafter the location of the path point, then the dot is xms\nabove the tempo curve. In the ﬁgures below, for simplicity\nof presentation we have translated the beat period into the\nmore commonly found representation as BPM. Whilst this\nthereby omits speciﬁc units for the expressive timing and\nphase shift information, we consider the beneﬁts in under-\nstanding the tempo information to make this worthwhile.\n3.1 The Beatles Dataset\nAn example of this can be seen in Figure 3. The input data\nwas used was ground-truth annotations to the Beatles’ song\n‘Taxman’ from the album ‘Revolver’ [12]. The annotations\nwere created in a semi-automatic manner, via the use of\na beat tracking algorithm and then corrected by hand [4].\nThe fact that an algorithm was used does mean that some\nsmoothing has taken place, however, our proposed decod-\ning process still provides timing data that offers insightful\ninformation for musicologists.snare rolls\nchorus I\n(‘cause I’m \nthe Taxman..’)chorus II\n (‘cause I’m the Taxman..’)verse I\n(‘Let me tell you \nhow it will be...’)verse II\n(‘Should five percent \nappear too small...’)Tempo \n(BPM)\nBeat IndexFigure 3. Tempo graph for The Beatles’ ‘Taxman’.\nThis analysis of the song indicates considerable com-\nplexity in Ringo Starr’s time-keeping. He is both sensitive\nand in control of small ﬂuctuations in tempo that generate\na ‘feel’ to the different sections of the song. The decoded\ntiming data displays clear small rises in tempo during the\nsnare rolls that precede both the ﬁrst and second choruses.\nThere are then clear drops in tempo for both choruses of\napproximately 2 BPM, and this remains the case for later\nchoruses beyond the scope of the Figure.\nOne can also observe a general trend in the expressive\ntiming such that the timing of the second and fourth beats\nof the bar appears to be marginally later than the ‘1’ and\nthe ‘3’. On these beats, the song tends to feature the snare\nbackbeat, as is common in rock music [10], and a regu-\nlar guitar motif consisting of a staccato chord. Calculating\nthe mean over the whole song conﬁrms this, with the mean\noffsets being 0:25, 3:86, 0:82 and2:18 ms for the respec-\ntive beats. Drummers consider that placing the snare hit\non ‘2’ and ‘4’ fractionally later, results in a more relaxed\nfeel [11]. Such analysis supports the hypothesis that trends\nin microtime deviation lend a particular ‘feel’ to a song.\n3.2 Beethoven’s Moonlight Sonata\nChew [3] presents a detailed analysis of the timing varia-\ntions in three different performances of Beethoven’s Piano\nSonata No. 14 in C Sharp Minor, Op. 27 No. 2: I. Ada-\ngio sostenuto, known as the ‘Moonlight Sonata’. These\nrecordings, by Daniel Barenboim1, Artur Schnabel2and\nMaurizio Pollini3, were initially used in an invited lec-\nture by Jeanne Bamberger. This piece consists of repeated\ngroups of four triplets in the right hand, with a movement\nbetween different chords. Such a repetitive structure suits\nit for revealing the tendencies of different performers with\n1On Beethoven: Moonlight, Pathtique and Appassionata Sonata CD\nHamburg, Germany: Deutsche Grammophon GmbH.\n2On Artur Schnabel CD United Kingdom: EMI Records Ltd.\n3On Beethoven Piano Sonatas: Moonlight and Pastorale CD, Ham-\nburg, Germany: Deutsche Grammophon GmbH.respect to their tempo and microtime variations. We have\nused of the same hand-annotated data, created using Sonic\nVisualiser.\nIn creating the tempo graphs, Chew comments on the\nnecessity of smoothing to make the data understandable by\nthe human eye, whilst warning that over-smoothing can re-\nsult in important details being obscured. By use of the pro-\nposed method, we obtain smooth tempo graphs, but also\npreserve information about expressive timing and phase\nshifts.\nThe tempo graph for Pollini’s performance is shown\nin Figure 4. Chew notes how the local minima of the\ntempo graph all occur on the bar boundaries. Bamberger\ncontrasts this with the rendition by Schnabel, explaining\nthat whereas other performers appear to ‘stop’ with each\nbass note at the beginning of the bar, Schnabel progresses\nthrough until the end of the ﬁrst complete phrase after four\nbars “as if in one long breath”. The extracted tempo and\ntiming information for Schnabel’s performance can be seen\nin Figure 5. Later in the piece, we can recognise a simi-\nlar pattern to that exhibited by Pollini, whereby there is a\nslowing at the beginning of each bar.\nThis example can also serve to demonstrate some ad-\nvantages of our proposed decoding method. The resulting\ntempo graph has less of the jagged edges that are still found\nin Chew’s smoothed tempo graph. This is due to the pro-\njection of other timing data, expressive timing and phase\nshifts, onto the tempo curve. Instead, these quantities are\nmade explicit and removed from the tempo curve, and\nthereby allowing us to calculate data relating to the phras-\ning of the notes. In this piece, we can observe that the third\ntriplet eighth note exhibits a tendency to be marginally ear-\nlier than the ﬁrst two notes of the bar. This would indicate\nthat it thus begins earlier and is held fractionally longer.\nWe have calculated the average deviation for each note and\nthese results are presented in Table 1.BarsFigure 4. Tempo graph for Pollini’s recording of Beethoven’s Moonlight Sonata. The lighter vertical lines indicate crotchet\nboundaries and the darker vertical lines indicate bar boundaries. The expressive timing information is represented by the\ndots and phase shifts are represented by lines. Where the dot is above the tempo graph, the event is late relative to the time\npredicted by the underlying tempo; where the dot is below the event is early. Similarly a phase shift later is represented\nby a vertical line upwards from the tempo graph and a phase shift early by a line below the tempo graph. The tempo is\nindicated by BPM values to the left in 8 BPM intervals. The expressive timing and phase shift quantities are such that the\nequivalent markers correspond to 20 ms intervals.\nBars\nFigure 5. Tempo graph for Schnabel’s recording of Beethoven’s Moonlight Sonata. Again the lighter vertical lines indicate\ncrotchet boundaries and the darker vertical lines indicate bar boundaries. the end of the ﬁrst phrase is after four bars.\n4. IMPLEMENTATION\nThe program was written in C++, using openFrameworks\nto provide visualisation using openGL libraries. The code\nis freely available for download at the Sound Softwarewebsite4, thereby allowing other researchers to import an-\nnotations. Both the resulting timing information and a ﬁle\nof the processed beat location times can then be exported as\ntext ﬁles. Sonic Visualiser supports the loading of the pro-\ncessed annotations, which can then be soniﬁed. In informal\n4https://code.soundsoftware.ac.uk/projects/performance-timing-\nanalyserTriplet note index\nPerformer 1 2 3\nBarenboim 7.27 2.56 0.83\nPollini 6.48 5.11 2.50\nSchnabel 5.20 4.45 0.76\nTable 1. Average deviation by triplet eighth note position\nin ms for the three performances.\ntests, our processing algorithm appeared to have smoothed\nthe data well, elimating timing errors whilst preserving the\ntiming variations we are interested in.\n5. CONCLUSION\nIn this paper, we present a new method for extracting the\noptimal tempo and timing path from a list of onset anno-\ntations. The output contains both tempo and expressive\ntiming information for the optimal path according to our\ncost parameters. Such information enables a detailed musi-\ncological analysis of how performance timing data relates\nto musical structure. We have investigated how such data\nmight be used in a classical case with the study of three\nperformances of Beethoven’s Moonlight Sonata, and in the\nrock and pop case through studying the timing of songs by\nThe Beatles.\nIn future, we seek to extend our application of this\nmethod to the analysis of other annotated audio and de-\nvelop a better understanding of how musicians make use\nof tempo and timing variations in expressive performance.\nWe envisage that such work might also lead to improve-\nments in the expressivity of computer-generated parts.\n6. ACKNOWLEDGEMENTS\nThanks to Elaine Chew for making available the annota-\ntions for the Beethoven piano sonata recordings. Thanks\nto the EPSRC and the Royal Academy of Engineering for\nsupporting this research.\n7. REFERENCES\n[1] C. Cannam, C. Landone, and M. Sandler. Sonic Vi-\nsualiser: An open source application for viewing,\nanalysing, and annotating music audio ﬁles. In Pro-\nceedings of the ACM Multimedia 2010 International\nConference, Firenze, Italy, October 2010., pages 1467–\n1468, 2010.\n[2] Chris Cannam, Chris Landone, Mark B. Sandler, and\nJ.P. Bello. The Sonic Visualiser: A visualisation plat-\nform for semantic descriptors from musical signals.\nInProceedings of the 7th International Conference on\nMusic Information Retrieval (ISMIR-06), 2006.\n[3] Elaine Chew. About time: Strategies of performance\nrevealed in graphs. Visions of Research in Music Edu-\ncation, 20, 2012.[4] M. E. P. Davies, N. Degara, and M. D. Plumbley. Eval-\nuation methods for musical audio beat tracking algo-\nrithms. technical report c4dm-tr-09-06. Technical re-\nport, Queen Mary University of London, Centre for\nDigital Music., 2009.\n[5] M. E. P. Davies and M. D. Plumbley. Context-\ndependent beat tracking of musical audio. IEEE Trans-\nactions on Audio, Speech and Language Processing,\n15(3):1009–1020, 2007.\n[6] Peter Desain and Henkjan Honing. Tempo curves con-\nsidered harmful: A critical review of the representation\nof timing in computer music. In Proceedings of Inter-\nnational Computer Music Conference, pages 143–149,\n1991.\n[7] Simon Dixon, Werner Goebl, and Gerhard Widmer.\nThe Performance Worm: Real time visualisation based\non langner’s represen- tation. In Proceedings of the In-\nternational Computer Music Conference, 2002.\n[8] Fabien Gouyon and Simon Dixon. A review of au-\ntomatic rhythm description systems. Computer Music\nJournal, 29(1):34–54, 2005.\n[9] Henkjan Honing. From time to time: The representa-\ntion of timing and tempo. Computer Music Journal,\n25(3):50–61, 2002.\n[10] Tommy Igoe. In the Pocket. Essential Grooves. Part 2.\nFunk. Modern Drummer, July 2006.\n[11] Vijay Iyer. Microstructures of Feel, Macrostructures\nof Sound: Embodied Cognition in West African and\nAfrican-American Musics. PhD thesis, University of\nCalifornia, Berkeley, 1998.\n[12] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Kolozali, D. Tidhar, and M. Sandler. Omras2 meta-\ndata project 2009. In Late-breaking session at the 10th\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2009), 2009.\n[13] M. F. McKinney, D. Moelants, M. E. P. Davies, and\nA. Klapuri. Evaluation of audio beat tracking and mu-\nsic tempo extraction algorithms. Journal of New Music\nResearch, 36(1):1–16, 2007.\n[14] Meinard M ¨uller, Verena Konz, Andi Scharfstein, Se-\nbastian Ewert, and Michael Clausen. Toward auto-\nmated extraction of tempo parameters from expressive\nmusic recordings. In Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR), Kobe, Japan., 2009.\n[15] Bruno H. Repp. Patterns of expressive timing in per-\nformances of a beethoven minuet by nineteen famous\npianists. Psychology of Music, 22:157–167, 1995.\n[16] Barry Vercoe and Miller Puckette. Synthetic Rehearsal,\ntraining the Synthetic Performer. In Proceedings of\nthe International Computer Music Conference (ICMC\n1985), pages 275–278, 1985."
    },
    {
        "title": "Influence of Peak Selection Methods on Onset Detection.",
        "author": [
            "Carlos Rosão",
            "Ricardo Ribeiro 0001",
            "David Martins de Matos"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417271",
        "url": "https://doi.org/10.5281/zenodo.1417271",
        "ee": "https://zenodo.org/records/1417271/files/Rosao0M12.pdf",
        "abstract": "Finding the starting time of musical notes in an audio sig- nal, that is, to perform onset detection, is an important task as this information can be used as the basis for high-level musical processing tasks. Many different methods exist to perform onset detection. However their results depend on a Peak Selection step that makes the decision whether an on- set is present at some point in time. In this paper we review a number of different Peak Selection methods and compare their influence in the performance of different onset detec- tion methods and on 4 distinct onset classes. Our results show that the post-processing method used deeply influ- ences both positively and negatively the results obtained.",
        "zenodo_id": 1417271,
        "dblp_key": "conf/ismir/Rosao0M12",
        "keywords": [
            "audio signal",
            "onset detection",
            "musical processing tasks",
            "Peak Selection step",
            "different onset detection methods",
            "onset classes",
            "post-processing method",
            "influence",
            "performance",
            "results"
        ],
        "content": "INFLUENCE OF PEAK SELECTION METHODS ON ONSET DETECTION\nCarlos Ros ˜ao\nISCTE-IUL\nL2F/INESC-ID Lisboa\nrosao@l2f.inesc-id.ptRicardo Ribeiro\nISCTE-IUL\nL2F/INESC-ID Lisboa\nrdmr@l2f.inesc-id.ptDavid Martins de Matos\nIST/UTL\nL2F/INESC-ID Lisboa\ndavid@l2f.inesc-id.pt\nABSTRACT\nFinding the starting time of musical notes in an audio sig-\nnal, that is, to perform onset detection, is an important task\nas this information can be used as the basis for high-level\nmusical processing tasks. Many different methods exist to\nperform onset detection. However their results depend on a\nPeak Selection step that makes the decision whether an on-\nset is present at some point in time. In this paper we review\na number of different Peak Selection methods and compare\ntheir inﬂuence in the performance of different onset detec-\ntion methods and on 4 distinct onset classes. Our results\nshow that the post-processing method used deeply inﬂu-\nences both positively and negatively the results obtained.\n1. INTRODUCTION\nIn general, music is composed by sounds generated si-\nmultaneously by several musical instruments of different\nkinds [7]. Thus, one can consider the notes played by these\nmusical instruments as the basic unit or syllable for a mu-\nsical signal [7]. These notes are what allows us humans to\nclap our hands when listening to a music or whistle/hum\nthe melody of a familiar song [5].\nThere has been intense research in this area for quite\nsome time, mostly because the information about the start-\ning moments of musical notes can be used as a ﬁrst step\nfor high-level music processing techniques, such as Chord\nEstimation, Harmonic Description or Music Genre Classi-\nﬁcation.\nIn this paper we are mainly interested in studying how\nthe post-processing part of the onset detection methods,\nthat is, the Peak Selection part in Fig. 1, responsible for de-\nciding whether a point in time is an onset, inﬂuences the re-\nsults obtained. This can be of great help in case one wants\nto know the more appropriate Onset Detection method –\nand consequently Peak Selection Method – to use in a par-\nticular application.\nIn the next section, we will present the most common\nonset detection methods, while in Section 3 we introduce\nthe Peak Selection Methods used. Section 4 describes our\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\nPre-processing\nPeak SelectionReductionAudio OnsetsFigure 1. Traditional onset detection work-ﬂow [4].\nexperiments and discusses the obtained results. The paper\nends with ﬁnal remarks and future work.\n2. ONSET DETECTION METHODS\nMany Onset Detection Methods have been proposed dur-\ning the years and most of them follow the general scheme\nin Fig. 1 which comprises the following steps [1, 4, 5]:\n\u000fPre-processing of the signal in order to highlight its\nmost important properties [1, 4].\n\u000fCreation of a Onset Detection Function, also called\nOnset Strength Signal (OSS)1, that is, a function\nwhose peaks should correspond to onset times [2].\n\u000fPeak Selection, in order to decide which peaks in the\nOnset Detection Function are onsets.\nNext, we brieﬂy review the Onset Detection Functions\nlater used to assess the inﬂuence of the Peak Selection part\nof detecting onsets. For a more general overview of onset\nOnset Detection Functions, check, for instance, [12] and\nfor a thorough comparison of the performances of the dif-\nferent OSS check, for instance [1] or [13].\nIn order to detect variations in the properties of the au-\ndio signal [2], one can create an OSS by lowering the sam-\nple rate of the signal without losing relevant information.\nThis a process called Reduction [1].\nAll the OSS we will explore are based on Spectral Fea-\ntures of the signal. In order to change from the time-domain\nto the spectral-domain representation of the audio, we make\nuse of the Short-time Fourier Transform (STFT).\nHigh Frequency Content Making use of the fact that typ-\nically, when compared to other audio sources, an on-\nset has relative high energy in higher frequencies [1,\n1In this paper we use the terms Onset Detection Function and OSS\ninterchangeably.11], it is possible to create a Onset Detection Func-\ntion that weights each STFT bin proportionally to its\nfrequency. This function is called High Frequency\nContent (HFC).\nSpectral Difference Another possibility to deﬁne an OSS\nis to create a function that measures the variation\nof magnitude between frequency bins [2, 4]. This\ntype of OSS is called Spectral Difference or Spec-\ntral Flux (SF).\nPhase Deviation One can also look for onsets by search-\ning for irregularities in the phase of consecutive fre-\nquency bins [2], and that is what does the Phase De-\nviation (PD) Onset Detection Function.\nIt is possible to improve this function by weighting\n– Weighted Phase Deviation (WPD) – and normal-\nization [2].\nComplex Domain It is possible to combine information\nfrom the both the energy and phase of the spectrum\nto create a Complex Domain (CD) function [3]. This\nkind of function looks for irregularities in the steady-\nstate of the signal [2] .\nA possible improvement for this method is to rectify\nthe function so that it ignores offsets and focuses on\nonsets [2] – Rectiﬁed Complex Domain (RCD).\n3. PEAK SELECTION METHODS\nA function created with any of the methods introduced in\nSection 2 will typically show well-localized maxima in po-\nsitions corresponding to onset times [1]. To extract the on-\nset times from the OSS, Peak Selection methods are used\nthat typically include the steps: Post-processing, Thresh-\nolding and Peak-picking.\n3.1 Post-processing\nPost-processing aims at making the Onset Detection Func-\ntion uniform so that the processes of thresholding and peak-\npicking will be easier. This process of increasing the uni-\nformity of the Onset Detection Function typically makes\nuse of normalization methods and ﬁlters.\nThe normalization typically works in one of two ways [2,\n5]: (i) Subtract the average value of the function from each\nvalue, so that the average will be zero and then divide by\nthe maximum value so that the function will be in the in-\nterval [-1,1]; (ii) Subtract the average value of the function\nfrom each value and then divide by the maximum absolute\ndeviation, so that the average will be 0 and the standard\ndeviation 1.\nThe ﬁlters used are typically low-pass ﬁlters [1, 2, 5],\nwhich, in general, select low frequencies up to the cut-\noff frequency (f c) and attenuate frequencies higher than\nfc[14] and can be deﬁned as\nyi=\u000bxi+ (1\u0000\u000b)yi\u00001 (1)\nwhere\u000bis the smoothing factor.3.2 Thresholding\nIn order to separate event-related from non-event-related\npeaks in the post-processed Onset Detection Function, d,\nit is common to build a threshold [1].\nOne can deﬁne a constant threshold [8], \u000e, although this\ntype of threshold is not appropriate, because it does not\nconsider the great dynamics common in a musical signal,\nleading to weak results [1]. It is much more common to use\nadaptive thresholds [1, 2, 5]. An adaptive threshold can be\nconstructed in several ways. The best way to overcome\nproblems when facing music pieces with great dynamic\nchange is to build a threshold function based on the local\nmean (Eq. 2) or local median (Eq. 3) of the Onset Detec-\ntion Function, d[6].\n~\u000e(n) =\u000e+\u0015mean(jd (n\u0000M)j;::::;jd(n+M)j) (2)\n~\u000e(n) =\u000e+\u0015median(jd(n\u0000M)j;::::;jd(n+M)j)(3)\nWhere\u0015and\u000eare positive constants, that can be tweaked,\nandMis the size of a window around each of the points of\nthe Onset Detection Function.\n3.3 Peak-picking\nAfter building a threshold function, one must choose which\nvalues of the Onset Detection Function that are larger than\nthe threshold correspond to onsets.\nOne can consider every value greater than the thresh-\nold (w = 0 in the following equation) as an onset, or one\ncan add the condition that it must be a local maximum\n(w> 0) [2, 4] (where wis a tweakable parameter that cor-\nresponds to the size of a window around the value):\no(n) =8\n>>><\n>>>:1ifd(n)>~\u000e(n)\nandd(n\u0000w)\u0014d(n)\u0014d(n+w);\n0otherwise:(4)\n4. RESULTS\nIn this section we will present the evaluation methods and\ndataset used as well as discuss the results obtained.\n4.1 Evaluation Methods\nWhen evaluating onset detection methods, the most com-\nmon criterion is the F-measure, that is deﬁned in Eq. 5.\nF-measure =2\n1\nP+1\nR=2PR\nP+R(5)\nWith Precision, P, and Recall, R, which can be computed\nin terms of the False Positive (FP), True Positive (TP) and\nFalse Negative (FN). In the particular case of onset detec-\ntion, one can interpret the TP as the correctly detected on-\nsets, the FP as falsely detected onsets and the FN as onsets\nthat were not detected.\nThe Precision, that is, the fraction of retrieved instances\nthat are relevant is deﬁned in Eq. 6.\nPrecision =TP\nTP+FP(6)On the other hand, the Recall, that is, the fraction of\nrelevant instances that are retrieved, is obtained by Eq. 7.\nRecall =TP\nTP+FN(7)\nThe Mirex Onset Detection Task speciﬁcations [9], and\nmost of the papers in this area, consider onsets detected as\nTP if they are in a window of 50ms around the annotated\nonset. On the other hand, if more than one detection falls\ninside the same tolerance window, only one is counted as\nTP, the others are considered as FP. When a detection is\ninside the tolerance window of two onset annotations, one\nTP and one FN are counted. We will evaluate our results\naccording to these speciﬁcations.\n4.2 Dataset\nTo run our experiments, we used a dataset built by Bello et\nal. for [1], referred to as the Bello Dataset.\nThe Bello Dataset is a hand-labelled and annotated data-\nset ﬁrst proposed in [1] and used in several papers, such\nas [2, 5]. It contains commercial and non-commercial rec-\nordings, covering a variety of musical styles and instru-\nmentations, totalling 23 songs and 1065 onsets [1]. The\nsongs are available in WA V format (sample rate 22.050\nkHz, mono, 16 bit) and their onset positions (in seconds)\nin text format.\nThe recordings of the dataset can be divided in 4 classes,\naccording to the characteristics of their onsets: Complex\nMixture (Mix), Pitched Non-Percussive (PNP), Pitched Per-\ncussive (PP), and Non-Pitched Percussive (NPP) as shown\nin Table 1.\nNo. Songs No. Onsets\nMix 7 271\nPNP 1 93\nPP 9 489\nNPP 6 212\nTotal 23 1065\nTable 1. Bello Dataset Structure\nOne can think of Mix onsets as onsets produced by any\npolyphonic music where several instruments are playing\ntogether, something that happens, for instance, in a rock\nor pop song. The NPP onsets are the ones typically pro-\nduced by percussion instruments such as drums or cym-\nbals, while the PP onsets are those that have a percussive\ncharacteristic but, nonetheless, still maintain a well deﬁned\npitch; this type of onsets appears, for instance, when a pi-\nano is playing. Finally, the PNP onsets are those that do\nnot have percussive characteristics and have a very well de-\nﬁned pitch; this category contains onsets from instruments\nsuch as bowed strings or wind instruments.\n4.3 Experiments\nIn order to assess the inﬂuence of Peak Selection Methods\non the results of onset detection, different simulations were\nrun each with a particular Peak Selection Method. Thesemethods were selected because they have been used in re-\ncent work [1, 2, 5].\nWe used the following abbreviations to name the used\nPeak Selection Methods:\nnorm Normalize the Onset Detection Function by divid-\ning by the absolute maximum and subtracting the av-\nerage value, so that the average will be zero.\nstdev Normalize the Onset Detection Function by divid-\ning by the maximum standard deviation and sub-\ntracting the average value, so that the average will\nbe zero.\nmean Create a running mean threshold (Eq. 2).\nmedian Create a running median threshold (Eq. 3).\nﬁlter Before normalization, smooth the Onset Detection\nFunction by applying a simple low-pass ﬁlter (Eq. 1).\nno-ﬁlter Do not apply the low-pass ﬁlter, that is, do not\nuse smoothing.\nlocal-max Consider as onsets every value in the Onset De-\ntection Function that is larger than zero, larger than\nthe threshold and is a local maximum in a window\nof 3 samples around it. I.e., use w= 3in Eq. 4.\nno-local-max Consider as onset every value greater than\nthe threshold. In other words, use w= 0in Eq. 4.\nA B C D E\nnorm\u0002\u0002\u0002\u0002\nstdev \u0002\nmean\u0002\nmedian\u0002\u0002\u0002\u0002\nﬁlter \u0002\nlocal-max\u0002\u0002\u0002\u0002\nTable 4. Components of the Peak Selection Methods A, B,\nC, D and E.\nFirst we run our experiments with the Peak Selection\nMethod median-norm-no-ﬁlter-local-max (A), then we re-\nplaced the running mean threshold with a running aver-\nage threshold with parameter M= 10 by running the\nexperiments with the Peak Selection Method mean-norm-\nno-ﬁlter-local-max (B). After that, in order to assess the\ninﬂuence of the type of normalization, we ran the exper-\niments by replacing the norm type of normalization with\nthe stdev type of normalization, that is, using the Peak Se-\nlection Method median-stdev-no-ﬁlter-local-max (C).\nWe also tested the inﬂuence of a smoothing step before\nthe Peak Selection – with the use of a simple low-pass ﬁlter\n– by running the experiments with the median-norm-ﬁlter-\nlocal-max (D) Peak Selection Method.\nFinally, to test the peak picking algorithm’s inﬂuence,\nwe ran the experiments without the local maximum con-\ndition, that is we used the median-norm-no-ﬁlter-no-local-\nmax (E) Peak Selection Method.A B C D E\nOSS F P R F P R F P R F P R F P R\nHFC 0.921 0.922 0.920 0.922 0.957 0.901 0.921 0.922 0.920 0.823 0.913 0.766 0.622 0.525 0.798\nSF 0.931 0.946 0.926 0.943 0.957 0.937 0.934 0.946 0.932 0.939 0.953 0.933 0.782 0.709 0.903\nPD 0.652 0.573 0.819 0.650 0.571 0.819 0.652 0.573 0.819 0.628 0.586 0.749 0.520 0.417 0.893\nWPD 0.916 0.959 0.882 0.922 0.933 0.918 0.914 0.945 0.891 0.828 0.900 0.778 0.603 0.507 0.816\nCD 0.947 0.978 0.923 0.946 0.987 0.913 0.943 0.970 0.923 0.872 0.931 0.835 0.583 0.482 0.820\nRCD 0.933 0.977 0.903 0.933 0.966 0.913 0.936 0.977 0.908 0.909 0.919 0.904 0.419 0.298 0.824\nTable 2. Results with P, Precision, F, F-measure and R, Recall, for NPP onsets in the Bello Dataset using all the 5 Peak\nSelection methods (A, B, C, D, E).\nA B C D E\nOSS F P R F P R F P R F P R F P R\nHFC 0.838 0.846 0.830 0.848 0.829 0.867 0.842 0.846 0.839 0.576 0.607 0.547 0.523 0.437 0.651\nSF 0.961 0.968 0.954 0.965 0.978 0.953 0.961 0.969 0.954 0.876 0.878 0.874 0.893 0.868 0.921\nPD 0.497 0.410 0.734 0.488 0.414 0.740 0.388 0.278 0.823 0.529 0.323 0.823 0.368 0.256 0.732\nWPD 0.810 0.796 0.826 0.811 0.793 0.830 0.811 0.793 0.830 0.470 0.545 0.414 0.666 0.641 0.692\nCD 0.883 0.892 0.874 0.899 0.876 0.923 0.903 0.883 0.923 0.441 0.547 0.370 0.543 0.488 0.611\nRCD 0.882 0.880 0.883 0.891 0.863 0.920 0.881 0.823 0.947 0.599 0.574 0.625 0.734 0.664 0.820\nTable 3. Results with P, Precision, F, F-measure and R, Recall, for PP onsets in the Bello Dataset using all the 5 Peak\nSelection methods (A, B, C, D, E).\n4.4 Discussion\nWhile running the experiments, we ﬁxed the window size\nof each STFT at 1024 samples (that is 46.4 ms in these\n22.05 kHz sampled signals) with a hop size of 50%. The\nparameters\u000eand\u0015were tweaked, in order to obtain the\nvalues that maximize the f-measure.\nThe results obtained by running our experiments with\nall the Peak Selection Methods described in the previous\nsection are shown in Tables 2, 3, 5 and 6.\nIn order to compare the methods, we consider as base\nthe results with the Peak Selection Method A and compare\nall others with this one. First, we will analyse the inﬂu-\nence of the Peak Selection Methods on the results obtained\nfor the different onset classes, next, we will analyse the in-\nﬂuence of the Peak Selection Methods on each OSS, and,\nﬁnally, we will make a global balance about the signiﬁ-\ncance of the compared results of the different Peak Selec-\ntion Methods.\n4.4.1 Onset Classes\nThe differences between running the experiments by using\na running-median threshold – Peak Selection Method A –\nor a running-mean threshold – Peak Selection Method B\n– have mixed behaviours according to the onset classes.\nIn the NPP and PP classes, the mean gives slightly better\nresults (1pp2better) than the median, while it improves\nfor certain OSS it gives worse results for others, but just 1-\n2pp differences for better or for worse. On the other hand,\nthe running-mean threshold is prone to give worse results\nby around 2-3pp in the Mix onset class.\nTo use a normalization based on the maximum standard\ndeviation – Peak Selection Method C – when comparing\nto a normalization based on the maximum absolute value\n– Peak Selection Method A – gives mixed behaviours ac-\ncording to the onset classes. In the NPP and PNP onset\nclasses, the results remain almost the same (the changes\nare less than 1pp) while for the PP the relevant changes\n2pp – percentage point.are a decrease of around 10pp for the PD function and a\nperformance increase of about 3pp for the HFC and CD\nfunctions. When it comes to the Mix onset class, the re-\nsults for the HFC and PD functions remain just the same,\nbut the other OSS functions have worse f-measure (2-3pp).\nWhen smoothing the Onset Detection Function – Peak\nSelection Method D – the results become quite different.\nFor the NPP onset class, the SF becomes slightly better\n(less than 1pp), while for all the other OSS, the results be-\ncome poorer from 3 to 10pp. In the case of PP onsets, the\nﬁlter improves about 3pp on the PD function, although it\ndecreases the results signiﬁcantly (10 to 40pp) for all other\nOSS. In the PNP onset classes, the behaviour is mixed ac-\ncording to the onset class. We have a positive boost of\naround 20pp for the PD OSS while for all the other func-\ntions the results get worse from 4pp to 30pp. For the Mix\nonset class, the results get considerably worse for all the\nOSS.\nFinally, when dropping the local maximum condition in\nthe peak picking algorithm – Peak Selection Method E –\nthe results become quite different, but there is a general\ntrend easy to spot: the results get worse for every OSS\nwithout exception. In the NPP the results are 15 to 50pp\nworse, while for the PP the results are 13 to 25pp worse.\nFor PNP onsets, in general, the results are around 30pp\nworse while for Mix onsets the results vary from 10pp to\n30pp worse.\n4.4.2 OSS\nMoving from running-median threshold to running-mean\nthreshold – Peak Selection Method B – gives, in general,\nslight improvements for the HFC OSS in all the onset clas-\nses, while for the SF OSS the behaviour is mixed. It im-\nproves slightly the SF in PP, NPP and PNP onset classes,\nwhile decreasing the performance in the Mix class, although\nthese improvements and decreases are very small (1-3pp).\nWe have similar behaviour for the WPD, CD and RCD On-\nset Detection Functions, with the increases and decreases\nnot going beyond 3pp. In the case of the PD OSS, the re-A B C D E\nOSS F P R F P R F P R F P R F P R\nHFC 0.553 0.519 0.591 0.552 0.519 0.591 0.553 0.519 0.591 0.405 0.471 0.355 0.358 0.242 0.688\nSF 0.911 0.888 0.935 0.915 0.858 0.978 0.914 0.914 0.914 0.869 0.847 0.892 0.696 0.595 0.839\nPD 0.615 0.479 0.860 0.615 0.479 0.860 0.615 0.479 0.860 0.803 0.770 0.839 0.184 0.101 1\nWPD 0.660 0.602 0.731 0.670 0.626 0.720 0.670 0.626 0.720 0.465 0.506 0.430 0.463 0.343 0.710\nCD 0.684 0.650 0.720 0.680 0.644 0.720 0.677 0.657 0.699 0.388 0.444 0.344 0.409 0.295 0.667\nRCD 0.808 0.745 0.882 0.808 0.745 0.882 0.808 0.745 0.882 0.503 0.500 0.505 0.562 0.425 0.828\nTable 5. Results with P, Precision, F, F-measure and R, Recall, for PNP onsets in the Bello Dataset using all the 5 Peak\nSelection methods (A, B, C, D, E).\nA B C D E\nOSS F P R F P R F P R F P R F P R\nHFC 0.812 0.753 0.881 0.814 0.757 0.881 0.814 0.757 0.881 0.597 0.686 0.528 0.512 0.435 0.626\nSF 0.880 0.922 0.842 0.867 0.895 0.841 0.867 0.889 0.846 0.853 0.844 0.863 0.679 0.693 0.665\nPD 0.540 0.396 0.851 0.540 0.403 0.818 0.544 0.409 0.808 0.491 0.373 0.718 0.458 0.337 0.713\nWPD 0.832 0.762 0.811 0.801 0.797 0.806 0.809 0.791 0.822 0.587 0.630 0.564 0.557 0.625 0.505\nCD 0.866 0.798 0.854 0.844 0.807 0.870 0.843 0.792 0.881 0.541 0.586 0.518 0.522 0.545 0.509\nRCD 0.824 0.823 0.770 0.795 0.818 0.761 0.814 0.814 0.803 0.715 0.680 0.745 0.650 0.652 0.653\nTable 6. Results with P, Precision, F, F-measure and R, Recall, for Mix onsets in the Bello Dataset using all the 5 Peak\nSelection methods (A, B, C, D, E).\nsults are quite similar for all the onset classes.\nBy using a normalization based on the maximum stan-\ndard deviation – Peak Selection Method C – the results are\nnot very different from the results obtained by using a nor-\nmalization based on the maximum absolute value – Peak\nSelection Method A. In the case of the HFC, SF, and RCD,\nwe obtain practically the same results (they change by no\nmore than 1pp) for all the onset classes. In the case of the\nPD OSS, we have losses of about 10pp for the PP onset\nclass but for the other classes the results remain basically\nthe same (they change by less than 1pp). For the WPD and\nCD functions the behaviour is mixed, that is, for some on-\nset classes the results improve while for others the results\nget poorer, although the magnitude of the changes in this\nOSS is less than 2pp, which means that the changes are not\nvery signiﬁcant. This Peak Selection Method improves the\nCD in the PP class, but makes its results worse in the PNP\nand Mix classes. On the other hand, it improves the WPD\nin the PNP class, but makes it worse in the Mix class.\nThe use of a smoothing ﬁlter on the Onset Detection\nFunction – Peak Selection Method D – causes the results,\nin general, to be much different than the results obtained\nwith the Peak Selection Method A. For the HFC OSS, the\nresults decrease from 10 to 25pp and for the SF the ten-\ndency is the same, except that for the NPP onset class\nthe results improve slightly (less than 1pp) and the global\nlosses are not so pronounced: they reach at most 9pp. In\nthe case of the PD function we obtain mixed behaviour: for\nthe NPP and Mix onsets the results are 2.5 and 5pp worse\nrespectively while for the PP onsets the results improve by\n3pp and for the PNP we have a 20pp improvement. The\nresults get about 2 to 34pp and 7.5 to 44pp worse for the\nWPD and CD OSS respectively, while for the RCD OSS\nthe results remain similar for NPP class, but get 9 to 30pp\nworse for the other onset classes. The ﬁlter has some kind\nof “good” effect only on the PD OSS, maybe because this\nkind of function is the most irregular and the ﬁlter brings\nsome positive uniformity, and on the other OSS one ob-\ntains an excess of uniformity with the ﬁlter, decreasing theprecision of the OSS.\nDropping the local maximum condition in the peak pick-\ning algorithm – Peak Selection Method E – makes, in gen-\neral, the results be much worse than the results of the Peak\nSelection Method A. For the HFC the results are all around\n30pp worse while the results can be to 20pp worse for the\nSF, 40pp worse for the PD and to 34pp worse for the WPD.\nFor the complex domain family, the results can be to 40pp\nworse for the CD and 50pp worse for the RCD.\n4.4.3 Balance\nHaving in mind the discussion of the two previous sub-\nsections, we can make a global balance. First of all, in\ngeneral, the differences between the results obtained by\napplying a running mean and a running median threshold\nare not statistically signiﬁcant (W = 291;p = 0:959 in\nthe Wilcoxon signed rank sum test with continuity correc-\ntion3) and they are dependent upon the particular onset\nclass and OSS, which implies that for certain applications\nthat need just a certain type of onsets, one speciﬁc type of\nthreshold can be chosen in favour of the other.\nConcerning the normalization methods, the differences\nbetween the results obtained with the two kinds of normal-\nization used are not statistically signiﬁcant (W = 290;p =\n0:975 in the Wilcoxon signed rank sum test with continuity\ncorrection).\nOn the other hand, the results obtained by the usage of\na smoothing ﬁlter get signiﬁcantly poorer (W = 427;p =\n0:004 in the Wilcoxon signed rank sum test with continu-\nity correction) in most of the cases, except for the single\ncase of the PD OSS. This means that one should not use a\nsmoothing ﬁlter at all (except maybe for the single case of\nthe PD function) or try to test a different ﬁlter from the one\nused in this study.\nFinally, not using the local maximum condition makes\nthe results get signiﬁcantly poorer (W = 500;p < 0:001\nin the Wilcoxon signed rank sum test with continuity cor-\n3All statistical tests were obtained using R [10].rection), which means that one should really use the local\nmaximum condition.\n5. CONCLUSIONS\nIn this paper we have compared the inﬂuence of 5 distinct\nPeak Selection Methods on the performance of some of the\nmost common onset detection methods. Our comparison\nfocused on both the inﬂuence of the peak selection on each\nparticular OSS but also on the inﬂuence of the results in\neach onset classes.\nWe have found that, in general, the Peak Selection Meth-\nod used can be of great inﬂuence on the results obtained,\nbut not all of them have the same magnitude of inﬂuence.\nGlobally, the inﬂuence of using a running-mean or running-\naverage threshold and of using a normalization based on\nthe maximum absolute value or on the maximum standard\ndeviation is quite small (at best around 3-4pp) and can be\nboth positive or negative, depending on the cases. On the\nother hand using a low-pass ﬁlter as a ﬁrst smoothing step\nand not using a local maximum condition as ﬁnal step can\nbe of great negative inﬂuence, sometimes worse by 50pp.\nWe also noticed that, globally, the SF OSS is the most\nrobust to Peak Selection changes, and the PD is the most\nsusceptible to changes.\nIn the future this work can be extended by adding a few\nOnset Detection methods to the comparison and also by\ntesting more Peak Selection Methods. One possibility is to\nadd more types of ﬁlters to the smoothing to see if the neg-\native inﬂuence continues or is just something related to the\nﬁlter we used. We also intend to check if these conclusions\napply to a larger dataset.\n6. ACKNOWLEDGEMENTS\nWe would like to thank Juan Pablo Bello at the NYU for\nfreely providing the dataset we used for our experiments.\nThis work was partially supported by national funds\nthrough FCT – Fundac ¸ ˜ao para a Ci ˆencia e a Tecnologia,\nunder project PEst-OE/EEI/LA0021/2011.\n7. REFERENCES\n[1] J.P. Bello, L. Daudet, S. Abdallah, C Duxbury,\nM Davies, and M B Sandler. A tutorial on onset de-\ntection in music signals. IEEE Transactions on Speech\nand Audio Processing, 13(5):1035–1047, 2005.\n[2] S. Dixon. Onset Detection Revisited. In Proc. of the\nInt. Conf. on Digital Audio Effects (DAFx-06), pages\n133–137, September 2006.\n[3] C. Duxbury, J.P. Bello, M. Davies, and M. Sandler. A\ncombined phase and amplitude based approach to on-\nset detection for audio segmentation. In Proc. 4th Euro-\npean Workshop on Image Analysis for Multimedia In-\nteractive Services (WIAMIS-03), pages 275–280, Sin-\ngapore, 2003. World Scientiﬁc Publishing Co. Pte. Ltd.[4] F. Eyben, S. B ¨ock, B. Schuller, and A. Graves. Uni-\nversal Onset Detection with Bidirectional Long Short-\nTerm Memory Neural Networks. In 11th International\nSociety for Music Information Retrieval Conference\n(ISMIR 2010), pages 589–594, 2010.\n[5] A Holzapfel, Y Stylianou, A C Gedik, and B Bozkurt.\nThree Dimensions of Pitched Instrument Onset Detec-\ntion. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 18(6):1517–1527, August 2010.\n[6] I. Kauppinen. Methods for detecting impulsive noise in\nspeech and audio signals. In 14th International Conf.\non Digital Signal Processing Proc. DSP 2002 (Cat.\nNo.02TH8628), volume 2, pages 967–970. IEEE.\n[7] A. Klapuri and M. Davy, editors. Signal Processing\nMethods for Music Transcription. Springer, 2006.\n[8] A.P. Klapuri, A.J. Eronen, and J.T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n14(1):342–355, 2006.\n[9] MIREX. Mirex 2011: Audio onset detection task.\nhttp://www.music-ir.org/mirex/wiki/\n2011:Audio_Onset_Detection, May 2011.\n[10] R Development Core Team. R: A language and en-\nvironment for statistical computing. 2008. ISBN 3-\n900051-07-0.\n[11] X. Rodet and F. Jaillet. Detection and modeling of fast\nattack transients. In Proc. of the International Com-\nputer Music Conference, pages 30–33, 2001.\n[12] C. Ros ˜ao and R. Ribeiro. Trends in Onset Detection.\nInProc. of the 2011 Workshop on Open Source and\nDesign of Communication, pages 75–81. ACM, 2011.\n[13] C. Ros ˜ao, R. Ribeiro, and D. Martins de Matos. Com-\nparing Onset Detection Methods Based on Spectral\nFeatures. In Proc. of the 2012 Workshop on Open\nSource and Design of Communication. ACM, 2012.\n[14] U. Z ¨olzer, X. Amatriain, D. Arﬁb, J. Bonada, G. De\nPoli, P. Dutilleux, G. Evangelista, F. Keiler, A. Loscos,\nD. Rocchesso, M. Sandler, X. Serra, and T. Todoroff.\nDAFX:Digital Audio Effects. Wiley, 2002."
    },
    {
        "title": "Detecting Melodic Motifs from Audio for Hindustani Classical Music.",
        "author": [
            "Joe Cheri Ross",
            "Vinutha T. P.",
            "Preeti Rao"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417587",
        "url": "https://doi.org/10.5281/zenodo.1417587",
        "ee": "https://zenodo.org/records/1417587/files/RossPR12.pdf",
        "abstract": "Melodic motifs form essential building blocks in Indian Classical music. The motifs, or key phrases, provide strong cues to the identity of the underlying raga in both Hindustani and Carnatic styles of Indian music.  Thus the automatic detection of such recurring basic melodic shapes from audio is of relevance in music information retrieval. The extraction of melodic attributes from poly- phonic audio and the variability inherent in the perfor- mance, which does not follow a predefined score, make the task particularly challenging. In this work, we consid- er the segmentation of selected melodic motifs from au- dio signals by computing similarity measures on time se- ries of automatically detected pitch values. The methods are investigated in the context of detecting the signature phrase of Hindustani vocal music compositions (bandish) within and across performances.",
        "zenodo_id": 1417587,
        "dblp_key": "conf/ismir/RossPR12",
        "keywords": [
            "Melodic motifs",
            "Indian Classical music",
            "Hindustani",
            "Carnatic",
            "Raga identity",
            "Automatic detection",
            "Music information retrieval",
            "Polyphonic audio",
            "Performance variability",
            "Signature phrase"
        ],
        "content": "DETECTING MELODIC MOTIFS FROM AUDIO FOR \nHINDUSTANI CLASSICAL MUSIC  \nJoe Cheri Ross*, Vinutha T . P.Ŧ and Preeti RaoŦ \nDepartment  of Computer Science and Engineering*   Department of Electrical EngineeringŦ \nIndian Institute  of Technology  Bombay,  \nMumbai  400076 , India  \njoe@cse.iitb.ac.in * \n{vinutha,prao}@ee.iitb.ac.inŦ \n \nABSTRACT \nMelodic motifs form essential building blocks in Indian \nClassical music. The motifs, or key phrases, provide \nstrong cues to the identity of the underlying raga  in both \nHindustani and Carnatic styles of Indian music.  Thus the \nautomatic detection of such recurring basic melodic \nshapes from audio is of relevance in music information \nretrieval. The extraction of melodic attributes from pol y-\nphonic audio and the variability inherent in the perfo r-\nmance, which does not follow a predefined score, make \nthe task particularly challenging. In this work, we consi d-\ner the segmentation of selected melodic motifs from a u-\ndio signals by computing similarity measures on time se-\nries of automatically detected pitch values. The methods \nare investigated in the context of detecting the signature \nphrase of Hindustani vocal music compositions ( bandish ) \nwithin and across performances.   \n1. INTRODUCTION \nHindustani classical music is primarily an oral tradition. \nWhile large archives of audio recordings are available, \nthere are few written scores even for widely performed \ncompositions. In such a scenario, retrieval of music based \non any relevant high level music descriptors such as raga  \n(melodic mode) or bandish  (a raga -specific composition \nfor vocal music ) relies entirely on available textual met a-\ndata, if any. It is therefore very attractive to consider the \nautomatic extraction of such metadata from audio recor d-\nings of concerts. Automatic detection of melodic motifs, \nfor instance, can provide useful inputs to raga  identific a-\ntion as well as identification of the bandish  itself [1]. Al-\nso, within a concert audio recording, it would be interes t-\ning to detect the occurrence of the characteristic melodic \nphrases thus providing a rich transcription to the listener \nand the serious student of music. In this work, we consi d-\ner the problem of detecting specific phrases from recor d-\ned performances given one instance of the phrase as te m-\nplate. We also attempt to understand the limitations of the \napproach in terms of the detection of phrases across pe r-\nformances and artistes. \nThere is no known previous work on the audio based detection of melodic phrases in Hindustani classical m u-\nsic. A considerable body of recent work, however, has \naddressed the discovery of melodic patterns from sym-\nbolic scores in Western folk music [2 ]. A continuous-\ntime pitch contour is derived from the score for use i n \nsegment alignment and classification. Likewise, the l i-\nmited reported work on audio signals is based on first o b-\ntaining note representations by monophonic pitch tra n-\nscription [3 ].  In the case of Hindustani classical music, \nhowever, the available symbolic notation is inadequate to \ndeal with tuning variations and complex ornamentation \nthat are fundamentally linked to raga  characteristics, cal l-\ning for a different approach to data representation and \npattern matching. \nIn the next section, we review the music background \nrequired to appreciate the problem, and outline the cha l-\nlenges. The database and evaluation methods are d e-\nscribed next. A framework for the signal processing and \npattern matching is proposed. The performance of the \nsystem is presented followed by a discussion of the r e-\nsults and prospects for future work. \n2. MOTIFS IN HINDUSTANI MUSIC \nHindustani music, especially the modern khyal  style, is a \npredominantly improvised music tradition operating \nwithin a well-defined raga  (melodic) and tala (rhythmic) \nframework. Apart from the permitted scale intervals \n(swaras ) that define a raga , it is its characteristic phrases \nthat complete the grammar and give it a unique identity \n[4]. Most ragas  can be adequately represented by up to 8 \nphrases which then become the essential building blocks \nof any melody. Thus the detection of recurring phrases \ncan help to identify the raga. Indeed musical training i n-\nvolves learning to associate characteristic phrases, or m o-\ntifs, with ragas. Melodic improvisation involves weaving \ntogether a unique melody bound by the chosen rhythmic \ncycle ( tala) and consistent with the raga  phraseology.  \nOften the improvisation is anchored within a known \ncomposition, or song, known as bandish  which provides a \nplatform for exposing a specific raga . The bandish  is \nidentified by its lyrics (especially its title phrase) and m e-\nlody corresponding to a specific raga, tala and laya \n(tempo). In the improvised section of the concert known \nas the bol-alap , the singer elaborates within each rhyt h-\nmic cycle of the tala using the words of the bandish  in-\nterspersed with solfege and held vowels, purposefully \nreaching the strongly accented first beat (the sam) of the \nnext rhythmic cycle on a fixed syllable of the signature   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012  International Society for Music Information Retrieval    \n \n \n \nFigure 1.  Top: spectrogram with superposed vocal pitch and mukhda  in boxes; below: first beat of each subcycle \n(S= sam) with aligned lyrics in vocal regions.\nphrase of the bandish .  This recurring phrase, known as \nthe mukh da, is the title phrase of the bandish and is d e-\nfined by its text as well as its melodic shape. It acts like a \nrefrain throughout the exposition, which can last several \nminutes, whereas the other lyrics of the bandish  can u n-\ndergo extensive variation in melodic shape in the course \nof improvisation .  \nWhile segmentation of characteristic phrases of the \nraga  from a recorded performance is clearly an interes t-\ning task that falls within the scope of melodic motif d e-\ntection, a trained musician is required to notate the re c-\norded performances in order to generate the ground-truth \nneeded for evaluation of any automatic system. On the \nother hand, the mukh da of the bandish  is easy to segment \nmanually due to the characteristic words of the lyrics and \nits specific location within the rhythmic cycle. Automatic \ndetection of the mukh da can serve to identify the bandish \napart from making possible a partial transcription of the \nperformance itself for the interested listener. Although \nthe mukhda  is detected easily by listening for the lyrics, \nautomatic segmentation cannot rely on such cues due to \nthe known difficulties of speech recognition from singing \nin polyphonic audio. We focus therefore on the melodic \nand rhythmic invariances to provide cues for the automa t-\nic detection of all occurrences of the mukh da, provided \none reference instance, across the audio recordings of \nbandish  of prominent artistes. Such work can also serve \nas the basis for more general melodic phrase detection \ncontexts. 3. DATABASE AND EVALUATION METHODS \nWe selected 4 full-length CD-quality recorded concerts \nof well-known Hindustani khyal vocalists. In all cases, \nthe accompanying instruments are the tanpura  (drone), \nharmonium  and tabla . The section of each concert cor-\nresponding to bandish -based improvisation ( bol-alap ) is \nextracted for this study.   Table 1 shows the artiste names  \nand bandish  titles with other relevant details including \nCD cover metadata and the duration of the bol-alap  sec-\ntion. All the performances use the popular tintal  rhythm \nc\nycle with 16 beats divided equally over 4 sections. The \nbeats are realized by the strokes of the tabla (percussion) \nwith the first beat of each section considered to be \nstressed in 3 of the 4 sections. All the mukhda  phrases, \nwhich may occur around any sam (first beat of the cycle) \nthroughout the performance, are manually labeled. This \nserves as the ground truth (“positives”) for the motif d e-\ntection evaluation. The tempo indicated for each piece is \nan average, with slow fluctuations in cycle length o b-\nserved throughout the recordings. Of the four recordings \nin Table 1, the first two correspond to the same bandish  \nby different artistes. The last recording is by a female v o-\ncalist. It was observed that this recording with its slow \ntempo exhibits the largest variations in the duration of the \nmukhda  even after accounting for local variations in cycle \nlength.  \n \n \nArtist e Raga Tala Bandish  Tempo \n(bpm)  Dur. \n(min)  #Phrases  \nPositive  Negative  \nBhimsen  Joshi (BJ)  Marwa  Tintal  Guru Bina Gyan  193 4.58 13 55 \nAjoy Chakraborti(AC)  Marwa  Tintal  Guru Bina Gyan  205 9.08 33 295 \nBhimsen  Joshi (BJ)  Puriya  Tintal  Jana na na na  204 9.36 17 97 \nKishori Amonkar (KA)  Deshkar  Tintal  Piya Jaag  43 22.3 44 176 \nTable 1. Description of database    \n \n \nFor further processing, the audio is converted to 16 \nkHz mono at 16 bits/sample. Fig. 1 shows the spectr o-\ngram (of the 3 kHz frequency range) of a duration \nslightly greater than 1 full rhythmic cycle extracted from \nthe Piya Jaag  recording by Kishori Amonkar. Superi m-\nposed on the spectrogram is the detected pitch contour (as \nobtained by the method presented later in Sec. 4). B e-\nneath the spectrogram is an annotation block depicting \nthe aligned tala cycles. The first beat of the cycle is the \nsam (S) corresponds to the dha stroke of the tabla. In Fig. \n1, the first beat of each sub-cycle is labeled ( dha (D) or \ntha (T)). The penultimate sub-cycle before the S is the \nkhali , as also evident from the absence of low frequency \ntabla partials in the spectrogram of this segment.  The \nmukhda  segments corresponding to the ut terance “ Piya \nJaag ” are enclosed in boxes. The mukhda  segments are \nobserved to be melodically similar and also similarly \naligned within the tala cycles. Note that the song syllable \nthat coincides with the sam (S) is sometimes left inco m-\nplete by the vocalist.    \nThe proposed motif detection method is evaluated in \nthe following experiments. 1) Within-concert detection  \naccuracy where each manually labeled motif serves once \nas the reference template for all remaining motifs in the \nsame artiste- bandish  recording; 2) across-concerts dete c-\ntion accuracy where the reference template of a particular \nartiste is used to find the motifs of the same bandish  by a \ndifferent artiste.  \n4. AUTOMATIC MOTIF DETECTION \nThe pitch contour depicted in Fig. 1 can be viewed as a \ntime series in which the desired phrase segments are e m-\nbedded.   As such, finding segments in the overall contour \nthat are similar to a given phrase would involve matching \nthe pitch at every time instant of the given phrase to the \npitch at every other time instant throughout the time s e-\nries [3]. It is of interest to explore methods to reduce the \nsearch complexity. In the present context, we can exploit \nthe additional knowledge about the rhythmic relationship. \nAs discussed in Sec. 3, the vocalist embeds the mukhda  \nphrase in the metrical cycle ( tala) so that a fixed syllable \ncoincides with the sam instant. The metrical space of \neach cycle is occupied by improvisation culminating with \nthe mukhda . Motivated by this, we approach the automa t-\nic detection of the mukhda  phrase from the audio by first \nidentifying a limited set of candidate phrases based on the \ndetected rhythm cycle structure, and then computing a \nmelodic similarity distance between the reference te m-\nplate and each of the candidates.  \nAs in any classification task, it is necessary to design \nan appropriate data representation and a suitable similar i-\nty model for the matching. In this section, we describe the \nsignal processing implementation of a pitch-based data \nrepresentation and consider similarity models that are \nsuited to the comparison of such time series. Finally, \ncandidate segments with distances from the reference \ntemplate lower than a threshold are the detected positives.     4.1 Signal Processing \n4.1.1 Vocal Pitch Detection \nIn Hindustani classical vocal music, the accompanying \ninstruments include the drone ( tanpura ), tabla , and often, \nthe harmonium  as well. The singing voice is usually d o-\nminant and the melody can be extracted from the detected \npitch of the predominant source in the polyphonic mix. \nMelody detection involves identifying the vocal segments \nand tracking the pitch of the vocalist. The drone and \nharmonium  are strongly pitched instruments. We ther e-\nfore employ a predominant-F0 extraction algorithm d e-\nsigned for robustness in the presence of pitched acco m-\npaniment [4]. This method is based on the detection of \nspectral harmonics helping to identify multiple pitch ca n-\ndidates in each 10 ms interval of the audio. Next pitch \nsaliency and continuity constraints are applied to estimate \nthe predominant melodic pitch.  The best of pitch dete c-\ntion methods achieve no more than 80% accuracy on p o-\nlyphonic audio. An important factor limiting the accuracy \nis the fixed choice of analysis parameters, which ideally \nshould be matched to the characteristics of the audio such \nas the pitch range of the singer and the rate of variation of \npitch. In the regions of rapid pitch modulation, charact e-\nristic of Indian classical singing, shorter analysis wi n-\ndows serve better to estimate the vocal harmonic fr e-\nquencies and amplitudes. Hence for better pitch detection \naccuracy, it is necessary to adapt the window length to \nthe signal characteristics. This is achieved automatically \nby the maximization of a signal sparsity measure co m-\nputed at each analysis instance (every 10 ms) for local \npitch detection [6]. Finally, it is necessary to identify the \nvocal regions in the overall tracked pitch. This is \nachieved by using the peculiar characteristics of Hindu s-\ntani music where the vocal segments are easily discrim i-\nnated from the instrumental pitches due to the different \ntemporal dynamics [7]. \n4.1.2 Motif Candidate Selection \nMotivated by the characteristic of the mukhda , namely \nit’s alignment with the sam stroke of the rhythm cycle, \nwhich the artiste pays great importance to achieve, th e \nsearch algorithm starts by restricting candidate melodic \nsegments to those that match rhythmically.  This can \nachieved via the automatic detection of the beat instants \nin the audio. In the spectrogram of Fig. 1, the tabla  \nstrokes corresponding to the beats of the tala cycle are \nvisible as vertical impulsive onsets. While the sam stroke \nitself is not particularly distinctive, the dha strokes (i n-\ncluding the sam) can be detected as the highest onsets in \nthe combined energies of two frequency bands: [5000, \n8000] and [0, 1000] Hz. The former band is relatively \nfree of interference from vocal partials while the latter \nband captures the low frequency partial of the dha stroke. \nThe filtered output power is subjected to a first-order di f-\nference and then half-wave rectified.  Spurious peaks are \nremoved by a local threshold. The consistency of the \nspacing of detected onsets with the known average tempo   \n \nis considered further to identify the largest peaks as the \ndha stroke onsets. \n \nFigure 2.  Two positive and one negative phrase of \nGuru Bina Gyan \n \nFigure 3.  Three positive and one negative (bottom \nright) phrase of Piya Jaag \nAll audio segments whose alignment around a detected \nonset matches that of the mukhda  are treated as potential \ncandidates for motif detection. The extracted segment e x-\ntends from the instant ( sam-t1) to ( sam+t2) where t1 and \nt2 are nominal values (number of beats in the 16-beat \ncycle) chosen based on the reference mukhda  instance.  \nSuch a data representation is inherently robust to the slow \ntempo variations that occur during the concert.  \nThe sequence of pitch values (in cents) obtained \nacross the extracted candidate audio segment is a time \nseries representation that is used further for similarity \nmatching with a reference time series that is similarly o b-\ntained. Figures 2 and 3 depict the pitch contours of a few \ncandidate segments showing examples of the melodic and \ntiming variability across mukhda  realization within co n-\ncerts. We observe that there are prominent differences in \nthe melodic pitch contour, both in terms of fine pitch va r-\niation as well as timing. The note ( swara ) sequence of the  \nGuru Bina Gyan  phrase is seen to be [ Sa, Sa, Ni, Re, Ni, \nDha]. However, since the word Gyan  is often left unsung \nby the artiste, the sam itself serves as the right limit (i.e. \nt1=5, t2=0) of the mukhda  in our task.  The swara corre s-\nponding to Piya Jaag  are [ Da, Pa, Ga, Pa ]. Here t1=1 and \nt2=2 were applied to delimit the mukhda . Any pitch gaps \nwithin the boundaries correspond to pauses. These are \nfilled by linear interpolation or extrapolation of neig h-\nbouring pitch values before similarity matching. \n4.2 Similarity Modeling \nDue to the beat-based method of candidate extraction, the \nsegments tend to be of different absolute durations d e-\npending on local tempo variations. Also, singing expre s-\nsiveness manifests itself in timing changes that can affect \nthe total duration of the sung phrase.  The sequence of \npitch values obtained at 10 ms intervals throughout the candidate audio segment can be viewed as a non-uniform \nlength time-series representation.  We explore two di s-\ntinct similarity measures for non-uniform length temporal \nsequences.  \nPiecewise aggregate approximation has been used to \nobtain dimension-reduced uniform length time-series for \nmotif discovery in bioinformatics [8]. We apply this m e-\nthod, called SAX, to convert a non-uniform length time \nseries of pitch in cents, computed every 10 ms, to a un i-\nform length, dimension-reduced sequence of symbols. \nThe string length W is varied to determine the optimum \ndimension of the data representation.  A given time-series \nis aggregated into W uniform length segments each \nrepresented by the averaged value of the segment. The \nreal-valued pitch in cents is retained as such but we also \nconsider quantizing pitch to the nearest semitone. Since \nthe tonic frequency is singer dependent in Indian music, \nthe semitone grid is anchored on the most prominent peak \nof an overall pitch histogram derived from the vocal pitch \ntrack across the test audio.  Since our present work is \nconfined to within-concert matching, a tonic detection \nerror is inconsequential. Next, the Euclidean distance b e-\ntween the two W-length sequences, the reference and the \ncandidate, is used as a similarity measure.  \nAnother widely used method to compare real-valued \ntime series related to each other through, possibly, nonl i-\nnear time-scaling, is the dynamic time-warping (DTW) \ndistance measure [9].  The distance between the so \naligned reference and candidate phrases is used as the s i-\nmilarity measure. Pathological warpings are avoided by \nincorporating the Sakoe-Chiba constraint on the width of \na diagonal band in the DTW path matrix. The absolute \ndifference in cents between pitch values is used as the \nlocal distance in the DTW path optimization. Any abs o-\nlute difference within 25 cents (i.e. a quarter tone) is \nrounded down to 0 cents. This is found to help reduce the \ninfluence of practically imperceptible pitch differences on \nthe warping path and therefore any unnecessary stre t-\nching of the path. \n5. EXPERIMENTS AND DISCUSSION \nGiven the database described Table 1, we evaluate the \ndifferent data representations and similarity measures on \nwithin-concert and across singer-concert motif detection \ntasks. Each candidate phrase extracted from the detected \nonsets as presented in Sec. 4 is labeled positive or neg a-\ntive depending on whether or not it is the actual motif \n(i.e.  mukhda  phrase). Table 1 shows the number of such \nphrase segments available for the evaluation of the moti f \ndetection methods. To maximize the use of the available \nannotated data, each labeled motif is considered as the \nreference once with all other motifs serving as positive \ntokens and the remaining candidates as negative tokens.  \nThus, the Piya Jaag  motif detection task can be evaluated \non 44x43 = 1892 positive pairs and 44x176 = 7744 neg a-\ntive pairs (i.e. each positive with all negatives). Table 2 \nsummarizes the experiments. The Experiment A consid-\ners motif detection from within the Guru Bina  recording \nof Bhimsen Joshi given a reference template from the \nsame recording. Similarly, the Experiments B, C and D \n  \n \nconsider the within-concert detection as specified in T a-\nble 2. The Experiment E uses the positive tokens of Guru \nBina  by BJ to detect the mukhda  in the same bandish  \nconcert by a different vocalist, AC. As it turns out, the \ntwo male singers are tuned to the same tonic. In each e x-\nperiment, the rate of false alarms for a given hit rate (co r-\nrect detections) is computed for each combination of s i-\nmilarity model and data representation. The similarity \nmeasures include SAX and DTW. The data represent a-\ntions chosen for the study are either the continuous pitch \nvalues (i.e. 1200 cents per octave) indicated by “q1200”, \nor the quantized versions (12 semitones per octave on an \nequitempered scale) indicated by “q12”.  \nFig. 4 shows an example of the distribution of di s-\ntances for positive-positive pairs and positive-negative \npairs. The recording is Piya Jaag  (Experiment D) ev a-\nluated with DTW-q1200.  We observe that the distances \nbetween the positive phrases cluster closely relative to the \ndistances between the positive-negative phrase pairs. \nThere is a limited overlap between the two distributions. \nThat the spread of the negative distances is relatively \nwide indicates the robustness of the distance measure in \nterms of its discrimination of melodic shapes. We also \nnote the presence of a small isolated cluster of positive \ndistances. A closer examination revealed that this \nstemmed from the wide timing variability across Piya \nJaag  phrases with its particularly slow tempo. Thus there \nwere at least two distinct behaviours within the set of \npositive phrases. The inter-phrase distances between the \nlonger duration phrases tended to be lower than the di s-\ntances involving shorter duration phrases. Fig. 5 shows \nthe ROC (hit rate versus false alarm rate) derived from \nthe distributions of Fig. 4 by varying the decision thr e-\nshold. We observe two bends in the curves, consistent \nwith the bimodal shape of the pdf.  \nTable 3 summarizes the classification results across \nthe experiments in terms of false alarm rate (FA) for a \nfixed HR chosen near the knee of the ROCs of the co r-\nresponding data. Given that the extracted candidate \nphrases have durations varying in the range of 2-4 sec \n(200-400 length string), we vary the SAX string length \naround W=50 (corresponding to the aggregation of 4- 8 \nsamples). Preliminary experiments revealed that W su b-\nstantially lower than this (i.e. more averaging) led to wo r-\nsened performance. We note that the performance of \nSAX improves with pitch quantization at a fixed string \nlength of 50. Increasing or decreasing the string length \naround this does not improve performance on the whole. \nThe DTW system performs substantially better than SAX \nin terms of reducing the FA at fixed hit rate. As in the \ncase of SAX, pitch quantization helps to improve perfo r-\nmance further in some cases. That DTW does relatively \nbetter indicates that non-uniform time warping is esse n-\ntial to achieve the needed alignment between phrases b e-\nfore distance computation. This is consistent with what is \nknown about the tradition where the essential melodic \nsequence of the mukhda  phrase is strongly adhered to by \nthe singer while metrical alignment is focused only on \ngetting to the specific syllable onset (e.g. Gyan  in Fig. 2 and Jaag in Fig. 3) on the first beat of the cycle (the \nsam). \n \nExpt  Bandish  Singer  #Phrases  \nPOS NEG  \nA Guru Bina  BJ   \n156  \n715 \nB Guru Bina  AC 1056  9735  \nC Jana na na na  BJ 272 1649  \nD Piya Jaag  KA 1892  7744  \nE Guru Bina   BJ vs AC  429 3835  \nTable 2.  Description of experiments with number of \npositive and negative phrase candidates available in \neach \n \nFigure 4.  DTW distances distribution for Piya \nJaag  recording \n \nFigure 5.  ROC curves for Piya Jaag  distribution \nFurther, comparing the Experiments E and B as a case of \nbetween-concert to within-concert performance of the \nmotif detection methods, we see that the FA is somewhat \nhigher in Experiment E which involves a reference motif  \n    \n  \n \nfrom the concert of the same bandish  by a different a r-\ntiste. This is consistent with the anticipated higher vari a-\nbility in motif contour across artistes. \n6. CONCLUSION \nSimilarity measures traditionally used in time-series \nmatching have been shown to perform well in the context \nof melodic motif detection in the improvised bandish  of \nHindustani vocal concert recordings. The processing of \nthe polyphonic audio signals needed to achieve a suitable \ndata representation was presented. Musical knowledge \nrelated to the metrical relation between the mukhda  motif \nand the underlying rhythmic structure was exploited to \nachieve a reduced search space, using available similarity \nmeasures, and possibly more robust detection. While the \nmukhda  context considered in this work is relevant in \nboth Hindustani and Carnatic vocal music (in the bol-\nalap and niraval  respectively), the detection of other ch a-\nracteristic raga  phrases would be a logical extension. It is \nnot clear whether rhythmic cues would help in this more \ngeneral melodic segmentation. Further extension to uns u-\npervised clustering of phrases in a concert recording can \ncontribute to higher-level classification tasks such as ra-\nga recognition as well to further research in audio tra n-\nscription for such musical traditions. \n7. ACKNOWLEDGEMENT \nThis work received partial funding from the European \nResearch Council under the European Union’s Seventh \nFramework Programme (FP7/2007-2013) / ERC grant \nagreement 267583 (CompMusic). \n \n8. REFERENCES \n[1] J. Chakravorty, B. Mukherjee and A. K. Datta: \n“Some Studies in Machine Recognition of Ragas in \nIndian Classical Music, ”  Journal of the Acoust. Soc. \nIndia , Vol. 17, No.3&4, 1989.  \n[2] Z. Juha sz: “Analysis Of Melody Roots In Hungarian \nFolk Music Using Self-Organizing Maps With \nAdaptively Weighted Dynamic Time Warping ,” \nJournal Applied Artificial Intelligence , Vol.21, \nNo.1, 2007.    \n [3] R. B. Dannenberg  and N. Hu: “Pattern Discovery \nTechniques for Music Audio ,” Journal of New M u-\nsic Research , Vol. 32, No.2,   2002.  \n \n[4] S. Rao, W. van der Meer and J. Harvey: “The Raga \nGuide: A Survey of 74 Hindustani Ragas ,” Nimbus \nRecords with the Rotterdam Conservatory of Music, \n1999.  \n[5] V. Rao and P. Rao: “Vocal Melody Extraction in the \nPresence of Pitched Accompaniment in Polyphonic \nMusic, ” IEEE Trans. Audio Speech and Language \nProcessing , Vol. 18, No.8,  2010.  \n[6] V. Rao, P. Gaddipati and P. Rao: “Signal-driven \nWindow-length Adaptation for Sinusoid Detection \nin Polyphonic Music ,”  IEEE Trans. Audio, Speech, \nand Language  Processing,  Vol. 20, No.1,  2012. \n \n[7] V. Rao, C. Gupta and P. Rao: \" Context-aware Fe a-\ntures for Singing Voice Detection in Polyphonic \nMusic,\"  Proc. of Adaptive Multimedia Retrieval , \n2011.  \n[8] J. Lin, E. Keogh, S. Lonardi and B. Chiu: “A S ym-\nbolic Representation of Time Series, with Implic a-\ntions for Streaming Algorithms,” In Proc. of the \nEighth ACM SIGMOD Workshop on Research I s-\nsues in Data Mining and Knowledge Discovery ,  \n2003 . \n[9] D. Berndt and J . Clifford: “Using Dynamic Time \nWarping to Find Patterns in Time Series, ” AAAI -94 \nWorkshop on Knowledge Discovery in Databases,  \n1994 . \n \n Method  Experiment A  Experiment B  Experiment C  Experiment D  Experiment E  \n HR FA HR FA HR FA HR FA HR FA \nSAX -q1200 -W50 1 .096 .94 .019 .86 .239 .87 .130 .94 .035 \nSAX -q12-W40 1 .084 .94 .016 .86 .231 .87 .135 .94 .024 \nSAX -q12-W50 1 .071 .94 .015 .86 .216 .87 .124 .94 .029 \nSAX -q12-W60 1 .091 .94 .014 .86 .210 .87 .133 .94 .023 \nDTW -q1200  1 .044 .94 .007 .86 .044 .87 .032 .94 .015 \nDTW -q12 1 .053 .94 .008 .86 .042 .87 .025 .94 .014 \nTable 3. Performance of SAX and DTW motif detection under different configurations. WX = SAX string dime n-\nsion is X; qY= quantized pitch levels per octave;  HR = hit rate;  FA = number of false alarms"
    },
    {
        "title": "Bayesian Nonnegative Harmonic-Temporal Factorization and Its Application to Multipitch Analysis.",
        "author": [
            "Daichi Sakaue",
            "Takuma Otsuka",
            "Katsutoshi Itoyama",
            "Hiroshi G. Okuno"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418163",
        "url": "https://doi.org/10.5281/zenodo.1418163",
        "ee": "https://zenodo.org/records/1418163/files/SakaueOIO12.pdf",
        "abstract": "Since important musical features are mutually depen- dent, their relations should be analyzed simultaneously. Their Bayesian analysis is particularly important to re- veal their statistical relation. As the first step for a uni- fied music content analyzer, we focus on the harmonic and temporal structures of the wavelet spectrogram ob- tained from harmonic sounds. In this paper, we present a new Bayesian multipitch analyzer, called Bayesian non- negative harmonic-temporal factorization (BNHTF). BN- HTF models the harmonic and temporal structures sepa- rately based on Gaussian mixture model. The input signal is assumed to contain a finite number of harmonic sounds. Each harmonic sound is assumed to emit a large num- ber of sound quanta over the time-log-frequency domain. The observation probability is expressed as the product of two Gaussian mixtures. The number of quanta is cal- culated in the ϵ-neighborhood of each grid point on the spectrogram. BNHTF integrates latent harmonic alloca- tion (LHA) and nonnegative matrix factorization (NMF) to estimate both the observation probability and the number of quanta. The model is optimized by newly designed de- terministic procedures with several approximations for the variational Bayesian inference. Results of experiments on multipitch estimation with 40 musical pieces showed that BNHTF outperforms the conventional method by 0.018 in terms of F-measure on average.",
        "zenodo_id": 1418163,
        "dblp_key": "conf/ismir/SakaueOIO12",
        "keywords": [
            "Bayesian",
            "multipitch",
            "analyzer",
            "harmonic",
            "temporal",
            "structures",
            "wavelet",
            "spectrogram",
            "Gaussian",
            "mixture"
        ],
        "content": "BAYESIAN NONNEGATIVE HARMONIC-TEMPORAL FACTORIZATION\nAND ITS APPLICA\nTION TO MULTIPITCH ANALYSIS\nDaichi Sakaue Takuma Otsuka Katsutoshi Itoyama Hiroshi G. Okuno\nGraduate School of Informatics, Kyoto University\nfdsakaue,ohtsuka,itoyama,okunog@kuis.kyoto-u.ac.jp\nABSTRACT\nSince important musical features are mutually depen-\ndent, their relations should be analyzed simultaneously.\nTheir Bayesian analysis is particularly important to re-\nveal their statistical relation. As the ﬁrst step for a uni-\nﬁed music content analyzer, we focus on the harmonic\nand temporal structures of the wavelet spectrogram ob-\ntained from harmonic sounds. In this paper, we present\na new Bayesian multipitch analyzer, called Bayesian non-\nnegative harmonic-temporal factorization (BNHTF). BN-\nHTF models the harmonic and temporal structures sepa-\nrately based on Gaussian mixture model. The input signal\nis assumed to contain a ﬁnite number of harmonic sounds.\nEach harmonic sound is assumed to emit a large num-\nber of sound quanta over the time-log-frequency domain.\nThe observation probability is expressed as the product\nof two Gaussian mixtures. The number of quanta is cal-\nculated in the \u000f-neighborhood of each grid point on the\nspectrogram. BNHTF integrates latent harmonic alloca-\ntion (LHA) and nonnegative matrix factorization (NMF) to\nestimate both the observation probability and the number\nof quanta. The model is optimized by newly designed de-\nterministic procedures with several approximations for the\nvariational Bayesian inference. Results of experiments on\nmultipitch estimation with 40 musical pieces showed that\nBNHTF outperforms the conventional method by 0.018 in\nterms of F-measure on average.\n1. INTRODUCTION\nMultipitch estimation [5, 7, 10, 19] is one of the most fun-\ndamental techniques of music information retrieval (MIR)\nbecause the temporal pattern of pitch strongly expresses\nthe content of musical pieces, especially in Western mu-\nsic. It is useful for a wide range of applications, including\ncontent-based music search [2], musical instrument identi-\nﬁcation [9], and chord recognition [15].\nOne promising technique in multipitch analysis is to\nassume a probabilistic generative model of musical sig-\nnals and then perform pattern matching between the model\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.\n\u0016k \u0016k+o2 Log-frequency\nTimeU\nH X\nFigure 1. Illustration of Bayesian nonnegative harmonic-\ntemporal\nfactorization. Basis and activation matrices are\nexplicitly modeled using Gaussian mixtures.\nand recorded signals using EM algorithm. Variational\nBayesian methods are particularly valuable because they\ncan ﬂexibly model the probabilistic relation between the\noccurrence pattern of pitch and many important musical\naspects, including harmonic structure, musical instrument,\nmusical structure [11], chord, onset, and emotion [8]. Our\ngoal is to formulate a music analyzer that estimates the re-\nlation between all such latent variables. At present, latent\nharmonic allocation (LHA) is the most suitable candidate\nfor further extensions.\nThe most important features in an observed wavelet\nspectrogram are the harmonic and temporal structures.\nThese structures are mutually dependent and should there-\nfore be analyzed simultaneously. Conventionally, LHA de-\nﬁnes the volume of a sound at a time frame as the relative\ncoefﬁcient to the total volume of that time frame. As a re-\nsult, there is no explicit variable that describes the actual\nvolume of each sound. This makes it difﬁcult to enhance\nthe model so that it considers the temporal envelope of the\nsounds.\nIn this paper, we present a new method that explicitly\nmodels the volume of each harmonic sound using a time\nseries of Poisson distributions and its harmonic and tem-\nporal structure using mixtures of Gaussians. This is illus-\ntrated in Figure 1. Here, the observed spectrogram is in-\nterpreted as a histogram of a large number of statistically\nindependent particles. This interpretation corresponds tothe integration of LHA and Bayesian NMF [3], because\nboth of them assumes the spectrogram as a histogram. In\nour model, the number of observations at each time-log-\nfrequency point is calculated in an \u000f-neighborhood of the\npoint. To do this, we integrate the probabilistic density\nfunctions (pdfs) of harmonic structures around the f-th fre-\nquency bin, with a quite small amount of a band of width\n\u000fF. For the temporal structure, we introduce a similar as-\nsumption using \u000fT. The objective is to ﬁt the generative\nmodel of BNHTF to the standard formulation of NMF.\nThe variational posterior distributions are approximately\nconjugate, when we take the limit \u000fF; \u000fT!0. The for-\nmulation strategy is similar to those formerly obtained by\nOchiai [12, 13]. Ours seem to deliver a more concrete in-\nterpretation of the generative process, rather than their con-\nstruction which depends on the direct use of Dirac delta\nfunction.\nOur method can also be viewed as a Bayesian NMF-\nbased method with adaptive harmonic basis. This method\nshould be quite valuable for many researchers, because\nsuch a model has been searched for long years [1, 14, 18].\nAs the proposed method is a variation of NMF, our method\ncan easily be extended further by using recent improve-\nments of NMF.\n2. CONVENTIONAL METHODS\n2.1 Harmonic Clustering\nThe spectral envelope of harmonic sound has several peaks\nat its fundamental frequency and the overtone frequencies.\nThis structure, known as harmonic structure, can be ap-\nproximated by using a mixture of Gaussians. In this paper,\nwe call this approach harmonic clustering . Harmonic clus-\ntering methods represent the wavelet spectrum of each har-\nmonic sound as a probabilistic density function described\nby a mixture of Gaussians. PreFEst [5], harmonic temporal\nclustering (HTC) [7], and LHA [19] are notable examples\nof harmonic clustering.\nThe mathematical representation is as follows. Let xbe\nthe log-frequency, \u0016kbe the logarithm of the fundamental\nfrequency of the k-th harmonic sound, and \u0015kbe the preci-\nsion of the Gaussian components. Moreover, let Mbe the\nnumber of harmonic partials considered in the model. The\nrelative weight of each harmonic partial is indicated using\n\u0011k= [\u0011 k1;\u0001\u0001\u0001; \u0011kM]. Following this, the k-th harmonic\nsound is represented as:\npk(xj\u0011k; \u0016k; \u0015k) =M∑\nm=1\u0011kmN(xj\u0016 k+om; \u0015\u00001\nk);(1)\nwhereNdenotes normal distribution and omdenotes the\nrelative position of the m-th overtone component on the\nlog-frequency axis. To retain the versatility of the model,\nin most cases, omis set so that the model represents com-\npletely harmonic sounds. The relationship between the log\nand linear frequency scales is deﬁned as:\nflog= 1200(log2f\u0000log2440 + 4:75): (2)2.2 Latent Harmonic Allocation\nLatent harmonic allocation (LHA) is a variational\nBayesian method of harmonic clustering that represents\neach time frame spectrum of the observed spectrogram us-\ning a mixture of harmonic sound models. The observed\nspectrum is interpreted as a histogram of numerous sound\nquanta that are generated by the observation model. The\nt-th time frame spectrum is represented as a linear com-\nbination of Kharmonic sounds with mixing coefﬁcients\n\u0019t= [\u0019 t1\u0001\u0001\u0001\u0019tK]. The emission probability of a sound\nquantum is described as:\npt(xj\u0019; \u0011; \u0016; \u0015 ) =K∑\nk=1\u0019tkpk(xj\u0011k; \u0016k; \u0015k): (3)\nLetxtfbe the value of the wavelet spectrogram at the\nt-th time frame and the f-th frequency bin. The over-\nall observation probability of the t-th time frame Xt=\n[xt1;\u0001\u0001\u0001; xtF]is described as:\npt(Xtj\u0019; \u0011; \u0016; \u0015)\n=F∏\nf=1(∑\nkm\u0019tk\u0011kmN(xfj\u0016k+om; \u0015\u00001\nk))xtf\n;(4)\nwhere xfdenotes the log-frequency of the f-th frequency\nbin.\nThe volume of the k-th sound at the t-th time frame is\nimplicitly determined by \u0019tk, which describes the relative\nweight of the k-th sound in the time frame. Because the to-\ntal volume of each time frame differs between the frames,\nit can be difﬁcult to discuss the temporal envelope of \u0019tk.\nTo solve this problem, we explicitly model the volume us-\ning a time series of Poisson distributions, which is similar\nto the construction of Bayesian NMF.\n2.3 Bayesian Nonnegative Matrix Factorization\nNonnegative matrix factorization (NMF) is a promising in-\nformation retrieval method that factorizes the observed ma-\ntrix of size N\u0002Minto two matrices of size N\u0002Kand\nK\u0002M. Naturally, we set K\u001cNandM. In music anal-\nysis, NMF is often applied to an STFT or a wavelet spec-\ntrogram. The two matrices are learned by minimizing a\ncost function, D(XjjUH) , whereX=fxtfgdenotes the\nobserved spectrogram and U=fuk\ntgandH=fhk\nfgde-\nnotes the factorized matrices. The cost function is usually\ncustomized for speciﬁc objectives [1, 4, 14, 17]. Following\nthis,xtfis approximated as:\nxtf\u0019K∑\nk=1hk\nfuk\nt: (5)\nAs in LHA, the index kstands for the speciﬁc spectral pat-\ntern. This pattern is described by the vector [hk\n1;\u0001\u0001\u0001; hk\nF].\nThe volume of the k-th basis in the t-th time frame is de-\nnoted by uk\nt.His often called the basis matrix, and Uis\ncalled the activation matrix.\nCemgil [3] proposed a full Bayesian inference of NMF,\nin which the joint posterior probability of latent variablesp(S;H;UjX)is estimated. Here, Sdenotes the set of K\nseparated spectrograms. The likelihoods and prior distri-\nbutions are written as:\np(xtfjs[k]\ntf) =\u000e(xtf\u0000K∑\nk=1sk\ntf); (6)\np(sk\ntfjhk\nf; uk\nt) =P(sk\ntfjhk\nfuk\nt); (7)\np(hk\nf) = Gam(hk\nfja0; b0); (8)\np(uk\nt) = Gam(uk\ntja0; b0); (9)\nwhere sk\ntfdenotes the observation of k-th sound. Here-\nafter, a square bracket indicates a set over the index vari-\nable, \u000edenotes delta function, Pdenotes Poisson distri-\nbution, and Gam denotes Gamma distribution. The prior\ndistributions of hk\nfanduk\ntare the conjugate priors, where\na0; b0are the hyperparameters of the distributions. A vari-\national EM algorithm is obtained based on a mean-ﬁeld\napproximation, p(S;H;UjX)\u0019q(S)q(H)q(U).\nThe main drawback of NMF is its inability to model\nspectral and temporal continuity using Gaussian mixtures.\nThough one can say that template-based approach can\nmodel these continuities, in that case, we cannot update\nthe envelope of basis and activation matrices adaptively.\nMany methods have been proposed to solve this problem\n[1,14,16,18], but these methods are difﬁcult to extend fur-\nther because they do not estimate Gaussian mixture densi-\nties based on variational Bayes.\n3. SIGNAL MODEL\nIn this section, we describe how to integrate the two\npromising methods: LHA and NMF. At ﬁrst, we describe\nour idea by introducing the spectral continuity. In these\nmethods, the wavelet spectrogram is interpreted as a his-\ntogram of sound quanta. Here, the volume of each sound\nis interpreted as the number of quanta. The number is de-\ntermined from a Poisson distribution for each time frame.\nNext, the distribution of sound quanta is determined from\na mixture of Gaussians. In the next subsection, we de-\nscribe a straightforward observation model. This model is\nnot suitable for the estimation, so we introduce several ap-\nproximations afterward to formulate a VB-EM algorithm.\n3.1 Generative Model\nThe volume of the k-th sound at the t-th time frame is\ndrawn from a Poisson distribution P(Sk\ntjuk\nt), similar to\nNMF. Next, we draw the number of observations of the m-\nth harmonic partial of the k-th sound following a multino-\nmial distribution. The relative weight of each component\nis determined by \u0011km.\np(Sk\nt) =P(Sk\ntjuk\nt) (10)\np(Sk[m]\nt) =M(Sk[m]\ntjSk\nt; \u0011k[m]) (11)\nHere,Mdenotes multinomial distribution.\nNext, we draw a set of observed particles: Xkm\nt=\n[xkm\nt1;\u0001\u0001\u0001; xkm\ntSkm\nt]. The value of each particle follows theGaussian distribution of the corresponding the m-th har-\nmonic partial. The likelihood of the observation is written\nas:\np(xkm\ntnj\u0016k; \u0015k) =N(xkm\ntnj\u0016k+om; \u0015\u00001\nk);(12)\np(Xkm\ntjSkm\nt; \u0016k; \u0015k) =Skm\nt∏\nn=1N(xkm\ntnj\u0016k+om; \u0015\u00001\nk):\n(13)\nTo generate the spectrogram of each harmonic partial,\nwe assume the number of particles observed in spectro-\ngrams as a histogram of particles that have a value error in\nthe range of \u000fF=2. This corresponds with our transforma-\ntion of the continuous probabilistic density functions into\ndiscrete probabilistic mass functions.\nxkm\ntf= #fnjxf\u0000\u000fF=2\u0014xkm\ntn\u0014xf+\u000fF=2g (14)\nxkm\nt:=Skm\nt\u0000F∑\nf=1xkm\ntf (15)\n^rkm\nf=∫xf+\u000fF=2\nxf\u0000\u000fF=2N(xj\u0016k+om; \u0015\u00001\nk)dx (16)\np(xkm\nt[f]; xkm\nt:) =M(xkm\nt[f]; xkm\nt:jSkm\nt;^rkm\n[f];^rkm\n:) (17)\nHere, #denotes the number of elements in the set, xkm\ntfde-\nnotes the number of particles allocated the f-th frequency\nbin,xkm\nt:denotes the number of particles which are not\nallocated any frequency bin, and ^rkm\nfdenotes the relative\nweight of each frequency bin. Further, ^rkm\nfis approxi-\nmated as:\n^rkm\nf\u0019\u000fFN(xfj\u0016k+om; \u0015\u00001\nk): (18)\nWe denote the right hand of the equation rkm\nf. Finally, the\nobserved spectrogram is obtained as a summation of the all\nharmonic components.\nxtf=∑\nkmxkm\ntf (19)\n3.2 Approximations\nFor the above formulations are not appropriate for\nBayesian estimation, we introduce the following approx-\nimation. The main objective is to marginalize the volume\nvariables Sk\ntandSkm\nt. To do this, we inspect the following\ncharacteristics of Poisson distribution.\nPoisson distribution gives the probability of nevent\nobservations in a unit time when the average occurrence\ninterval is \u0015\u00001. Next, we consider to distribute the\nobservations into Kclasses, following the distribution:\nM(n[k]jn; p [k])P(nj\u0015). The marginal probability of each\nclass of the multinomial distribution follows a binomial\ndistribution, so p(nkjn; pk) = Bin( nkjn; pk). Further,\nwe assume that p(nkjpk; \u0015) = P(nkjpk\u0015)because the\nevent of the k-th class occurs in an average time interval\nof(pk\u0015)\u00001. In the following section, we formulate a VB-\nEM algorithm based on this observation model.4. BAYESIAN NONNEGATIVE HARMONIC\nFACTORIZATION\nIn this section,\nwe describe the formulation of our model\nand the update procedures of Bayesian nonnegative har-\nmonic factorization (BNHF). The probabilistic mass func-\ntion of the intermediate spectrogram of the m-th harmonic\npartial and the k-th harmonic sound is ﬁrst described.\nThe spectrogram is generated following the activation uk\nt,\nwhich is the relative weight of each harmonic partial \u0011km.\nThe prior distributions are selected to imitate LHA and\nNMF. This is formulated as follows.\np(skm\ntfjuk\nt; \u0011k; \u0016k; \u0015k)\n\u0019 P(skm\ntfj\u000fFuk\nt\u0011kmN(xfj\u0016k+om; \u0015\u00001\nk)) (20)\np(uk\nt) = Gam(uk\ntja0; b0) (21)\np(\u0011k) = Dir( \u0011kj\u000b0\nm)/M∏\nm=1\u0011\u000b0\nm\u00001\nkm(22)\np(\u0016k; \u0015k) =N(\u0016kjm0;(\f0\u0015k)\u00001)W(\u0015kjw0; \u00170)(23)\nHere,Wdenotes Wishart distribution and a0,b0,\u000b0\nm,m0,\n\f0,w0, and \u00170are the hyperparameters. The prior dis-\ntributions are not conjugate, and thus the analytic varia-\ntional Bayesian inference of the posterior distributions is\nintractable. Instead, we will follow a limit that \u000fF!0.\nUnder this condition, the posterior distributions are written\nin an approximately conjugate form. First, we assume the\nfollowing factorization.\nq(S; u; \u0011; \u0016; \u0015 ) =q(S)∏\ntkq(uk\nt)K∏\nk=1fq(\u0011k)q(\u0016k; \u0015k)g\n(24)\nThis is known as mean-ﬁeld approximation.\n4.1 VB-E Step\nDuring the VB-E step, we calculate the temporal estima-\ntion of separated source spectrogram skm\ntf.\nlnq\u0003(s[km]\ntf) = ln( XjS) +E[p(Sju; \u0011; \u0016; \u0015 )]\n= ln\u000e(xtf\u0000∑\nkmskm\ntf) +E[lnuk\nt+ ln\u0011km\n+ lnN(xfj\u0016k+om; \u0015\u00001\nk)] (25)\nHereafter, all constant variables that do not affect the in-\nference are omitted. The optimal posterior distribution is a\nmultinomial distribution.\nq(s[km]\ntf) =M(s[km]\ntfjxtf; \r[km]\ntf) (26)\nln ~\rkm\ntf=E[lnuk\nt+ ln\u0011km+ lnN(xfj\u0016k+om; \u0015\u00001\nk)]\n(27)\n\rkm\ntf=~\rkm\ntf∑\nk0m0~\rk0m0\ntf(28)\nHere,Mdenotes multinomial distribution.4.2 VB-M Step\nDuring the VB-M step, we update the posteriordistribu-\ntions of uk\nt; \u0011km; \u0016k, and \u0015k. For example, we describe\nthe Bayesian estimation of uk\ntin detail. The logarithm of\nthe optimal posterior distribution is written as:\nlnq\u0003(uk\nt) =ES[lnp(Sju; \u0011; \u0016; \u0015 )] + ln p(uk\nt)\n=∑\nfmE[skm\ntf] lnuk\nt+ (a 0\u00001) lnuk\nt\n\u0000∑\nfm\u000fFuk\nt\u0011kmN(xfj\u0016k+om; \u0015\u00001\nk):(29)\nTaking the limit \u000fF!0, we obtain the following update:\nq\u0003(uk\nt)\u0019Gam(uk\ntjak\nt; b0);where (30)\nak\nt=a0+∑\nfmE[skm\ntf]: (31)\nThe same is true for \u0011km; \u0016k, and \u0015k: the optimal pos-\nterior distributions have the conjugate form when we take\nthe limit \u000fF!0. The approximated posterior distributions\nare written as:\nq\u0003(\u0011k)\u0019Dir(\u0011 kj\u000bk); (32)\nq\u0003(\u0016k; \u0015k)\u0019 N (\u0016kjmk;(\fk\u0015k)\u00001)W(\u0015kjwk; \u0017k);(33)\nwhere the posterior hyperparameters are written as:\n\u000bkm=\u000b0\nm+∑\ntfE[skm\ntf]; (34)\nmk=m0\f0+∑\ntfmE[skm\ntf](xf\u0000om)\n\f0+∑\ntfmE[skm\ntf]; (35)\n\fk=\f0+∑\ntfmE[skm\ntf]; (36)\nw\u00001\nk=w\u00001\n0+\f0m2\n0+∑\ntfmE[skm\ntf](xf\u0000om)2\u0000\fkm2\nk;\n(37)\n\u0017k=\u00170+∑\ntfE[skm\ntf]: (38)\nThe\nupdate equation of BNHF is quite similar to that of\nLHA, and these two methods had exactly the same result in\nour experiment. The difference is that our model explicitly\nmodels the volume of each sound, which makes it easier\nto consider the temporal continuity. This is described in\nmore detail in the next section. We can also formulate the\nGibbs sampler by using a similar approximation in which\nthe space complexity is of the order O(TFKM ), instead\nof the O(TNKM )for LHA. The derivations are omitted\ndue to space restrictions.\n5. BAYESIAN NONNEGATIVE\nHARMONIC-TEMPORAL FACTORIZATION\nHere, we describe how to introduce temporal continuity to\nBNHF. The temporal structure is modeled using a mixture\nof Gaussians arranged at regular intervals. The intensity ofN\u001cMTF\nK\u0012\u0011\u0013\u0010\n\u0016\u0015\u0017\u0014\nxtf\u001b\n\u0016\u0015\u0017\u0014\ns\u001ckm\ntf?\u0012\u0011\u0013\u0010\nuk\n\u001c\u001b a0; b0\n6\n\u0012\u0011\u0013\u0010\n\u0011k\u001b \u000b0\nm\b\b \u0019\u0012\u0011\u0013\u0010\n\u0016k\nHH Y\n\u0012\u0011\u0013\u0010\n\u0015k6\u001b m0; \f0\n\u001b w0; \u00170\nFigure 2 . Graphical model of proposed method. Single\nsolid lines indicate latent\nvariables and double solid lines\nindicate observed variables.\neach Gaussian component is decided using a Gamma dis-\ntribution. Let Tbe the interval, \u001cbe the index of Gaussian\ncomponents, \u0015Tbe the precision of each Gaussian com-\nponent, and uk\n\u001cbe the intensity of \u001c-th Gaussian compo-\nnent of k-th sound. The joint distribution can be written\nasp(X; S; u; \u0011; \u0016; \u0015 ). A graphical model of the method is\nshown in Figure 2.\nThe conditional probability of s\u001ckm\ntfis:\np(s\u001ckm\ntfju; \u0011; \u0016; \u001c ) =P(s\u001ckm\ntfj\u000fTuk\n\u001cN(ytj\u001cT; \u0015\u00001\nT)\n\u0002\u000fF\u0011kmN(xfj\u0016k+om; \u0015\u00001\nk));(39)\nwhere ytis the temporal position of the t-th time frame.\nThe corresponding optimal posterior distribution is:\nq(s[\u001ckm]\ntf) =M(s[\u001ckm]\ntfjxtf; \r[\u001ckm]\ntf); (40)\n\r\u001ckm\ntf/exp(E[ln uk\n\u001c+ ln\u0011km+\nlnN(xfj\u0016k+om; \u0015\u00001\nk)] + ln N(tj\u001cT; \u0015\u00001\nT))(41)\nFurther, the optimal posterior distribution of uk\n\u001cis approx-\nimated as:\nq(uk\n\u001c)\u0019Gam( uk\n\u001cjak\n\u001c; b0);where (42)\nak\n\u001c=a0+∑\ntfmE[s\u001ckm\ntf]: (43)\n6. EVALUATION\nIn this section, we compare the performance of three multi-\npitch estimation methods: LHA, NHF, and NHTF. We then\ndiscuss their performance in detail.\n6.1 Estimation Target\nFor the experiment, we used 40 musical pieces from\nthe RWC Music Database [6]. These included ﬁve pi-\nano solo pieces (RM-J001 to RM-J005), ﬁve guitar solo\npieces (RM-J006 to RM-J010), ten jazz duo pieces (RM-\nJ011 to RM-J020), ten jazz pieces played with three or\nmore players (RM-J021 to RM-J030), and ten classical\nchamber pieces (RM-C012 to RM-C021). All the pieces\nwere recorded from MIDI ﬁles using a MIDI synthesizer\n(Yamaha MOTIF-XS.) The drum tracks were muted, and\nthe number of players was counted without including theTable 1. Calculated F-measures.\nNon-infomative Infomative\nMusic Type LHA BNHF BNHTF LHA BNHF BNHTF\nPiano Solo 0.558 0.558 0.590 0.584 0.584 0.590\nGuitar Solo 0.684 0.684 0.726 0.728 0.728 0.740\nJazz (Duo) 0.524 0.524 0.545 0.552 0.552 0.556\nJazz (Trio\u0018) 0.523 0.523 0.548 0.536 0.536 0.541\nChamber 0.481 0.481 0.508 0.503 0.503 0.512\ndrum player. The recorded signals were truncated to the\nﬁrst 32 seconds\nto reduce the large computational time\nneeded for the experiment. They were transformed into\nwavelet spectrograms using Gabor wavelets with a time\nresolution of 16 [msec], frequency bins from 30 to 3000\n[Hz], and a frequency resolution of 12 [cents]. The ground\ntruths were constructed using the reference MIDI ﬁles.\n6.2 Experimental Settings\nHere, we describe the experimental settings. For the esti-\nmation, two prior distribution settings were evaluated. In\nthe ﬁrst one, all priors were set to be non-informative. That\nis,a0; b0; \u000b0\nm; \f0; w0;and\u00170were set to unity and m0\nwas set to zero. In the other one, the prior distribution\nof the harmonic structure was set appropriately. That is,\n\u000b0\nm= 0:6547Nm\u00002, where Nis the number of total ob-\nservations. The other hyperparameters were set to be non-\ninformative. The setting of \u000b0\nmwas the same setting as\nHTC.\nAs an initialization, the relative weight of the source\nmodel in the t-th frame was set to the sum of the ampli-\ntudes of the nearest frequency bins of its overtones. The\ninitial overtone weights were set to decay exponentially.\nModel orders KandMwere set to 73 and 6, respec-\ntively, where the number of overtones Mis equal to HTC\n[7]. The EM algorithm was truncated at 200 iterations. The\nnumber of iterations was determined experimentally on the\nbasis of estimation accuracy saturation.\nAfter the iterations, the estimated pitches were extracted\nfrom the posterior hyperparameters. Let rbe the threshold.\nThe effective observation count of the k-th basis in the t-th\ntime frame, Ntk, satisﬁes Ntk\u0015rmax tkNtk, is consid-\nered to be sounding. The threshold was optimized experi-\nmentally for each piece and the method used to evaluate the\npotential performance of each model. The model frequen-\ncies were allocated the nearest note number. The estimated\nresult and the ground truth were transformed into T\u0002128\nbinary maps, and the F-measure was then calculated. Let\nNbe the number of entries in the estimated map, Cbe the\nnumber of entries in the truth map, and Rbe the number\nof correct entries in the estimated map. The resultant F-\nmeasure is calculated as F= 2R=(N +C). The larger the\nvalue of the F-measure, the better the performance.\n6.3 Results\nThe results are shown in Table 1. Our method outperforms\nthe conventional method for all the music type and both\nprior settings. The highest performance was attained withTime frame\nNote number\nFigure 3 . Example of estimated result of the proposed\nmethod with musical piece RM-J007\n.\nappropriate prior for four of the ﬁve groups of musical\npieces and with non-informative for the remaining group.\nThis indicates the importance of the joint estimation of the\nharmonic and temporal structures. Figure 3 illustrates an\nexample of the estimated result of the proposed model.\n7. RELATED WORKS\nOur method have a strong relation to the model proposed\nby Ochiai et., al [12,13]. Ours and theirs are notably differ-\nent in three points. Firstly, they do not explicitly model the\nspectral continuity. Secondly, they use Dirac delta function\ninstead of the \u000f-neighborhood to achieve the estimation of\nGaussian mixture distribution. By following this strategy,\nthe estimation is in conjugate form, but the interpretation\nof the model becomes difﬁcult. Thirdly, they model the\ntemporal structure based on the joint estimation of proba-\nbilistic context-free grammar and Gaussian mixture distri-\nbution.\n8. CONCLUSION\nWe presented a new multipitch analyzer based on vari-\national Bayes that explicitly models the harmonic and\ntemporal structures separately based on Gaussian mixture\nmodel. Several priors were set to be not conjugate, in or-\nder to integrate NMF and LHA. The variational posterior\ndistributions become conjugate under several approxima-\ntions. Our method can be viewed as a Bayesian NMF\nmethod with adaptive harmonic basis. Evaluation results\nshowed that the proposed method outperform the con-\nventional multipitch analyzer, latent harmonic allocation\n(LHA). In the future, we will propose a more precise mod-\neling using recent improvements to NMF, including the\nsource-ﬁlter model [17] and nonparametric models [11].\nThis research is partially supported by KAKENHI (S) No.\n24220006.9. REFERENCES\n[1] A. Bertin et al. Enforcing harmonicity and smoothness\nin bayesian non-negative matrix factorization applied to\npolyphonic music transcripntion. IEEE Trans. on ASLP ,\n18(3):538–549, 2010.\n[2] B. A. Casey et al. Content-based music information re-\ntrieval: Current directions and future challenges. Proc. of\nthe IEEE , 96(4):668–696, 2008.\n[3] A. T. Cemgil. Bayesian inference for nonnegative ma-\ntrix factorization models. Technical Report CUED/F-\nINFENG/TR.609, 2008.\n[4] D. FitzGerald et al. On the use of the beta divergence for\nmusical source separation. In Proc. ISSC , pages 1–6, 2010.\n[5] M. Goto. A real-time music-scene-analysis system:\nPredominant-F0 estimation for detecting melody and bass\nlines in real-world audio signals. Speech Communication ,\n43(4):311–329, September 2004.\n[6] M. Goto et al. RWC music database: Popular, classical,\nand jazz music databases. In Proc. ISMIR, pages 287–288,\n2002.\n[7] H. Kameoka et al. A multipitch analyzer based on har-\nmonic temporal structured clustering. IEEE Trans. on\nASLP , 15(3):982–994, 2007.\n[8] Y. E. Kim et al. Music emotion recognition: A state of the\nart review. In Proc. ISMIR , pages 255–266, 2010.\n[9] T. Kitahara et al. Instrument identiﬁcation in polyphonic\nmusic: feature weighting to minimize inﬂuence of sound\noverlaps. EURASIP J. Appl. Signal Process., 2007:1–15,\n2007.\n[10] A. Klapuri. Multipitch analysis of polyphonic music and\nspeech signals using an auditory model. IEEE Trans. on\nASLP , 16(2):255–266, 2008.\n[11] M. Nakano et al. Nonparametric Bayesian music parser. In\nProc. ICASSP , pages 461–464, 2012.\n[12] K. Ochiai et al. Explicit beat structure modeling for non-\nnegative matrix factorization-based multipitch analysis. In\nProc. ICASSP , pages 133–136, 2012.\n[13] K. Ochiai et al. Hierarchical Bayesian modeling of the gen-\nerating process of music signals for automatic transcrip-\ntion. In ASJ Spring Meeting, 2012 (in Japanese).\n[14] S. A. Raczy ´nski et al. Multipitch analysis with harmonic\nnonnegative matrix approximation. In Proc. ISMIR, pages\n381–386, 2007.\n[15] Y. Ueda et al. HMM-based approach for automatic chord\ndetection using reﬁned acoustic features. In Proc. ICASSP ,\npages 5518–5521, 2010.\n[16] E. Vincent et al. Adaptive harmonic spectral decomposi-\ntion for multiple pitch estimation. IEEE Trans. on ASLP,\n18(3):528–537, 2010.\n[17] T. Virtanen et al. Analysis of polyphonic audio using\nsource-ﬁlter model and nonnegative matrix factorization.\nInAdvances in Models for Acoustic Processing , 2006.\n[18] N. Yasuraoka et al. I-Divergence-based dereverberation\nmethod with auxiliary function approach. In Proc. ICASSP ,\npages 369–372, 2011.\n[19] K. Yoshii and M. Goto. A nonparametric Bayesian multi-\npitch analyzer based on inﬁnite latent harmonic allocation.\nIEEE Trans. on ASLP , 20(3):717–730, 2012."
    },
    {
        "title": "A Multipitch Approach to Tonic Identification in Indian Classical Music.",
        "author": [
            "Justin Salamon",
            "Sankalp Gulati",
            "Xavier Serra"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1257114",
        "url": "https://doi.org/10.5281/zenodo.1257114",
        "ee": "https://ismir2012.ismir.net/event/papers/499-ismir-2012.pdf",
        "abstract": "Introduction\n\nThese datasets comprise audio excerpts and manually doneannotationsof the tonic pitch of the lead artist for each audio excerpt. Each excerpt is accompanied by its associated editorial metadata. These datasets can be used to develop and evaluate computational approaches for automatic tonic identification in Indian art music. These datasets have been used in several articles mentioned below.A majority of these datasets come from theCompMusic corporaof Indian art music, for which each recording is associated with aMBID. With the MBID other information can be obtained using theDunya API. We here provide an overview of the tonic identification datasets.\n\nDatasets\n\nThe statistics about the datasets for tonic identification is listed in the table below. These six datasets are used in [1] for a comparative evaluation. To the best of our knowledge these are the largest datasets available for tonic identification for Indian art music. These datases vary in terms of the audio quality, recording period (decade), the number of recordings for Carnatic, Hindustani, male and female singers and instrumental and vocal excerpts. For a detailed information about these datasets we refer to Chapter 3 of thisthesis.\n\nAll the datasets (annotations) are version controlled. To know how the features are extracted visit the companion page for thepublication.\n\nThe audio files corresponding to these datsets are made available on request for only research purposes. To obtain the files, please refer to this Zenodo entry.\n\nAnnotation Format\n\nThe tonic annotations are availabe both in tsv and json format.\n\nTSV: relative path to audiotabtonic(Hz)tabCarnatic or Hindustanitabartist_nametabgender of the singervocal or instrumental\n\nJSON:{\n &#39;artist&#39;: name of the lead artist if available,\n\n &#39;filepath&#39;: relative path to the audio file,\n\n &#39;gender&#39;: gender of the lead singer if available,\n\n &#39;mbid&#39;: musicbrainz id when available,\n\n &#39;tonic&#39;: tonic in Hz,\n\n &#39;tradition&#39;: Hindustani or Carnatic,\n\n &#39;type&#39;: vocal or instrumental\n }\n\n\nwhere keys of the main dictionary are the filepaths to the audio files (feature path is exactly the same with a different extension of the file name).\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite:\n\n\nGulati, S., Bellur, A., Salamon, J., Ranjani, H. G., Ishwar, V., Murthy, H. A.,  Serra, X. (2014). Automatic Tonic Identification in Indian Art Music: Approaches and Evaluation.Journal of New Music Research,43(01), 5573.\n\n\nhttp://hdl.handle.net/10230/25675\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nIf you have any questions or comments about the dataset, please feel free to email: [sankalp (dot) gulati (at) gmail (dot) com], or[sankalp (dot) gulati (at) upf (dot) edu]\n\n\n\nhttp://compmusic.upf.edu/iam-tonic-dataset\n\n",
        "zenodo_id": 1257114,
        "dblp_key": "conf/ismir/SalamonGS12",
        "keywords": [
            "Introduction",
            "Datasets",
            "Statistics",
            "Annotation Format",
            "Audio Files",
            "Publication Citation",
            "Dataset Contact",
            "Datasets",
            "Evaluation",
            "Datasets Variation"
        ],
        "content": "A MULTIPITCH APPROACH TO TONIC IDENTIFICATION IN INDIAN\nCLASSICAL MUSIC\nJustin Salamon, Sankalp Gulati and Xavier Serra\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\nfjustin.salamon,sankalp.gulati,xavier.serra g@upf.edu\nABSTRACT\nThe tonic is a fundamental concept in Indian classical mu-\nsic since it constitutes the base pitch from which a lead\nperformer constructs the melodies, and accompanying in-\nstruments use it for tuning. This makes tonic identiﬁcation\nan essential ﬁrst step for most automatic analyses of Indian\nclassical music, such as intonation and melodic analysis,\nand raga recognition. In this paper we address the task\nof automatic tonic identiﬁcation. Unlike approaches that\nidentify the tonic from a single predominant pitch track,\nhere we propose a method based on a multipitch analysis of\nthe audio. We use a multipitch representation to construct\na pitch histogram of the audio excerpt, out of which the\ntonic is identiﬁed. Rather than manually deﬁne a template,\nwe employ a classiﬁcation approach to automatically learn\na set of rules for selecting the tonic. The proposed method\nreturns not only the pitch class of the tonic but also the pre-\ncise octave in which it is played. We evaluate the approach\non a large collection of Carnatic and Hindustani music, ob-\ntaining an identiﬁcation accuracy of 93%. We also discuss\nthe types of errors made by our proposed method, as well\nas the challenges in generating ground truth annotations.\n1. INTRODUCTION\nOne of the fundamental concepts in Indian classical mu-\nsic is the tonic. The tonic is a base pitch chosen by the\nperformer, and serves as the foundation for the melodic\ntonal relationships throughout the performance. Every per-\nformer chooses a tonic pitch which best allows them to\nfully explore their vocal (or instrumental) pitch range for a\ngiven raga exposition [3]. Consequently, all accompanying\ninstruments are tuned with relation to the tonic chosen by\nthe lead performer.\nSince the entire performance is relative to the tonic (cor-\nresponding to the Sanote of the raga), the lead performer\nneeds to hear the tonic pitch throughout the concert. This\nis provided by a constantly sounding drone which plays in\nthe background and reinforces the tonic. The drone may be\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.produced by a variety of instruments such as the Tanpura ,\nthe electronic Shruti box , or by the sympathetic strings of\nan instrument such as the Sitar orVeena . Along with the\ntonic, the drone typically produces other important notes\nin the raga such as the Pa(ﬁfth) or the Ma(fourth), and\nslightly less often the seventh ( Ni), depending on the choice\nof raga. This drone serves as the reference sound that es-\ntablishes all the harmonic and melodic relationships during\na given performance. Other notes used in the performance\nderive their meaning and purpose in relation to the Saand\nthe tonal context established by the particular raga [2].\nWhen considering the computational analysis of Indian\nclassical music, it becomes evident that identifying the tonic\nis a crucial ﬁrst step for more detailed tonal studies such as\nintonation [9], motif analysis [13] and raga recognition [1].\nThis makes automatic tonic identiﬁcation a fundamental\nresearch problem. However, despite its importance in In-\ndian classical music, the problem of automatic tonic identi-\nﬁcation has received very little attention from the research\ncommunity to date.\nTo the best of our knowledge, all previous approaches\nfor automatic tonic identiﬁcation are based on applying\nmonophonic pitch trackers to the audio recording, meaning\nthey solely use the information proportioned by the pre-\ndominant melody [16]. In some cases a monophonic pitch\ntracker is used even though the audio recording contains\nseveral instruments playing simultaneously [12]. These\napproaches have also been fairly restricted in terms of the\nmusical content studied: in [16] only the Alap sections\nof 118 solo vocal recordings are used for evaluation, and\nin [12] the evaluation material is restricted to Sampurna\nraga. Both approaches also restrict the allowed frequency\nrange for the tonic to a single octave, a limitation which\ncan not be imposed if we wish to devise a single method\nfor tonic identiﬁcation for both male and female vocal per-\nformances.\nIn this paper we propose a method for tonic identiﬁca-\ntion in Indian classical music based on a multipitch anal-\nysis of the audio signal. The motivation for a multipitch\napproach is twofold: ﬁrst, the music material under inves-\ntigation often includes several instruments playing simulta-\nneously. Apart from the lead performer, recordings contain\nthe drone instrument, and may also include other predom-\ninant instruments such as the violin, as well as percussive\ninstruments. Second, we know that the tonic is continually\nreinforced by the drone instrument, an important fact thatTime (s)Frequency\n0 1 2 3 4 5 6 7 8 95000\n4500\n4000\n3500\n3000\n2500\n2000\n1500\n1000\n500\n0Figure 1 . Spectrogram of an excerpt of Hindustani mu-\nsic with two clearly visible types of harmonic series, one\nbelonging to the drone and the other to the lead voice.\nis not exploited if we only extract a single pitch estimate\nfor each frame of the recording. To illustrate this point, in\nFigure 1 we display the spectrogram for an excerpt of Hin-\ndustani music [2]. Two types of harmonic series are clearly\nvisible in the spectrogram: the ﬁrst type of harmonic series,\nwhich consist of almost perfectly ﬂat lines, belong to the\nnotes of the drone instrument (playing SaandPa). The sec-\nond type of harmonic series (which starts roughly at time\n2s) belongs to the voice of the lead performer. Evidently, if\nwe only consider the pitch of the lead performer, we loose\nthe pitch information proportioned by the drone instrument\nwhich in this case is a better indicator of the tonic pitch.\nAt the outset of this study, we deﬁned three goals for\nthe method to be developed: ﬁrst, it should be applicable\nto a wide range of performances, including both the Car-\nnatic [18] and Hindustani musical styles, male and female\nsingers, and different recording conditions. Second, the\napproach should identify the tonic pitch in the correct oc-\ntave, without restricting the allowed frequency range to a\nsingle octave. Finally, the approach should be able to iden-\ntify the tonic using a limited segment of the full recording,\nand this segment can be taken from any part of the piece.\nThe structure of the remainder of the paper is as fol-\nlows. In Section 2 we present our proposed tonic identi-\nﬁcation method. In Section 3 we describe the evaluation\nmethodology employed in this study, including the music\ncollection used for evaluation and the annotation procedure\nused to generate the ground truth. Then, in Section 4 we\npresent and discuss the results of the evaluation, and ﬁnally\nin Section 5 we provide some conclusions and proposals\nfor future work.\n2. PROPOSED METHOD\nThe proposed method is comprised of four main blocks: si-\nnusoid extraction, salience function, candidate generation\nand tonic selection. The ﬁrst two blocks of the system were\noriginally proposed as part of a predominant melody ex-\n!\"#$%&'\"(#)($*&++\"#,$-\"./$/&0*1#\")$-(\",/2#,$Audio signal \n3+().0&'$.0&#%410*$\n50(67(#)89:*+'\".7;($)100()21#$Sinusoid extraction Salience function Candidate Generation \nTonic selection Tonic Spectral peaks Time-frequency salience Tonic candidates \n3&'\"(#)($<(&=$<\")=\"#,$\n<\".)/$>\"%.1,0&*$\n>\"%.1,0&*$<(&=$<\")=\"#,$\n?#.(0@&'$A(+0(%(#.&21#$\nClassifier Figure 2 . Block diagram of the proposed tonic identiﬁca-\ntion method.\ntraction system [14, 15], and have been adapted here for\nthe task of tonic identiﬁcation. In the following sections\nwe describe the processing steps involved in each of the\nfour blocks of the system. A block diagram of the pro-\nposed method is provided in Figure 2.\n2.1 Sinusoid Extraction\nIn the ﬁrst step of the method, we extract sinusoidal com-\nponents, i.e. spectral peaks, from the audio signal. The\nsinusoid extraction process is divided into two stages as\ndepicted in Figure 2: spectral transform and sinusoid fre-\nquency/amplitude correction.\nWe start by applying the Short-Time Fourier Transform\n(STFT) given by:\nXl(k) =M\u00001X\nn=0w(n)\u0001x(n+lH)e\u0000j2\u0019\nNkn; (1)\nl= 0;1;::: andk= 0;1;:::;N\u00001\nwherex(n)is the time signal, w(n)the windowing func-\ntion,lthe frame number, Mthe window length, Nthe\nFFT length and Hthe hop size. We use the Hann window-\ning function with a window size of 46:4ms, a hop size of\n2:9ms and a\u00024zero padding factor, which for data sam-\npled atfS= 44:1kHz givesM= 2048 ,N= 8192 and\nH= 128 . Given the FFT of a single frame Xl(k), spectral\npeaks are selected by ﬁnding all the local maxima kmof\nthe magnitude spectrum jXl(k)j.\nThe location of the spectral peaks is limited to the bin\nfrequencies of the FFT, which for low frequencies can re-\nsult in a relatively large error in the estimation of the peak01234567891012001400160018002000220024002600280030003200\nTime (s)Frequency (cents)Upper Pa (5th) \nLower Pa (5th) Sa (tonic) Lead voice Figure 3 . Peaks of the salience function for an excerpt of\nHindustani music.\nfrequency. To overcome this quantisation, in the second\nstage of this block we apply the approach described in [4],\nin which the phase spectrum \u001el(k)is used to calculate the\npeak’s instantaneous frequency (IF) and amplitude, which\nprovide a more accurate estimate of the peak’s true fre-\nquency and amplitude.\n2.2 Salience Function (Multipitch Representation)\nWe use the extracted spectral peaks to compute a salience\nfunction – a multipitch time-frequency representation of\npitch salience over time. The salience computation is based\non harmonic summation similar to [8], where the salience\nof a given frequency is computed as the sum of the weighted\nenergies found at integer multiples (harmonics) of that fre-\nquency. Peaks of the salience function at each frame repre-\nsent salient pitches in the music recording. Note that whilst\nthe concepts of pitch (which is perceptual) and fundamen-\ntal frequency (which is a physical measurement) are not\nidentical, for simplicity in this paper we will use these two\nterms interchangeably.\nOur salience function covers a pitch range of nearly ﬁve\noctaves from 55Hz to 1.76kHz, quantized into 600 bins\non a cent scale (10 cents per bin). The reader is referred\nto [14, 15] for further details about the mathematical for-\nmulation of the salience function. In Figure 3 we plot the\npeaks of the salience function for the same excerpt from\nFigure 1. The tonic ( Sa) pitch which is played by the drone\ninstrument is clearly visible, as well as the upper and lower\nﬁfth ( Pa), and the pitch trajectory of the voice.\n2.3 Tonic Candidate Generation\nAs explained earlier, the peaks of the salience function rep-\nresent the pitches of the voice and other predominant in-\nstruments present in the recording at every point in time.\nThus, by computing a histogram of the pitch values for the\nentire excerpt, we obtain an estimate of which pitches are\nrepeated most often throughout the excerpt. Though pitch\nhistograms have been used previously for tonic identiﬁca-\n1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 320000.20.40.60.81\nFrequency (cents)Normalised countFigure 4 . Pitch histogram for an excerpt of Hindustani\nmusic.\ntion [12], they were constructed using only the most pre-\ndominant pitch at each frame, which means that in many\ncases the tonal information provided by the drone instru-\nment is not taken into consideration.\nWe start by taking the peaks of the salience function at\neach frame. Since the frequency range for the tonic pitch\nselected by singers in Indian classical music is relatively\nlimited, we can reduce the range from which salient pitches\nare selected. To ensure we cover the complete range for\nboth male and female singers, we consider salient pitches\nwith a fundamental frequency ranging from 110 Hz to 370\nHz. Importantly, note that this range spans almost 2 oc-\ntaves, meaning the system must be able to identify not only\nthe correct tonic pitch class, but also the octave in which it\nis played. Within this range, at each frame we take the top\nﬁve peaks (pitches) of the salience function.\nThe selected pitches are used to construct a pitch his-\ntogram. As the drone is usually weaker than the lead voice,\nwe avoid weighting each peak by its magnitude. The re-\nsulting pitch histogram goes from 110 Hz to 370 Hz and\nhas a resolution of 10 cents. Peaks of the histogram repre-\nsent the most frequent pitches in the excerpt, one of which\nwill be the tonic. In Figure 4 we present the histogram\ncomputed from the complete 3 minute excerpt used in the\nprevious examples. The pitch axis is plotted in cents, and\nthe histogram is normalised by the magnitude of its highest\npeak. For the excerpt under consideration, we note three\nclear peaks: the tonic Sa(2040 cents), the upper Pa(2740\ncents) and the tonic again, one octave up (3240 cents). This\nillustrates one of the challenges the system will have to\ndeal with – selecting the tonic at the correct octave. It also\nhighlights another important issue – the peak correspond-\ning to the tonic will not always be the highest peak in the\nhistogram, meaning the (perhaps na ¨ıve) approach of select-\ning the highest peak of the histogram would not provide\nsatisfactory results.\n2.4 Tonic Selection\nAs the tonic will not always be the highest peak of the his-\ntogram, we take the top 10 peaks of the pitch histogram\npi(i= 1:::10), one of which represents the pitch of the\ntonic. As mentioned in the introduction, all other notes\npresent in the musical piece are tuned with relation to the\ntonic. Bearing this in mind, we hypothesize that the tonic\ncan be identiﬁed based on the pitch intervals between themost frequent notes in the recording and their rate of occur-\nrence. For example, in the excerpt in Figure 3, the drone\nplays the tonic alongside the lower and upper ﬁfth. Thus,\na ﬁfth relationship between two frequent notes might serve\nas a good indicator for the tonic.\nIn the study of Western music, templates learned from\nmusic cognition experiments have been used for the re-\nlated task of key detection, where a pitch histogram (de-\nrived from a symbolic representation of the musical piece)\nis matched against templates representing the probability\nof different pitch classes given a certain tonal context [10].\nApproaches based on training a classiﬁer to determine the\nkey of a musical piece using chroma features automati-\ncally extracted from the audio signal have also been pro-\nposed [5]. In this study, we propose a classiﬁcation ap-\nproach to automatically learn the best set of rules for se-\nlecting the tonic, based on the pitch intervals between the\nmost frequent notes in the piece and their relative rate of\noccurrence (as indicated by the magnitude of the peaks of\nthe pitch histogram).\nWe start by annotating for each piece the rank i=I\nof the tonic (in terms of peak magnitude) out of the top 10\npeakspiof the pitch histogram. Then, we encode the 10\ntonic candidates as the distance (in semitones) between ev-\nery candidate piand the highest candidate in the histogram\np1. This gives us a set of features fi(i= 1:::10), where\nfirepresents the distance (in semitones) between piand\np1. The features fiand the annotated rank of the tonic I\nare used to train a classiﬁer for selecting the tonic. That\nis, we pose the task of tonic identiﬁcation as a classiﬁca-\ntion problem where we have 10 classes (10 candidates) and\nthe classiﬁer must choose the rank of the candidate corre-\nsponding to the tonic. Note that for all ﬁles in our collec-\ntion the tonic was always amongst the top 10 peaks piof\nthe pitch histogram.\nFor classiﬁcation we use the Weka data-mining soft-\nware [7]. We start by performing attribute selection using\ntheCfsSubsetEval attribute evaluator and BestFirst search\nmethod [6] with a 10-fold cross validation, only keeping\nfeatures that were used in at least 80% of the folds. The\nselected features were: f2,f3,f5,f6,f8andf9. Then, we\ntrain a C4.5 decision tree [11] in order to learn the optimal\nset of rules for selecting the tonic based on the pitch inter-\nvals between the tonic candidates. Note that we also evalu-\nated other classiﬁcation algorithms, namely support vector\nmachines (SMO with polynomial kernel) and an instance-\nbased classiﬁer (k*) [19]. However, the accuracy obtained\nusing the decision tree was signiﬁcantly higher (6% better\nthan SVM and 5% better than k*), and so for the rest of\nthe paper we will focus on the results obtained using this\nclassiﬁer. Additionally, using a decision tree has the ad-\nvantage that the resulting classiﬁcation rules can be easily\ninterpreted and, as shall be seen, are musically meaningful.\nThe resulting tree is presented in Figure 5. As it turns\nout, only 3 features are ﬁnally used: f2,f3andf5. Another\ninteresting observation is that the pitch intervals used by\nthe tree for making decisions correspond quite well to the\nintervals between the notes commonly played by the droneinstrument: 5 (i.e. 500 cents) corresponds to the interval\nbetween the lower Paand the tonic Sa, and 7 (700 cents)\nto the interval between the Saand upper Pa. Note that a\ndistance of 500 cents may also correspond to the distance\nbetween the Saand upper Ma, which might be a cause for\nconfusion in our system, and we will assess this when we\nanalyse the results.\nExamining the rules of the tree, we see that the most\nimportant relationship is between the top two peaks of the\nhistogram. When the second highest peak is more than\n500 cents above the highest peak, the latter is chosen as\nthe tonic. Examining the data we found that this almost al-\nways corresponds to one of two cases – the second peak is\neither Pa(i.e.Patuning) or Saone octave above the tonic.\nBranching left, the tree checks whether the highest peak is\nactually Pa(700 cents above the tonic). To conﬁrm this it\nchecks if the third peak is found 500 cents above the high-\nest peak (thus corresponding to Saone octave above the\ntonic). In this case the highest peak is indeed Pa, and the\nsecond highest peak is the tonic. Otherwise, we have a\ncase of Matuning (the second peak is tuned to Ma), and\nthe highest peak is the tonic. Similar interpretations can be\nmade for the remaining rules of the tree.\nf2 f2 f3 f3 f5 2nd 1st 3rd 4th 5th 1st > 5 <= 5 > -7 <= -7 > -6 <= -6 > 5 <= 5 > -11 <= -11 \nFigure 5 . Obtained decision tree for tonic identiﬁcation.\n3. EVALUATION METHODOLOGY\n3.1 Music Collection\nThe music collection used to evaluate the proposed ap-\nproach was compiled as part of the CompMusic project\n[17]. It consists of 364 excerpts of Indian classical music\nincluding both Hindustani (38%) and Carnatic (62%) mu-\nsic. The excerpts were extracted from 231 unique perfor-\nmances by 36 different artists, including both male (80%)\nand female (20%) singers. Every excerpt is 3 minutes long,\nand extracted from either the beginning, middle or end of\nthe full recording (for recordings longer than 12 minutes\nwe are able to extract all 3 excerpts, for shorter record-\nings a single excerpt from the beginning of the piece was\ntaken). Including excerpts from sections other than the be-\nginning of the piece is important, since in both the Hin-120 140 160 180 200 220 240 260 2800102030405060\nFrequency (Hz)Number of instances  \nFemale singersMale singersFigure 6. Distribution of tonic frequency for male and fe-\nmale vocal performances in our music collection.\ndustani and Carnatic music traditions different sections of\na performance can have very different acoustic character-\nistics. In Figure 6 we display the distribution of tonic fre-\nquencies in our collection for both male and female singers.\n3.2 Annotation Procedure\nThe tonic frequency for each excerpt was manually anno-\ntated by the authors. To assist the annotation process, we\nused the candidate generation part of our proposed method\nto extract 10 candidate frequencies for the tonic in the range\nof 110 Hz to 300 Hz. The annotator could then listen to the\ncandidate frequencies one by one together with the original\nrecording in order to identify the tonic frequency. Note that\nfor all excerpts in our collection the true tonic frequency\nwas present in one of the 10 candidates provided by the\nsystem.\nIt is worth noting that as part of the annotation process,\nthe listener must determine the octave in which the tonic\nis played. Since the drone instrument may play the tonic\npitch in two octaves simultaneously, the octave of the tonic\nis determined by the vocal range of the singer rather than\nthe drone instrument directly. Whilst in most cases the cor-\nrect octave is fairly unambiguous for vocal performances,\nwe encountered a small number of cases in which deter-\nmining the octave of the tonic was more difﬁcult. In future\nwork, we intend to study the relation between performer\nand drone instrument in greater depth, as well as conduct\nlistening tests to assess the degree of agreement between\nlisteners when asked to determine the octave of the tonic.\n4. RESULTS\nWe evaluate the proposed classiﬁcation-based approach us-\ning 10-fold cross validation. The experiment is repeated 10\ntimes, and the average results for all 10 repetitions are re-\nported. In Figure 7 we present the classiﬁcation accuracy\nobtained for our collection of 364 excerpts, as well as a\nbreakdown of the results based on musical style and gen-\nder of the lead performer.\nWe see that the proposed approach obtains a classiﬁca-\ntion accuracy (hence tonic identiﬁcation accuracy) of 93%\nfor our complete collection. Importantly, since the allowed\nAll Hindustani Carnatic Male Female707580859095100Classification AccuracyFigure 7. Classiﬁcation accuracy for the proposed ap-\nproach. All excerpts 93%, Hindustani 98%, Carnatic 90%,\nMale 95% and Female 88%.\ntonic frequency range spans more than one octave, it means\nwe are correctly identifying not only the pitch-class of the\ntonic, but also the octave at which it is played. Next, we ex-\namine the results depending on the musical style. We see\nthat we have almost perfect classiﬁcation for Hindustani\nmusic (98%), whilst for Carnatic music the performance\nis somewhat lower (90%). When examining the data, we\nnoted that in the Carnatic excerpts there were more cases\nwhere the Tanpura was quite weak (in terms of loudness).\nConsequently, this results in frames where the pitch corre-\nsponding to the tonic does not have a prominent peak in\nthe salience function. This in turn means the peak of the\npitch histogram which corresponds to the tonic has a fairly\nlow rank, leading to incorrect identiﬁcation of the tonic.\nWhen considering identiﬁcation accuracy as a function\nof the gender of the lead performer, we see that the system\nperforms better for pieces performed by male singers com-\npared to those performed by female singers. A possible\ncause for this is the different amount of male and female\nperformances in our collection. Since there are consider-\nably more male performances, the rules learned by the sys-\ntem are better suited for identifying the tonic in this type\nof musical material. Another factor that was identiﬁed as\ninﬂuential was the frequency range used to compute the\npitch histogram. Whilst our frequency range covers the\nentire range in which we expect to ﬁnd the tonic for both\nmale and female cases, for high frequency tonics this range\nwill not include the higher Saone octave above the tonic.\nAs it turns out, the presence of a higher Sais one of the\ncues used by the system, and for many female excerpts it\nis outside the range of the pitch histogram. In the future,\nwe intend to experiment with different frequency ranges\nfor the pitch histogram, as well as consider separate ranges\nfor male and female performances to see whether perfor-\nmance can be improved by including this extra piece of\ninformation prior to classiﬁcation.\nAs a ﬁnal step in our analysis of the results, we checked\nwhat types of errors were the most common in our evalu-\nation. We found that for male singers the most common\nerror was selecting the higher PaorMaas the tonic, whilst\nfor females it was selecting the lower PaorMa. This is un-\nderstandable, as these are two important notes that are of-\nten played by the drone instrument in addition to the tonic.\nThe difference in tonic frequency for males and females,together with the frequency range used for the pitch his-\ntogram, explains why for males we erroneously select a\nhigher note, whilst for females we select a lower one. Ad-\nditionally, for female singers we found that the confusion\nwas often caused due to the use of Matuning (Sa -Ma-Sa)\nof the drone instrument. If the higher Sais not present, the\nMatuning is equivalent to a rotated version of Patuning,\nresulting in the wrong rule being applied.\n5. CONCLUSION\nIn this paper we presented a novel approach for tonic iden-\ntiﬁcation in Indian classical music. Our method is based\non a multipitch analysis of the audio signal, in which the\npredominant pitches in the mixture are used to construct\na pitch histogram representing the most frequently played\nnotes in the piece. In this way, our representation also cap-\ntures the notes played by the drone instrument, and not\nonly the pitch of the lead performer. Using a classiﬁcation\napproach, we were able to automatically learn the best set\nof rules for tonic identiﬁcation given our pitch histogram\nrepresentation. The resulting decision tree was evaluated\non a large collection of excerpts consisting of a wide selec-\ntion of pieces, artists and recording conditions, and was\nshown to obtain high tonic identiﬁcation accuracy. Im-\nportantly, the approach is suitable for both Hindustani and\nCarnatic music, male and female performances, and only\nrequires a short excerpt of the full performance. In addi-\ntion, the rules learned by the system are easy to interpret\nand musically coherent.\nFollowing presentation of the results, we discussed the\ntypes of errors most commonly made by the proposed tonic\nidentiﬁcation method, and the main causes for these errors\nwhere identiﬁed. Finally, we proposed some directions\nfor future work, including a study of tonic octave percep-\ntion, considering different frequency ranges for the pitch\nhistogram in our proposed method, and devising gender-\nspeciﬁc tonic identiﬁcation approaches.\n6. ACKNOWLEDGMENTS\nThe authors would like to thank Joan Serr `a, Emilia G ´omez\nand Perfecto Herrera for their useful comments. This re-\nsearch was funded by the Programa de Formaci ´on del Pro-\nfesorado Universitario (FPU) of the Ministerio de Edu-\ncaci´on de Espa ˜na and the European Research Council un-\nder the European Union’s Seventh Framework Programme\n(FP7/2007-2013) / ERC grant agreement 267583 (Comp-\nMusic).\n7. REFERENCES\n[1] P. Chordia, J. Jagadeeswaran, and A. Rae. Automatic\ncarnatic raag classiﬁcation. J. of the Sangeet Research\nAcademy (Ninaad), 2009.\n[2] A. Danielou. The Ragas of Northern Indian Music. Mun-\nshiram Manoharlal Publishers, New Delhi, 2010.\n[3] B. C. Deva. The Music of India: A Scientiﬁc Study. Mun-\nshiram Manoharlal Publishers, Delhi, 1980.[4] K. Dressler. Sinusoidal extraction using an efﬁcient imple-\nmentation of a multi-resolution FFT. In Proc. of the Int.\nConf. on Digital Audio Effects (DAFx-06), pages 247–252,\nMontreal, Quebec, Canada, Sept. 2006.\n[5] E. G ´omez and P. Herrera. Estimating the tonality of of\npolyphonic audio ﬁles: Cognitive versus machine learning\nmodelling strategies strategies. In 5th Int. Conf. on Music\nInfo. Retrieval, Barcelona, Spain, Oct. 2004.\n[6] M. Hall. Correlation-based Feature Selection for Machine\nLearning. PhD thesis, University of Waikato, Hamilton,\nNew Zealand, 1999.\n[7] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann,\nand I. H. Witten. The weka data mining software: an update.\nSIGKDD Explor. Newsl., 11:10–18, November 2009.\n[8] A. Klapuri. Multiple fundamental frequency estimation by\nsumming harmonic amplitudes. In 7th Int. Conf. on Music\nInfo. Retrieval, Victoria, Canada, October 2006.\n[9] G. K. Koduri, J. Serr `a, and X. Serra. Characterization\nof intonation in carnatic music by parametrizing pitch his-\ntograms. In 13th Int. Soc. for Music Info. Retrieval Conf.,\nPorto, Portugal, Oct. 2012.\n[10] C. L. Krumhansl. Cognitive Foundations of Musical Pitch.\nOxford University Press, New York, 2001.\n[11] R. Quinlan. C4.5: Programs for Machine Learning . Mor-\ngan Kaufmann Publishers, San Mateo, CA, 1993.\n[12] T.V . Ranjani, H.G.; Arthi, S.; Sreenivas. Carnatic music\nanalysis: Shadja, swara identiﬁcation and rAga veriﬁcation\nin AlApana using stochastic models. Applications of Signal\nProcessing to Audio and Acoustics (WASPAA), IEEE Work-\nshop, pages 29–32, 2011.\n[13] J. C. Ross, T. P. Vinutha, and P. Rao. Detecting melodic\nmotifs from audio for Hindustani classical music. In 13th\nInt. Soc. for Music Info. Retrieval Conf., Porto, Portugal,\nOct. 2012.\n[14] J. Salamon and E. G ´omez. Melody extraction from poly-\nphonic music signals using pitch contour characteristics.\nIEEE Transactions on Audio, Speech, and Language Pro-\ncessing, 20(6):1759–1770, Aug. 2012.\n[15] J. Salamon, E. G ´omez, and J. Bonada. Sinusoid extraction\nand salience function design for predominant melody esti-\nmation. In Proc. 14th Int. Conf. on Digital Audio Effects\n(DAFX-11), pages 73–80, Paris, France, Sep. 2011.\n[16] R. Sengupta, N. Dey, D. Nag, A. Datta, and A. Mukerjee.\nAutomatic Tonic ( SA ) Detection Algorithm in Indian Clas-\nsical V ocal Music. In National Symposium on Acoustics,\npages 1–5, 2005.\n[17] X. Serra. A multicultural approach in music information\nresearch. In 12th Int. Soc. for Music Info. Retrieval Conf.,\nMiami, USA, Oct. 2011.\n[18] T. Viswanathan and M. H. Allen. Music in South India.\nOxford University Press, 2004.\n[19] I. H. Witten and E. Frank. Data mining: practical ma-\nchine learning tools and techniques. Morgan Kaufmann,\nWaltham, USA, 2nd edition, 2005."
    },
    {
        "title": "Statistical Characterisation of Melodic Pitch Contours and its Application for Melody Extraction.",
        "author": [
            "Justin Salamon",
            "Geoffroy Peeters",
            "Axel Röbel"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416462",
        "url": "https://doi.org/10.5281/zenodo.1416462",
        "ee": "https://zenodo.org/records/1416462/files/SalamonPR12.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416462,
        "dblp_key": "conf/ismir/SalamonPR12"
    },
    {
        "title": "Current Challenges in the Evaluation of Predominant Melody Extraction Algorithms.",
        "author": [
            "Justin Salamon",
            "Julián Urbano"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418041",
        "url": "https://doi.org/10.5281/zenodo.1418041",
        "ee": "https://zenodo.org/records/1418041/files/SalamonU12.pdf",
        "abstract": "In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict per- formance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time off- set between the annotation and the audio, can have a dra- matic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annota- tion and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction.",
        "zenodo_id": 1418041,
        "dblp_key": "conf/ismir/SalamonU12",
        "keywords": [
            "Audio Melody Extraction algorithms",
            "MIREX benchmark",
            "Evaluation and comparison",
            "Short audio clips",
            "Ground truth annotations",
            "Common protocol",
            "Realistic music material",
            "Reliable and meaningful evaluation",
            "New and larger collections",
            "Unreliable results"
        ],
        "content": "CURRENT CHALLENGES IN THE EV ALUATION OF\nPREDOMINANT MELODY EXTRACTION ALGORITHMS\nJustin Salamon\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\njustin.salamon@upf.eduJuli´an Urbano\nDepartment of Computer Science\nUniversity Carlos III of Madrid, Legan ´es, Spain\njurbano@inf.uc3m.es\nABSTRACT\nIn this paper we analyze the reliability of the evaluation\nof Audio Melody Extraction algorithms. We focus on the\nprocedures and collections currently used as part of the\nannual Music Information Retrieval Evaluation eXchange\n(MIREX), which has become the de-facto benchmark for\nevaluating and comparing melody extraction algorithms.\nWe study several factors: the duration of the audio clips,\ntime offsets in the ground truth annotations, and the size\nand musical content of the collection. The results show\nthat the clips currently used are too short to predict per-\nformance on full songs, highlighting the paramount need\nto use complete musical pieces. Concerning the ground\ntruth, we show how a minor error, speciﬁcally a time off-\nset between the annotation and the audio, can have a dra-\nmatic effect on the results, emphasizing the importance of\nestablishing a common protocol for ground truth annota-\ntion and system output. We also show that results based on\nthe small ADC04, MIREX05 and INDIAN08 collections\nare unreliable, while the MIREX09 collections are larger\nthan necessary. This evidences the need for new and larger\ncollections containing realistic music material, for reliable\nand meaningful evaluation of Audio Melody Extraction.\n1. INTRODUCTION\nThe task of melody extraction has received growing at-\ntention from the research community in recent years [4–\n7, 10–12]. Also referred to as Audio Melody Extraction,\nPredominant Melody Extraction, Predominant Melody Es-\ntimation or Predominant Fundamental Frequency (F0) Es-\ntimation, the task involves automatically obtaining a se-\nquence of frequency values representing the pitch of the\nmain melodic line from the audio signal of a polyphonic\npiece of music. As the number of researchers working on\nthe task grew, so did the need for proper means of eval-\nuating and comparing the performance of different algo-\nrithms. In 2004, the ﬁrst Audio Description Contest (ADC)\nwas hosted by the Music Technology Group at Universitat\nPompeu Fabra in Barcelona, Spain. This initiative later\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.evolved into the Music Information Retrieval Evaluation\neXchange (MIREX) [3], which is held annually in con-\njunction with the ISMIR conference.\nMIREX has become the de-facto benchmark for eval-\nuating and comparing the performance of melody extrac-\ntion algorithms, with over 50 algorithms evaluated since\nthe ﬁrst run in ADC 2004. Whilst this is without doubt\nan indication of the formalization of the topic as an estab-\nlished research area, it has recently been argued that some\nof the evaluation procedures employed by the Music Infor-\nmation Retrieval (MIR) research community still lack the\nrigor found in other disciplines such as Text IR [13]. In\nthis paper we examine the evaluation of melody extraction\nalgorithms, as currently carried out in the MIREX Audio\nMelody Extraction (AME) task. We focus on three as-\npects of the evaluation: ﬁrst, we examine the annotation\nprocedure used for generating a ground truth for evalua-\ntion. Speciﬁcally, we study the inﬂuence of a systematic\nerror in the annotations, in the form of a ﬁxed time off-\nset between the ground truth annotation and the output of\nthe algorithms. This issue is particularly relevant, as such\nan error has actually been detected in past MIREX AME\nevaluations. Next, we consider the duration of the audio\nexcerpts (clips) used for evaluation. Currently all collec-\ntions used for evaluation are comprised of short excerpts\ntaken from full songs. The underlying assumption is that\nperformance on a short clip is a good predictor for per-\nformance on a full song. However to date this assump-\ntion has neither been conﬁrmed nor confuted. Finally, we\nconsider the aspect of collection size. Currently, the size\nof most collections used for AME evaluation is relatively\nsmall compared to collections used in other IR tasks, and\nso we assess whether this presents any problems or not.\nThrough these factors, we aim to assess the reliability of\nthe evaluation procedure, as well as the meaningfulness of\nthe results and the conclusions that are drawn from them.\nThe remainder of the paper is as follows. In Section 2\nwe explain the current evaluation procedure for AME al-\ngorithms. Section 3 takes a closer look at the annotation\nprocedure, assessing the potential inﬂuence of a system-\natic error in the annotation process. In Section 4 we study\nthe relationship between system performance and clip du-\nration. In Section 5 we consider the inﬂuence of the size of\nthe music collection used for evaluation. Then, in Section\n6 we provide further insight into the results obtained in the\nprevious sections, and ﬁnally we present the conclusions\nin Section 7.2. MELODY EXTRACTION EV ALUATION\nWe start by describing the current procedure for evaluating\nmelody extraction algorithms, as carried out in the yearly\nMIREX AME evaluation.\n2.1 Ground Truth Annotation\nThe ground truth for each audio excerpt is generated using\nthe following procedure: ﬁrst, the annotator must acquire\nthe audio track containing just the melody of the excerpt.\nThis is done by using multitrack recordings for which the\nseparate tracks are available. Given the melody track, the\npitch of the melody is estimated using a monophonic pitch\ntracker with a graphical user interface such as SMSTools1\nor WaveSurfer2, producing an estimate of the fundamental\nfrequency (F0) of the melody in every frame. This anno-\ntation is then manually inspected and corrected in cases of\noctave errors (double or half frequency) or when pitch is\ndetected in frames where the melody is not present (un-\nvoiced frames). Finally, the estimated frequency sequence\nis saved into a ﬁle with two columns - the ﬁrst contain-\ning the time-stamp of every frame, starting from time 0,\nand the second the value of the fundamental frequency in\nHertz. In ADC 2004 a hop size of 5.8 ms was used for the\nannotation, and since 2005 a hop size of 10 ms between\nframes is used. Frames in which there is no melody present\nare labelled with 0 Hz.\n2.2 Evaluation Measures\nAn algorithm’s output for a single excerpt is evaluated by\ncomparing it to the ground truth annotation on a frame-by-\nframe basis, and computing ﬁve measures which summa-\nrize its performance for the complete excerpt. For a full\nmusic collection, these ﬁve measures are computed per\nexcerpt and then averaged over the entire collection. To\nfacilitate the evaluation, algorithms are required to pro-\nvide the output in the same format as the ground truth.\nThe only difference between the algorithm’s output and\nthe ground truth annotation is that for frames estimated as\nunvoiced (i.e. no melody present) by the algorithm, the al-\ngorithm may return either 0 Hz (as in the ground truth) or\na negative frequency value. The negative value represents\nthe algorithm’s pitch estimation in case its voicing estima-\ntion is wrong and the melody is actually present in that\nframe. This allows us to separate two different aspects in\nthe evaluation - the algorithm’s voicing estimation (deter-\nmining when the melody is present and when it is not) and\nthe algorithm’s pitch estimation (determining the F0 of the\nmelody). The ﬁve evaluation measures currently employed\nin MIREX, as deﬁned in [11], are summarized in Table 1.\n2.3 Music Collections\nOver the years, efforts by different researchers/groups have\nbeen made to generate annotated music collections for AME\nevaluation. The combination of the limited amount of multi-\ntrack recordings freely available, and the time-consuming\n1http://mtg.upf.edu/technologies/sms\n2http://www.speech.kth.se/wavesurfer/Voicing Recall Rate: the proportion of frames labeled as voiced in the\nground truth that are estimated as voiced by the algorithm.\nVoicing False Alarm Rate : the proportion of unvoiced frames in the\nground truth that are estimated as voiced by the algorithm.\nRaw Pitch Accuracy: the proportion of voiced frames in the ground\ntruth for which the F0 estimated by the algorithm is within \u00061\n4tone (50\ncents) of the ground truth annotation.\nRaw Chroma Accuracy: same as the raw pitch accuracy, except that\nboth the estimated and ground truth F0 sequences are mapped into a\nsingle octave, in this way ignoring octave errors in the estimation.\nOverall Accuracy: combines the performance of the pitch estimation\nand voicing detection to give an overall performance score. Deﬁned as\nthe proportion of frames (out of the entire excerpt) correctly estimated\nby the algorithm, i.e. unvoiced frames that are labeled as unvoiced and\nvoiced frames with a correct pitch estimate.\nTable 1. AME evaluation measures used in MIREX.\nCollection Description\nADC2004 20 excerpts of roughly 20s in the genres of pop, jazz and\nopera. Includes real recordings, synthesized singing and\naudio generated from MIDI ﬁles. Total play time: 369s.\nMIREX05 25 excerpts of 10-40s duration in the genres of rock,\nR&B, pop, jazz and solo classical piano. Includes real\nrecordings and audio generated from MIDI ﬁles. Total\nplay time: 686s.\nINDIAN08 Four 1 minute long excerpts from north Indian classical\nvocal performances. There are two mixes per excerpt with\ndiffering amounts of accompaniment resulting in a total\nof 8 audio clips. Total play time: 501s.\nMIREX09 374 Karaoke recordings of Chinese songs (i.e. recorded\nsinging with karaoke accompaniment). Each recording is\nmixed at three different levels of signal-to-accompaniment\nratiof-5dB, 0dB, +5dBg resulting in a total of 1,122 audio\nclips. Total play time: 10,022s.\nTable 2. Test collections for AME evaluation in MIREX.\nannotation process, means most of these collections are\nquite small compared to those used in other MIR disci-\nplines. In Table 2 we provide a summary of the music col-\nlections used in MIREX for AME evaluation since 2009.\n3. GROUND TRUTH ANNOTATION OFFSET\nIn this section we study the inﬂuence of a speciﬁc type of\nsystematic error in the annotation on the results. Whilst\nthere are other aspects of the annotation process that are\nalso worth consideration, we ﬁnd this issue to be of partic-\nular interest, since it was actually identiﬁed recently in one\nof the music collections used for Audio Melody Extraction\nevaluation in MIREX.\nAs explained in the previous section, all AME evalua-\ntion measures are based on a frame-by-frame comparison\nof the algorithm’s output to the ground truth annotation.\nHence, if there is a time offset between the algorithm’s\noutput and the ground truth annotation, this will cause a\nmismatch in all frames. Since melody pitch tends to be\ncontinuous, a very small time offset may not be noticed.\nHowever, as we increase the offset between the two se-\nquences, we expect it to have an increasingly detrimental\neffect on the results.\nTo evaluate the effect of such an offset, we compiled a\ncollection of 30 music clips from publicly available MIREX\ntraining sets: 10 from ADC 2004, 9 similar to MIREX05\nand 11 similar to MIREX09. We used the ground truth\nannotations generated by the original authors of each col-\nlection, and ensured that the ﬁrst frame of each annota-tion was centered on time 0. For evaluation, we use the\noutput of six different melody extraction algorithms that\nwere kindly provided by their authors: KD [4], DR3[5],\nFL [6], HJ [7], RP [9] and SG [12]. For each algorithm,\nwe computed the mean raw pitch and overall accuracy for\nthe entire collection, as a function of a ﬁxed time offset in-\ntroduced in the ground truth annotation, from -50 ms to 50\nms using 1 ms steps. To emulate offsets smaller than the\nhop size of the annotation (10 ms), the ground truth was\nupsampled using linear interpolation.\n3.1 Results\nIn Figure 1 we display the results of the evaluation, where\nwe have subtracted from all values the score at offset 0. In\nthis way, the graph reﬂects the absolute difference between\nthe score at a given offset and the optimal score of the algo-\nrithm (assuming it is centered on time 0). Plot (a) contains\nthe results for the raw pitch measure, and plot (b) for the\noverall accuracy.\n−50 0 50−30−25−20−15−10−505\nOffset (ms)(a)Raw Pitch Accuracy\n−50 0 50−30−25−20−15−10−505\nOffset (ms)(b)Overall Accuracy\n  SG\nKD\nDR\nHJ\nFL\nRP\nFigure 1. Absolute performance drop versus annotation\noffset: (a) raw pitch accuracy, (b) overall accuracy.\nAs can be seen, the effect of the offset is quite dramatic,\ncausing an absolute drop of up to 25% in the raw pitch\naccuracy and 20% in the overall accuracy for the most ex-\ntreme offset evaluated (50 ms). Though a 50 ms offset is\nperhaps an exaggerated case, in 2011 it was discovered that\none of the MIREX collections had a 20ms offset. In our\nevaluation, a 20 ms offset would cause the most affected\nalgorithms to loose 17% in raw pitch accuracy, and 13%\nin overall accuracy. Another interesting observation is that\nsome algorithms do not perform best at offset 0 (most visi-\nbly RP, whose peak performance is at -6 ms). This empha-\nsizes the fact that it does not sufﬁce for the annotation to\nbe centered on time 0, but rather, that there must be a strict\nconvention to which both the annotations and algorithms\nadhere. Finally, we found there is a correlation between\nabsolute performance and the effect of annotation offset:\nthe higher the absolute performance of the algorithm, the\nmore sensitive it is to an offset in the annotation. This is\n3The output was computed using a different implementation than that\nof the paper, available at: https://github.com/wslihgt/separateLeadStereoparticularly important, since it suggests that the best algo-\nrithms are those who will be most affected by this type of\nsystematic error.\n4. CLIP DURATION\nA common criticism of evaluation in MIR, and particu-\nlarly in MIREX, is the use of clips instead of full songs.\nOne might argue that the use of clips is unrealistic and that\nobserved performance on those clips may be very different\nfrom performance on full songs [13]. The collections used\nin the AME evaluation contain some very short excerpts,\nsome only 10 seconds long. The use of such small clips is\nespecially striking in AME: these clips contain primarily\nvoiced frames, and so the generalization of the results to\nfull songs should be questioned. We designed an experi-\nment to assess the effect of clip duration on the reliability\nof the AME evaluations.\nFor each of the 30 clips used in the previous experiment\n(referred to as the x1clips), we created a series of subclips:\n2 subclips of half the duration, 3 subclips of one third of\nthe duration, and 4 subclips of one forth of the duration\n(referred to as the x1/2, x1/3 andx1/4 subclips). Note that\nthex1/4 subclips can also be considered as x1/2 versions of\nthex1/2 subclips. This gives us 180 x1/2 subclips, 90 x1/3\nsubclips and 120 x1/4 subclips, all of which were used to\nevaluate the six algorithms. We computed the performance\ndifference between all subclips and their corresponding x1\nversions, leading to a grand total of 2340 data-points.\n4.1 Results\nIn Figure 2 we show the log-scaled distribution of relative\nperformance differences. Mean differences vary between\n13% and 21% for overall accuracy and raw pitch, while\nfor voicing false-alarm the means are around 50%. We\nnote that there is a large amount of outliers in the distri-\nbutions. However, these outliers were not found to corre-\nspond to particular songs or algorithms (they are rather ran-\ndomly distributed). There seems to be a clear correlation:\nthe shorter the subclips, the larger the performance differ-\nences (all signiﬁcant by a 1-tailed Wilcoxon test, \u000b=0.01).\nIn principle, therefore, one would want the clips used for\nevaluation to be as long as possible; ideally, the full songs.\nIn Figure 3 we plot the log-scaled relative performance\ndifferences in overall accuracy, this time as a function of\nthe log-scaled actual subclip duration (other measures pro-\nduce very similar plots). We see that the negative correla-\ntion between subclip duration and performance difference\nappears to be independent of the duration of the x1clip. We\nﬁtted a non-linear model of the form diff=a\u0001durationb,\nwhereaandbare the parameters to ﬁt, to the results of\neach of the relative durations (x1/2, x1/3, x1/4), and as the\nplot shows, they are very similar. In fact, an ANCOV A\nanalysis revealed no signiﬁcant difference between them.\nThis suggests that the error decreases as the clip duration\nincreases, regardless of the duration of the full song.●●●●●●\n●●●●●●●\n●●●●●●●\n●●●●●●●●●\n●\n●●●●\n●●\n●●\n●●●\n●●●●●●\n●●●●●\n●●\n●\n●●●●●●●●●\n●● ●\n●●●●\n●●●\n●●\n●●●●●●●●●●\n●●\n●●●●\n●●\n●●●●●●\n●●\n●●●\n●●\n●●●●●\n●●●●\n● ●●●●\n●●●●●\n●●●●●\n●●●●●●●●\n●●●●\n●●●●\n●●\n●●●●●\n●●●●●\n●●●\n●\n●●●●\n●●●●\n●●●●●\n●●●\n●●●\n●●●\n●●●●\n●●●●●●\n●●\n●●●●●●●\n●●●\n●●\n●●●●●●●\n●●●●●●\n●●●●●●●●●\n●● ●\n●●\n●●\n●●●●\n●●●●\n●●\n●●\n●●●●\n●\n●●●●●●●\n●●●●●\n●\n●●●\n●●\n●● ● ●\n●●●●●● ●\n●●●●●\n●●\n●●●●●●\n●●\n●●●●\n●●\n●●●●●●\n●●● ●\n●\n●●●\n●●\n●●\n●●●\n●●●●●●●●\n●●●●●●●●●●\n●●●●●\n●\n●●\n●\n●●●●●●\n●●●\n●●●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●●\n●●●●●\n●●●●● ●\n●●●\n●●●●●\n●●●●\n●●●●●●\n●●●●\n●●●●●●●●●●\n●●●●●●●\n●\n●●●\n●●●●\n●●●●●●●●\n●●\n●●●\n●●\n●● ●\n●●●●●\n●●●● ● ●\n●●●●\n●●●\n●●\n●●●\n●●●●\n●●●●●●●\n●●\n●●●●●●\n●●● ●\n●\n●●\n●●\n●●●●\n●\n●●●\n●●●\n●●●●\n●●●●●●●●\n●●\n●●●●\n●\n●●\n●\n●●●\n●\n●●●●●●●\n●●\n●●\n●●●●●\n●\n●●\n●●\n●\n●●●● ●●\n●●\n●●\n●●\n●●●\n●●●\n●●●\n●●●\n●●●●\n●●●\n●● ●●\n●\n●●\n●●\n●●●●●●●\n●\n●●\n●●●●\n●●●\n●\n●Overall                   Raw                     Voicing \nAccuracy                 Pitch                False−Alarm\nSubclip relative duration% of performance difference\n1 10 100 1000 1000\n1/4 1/3 1/2 1/4 1/3 1/2 1/4 1/3 1/2Figure 2. Releative performance differences between sub-\nclips and their corresponding x1clips. Blue crosses mark\nthe means of the distributions.\nOverall Accuracy (r = −0.317)\nSubclip absolute duration in seconds% of performance difference\n0.05 0.5 5 50 500\n2 3 5 7 10 14 17l\nll1/2\n1/31/4\nFigure 3. Relative performance differences with subclips\nas a function of subclip actual duration.\n5. COLLECTION SIZE\nRegardless of the effectiveness measure used, an AME ex-\nperiment consists of evaluating a set of algorithms Ausing\na set of songsS. Such an evaluation experiment can be\nviewed as ﬁtting the following model:\nyas=y+ya+ys+\"as (1)\nwhereyasis the score of algorithm afor songs,yis the\ngrand average score of all possible algorithms over all pos-\nsible songs, yais the algorithm effect (the average devi-\nation of algorithm afrom the grand average y),ysis the\nsong effect and \"asis a residual modeling the particular\ndeviation of algorithm afor songs. In our case, where we\ndo not consider other effects such as annotators, this \"as\nresidual actually models the algorithm-song interaction ef-\nfect: some algorithms are particularly better (or worse) for\nparticular songs.\nWhen a researcher carries out an AME evaluation ex-\nperiment, they evaluate how well an algorithm performs\nfor the setSof songs, but ideally they want to general-\nize from the performance of that speciﬁc experiment to\nthe average score the algorithm would obtain for the pop-\nulation of all songs represented by the sample S, not just\nthe sample itself. The reliability when drawing such gen-\neral conclusions based on the observations on samples (test\ncollections) can be measured with Generalizability Theory\n(GT) [1, 2].From the model in Eq. 1 we can identify two sources of\nvariability in the observed scores: actual performance dif-\nferences among algorithms and difﬁculty differences among\nsongs. Ideally, we want most of the variability in yasto be\ndue to the algorithm effect, that is, the observed effective-\nness differences to be due to actual differences between al-\ngorithms and not due to other sources of variability such as\nsongs, annotators, or speciﬁc algorithm-song interactions.\nNote that this does not mean a collection should not con-\ntain varied musical content. Ideally, we want an algorithm\nto work well for all types of musical material, and hence\na varied collection in terms of content does not necessar-\nily imply large performance variability due to the song ef-\nfect. However, a small collection that contains songs with\na great degree of variability (in terms of difﬁculty) is likely\nto result in performance variability that is dominated by\nthe song effect and possibly by algorithm-song interactions\n(e.g. algorithm X is especially good for jazz but poor for\nrock), thus reducing our ability to claim that the observed\ndifferences between the algorithms can be generalized to\nthe universe of all songs. Using GT [1, 2], we can mea-\nsure the proportion of observed variability that is due to\nactual differences between the algorithms. This proportion\nreﬂects the stability of the evaluation, and as such it is also\na measure of efﬁciency: the higher the stability, the fewer\nthe songs necessary to reliably evaluate algorithms [1, 8].\nGT does not only help evaluate the stability of past collec-\ntions, but also estimate the reliability of yet-to-be created\ncollections as a function of their size. However, the results\nof GT only hold if the original data used for the analysis\nisrepresentative of the wider population of songs to which\nwe want to generalize in the future.\n5.1 Variance Analysis and Collection Stability\nIn the model in Eq. 1, the grand mean yis a constant, and\nthe other effects can be modeled as random variables with\ntheir own expectation and variance. As such, the variance\nof the observed scores is modeled as the sum of these vari-\nance components:\n\u001b2=\u001b2\na+\u001b2\ns+\u001b2\nas (2)\nwhere\u001b2\nais the variance due to the algorithm effect, \u001b2\nsis\nthe variance due to the song effect, and \u001b2\nasis the variance\ndue to the algorithm-song interaction effect (the residual).\nThis variance decomposition can be estimated by ﬁtting a\nfully-crossed ANOV A model for Eq. 1:\nb\u001b2\nas= EMSas= EMSresidual\nb\u001b2\na=EMSa\u0000b\u001b2\nas\njSj;b\u001b2\ns=EMSs\u0000b\u001b2\nas\njAj(3)\nwhere EMSxis the expected Mean Square of component\nx. In practice, EMSxis approximated by the Mean Square\nof component xas computed with the ANOV A model [1,\n2]. Using the estimates in Eq. 3 we can estimate the pro-\nportion of variability due to the algorithm effect as per\nEq. 2. The stability of the evaluation can then be quan-\ntiﬁed with the dependability index \b:Overall Accuracy Raw Pitch V oicing False-Alarm\nb\u001b2\na b\u001b2\ns b\u001b2\nasb\b b\u001b2\na b\u001b2\ns b\u001b2\nasb\b b\u001b2\na b\u001b2\ns b\u001b2\nasb\b\nADC04 27% 27% 46% .879 23% 28% 49% .859 55% 21% 23% .961\nMIREX05 11% 47% 42% .758 15% 54% 31% .817 57% 20% 23% .971\nINDIAN08 16% 50% 34% .600 24% 57% 19% .721 70% 13% 16% .950\n04 + 05 + 08 16% 39% 45% .909 16% 43% 41% .912 56% 21% 23% .986\nMIREX09 0dB 52% 20% 28% .998 50% 20% 31% .997 81% 5% 14% .999\nMIREX09 -5dB 40% 23% 37% .996 40% 24% 35% .996 82% 5% 13% .999\nMIREX09 +5dB 58% 17% 26% .998 48% 18% 34% .997 83% 4% 14% .999\nTable 3. Variance components and b\bscore for all three measures and all six collections plus the joint 04+05+08 collection.\nOverall Accuracy\nNumber of songsΦ (score stability)\n1 2 5 10 20 50 100 3740.6 0.7 0.8 0.9 1.0l\nl\nlll\nADC04\nMIREX05INDIAN0804+05+08MIREX09 0dBMIREX09 −5dBMIREX09 +5dB\nRaw Pitch\nNumber of songsΦ (score stability)\n1 2 5 10 20 50 100 3740.6 0.7 0.8 0.9 1.0l\nl\nlll\nADC04\nMIREX05INDIAN0804+05+08MIREX09 0dBMIREX09 −5dBMIREX09 +5dB\nVoicing False−Alarm\nNumber of songsΦ (score stability)\n1 2 5 10 20 50 100 3740.6 0.7 0.8 0.9 1.0ll\nlll\nADC04\nMIREX05INDIAN0804+05+08MIREX09 0dBMIREX09 −5dBMIREX09 +5dB\nFigure 4. Dependability index as a function of the number of songs for Overall Accuracy (left), Raw Pitch (middle) and\nV oicing False-Alarm (right). The points mark the actual number of songs per collection.\n\b =\u001b2\na\n\u001b2a+\u001b2\ns+\u001b2\nas\njSj(4)\nwhich measures the ratio between algorithm variance and\nthe variance in absolute effectiveness scores (total vari-\nance) [1, 2]. This measure increases with the song set size\n(i.e. with an inﬁnite number of songs all the observed vari-\nability would be due to algorithm differences) [8].\n5.2 Results\nIn Table 3 we show the estimated proportion of variability\ndue to the algorithm, song and algorithm-song interaction\neffects. For these calculations we used the results of the\nMIREX campaign directly, combining the results of the\nﬁve algorithms from MIREX 2010 and ten algorithms from\nMIREX 2011. In both years the same six test-collections\nwere used for evaluation, so we can consider the grouping\nof algorithms from both years as a single larger evaluation\nround leading to a fully crossed experimental design. We\nalso joined the three smaller collections into a single larger\none referred to as “04+05+08”, discussed in Section 6.\nIn general, it can be seen that the estimated variance due\nto the algorithm effect is much larger in the MIREX09 col-\nlections. For overall accuracy, the average is 50%, while\nfor the earlier collections it is just 18%, and as low as\n11% for MIREX05. These differences show that gener-\nalizations of results based on the earlier collections are not\nvery reliable, especially in the case of the MIREX05 and\nINDIAN08 collections, because a large part of the vari-\nability in the scores is due to the song characteristics rather\nthan differences between the algorithms.\nFigure 4 shows the estimated dependability index as a\nfunction of the number of songs used (log scaled). The\npoints mark the value of b\bfor the actual number of songs\nin each collection (cf. Table 3). Again we observe that theMIREX09 collections are considerably more stable than\nthe earlier collections, especially MIREX05 and INDIAN08,\nwhere b\bis as low as 0.6. More interesting is the fact\nthat the dependability index in the MIREX09 collections\nrapidly converges to 1, and there is virtually no apprecia-\nble difference between using all 374 songs in the collection\nor just 100: b\bwould only drop from an average of 0.997 to\n0.990, showing that most of the variability in performance\nscores would still be attributable to the algorithm effect.\nHowever, we must also consider the content validity of this\ncollection (i.e. whether it is representative or not) [13]. We\ndiscuss this in the next section.\n6. DISCUSSION\nStarting with the annotation offset issue, we note that there\nare two crucial parameters that must be ﬁxed in order to\nprevent this problem: the precise time of the ﬁrst frame,\nand the hop size. Since 2005, all the annotations use a\nhop size of 10 ms, and all algorithms are required to use\nthis hop size for their output. However, the exact time of\nthe ﬁrst frame has not been explicitly agreed upon by the\ncommunity. When the short-time Fourier transform (or any\nother transform which segments the audio signal into short\nframes) is used, it is common practice to consider the time-\nstamp of each frame to be the time exactly at the middle of\nthe frame. Thus, for the ﬁrst frame to start exactly at time\nzero, it must be centered on the ﬁrst sample of the audio\n(ﬁlling the ﬁrst half of the frame with zeros). Nonetheless,\nwhile this is common practice, it is not strictly imposed,\nmeaning algorithms and annotators might, rather than cen-\nter the ﬁrst frame on the ﬁrst sample, start the frame at this\nsample. In this case, the frame will not be centered on time\nzero, but rather on an arbitrary time which depends on the\nlength of the frame. Since different algorithms and annota-\ntions use different frame sizes, this scenario could lead to\na different ﬁxed offset between every algorithm and everyannotation, leading to a systematic error in the evaluation.\nIn terms of clip duration, we saw that there is a clear\ncorrelation between the relative duration of the clip (com-\npared to the full song) and evaluation error, suggesting that\nperformance based on clips might not really predict per-\nformance on full songs. However, Figure 3 suggests that\nthis correlation is independent of the actual duration of the\nfull song. That is, there might be a duration threshold of x\nseconds for which observed performance on clips does pre-\ndict performance on full songs (within some error rate), no\nmatter how long they are. While counter-intuitive at ﬁrst,\nthis result does somehow agree with general statistical the-\nory. How large a sample needs to be in order to reliably\nestimate unknown parameters of the underlying popula-\ntion, is independent of how large the population actually\nis, as long as the sample is representative of the popula-\ntion. This usually requires to sample randomly or follow\nother techniques such as systematic or stratiﬁed sampling.\nFor AME evaluation it does not make sense to randomly\nsample frames of a song, but the results suggest that there\nmight be a sampling technique such that audio clips, if se-\nlected appropriately, can be representative of the full songs.\nRegarding the collection size, we observed that the ear-\nlier ADC04, MIREX05 and INDIAN08 collections are un-\nstable because a larger proportion of the variability in the\nobserved performance scores is due to song difﬁculty dif-\nferences rather than algorithm differences. As such, re-\nsults from these collections alone are expected to be un-\nstable, and therefore evaluations that rely solely on one\nof these collections are not very reliable. In Table 3 (and\nFigure 4) we see that by joining these collections into a\nsingle larger one (“04+05+08”) the evaluation results are\nconsiderably more stable ( b\b>0:9for all three measures),\nand so we recommend fusing them into a single collec-\ntion for future evaluations. On the other hand, we saw\nthat the MIREX09 collections are in fact much larger than\nnecessary: about 25% of the current songs would sufﬁce\nfor results to be highly stable and therefore generalize to\na wider population of songs. However, all MIREX09 mu-\nsic material consists of Chinese karaoke songs with non-\nprofessional singers, and therefore we should expect the re-\nsults to generalize to thispopulation of songs, but not to the\ngeneral universe of allsongs (essentially everything that is\nnot karaoke). Therefore, the AME community is found in\nthe situation where the collections with sufﬁciently varied\nmusic material are too small to be reliable, while the ones\nthat are reliable contain very biased music material.\n7. CONCLUSION\nIn this paper we analyzed the reliability of the evaluation\nof Audio Melody Extraction algorithms, as performed in\nMIREX. Three main factors were studied: ground truth\nannotations, clip duration and collection size. We demon-\nstrated how an offset between the ground truth and an al-\ngorithm’s output can signiﬁcantly degrade the results, the\nsolution to which is the deﬁnition and adherence to a strict\nprotocol for annotation. Next, it was shown that the clips\ncurrently used are too short to predict performance on fullsongs, stressing the need to use complete musical pieces.\nIt was also shown that results based on one of the ADC04,\nMIREX05 or INDIAN08 collections alone are not reliable\ndue to their small size, while the MIREX09 collection,\nthough more reliable, does not reﬂect real-world musical\ncontent. The above demonstrates that whilst the MIREX\nAME evaluation task is an important initiative, it currently\nsuffers from problems which require urgent attention. As\na solution, we propose the creation of a new and open test\ncollection through a joint effort of the research community.\nIf the collection is carefully compiled and annotated, keep-\ning in mind the issues mentioned here, it should, in theory,\nsolve all of the aforementioned problems that current AME\nevaluation suffers from. Furthermore, we could consider\nthe application of low-cost evaluation methodologies that\ndramatically reduce the annotation effort required [14]. Fi-\nnally, in the future it would also be worth studying the ap-\npropriateness of the evaluation measures themselves, the\naccuracy of the manual ground truth annotations and fur-\nther investigate the effect of clip duration.\n8. ACKNOWLEDGMENTS\nWe would like to thank the authors of the melody extrac-\ntion algorithms for their contribution to our experiments.\nThis work was supported by the Programa de Formaci ´on\ndel Profesorado Universitario (FPU) and grant TIN2010-\n22145-C02-02 of the Spanish Government.\n9. REFERENCES\n[1] D. Bodoff. Test theory for evaluating reliability of IR test col-\nlections. Inf. Process. Manage., 44(3), 2008.\n[2] R. L. Brennan. Generalizability Theory. Springer, 2001.\n[3] J. Downie. The music information retrieval evaluation ex-\nchange (2005–2007): A window into music information re-\ntrieval research. Acoustical Science and Technology, 2008.\n[4] K. Dressler. Audio melody extraction for mirex 2009. In Mu-\nsic Inform. Retrieval Evaluation eXchange (MIREX), 2009.\n[5] J.-L. Durrieu, G. Richard, B. David, and C. F ´evotte.\nSource/ﬁlter model for unsupervised main melody extraction\nfrom polyphonic audio signals. IEEE TASLP, 18(3), 2010.\n[6] B. Fuentes, A. Liutkus, R. Badeau, and G. Richard. Proba-\nbilistic model for main melody extraction using constant-Q\ntransform. In IEEE ICASSP, 2012.\n[7] C. Hsu, D. Wang, and J. Jang. A trend estimation algorithm\nfor singing pitch detection in musical recordings. In IEEE\nICASSP, 2011.\n[8] E. Kanoulas and J. Aslam. Empirical justiﬁcation of the gain\nand discount function for nDCG. In ACM CIKM, 2009.\n[9] R. P. Paiva. Melody Detection in Polyphonic Audio. PhD the-\nsis, University of Coimbra, Portugal, 2007.\n[10] R. P. Paiva, T. Mendes, and A. Cardoso. Melody detection in\npolyphonic musical signals: Exploiting perceptual rules, note\nsalience, and melodic smoothness. Comput. Music J., 2006.\n[11] G. E. Poliner, D. P. W. Ellis, F. Ehmann, E. G ´omez, S. Ste-\nich, and B. Ong. Melody transcription from music audio: Ap-\nproaches and evaluation. IEEE TASLP, 15(4), 2007.\n[12] J. Salamon and E. G ´omez. Melody extraction from poly-\nphonic music signals using pitch contour characteristics.\nIEEE TASLP, 20(6), 2012.\n[13] J. Urbano. Information retrieval meta-evaluation: Challenges\nand opportunities in the music domain. In ISMIR, 2011.\n[14] J. Urbano and M. Schedl. Towards Minimal Test Collections\nfor Evaluation of Audio Music Similarity and Retrieval. In\nWWW Workshop on Advances in MIR, 2012."
    },
    {
        "title": "Putting the User in the Center of Music Information Retrieval.",
        "author": [
            "Markus Schedl",
            "Arthur Flexer"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417941",
        "url": "https://doi.org/10.5281/zenodo.1417941",
        "ee": "https://zenodo.org/records/1417941/files/SchedlF12.pdf",
        "abstract": "Personalized and context-aware music retrieval and recom- mendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Al- though first steps towards such systems have recently been presented at ISMIR and similar venues, this vision is still far away from being a reality. In this paper, we investi- gate and discuss literature on the topic of user-centric mu- sic retrieval and reflect on why the breakthrough in this field has not been achieved yet. Given the different exper- tises of the authors, we shed light on why this topic is a particularly challenging one, taking a psychological and a computer science view. Whereas the psychological point of view is mainly concerned with proper experimental de- sign, the computer science aspect centers on modeling and machine learning problems. We further present our ideas on aspects vital to consider when elaborating user-aware music retrieval systems, and we also describe promising evaluation methodologies, since accurately evaluating per- sonalized systems is a notably challenging task.",
        "zenodo_id": 1417941,
        "dblp_key": "conf/ismir/SchedlF12",
        "keywords": [
            "Personalized",
            "context-aware",
            "music retrieval",
            "recommendation",
            "algorithms",
            "vision",
            "reality",
            "user-centric",
            "psychological",
            "computer science"
        ],
        "content": "PUTTING THE USER IN THE CENTER\nOF MUSIC INFORMATION RETRIEVAL\nMarkus Schedl\nDepartment of Computational Perception,\nJohannes Kepler University, Linz, AustriaArthur Flexer\nAustrian Research Institute for\nArtiﬁcial Intelligence, Vienna, Austria\nABSTRACT\nPersonalized and context-aware music retrieval and recom-\nmendation algorithms ideally provide music that perfectly\nﬁts the individual listener in each imaginable situation and\nfor each of her information or entertainment need. Al-\nthough ﬁrst steps towards such systems have recently been\npresented at ISMIR and similar venues, this vision is still\nfar away from being a reality. In this paper, we investi-\ngate and discuss literature on the topic of user-centric mu-\nsic retrieval and reﬂect on why the breakthrough in this\nﬁeld has not been achieved yet. Given the different exper-\ntises of the authors, we shed light on why this topic is a\nparticularly challenging one, taking a psychological and a\ncomputer science view. Whereas the psychological point\nof view is mainly concerned with proper experimental de-\nsign, the computer science aspect centers on modeling and\nmachine learning problems. We further present our ideas\non aspects vital to consider when elaborating user-aware\nmusic retrieval systems, and we also describe promising\nevaluation methodologies, since accurately evaluating per-\nsonalized systems is a notably challenging task.\n1. WHY CARE ABOUT THE USER?\nIn our discussion of the importance and the challenges of\ndevelopment and evaluation in Music Information Retrieval\n(MIR) we distinguish between systems-based and user-cen-\ntric MIR. We deﬁne systems-based MIR as all research\nconcerned with experiments existing solely in a computer,\ne.g. evaluation of algorithms on digital databases. In con-\ntrast, user-centered MIR always involves human subjects\nand their interaction with MIR systems.\nSystems-based MIR has traditionally focused on com-\nputational models to describe universal aspects of human\nmusic perception, for instance, via elaborating musical fea-\nture extractors or similarity measures. Doing so, the ex-\nistence of an objective “ground truth” is assumed, against\nwhich corresponding music retrieval algorithms (e.g., play-\nlist generators or music recommendation systems) are eval-\nuated. To give a common example, music retrieval ap-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.proaches have been evaluated via genre classiﬁcation ex-\nperiments for years. Although it was shown already in\n2003 that musical genre is an ill-deﬁned concept [1], genre\ninformation still serves as a proxy to assess music similar-\nity and retrieval approaches in systems-based MIR.\nOn the way towards user-centered MIR, the coarse and\nambiguous concept of genre should either be treated in a\npersonalized way or replaced by the concept of similar-\nity. When humans are asked to judge the similarity be-\ntween two pieces of music, however, certain other chal-\nlenges need to be faced. Common evaluation strategies\ntypically do not take into account the musical expertise and\ntaste of the users. A clear deﬁnition of “similarity” is of-\nten missing too. It might hence easily occur that two users\napply a very different, individual notion of similarity when\nassessing the output of music retrieval systems. While a\nﬁrst person may experience two songs as rather dissimilar\ndue to very different lyrics, a second one may feel a much\nhigher resemblance of the very same songs because of a\nsimilar instrumentation. Similarly, a fan of Heavy Metal\nmusic might perceive a Viking Metal track as dissimilar to\na Death Metal piece, while for the majority of people the\ntwo will sound alike.\nThe above examples illustrate that there are many as-\npects that inﬂuence what a human perceives as similar in a\nmusical context. These aspects can be grouped into three\ndifferent categories according to [29]: music content, mu-\nsic context, and user context. Examples for each category\nare given in Figure 1. It is exactly this multifaceted and\nindividual way of music perception that has largely been\nneglected so far when elaborating and evaluating music re-\ntrieval approaches, but should be given more attention, in\nparticular considering the trend towards personalized and\ncontext-aware systems.\nApersonalized system is one that incorporates infor-\nmation about the user into its data processing part (e.g.,\na particular user taste for a movie genre). A context-aware\nsystem, in contrast, takes into account dynamic aspects of\nthe user context when processing the data (e.g., location\nand time where/when a user issues a query). Although\nthe border between personalization and context-awareness\nmay appear fuzzy from this deﬁnition, in summary, person-\nalization usually refers to the incorporation of more static,\ngeneral user preferences, whereas context-awareness refers\nto the fact that frequently changing aspects of the user’s en-\nvironmental, psychological, and physiological context are\nconsidered.music \ncontent\nExamples:\n-r h y t h m\n-t i m b r e- melody- harmony- loudnessmusic \ncontext\nuser \ncontextExamples:\n- semantic labels- song lyrics- album cover artwork- artist's background- music video clips\nExamples:\n- mood- activities- social context- spatio-temporal context- physiological aspects\nmusic \nperception\nFigure 1. Factors that inﬂuence human music perception.\nThe remainder of this paper is organized as follows.\nSection 2 reviews approaches that, in one way or the other,\ntake the user into account when building music retrieval\nsystems. Evaluation strategies for investigating user-centric\nMIR are discussed in Section 3. In Section 4, we eventu-\nally summarize important factors when creating and eval-\nuating user-aware music retrieval systems.\n2. HOW TO MODEL THE USER?\nExisting user-aware systems typically model the user in a\nvery simplistic way. For instance, it is common in collab-\norative ﬁltering approaches [22, 28] to build user proﬁles\nonly from information about a user uexpressing an inter-\nest in item i. As an indicator of interest may serve, for\nexample, a click on a particular item, a purchasing transac-\ntion, or in MIR the act of listening to a certain music piece.\nSuch indications, in their simplest form, are stored in a bi-\nnary matrix where element r(u; i )denotes the presence or\nabsence of a connection between user uand item i. In com-\nmon recommendation systems, a more ﬁne-grained scale\nfor modeling the user interest in an item is typically em-\nployed – users frequently rate items according to a Likert-\ntype scale, e.g., by assigning one to ﬁve stars to it. Matrix\nfactorization techniques are subsequently applied to rec-\nommend novel items [19].\nTaking a closer look at literature about context-awareretrieval and recommendation in the music domain, we can\nsee that approaches differ considerably in terms of how the\nuser context is deﬁned, gathered, and incorporated. The\nmajority of approaches rely solely on one or a few aspects\n(temporal features in [7], listening history and weather con-\nditions in [21], for instance), whereas comprehensive user\nmodels are rare in MIR. One of the few exceptions is Cun-\nningham et al.’s study [8] that investigates if and how var-\nious factors relate to music taste (e.g., human movement,\nemotional status, and external factors such as temperature\nand lightning conditions). Based on the ﬁndings, the au-\nthors present a fuzzy logic model to create playlists.\nThere further exists some work that assumes a mobile\nmusic consumption scenario. The corresponding systems\nfrequently aim at matching music with the current pace of\na walker or jogger, e.g., [3, 24]. Such systems typically try\nto match the user’s heartbeat with the music played [23].\nHowever, almost all proposed systems require additional\nhardware for context logging, e.g., [8, 9, 11].\nIn [15] a system that matches tags describing a partic-\nular place with tags describing music is presented. Em-\nploying text-based similarity measures between the multi-\nmodal sets of tags, Kaminskas and Ricci propose their sys-\ntem for location-based music recommendation. Baltrunas\net al. [2] suggest a context-aware music recommender sys-\ntem for music consumption while driving. Although the\nauthors take into account eight different contextual factors(e.g., driving style, mood, road type, weather, trafﬁc con-\nditions), their application scenario is quite restricted and\ntheir system relies on explicit human feedback, which is\nburdensome.\nZhang et al. present CompositeMap [34], a model that\ntakes into account similarity aspects derived from music\ncontent as well as social factors. The authors propose a\nmultimodal music similarity measure and show its appli-\ncability to the task of music retrieval. They also allow a\nsimple kind of personalization of this model by letting the\nuser weight the individual music dimensions on which sim-\nilarity is estimated. However, they do neither take the user\ncontext into consideration, nor do they try to learn a user’s\npreferences.\nIn [26] Pohle et al. present preliminary steps towards\na simple personalized music retrieval system. Based on a\nclustering of community-based tags extracted from last.fm,\na small number of musical concepts are derived using Non-\nNegative Matrix Factorization (NMF) [20,32]. Each music\nartist or band is then described by a “concept vector”. A\nuser interface allows for adjusting the weights of the indi-\nvidual concepts, based on which artists that match the re-\nsulting distribution of the concepts best are recommended\nto the user. Zhang et al. propose in [34] a very similar kind\nof personalization strategy via user-adjusted weights.\nKnees and Widmer present in [17] an approach that in-\ncorporates relevance feedback [27] into a text-based music\nsearch engine [16] to adapt the retrieval process to user\npreferences. The search engine proposed by Knees et al.\nbuilds a model from music content features (MFCCs) and\nmusic context features (term vector representations of artist-\nrelated Web pages). To this end, a weight is computed for\neach (term, music item)-pair, based on the term vectors.\nThese weights are then smoothed, taking into account the\nclosest neighbors according to the content-based similarity\nmeasure (Kullback-Leibler divergence on Gaussian Mix-\nture Models of the MFCCs). To retrieve music via natu-\nral language queries, each textual query issued to the sys-\ntem is expanded via a Google search, resulting again in\na term weight vector. This query vector is subsequently\ncompared to the smoothed weight vectors describing the\nmusic pieces, and those with smallest distance to the query\nvector are returned.\nN¨urnberger and Detyniecki present in [25] a variant of\ntheSelf-Organizing Map (SOM) [18] that is based on a\nmodel that adapts to user feedback. To this end, the user\ncan move data items on the SOM. This information is fed\nback into the SOM’s codebook, and the mapping is adapted\naccordingly.\nIn [33] Xue et al. present a collaborative personalized\nsearch model that alleviates the problems of data sparse-\nness andcold-start for new users by combining informa-\ntion on different levels (individuals, interest groups, and\nglobal). Although not explicitly targeted at music retrieval,\nthe idea of integrating data about the user, his peer group,\nand global data to build a social retrieval model might be\nworth considering for MIR purposes.\nThe problem with the vast majority of approaches pre-sented so far is that evaluation is still carried out without\nsufﬁcient user involvement. For instance, [7, 25, 26] seem-\ningly do not perform any kind of evaluation involving real\nusers, or at least do not report it. Some approaches are\nevaluated on user-generated data, but do not request feed-\nback from real users during the evaluation experiments.\nFor example, [16] makes use of collaborative tags stored\nin a database to evaluate the proposed music search en-\ngine. Similarly, [21] relies on data sets of listening his-\ntories and weather conditions, and [33] uses a corpus of\nWeb search data. Even if real users are questioned dur-\ning evaluation, their individual properties (such as taste,\nexpertise, or familiarity with the music items under inves-\ntigation) are regularly neglected in evaluation experiments.\nIn these cases, evaluation is typically performed to answer\na very narrow question in a restricted setting. To give an\nexample, the work on automatically selecting music while\ndoing sports, e.g. [3,23,24], is evaluated on the very ques-\ntion of whether pace or heartbeat of the user does synchro-\nnize with the tempo of the music. Likewise Kaminskas\nand Ricci’s work on matching music with places of inter-\nest [15], even though it is evaluated by involving real users,\ncomprises only the single question whether the music sug-\ngested by their algorithm is suited for particular places of\ninterest. Different dimensions of the relation between im-\nages and music are not addressed. Although this is per-\nfectly ﬁne for the intended use cases, such highly speciﬁc\nevaluation settings are not able to provide answers to more\ngeneral questions of music retrieval and recommendation,\nforemost because these settings fail at offering explana-\ntions for the (un)suitability of the musical items under in-\nvestigation.\nAn evaluation approach that tries to alleviate this short-\ncoming is presented in [4], where subjective listening tests\nto assess music recommendation algorithms are conducted\nusing a multifaceted questionnaire. Besides investigating\nthe enjoyment a user feels when listening to the recom-\nmended track (“liking”), the authors also ask for the user’s\n“listening intention”, whether or not the user knows artist\nand song (“familiarity”), and whether he or she would like\nto request more similar music (“give-me-more”). A simi-\nlar evaluation scheme is suggested in [12]. However, Firan\net al. only investigate liking and novelty.\nIn summary, almost all approaches reported are still more\nsystems-based than user-centric.\n3. HOW TO EVALUATE USER-CENTERED MIR?\nIn what follows we will argue that whereas evaluation of\nsystems-based MIR has quite matured, evaluation of user-\ncentered MIR is still in its infancy. Let us start by re-\nviewing what the nature of experiments is in the context\nof MIR. The basic structure of MIR experiments is the\nsame as in any other experimental situation: the question\nis whether there are effects of the variation of the indepen-\ndent variables (also called factors) on the dependent vari-\nables. In the case of systems-based MIR, independent vari-\nables are e.g. type and certain parameter characteristics of\nthe algorithms used or type and characteristics of the dataset in question. Typical dependent variables are various\nperformance measures like accuracy, precision, root mean\nsquared error or training time. A standard computer exper-\niment is genre classiﬁcation where the independent vari-\nable is the type of classiﬁcation algorithm, say algorithm A\nand B, and the dependent variable is the achieved accuracy.\nStatistical testing is used to ensure that the observed effects\non the dependent variables are caused by the varied inde-\npendent variables and not by mere chance, i.e. to ascertain\nthat the observed differences are too large to attribute them\nto random inﬂuences only. Besides using the proper statis-\ntical instruments to establish statistical signiﬁcance of re-\nsults it is equally important to make sure to control all im-\nportant factors in the experimental design. Any factor that\nis able to inﬂuence the dependent variables has to be part of\nthe experimental design. E.g. if algorithm A, compared to\nalgorithm B, works better for electronic dance music than\nfor rock music then any experimental design not contain-\ning dance music will obscure differences between A and\nB. The important thing to note is that for systems-based\nMIR which uses only computer experiments it is compara-\nbly easy to control all important factors which could have\nan inﬂuence on the dependent variables. This is because\nthe number of factors is both manageable and controllable\nsince the experiments are being conducted on computers\nand not in the real world.\nAlready early on in the history of MIR research, gaps\nconcerning the evaluation of MIR systems have been iden-\ntiﬁed. Futrelle and Downie [14], in their review of the ﬁrst\nthree years of the ISMIR conference published in 2003,\nidentify two major problems: (i) no commonly accepted\nmeans of comparing retrieval techniques, (ii) few if any at-\ntempts to study potential users of MIR systems. The ﬁrst\nproblem concerns evaluation of computer experiments and\nthe second problem the barely existing inclusion of users\nin MIR studies. Flexer [13], in his review of the 2004\nISMIR conference [5], argues for the necessity of statis-\ntical evaluation of MIR experiments. He presents mini-\nmum requirements concerning statistical evaluation by ap-\nplying fundamental notions of statistical hypotheses test-\ning to MIR research. His discussion is concerned with\nsystems-based MIR, the example used throughout the pa-\nper is that of automatic genre classiﬁcation based on audio\ncontent analysis. The MIR community is criticized for the\nlack of statistical evaluation it uses, e.g. only two papers\nin the ISMIR 2004 proceedings [5] employed a statistical\ntest to prove signiﬁcance of their results. These ongoing\ndiscussions about evaluation of MIR experiments have led\nto a ﬁrst evaluation benchmark taking place at the ISMIR\nconference 2004 [6] and further on to the establishment of\nthe annual evaluation campaign for MIR algorithms (Mu-\nsic Information Retrieval Evaluation eXchange, MIREX)\n[10]. In 2011, MIREX consisted of 16 tasks ranging from\naudio classiﬁcation, cover song identiﬁcation, audio key\ndetection to structural segmentation and audio tempo es-\ntimation. All but two tasks are concerned with systems-\nbased MIR and a purely computer-based evaluation of al-\ngorithms. The two exceptions using human evaluations ina more real-world setting are Audio Music Similarity and\nRetrieval andSymbolic Melodic Similarity. Starting with\nthe MIREX 2006 evaluation [10] statistical tests are being\nused to analyze results.\nThe situation concerning evaluation of user-centric MIR\nresearch is far less well developed. In a recent comprehen-\nsive review [31] of user studies in the MIR literature by\nWeigl and Guastavino, papers from the ﬁrst decade of IS-\nMIR conferences and related MIR publications were ana-\nlyzed. A central result is that MIR research has a mostly\nsystems-centric focus. Only twenty papers fell under the\nbroad category of “user studies” which is an alarmingly\nsmall number given that 719 articles have been published\nin the ISMIR conference series alone. To make things\nworse, these user studies are “predominantly qualitative in\nnature” and of “largely exploratory nature” [31]. The ex-\nplored topics range from e.g. user requirements and infor-\nmation needs, insights into social and demographic factors\nto user-generated meta-information and ground truth. This\nall points to the conclusion that evaluation of user-centered\nMIR is at its beginning and that especially a more rigorous\nquantitative treatment is still missing.\nIn discussing the challenges of quantitative evaluation\nof user-centered MIR we like to turn to an illustrative ex-\nample: the recent 2011 Audio Music Similarity and Re-\ntrieval task1within the annual MIREX [10] evaluation\ncampaign. Each of 18 competing algorithms was given\n7000 songs (30 second audio clips) for which they com-\nputed similarity rankings. The data consisted of 10 equally\nsized genre classes ranging from classic music to rock to\nhip-hop. From the 7000 songs, “100 songs were randomly\nselected from the 10 genre groups (10 per genre) as queries\nand the ﬁrst 5 most highly ranked songs out of the 7000\nwere extracted for each query (after ﬁltering out the query\nitself, returned results from the same artist were also omit-\nted). Then, for each query, the returned results (candi-\ndates) from all participating algorithms were grouped and\nwere evaluated by human graders”1. For each individual\nquery/candidate pair, a single human grader provided both\na FINE score (from 0 (failure) to 100 (perfection)) and a\nBROAD score (not similar NS, somewhat similar SS, very\nsimilar VS) indicating how similar the songs are in their\nopinion. The independent variable here is the type of algo-\nrithm used to compute the similarity rankings. The depen-\ndent variables are the subjects’ broad and ﬁne appraisal of\nthe perceived similarity. But since this is a real-world ex-\nperiment involving human subjects there is a whole range\nof factors that have not been assessed. E.g. there are social\nand demographic factors that might clearly inﬂuence the\nuser’s judgment of music similarity: their age, gender, cul-\ntural background and especially their musical history, ex-\nperience and knowledge. But also factors concerning their\nmomentary situation during the actual listening experiment\nmight have an inﬂuence: time of day, mood, physical con-\ndition. Not to forget more straightforward variables like\ntype of speakers or headphones used for the test. As al-\n1The 2011 results and details can be found at:\nhttp://www.music-ir.org/mirex/wiki/2011:\nAudio Music Similarity andRetrieval Resultsready mentioned in section 1, even the choice of depen-\ndent variable is debatable. After all, what does “similar”\nreally mean in the context of music? Timbre, mood, har-\nmony, melody, tempo, etc might all be valid answers for\ndifferent people. This points to a certain lack of rigor\nconcerning the instruction of subjects during the experi-\nment. This enumeration of potential problems is not in-\ntended to badmouth this MIREX task which still is a valu-\nable contribution and an applaudable exception to the rule\nof computer-only evaluation. But it is meant as a warn-\ning and to highlight the explosion of independent variables\nand factors that might add to the variance of observed re-\nsults and might obscure signiﬁcant differences. In princi-\nple, all such factors have to be recorded and made indepen-\ndent variables in the overall experimental design.\nIf MIR is to succeed in maturing from purely systems-\nbased to user-centered research we will have to leave the\nnice and clean world of our computers and face the often\nbewilderingly complex real world of real human users and\nall the challenges this entails for proper design and evalu-\nation of experiments. To make this happen it will be nec-\nessary that our community with a predominantly engineer-\ning background opens up to the so-called “soft sciences”\nof e.g. psychology and sociology which have developed\ninstruments and methods to deal with the complexity of\nhuman subjects.\n4. DISCUSSION AND CONCLUSIONS\nIncorporating real users in both the development and as-\nsessment of music retrieval systems is of course an expen-\nsive and arduous task. However, recent trends in music\ndistribution, in particular the emergence of music stream-\ning services that make available millions of tracks to their\nusers, call for intelligent personalized and context-aware\nsystems to deal with this abundance. Concerning the de-\nvelopment of such systems, we believe that the following\ntwo reasons have prevented major breakthroughs so far: (i)\na general lack of research on user-centered systems, (ii) a\nlack of awareness concerning the complexity of evaluation\nof user-centered systems. In designing such systems, the\nuser should already be taken into account at an early stage\nduring the development process. We need to better un-\nderstand what the user’s individual requirements are and\naddress these requirements in our implementations. Other-\nwise it is unlikely that even the spifﬁest personalized sys-\ntems will succeed (without frustrating the user). We hence\nidentify the following four key requirements for elaborat-\ning user-centric music retrieval systems:\nPersonalization aspects have to be taken into account.\nIn this context, it is important to note the highly subjec-\ntive, cognitive component in the understanding of music\nand judgement of its personal appeal. Therefore, designing\nuser-aware music applications requires intelligent machine\nlearning techniques, in particular, preference learning ap-\nproaches that relate the user context to concise, situation-\ndependent music preferences.\nUser models that encompass different social scopes are\nneeded. They may aggregate an individual model, an in-terest group model, a cultural model, and a global model.\nFurthermore, the user should be modeled as comprehen-\nsively as possible, in a ﬁne-grained and multifaceted man-\nner. With today’s sensor-packed smartphones and other in-\ntelligent devices it has become easy to perform extensive\ncontext logging. Of course, privacy issues must also be\ntaken seriously.\nMultifaceted similarity measures that combine differ-\nent feature categories (music content, music context, and\nuser context) are required. The corresponding represen-\ntation models should then not only allow to derive simi-\nlarity between music via content-related aspects, such as\nbeat strength or instruments playing, or via music context-\nrelated properties, such as the geographic origin of the per-\nformer or a song’s lyrics, but also to describe users and\nuser groups in order to compute a listener-based similarity\nscore.\nEvaluation of user-centric music retrieval approaches\nhas to include all independent variables that are able to in-\nﬂuence the dependent variables into the experimental de-\nsign. In particular, such factors may well relate to indi-\nvidual properties of the human assessors. Furthermore, it\nis advisable to make use of recent approaches that mini-\nmize the amount of labor required by the human assessors,\nwhile at the same time maintaining the signiﬁcance of the\nexperiments. This can be achieved, for instance, by em-\nploying the concept of “Minimal Test Collections” in the\nevaluation of music retrieval systems [30].\nBy paying attention to these advices, we are sure that\nthe exciting ﬁeld of user-centric music information retrieval\nwill continue to grow and eventually provide us with al-\ngorithms and systems that offer personalized and context-\naware access to music in an unintrusive way.\n5. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Science Fund\n(FWF): P22856-N23 and P24095 as well as by the Euro-\npean Commission, FP7 (Seventh Framework Programme),\nICT-2011.1.5 Networked Media and Search Systems, grant\nagreement no. 287711.\n6. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet. Representing Musical\nGenre: A State of the Art. Journal of New Music Re-\nsearch, 32(1):83–93, 2003.\n[2] L. Baltrunas, M. Kaminskas, B. Ludwig, O. Moling,\nF. Ricci, K.-H. L ¨uke, and R. Schwaiger. InCarMusic:\nContext-Aware Music Recommendations in a Car. In\nProc. EC-Web, 2011.\n[3] J. T. Biehl, P. D. Adamczyk, and B. P. Bailey. DJog-\nger: A Mobile Dynamic Music Device. In CHI 2006:\nExtended Abstracts, 2006.\n[4] D. Bogdanov and P. Herrera. How Much Metadata\nDo We Need in Music Recommendation? A Subjec-\ntive Evaluation Using Preference Sets. In Proc. ISMIR,\n2011.[5] C.L. Buyoli and R. Loureiro. Fifth International Con-\nference on Music Information Retrieval. Universitat\nPompeu Fabra, 2004.\n[6] P. Cano, E. G ´omez, F. Gouyon, P. Herrera, M. Kop-\npenberger, B. Ong, X. Serra, S. Streich, and N. Wack.\nISMIR 2004 Audio Description Contest. 2006.\n[7] T. Cebri ´an, M. Planagum `a, P. Villegas, and X. Amatri-\nain. Music Recommendations with Temporal Context\nAwareness. In Proc. RecSys, 2010.\n[8] S. Cunningham, S. Caulder, and V . Grout. Saturday\nNight or Fever? Context-Aware Music Playlists. In\nProc. Audio Mostly, 2008.\n[9] S. Dornbush, J. English, T. Oates, Z. Segall, and\nA. Joshi. XPod: A Human Activity Aware Learning\nMobile Music Player. In Proc. Workshop on Ambient\nIntelligence, IJCAI, 2007.\n[10] J. S. Downie. The Music Information Retrieval Evalua-\ntion eXchange (MIREX). D-Lib Magazine, Dec 2006.\n[11] G. T. Elliott and B. Tomlinson. PersonalSoundtrack:\nContext-aware Playlists that Adapt to User Pace. In\nCHI 2006: Extended Abstracts, 2006.\n[12] Claudiu S. Firan, Wolfgang Nejdl, and Raluca Paiu.\nThe Beneﬁt of Using Tag-Based Proﬁles. In Pro-\nceedings of the 5th Latin American Web Congress\n(LA-WEB), pages 32–41, Santiago de Chile, Chile,\nOctober–November 2007.\n[13] A. Flexer. Statistical Evaluation of Music Informa-\ntion Retrieval Experiments. Journal of New Music Re-\nsearch, 35(2):113–120, June 2006.\n[14] J. Futrelle and J. S. Downie. Interdisciplinary Research\nIssues in Music Information Retrieval: ISMIR 2000-\n2002. Journal of New Music Research, 32(2):121–131,\n2003.\n[15] M. Kaminskas and F. Ricci. Location-Adapted Music\nRecommendation Using Tags. In Proc. UMAP, 2011.\n[16] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A Mu-\nsic Search Engine Built upon Audio-based and Web-\nbased Similarity Measures. In Proc. SIGIR, 2007.\n[17] P. Knees and G. Widmer. Searching for Music Using\nNatural Language Queries and Relevance Feedback. In\nProc. AMR, 2007.\n[18] T. Kohonen. Self-Organizing Maps, volume 30 of\nSpringer Series in Information Sciences. Springer,\nBerlin, Germany, 3rd edition, 2001.\n[19] Y . Koren, R. Bell, and C. V olinsky. Matrix Factor-\nization Techniques for Recommender Systems. Com-\nputer, 42, Aug 2009.\n[20] D. D. Lee and H. S. Seung. Learning the Parts of Ob-\njects by Non-negative Matrix Factorization. Nature,\n401(6755):788–791, 1999.[21] J. S. Lee and J. C. Lee. Context Awareness by Case-\nBased Reasoning in a Music Recommendation System.\nInProc. UCS, 2007.\n[22] G. Linden, B. Smith, and J. York. Amazon.com Rec-\nommendations: Item-to-Item Collaborative Filtering.\nIEEE Internet Computing, 4(1), 2003.\n[23] H. Liu, J. Hu, and M. Rauterberg. Music Playlist\nRecommendation Based on User Heartbeat and Music\nPreference. In Proc. ICCTD, 2009.\n[24] B. Moens, L. van Noorden, and M. Leman. D-Jogger:\nSyncing Music with Walking. In Proc. SMC, 2010.\n[25] A. N ¨urnberger and M. Detyniecki. Weighted Self-\nOrganizing Maps: Incorporating User Feedback. In\nProc. ICANN/ICONIP, 2003.\n[26] T. Pohle, P. Knees, M. Schedl, and G. Widmer.\nBuilding an Interactive Next-Generation Artist Recom-\nmender Based on Automatically Derived High-Level\nConcepts. In Proc. CBMI, 2007.\n[27] J. J. Rocchio. Relevance Feedback in Information Re-\ntrieval. In Gerard Salton, editor, The SMART Retrieval\nSystem - Experiments in Automatic Document Process-\ning, pages 313–323. Englewood Cliffs, NJ: Prentice-\nHall, 1971.\n[28] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-\nbased Collaborative Filtering Recommendation Algo-\nrithms. In Proc. WWW, 2001.\n[29] M. Schedl and P. Knees. Personalization in Multimodal\nMusic Retrieval. In Proc. AMR, 2011.\n[30] J. Urbano and M. Schedl. Towards Minimal Test Col-\nlections for Evaluation of Audio Music Similarity and\nRetrieval. In Proc. AdMIRe, 2012.\n[31] D. Weigl and C. Guastavino. User Studies in the Music\nInformation Retrieval Literature. In Proc. ISMIR, 2011.\n[32] W. Xu, X. Liu, and Y . Gong. Document Cluster-\ning Based on Non-negative Matrix Factorization. In\nProc. SIGIR, 2003.\n[33] G.-R. Xue, J. Han, Y . Yu, and Q. Yang. User Lan-\nguage Model for Collaborative Personalized Search.\nACM Transactions on Information Systems, 27(2), Feb\n2009.\n[34] B. Zhang, J. Shen, Q. Xiang, and Y . Wang. Com-\npositeMap: A Novel Framework for Music Similarity\nMeasure. In Proc. SIGIR, 2009."
    },
    {
        "title": "Feature Learning in Dynamic Environments: Modeling the Acoustic Structure of Musical Emotion.",
        "author": [
            "Erik M. Schmidt",
            "Jeffrey J. Scott",
            "Youngmoo E. Kim"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414846",
        "url": "https://doi.org/10.5281/zenodo.1414846",
        "ee": "https://zenodo.org/records/1414846/files/SchmidtSK12.pdf",
        "abstract": "While emotion-based music organization is a natural pro- cess for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature repre- sentation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based fea- tures is the ambiguity of the ground-truth. Even using the smallest time window, opinions about emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response la- bels to music in the arousal-valence (A-V) emotion space with time-varying stochastic distributions. Current meth- ods for automatic detection of emotion in music seek per- formance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor clas- sification performance gains, but has provided little insight into the relationship between audio and emotional associ- ations. In this work, we seek to employ regression-based deep belief networks to learn features directly from mag- nitude spectra. Taking into account the dynamic nature of music, we investigate combining multiple timescales of ag- gregated magnitude spectra as a basis for feature learning.",
        "zenodo_id": 1414846,
        "dblp_key": "conf/ismir/SchmidtSK12",
        "keywords": [
            "quantifying",
            "difficulty",
            "dominant feature",
            "music emotion recognition",
            "ambiguity",
            "ground-truth",
            "human response labels",
            "arousal-valence space",
            "automatic detection",
            "emotion in music"
        ],
        "content": "FEATURE LEARNING IN DYNAMIC ENVIRONMENTS:\nMODELING THE ACOUSTIC STRUCTURE OF MUSICAL EMOTION\nErik M. Schmidt, Jeffrey Scott, and Youngmoo E. Kim\nMusic and Entertainment Technology Laboratory (MET-lab)\nElectrical and Computer Engineering, Drexel University\nfeschmidt,jjscott,ykimg@drexel.edu\nABSTRACT\nWhile emotion-based music organization is a natural pro-\ncess for humans, quantifying it empirically proves to be a\nvery difﬁcult task, and as such no dominant feature repre-\nsentation for music emotion recognition has yet emerged.\nMuch of the difﬁculty in developing emotion-based fea-\ntures is the ambiguity of the ground-truth. Even using the\nsmallest time window, opinions about emotion are bound\nto vary and reﬂect some disagreement between listeners.\nIn previous work, we have modeled human response la-\nbels to music in the arousal-valence (A-V) emotion space\nwith time-varying stochastic distributions. Current meth-\nods for automatic detection of emotion in music seek per-\nformance increases by combining several feature domains\n(e.g. loudness, timbre, harmony, rhythm). Such work has\nfocused largely in dimensionality reduction for minor clas-\nsiﬁcation performance gains, but has provided little insight\ninto the relationship between audio and emotional associ-\nations. In this work, we seek to employ regression-based\ndeep belief networks to learn features directly from mag-\nnitude spectra. Taking into account the dynamic nature of\nmusic, we investigate combining multiple timescales of ag-\ngregated magnitude spectra as a basis for feature learning.\n1. INTRODUCTION\nThe problem of automated recognition of emotional con-\ntent (mood) within music has been the subject of increasing\nattention among the music information retrieval (Music-\nIR) research community [1]. While there has been much\nprogress in machine learning systems for estimating hu-\nman emotional response to music, little progress has been\nmade in terms of intuitive feature representations. Cur-\nrent methods generally focus on combining several fea-\nture domains (e.g. loudness, timbre, harmony, rhythm) and\nperforming dimensionality reduction techniques to extract\nthe most relevant informaiton. In many cases these meth-\nods have failed to provide enhanced classiﬁcation perfor-\nmance, and they leave much to be desired in terms of un-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.derstanding the complex relationship between emotional\nassociations and acoustic content.\nThe Music Information Retrieval Evaluation eXchange\n(MIREX)1audio mood classiﬁcation task provides an ex-\ncellent illustration of this. Shown in Figure 1 is the per-\nformance of MIREX submissions for each year. The ﬁrst\nyear MIREX ran the task it received 9 submissions, and\nthe best performing system achieved 61.50% performance\non the 6-class problem using a feature space spanning 16-\ndimensions [2]. Each year the task has received a larger\nnumber of submissions, with exponentially larger feature\nlibraries, but have failed to produce signiﬁcant perfor-\nmance gains. Most recently, in 2010 the task received 36\nsubmissions with the best system mining a 70-dimensional\nfeature space, but achieved only 64.17% [3]. These results\nperhaps indicate that the data necessary for informing sys-\ntems for this problem is not present in any current feature\nset.\nHuman judgments are necessary for deriving emotion\nlabels and associations, but individual perceptions of the\nemotional content of a given song or musical excerpt are\nbound to vary and reﬂect some degree of disagreement be-\ntween listeners. This lack of speciﬁcity presents signiﬁ-\ncant challenges for developing informative feature repre-\nsentations for content-based music emotion prediction. In\nprevious work we have investigated modeling emotional\nresponses to music as both a singular point [4] as well as a\nstochastic distribution [5] over the arousal-valence (A-V)\nspace of emotional affect. In this two dimensional repre-\nsentation valence indicates positive versus negative emo-\ntion and arousal reﬂects emotional intensity [6].\nThe ambiguous nature of musical emotion makes it an\nespecially interesting problem for the application of fea-\nture learning. Using deep belief networks (DBNs) [7] we\ndevelop methods for learning emotion-based acoustic rep-\nresentations directly from magnitude spectra. In previous\nwork, we have found these models to be powerful meth-\nods for generating reduced dimensionality representations\nof raw input spectra [8]. In that approach, we learned fea-\ntures directly from spectra at the 20msec rate of our win-\ndowed Short-Time Fourier Transform (STFT). In doing so,\nwe provided a direct comparison between the DBN model\nfor extracting features to standard acoustic representations\nsuch as MFCCs.\n1http://www.music-ir.org/mirex2007 2008 2009 2010203040506070\nYearOverall Accuracy (%)MIREX Mood Classification Performance By Year\n  \n1 Submission5 Submissions10 Submissions15 Submissions20 SubmissionsFigure 1. MIREX mood classiﬁcation task performance\nby year.\nBut as humans require a larger time window than\n20msec to determine emotions, and building upon that\nwork we seek to improve our performance by informing\nour learned feature representations with spectra aggregated\nat multiple timescales. In addition, we investigate a univer-\nsal background model (UBM) approach to feature learn-\ning. As DBN training follows an unsupervised pretrain-\ning, we investigate bootstrapping a much larger unlabeled\ndataset in developing our models. Given the challenges of\ncollecting emotion annotated data, pretraining on a limited\ndataset is insufﬁcient to form a general music model for\nﬁnetuning. By bootstrapping a larger dataset we demon-\nstrate signiﬁcant improvement in using our DBN models\nfor emotion prediction, and modest gains when using our\nlearned features in a separate supervised machine learning\napproach.\nWe compare the learned feature representations to other\nstate-of-the-art representations investigated in prior work\n[4, 5, 9]. In these experiments, we use the DBN hidden\nlayer outputs as features to predict the training labels us-\ning a separate linear regression model. In all experiments\nwe show that the features generated by the DBN outper-\nform all other features, and that the topology is especially\npromising in providing insight into the relationship be-\ntween acoustic data and emotional associations.\n2. BACKGROUND\nFeature learning has only recently gained attention in the\nmachine listening community. Lee et al. was the ﬁrst to ap-\nply deep belief networks to acoustic signals, employing an\nunsupervised convolutional approach [10]. Their system\nemployed PCA to provide a dimensionality reduced repre-\nsentation of the magnitude spectrum as input to the DBN\nand showed slight improvement over MFCCs for speaker,\ngender, and phoneme detection.\nHamel and Eck applied deep belief networks (DBNs) to\nthe problems of musical genre identiﬁcation and autotag-\nging [11]. Their approach used raw magnitude spectra as\nthe input to their DBNs, which were constructed from three\nlayers, employing ﬁfty units at each layer. The system was\ntrained using a greedy-wise pre-training and ﬁne-tuned on\na genre classiﬁcation dataset, consisting of 1000 30-second\nclips. The system took 104 hours to train, and as a result\nwas not cross-validated. Applied to a genre classiﬁcationtask, the learned features achieved a classiﬁcation accuracy\nof 0.843, which was an increase over MFCCs at 0.790. The\nlearned model was also used to inform an autotagging al-\ngorithm, which scored 0.73 in terms of mean accuracy, a\nslight improvement over MFCCs at 0.70.\n3. GROUND TRUTH DATA COLLECTION\nIn prior work, we developed an online collaborative anno-\ntation activity based on the two-dimensional A-V model\n[12]. In this activity, participants use a graphical inter-\nface to indicate a dynamic position within the A-V space\nto annotate 30-second music clips. Each subject provides\na check against the other, reducing the probability of non-\nsense labels. The song clips used are drawn from the “us-\npop2002” database.2Using initial game data, we con-\nstructed a corpus of 240 15-second music clips, which\nwere selected to approximate an even distribution across\nthe four primary quadrants of the A-V space.\nIn more recent work we have developed a Mechanical\nTurk (MTurk) activity to collect annotations on the same\ndataset [13]. The purpose of the MTurk activity was to\nprovide a dataset collected through more traditional means\nto assess the effectiveness of the game, speciﬁcally to de-\ntermine any biases created though collaborative labeling.\nOverall, the datasets were shown to be highly correlated,\nwith arousal r= 0:712, and a valence r= 0:846. This\nnew dataset is available to the research community,3and is\ndensely annotated, containing 4;064label sequences in to-\ntal,16:93\u00062:690 ratings per song. In this work we demon-\nstrate the application of this densely annotated corpus for\nemotion-based feature learning.\n4. ACOUSTIC FEATURE COLLECTION\nSince our focus is on learning features that are speciﬁ-\ncally tuned to emotion prediction, we limit our compar-\nisons to features that performed well in previous work.\nThe features are also commonly used in the machine lis-\ntening community and provide a reasonable baseline for\ntesting. Our collection (Table 1) consists of the two highest\nperforming features in prior work, Spectral Contrast and\nMFCCs [4,5], as well as the Echo Nest Timbre (ENT) fea-\ntures.\nFeature Description\nSpectral Contrast\n[14]Rough representation of the harmonic\ncontent in the frequency domain.\nMel-frequency\ncepstral coefﬁcients\n(MFCCs) [15]Low-dimensional representation of\nthe spectrum warped according to the\nmel-scale. 20 dimensions used.\nEcho Nest Timbre\nfeatures (ENTs)4Proprietary 12-dimensional beat-\nsynchronous timbre feature\nTable 1. Acoustic feature collection for music emotion\nprediction.\n2http://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html\n3http://music.ece.drexel.edu/research/emotion/moodswingsturk\n4http://developer.echonest.com5. DEEP BELIEF NETWORKS\nA fully trained deep belief network shares an identical\ntopology to a neural network, though they offer a far-\nsuperior training procedure, which begins with an unsu-\npervised pre-training that models the hidden layers as re-\nstricted Boltzman machines (RBMs) [7,16,17] . A graphi-\ncal depiction of an RBM is shown in Figure 2. An RBM is\na generative model that contains only a single hidden layer,\nand in simplistic terms they can be thought of two sets of\nbasis vectors, one which reduces the dimensionality of the\ndata and the other that reconstructs it.\nHidden\nLayer\nVisibleLayer\nFigure 2. Restricted Boltzman machine topology.\nRBMs are Markov random ﬁelds (MRFs) with hidden\nunits, in a two layer architecture where we have visible\nunits vand hidden units h. This has an energy function of\nthe form,\nE(v;h) =\u0000X\ni2freqbivi\u0000X\nj2featurescjhj\u0000X\ni;jvihjwij(1)\nwhere in this case the input is a spectrogram v2R1\u0002Iand\nthe hidden layer h2R1\u0002J. The model has parameters\nW2RI\u0002J, with biases c2R1\u0002Jandv2R1\u0002I. Dur-\ning pre-training, we learn restricted Boltzman machines\n“greedily,” where we learn them one at a time from the\nbottom up. That is, after we learn the ﬁrst RBM we retain\nonly the forward weights, and use them to create the input\nfor training the next RBM layer.\nAs in the typical approach to deep learning, after pre-\ntraining we form a multi-layer perceptron using only the\nforward weights of the RBM layers. However, in typi-\ncal approaches the ﬁnal step is to attach logistic regres-\nsion layer to the output of the MLP, and the full system is\nﬁne-tuned for classiﬁcation using gradient descent. Since\nour goal is to learn feature detectors for a regression prob-\nlem, we instead attach a simple linear regression layer\nand report the prediction error for ﬁne-tuning as the mean\nsquared error of the estimators. Squared error is chosen\nas opposed to Euclidean error for speed and numerical sta-\nbility, as both functions have the same minimum. Further-\nmore, we elect to do our ﬁne-tuning using conjugate gradi-\nent optimization, which we found to outperform gradient\ndescent for our topology during initial testing.\nWe trained our DBNs using Theano,5a Python-based\npackage for symbolic math compilation, and Scipy’s opti-\nmization toolbox for the conjugate gradient optimization.\nTheano is an extremely powerful tool for machine learn-\ning problems because it combines the simplicity of Python\n5http://deeplearning.net/software/theano/with the power of compiled C, which can target the CPU\nor GPU.\n6. EXPERIMENTS AND RESULTS\nIn the following experiments we investigate employing\ndeep belief networks for emotion-based acoustic feature\nlearning. In all experiments, the model training is cross-\nvalidated 5 times, dividing the dataset into 50% training,\n20% veriﬁcation, and 30% testing. To avoid the well-\nknown album-effect, we ensured that any songs that were\nrecorded on the same album were either placed entirely in\nthe training or testing set.\nAll learned features are then evaluated in the context of\nmultiple linear regression (MLR), as we have investigated\nin prior work [4,5,18]. MLR provides extremely high com-\nputational efﬁciency, making it ideal for discriminating be-\ntween relative usefulness of many feature domains.\n6.1 Short-time Feature Learning\nIn the ﬁrst set of experiments, we investigate learning fea-\ntures directly from short-time magnitude spectra. We have\ninvestigated this approach in prior work in the context of\na different dataset [4], and we investigate it here to com-\npare performance with the Turk dataset and to provide a\nbaseline for our further work. As with our previous work,\nwe use 3 hidden layers in all experiments, each containing\n50 nodes. Furthermore, we run pre-training for 50 epochs\nwith a learning rate of 0.001. During the conjugate gradi-\nent ﬁne-tuning stage we attach an additional multiple linear\nregression (MLR) layer to the output of the DBN. As this\nstage is supervised, for each input example xi, we train the\nmodel to produce the emotion space parameter vector yi,\nyi= [\u0016a; \u0016v]: (2)\nShown in Table 2, are the results for employing the\nlearned features for multiple linear regression. Features\nare ﬁrst extracted on 20msec intervals, and then appropri-\nately aggregated to match the one second intervals of our\nlabels. Results for these features which are learned from\nsingle frames are shown as DBN-SF. We additionally show\nthe KL-divergence for the Gaussian ground-truth represen-\ntation used in prior work [5, 18]. Where we develop re-\ngressors to predict the parameterization vector yiof a two-\ndimensional Gaussian in A-V space,\nyi= [\u0016a; \u0016v; \u001b2\naa; \u001b2\nvv; \u001b2\nav]: (3)\n6.2 Multi-frame Feature Learning\nWhile future work on more sophisticated ﬁne-tuning ap-\nproaches or better stochastic models in pre-training may\nimprove performance, the largest issue is the inherent lim-\nitation in using a single short-time window. Human emo-\ntional associations necessarily require more than a \u001820ms\nshort-time window, and thus future approaches must take\ninto account the variation of acoustic data over a larger\nperiod of time. In these experiments we investigate the\ndevelopment of models that incorporate multiple spectralHidden\nLayer\nVisibleLayerHiddenLayerHidden\nLayer\nVisibleLayer\n•••\n••••••\n•••\n1 second2 seconds4 secondsMLR\nMLRA-V Coordinate\nµa\nµv\nσaa\nσvv\nσavMLRA-V DistributionFigure 3. Feature learning system architecture showing the temporal aggregation, deep belief network and subsequent\ntraining of linear regressors to predict multi-dimensional A-V distributions.\nwindows to derive musical emotion. Taking spectral ag-\ngregations of the past one second, past two seconds, and\npast four seconds, we concatenate the resulting vectors\nas inputs to the system. As each spectrum frame is 257-\ndimensional vector, the total DBN input is now 771 di-\nmensions. A diagram showing the multi-rate temporal in-\ntegration, DBN training and linear regression to the emo-\ntion space is shown in Figure 3. Results for multi-frame\n(MF) feature learning can be found in Table 2 labeled as\nDBN-MF.\nFor this new approach, we provide visualizations of the\nlearned features. Figure 4 shows the input spectrogram in\nlog-magnitude for proper visualization, though we do not\ntake the log for the actual model input.\nFigure 4. Log-magnitude spectrogram of input audio.\nIn the original spectrogram (Figure 4) we see the verse\ntransition into the ﬁrst chorus of the Soulive rendition of\nthe Beatles song Come Together starting around frame 500.\nWe see a similar pattern in the spectrogram between frames\n1488-1800, which is the only other part of the clip where\nthe percussion includes cymbals. Shown in Figure 5 are\nthe resulting features from the intermediary layer outputs.\nNote that the structural information in the spectrogram is\nretained in the hidden layer outputs rendered in Figure 5.\nWe also wish to investigate the reconstruction of the\noriginal input aggregated spectrogram from the hidden\nlayer outputs. Figure 6 depicts this reconstruction which\nFigure 5. DBN hidden layer outputs using the aggregate\nspectral frames as input.\nwas generated using the method outlined in previous work\n[8]. Due to the concatenations of multiple time scale ag-\ngregations, we adjust the y-axis to display the correct fre-\nquency values for each. The top contains the last one sec-\nond aggregations, below that is the aggregation from the\nlast two seconds, and the last four seconds is at the bottom.\n6.3 Universal Background Model Feature Learning\nIn order to improve our results with the multi-frame ap-\nproach, we seek to harness the power of our much larger\nunlabeled music dataset. As DBN training relies on a two\nstep training process, the ﬁrst of which is unsupervised,\nthere is no reason we should not use every piece of avail-\nable data. In training our RBMs with our larger dataset,\nwe get a much more accurate portrail of the distribution of\nmusic, and therefore create a much more accurate music\nmodel, which we can then ﬁnetune for musical emotion,Figure 6. Reconstruction of the original aggregated spec-\ntrogram used as the DBN input. Top is last one second\naggregates, middle last 2 seconds, bottom last 4 seconds.\nor any other supervised machine learning problem. As this\nmodel is a general music model, we refer to it as a univer-\nsal background model (UBM). For our larger dataset we\nuse the uspop2002 dataset in its entirety, which contains\nnearly 8000 songs. Even after aggregating our spectra at\none-second intervals this adds up to \u001826 GB of training\ndata. Results for the universal background model approach\nare shown in Table 2 as DBN-UBM.\nFeature Average Mean Average KL\nType Distance Divergence\nMFCC 0:140\u00060:005 1 :28\u00060:157\nChroma 0:182\u00060:006 3 :33\u00060:294\nSpectral Shape 0:153\u00060:006 1 :51\u00060:160\nSpectral Contrast 0:138\u00060:005 1:29 \u00060:160\nENT 0:151\u00060:006 1 :41\u00060:175\nDBN-SF Model Error 0:203\u00060:009 -\nDBN-SF Layer 1 0:138\u00060:005 1 :25\u00060:142\nDBN-SF Layer 2 0:133\u00060:004 1:19 \u00060:129\nDBN-SF Layer 3 0:133\u00060:002 1 :21\u00060:180\nDBN-MF Model Error 0:194\u00060:032 -\nDBN-MF Layer 1 0:131\u00060:006 1 :15\u00060:106\nDBN-MF Layer 2 0:131\u00060:004 1 :14\u00060:107\nDBN-MF Layer 3 0:129\u00060:004 1:12 \u00060:114\nDBN-UBM Model Error 0:140\u00060:015 -\nDBN-UBM Layer 1 0:129\u00060:006 1 :12\u00060:091\nDBN-UBM Layer 2 0:128\u00060:004 1 :13\u00060:097\nDBN-UBM Layer 3 0:128\u00060:004 1:11 \u00060:090\nTable 2. Emotion regression results for ﬁfteen second\nclips. DBN-SF are features learned from single frames\n(SF), DBN-MF are features learned from multi-frame\n(MF) aggregations, and DBN-UBM are features learned\nwith a universal background model (UBM) approach to\nDBN pretraining. KL-divergence is not applicable to\nmodel error.\n7. DISCUSSION AND FUTURE WORK\nIn looking at the ﬁrst set of results for learning features\nfrom single frames (DBN-SF), we see second layer fea-\ntures perform best for this method, outperforming spec-\ntral contrast, which is the best performing standard fea-\nture. This result is consistent with prior work [8], thoughhere we ﬁnd the DBN-SF features to be better than spectral\ncontrast both in predicting single points and distributions.\nIn that work we found the DBN features to be more ac-\ncurate in terms of mean prediction and spectral contrast to\nperform slightly better in terms of KL, though we strongly\nemphasized an incorrect mean to be a much worse an error\nthan an incorrectly sized or rotated covariance.\nIn trying to improve our features by including multiple\ntimescales we see improvement in mean error from 0:133\nto0:129, which is encouraging. In analyzing the recon-\nstructed spectra from ﬁrst layer features, we get a very in-\nteresting result, which is similar to our prior work with [8].\nThe overall representation if very sparse in terms of fre-\nquency and seems to target very speciﬁc frequencies to\ncontribute to the overall emotion features. Analyzing the\nfeatures in Figure 5, we note that there is most deﬁnitely an\nemotion change as we progress from the slower and heav-\nily minor sounding verse into the higher tempo rock cho-\nrus. We see changes reﬂected in all three layers’ features\nin that area of the clip. We also note that it appears as if the\nfeatures don’t exactly line up with the spectrogram, which\nis a result of including past data in our feature computa-\ntion. When the spectrum changes abruptly it takes several\nframes for our model to catch up. We do not see this as a\nlimitation as humans have a reaction time too, which per-\nhaps is reﬂected in the fact that these features are better\nsuited for time-varying emotion prediction.\nAt a normalized error of 0:128, our simple MLR\nmethod with DBN features outperforms two of the three\nfeatures investigated in our prior work with conditional\nrandom ﬁelds (CRFs) [19], which is a much more sophis-\nticated method. Furthermore, while the performance in-\ncrease between the third layer features of DBN-MF and\nDBN-UMB is small, the performance of the DBN model\nitself is reduced from 0:194 to0:140, which we ﬁnd to be\nhighly encouraging. These results indicate that UBM pre-\ntraining is providing us a model that is much better suited\nfor emotion ﬁnetuning.\nIn future work, we plan to investigate shrinking layer\nsizes in the UBM approach where we can perhaps take bet-\nter advantage of the dimensionality reduction power of the\nRBM. Furthermore, we see that it may be interesting to\ninvestigate multiple stages of ﬁnetuning. We would ﬁrst\nfollow the approach of [7] for reducing the dimensionality\nof unlabeled data. It may be possible to gain a more accu-\nrate UBM by applying a ﬁnetuning stage that involved un-\nraveling the model to reconstruct the unlabeled data. Those\nmodel parameters could then be adapted to emotion, or any\nother type of music prediction.\n8. ACKNOWLEDGMENT\nThis work is supported by National Science Foundation\naward IIS-0644151.\n9. REFERENCES\n[1] Y . E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton,\nP. Richardson, J. Scott, J. A. Speck, and D. Turnbull,“Music emotion recognition: A state of the art review,”\ninISMIR, Utrecht, Netherlands, 2010.\n[2] G. Tzanetakis, “Marsyas submissions to MIREX\n2007,” MIREX 2007.\n[3] J.-C. Wang, H.-Y . Lo, S.-K. Jeng, and H.-M. Wang,\n“Mirex 2010: Audio classiﬁcation using semantic\ntransformation and classiﬁer ensemble,” in MIREX,\n2010.\n[4] E. M. Schmidt, D. Turnbull, and Y . E. Kim, “Feature\nselection for content-based, time-varying musical emo-\ntion regression,” in ACM MIR, Philadelphia, PA, 2010.\n[5] E. M. Schmidt and Y . E. Kim, “Prediction of time-\nvarying musical mood distributions from audio,” in IS-\nMIR, Utrecht, Netherlands, 2010.\n[6] J. A. Russell, “A complex model of affect,” J. Person-\nality Social Psychology, vol. 39, pp. 1161–1178, 1980.\n[7] G. E. Hinton and R. R. Salakhutdinov, “Reducing the\ndimensionality of data with neural networks,” Science,\nvol. 313, no. 5786, pp. 504–507, July 2006.\n[8] E. M. Schmidt and Y . E. Kim, “Learning emotion-\nbased acoustic features with deep belief networks,” in\nWASPAA, New Paltz, NY , 2011.\n[9] E. M. Schmidt, M. Prochup, J. Scott, B. Dolhansky,\nB. G. Morton, and Y . E. Kim, “Relating perceptual\nand feature space invariances in music emotion recog-\nnition,” in CMMR, London, U.K., 2012.\n[10] H. Lee, Y . Largman, P. Pham, and A. Y . Ng, “Unsu-\npervised feature learning for audio classiﬁcation using\nconvolutional deep belief networks,” in NIPS. MIT\nPress, 2009.[11] P. Hamel and D. Eck, “Learning features from music\naudio with deep belief networks,” in ISMIR, Utrecht,\nNetherlands, 2010.\n[12] Y . E. Kim, E. Schmidt, and L. Emelle, “MoodSwings:\nA collaborative game for music mood label collection,”\ninISMIR, Philadelphia, PA, September 2008.\n[13] J. A. Speck, E. M. Schmidt, B. G. Morton, and Y . E.\nKim, “A comparative study of collaborative vs. tradi-\ntional annotation methods,” in ISMIR, Miami, Florida,\n2011.\n[14] D. Jiang, L. Lu, H. Zhang, J. Tao, and L. Cai, “Mu-\nsic type classiﬁcation by spectral contrast feature,” in\nProc. Intl. Conf. on Multimedia and Expo, vol. 1, 2002,\npp. 113–116.\n[15] S. Davis and P. Mermelstein, “Comparison of paramet-\nric representations for monosyllabic word recognition\nin continuously spoken sentences,” Acoustics, Speech\nand Signal Processing, IEEE Transactions on, vol. 28,\nno. 4, pp. 357–366, 1980.\n[16] G. E. Hinton, S. Osindero, and Y . Teh, “A fast learning\nalgorithm for deep belief nets,” Neural Computation,\nvol. 18, no. 7, pp. 1527–1554, 2006.\n[17] Y . Bengio, P. Lamblin, D. Popovici, and H. Larochelle,\n“Greedy layer-wise training of deep networks,” in\nNIPS. MIT Press, 2007.\n[18] E. M. Schmidt and Y . E. Kim, “Prediction of time-\nvarying musical mood distributions using Kalman ﬁl-\ntering,” in Proc. of the 9th IEEE Intl. Conf. on Ma-\nchine Learning and Applications (ICMLA), Washing-\nton, D.C., 2010.\n[19] ——, “Modeling musical emotion dynamics with con-\nditional random ﬁelds,” in ISMIR, Miami, FL, 2011."
    },
    {
        "title": "Score Analyzer: Automatically Determining Scores Difficulty Level for Instrumental e-Learning.",
        "author": [
            "Véronique Sébastien",
            "Henri Ralambondrainy",
            "Olivier Sébastien",
            "Noël Conruyt"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416518",
        "url": "https://doi.org/10.5281/zenodo.1416518",
        "ee": "https://zenodo.org/records/1416518/files/SebastienRSC12.pdf",
        "abstract": "Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often re- ly on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Com- ponents Analysis and compared to human estimations. Lastly we discuss the integration of this work to @- MUSE, a collaborative score annotation platform based on multimedia contents indexation.",
        "zenodo_id": 1416518,
        "dblp_key": "conf/ismir/SebastienRSC12",
        "keywords": [
            "huge sheet music collections",
            "public domain scores",
            "instrumental e-Learning",
            "Score Analyzer prototype",
            "MusicXML piece",
            "difficulty level extraction",
            "Musical Sign Base (MSB)",
            "technical instrumental difficulties",
            "Principal Components Analysis",
            "collaborative score annotation platform"
        ],
        "content": "SCORE ANALYZER: AUTOMATICALLY DETERMINING \nSCORES DIFFICULTY LEVEL  FOR INSTRUMENTAL \nE-LEARNING \nVéronique Sébastien, Henri Ralambondrainy, Olivier Sébastien , Noël Conruyt  \nIREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525  \nUniversity of Reunion  Island , Saint -Denis, Reunion (FRANCE)  \nveronique.sebastien/ henri.ralambondrainy /olivier.sebastien  \n/noel.conruyt@univ -reunion.fr  \nABSTRACT \nNowadays, huge sheet music collections exist on the \nWeb, allowing people to access public domain scores for \nfree. However, beginners may be lost in finding a score \nappropriate to their instrument level, and should often r e-\nly on themselves to start out on the chosen piece . In this \ninstrumental e-Learning context, we propose a Score \nAnalyzer prototype in order to automatically extract the \ndifficulty level of a MusicXML piece and suggest advice \nthanks to a Musical Sign Base (MSB). To do so, we first \nreview methods related to score performance information \nretrieval. We then identify seven criteria to characteriz e \ntechnical instrumental difficulties and propose methods to \nextract them from a MusicXML score. The relevance of \nthese criteria is then evaluated through a Principal Co m-\nponents Analysis and compared to human estimations. \nLastly we discuss the integration of this work to @-\nMUSE, a collaborative score annotation platform based \non multimedia contents indexation. \n1. INTRODUCTION \nIn the context of knowledge transmission, musical know-\nhow presents specific features to be efficiently preserved \nand shared. Indeed, to play correctly and nicely an i n-\nstrument, one should at the same time acquire physical \n(gestures, hands position, listening) and intellectual (m u-\nsic theory, score reading) skills. As such, conceiving a \nservice to preserve, transmit and share musical know-how \nis a complex issue, as we deal with both music hearing \nfaculties and artistic gestures production. \nWhile more and more instrumental e-Learning services \nare proposed to music amateurs (Garage Band1, \nSong2See2, iScore3), few of them aims at sharing instr u-\nmental know-how on a large scale. Therefore, we propose \nto build a Musical Sign Base (MSB), grounded on the \n                                                           \n1 http://www.apple.com/fr/ilife/garageband/  \n2 http://www.songquito.com/index.php/en/  \n3 http://rcmusic.ca/iscore -home-page Sign Management methodology [1], in order to collect \nannotated performances (personal interpretations or \nstances) each related to a given musical work (class). \nThis base can be used to compare various performances \nfrom music experts or students, and also to dynamically \nbuild new music lessons from the available content. To \nallow musicians to feed this base, we designed a colla-\nborative score annotation platform: @-MUSE \n(@nnotation platform for MUSical Education) . It allows \nusers to illustrate abstract scores (notation) with \ndia content depicting advices , exercises or questions \ndexed on the piece (annotation) [2]. However, learners \nmay want to be guided in their choice of a new piece to \nlearn, and to obtain rapidly some starting recommend a-\ntions to begin learning it on appropriate bases, before any \nteacher can annotate the piece. That is why , annotations \ncreated previously on similar pieces can be useful in this \nframe in order to depict basic information on the new \npiece. \nTo do so, we present in this paper a Score Analyzer \nprototype in order to automatically identify remarkable \nparts in a musical piece, from a performer viewpoint. For \nthe time being, we choose to concentrate on the piano for \nseveral reasons: the authors are pianists and work in co l-\nlaboration with piano and guitar experts from music co n-\nservatories, but also, the piano repertoire is extremely \nrich, both historically and technically . Indeed, we want \nour system to be able to manage not only basic know-\nhow, but also advanced one, on virtuoso instrumental \nworks. \nIn the first part of this work, we explore existing m e-\nthods to automatically extract musicological and techni c-\nal information from a digital score. For this knowledge to \nbe relevant to performers, we base this study on the needs \nof a pianist who would discover a new piece, following \nthe process generally used by piano teachers to introduce \na new work to their students. We then propose seven cr i-\nteria to characterize technical instrumental difficulties \nand give methods to extract them from a MusicXML \nscore. The relevance of these criteria is then evaluated \nthrough a Principal Components Analysis (PCA) and \ncompared to human estimations. Lastly we discuss the \nintegration of this work to @-MUSE, our collaborative \nscore annotation platform.  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee p rovided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012 International Society for Music Information Retrieval    \n \n2. MUSIC EDUCATION AND ARTIFICIAL \nINTELLIGENCE \nThe learning of an instrument generally consists in assi-\nmilating a basic repertoire to progress while enjoying \nplaying real artistic compositions instead of only repea t-\ning scales mechanically, which can be boring and dem o-\ntivating. Most of these technical points are directly dealt \nin the context of the considered pieces. This is why it is \nessential to select an appropriate corpus for the learner, \nand  to quickly detect remarkable technical points in o r-\nder to assimilate them, and then concentrate on higher-\nlevel considerations, such as expression and musicality. \nPointing such features is generally the job of the teacher, \nuntil the learner is able to detect them by himself (self-\nregulation) . In the frame of the @-MUSE project, our aim \nis to assist musicians in this procedure using descriptive \nlogics adapted to each piece genre (baroque, classical, \nromantic, etc) . Figure 1 details a generic model to i n-\nstance each descriptive logic. It is derived from how \nteachers introduce new pieces to their students [4]. To \nextract the different necessary information, we use the \nstandard MusicX ML format [3] which describes scores \nlogically, staff by staff, measure by measure, and lastly \nnote by note ( Figure 2). \nAs shown on Figure 1, the first step in our model \nconsists in placing the musical work in its context \n(composer, period, form metadata). In our frame, it can be done using metadata such as title or composer, present \nin the MusicXML file. In addition, specialized music web \nservices such as MusicBrainz1 or Last.fm2 can be queried \nto obtain more metadata to illustrate the piece (for \ninstance, a portrait and biography of the composer, or an \nindication about the piece style). Several performances of \nthe piece can be retrieved from video sharing websites in \norder to get a glimpse of how the piece should sound. \nThe second step is to analyze the global form of the \npiece. Most information about it exists within the piece \ntitle (i.e.: Sonata, Fugue, etc.). The challenge is thus to \ndetect the main parts of the piece which characterize its \nform (i.e.: Introduction, part 1, part 2, Coda). Indeed, \ngrasping its structure is essential to performers, as each \npart may sound totally differently (especially on a d-\nvanced pieces). In our frame, this also enables a better \nindexation for annotations. To achieve that goal, we pr o-\npose to rely on some of the characteristic tags within th e \nMusicXML file. Indeed, score symbols such as direction \ntexts (e.g. “meno mosso”), tempo and key modifications, \ndouble bars generally indicate the beginning of a new part \nwithin the piece. While this method seems quite \"naïve\", \nit gives acceptable results most of the time. Some exce p-\ntions may occur\n, especially on contemporary pieces, \nwhich present unconventional structures. \nAfter indicating main parts of the piece, the teacher \ngenerally brings the attention of the learner on the r e-\nmarkable rhythmic or harmonic patterns the piece is build \non (if any), leading to more technical and detailed pra c-\ntice. In our work, discovering predefined patterns such as \nscales, arpeggios or trills may be done using a memory \nwindow of successive intervals. Indeed, scales will cor-\nrespond to sequences of ascendant or descendant seconds, \narpeggios to sequences of triples, etc. Each detected pa t-\ntern can then be linked to a generic annotation explaining \nhow to work on it. However, detecting more complex and \n                                                           \n1 http://musicbrainz.org,  visited on the 10/04/2012.  \n2 http://www.last.fm, visited on the 10/04/2012.  \n \n \nFigure 1. Generic model for musical pieces descriptive logics  \n \n \nFigure 2. Musical score logical structure  \n   \n \nnon-determined patterns remains a challenge, as it does \nnot only involve rhythms and pitch features, but also p o-\nlyphonic ones. Moreover, it does not present a unified \ndefinition of “similarity”. Two fragments can be cons i-\ndered as “similar”, without having the same pitches, but \nby possessing similar intervals (transposition). Several \nworks exist on Musical Pattern Discovery. Among them, \n[5] presents a method based on time windows and define \ndifferent types of patterns (abstract patterns, prefixes, pa t-\nterns network). Still, each suggestion given by our system \ncalls for a validation by a music professional. \nIn order to semantically annotate the detected stru c-\ntures, we need a musical form ontology. While the Music \nOntology [6] is particularly fitted to the music industry, it \nlacks some concepts to be effective in music education. \nMore specialized ontologies exist, such as the Symbolic \nMusic Ontology (allowing to manipulate Voices and M o-\ntifs concepts), the Chord Ontology or the Neuma ontol o-\ngy (for Gregorian Music) [7], however, a real form ta x-\nonomy has yet to be built to manage the manipulation of \nconcepts such as Sonata, Fugue, Theme or Coda. \nThe last step of our introduction lesson is to underline \nspecific difficulties of the piece. This will allow us to both specify the global level of the piece, and to detect its \ntechnical difficulties measure by measure. To do so, we \npropose in what follows seven criteria to evaluate a piano \npiece difficulty. \n3. CRITERIA DEFINITION AND RETRIEVAL \nIn Table 1, we propose seven criteria affecting the \nlevel of a piece for the piano and detail how they can be \nestimated from a MusicXML file. These criteria were d e-\nfined on the base of pianists experiences, both profe s-\nsionals and amateurs. They may be applied to other i n-\nstruments with some adaptations (see Instruments column \nin Table 1). Globally, a piano piece difficulty depends on \nits tempo, its fingering, its required hand displacements, \nas well as its harmonic, rhythmic and polyphonic fea-\ntures. Although we define each criteria separately, they \naffect each other in a complex manner. In particular, fi n-\ngerings remain hard to extract from a score, as most M u-\nsicXML files do not contain this information. Indeed, \nwhile other criteria reside in the basic notation layer \n(notes pitch and duration), the fingering is from the ann o-\ntation layer and directed at humans only (human perfo r-\nmance information). \nPerformance diff i-\nculty criterion  Definition  MusicXML implementation  Instruments  \nPlaying speed  The required fingers velocity to p lay the piece. Depends on the \ntempo and the shortest significant note value (i.e. a piece presen t-\ning a high tempo may contain only long values, and conversely, a \npiece with a low tempo may contain groups of short notes thus \nincreasing the required fingers agility for the players)  <note><type>  elements \nTempo attribute in <sound> element \nAll \nFingering  Fingering: choice of finger and hand position on various instr u-\nments. Different notations exist  according to the instrument. (e x: \nin piano: 1 = thumb, 2 = inde x finger, 3 = middle finger, etc.)  \nCost functions are used on intervals to extract the general finge r-\ning difficulty level  \nSee [8][8][9] for more detail.  <fingering>  element within each \n<note> element \n All, requires ada p-\ntations in co n-\nstraints and costs \nfunctions (some \ninstruments do not \nuse thumbs)  \nHand Displacement  Ratio of hands displacements greater than an octave (12 s emi-\ntones). Depends on the duration of the interval: if the duration \nexceeds 2 beats (i.e. 2 quarters in 4/4, 2 eights in 6/8), the di s-\nplacements is not considered as difficult. The difficulty degree of \nthe displacement evolves with its size (in pitch), it s duration and \nits fingering  Combined <note> elements where \n<pitch> gap > 12 and <duration>  \ngap < 2 beats  \n All, requires ada p-\ntations depending \non the instrument \nmorphology  \nPolyphony  Chords ratio (aggregate of musical pitches simultaneously  at-\ntacked) \nPolyphonic difficulties may increase with the number of notes \nplayed at the same time and their fingerings.  \nSimultaneous voices (in a Fugue for instance) constitute special \ncases of polyphonic difficulties to treat.  <chord> element \n All (except for m o-\nnophonic instru-\nments, such as the \nflute) \nHarmony  Ratio of differences from the piece main tonality. Characterized \nby the amount of accidental alterations.  <alter> and <accidental>  elements \n All \nIrregular Rhythm  Ratio of irregular p olyrhythm s (simultaneous sound ing of two or \nmore independent rhythms). Example : synchronizing a triplets \nover duplets  <time-modification>  element \n All (except for m o-\nnophonic instr u-\nments) \nLength The number of pages of the score. May also be measured in bars \nnumber to avoid dependency to the page layout.  new-page attributes or  <measure>  \nelements All \nTable 1. Performance difficulty criteria in piano practice  \n   \n \nSeveral works present methods to automatically d e-\nduce fingerings on a given musical extract for piano \n([8][9][10]). Most of them are based on dynamic pr o-\ngramming. All possible fingers combinations are genera t-\ned and evaluated, thanks to cost functions. The latter are \ndetermined by kinematic considerations. Some functions, \neven consider the player’s hand size to adjust its results . \nThen, expensive (in term of effort) combinations are su p-\npressed until only one remains, which will be displayed \nas the resulting fingering. While the result often differs \nfrom a fingering determined by a human expert, it r e-\nmains largely playable and exploitable in the frame of an \neducational usage. However, few algorithms can process \npolyphonic extracts, and many other cases are ignored \n(i.e., left hand, finger substitutions, black and white keys \nalternation).  \nEven if more work is needed on this issue, the use of \ncost functions remains relevant as it is close from the \nprocess humans implicitly apply while working on a mu s-\nical piece. Therefore, we use this method in our Score \nAnalyzer prototype to translate extracted criteria into di f-\nficulty indicators (see part 5). But to do so, we need to \nstudy how our criteria discriminate a corpus of piano \npieces, both objectively (through a components analysis) \nand subjectively (based on pianists experience ). \n4. PIANO SCORES CORPUS CLUSTERING \nTo study how our criteria discriminate scores, we re a-\nlized a PCA on a sample of fifty piano pieces ( Figure 5). \nThe pieces were selected to be representative of a classic-\nal piano cursus in a French Music Conservatory. Most \npieces concern intermediate to advanced players, fewer \ntarget beginners and virtuosi. Most MusicXML files were \nretrieved from online music notation communities such as \nMuseScore.com, Noteflight or the Werner Icking Music \nArchives. Some were generated from PDF files using the \nSmartScore™ OCR software. \nThe criteria defined in Table 1 were extracted on each \npiece. Displacements, chords and harmonic characteri s-\ntics are distinguished whether they occur on the right \n(RH) or the left hand (LH). Fingerings were not exploited \nfor the time being as work is in progress to deduce them \nfrom MusicXML files (see part 3). Our analysis thus \ncounts 9 numeric variables ( Figure 4), and 1 nominal v a-\nriable (composer). Each ratio is calculated on the base of \nthe total number of notes (e.g. harmonic criteria), or the \ntotal number of hands positions (e.g. displacements, \nchords) within the piece. A displacement is thus defined \nas a pair composed of two successive hand positions. \nA correlations study (Figure 3 ) points out some links \nbetween variables. Some are musically natural (i.e. ha r-\nmonyLH and harmonyRH, harmonic characteristics co n-\ncern both hands). We also note a strong correlation (81%) \nbetween chordsLH and displacementsLH. This value \ncould characterize accompaniments presenting an altern a-\ntion of a low-pitched bass and a middle or high-pitched \nchord, thus inducing regular large displacements and chords at the left \nhand (ragtime , \nwaltz). Lastly, \nthe piece length \ncan be linked to \nits playing speed, \nwhich charact e-\nrizes advanced \nand virtuosi \nworks, deman d-\ning an important \nfingers velocity \non a long dur a-\ntion (stamina). \nThe PCA then gives an optimal projection of each \npiece in the 2D space of the first principal components . \nFigure 5 presents this projection as well as the three \nclasses detected by the analysis. This clustering was re a-\nlized through a hierarchical clustering using the Ward’s \nmethod [11] on the first few principal components. The \nresulting tree is then cut according to its corresponding \nindices, in order to find an appropriate number of clu s-\nters. Lastly, this clustering is consolidated using a k-\nmeans algorithm. The first interpretation of these three \nclasses validates the relevance of our criteria to estimate \nthe difficulty level of a piano piece. Indeed, we notice \nthat at least two of the classes naturally regroup pieces \naccording to their level (class 1 and 2). A further observ a-\ntion backed by a Student test (variable means compar i-\nsons between the whole population and the clusters ) gives \na better interpretation of the classes. Class 1 mostly r e-\ngroups pieces addressed to beginners ( Kinderszenen , \nSchumann’s Choral) and to intermediate musicians \n(Bach’s Invention , Sonatines ). The Student test confirms \nthis tendency, as most variables remain below average for \nthis class: few chords, displacements and pages, simple \nharmonies (C major or A minor). Yet, the tempo remains \nlively. Rhythmic difficulties are noticeable on interm e-\ndiate pieces. They generally feature characteristic rhyt h-\n \nFigure 3. Variables correlation map  \n \n \nFigure 4. Student test (means comparison)  \n   \n \nmic patterns which constitute interesting educational m a-\nterial (e.g. 1st Arabesque by Debussy ). Class 2 contains \nadvanced to virtuoso works (Chopin’s Etude, Ravel’s \nToccata), featuring a vivid tempo, large and numerous \ndisplacements on the keyboard, a complex harmony and \nmany chords. We also note some borderline individuals \n(The Little Negro  by Debussy, or th e 2nd Gymnopédie  by \nSatie), which could be considered as beginner pieces but \nstill present uncommon harmonic and rhythmic stru c-\ntures, thus being hard to classify objectively. Class 3 \nseems to regroup pieces featuring a left hand playing a \n“bass+chord” accompaniment (ragtime, waltz, cakewalk). \nThe level of most pieces is intermediate. Indeed, the St u-\ndent test indicates that despite the high ratio of displac e-\nments and chords, the low tempo and the simplicity of the \nharmonies compensate for it. As such, this particular \nclass is also representative of specific musical genres. \nThis clustering serves as a complement to the “bounds ” \napproach used in Score Analyzer. \n5. SCORE ANALYZER PROTOTYPE \nThe criteria presented in the previous sections have \nbeen implemented in a Web application called Score \nAnalyzer1 (SA). This module is integrated to the \n@-MUSE platform as a Web service in order to automa t-\nically evaluate a piece level and identify its difficult parts. \nThe SA engine takes any well-formed MusicXML file as \ninput and parses it to extract knowledge exploitable from \na performer point of view. Following the scheme we d e-\ntailed previously ( Figure 1), the context of the piece is \nbriefly analyzed (title, composer) and a few statistics are \n                                                           \nhttp://e-piano.univ-reunion.fr/tests/ScoreAnalyser/readScore.php ,  \nvisited on the 05/06/2012 , beta version.  displayed. Then, main parts of the piece are identified, \nand lastly, difficulty estimations are given for each crit e-\nrion, using a mark from 1 (beginner/easy) to 4 (virtuoso). \nA mean is also calculated to give a global appreciation of \nthe piece difficulty. This allows a better readability of the \noutputs for musicians. For each criterion, bounds were \ndefined with the help of teachers: for instance, a chord \nratio under 10% corresponds to the mark 1, while a di s-\nplacement ratio above 20% corresponds to a 4. These \nbounds determination was transparent for teachers, as \nthey were simply asked to rate each criteria from 1 to 4 \non a training corpus. The given marks were then corre-\nlated to the ratio extracted on each piece, in order to cal i-\nbrate average bounds corresponding to the difficulty l e-\nvels felt by musicians. Thus, we notice that most of the \ncriteria do not have a linear distribution, which const i-\ntutes a pianistic reality. The synchronization between \nboth hands is also taken into account. For instance, if \neach hand obtains a mark of 2 for the displacements crit e-\nrion, then the global difficulty mark for this criterion will \nbe 3, as synchronizing both hands will create an additio n-\nal difficulty.  \nAs such, we define this method as “semi-objective”. \nIndeed, score level estimation can never be a totally o b-\njective task: players will judge a piece differently accor d-\ning to their taste, level or background. Therefore, we use \ntwo distinct methods to validate SA estimations. The first \none consists in confronting it to the clustering obtained \nthrough the PCA described in the previous part. This is \nthe “objective” validation. The second one simply co n-\nsists in confronting SA results to pianists estimations \n(“subjective” validation). To facilitate the comparisons , \nwe merged advanced and virtuosi pieces into the same \nclass within SA . The contingencies table ( Table 2) allows \nto better visualize the differences between the PCA and \n \nFigure 5. Individuals projection on the PCA first two axes  and corpus details  \n   \n \nScore Analyzer’s results. While they seem numerous , on-\nly one is a major disagreement (3/1 marks on Beethoven \nSonata in F). The other distinctions, especially the inte r-\nmediate/beginners ones, may be due to the fact that h u-\nmans balance criteria whereas the PCA considers each of \nthem of equal importance. Therefore, we noticed that for \npianists, an increase of the displacement ratio raises the \npiece level much faster than other criteria. Moreover, as \nstated in the previous part, the clustering given by the \nPCA is also affected by the musical genre of the piece. \nHumans do not tend to be affected by this metadata, even \nif some genres are naturally associated with higher levels \n(i.e. impressionist or contemporary music).  \nFor the “subjective” evaluation, we asked three piano \nteachers to estimate the difficulty level of each piece by \nattributing it a mark between 1 and 3. No criteria were \nimposed. When opinions differ, the final mark is picked \naccording to the majority. The results given in Table 3 \nshow a better correspondence between SA estimations \nand human ones, which reinforces the “bounds” method \ndefined previously. The main difference consists in und e-\nrestimations from SA, especially on advanced pieces . In-\ndeed, pianists also take expression and musicality diff i-\nculties into account, while our system only consider tec h-\nnical difficulties. Therefore, this study leads us to pursue \nour work by expanding the set of criteria to improve our \nestimations. \n6. CONCLUSION \nIn this paper, we proposed an automatic Score Analyzer \nto determine the difficulty level of piano pieces. This pr o-\ntotype is based on seven criteria characterizing technical \nfeatures of a piano piece: playing speed, fingerings, hands displacements, polyphony, harmony, rhythm and length. \nWe thus proposed methods to extract these criteria from a \nMusicXML scores, and realized a PCA to validate them. \nThis analysis permitted to establish three classes among a \ncorpus of fifty selected piano pieces. These classes were \nthen confronted to Score Analyzer estimations, which are \ntuned according to piano teachers expertise. \nImprovements on this work include the integration of \nfingering related difficulties, but also the adaptation to \nstudents levels. Indeed, the sense of difficulty within a \nmusical work is mostly dependent from the musician’s \nbackground. We thus imagine a weighting system to pe r-\nsonalize our analysis. We also intend to implement local \nanalysis (by measures ) in order to identify specific diff i-\ncult parts. The criteria decomposition would then allow to \nextract the main cause of the difficul ty and thus link it to \nan annotation created on the @-MUSE platform. Other \nperspectives include integration of “expressive” criteria \n(emotions, nuances, rubato, attacks), as well as adapt a-\ntions and tests on scores for different instruments. \n7. REFERENCES \n[1] V. Sébastien, D. Sébastien, N. Conruyt, \"Constituting a \nMusical Sign Base through Score Analysis and \nAnnotation\", International Journal On Advances in \nNetworks and Services , No 3&4, pp. 386-398, 2011.  \n[2] M. A. Winget: “Annotations on musical scores by \nperforming musicians: Collaborative models, interactive \nmethods, and music digital library tool development”, \nJournal of the American Society for Information Science \nand Technology , 2008. \n[3] G. Castan, M. Good, and P. Roland: “Extensible Markup \nLanguage (XML) for Music Applications: An \nIntroduction”, The Virtual Score: Representation, \nRetrieval, Restoration , MIT Press, pp. 95-102, 2001.  \n[4] M. W. Camp: Teaching piano: the synthesis of mind, ear \nand body, Alfred Music Publishing, 1992. \n[5] O. Lartillot: “Une analyse musicale automatique suivant \nune heuristique perceptive”, 3ème Conférence \nInternationale Francophone sur l’Extraction et la Gestion \ndes Connaissances , EGC 03, Lyon, 2003. \n[6] Y. Raimond, S. Abdallah, M. Sandler, and F. Giasson: \n“The Music Ontology”, Proceedings of the International \nConference on Music Information Retrieval , ISMIR, 2007. \n[7] P. Rigaux: “Neuma Ontology Specification”, Project \nNeuma Report , Lamsade-CNRS, ANR-08, 2008. \n[8] C.-C. Lin: “ An Intelligent Virtual Piano Tutor ”, National \nChung Cheng University 2006. \n[9] A. Al Kasimi, E. Nichols, and C. Raphael, “A simple \nalgorithm for automatic generation of polyphonic piano \nfingerings”: 8th International Conference on Music \nInformation Retrieval , Vienna, 2007. \n[10] R. Parncutt, J. A. Sloboda, M. Raekallio, E. F. Clarke, and \nP. Desain: “An Ergonomic Model of Keyboard Fingering \nfor Melodic Fragments”, Music Perception: An \nInterdisciplinary Journal , Vol. 14, No. 4, 1997, pp. 341-\n382.  \n[11] J. H. Ward: “Hierarchical Grouping to Optimize an \nObjective Function”, Journal of the American Statistical \nAssociation , No. 48, pp. 236–244, 1963. \n \nTable 3. Contingencies t able between SA and teachers  marks \n \n \nTable 2. Contingencies table between SA and PCA marks"
    },
    {
        "title": "On Measuring Syncopation to Drive an Interactive Music System.",
        "author": [
            "George Sioros",
            "Andre Holzapfel",
            "Carlos Guedes"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416194",
        "url": "https://doi.org/10.5281/zenodo.1416194",
        "ee": "https://zenodo.org/records/1416194/files/SiorosHG12.pdf",
        "abstract": "In this paper we address the problem of measuring synco- pation in order to mediate a musically meaningful interac- tion between a live music performance and an automati- cally generated rhythm. To this end we present a simple, yet effective interactive music system we developed. We shed some light on the complex nature of syncopation by looking into MIDI data from drum loops and whole songs. We conclude that segregation into individual rhythmic layers is necessary in order to measure the syn- copation of a music ensemble. This implies that measuring syncopation on polyphonic audio signals is not yet tractable using the current state-of-the-art in audio analysis.",
        "zenodo_id": 1416194,
        "dblp_key": "conf/ismir/SiorosHG12",
        "keywords": [
            "syncopation",
            "musical interaction",
            "live music performance",
            "automatically generated rhythm",
            "interactive music system",
            "MIDI data",
            "rhythmic layers",
            "audio analysis",
            "tractable",
            "current state-of-the-art"
        ],
        "content": "ON MEASURING SYNCOPATION TO DRIVE AN \nINTERACTIVE MUSIC SYSTEM \nGeorge Sioros  André Holzapfel  Carlos Guedes  \nFaculdade de Engenharia \nda Universidade do Porto \ngsioros@gmail.com  Music Technology Group,  \nUniversitat Pompeu Fabra  \nhannover@csd.uoc.gr  Faculdade de Engenharia \nda Universidade do Porto \n/ INESC Porto  \ncguedes@fe.up.pt  \nABSTRACT \nIn this paper we address the problem of measuring sync o-\npation in order to mediate a musically meaningful intera c-\ntion between a live music performance and an automat i-\ncally generated rhythm. To this end we present a simple, \nyet effective interactive music system we developed. We \nshed some light on the complex nature of syncopation by \nlooking into MIDI data from drum loops and whole \nsongs. We conclude that segregation into individual \nrhythmic layers is necessary in order to measure the syn-\ncopation of a music ensemble. This implies that measuring \nsyncopation on polyphonic audio signals is not yet tractable \nusing the current state- of-the-art in audio analysis.  \n1. INTRODUCTION \nRhythmic syncopation is an essential notion both in ana-\nlyzing and characterizing music as well as in automatica l-\nly generating musically interesting rhythmic performan c-\nes. It is commonly related to rhythmic complexity and \ntension. Several operational and formal definitions of \nsyncopation have been given (see [1] , [2]), such as the \none found in the New Harvard Dictionary of Music  which \ndescribes syncopation as a temporary contradiction to the \nprevailing meter.  \nVarious syncopation metrics have been reported (see \n[3], [4]), however, a reliable computational model that \ncan measure syncopation directly on an actual music pe r-\nformance does not yet exist . Most metrics use binary re p-\nresentations as input and disregard information contained \nin the amplitudes of events. However, music performan c-\nes are usually captured as audio signals or MIDI events, \nand in both cases the amplitudes of events play an i m-\nportant role in rhythm perception. A new syncopation \nmeasure was recently reported by Sioros and Guedes [5] \n(referred to as SG henceforth ) that considers the ampl i-\ntude of events can be applied to obtain a more detailed \nrepresentation of rhythm. This  kind of representation is \ncloser to an actual music signal; it resembles a mon o-phonic real time strea\nm of MIDI events. \nWe aim to develop a system that uses syncopation to \nmediate the interaction between a musician performing \nlive and an automatic rhythm generator. To th is end, we \nexplore the difficulties in measuring syncopation in a live \nmusic performance . The current study focuses on measu r-\ning syncopation in MIDI streams, from which we draw \nconclusions on how to measure syncopation in audio si g-\nnals. \nWe examined the difficulties in measuring syncopation \non rhythmic patterns derived from multichannel, multi-\ntimbre MIDI streams by analyzing two datasets, one \ncomprised of short drum-loops and the second of whole \nsongs in various genres . We used the Longuet-Higgins \nand Lee’s metric [6] (referred to as LHL) as it is well-\nknown and shows good agreement with human judgments \n([3], [7]). We conclude that the segregation of the instr u-\nments in the performance is needed to obtain meaningful \nsyncopation measurements. A comparison between the \nSG and the LHL metrics was performed, which shows \nagreement between the two measures, and deviations that \ncan be attributed to processing the amplitude information \nin the SG metric. \nFinally, we developed a software system that maps real \ntime syncopation measurements to aspects of a rhythmic \nperformance automatically generated by the \nkin.rhythmicator software [8]. The measurements are pe r-\nformed on either audio or MIDI inputs, as long as they \nare the result of a single instrument. The system serves as \na tool for exploring, designing and creating interactive \nmusic performances.  \nIn Section 2, we describe the two syncopation metrics \nused in the current study. In Section 3, a small study on \nsyncopation follows, where the two MIDI datasets are e x-\namined and a comparison between the two metrics is \nmade. In Section 4, we describe the interactive music sy s-\ntem we have developed. \n2. SYNCOPATION MEASUREMENTS \n2.1 Binary representations of rhythms. \nThe computation of syncopation using the LHL alg o-\nrithm [6] is based on a hierarchical metrical structure co n-\nstructed by stratifying the given meter into metrical lay-\ners. The structure can be represented as a tree diagram  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made  or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.   \n© 2012 International Society for Music Information Retrieval    \n \n with the whole bar at the top and the lower metrical levels \nunder it. The exact form of the tree depends on the time \nsignature of the meter. For a 4/4 meter, the bar is subd i-\nvided first into half notes, then quarter notes, eighth notes \netc.  until a level is reached which represents the shortest \nnote value to be considered. In this way the meter is sub-\ndivided into pulses that belong to different metrical le v-\nels,. In the LHL measure, each pulse is assigned a me t-\nrical weight according to the metrical level it belongs to, \nstarting with 0 for the whole bar level, 1 for the half \nnote, 2 for the quarter note, and for each following level \nthe weights are further lowered by 1. While in many stu d-\nies, e.g.[7], the lowest level chosen is the 8th note, for a p-\nplications involving actual music data at least the 16th \nnote level is necessary. In the current study, all syncop a-\ntion measurements are done on rhythmic patterns in 4/4 \nstratified to the 16th note level, however, in the examples \ngiven in this section we only show metrical levels down to \nthe 8th note level for visual clarity .  \n Given a monophonic note sequence, we can compute a \nsyncopation score based on the weights of the metrical \npositions of the note s and the rests. Note durations are \nignored. A score is assigned to each rest that is the diffe r-\nence between the weight for that pulse minus the weight \nfor the preceding note event. Summing all scores yields \nthe syncopation value for the sequence. Some examples \nare given in Table 1. We placed the syncopation scores \nassigned to the rest-note combinations at the metrical p o-\nsitions of the rests . The depicted sequences are consi d-\nered to wrap around as loops . In Example 2 we get a sy n-\ncopation score of (0(3))+(2(3))+(2(3))= 3 + 1 + 1 = 5.  \n A closer look at these examples reveals, in certain \ncases, that the results of the algorithm contradict our m u-\nsical intuition . Example 1 receives a value of 0 since it \ncontains no rests. Example 3, however, also receives a \nsyncopation of 0, against our experience that it is more \nsyncopated than Ex\nample 1. This arises because negative \nscores compensate positive scores: (-3-0) + ( -1- (- 3)) + \n(- 2- (- 3)) = 0. We note that summing only positive \nscores in Example 3 would yield a positive syncopation \nvalue. The negative values computed by the LHL alg o-\nrithm negatively correlate with what could be referred to \nas metrical strength: while the sequence of 8th notes in \nExample 1 has a neutral score, Example 4 supports the beat more strongly , as indicated by the larger negative \nvalues. However, since we are mainly interested in reli a-\nbly detecting the syncopation of a bar , we can sum only \nthe positive scores (last column) in Table 1.  \nIn the present study, the results of the algorithm are \nnormalized by the maximum possible score—one less \nthan the total number of pulses in a bar—in order for the \nresults to be independent of the number of pulses in the \nbar and the lowest stratification level chosen. The norma l-\nized syncopation will be referred to as NLHL- p. \n Since the LHL algorithm was proposed for individual \nmonophonic sequences, we need a method to compute \nsyncopation when several layers of events take place si m-\nultaneously. This is the case of multiple instruments in a \nmusic ensemble, where a different rhythmic pattern might \nbe performed on each instrument. The overall syncop a-\ntion of the performance depends on how syncopated each \none of the patterns is. We will explain the applied method \nby examining the four rhythmic patterns of Table 1 as if \nthey were the four parts of a quartet. To obtain an overall \nsyncopation measure for the polyphonic sequence we \ncombine the maximum values for each metrical position \nand sum them . This results in a syncopation value of \n7/7=1 for our example (last row of Table 1). Note that \nthis polyphonic syncopation value can exceed the value of \n1, which follows our musical intuition that the syncop a-\ntion of a combination of instruments can be higher than \ntheir individual maximum values. This polyphonic sync o-\npation will be referred to as POLYSYNC in the following \nsections. \n2.2 Sequences of amplitude values. \nWe now provide an overview of the syncopation measure \nproposed by Sioros & Guedes in [5] (SG). This algorithm \ncan be applied to a more complex representation of \nrhythmic patterns which, in addition to the metrical pos i-\ntions of the events, also includes their amplitudes. We a l-\nso discuss the advantages of this kind of representation \nover a binary one with regard to measuring syncopation. \nAs in the case of the LHL algorithm described above , \nthe SG syncopation measure is also based on a hiera r-\nchical model of musical meter. It compares a sequence of \namplitude values to a metrical template similar to the one \ndescribed in [9] . The algorithm is performed in two pha s-\n pulses / sequences  weights / scores  syncopation  \n4/4 in \n  1 2 3 4 5 6 7 8 0 -3 -2 -3 -1 -3 -2 -3 LHL LHL-p  \nExample 1 \n \n \n \n \n \n \n \n         0 0 \nExample 2 \n \n \n \n \n \n \n \n 3  1    1  5 5 \nExample 3 \n \n \n \n \n \n \n \n  (-3)   2  1  0 3 \nExample 4 \n \n \n \n \n \n \n \n  (-3)  (-1)     -4 0 \ncombined  3  1  2  1   7 \nTable 1: Computation of the LHL syncopation metric on 4 example sequences. LEFT: the four sequences in binary \nform. RIGHT, the corresponding weights and scores of the pulses.  The negative scores are shown in parentheses as they \nare ignored in our LHL -p measure.    \n \n es. First, it tries to identify loud events that do not occur \nregularly on the beat in any metrical level. These isolated \nevents contribu te to the overall syncopation feel of the \npattern. The second phase of the algorithm scales this \ncontribution according to the potential of each metrical \nposition to produce syncopation . The algorithm is pe r-\nformed in five steps ( Figure 1). The first phase includes \nsteps 1 to 3 and the second phase, steps 4 and 5. We will \ndemonstrate the algorithm by calculating step by step the \nsyncopation of pulse 5 in Figure 1.  \nIn the first phase, the events that occur regularly on \nsome metrical level are eliminated. Step 1 consists of d e-\ntermining the metrical levels each pulse belongs to, a c-\ncording to the time signature of the meter. In the example \nof Figure 1, pulse 5 belongs to the half note metrical level \n(level 1), as well as to all lower ones, i.e. the quarter (2 ) \nand eighth (3) note levels. In step 2 the amplitude diffe r-\nences are taken from the neighboring events and are ave r-\naged in pairs for each metrical level. The corresponding \namplitude differences and averages for pulse 5 would be: \ni) at the half note metrical level, pulses 5 – 1 in the cur-\nrent bar and 5 – 1 in the next bar; ii) at the quarter note \nlevel, pulses 5 – 3 and  5 – 7; and iii) at the eighth note \nlevel, pulses 5 – 4 and 5 – 6. In step 3, the lowest value of \nthe calculated averages is taken as the syncopation score \nof the pulse. If the amplitudes of the events in pulses 1, 5 \nand 7 of the example are 0.5, 1.0 and 0.5, then the three \naverages are 0.75, 0.75 and 1. Taking the minimum, the \nsyncopation score of pulse 5 is 0.75. \n The second phase of the algorithm (steps 4 and 5) is \nneeded to account for the fact that not all the metrical p o-\nsitions have equal potentials to contradict the prevailing \nmeter: the higher the metrical level the lower its syncop a-\ntion potential. In step 4 the syncopation potentials are ca l-\nculated for each pulse as 1-0.5m, where m is the highest \nmetrical level the pulse belongs to . In step 5, the syncop a-\ntion score for each pulse is multiplied by the correspon d-\ning syncopation potential. For pulse 5 of the example m = \n1 and the final syncopation is 0.75 x (1-0.51) = 0.375. The \nfinal result is calculated as the sum of the syncopation of \nthe individual events and is further normalized to the maximum possible syncopation for the same number of \nevents in the bar. This maximum corresponds to a pattern \nwhere all events are place d at the lowest metrical level \nand with amplitude equal to 100%. \nThe two syncopation measures used in this article have \nan important difference. The SG algorithm is applied to a \nmore detailed representation of the rhythmic patterns that \nincludes the amplitudes of the events. This makes it po s-\nsible for the SG algorithm to measure syncopation in \ndrum rolls or arpeggios where events are present in all \nmetrical positions and the syncopation arises from accents \nin offbeat positions.  \n3. A SMALL STUDY ON SYNCOPATION \n3.1 Methodology \nWe applied the NLHL-p and the SG  algorithm to two dif-\nferent kinds of MIDI datasets. The MIDI data was i m-\nported and quantized to the 16th note metrical grid. Syn-\ncopation measurements using the SG algorithm were o b-\ntained from sequences of amplitudes derived by the MIDI \nnote velocities. When more than one note event was \nfound at the same metrical position, the one with highe st \nvelocity was kept. The first dataset, which will be referred \nto as the Loops-dataset, consists of 602 drum loops fro m \nthe following genres: Rock, Funk and Disco/Dance. The \nsecond dataset, which will be referred to as RWC36, co n-\nsists of the first 36 songs from the RWC Music Genre1 \ndataset that belong to genres of Western popular music. \nIn contrast to the Loops-dataset, the RWC dataset co n-\ntains whole songs, with each instrument par t found in a \ndifferent MIDI track. All loops and songs examined in \nhere belong to the 4/4 meter, as comparing results b e-\ntween rhythms in different meters is a difficult and une x-\namined topic which is outside of the scope of this paper.  \n We used the algorithms to analyze the differences in \nthe musical genres and instruments in terms of syncop a-\ntion. This analysis reveals which of the examined genres \nmake most use of rhythmic syncopation, as well as how \nthis syncopation is distributed among the various instr u-\nments and sections of the songs. It serves as a first t o-\nwards understanding how syncopation could be measured \nin audio signals. It must be noted that the results cannot \nbe evaluated against a grou nd-truth, as there is no sync o-\npation ground-truth available for any music dataset. In-\nstead, we verified that the results are consistent with what \nis known about and expected for the syncopation in \nWestern popular music. The same qualitative results were \nobserved for both algorithms, so we restrict the represe n-\ntation of the results in sections 3.2 and 3.3 to the NLHL- p \nalgorithm. In section 3.4 we make a more detailed co m-\nparison of the two measures. \n                                                           \n1 http://staff.aist.go.jp/m.goto/RWC-MDB/  \n \nFigure 1:  Example of the SG algorithm. Calculation of \nthe syncopation of the 2nd half note in a 4/4 meter (pulse 5).     \n \n 3.2 Loops-dataset \n The Loops-dataset contains only short MIDI drum-loops \nof a few bars that use only the general MIDI drums set \nsounds. We measured the syncopation in every bar found \nin the MIDI files of the Loops-dataset, by applying \nNLHL-p algorithm to each bar separately, as if it consti-\ntuted an independent \nloop. We were able to obtain a large \nnumber of syncopation measurements for three musical \nstyles: Dance/Disco (198 bars), Funk (286 bars) and \nRock/Heavy (484 bars). The histograms of the measured \nsyncopation values are depicted in Figure 2. In the upper \nparts of the figure, the measurements were performed on \nthe complete group of the general MIDI sounds, in effect \nignoring MIDI note numbers. In this case, the Di s-\nco/Dance genre appears to be almost totally un-\nsyncopated. While Rock and Funk appear slightly more \nsyncopated, they still seem to contradict our expectations \nfor higher syncopation. If we examine the rhythmic pa t-\nterns of the bass-drum/snare-drum pair separately, igno r-\ning all other drum sounds, we get more meaningful results \nas shown in the lower part of Figure 2. These histograms \nshow an increasing percentage of syncopated bars from \nDisco/Dance to Rock/Heavy to Funk, as expected fr om \nthese styles. This is a first indication towar ds the more \ngeneral conclusion of this study that syncopation needs to \nbe measured in the individual rhythmic patterns that co m-prise a musical performance, implying that at least a basic \nsource/instrument separation is necessary. \n3.3 RWC-dataset \nIn contrast to the Loops-dataset, the RWC-dataset co n-\ntains complete songs, with several instruments, each in its \nown MIDI track. We computed the NLHL- p syncopation \nmeasure for each track separately and combined the most \nsyncopated events to compute the overall syncopation of \nthe ensemble, using the POLYSYNC method described in \nSection 2.1 . The drum tracks were separated into the fol-\nlowing groups: Bass-Drum/Snare, Cymbals, and Open-\nhihat. Such a separation was found to be appropriate from \nthe analysis of the loops dataset. We also applied the \nsame syncopation algorithm to the complete ensemble, \nconsidering all note events regardless of the tracks or i n-\nstruments (SUMSYNC method ). The results for two re p-\nresentative songs of the collection are shown Figure 3. \nThe two methods clearly give different syncopation r e-\nsults. They only coincide when a single instrument is sy n-\ncopating while the rest are silent or when all instrument \nplay in unison. Computing the syncopation on the whole \nensemble fails to capture the actual syncopation in the \nsong, and only when we combined the syncopation mea s-\nurements for each individual instrument the results r e-\nflected the actual performance, as can be seen from the \nPOLYSYNC and SUMSYNC curves. Additionally , a \nmuch stronger syncopation is encountered in the Funk \nsong, and with a wider distribution among the instruments \nand among the different sections of the song, as seen in \nthe syncopation matrices of Figure 3. \n The above conclusions are not limited to the two d e-\npicted examples but are quite common for all 36 songs of \nthe collection . In fact, in less than 2% of the total bars \nthat have syncopated events in some MIDI track, the two \nmethods, the POLYSYNC and SUMSYNC, agree with \neach other. In contrast, almost 90% of the examined bars \nshow detectable syncopation when using the POLYSYNC \nmethod. The syncopation measured in the rhythmic pa t-\nterns derived from the complete ensemble shows little to \nno syncopation and only when combining information of \nthe various individual instruments can we get a realistic \npicture which agrees with our experience. This implies \n \nFigure 2: Histograms of the number of bars each sync o-\npation score was calculated by the NLHL -p algorithm, for \nthe three most frequent styles in the Loops -dataset. Upper \nrow: for the complete drum -set, lower row: only for the \nbass-drum and snare -drum events.  \n \nSong 22 :  “Get on up and dance” (Funk)  \n \nSong 1: ”Wasting Time” (Popular)  \nFigure 3: Syncopation scores for two songs of the RWC collection. Top: individual instruments. Bottom: overall syncopation \nfor the whole ensemble (SUMSYNC) and as th e combination of the scores of the individual instruments (POLYSYNC).    \n \n that detection of syncopation in audio signals is only po s-\nsible after at least some basic instrument segregation. \n Figure 4 shows how the measured syncopation is r e-\nlated to the density of events per metrical cycle. As e x-\npected for very high density values, the measured sync o-\npation is close to zero, as all metrical positions are occ u-\npied by an event. Lower than average syncopation values \nare also obtained when only one event exists in a whole \nbar. Interestingly, a low mean NLHL- p value appears for \nbars with eight events . This is related to the fact that we \nonly analyzed music in 4/4 where the most typical pattern \nwith eight events would be a sequence of 8th notes that \nmerely tend to keep the beat and therefore have no sync o-\npation. Again, if we would consider the amplitudes of the \nevents the average syncopation might increase. \n Some conclusions about the different music genres in \nthe RWC collection and their use of syncopation can also \nbe made. They cannot, however, be generalized as the \nnumber of examined songs was very small. Rap and Funk \nsongs are characterized by the highest syncopation values . \nIn Rap, syncopation is mainly encountered in the vocals, \nwhereas in F unk it is always spread among several in-\nstruments. Notably, the Modern Jazz pieces were not \ncharacterized by high mean values, with the lead instr u-\nment in the trios always being more syncopated than the \naccompaniment.  \n3.4  Comparing NLHL-p and SG measure \nWe will now compare the two measures by considering \nthe NLHL-p as ground truth, using all separate MIDI \ntracks of the RWC data. Bars were marked as syncopated \nwhen the NLHL-p measure showed syncopation non-zero \nvalues. Then we examined how well the SG measure d e-\ntected those syncopated bars by applying a threshold d to \nthe SG measurements, above which the bars were consi d-ered to be syncopated. The comparison was made in \nterms of F-measure, Precision and Recall ( Figure 5). \n The comparison of the two measures shows a good \nagreement between them in detecting syncopation. The \noptimum threshold according to the F-measure is d=0.2 \n(F-measure=93.11%). The two measures exhibit a diffe r-\nent behavior at low threshold values, where the Precision \n(i.e. the ratio between number of correct detections and \nnumber of all detections) is lower. This is caused by the \nfact that the SG algorithm results in an almost continuous \nsyncopation measurement that can distinguish between \nrhythmic patterns based on small differences in the ampli-\ntudes of events. In contrast, the LHL measure gives a syn-\ncopation ranking of 16 steps, as it depends only on the \nexistence or not of an event in each of the 16 pulses of a bar.   \n In principle, it is possible to use both algorithms, the \nLHL and the SG, for measuring the syncopation in a m u-\nsic performance in real time. As shown here, both result \nin similar syncopation values for most cases, yet, the SG \nalgorithm seems to be advantageous when syncopation \noriginates from accenting certain notes in a sequence, e.g. \nin drum rolls. Thus, we chose the SG algorithm to deve l-\nop our system that generates rhythms based on real-time \nsyncopation measurements of user performances. \n4.  A SYNCOPATION DRIVEN INTERACTIVE \nMUSIC SYSTEM \nWe developed an interactive music system based on real-\ntime syncopation measurements. The system comprises \nfour Max4Live devices—Max/MSP based applications \nthat have the form of plugins for the Ableton Live s e-\nquencer1. Two devices measure the syncopation and de n-\nsity of events in the input music signal, one maps those \nmeasurements to any parameter inside the Ableton Live \nenvironment and the kin.rhythmicator [8] device gene r-\nates rhythmic patterns . The input music signal can either \nbe MIDI note events directly grabbed from music instr u-\nments and MIDI clips, or it can be simple monophonic \naudio that is fed to the [bonk~] [10] object for onset d e-\ntection. Both MIDI and audio signals should be mon o-\nphonic, i.e. the result of a performance on a single instr u-\nment. Otherwise, the syncopation measurements will not \nreflect the syncopation of the input, as shown in the Se c-\ntion 3. The MIDI input or the detected onsets are conver t-\ned into a sequence of amplitudes, suitable for measuring \nsyncopation with the SG algorithm. The measurements \nare performed against a metrical template automatically  \ngenerated according to the time signature of the Ableton \nLive Set. The implementation of the SG algorithm is sim i-\nlar to the one used in the kin.recombinator application d e-\nscribed in [5] with the addition of the normalization d e-\nscribed in section 2.2. In addition to the syncopation, the \n                                                           \n1 http://www.ableton.com ; http://www.cycling74.com;  \n \nFigure 4: Mean syncopation vs. density of events per bar.  \n \nFigure 5: F-measure, Precision, and Recall for  the detect-\ned syncopated bars by the  SG algorithm with respect to \nthe NLHL-p.   \n \n density of events per bar is also calculated. The measur e-\nments are then received by a second device that maps \nthem to any parameter of any other device that the user \nchooses. The user also controls the exact form of the \nmapping. \n A device like the kin.rhythmicator can be used to a u-\ntomatically generate rhythms. The kin.rhythmicatror fe a-\ntures a real time control over the complexity of the gene r-\nated patterns, by controlling the amount of syncopation, \nvariation and the strength of the metrical feel. It was ch o-\nsen exactly for its explicit control of the syncopation. A \npossible “chain” of devices is shown in Figure 6. In this \nway, a user can “prepare” the rhythmicator to interact in \nreal time with a musician, e.g. as the musician performs \nmore complex and syncopated rhythms the automatically \ngenerated patterns are more steady and simple, while \nwhen the musician tends to perform simpler and less \n“dense” rhythms, the generated  patterns become more \ncomplex,  creating a more syncopated result. \nSimple to complex mappings can be realized, invol v-\ning several parameters in several devices and more than \none performer. The described devices are meant as a way \nof creating direct links between musically meaningful \nqualities of a performance and an automatically generated \noutput. \nThe Max4Live devices are available at our website:  \nhttp://smc.inescporto.pt/kinetic/ \n5. CONCLUSIONS \nIn this paper we presented an interactive music system \ndriven by syncopation measurements. In order to better \nunderstand and be able to reliably measure syncopation in \nan actual music performance, we analyzed two MIDI d a-\ntasets, one consisting of drum loops, and one of whole \nsongs using the NLHL-p and the SG syncopation metrics . \nWe concluded that in any musical signal, whether it is a \nMIDI stream or an audio signal, it is important for sync o-\npation measurements that it is first separated into the i n-\ndividual rhythmic layers or the instruments that comprise \nit. Our findings are of particular importance for our future \nresearch that focus es in computing syncopation in more \ncomplex music signals in order to drive a meaningful i n-\nteraction between a musician and a rhythm that is being \nautomatically generated. 6. AKNOWLEDGMENTS \nThis work was partly supported by the Portuguese Fou n-\ndation for Science and Technology, within project ref. \nSFRH / BPD / 51348 / 2011 and PTDC / EAT- MMU / \n112255 / 2009. \n7. REFERENCES \n[1] D. Huron and A. Ommen, “An Empirical Study of \nSyncopation in American Popular Music, 1890-\n1939,” Music Theory Spectrum , vol. 28, no. 2, pp. \n211-231, 2006. \n[2] D. Temperley, “Syncopation in rock: a perceptual \nperspective,” Popular Music , vol. 18, no. 1, pp. 19-\n40, 1999. \n[3] F. Gómez, E. Thul, and G. Toussaint, “An exper i-\nmental comparison of formal measures of rhythmic \nsyncopation,” in Proceedings of the International \nComputer Music Conference , 2007, pp. 101- 104. \n[4] I. Shmulevich and D.-J. Povel, “Measures of te m-\nporal pattern complexity,” Journal of New Music \nResearch, vol. 29, no. 1, pp. 61-69, 2000. \n[5] G. Sioros and C. Guedes, “Complexity Driven R e-\ncombination of MIDI Loops,” in Proceedings of the \n12th International Society for Music Information \nRetrieval Conference , 2011, pp. 381- 386. \n[6] H. C. Longuet-Higgins and C. S. Lee, “The rhythmic \ninterpretation of monophonic music,” Music Perce p-\ntion, vol. 1, no. 4, pp. 424–441, 1984. \n[7] W. T. Fitch and A. J. Rosenfeld, “Perception and \nProduction of Syncopated Rhythms,” Music Perce p-\ntion, vol. 25, no. 1, pp. 43-58, 2007. \n[8] G. Sioros and C. Guedes, “Automatic rhythmic pe r-\nformance in Max/MSP: the kin. rhythmicator,” in \nProceedings of the International Conference on \nNew Interfaces for Musical Expression , 2011, pp. \n88-91. \n[9] F. Lerdahl and R. Jackendoff, A Generative Theory \nof Tonal Music,  Cambridge: The MIT Press, 1996. \n[10] M. S. Puckette, T. Apel, and D. D. Zicarelli, “Real-\ntime audio analysis tools for Pd and MSP analysis,” \nin International Computer Music Conference , 1998, \nvol. 74, pp. 109-112.  \n \nFigure 6: The Max4Live devices. The left most device  receives the real time input and calculates its syncopation. The \nmiddle device receives the syncopation value and maps it to the radius parameter of the right most device, the rhythm i-\ncator. Finally, the rhythmicator generates a rhythm with the complexity being controlled by the syncopation measurements."
    },
    {
        "title": "Evaluation of Musical Features for Emotion Classification.",
        "author": [
            "Yading Song",
            "Simon Dixon",
            "Marcus T. Pearce"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415854",
        "url": "https://doi.org/10.5281/zenodo.1415854",
        "ee": "https://zenodo.org/records/1415854/files/SongDP12.pdf",
        "abstract": "Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recogni- tion. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only lim- ited success has been obtained in learning automatic classi- fiers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words “happy”, “sad”, “angry” and “relaxed”, on the Last.FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio fea- tures are extracted using standard algorithms. Two clas- sifiers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dy- namics, and, to a lesser extent, harmony. We also find that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classification.",
        "zenodo_id": 1415854,
        "dblp_key": "conf/ismir/SongDP12",
        "keywords": [
            "music",
            "emotion",
            "recognition",
            "features",
            "rhythm",
            "timbre",
            "spectrum",
            "lyrics",
            "support",
            "vector"
        ],
        "content": "EV ALUATION OF MUSICAL FEATURES FOR EMOTION\nCLASSIFICATION\nYading Song, Simon Dixon, Marcus Pearce\nCentre for Digital Music, Queen Mary University of London\nfyading.song, simon.dixon, marcus.pearceg@eecs.qmul.ac.uk\nABSTRACT\nBecause music conveys and evokes feelings, a wealth of\nresearch has been performed on music emotion recogni-\ntion. Previous research has shown that musical mood is\nlinked to features based on rhythm, timbre, spectrum and\nlyrics. For example, sad music correlates with slow tempo,\nwhile happy music is generally faster. However, only lim-\nited success has been obtained in learning automatic classi-\nﬁers of emotion in music. In this paper, we collect a ground\ntruth data set of 2904 songs that have been tagged with one\nof the four words “happy”, “sad”, “angry” and “relaxed”,\non the Last.FM web site. An excerpt of the audio is then\nretrieved from 7Digital.com, and various sets of audio fea-\ntures are extracted using standard algorithms. Two clas-\nsiﬁers are trained using support vector machines with the\npolynomial and radial basis function kernels, and these are\ntested with 10-fold cross validation. Our results show that\nspectral features outperform those based on rhythm, dy-\nnamics, and, to a lesser extent, harmony. We also ﬁnd that\nthe polynomial kernel gives better results than the radial\nbasis function, and that the fusion of different feature sets\ndoes not always lead to improved classiﬁcation.\n1. INTRODUCTION\nIn the past ten years, music emotion recognition has at-\ntracted increasing attention in the ﬁeld of music informa-\ntion retrieval (MIR) [16]. Music not only conveys emotion,\nbut can also modulate a listener’s mood [8]. People report\nthat their primary motivation for listening to music is its\nemotional effect [19] and the emotional component of mu-\nsic has been recognised as most strongly associated with\nmusic expressivity [15].\nRecommender systems for managing a large personal\nmusic collections typically use collaborative ﬁltering [28]\n(historical ratings) and metadata- and content-based ﬁlter-\ning [3] (artist, genre, acoustic features similarity). Emo-\ntion can be easily incorporated into such systems to sub-\njectively organise and search for music. Musicovery1,\n1http://musicovery.com/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.for example, has successfully used a dimensional model\nof emotion within its recommendation system.\nAlthough music emotion has been widely studied in psy-\nchology, signal processing, neuroscience, musicology and\nmachine learning, our understanding is still at an early stage.\nThere are three common issues: 1. collection of ground\ntruth data; 2. choice of emotion model; 3. relationships\nbetween emotion and individual acoustic features [13].\nSince 2007, the annual Music Information Retrieval Eval-\nuation eXchange (MIREX)2has organised an evaluation\ncampaign for MIR algorithms to facilitate ﬁnding solu-\ntions to the problems of audio music classiﬁcation. In\nprevious studies, signiﬁcant research has been carried out\non emotion recognition including regressor training: us-\ning multiple linear regression [6] and Support Vector Ma-\nchines (SVM) [23,37], feature selection [35,36], the use of\nlyrics [13] and advanced research including mood classiﬁ-\ncation on television theme tunes [30], analysis with elec-\ntroencephalogram (EEG) [18], music expression [32] and\nthe relationship with genre and artist [12]. Other relevant\nwork on classiﬁcation suggests that feature generation can\noutperform approaches based on standard features in some\ncontexts [33].\nIn this paper, we aim to better explain and explore the\nrelationship between musical features and emotion. We\nexamine the following parameters: ﬁrst, we compare four\nperceptual dimensions of musical features: dynamics, spec-\ntrum, rhythm, and harmony; second, we evaluate an SVM\nassociated with two kernels: polynomial and radial basis\nfunctions; third, for each feature we compare the mean and\nstandard deviation feature value. The results are trained\nand tested using semantic data retrieved from last.fm3and\naudio data from 7digital4.\nThis paper is structured as follows. In section 2, three\npsychological models are discussed. Section 3 explains the\ndataset collection we use in training and testing. The pro-\ncedure is described in section 4, which includes data pre-\nprocessing (see section 4.1), feature extraction (see section\n4.2) and classiﬁcation (see section 4.3). Section 5 explains\nfour experiments. Finally, section 6 concludes the paper\nand presents directions for future work.\n2http://www.music-ir.org/mirex/wiki/MIREX HOME\n3http://www.last.fm/\n4http://www.7digital.com/2. PSYCHOLOGICAL EMOTION MODELS\nOne of the difﬁculties in representing emotion is to distin-\nguish music-induced emotion from perceived emotion be-\ncause the two are not always aligned [5]. Different psycho-\nlogical models of emotion have been compared in a study\nof perceived emotion [7].\nMost music related studies are based on two popular\napproaches: categorical [10] and dimensional [34] mod-\nels of emotion. The categorical approach describes emo-\ntions with a limited number of innate and universal cate-\ngories such as happiness, sadness, anger and fear. The di-\nmensional model considers all affective terms arising from\nindependent neurophysiological systems: valence (nega-\ntive to positive) and arousal (calm to exciting). Recently a\nmore sophisticated model of music-induced emotion - the\nGeneva Emotion Music Scale (GEMS) model - consisting\nof 9 dimensions, has been proposed [42]. Our results and\nanalysis are based on the categorical model since we make\nour data collection through human-annotated social tags\nwhich are categorical in nature.\n3. GROUND-TRUTH DATA COLLECTION\nAs discussed above, due to the lack of ground truth data,\nmost researchers compile their own databases [41]. Man-\nual annotation is one of the most common ways to do this.\nHowever, it is expensive in terms of ﬁnancial cost and hu-\nman labour. Moreover, terms used may differ between in-\ndividuals. Different emotions may be described using the\nsame term by different people which would result in poor\nprediction [38]. However, with the emergence of music\ndiscovery and recommendation websites such as last.fm\nwhich support social tags for music, we can access rich\nhuman-annotated information. Compared with the tradi-\ntional approach of web mining which gives noisy results,\nsocial tagging provides highly relevant information for mu-\nsic information retrieval (MIR) and has become an im-\nportant source of human-generated contextual knowledge\n[11]. Levy [24] has also shown that social tags give a high\nquality source of ground truth data and can be effective in\ncapturing music similarity [40].\nThe ﬁve mood clusters proposed by MIREX [14] (such\nas rollicking, literate, and poignant) are not popular in so-\ncial tags. Therefore, we use four basic emotion classes:\nhappy, angry, sadandrelaxed, considering these four emo-\ntions are widely accepted across different cultures and cover\nthe four quadrants of the 2-dimensional model of emo-\ntion [22]. These four basic emotions are used as seeds to\nretrieve the top 30 tags from last.fm. We then obtain a list\nof songs labelled with the retrieved tags. Table1 and table\n2 show an example of the retrieved results.\nGiven the retrieved titles and the names of the singers,\nwe use a public API to get preview ﬁles. The results cover\ndifferent types of pop music, meaning that we avoid partic-\nular artist and genre effects [17]. Since the purpose of this\nstep is to ﬁnd ground truth data, issues such as cold start,\nnoise, hacking, and bias are not relevant [4, 20].\nMost datasets on music emotion recognition are quiteHappy Angry Sad Relax\nhappy angry sad relax\nhappy hardcore angry music sad songs relax trance\nmakes me happy angry metal happysad relax music\nhappy music angry pop music sad song jazz relax\nhappysad angry rock sad & beautiful only relax\nTable 1. Top 5 tags returned by last.fm\nSinger Title\nNoah And The Whale 5 Years Time\nJason Mraz I’m Yours\nRusted Root Send Me On My Way\nRoyksopp Happy Up Here\nKaren O and the Kids All Is Love\nTable 2. Top songs returned with tags from the “happy”\ncategory.\nsmall (less than 1000 items), which indicates that 2904\nsongs (see table 3) for four emotions retrieved by social\ntags is a good size for the current experiments. The dataset\nwill be made available5, to encourage other researchers to\nreproduce the results for research and evaluation.\nEmotion Number of Songs\nHappy 753\nAngry 639\nSad 763\nRelaxed 749\nOverall 2904\nTable 3. Summary of ground truth data collection\n4. PROCEDURES\nThe experimental procedure consists of four stages: data\ncollection, data preprocessing, feature extraction, and clas-\nsiﬁcation, as shown in ﬁgure 1.\n4.1 Data Preprocessing\nAs shown in Table 1, there is some noise in the data such as\nconfusing tags and repeated songs. We manually remove\ndata with the tag happysad which existed in both the happy\nand sad classes and delete the repeated songs, to make sure\nevery song will only exist once in a single class. Moreover,\nwe convert our dataset to standard wav format (22,050 Hz\nsampling rate, 16 bit precision and mono channel). The\nsong excerpts are either 30 seconds or 60 seconds, rep-\nresenting the most salient part of the song [27], therefore\nthere is no need to truncate. At the end, we normalise the\nexcerpts by dividing by the highest amplitude to mitigate\ntheproduction effect of different recording levels.\n4.2 Feature Extraction\nAs suggested in the work of Saari and Eerola [35], two dif-\nferent types of feature (mean and standard deviation) with\n5The dataset can be found at https://code.soundsoftware.ac.uk/projects-\n/emotion-recognitionFigure 1. Procedure\na total of 55 features were extracted using the MIR tool-\nbox6[21] (shown in table 4). The features are categorized\ninto the following four perceptual dimensions of music lis-\ntening: dynamics, rhythm, spectral, and harmony.\n4.3 Classiﬁcation\nThe majority of music classiﬁcation tasks [9] (genre clas-\nsiﬁcation [25,39], artist identiﬁcation [29], and instrument\nrecognition [31]) have used k-nearest neighbour (K-NN)\n[26] and support vector machines (SVM) [2]. In the case\nof audio input features, the SVM has been shown to per-\nform best [1].\nIn this paper, therefore, we choose support vector ma-\nchines as our classiﬁer, using the implementation of the se-\nquential minimal optimisation algorithm in the Weka data\nmining toolkit7. SVMs are trained using polynomial and\nradial basis function (RBF) kernels. We set the cost factor\nC= 1:0, and leave other parameters unchanged. An in-\nternal 10-fold cross validation is applied. To better under-\nstand and compare features in four perceptual dimensions,\nour experiments are divided into four tasks.\nExperiment 1: we compare the performance of the two\nkernels (polynomial and RBF) using various features.\nExperiment 2: four classes (perceptual dimensions) of\nfeatures are tested separately, and we compare the results\nto ﬁnd a dominant class.\nExperiment 3: two types of feature descriptor, mean and\nstandard deviation, are calculated. The purpose is to com-\npare values for further feature selection and dimensionality\nreduction.\n6Version 1.3.3: https://www.jyu.ﬁ/music/coe/materials/mirtoolbox\n7http://www.cs.waikato.ac.nz/ml/weka/Dimen. No. Features Acronyms\nDynamics 1-2 RMS energy RMSm, RMSstd\n3-4 Slope Ss, Sstd\n5-6 Attack As, Astd\n7 Low energy LEm\nRhythm 1-2 Tempo Ts, Tstd\n3-4 Fluctuation peak (pos, mag) FPm, FMm\n5 Fluctuation centroid FCm\nSpec. 1-2 Spectrum centroid SCm, SCstd\n3-4 Brightness BRm, BRstd\n5-6 Spread SPm, SPstd\n7-8 Skewness SKm, SKstd\n9-10 Kurtosis Km, Kstd\n11-12 Rolloff95 R95s, R95std\n13-14 Rolloff85 R85s, R85std\n15-16 Spectral Entrophy SEm, SEstd\n17-18 Flatness Fm, Fstd\n19-20 Roughness Rm, Rstd\n21-22 Irregularity IRm. IRstd\n23-24 Zero crossing rate ZCRm, ZCRstd\n25-26 Spectral ﬂux SPm, SPstd\n27-28 MFCC MFm, MFstd\n29-30 DMFCC DMFm, DMFstd\n31-32 DDMFCC DDm, DDstd\nHarmony 1-2 Chromagram peak CPm, CPstd\n3-4 Chromagram centroid CCm, CCstd\n5-6 Key clarity KCm, KCstd\n7-8 Key mode KMm, KMstd\n9-10 HCDF Hm, Hstd\nTable 4. The feature set used in this work; m = mean, std\n=standard deviation.\nExperiment 4: different combinations of feature classes\n(e.g., spectral with dynamics) are evaluated in order to de-\ntermine the best-performing model.\n5. RESULTS\n5.1 Experiment 1\nIn experiment 1, SVMs trained with two different kernels\nare compared. Previous studies [23] have found in the case\nof audio input that the SVM performs better than other\nclassiﬁers (Logistic Regression, Random Forest, GMM,\nK-NN and Decision Trees). To our knowledge, no work\nhas been reported explicitly comparing different kernels\nfor SVMs. In emotion recognition, the radial basis func-\ntion kernel is a common choice because of its robustness\nand accuracy in other similar recognition tasks [1].\nPolynomial RBF\nFeature Class Accuracy Time Accuracy Time No.\nDynamics 37.2 0.44 26.3 32.5 7\nRhythm 37.5 0.44 34.5 23.2 5\nHarmony 47.5 0.41 36.6 27.4 10\nSpectral 51.9 0.40 48.1 14.3 32\nTable 5. Experiment 1 results: time = model building time,\nNo. = number of features in each class\nThe results in table 5 show however that regardless of\nthe features used, the polynomial kernel always achieved\nthe higher accuracy. Moreover, the model construction\ntimes for each kernel are dramatically different. The av-\nerage construction time for the polynomial kernel is 0.4\nseconds, while the average time for the RBF kernel is 24.2seconds, around 60 times more than the polynomial ker-\nnel. The following experiments also show similar results.\nThis shows that polynomial kernel outperforms RBF in the\ntask of emotion recognition at least for the parameter val-\nues used here.\n5.2 Experiment 2\nIn experiment 2, we compare the emotion prediction re-\nsults for the following perceptual dimensions: dynamics,\nrhythm, harmony, and spectral. Results are shown in ﬁg-\nure 2). Dynamics and rhythm features yield similar re-\nsults, with harmony features providing better results, but\nthe spectral class with 32 features achieves the highest ac-\ncuracy of 51.9%. This experiment provides a baseline model,\nand further exploration of multiple dimensions is performed\nin experiment 4.\nFigure 2. Comparison of classiﬁcation results for the four\nclasses of features.\n5.3 Experiment 3\nIn this experiment, we evaluate different types of feature\ndescriptors, mean value and standard deviation for each\nfeature across all feature classes, for predicting the emotion\nin music. The results in table 6 show that the use of both\nmean and standard deviation values gives the best results\nin each case. However, the processing time increased, so\nchoosing the optimal descriptor for each feature is highly\ndesirable. For example, choosing only the mean value in\nthe harmony class, we lose 2% of accuracy but increase\nthe speed while the choice of standard deviation results in\naround 10% accuracy loss. As the number of features in-\ncreases, the difference between using mean and standard\ndeviation will be reduced. However, more experiments are\nneeded to explain why the mean in harmony and spectral\nfeatures, and standard deviation values of dynamics and\nrhythm features have higher accuracy scores.\n5.4 Experiment 4\nIn order to choose the best model, the ﬁnal experiment\nfuses different perceptual features. As presented in table 7,\noptimal accuracy is not produced by the combination of all\nfeatures. Instead, the use of spectral, rhythm and harmony\n(but not dynamic) features produces the highest accuracy.Features Class Polynomial No. features\nDynamics all 37.2 7\nDynamics mean 29.7 3\nDynamics std 33.8 3\nRhythm all 37.5 5\nRhythm mean 28.7 1\nRhythm std 34.2 1\nHarmony all 47.5 10\nHarmony mean 45.3 5\nHarmony std 38.3 5\nSpectral all 51.9 32\nSpectral mean 49.6 16\nSpectral std 47.5 16\nSpec+Dyn all 52.3 39\nSpec+Dyn mean 50.5 19\nSpec+Dyn std 48.7 19\nSpec+Rhy all 52.3 37\nSpec+Rhy mean 49.8 17\nSpec+Rhy std 47.8 17\nSpec+Har all 53.3 42\nSpec+Har mean 51.3 21\nSpec+Har std 50.3 21\nHar+Rhy all 49.1 15\nHar+Rhy mean 45.6 6\nHar+Rhy std 41.2 6\nHar+Dyn all 48.8 17\nHar+Dyn mean 46.9 8\nHar+Dyn std 42.4 8\nRhy+Dyn all 41.7 12\nRhy+Dyn mean 32.0 4\nRhy+Dyn std 38.8 4\nTable 6. Comparison of mean and standard deviation (std)\nfeatures.\nFeatures Accuracy No. features\nSpec+Dyn 52.3 39\nSpec+Rhy 52.3 37\nSpec+Har 53.3 42\nHar+Rhy 49.1 15\nHar+Dyn 48.8 17\nRhy+Dyn 41.7 12\nSpec+Dyn+Rhy 52.4 44\nSpec+Dyn+Har 53.8 49\nSpec+Rhy+Har 54.0 47\nDyn+Rhy+Har 49.7 22\nAll Features 53.6 54\nTable 7. Classiﬁcation results for combinations of feature\nsets.\n6. CONCLUSION AND FUTURE WORK\nIn this paper, we collected ground truth data on the emo-\ntion associated with 2904 pop songs from last.fm tags. Au-\ndio features were extracted and grouped into four percep-\ntual dimensions for training and validation. Four experi-\nments were conducted to predict emotion labels. The re-\nsults suggest that, instead of the conventional approach us-\ning SVMs trained with a RBF kernel, a polynomial ker-\nnel yields higher accuracy. Since no single dominant fea-\ntures have been found in emotion recognition, we explored\nthe performance of different perceptual classes of feature\nfor predicting emotion in music. Experiment 3 found that\ndimensionality reduction can be achieved through remov-\ning either mean or standard deviation values, halving the\nnumber of features used, with, in some cases, only 2% ac-\ncuracy loss. The last experiment found that inclusion of\ndynamics features with the other classes actually impairedthe performance of the classiﬁer while the combination of\nspectral, rhythmic and harmonic features yielded optimal\nperformance.\nIn future work, we will expand this research both in\ndepth and breadth, to ﬁnd features and classes of features\nwhich best represent emotion in music. We will examine\nhigher-level dimensions such as temporal evolution fea-\ntures, as well as investigating the use of auditory mod-\nels. Using the datasets retrieved from Last.fm, we will\ncompare the practicability of social tags with other human-\nannotated datasets in emotion recognition. Through these\nstudies of subjective emotion, we will develop methods for\nincorporating other empirical psychological data in a sub-\njective music recommender system.\n7. ACKNOWLEDGEMENTS\nWe acknowledge the support of the Queen Mary University\nof London Postgraduate Research Fund (QMPGRF) and\nthe China Scholarship Council. We would like to thank\nthe reviewers and Emmanouil Benetos for their advice and\ncomments.\n8. REFERENCES\n[1] K. Bischoff, C. S. Firan, R. Paiu, W. Nejdl, C. Laurier,\nand M. Sordo. Music Mood and Theme Classiﬁcation\n- A Hybrid Approach. In 10th International Society for\nMusic Information Retrieval Conference, number Is-\nmir, pages 657–662, 2009.\n[2] E. Boser, N. Vapnik, and I. M. Guyon. Training Algo-\nrithm Margin for Optimal Classiﬁers. In ACM Confer-\nence on Computational Learning Theory, pages 144–\n152, 1992.\n[3] P. Cano, M. Koppenberger, and N. Wack. Content-\nbased Music Audio Recommendation. In Proceedings\nof the 13th annual ACM international conference on\nMultimedia, number ACM, pages 211–212, 2005.\n[4] O. Celma. Foaﬁng the Music : Bridging the Semantic\nGap in Music Recommendation. In The Semantic Web-\nISWC, 2006.\n[5] T. Eerola. Are the Emotions Expressed in Mu-\nsic Genre-speciﬁc? An Audio-based Evaluation of\nDatasets Spanning Classical , Film , Pop and Mixed\nGenres. Journal of New Music Research, 40(March\n2012):349–366, 2011.\n[6] T. Eerola, O. Lartillot, and P. Toiviainen. Prediction\nof Multdimensional Emotional Ratings in Music from\nAudio Using Multivariate Regression Models. In 10th\nInternational Society for Music Information Retrieval\nConference, number Ismir, pages 621–626, 2009.\n[7] T. Eerola and J.K. Vuoskoski. A Comparison of the\nDiscrete and Dimensional Models of Emotion in Mu-\nsic.Psychology of Music, 39(1):18–49, August 2010.[8] Y . Feng and Y . Zhuang. Popular Music Retrieval by\nDetecting Mood. In International Society for Music In-\nformation Retrieval Conference, volume 2, pages 375–\n376, 2003.\n[9] Z. Fu, G. Lu, K.M. Ting, and D. Zhang. A Sur-\nvey of Audio-based Music Classiﬁcation and Anno-\ntation. IEEE Transactions on Multimedia, 13(2):303–\n319, 2011.\n[10] K. Hevner. Experimental studies of the elements of ex-\npression in music. The American Journal of Psychol-\nogy, 48:246–268, 1936.\n[11] X. Hu, M. Bay, and J.S. Downie. Creating a Simpliﬁed\nMusic Mood Classiﬁcation Grouth-truth Set. In Inter-\nnational Conference on Music Information Retrieval,\npages 3–4, 2007.\n[12] X. Hu and J.S. Downie. Exploring Mood Metadata:\nRelationships with Genre, Artist and Usage Metadata.\nIn8th International Conference on Music Information\nRetrieval, 2007.\n[13] X. Hu, J.S. Downie, and A.F. Ehmann. Lyric Text Min-\ning in Music Mood Classiﬁcation. In 10th Interna-\ntional Society for Music Information Retrieval Confer-\nence, number Ismir, pages 411–416, 2009.\n[14] X. Hu, J.S. Downie, C. Laurier, and M. Bay. The\n2007 MIREX Audio Mood Classiﬁcation Task: Les-\nson Learned. In International Society for Music Infor-\nmation Retrieval Conference, pages 462–467, 2008.\n[15] P. N. Juslin, J. Karlsson, E. Lindstr ¨om, A. Friberg, and\nE. Schoonderwaldt. Play it Again with Feeling: Com-\nputer Feedback in Musical Communication of Emo-\ntions. Journal of experimental psychology. Applied,\n12(2):79–95, June 2006.\n[16] Y .E. Kim, E.M. Schmidt, R. Migneco, B.G. Morton,\nP. Richardson, J. Scott, J.A. Speck, and D. Turnbull.\nMusic Emotion Recognition: A State of the Art Re-\nview. In 11th International Society for Music Informa-\ntion Retrieval Conference, number Ismir, pages 255–\n266, 2010.\n[17] Y .E. Kim, D.S. Williamson, and S. Pilli. Towards\nQuantifying the Album Effect in Artist Identiﬁcation.\nInInternational Society for Music Information Re-\ntrieval Conference, 2006.\n[18] S. Koelstra, C. Muhl, and M. Soleymani. Deap: A\nDatabase for Emotion Analysis Using Physiological\nSignals. IEEE Trans. on Affective Computing, pages 1–\n15, 2011.\n[19] C.L. Krumhansl. Music : A Link Between Cogni-\ntion and Emotion. American Psychological Society,\n11(2):45–50, 2002.\n[20] P. Lamere. Social Tagging and Music Information Re-\ntrieval. Journal of New Music Research, 37(2):101–\n114, June 2008.[21] O. Lartillot and P. Toiviainen. MIR in Matlab (II): A\nToolbox for Musical Feature Extraction from Audio.\nInInternational Conference on Music Information Re-\ntrieval, number Ii, pages 237–244, 2007.\n[22] C. Laurier and J. Grivolla. Multimodal Music Mood\nClassiﬁcation Using Audio and Lyrics. In Int. Conf.\nMachine Learning and Applications, pages 1–6, 2008.\n[23] C. Laurier, P. Herrera, M. Mandel, and D. Ellis. Audio\nMusic Mood Classiﬁcation Using Support Vector Ma-\nchine. In MIREX task on Audio Mood Classiﬁcation,\npages 2–4, 2007.\n[24] M. Levy. A Semantic Space for Music Derived from\nSocial Tags. In Austrian Compuer Society, volume 1,\npage 12. Citeseer, 2007.\n[25] B. Lines, E. Tsunoo, G. Tzanetakis, and N. Ono. Be-\nyond Timbral Statistics : Improving Music Classiﬁca-\ntion Using Percussive. IEEE Transactions on Audio,\nSpeech and Language Processing, 19(4):1003–1014,\n2011.\n[26] T. M. Cover and P. E. Hart. Nearest Neighbor Pattern\nClassiﬁcation. IEEE Transactions on Information The-\nory, 13(1):21–27, 1967.\n[27] K.F. MacDorman, S. Ough, and C. Ho. Automatic\nEmotion Prediction of Song Excerpts: Index Construc-\ntion, Algorithm Design, and Empirical Comparison.\nJournal of New Music Research, 36(4):281–299, De-\ncember 2007.\n[28] T. Magno and C. Sable. A Comparison of Signal of\nSignal-based Music Recommendation to Genre Labels,\nCollaborative Filtering, Musicological Analysis, Hu-\nman Recommendation and Random Baseline. In Pro-\nceedings of the 9th International Conference of Music\nInformation Retrieval, pages 161–166, 2008.\n[29] M. Mandel. Song-level Features and Support Vec-\ntor Machines for Music Classiﬁcation. In Proc. Inter-\nnational Conference on Music Information Retrieval,\n2005.\n[30] M. Mann, T.J. Cox, and F.F. Li. Music Mood Classi-\nﬁcation of Television Theme Tunes. In 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence, number Ismir, pages 735–740, 2011.\n[31] J. Marques and P.J. Moreno. A Study of Musical In-\nstrument Classiﬁcation Using Gaussian Mixture Mod-\nels and Support Vector Machines, 1999.\n[32] L. Mion and G.D. Poli. Score-Independent Audio\nFeatures for Description of Music Expression. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing, 16(2):458–466, 2008.\n[33] F. Pachet and P. Roy. Analytical Features: A\nKnowledge-Based Approach to Audio Feature Gener-\nation. EURASIP Journal on Audio, Speech, and Music\nProcessing, 2009(2):1–23, 2009.[34] J.A. Russell, A. Weiss, and G.A. Mendelsohn. Af-\nfect Grid: A Single-item Scale of Pleasure and\nArousal. Journal of Personality and Social Psychology,\n57(3):493–502, 1989.\n[35] P. Saari, T. Eerola, and O. Lartillot. Generalizability\nand Simplicity as Criteria in Feature Selection: Appli-\ncation to Mood Classiﬁcation in Music. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n19(6):1802–1812, 2011.\n[36] E.M. Schmidt, D. Turnbull, and Y .E. Kim. Feature\nSelection for Content-Based, Time-Varying Musical\nEmotion Regression Categories and Subject Descrip-\ntors. In Multimedia Information Retrieval, pages 267–\n273, 2010.\n[37] B. Schuller, J. Dorfner, and G. Rigoll. Determination\nof Nonprototypical Valence and Arousal in Popular\nMusic: Features and Performances. EURASIP Journal\non Audio, Speech, and Music Processing, 2010:1–19,\n2010.\n[38] D. Turnbull, L. Barrington, and G. Lanckriet. Five Ap-\nproaches to Collecting Tags for Music. In Proceedings\nof the 9th International Conference of Music Informa-\ntion Retrieval, pages 225–230, 2008.\n[39] G. Tzanetakis and P. Cook. Musical Genre Classiﬁca-\ntion of Audio Signals. IEEE Transactions on Speech\nand Audio Processing, 10(5):293–302, 2002.\n[40] D. Wang, T. Li, and M. Ogihara. Tags Better Than\nAudio Features? The Effect of Joint use of Tags and\nAudio Content Features for Artistic Style Clutering.\nIn11th International Society on Music Information\nRetrieval Conference, number ISMIR, pages 57–62,\n2010.\n[41] D. Yang and W.S. Lee. Disambiguating Music Emo-\ntion Using Software Agents. In Proceedings of the 5th\nInternational Conference on Music Information Re-\ntrieval, pages 52–58, 2004.\n[42] M. Zentner, D. Grandjean, and K.R. Scherer. Emo-\ntions evoked by the sound of music: characterization,\nclassiﬁcation, and measurement. Emotion (Washing-\nton, D.C.), 8(4):494–521, August 2008."
    },
    {
        "title": "Extracting Semantic Information from an Online Carnatic Music Forum.",
        "author": [
            "Mohamed Sordo",
            "Joan Serrà",
            "Gopala K. Koduri",
            "Xavier Serra"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416828",
        "url": "https://doi.org/10.5281/zenodo.1416828",
        "ee": "https://zenodo.org/records/1416828/files/SordoSKS12.pdf",
        "abstract": "By mining user-generated text content we can obtain music- related information that could not otherwise be extracted from audio signals or symbolic score representations. In this paper we propose a methodology for extracting music-related semantic information from an online discussion forum, rasikas.org, dedicated to the Carnatic music tradition. We first define a dictionary of relevant terms within categories such as raagas, taalas, performers, composers, and instruments, and create a complex network representation by matching such dictionary against the fo- rum posts. This network representation is used to iden- tify popular terms within the forum, as well as relevant co-occurrences and semantic relationships. This way, for instance, we are able to learn the instrument played by a performer with 95% accuracy, to discover the confusion between two raagas with different naming conventions, or to infer semantic relationships regarding lineage or musi- cal influence. This contribution is a first step towards the automatic creation of ontologies for specific musical cul- tures.",
        "zenodo_id": 1416828,
        "dblp_key": "conf/ismir/SordoSKS12",
        "keywords": [
            "music-related semantic information",
            "online discussion forum",
            "Carnatic music tradition",
            "dictionary of relevant terms",
            "complex network representation",
            "popular terms identification",
            "relevant co-occurrences",
            "semantic relationships",
            "instrument played by a performer",
            "infer semantic relationships"
        ],
        "content": "EXTRACTING SEMANTIC INFORMATION FROM AN ONLINE\nCARNATIC MUSIC FORUM\nMohamed Sordo1, Joan Serr `a2, Gopala K. Koduri1and Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain.\n2Artiﬁcial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain.\n{mohamed.sordo,gopala.koduri,xavier.serra}@upf.edu, jserra@iiia.csic.es\nABSTRACT\nBy mining user-generated text content we can obtain music-\nrelated information that could not otherwise be extracted\nfrom audio signals or symbolic score representations. In\nthis paper we propose a methodology for extracting\nmusic-related semantic information from an online\ndiscussion forum, rasikas.org, dedicated to the Carnatic\nmusic tradition. We ﬁrst deﬁne a dictionary of relevant\nterms within categories such as raagas, taalas, performers,\ncomposers, and instruments, and create a complex network\nrepresentation by matching such dictionary against the fo-\nrum posts. This network representation is used to iden-\ntify popular terms within the forum, as well as relevant\nco-occurrences and semantic relationships. This way, for\ninstance, we are able to learn the instrument played by a\nperformer with 95% accuracy, to discover the confusion\nbetween two raagas with different naming conventions, or\nto infer semantic relationships regarding lineage or musi-\ncal inﬂuence. This contribution is a ﬁrst step towards the\nautomatic creation of ontologies for speciﬁc musical cul-\ntures.\n1. INTRODUCTION\nUnderstanding music requires (also) understanding how\nlisteners perceive music, how they consume it or enjoy it,\nand how they share their tastes among other people. The\nonline interaction among users results in the emergence of\nonline communities. These interactions generate digital\ncontent that is very valuable for the study of many top-\nics, in our case for the study of music. According to [11],\nan online community can be deﬁned as a persistent group\nof users of an online social media platform with shared\ngoals, a speciﬁc organizational structure, community ritu-\nals, strong interactions and a common vocabulary. Our aim\nin this paper is to study and analyze an online community\ndedicated to the Carnatic music tradition, rasikas.org.\nCarnatic music is the art music of south India [12]. This\nis a very old and alive tradition with a very engaged and\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.active community. The music lovers of Carnatic music are\nknown as rasikas1, and their involvement in music related\nactivities and events is fundamental for the preservation\nand evolution of this music. Interestingly, the interactions\nbetween artists and rasikas can inﬂuence the evolution of\nthe music concepts in the tradition. For instance, raagas2,\noften described as collections of phrases, evolve over time\n(hence, it is often said that a given raaga today is not the\nsame as it was a hundred years ago). When a performer\nexperiments with a new phrase, rasikas respond to show\ntheir appreciation if they believe that the phrase enriched\ntheir experience of the raaga.\nWebsites and online forums have become very relevant\nvenues with which to support and sustain the Carnatic mu-\nsic tradition. Online communities have emerged in which\ngroups or rasikas share music content and discuss among\nthem. Rasikas.org is one such forum in which users engage\nin many types of discussions, some of them quite engaged,\ncovering most relevant Carnatic music related topics. It\nclearly does not reﬂect the whole community of rasikas,\nbut it is an interesting forum from which to learn about\nCarnatic music and about the opinions of some very pas-\nsionate and active rasikas. We will be using rasikas.org to\nperform the experiments in this paper.\nExtracting semantic information from online forums has\nbecome an important area of research in the last few years.\nFor instance, Yang et al. [15] proposed a method to extract\nstructured data from all types of online forums. Weimer\net al. [13] and Chen et al. [2] proposed models to iden-\ntify high quality posts and topics, respectively. Zhu et\nal. [16], on the other hand, generated relation networks for\ntopic detection and opinion-leader detection. In addition,\na considerable number of approaches devoted to mining\nuser-generated text content have been proposed in the mu-\nsic information retrieval (MIR) community (e.g. [1, 4, 8,\n14]). Nevertheless, to the best of our knowledge, none of\nthe highlighted approaches in MIR has exploited the inner\nstructure of online discussion forums.\nIn this paper we propose a method for extracting se-\nmantic information from an online Carnatic music forum,\nspeciﬁcally rasikas.org. We deﬁne a dictionary of Carnatic\nmusic terms and create a complex network representation\nof the online forum by matching such dictionary against\n1Rasika, in sanskrit, literally means “the one who derives pleasure”.\n2A raaga is the fundamental melodic framework for composition and\nimprovisation in Indian classical music.the forum posts. We study different network measures re-\nlated to the aforementioned network, including node rele-\nvance, node co-occurrence and term relations via semanti-\ncally connecting words. This allows us to obtain meaning-\nful information from the forum’s discussions.\nThe rest of the paper is organized as follows. Section 2\npresents the studied online forum, rasikas.org, and the gen-\nerated Carnatic music dictionary. The methodology for\ncreating a complex network representation of the forum\ntext content is described in Section 3. We present and dis-\ncuss results related to the aforementioned network mea-\nsures in Section 4 and conclude in Section 5.\n2. DATA GATHERING\n2.1 Dictionary\nWe ﬁrst build a dictionary that will help us identify and ex-\ntract Carnatic music terms from a text. For that we gather\nthe editorial metadata of an extensive list of Carnatic mu-\nsic CD’s from MusicBrainz.org, an open music encyclope-\ndia. The metadata includes names of recordings, releases,\nworks (compositions), composers/lyricists and performers,\nand also information about raagas and taalas3, two key\nconcepts in Carnatic music. To improve our dictionary we\nalso consider the English Wikipedia as an additional source\nof information. Similar to [10], we obtain a list of Carnatic\nmusic terms from dbpedia.org, a machine-readable repre-\nsentation of Wikipedia. We start from the seed category\n“Carnatic music” and explore the inherent structure of the\ndbpedia categorization in order to get all the terms related\nto the seed. The ﬁnal dictionary is then created by merg-\ning MusicBrainz metadata and Wikipedia categories, and\nstored as a ﬂat taxonomy of category terms (e.g. raaga–\nbhairavi, intrument–mdridangam, etc.).\nThe main problem of this dictionary is that it suffers\nfrom noisiness/misspelling errors, mainly due to the di-\nverse transliterations to English of Indian languages terms.\nFor instance, the name Tyagaraja (a legendary composer of\nCarnatic music) can also be written as Thayagaraja, Thi-\nagaraja, Tyagayya, Thiyagaraja, Thagraja, etc. In order\nto clean the dictionary, we apply a string matching method\nbased on a linear combination of the longest common sub-\nsequence and Levenshtein algorithms [3] to ﬁnd all dupli-\ncate terms, which we manually ﬁlter to maintain a single\ncommon description for each of them.\n2.2 The rasikas.org forum\nThe text we analyze is extracted from rasikas.org, a dedi-\ncated forum of Carnatic music lovers. As many discussion\nforums, rasikas.org is divided into different sub-forums, 20\nin this case. Each forum contains a list of threads and each\nthread has a number of posts. Typically, a thread is consid-\nered to contain a topic which is discussed in all the posts\nof that thread. It is interesting to note the ratio of number\nof posts per thread (Table 1). A median value of 5 means\nthat half of the threads have only 5 posts or less. Regarding\n3Complementary to raaga, taala is the rhythmic framework for com-\nposition and improvisation.Num. topics 16;595\nNum. posts 192; 292\nPosts per thread \u0016=11:59,\u001b= 34:49,median = 5\nNum. active threads 1362 active in the last 12 months\nNum. users 4;332(with at least one post)\nNum. active users 929active in the last 12 months\nTable 1. Some statistics of rasikas.org forum.\nthe forum users, even though there are 4,332 users in total,\nonly a subset of them, 929, have been active in the last 12\nmonths (a user is considered to be active if she has written\nat least one post in the last Nmonths andMposts overall).\nWe crawled the entire rasikas.org forum and stored it\nlocally. Not all the sub-forums are of our interest, though.\nA few of the sub-forums are not directly related to music,\nwhilst some others discuss topics from other music cul-\ntures (e.g. Hindustani). We ﬁnally selected a subset of the\nsub-forums that we considered relevant for our study. This\nmade a total of 11 sub-forums, 14,309 threads and 172,249\nposts.\n3. METHODOLOGY\n3.1 Step 1: Text processing\nTo extract musically-related semantic information from\nrasikas.org we generate a complex network representation\nof it. Nonetheless, before building the network, we apply\nsome text processing techniques to match our Carnatic mu-\nsic dictionary against the forum posts. Hence, in the ﬁrst\nstep, we iterate over the posts of all the topics for a given\nsubset of sub-forums. For each post, the text is tokenized\nwith Penn Treebank, a classical tokenizing technique. The\nwords are then tagged using the Maxent Treebank part-of-\nspeech (POS) tagger. We use the NLTK toolkit4imple-\nmentation of both the tokenizer and the POS tagger. These\nmethods and implementations are classical choices in nat-\nural language processing and computational linguistics [5].\nOnce the text is tokenized and tagged, the method pro-\nceeds to match the dictionary of Carnatic music terms\nagainst the list of tagged tokens. Given that some terms\nin the dictionary are word n-grams (i.e. terms with more\nthan one word), the dictionary is sorted by the number\nof words, matching the longest terms ﬁrst. Additionally,\nstemmed adjectives and nouns provided by the POS tag-\nging are also included, except for stop words and words\nwith less than 3 characters. The non-matched words are\nnot removed from the list of tokens, but rather marked as\nnon-eligible. For example, the sentence “the difference be-\ntween AbhEri and dEvagAndhAram” is converted to “**\ndifference ** AbhEri ** dEvagAndhAram”, where ** de-\nnotes a non-eligible word.\n4http://www.nltk.orgFigure 1. A plot of a subnetwork containing Carnatic music terms with the highest degree. The thickness of the edges\nrepresents their weight.\nRank Raagas Taalas Instruments Performers Composers\n1 Nata Adi Violin Chembai Tyagaraja\n2 Kalyani Rupakam Mridangam Madurai Mani Iyer Annamacharya\n3 Bhairavi Chapu V ocal Charulatha Mani Purandara Dasa\n4 Ragamalika Jhampa Ghatam Kalpakam Swaminathan Swati Tirunal\n5 Kannada Misram Morsing Lalgudi Jayaraman Papanasam Sivan\nTable 2. Top-5 nodes with highest betweenness centrality in the network, organized by category.\n3.2 Step 2: Network creation\nIn the second step, an undirected weighted network is cre-\nated by iterating over the processed posts. Each matched\nterm is assigned to a node in the network, and an edge/link\nbetween two nodes is added if the two terms are close in\nthe text. The link weight accounts then for the number of\ntimes two matched terms appear close in the text.\nText closeness is deﬁned as the number of intermediate\nwords between two terms. Thus, we introduce a distance\nparameterLthat will determine which terms are associ-\nated with each other. Keeping the non-matched words in\nthe posts (although they are not ﬁnally eligible) is impor-\ntant for calculating this distance. Using the example from\nStep 1, AbhEri anddEvagAndhAram are considered to be\nat a distance of L= 2. Our assumption is that words that\nare closer in text are more likely to be related.\n3.3 Step 3: Network cleaning\nThe resulting network from step 2 is very dense (it has\n24;420nodes and 1;564; 893links). The average degree\nis 128.16, which is very high for such a small network. In\naddition, we ﬁnd that the network contains a lot of noise.\nIn particular, we ﬁnd the presence of many spurious nouns\nand adjectives. Therefore, we introduce a frequency thresh-\noldF, which ﬁlters out the nouns and adjectives that ap-\npear less than Ftimes.\nThresholdsLandFyield a more sparse network. How-\never, it could still be possible that some non-statistically\nsigniﬁcant term relations were reﬂected in the network links.\nThus, we decide to apply a sensible ﬁlter to the network\ntopology, the disparity ﬁlter [9]. The disparity ﬁlter is a lo-\ncal ﬁlter that compares the weights of all links attached toa given node against a null model, keeping only the links\nthat cannot be explained by the null model under a cer-\ntain conﬁdence level \u000b5. This conﬁdence level \u000bcan be\nthought of as a p-value (p = 1\u0000\u000b) assessing the statistical\nsigniﬁcance of a link.\n3.4 Network statistics and parameter conﬁguration\nAfter applying the three aforementioned steps, we obtain\nseveral network conﬁgurations depending on the different\nparameter values L,Fandp. After preliminary assess-\nment, which we omit due to space constraints, we decided\nto useL= 5,F= 10 andp= 0:01. Fig. 1 shows a\nsubset of the obtained network. With this conﬁguration we\nobtain a weighted undirected network of 10;928nodes (in-\ncluding nouns and adjectives) and 39;067edges, resulting\nin an average degree of 7:15. Furthermore, the network\nhas an average clustering coefﬁcient of 0:309 and a short-\nest path of 3:658, which are very common values for many\nother real-world networks [6].\n4. RESULTS AND DISCUSSION\n4.1 Node betweennes centrality\nThe resulting weighted network can be analyzed by using\ndifferent network measures. One of such measures is be-\ntweenness centrality. The betweenness centrality measures\nthe importance of a node to the network. It does so by ﬁnd-\ning the number of shortest paths from all the nodes to all\nother nodes that pass through that node [6].\n5The null model assumes that the strength of a given node is homoge-\nneously distributed among all its links.Parameter conﬁguration Num. matched Num. matched Hit % MRR\nperformers performer-instrument\npairs\nF= 10,L= 5,p= 0:01 104 63 95.24 95.24\nF= 10,L= 10 ,p= 0:01 114 70 80.00 85.48\nTable 3. Predicted performer/instrument pairs using frequent co-occurrences.\nTable 2 illustrates the top-5 nodes with highest between-\nness centrality for each term category. Regarding the raa-\ngas, Kalyani andBhairavi (two major raagas in Carnatic\nmusic) are two frequent choices of artists in order to do\na Raagam-Taanam-Pallavi, a complete exposition of their\nskills in a given raaga [12]. Ragamalika is not exactly a\nraaga per se, but a performance where the performer sings\nmore than a raaga in a single composition. In practice,\nhowever, since all the combinations are often named as\njustRagamalika, without referring to the constituent raa-\ngas, the term Ragamalika ends up being one of the most\nfrequent ones. Kannada is unfortunately an ambiguous\nterm. It can refer to a raaga or to the ofﬁcial language of\nthe south Indian state Karnataka. This is probably why the\nterm might have acquired more weight than, for instance,\nthe raaga Thodi, which is in principle more popular. As for\ntaalas, Adiis the most preferred taala in Carnatic compo-\nsitions (511 out of 917 recordings in our Carnatic music\ncollection in MusicBrainz are performed in Aditaala). In\nthe case of Carnatic music performers, Lalgudi Jayaraman\nis one of the violin trinity of Carnatic music, the three vi-\nolinists who are considered the irrefutable masters of the\nart. Moreover, he is also a renowned composer in the\nmodern times, which can explain why he was ranked high\nin the betweenness centrality. Finally, regarding Carnatic\ncomposers, Tyagaraja’s compositions constitute a signiﬁ-\ncant proportion of the Carnatic music repertoire (102 out\nof the 293 works in our Carnatic music collection were\ncomposed by Tyagaraja). Annamacharya is also a com-\nposer with a large number of keerthanas, many of which\nare sung either at the beginning or towards the end of a Car-\nnatic music concert. Keerthana is different from, and gen-\nerally not as elaborate as, the compositional form called\nKriti. Kriti forms the crux of the compositional repertoire\nin Carnatic music, and is considered to have evolved from\nKeerthana. Purandara Dasa is called the father of Carnatic\nmusic, both for his compositions and his systematization of\nCarnatic music learning process. Papanasam Sivan was a\nprominent composer from the state of Tamilnadu. Indeed,\nhe is often referred to as Tamil Tyagaraja. It happens that\nmany users of rasikas.org are from that region too.\n4.2 Term co-occurrences\nIn this section we analyze co-occurrences of Carnatic mu-\nsic terms. There are two possible ways to measure co-\noccurrence of terms in a network. We distinguish between\nfrequent and relevant co-occurrences.4.2.1 Frequent co-occurrences\nBy assuming that terms that co-occur most frequently have\na strong relation we can obtain much knowledge from the\nnetwork. As an example, we will show that network co-\noccurrent terms allow for correctly guessing the instrument\nof a performer. This is a non-trivial task whose accuracy\ncan be evaluated objectively, since ground truth data is\nrelatively easy to obtain. In particular, we use the same\nsources from which we derived the Carnatic music dictio-\nnary, Wikipedia and MusicBrainz. From MusicBrainz, we\nget all the relations between performers and recordings. In\nthe case of Wikipedia, we match a list of instruments (de-\nﬁned in our dictionary) against the text of Wikipedia ab-\nstracts. The ground truth is ﬁnally ﬁltered by removing\nthe cases where a performer has more than one instrument\n(vocals are also considered instruments here).\nTo evaluate the instrument-performer pairing task we\nuse the weight of the links to rank the list of instrument\nneighbors for each performer (i.e. to rank connected nodes\nrepresenting an instrument). Two measures are then used\nto evaluate the predicted instruments in the network: hit\npercentage and mean reciprocal rank (MRR). The hit per-\ncentage refers to correctly predicting the instrument in the\nﬁrst rank. Sometimes, however, the correct answer does\nnot fall in the ﬁrst rank, but rather in the second, third, or\nother ranks. This can be measured with MRR, which we\ndeﬁne as\nMRR =100\nNNX\nn=1rn; (1)\nwhereNis the number of co-occurring instruments and rn\nis 1 if and only if the n-th instrument is the correct one for\na given performer.\nAs we can observe in Table 3, by considering simple\nco-occurrences in the network we already achieve an accu-\nracy of 95%, which means that we correctly predict 60 out\nof the 63 performer-instrument pairs available. For com-\nparative purposes, we also include the results obtained by\nincreasing the link threshold parameter L, which increases\nthe density of the network substantially. Even though the\nlatter conﬁguration increases the number of matched per-\nformers and instruments per performers, the accuracy of\nthe predicted instruments drops signiﬁcantly, meaning that\na larger value of Lis also adding noise to the network.\n4.2.2 Relevant co-occurrences\nApart from evaluating co-occurrences by their frequency,\nwe also compute a relevance score for the co-occurrence.Raaga Raaga Relevance\nKedaram Gowla 0.121\nBhavani Bhavapriya 0.109\nManavati Manoranjami 0.092\nKalavati Yagapriya 0.088\nNadanamakriya Punnagavarali 0.081\nTable 4. Relevant co-occurrences of raagas with other raa-\ngas.\nRaaga Composer Relevance\nAbhang Tukaram 0.159\nYaman kalyani Vyasa Raya 0.149\nPharaz Dharmapuri Subbarayar 0.143\nReethi Gowlai Subbaraya Sastri 0.122\nAndolika Muthu Thandavar 0.108\nTable 5. Relevant co-occurences of raagas with com-\nposers.\nIn the network, this means that we compute a relevance\nweight for the edge between a pair of nodes. The relevance\nscoreRi;jfor a link between nodes iandjis obtained by\nRi;j=wi;j\n1\n2(di+dj); (2)\nwherewi;jis the weight of the link and dxis the degree of\nnodex. This score is giving more relevance to the nodes\nthat are more probable to have some relationship [6, 9].\nWe apply this relevance measure of co-occurrence to\ncombinations of the previously-mentioned term categories.\nIn this experiment we study two such combinations: raaga-\nraaga and raaga-composer. Tables 4 and 5 show the top-5\nmore relevant co-occurrences of these two combinations,\nwhich we now comment.\nRaaga-raaga By looking back at the rasikas.org discus-\nsion forum, we can conﬁrm that the co-occurrences found\nin Table 4 are related to discussions between different raa-\ngas, or similar raagas with different naming conventions.\nFor instance, Bhavapriya raaga is also called Bhavani by\ndisciples of the famous composer Muthuswami Deekshi-\ntar. For the co-occurrence of Kalavati andRagapriya, a\ndiscussion in a forum thread indicates that Kalavati and\nRagapriya are often confused with each other due to a few\nhistoric reasons (e.g. naming convention). Some members\nof the forum posted several facts and technical arguments\nthat are tied to both raagas6. It should be noted that al-\nthough our method helps us to ﬁnd these important dis-\ncussions in the forum, it does not have the knowledge that\nthey are in fact discussions. We believe that extracting con-\ntextual information of the co-occurrences in the text could\nhelp to predict the type of relation between two terms.\n6http://www.rasikas.org/forum/viewtopic.php?t=\n14435Raaga-composer Intuitively, a relevant co-occurrence of\na raaga and a composer might mean that a composer is\nknown by or uses more frequently a particular raaga. In-\ndeed, that is the case of the relations in Table 5. Vyasa\nRaya’s most famous composition, “Krishna Nee Bagane”\nis in Yaman Kalyani raaga. Dharmapuri Subbarayar has a\npopular composition in Pharaz raaga called “Smara Sun-\ndaranguni”. It is also the case of “Sevikka Vendumayya”\ncomposition by Muthu Thandavar which is in Andolika\nraaga . Contrasting a little bit with these good agreements,\nthe most relevant raaga-composer co-occurrences include\nAbhang, which is a devotional poetry and not a raaga. This\nis due to a misleading tag in our vocabulary suggesting that\nAbhang is a raaga name, which suggests that a more accu-\nrate cleaning process is needed for some terms.\n4.3 Term semantic relations\nThe aim of this last experiment is to extract semantically\nmeaningful relationships between pairs of Carnatic music\nterms. From the network perspective, given a pair of nodes,\nwe want to ﬁnd another node that is connected to both\nnodes, and that corresponds to a semantically meaningful\nrelationship concept. We call this node a connecting word.\nFor this experiment we use a predeﬁned list of connecting\nwords, including concepts of lineage or family (mother, fa-\nther, husband, uncle, etc.) and musical inﬂuence (guru or\ndisciple) to identify the relationship between pairs of com-\nposers and/or performers.\nA straightforward approach is to use the same network\nas before and match the list of predeﬁned connecting words\nin the common neighbors of a pair of nodes. However, the\nglobal nature of the network does not allow to capture the\nconnecting words correctly, since a connecting word can\nbe related to any of the two compared terms separately.\nThus, another approach has to be considered. A possi-\nble solution is to apply the proposed methodology locally.\nThat is, instead of creating a single, global network, the\nmethod described in Sec. 3 can be applied for each post\ntext individually. For each generated small network, we\nidentify all the common neighbors of a pair of composers\nand/or performers that are related to the concepts of lin-\neage and musical inﬂuence. This experiment was evalu-\nated manually using Wikipedia and by asking a Carnatic\nmusic expert.\nFrom a total of 24 relations found in the network, our\nmethod correctly infers 14 (58% of accuracy). A closer\nlook at the misclassiﬁed term relations reveals that many of\nthe wrong predictions were due to ambiguity problems of\nnatural language. For example, our method assigns the re-\nlation guru for the following pairs of performers:\n(Karaikudi Mani, G. Harishankar ) and (Karaikudi Mani,\nMysore Manjunath). However, the term guru can also be\nused as a preﬁx name of a person, in this case Guru\nKaraikudi Mani. Finally, given for instance the following\nsentence, “Abhisek is grandson of Palghat Raghu and dis-\nciple of P.S. Nayaranaswamy”, and since the name Abishek\nis not included in our dictionary, our method incorrectly in-\nfers a relation of disciple between Palghat Raghu andP .S.Nayaranaswamy. It is clear that more advanced natural\nlanguage processing techniques should be applied in order\nto solve these ambiguities. Nevertheless, our method is\nalready inferring some relations with a certain amount of\nconﬁdence, using very simple heuristics.\n5. CONCLUSION AND FUTURE WORK\nWe presented a method for extracting musically-meaningful\nsemantic information from an online discussion forum, ded-\nicated to the Carnatic art music tradition. For that, we de-\nﬁned a dictionary of Carnatic music terms (from concepts\nsuch as raagas, taalas, performers, etc.), and created an\nundirected weighted network by matching such dictionary\nagainst the forum posts. Three experiments were ran to\nstudy different characteristics of the resulting network, ex-\ntracting valuable information such as term relevance, term\nco-occurrence and term relations via semantically connect-\ning words. In the ﬁrst experiment, nodes with higher be-\ntweenness centrality were usually highly correlated with\nthe popularity of Carnatic music terms. In the second ex-\nperiment, we showed that our method is able to predict\nthe instrument of a performer with a 95% of accuracy.\nFurthermore, we were able to identify node pairings that\nare relevant discussions in the forum, using a relevant co-\noccurrence measure. Even though these discussions were\nfound, our methodology does not have the knowledge that\nthe co-occurrences are in fact discussions. Extracting con-\ntextual information of the co-occurrences in the text could\nhelp to predict the type of relation between two terms. In\nthe last experiment, we showed that with simple heuristics\none can predict semantic relations related to lineage and\nmusical inﬂuence with a certain level of conﬁdence.\nThere are many avenues for future work. Besides the\nextraction of contextual information and the use of more\nsophisticated natural language processing techniques, we\nplan to explore methods that can capture users’ opinions\nusing, for instance, algorithms from sentiment analysis [7].\nRegarding the forum structure, not all the posts or topics\nare relevant enough to be added to the network. There-\nfore, we want to ﬁnd techniques to impose a conﬁdence\nvalue per post, depending on the users’ relevance to the\nforum. Another relevant issue to be tackled is the use of\na more complete Carnatic music vocabulary. For that, we\nplan to manually improve the metadata that can be found\nin MusicBrainz and to use more resources to increment the\nsize of the dictionary. In order to ease reproducibility of\nour work and to stimulate further research on the topic,\nthe data and code used for these experiments are publicly-\navailable7.\n6. ACKNOWLEDGMENTS\nThis research was partly funded by the European Research\nCouncil under the European Union’s Seventh Framework\nProgram, as part of the CompMusic project (ERC\ngrant agreement 267583). Joan Serr `a acknowledges\n7http://www.dtic.upf.edu/\\%7Emsordo/research/\nismir2012/index.htmlAEDOC069/2010 from Consejo Superior de Investigacio-\nnes Cient ´ıﬁcas, 2009-SGR-1434 from Generalitat de Cata-\nlunya, TIN2009-13692-C03-01 from the Spanish Govern-\nment, and EU Feder Funds.\n7. REFERENCES\n[1] O. Celma, P. Cano, and P. Herrera. Search Sounds: An audio\ncrawler focused on weblogs. In Proc. of 7th Intl. Conf. on\nMusic Information Retrieval, Victoria, Canada, 2006.\n[2] Y . Chen, X.Q. Cheng, and Y .L. Huang. A wavelet-based\nmodel to recognize high-quality topics on web forum. In\nWeb Intelligence and Intelligent Agent Technology, 2008.\nIEEE/WIC/ACM Intl. Conf. on, 2008.\n[3] D. Gusﬁeld. Algorithms on strings, trees, and sequences:\ncomputer science and computational biology. Cambridge\nUniversity Press, 1997.\n[4] P. Lamere. Social tagging and Music Information Retrieval.\nJournal of New Music Research, 37(2):101–114, 2008.\n[5] C.D. Manning and H. Sch ¨utze. Foundations of statistical nat-\nural language processing. MIT Press, 1999.\n[6] M.E.J. Newman. Networks: An Introduction. Oxford Univer-\nsity Press, 2010.\n[7] B. Pang and L. Lee. Opinion mining and sentiment analysis.\nFoundations and Trends in Information Retrieval, 2(1-2):1–\n135, 2008.\n[8] M. Schedl and T. Pohle. Enlightening the Sun: A User Inter-\nface to Explore Music Artists via Multimedia Content. Multi-\nmedia Tools and Applications: Special Issue on Semantic and\nDigital Media Technologies, 49(1):101–118, 2010.\n[9] M. Serrano, M. Bogu ˜n´a, and A. Vespignani. Extracting the\nmultiscale backbone of complex weighted networks. Proc. of\nthe National Academy of Sciences of the USA, 2009.\n[10] M. Sordo, F. Gouyon, and L. Sarmento. A Method for Ob-\ntaining Semantic Facets of Music Tags. In 1st Workshop On\nMusic Recommendation And Discovery, ACM RecSys, 2010.\n[11] K. Stanoevska-Slabeva. Toward a community-oriented design\nof internet platforms. Intl. Journal of Electronic Commerce,\n6(3):71–95, 2002.\n[12] T. Viswanathan and M.H. Allen. Music in South India. Ox-\nford University Press, 2004.\n[13] M. Weimer and I. Gurevych. Predicting the perceived quality\nof web forum posts. In Proc. of the 2007 Conf. on Recent\nAdvances in Natural Language Processing, 2007.\n[14] B. Whitman and S. Lawrence. Inferring Descriptions and\nSimilarity for Music from Community Metadata. In Proc. of\nIntl. Computer Music Conf., 2002.\n[15] J. Yang, R. Cai, Y . Wang, J. Zhu, L. Zhang, and W. Ma. In-\ncorporating Site-Level Knowledge to Extract Structured Data\nfrom Web Forums. In Proc. of the 18th Intl. Conf. on World\nWide Web, 2009.\n[16] T. Zhu, B. Wu, and B. Wang. Extracting relational network\nfrom the online forums: Methods and applications. In Emer-\ngency Management and Management Sciences, IEEE Intl.\nConf. on, 2010."
    },
    {
        "title": "Real-time Online Singing Voice Separation from Monaural Recordings Using Robust Low-rank Modeling.",
        "author": [
            "Pablo Sprechmann",
            "Alexander M. Bronstein",
            "Guillermo Sapiro"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414820",
        "url": "https://doi.org/10.5281/zenodo.1414820",
        "ee": "https://zenodo.org/records/1414820/files/SprechmannBS12.pdf",
        "abstract": "Separating the leading vocals from the musical ac- companiment is a challenging task that appears nat- urally in several music processing applications. Ro- bust principal component analysis (RPCA) has been recently employed to this problem producing very suc- cessful results. The method decomposes the signal into a low-rank component corresponding to the ac- companiment with its repetitive structure, and a sparse component corresponding to the voice with its quasi- harmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust low- rank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low la- tency and a fraction of the complexity of the original optimization method. These approximants allow in- corporating elements of unsupervised, semi- and fully- supervised learning into the RPCA and RNMF frame- works. Our basic implementation shows several or- ders of magnitude speedup compared to the exact sol- vers with no performance degradation, and allows on- line and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance.",
        "zenodo_id": 1414820,
        "dblp_key": "conf/ismir/SprechmannBS12",
        "keywords": [
            "audio",
            "processing",
            "unsupervised",
            "semi-supervised",
            "fully-supervised",
            "principal",
            "component",
            "analysis",
            "RPCA",
            "RNMF"
        ],
        "content": "REAL-TIME ONLINE SINGING VOICE SEPARATION FROM\nMONAURAL RECORDINGS USING ROBUST LOW-RANK\nMODELING\nPablo Sprechmann\nUniversity of Minnesota\nsprec009@umn.eduAlex Bronstein\nTel Aviv University\nbron@eng.tau.ac.ilGuillermo Sapiro\nUniversity of Minnesota\nguille@umn.edu\nABSTRACT\nSeparating the leading vocals from the musical ac-\ncompaniment is a challenging task that appears nat-\nurally in several music processing applications. Ro-\nbust principal component analysis (RPCA) has been\nrecently employed to this problem producing very suc-\ncessful results. The method decomposes the signal\ninto a low-rank component corresponding to the ac-\ncompaniment with its repetitive structure, and a sparse\ncomponent corresponding to the voice with its quasi-\nharmonic structure. In this paper we ﬁrst introduce a\nnon-negative variant of RPCA, termed as robust low-\nrank non-negative matrix factorization (RNMF). This\nnew framework better suits audio applications. We\nthen propose two efﬁcient feed-forward architectures\nthat approximate the RPCA and RNMF with low la-\ntency and a fraction of the complexity of the original\noptimization method. These approximants allow in-\ncorporating elements of unsupervised, semi- and fully-\nsupervised learning into the RPCA and RNMF frame-\nworks. Our basic implementation shows several or-\nders of magnitude speedup compared to the exact sol-\nvers with no performance degradation, and allows on-\nline and faster-than-real-time processing. Evaluation\non the MIR-1K dataset demonstrates state-of-the-art\nperformance.\n1. INTRODUCTION\nThe leading voice in musical pieces carries valuable\ninformation about the song. A system capable of sep-\narating the singing voice from the music accompani-\nment can be used to facilitate a number of applications\nsuch as music information retrieval, singer identiﬁca-\nPermission to make digital or hard copies of all or part of this work\nfor personal or classroom use is granted without fee provided that\ncopies are not made or distributed for proﬁt or commercial advan-\ntage and that copies bear this notice and the full citation on the ﬁrst\npage.\nc\r2012 International Society for Music Information Retrieval.tion, or lyric recognition.\nSeparating the leading singing voice from the mu-\nsical background from a monaural recording is very\nchallenging. Existing approaches can be classiﬁed ac-\ncording to the level of supervision that they require.\nSupervised approaches tend to have a model for either\nthe musical background, the singing voice, or both,\nand in general map the mixture signals onto a feature\nspace where the separation is performed, e.g. [4, 11,\n15, 19]. A common drawback of these methods is the\nneed to identify the vocal segments beforehand, typi-\ncally using features such as the Mel-Frequency Cep-\nstrum Coefﬁcients (MFCC). Unsupervised approaches\nmake basic fundamental assumptions requiring no pri-\nor training or particular features. For example, in [13]\nthe authors tackle the separation by extracting the re-\npeating background (music) from the non-repeating\nforeground (voice). Most relevant for our work is the\nmethod proposed in [9]. The authors model the repet-\nitive structure of the accompaniment with a low-rank\nlinear model, while the singing voice is regarded as\nsparse and non-repetitive. The separation is performed\nusing robust PCA (RPCA) [3], producing state-of-the-\nart results. Common drawbacks of unsupervised ap-\nproaches include the requirement to observe the whole\naudio track to perform the separation and the fact that,\nunlike supervised models, the obtained sources might\nnot follow known characteristics of the signals.\nIn this paper, we consider the promising results pre-\nsented in [9] as a starting point. We ﬁrst develop an\nextension of RPCA in which the low rank model is\nrepresented as a non-negative linear combination of\nnon-negative basis vectors. This is done following re-\ncent results connecting non-convex optimization with\nnuclear norm optimization [17,18] (further references\nare given in Section 2). As with standard non-negative\nmatrix factorization (NMF) methods, this new model\nis more appropriate to represent audio signals, being\napplied to the magnitude of the spectrum. The use of\nrobust NMF (RNMF) is not restricted to this applica-\ntion and the usage in combination with divergences\nin lieu of Euclidean distances is straightforward. The\n1proposed framework can also be seen as an extension\nof the robustiﬁcation of NMF introduced in [22]; not\nonly does our model consider a sparse variable ac-\ncounting for outliers (singing voice), but it also adds\na regularization term that minimizes the rank of the\nlinear model.\nIn Section 3 we show that the RPCA and RNMF\nframeworks induce an architecture of multi-layer feed-\nforward networks designed to approximate the output\nof the exact optimization algorithms at a fraction of\ntheir computational cost and with no decrease in per-\nformance in our various experiments. Moreover, this\nnew framework allows to incorporate unsupervised,\nsemi- and fully-supervised learning into RPCA and\nRNMF. In this way, we aim at taking the advantages\nof the unsupervised methods while minimizing their\ndrawbacks via realistic learning. When combined with\nlearning as here proposed, the obtained networks pro-\nduce over 1dB improvement in the signal-to-distortion\nratio when compared to the optimization-based RPCA\n(extensive experimental results are presented in Sec-\ntion 4), and, after the ofﬂine learning, are computable\nonline and faster than real time without the need to\nobserve the whole audio ﬁle.\nThese proposed networks are closely related to the\nones introduced in [6], used to produce meaningful\naudio features for music style and gender classiﬁca-\ntion [7]. These approaches are examples of recent suc-\ncessful efforts in the machine learning community to\nproduce fast trainable (auto-)encoders of sparse fea-\ntures of visual and audio signals (see [5, 16] and ref-\nerences therein). While the work in this paper comes\nfrom these ideas, it presents a fundamental difference\nin the sense that the proposed networks do not com-\npute features, but perform the full separation of the\nsinging voice from the musical accompaniment.\n2. LOW-RANK SPARSE MODELS\n2.1 Robust PCA\nPrincipal component analysis (PCA) is the most widely\nused statistical technique for dimensionality reduction.\nIts performance is, however, highly sensitive to the\npresence of samples not following the assumed model\n(subspace); even a single outlier in the data matrix can\nrender the estimation of the low rank component arbi-\ntrarily far from the true model. In [3, 21], a very el-\negant remedy was developed for this shortcoming, in\nwhich the low rank matrix is determined as the min-\nimizer of a convex program. The basic idea is to de-\ncompose the data matrix XasX=L+O2Rm\u0002n,\nwhere Lis a low rank matrix and Oan error matrix\nwith a sparse number of non-zero coefﬁcients with\narbitrarily large magnitude. RPCA can be solved byminimizing the convex program\nmin\nL;OkLk\u0003+\u0015kOk1s.t.X=L+O; (1)\nwherek\u0001k\u0003denotes the matrix nuclear norm, deﬁned\nas the sum of the singular values (the convex surro-\ngate of the rank), and \u0015is a positive scalar parameter\ncontrolling the sparsity of the outliers. Several efﬁ-\ncient optimization algorithms have been proposed for\nsolving (1) as, for example, the augmented Lagrangian\napproach presented in [12].\nWhen the observations are noisy, the equality con-\nstraint in (1) no longer holds. The RPCA model can\nbe reformulated as\nmin\nL;OkLk\u0003+\u0015kOk1s.t.kX\u0000L\u0000Ok2\nF\u0014\u000f;(2)\nwithk\u0001k Fdenoting the Frobenius norm, and \u000fa pa-\nrameter controlling the approximation error [21].\n2.2 Robust PCA via non-convex factorization\nIn this paper, we tackle the RPCA problem by solving\nthe unconstrained optimization problem\nmin\nL;O1\n2kX\u0000L\u0000Ok2\nF+\u0015\u0003kLk\u0003+\u0015kOk1:(3)\nThis formulation is equivalent to (2) in the sense that\nfor every\u000f >0one can ﬁnd a \u0015\u0003>0such that both\nproblems admit the same solution. The unconstrained\nformulation can be efﬁciently optimized via proximal\nmethods as in [3].\nIn [17] it was shown that the nuclear norm of a ma-\ntrix can be reformulated as a penalty over all possible\nfactorizations,\nkLk\u0003= min\nU;S1\n2kUk2\nF+1\n2kSk2\nFs.t.US=L;(4)\nwith the minimum achieved via Singular Value De-\ncomposition (SVD) [14]. In (3), neither the rank of\nLnor the level of sparsity in Oare assumed known\na priori. However, in common applications, it is rea-\nsonable to have a rough upper bound, rank(L) \u0014q.\nCombining this with (4), we reformulate (3) as the\nminimization\nmin\nU;S;O1\n2kX\u0000US\u0000Ok2\nF+\n\u0015\u0003\n2(kUk2\nF+kSk2\nF) +\u0015kOk1(5)\noverU2Rm\u0002q,S2Rq\u0002n, andO2Rm\u0002n. This\ndecomposition reveals interesting structure hidden in\nthe problem. The low rank component can now be\nthought of as an under-complete dictionary U, withq\natoms, multiplied by a matrix Scontaining the corre-\nsponding coefﬁcients for each data vector in X. This\n2interpretation brings the RPCA problem close to that\nof matrix factorization and sparse coding.\nThis new factorized formulation drastically reduces\nthe number of optimization variables from 2nm to\nnm+q(n+m). While problem (5) is no longer con-\nvex, it can be shown that any of its stationary points\nsatisfyingjjX\u0000US\u0000Ojj2\n2\u0014\u0015\u0003, is an optimal solu-\ntion of (5) [14]. Thus, the problem can be solved using\nalternating minimization or block coordinate schemes,\nwithout the risk of remaining stuck in a local mini-\nmum. This redounds in a signiﬁcant speed-up in the\noptimization [18].\n2.3 Robust NMF\nIn many applications, such as spectrogram decompo-\nsitions, it desirable to ﬁnd non-negative factorizations.\nThis is in the heart of the non-negative matrix factor-\nization paradigm . We now extend (5) to consider the\nlow rank and the outlier terms to be non-negative,\nmin\nU\u00150;S\u00150;O\u001501\n2kX\u0000US\u0000Ok2\nF+\n\u0015\u0003\n2(kUk2\nF+kSk2\nF) +\u0015kOk1:(6)\nThis new formulation is no longer equivalent to (3).\nIn fact, applying (4) directly to the matrix US, we ob-\ntain^U^Swith the factors ^Sand^Unot being necessar-\nily non-negative. Adding the non-negativity constraint\nproduces the inequality\njjUSjj\u0003\u00141\n2min\n^S\u00150; ^U\u00150jj^Ujj2\nF+jj^Sjj2\nF: (7)\nThus, the sum of the Frobenius norms of the non-\nnegative matrices SandUregularizes an upper bound\nof the nuclear norm of their product.\nStandard NMF is obtained as a particular case by\nsetting to zero both \u0015\u0003and\u0015, while the robust ver-\nsion of NMF introduced in [22] is obtained when only\n\u0015\u0003is selected as zero. In this paper we use RNMF\nas stated in (6), however its extension to more general\nﬁtting terms such as \f-divergences is straightforward.\nProblem (6) can be optimized using multiplicative al-\ngorithms, commonly used in the NMF context.\n2.4 Robust non-negative projections\nLet us now assume to be given a low dimensional\nmodel, U2Rm\u0002q, learned from some data X\u0019\nUS+O2Rm\u0002n. A new input vector xdrawn from\nthe same distribution as Xcan be decomposed into\nx=Us+n+o, where Usrepresents the low dimen-\nsional component, nis a small perturbation, and ois\na sparse outlier vector. It can be obtained via\nmin\ns\u00150;o\u001501\n2kx\u0000Us\u0000ok2\n2+\u0015\u0003\n2ksk2\n2+\u0015kok1;(8)\nr e y a L 1 r e y a L TU t H W\nx bin\nzin0bout\nzoutbin\nzins l\nobout=bin+. . .\nH(zout−zin)\nzout=. . .\nmax{bin−t,0}zout=. . .\nmax{bin−t,0}· · ·\n· · ·zoutFigure 1. RNMF encoder architecture with Tlayers.\na convex problem similar to the one of standard sparse\ncoding. The solution can be obtained via proximal\nmethods [1], which split the objective function (8) into\na smooth part (the ﬁrst two terms), and a non-differen-\ntiable part (the `1norm of the outliers vector). Proxi-\nmal methods iterate between a gradient descent on the\nsmooth function and an application of the proximal\noperator (which assumes a closed form of one-sided\nsoft-thresholding), as detailed in Algorithm 1. This\nalgorithm is conceptually very similar to the popular\niterative shrinkage and thresholding algorithm (ISTA)\n[2]. We do not use this algorithm as an explicit tool,\nbut rather as a motivation of the architecture of a feed-\nforward network capable of accurately performing the\nseparation in real time, as discussed next.\ninput : Data x, dictionary U.\noutput: Nonnegative coefﬁcient vector sand\nnonnegative outlier vector o.\nDeﬁne\nH=I\u00001\n\u000b\u0012\nUTU+\u0015\u0003I UT\nU (1 +\u0015\u0003)I\u0013\n,\nW=1\n\u000b\u0012\nUT\nI\u0013\n, andt=\u0015\n\u000b\u00120\n1\u0013\n.\nInitialize z=0,b=Wx.\nrepeat\ny= maxfb\u0000t;0g\nb=b+H(y\u0000z)\nz=y\nuntil until convergence ;\nOutput (o;s) =z.\nAlgorithm 1: RNMF given the dictionary U.\n3. FAST ROBUST SPARSE MODELING\nTo avoid the computational complexity inherent to ex-\nact sparse coding algorithms, it has been recently pro-\nposed to learn non-linear regressors capable of pro-\nducing good approximations in a ﬁxed amount of time\n[6,10]. We follow these ideas to obtain encoders capa-\nble of efﬁciently approximating the solution of RPCA\nand RNMF.1We ﬁrst discuss the general framework\nand then describe speciﬁc uses.\n1Due to space constraints, we show details only for RNMF;\nRPCA can be obtained by removing the non-negativity constraints\nand modifying the proximal operator.\n3We aim at constructing a parametric regressor z=\n(o;s) = h(x;\u0002), with some set of parameters, col-\nlectively denoted as \u0002, capable of accurately perform-\ning the singing voice separation for a given training\nsampleX=fx1;:::;xng. Here, each xirepresents\nthe magnitude spectrum of a mixture of voice and mu-\nsic; training samples may come from many different\nsingers and songs.\nAs in [6], we design an architecture for the en-\ncoders based on an exact optimization algorithm, in\nthis case Algorithm 1. We propose a multi-layer ar-\ntiﬁcial neural networks where each layer implements\na single iteration of the algorithm, as depicted in Fig-\nure 1. The parameters of the network are the matrices\nWandHand the thresholds t.2These encoder ar-\nchitectures are continuous and almost everywhere C1\nwith respect to the parameters, allowing the use of\n(sub)gradient descent methods for training.\nWe train the encoders by minimizing over Xfunc-\ntions of the form\nL(\u0002) =1\njXjX\nxi2XL(\u0002; xi); (9)\nwhereL(\u0002; xi)is a function that measures the qual-\nity of the code zi=h(xi;\u0002). Speciﬁcally, we itera-\ntively select a random subset of Xand then update the\nnetwork parameters as \u0002 \u0002\u0000\u0016@L(\u0002)\n@\u0002, where\u0016\nis a decaying step, repeating the process until conver-\ngence. The decoder is just a linear operator given by a\ndictionary U, see Figure 1.\nOnce trained, the parameters \u0002and the dictionary\nUare ﬁxed, and the network is used to sequentially\nprocess new data. The latency of both the RPCA and\nRNMF networks (referred henceforth as NN RPCA\nandNN RNMF, respectively) is of the order of a single\nSTFT frame (hundreds of milliseconds), while the ex-\nact algorithms require the entire signal to be observed.\n3.1 Training regimes\nTraining of the proposed RPCA and RNMF encoders\nis possible under different regimes. We refer as su-\npervised to the setting where the training set consists\nof the mixed signal xi=o\u0003\ni+l\u0003\ni, and the synchro-\nnized ground truth voice and accompaniment signals\no\u0003\niandl\u0003\ni(each vector corresponding to the magni-\ntude spectrogram). In that case, we set L(\u0002; xi) =\njjUsi\u0000l\u0003\nijj2\n2+jjoi\u0000o\u0003\nijj2\n2, with (oi;si) =h(xi;\u0002).\nForNN RPCA, the dictionary Uis established using\nSVD applied to the clean accompaniment samples, l\u0003\ni,\nwhile for NN RNMF, the non-negative dictionary U\nis constructed running the multiplicative RNMF algo-\nrithm on the training data.\n2In the network, extra ﬂexibility is obtained by learning different\nthresholds tifor each component.Table 1. Performance on the recovered vocal track on\nMIR-1K.\nMethod GNSDR GSNR GSAR GSIR\nIdeal freq. mask 13.48 5.46 13.65 31.22\nADMoM RPCA [9] 5.00 2.38 6.68 13.76\nProximal RPCA 5.48 3.29 7.02 13.91\nNN RPCA Untrained 5.30 2.66 6.80 13.00\nNN RPCA Unsupervised 5.62 2.87 6.90 14.02\nNN RPCA Supervised 6.38 3.18 7.22 16.47\nNN RPCA Dict. update 6.42 3.19 7.23 16.57\nMultiplicative RNMF 5.60 3.39 6.94 14.67\nNN RNMF Untrained 1.62 0.00 5.85 5.13\nNN RNMF Unsupervised 5.00 2.66 6.63 11.89\nNN NMF Supervised 6.36 3.37 7.10 16.96\nNN RNMF Dict. update 6.55 3.55 7.24 17.65\nWe refer as semi-supervised to the setting in which\nisolated samples of voice and background are avail-\nable, but are not synchronized (the xiare now either\nthe voice or the accompaniment). The training of the\nnetwork is performed in the same way as the super-\nvised case, but setting to zero the missing source.\nFinally, in the unsupervised setting we only have\naccess to mixtures as training data and the objective\nL(\u0002; xi) =1\n2kxi\u0000Usi\u0000oik2\n2+\u0015\u0003\n2ksik2\n2+\u0015koik1\nis used to directly minimize the cost in (6).\nDictionary adaptation. The performance of both the\nRPCA and RNMF networks can be further improved\nif the dictionary U(decoder) is updated during the\ntraining. In the unsupervised setting, for NN RPCA,\nUis updated via gradient descent as before, while in\nNN RNMF via the standard multiplicative update,\nU U\fYST\nU(SST+\u0015\u0003I); (10)\nwhere X= (x1;:::;xn)is the input matrix, S=\n(s1;:::;sn)is the matrix of the corresponding codes,\nY= (x1\u0000o1;:::;xn\u0000on), and\fand the fraction\ndenote, respectively, element-wise multiplication and\ndivision. This update minimizes the objective in (6)\nfor ﬁxed OandS, and is guaranteed to preserve the\nnon-negativity of U. Analogously, in the semi- and\nfully-supervised scenarios, Ucan be updated by min-\nimizing the corresponding L(\u0002) using the ground-\ntruth music accompaniment. Again using gradient de-\nscent and multiplicative updates for RPCA and RNMF\nrespectively.\n4. EXPERIMENTAL RESULTS\nDataset. We evaluate the separation performance of\nthe proposed methods on the MIR-1K dataset [8], con-\ntaining 1000 16 kHz clips extracted from 110Chinese\nkaraoke songs performed by 19amateur singers (11\nmales and 8females). Each clip duration ranges from\n4to13seconds, totaling about 133minutes. We re-\nserved about 23minutes of audio sang by one male\n43.544.555.566.57\n0 5 10 15 20ADMoM RPCA\nProx. RPCA\nNN RPCA\nMult. RNMF\nNN RNMF\nNumber of layers TGNSDR [dB]  \nRank upper bound qGNSDR [dB]  \n55.25.45.65.866.26.46.66.87\n5 10 15 20 25 30 35 40 45 50ADMoM RPCA\nProx. RPCA\nNN RPCA\nMult. RNMF\nNN RNMFFigure 2. Performance of the supervised NN RPCA andNN RNMF on the MIR-1K dataset for different number of layers T\n(left, qﬁxed to 20), and values of the rank bound q(right, Tﬁxed to 10). GNSDR of the recovered vocal track is used as the\ncomparison criterion. For reference, the performance of exact RPCA and RNMF is given.\nand one female singers (abjones andamy) for the pur-\npose of training; the remaining 110minutes of 17sin-\ngers were used for testing. The voice and the music\ntracks were mixed linearly with equal energy.\nEvaluation. As the evaluation criteria, we used the\nBSS-EV AL metrics [20], which calculate the source-\nto-distortion ratio (SDR),3thesource-to-artifacts ra-\ntio(SAR), and the source-to-interference ratio (SIR).\nAs in [9], we computed the global normalized SDR,\nGNSDR =NX\ni=1\u000ei(SDR(^s;s)\u0000SDR(x;s));\nwhere ^sandsare the corresponding original and esti-\nmated voice signal, xis the mixture, \u000eiis the relative\nduration of each of the Ntesting pieces. Preﬁx “G”\nindicates average sample performance,e.g. GSAR. We\nalso computed the signal-to-noise ratio (SNR).\nComparison of separation methods. We evaluated\nthe proposed NN RPCA andNN RNMF using the dif-\nferent training settings discussed in Section 3.1. In all\nour examples (except when explicitly mentioned), we\nusedT= 10 layers andq= 20 . We compare these re-\nsult against three exact solvers: ADMoM RPCA solv-\ning (1) with \u0015= 1=pn(as suggested in [9]) via the\nalternating direction method of multipliers [12], for\nwhich the code from [9] was used; Proximal RPCA\nsolving (3) using the proximal method from [3], with\n\u0015=p\n2n\u001b and\u0015\u0003=p\n2\u001bwith\u001b= 0:3set follow-\ning [3]; and Multiplicative RNMF solving (6) using\nthe standard multiplicative algorithm.\nIn all experiments, the spectrogram of each mix-\nture was computed using a window size of 1024 and\na step size of 256samples (at 16KHz sampling rate).\nTraining was performed using 1000 safe-guarded gra-\ndient descent iterations on a random subset of 10:000\nspectral frames for training and the same amount of\ndistinct frames for cross-validation.\n3In this work the SDR is computed using the latest release of the\nBSS-EV AL code. The reported values are higher (equally for all\nalgorithms) than the ones reported in [9], since they used the older\nrelease of that package.Table 1 summarizes the performance of the com-\npared methods. The best performance is achieved by\ntheNN RNMF with trained dictionary. The use of\nthe proximal RPCA algorithm allowing for inexact re-\nconstruction of the data (thus accounting for unstruc-\ntured noise) gives almost 0:5dB improvement over\n[9]. The use of unsupervised training was more suc-\ncessful in the NN RPCA; however, both NN RPCA and\nNN RNMF outperform ADMoM RPCA.\nThe complexity of the proposed systems is signif-\nicantly lower to the one of exact algorithms: our un-\noptimized Matlab code that uses GPU acceleration is\ncapable of computing the networks about 70faster\nthan real time, while a preliminary implementation on\niPhone 4S is online and 6\u00007times faster than real\ntime (after ofﬂine training).\nParameter selection. We also evaluated the perfor-\nmance of the supervised RPCA and RNMF networks\nas a function of the two principal parameters: the num-\nber of layers Tand the rank bound q, see Figure 2.\nSupervised learning has a dramatic effect on the\nperformance of the networks. With just two layers, the\nRPCA network already outperforms the exact RPCA\nalgorithms; as a reference, an untrained network, with\nthe parameters W;H, and tset according to Algo-\nrithm 1, requires over 15layers to approach this per-\nformance. This phenomenon is even more pronounced\nin the case of RNMF. The inﬂuence of the number\nof layers quickly saturates; slight oscillations in the\nGNDSR are due to the randomization used at training.\nIn contrast, the effect of qis less dramatic. The\nnetworks outperform the exact algorithms already for\nq= 5 and the performance saturates for q\u001530.\nThis is radically different from the behavior of stan-\ndard NMF approaches, in which setting the number\nof columns in the non-negative factor Usigniﬁcantly\naffects the performance. In fact, RNMF with \u0015\u0003= 0\nas [22] yields 5:60 dB GNSDR for q= 1, which drops\nto2:88 dB forq= 3and to\u00002:5 dB forq= 10.\nSupervised training settings. We evaluated the in-\nﬂuence of the different training regimes on the perfor-\n5Table 2. Performance of NN RNMF on the vocal trak of\nSunrise song. Audio ﬁles are available for download here\nMethod NSDR SNR SAR SIR\nIdeal freq. mask 14.98 5.84 18.46 39.40\nADMoM RPCA [9] 1.61 2.99 11.13 6.60\nSupervised (MIR-1K ) 7.16 4.86 14.21 13.25\nSupervised (We are in love) 7.85 5.47 15.35 13.59\nSupervised (Sunrise) 10.93 5.67 16.16 19.20\nSemi-supervised (We are in love) 7.35 4.69 11.39 20.01\nSemi-supervised (Sunrise) 8.46 5.11 12.20 23.97\nmance of the networks on Shannon Hurley’s song Sun-\nrise, available from archive.org. The song was\nresampled at 16kHz and voice was artiﬁcially mixed\nwith the guitar accompaniment with equal energies.\nThree distinct datasets were used for training the nets:\ntwo singers from MIR-1K used in the previous experi-\nments; another Shannon Hurley’s song We are in love;\nand the same Sunrise, song on which the testing was\nperformed (given only for comparison). Supervised\nand semi-supervised regimes were used.\nTable 2 summarizes the obtained results. RNMF\nnetworks trained using mixtures from MIR-1K outper-\nform [9] by nearly 5:5dB GNSDR; training on more\nsinger-speciﬁc data (We are in love song) improves\nthis result by about 0:7dB. ; ﬁnally, training on a mix-\nture from the same song yields over 3:5dB improve-\nment. We conclude that training the networks on un-\nrelated singers and accompaniments already achieves\nvery high performance. Semi-supervised training on\ntheWe are in love song yields a minor improvement\nover MIR-1K, and cedes 0:5dB to the fully-supervised\ntraining. We conclude that in the absence of synchro-\nnized voice and music tracks for supervised training,\nsemi-supervised training still produces comparable re-\nsults.\n5. CONCLUSION\nMarrying ideas from convex optimization with multi-\nlayer neural networks, we have developed efﬁcient ar-\nchitectures for real-time online single-channel sepa-\nration of singing voice from musical accompaniment.\nOur approach achieves state-of-the-art results on the\nMIR-1K datasets with orders of magnitude improve-\nment in runtime and latency. In future work, we are\ngoing to extend this framework to denoising and si-\nmultaneous separation and speaker identiﬁcation.\n6. REFERENCES\n[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex opti-\nmization with sparsity-inducing norms. In Optimization for Ma-\nchine Learning. MIT Press, 2011.\n[2] A. Beck and M. Teboulle. A fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM J.\nImg. Sci., 2:183–202, March 2009.[3] E. Cand `es, X. Li, Y . Ma, and J. Wright. Robust principal com-\nponent analysis? Journal of the ACM, 58(3), May 2011.\n[4] J.-L. Durrieu, B. David, and G. Richard. A musically motivated\nmid-level representation for pitch estimation and musical audio\nsource separation. J. Sel. Topics Signal Processing, 5(6):1180–\n1191, 2011.\n[5] I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Y . Ng. Measuring\ninvariances in deep networks. In NIPS, pages 646–654. 2009.\n[6] K. Gregor and Y . LeCun. Learning fast approximations of\nsparse coding. In ICML, pages 399–406, 2010.\n[7] M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y . LeCun. Unsu-\npervised learning of sparse features for scalable audio classiﬁ-\ncation. In ISMIR, 2011.\n[8] C.L. Hsu and J.S.R. Jang. On the improvement of singing voice\nseparation for monaural recordings using the MIR-1K dataset.\nIEEE Trans. on Audio, Speech, and Lang. Proc., 18(2):310–\n319, 2010.\n[9] P.-S. Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-\nJohnson. Singing-voice separation from monaural recordings\nusing robust principal component analysis. In ICASSP, 2012.\n[10] K. Kavukcuoglu, M.A. Ranzato, and Y . LeCun. Fast inference\nin sparse coding algorithms with applications to object recogni-\ntion. arXiv:1010.3467, 2010.\n[11] Y . Li and D. L. Wang. Separation of singing voice from music\naccompaniment for monaural recordings. IEEE Trans. on Au-\ndio, Speech & Lang. Proc., 15(4):1475–1487, 2007.\n[12] Z. Lin, M. Chen, and Y . Ma. The augmented lagrange multi-\nplier method for exact recovery of corrupted low-rank matrices.\nArxiv preprint arXiv:1009.5055, 2010.\n[13] A. Liutkus, Z. Rai, R. Badeau, B. Pardo, and G. Richard. Adap-\ntive ﬁltering for music/voice separation exploiting the repeating\nmusical structure. In ICASSP, 2012.\n[14] G. Mateos and G. B. Giannakis. Robust PCA as bi-\nlinear decomposition with outlier-sparsity regularization.\narXiv.org:1111.1788, 2011.\n[15] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval. Adapta-\ntion of bayesian models for single-channel source separation\nand its application to voice/music separation in popular songs.\nIEEE Trans. on Audio, Speech & Lang. Proc., 15(5):1564–\n1578, 2007.\n[16] M. A. Ranzato, F. J. Huang, Y .-L. Boureau, and Y . LeCun. Un-\nsupervised learning of invariant feature hierarchies with appli-\ncations to object recognition. In CVPR, 2007.\n[17] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-\nrank solutions of linear matrix equations via nuclear norm min-\nimization. SIAM Rev., 52(3):471–501, 2010.\n[18] B. Recht and C. R ´e. Parallel stochastic gradient algorithms for\nlarge-scale matrix completion. 2011.\n[19] S. Vembu and S. Baumann. Separation of vocals from poly-\nphonic audio recordings. In ISMIR, pages 337–344, 2005.\n[20] E. Vincent, R. Gribonval, and C. F ´evotte. Performance mea-\nsurement in blind audio source separation. IEEE Trans. on Au-\ndio, Speech, and Lang. Proc., 14(4):1462–1469, 2006.\n[21] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier\npursuit. In NIPS, pages 2496–2504. 2010.\n[22] L. Zhang, Z. Chen, M. Zheng, and X. He. Robust non-negative\nmatrix factorization. Frontiers of Electrical and Electronic En-\ngineering in China, 6:192–200, 2011.\n6"
    },
    {
        "title": "Listening Level Changes Music Similarity.",
        "author": [
            "Michael Terrell",
            "György Fazekas",
            "Andrew Simpson 0003",
            "Jordan B. L. Smith",
            "Simon Dixon"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415710",
        "url": "https://doi.org/10.5281/zenodo.1415710",
        "ee": "https://zenodo.org/records/1415710/files/TerrellFSSD12.pdf",
        "abstract": "We examine the effect of listening level, i.e. the abso- lute sound pressure level at which sounds are reproduced, on music similarity, and in particular, on playlist gener- ation. Current methods commonly use similarity metrics based on Mel-frequency cepstral coefficients (MFCCs), which are derived from the objective frequency spectrum of a sound. We follow this approach, but use the level-dependent auditory spectrum, evaluated using the loudness models of Glasberg and Moore, at three listening levels, to produce auditory spectrum cepstral coefficients (ASCCs). The AS- CCs are used to generate sets of playlists at each listen- ing level, using a typical method, and these playlists were found to differ greatly. From this we conclude that music recommendation systems could be made more perceptu- ally relevant if listening level information were included. We discuss the findings in relation to other fields within MIR where inclusion of listening level might also be of benefit.",
        "zenodo_id": 1415710,
        "dblp_key": "conf/ismir/TerrellFSSD12",
        "keywords": [
            "listening level",
            "sound pressure level",
            "music similarity",
            "playlist generation",
            "Mel-frequency cepstral coefficients (MFCCs)",
            "loudness models",
            "auditory spectrum cepstral coefficients (ASCCs)",
            "music recommendation systems",
            "perceptually relevant",
            "fields within MIR"
        ],
        "content": "LISTENING LEVEL CHANGES MUSIC SIMILARITY\nMichael J. Terrell Gy ¨orgy Fazekas Andrew J. R. Simpson Jordan Smith Simon Dixon\nCentre for Digital Music, Queen Mary University of London\nMile End Road, London, E1 4NS, UK\nmichael.terrell@eecs.qmul.ac.uk, gyorgy.fazekas@eecs.qmul.ac.uk\nandy.simpson@eecs.qmul.ac.uk, jordan.smith@eecs.qmul.ac.uk\nsimon.dixon@eecs.qmul.ac.uk\nABSTRACT\nWe examine the effect of listening level, i.e. the abso-\nlute sound pressure level at which sounds are reproduced,\non music similarity, and in particular, on playlist gener-\nation. Current methods commonly use similarity metrics\nbased on Mel-frequency cepstral coefﬁcients (MFCCs), which\nare derived from the objective frequency spectrum of a\nsound. We follow this approach, but use the level-dependent\nauditory spectrum, evaluated using the loudness models of\nGlasberg and Moore, at three listening levels, to produce\nauditory spectrum cepstral coefﬁcients (ASCCs). The AS-\nCCs are used to generate sets of playlists at each listen-\ning level, using a typical method, and these playlists were\nfound to differ greatly. From this we conclude that music\nrecommendation systems could be made more perceptu-\nally relevant if listening level information were included.\nWe discuss the ﬁndings in relation to other ﬁelds within\nMIR where inclusion of listening level might also be of\nbeneﬁt.\n1. INTRODUCTION\nThe auditory system can be thought of, in signal processing\nterms, as a level-dependent ﬁlter bank, where each compo-\nnent is known as an auditory ﬁlter [15]. Incoming sound\nis ﬁrst processed by the frequency and direction depen-\ndent ﬁlter of the pinna (outer ear), before passing through\nthe ear canal, which acts as a narrowband resonant ampli-\nﬁer. The acoustic pressure at the ear-drum is mechanically\ntransmitted, via the amplifying stage of the middle-ear os-\nsicles, to the ﬂuid of the cochlea (inner ear) via the oval\nwindow [19]. Due to continuous variation in mass and\nstiffness along the basilar membrane, the cochlea provides\na tonotopic representation (arranged in order of frequency)\nof sound energy spectrum that is broadly consistent with\nFourier analysis.\nWithin the cochlea, inner hair cells are tonotopically ar-\nranged along the basilar membrane. The inner hair cells are\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.innervated with neurons that provide the ﬁring-rate coded\nsignal that is sent to the brain via the auditory nerve. The\ninner hair cells are accompanied by respective outer hair\ncells. Pressure gradients in the cochlear ﬂuid cause the\ninner hair cells at any given location to be deﬂected in a\nshearing motion which results from place-frequency de-\npendent resonance of the basilar membrane. At the same\ntime, the motile outer hair cells act in phase-locked syn-\nchrony to amplify the excitation. This system is known as\nthe cochlear ampliﬁer.\nEach inner hair cell is innervated with a population of\nneurons that code the local signal in terms of the rate-level\nfunction (the function that relates the rate of neuron ﬁr-\ning to the perceived intensity level). The stochastic ﬁring\nrate-level function of a neuron, or a population of neurons,\ncan be thought of as having three distinct stages: sponta-\nneous ﬁring, threshold, and saturation. Below threshold,\nthe neuron ﬁres randomly at a low rate. Between threshold\nand saturation, the function is close to linear and provides\na good coding of level. Above saturation point, increase\nin level does not result in a proportional increase in ﬁring\nrate. Thus, with increase in sound pressure level, an in-\ncreasing area of inner hair cells on the basilar membrane\nare excited beyond neural threshold. Within the context of\nthe excitation pattern model described above, this is known\nas spread of excitation.\nThe action of the cochlear ampliﬁer gives rise to strongly\nlevel-dependent tuning of the auditory ﬁlter. At low levels,\nthe phase-locked action of outer hair cells provides tono-\ntopically localised ampliﬁcation, which results in a narrow\nauditory ﬁlter. At high sound pressure levels, the cochlear\nampliﬁer is not able to contribute ampliﬁcation, due to me-\nchanical limits, and so the auditory ﬁlter becomes broader\nwith increase in level.\nThe parameters of the human auditory ﬁlter have been\ndetermined using psychophysical methods [17] and are rep-\nresented in terms of equivalent rectangular bandwidth (ERB).\nWithin the music information retrieval (MIR) community,\nthe auditory ﬁlters are typically more broadly represented\nin terms of the approximately analogous Mel frequency\nscale [21]. The Mel scale is deﬁned in terms of equal pitch\ndistance. Both scales produce a “non-linear mapping” of\nthe frequency domain.\nMel-frequency cepstrum coefﬁcients (MFCC), derived\nusing the discrete cosine transform, have been used forspeech recognition [9], music modelling [13] and music\nsimilarity [12]. The ERB scale has been used to improve\nspeech feature extraction [20]. Other related work [9] used\na gammatone auditory ﬁlter-bank [8] (derived from non-\nhuman physiology) in the place of ERBs. The resulting\ncoefﬁcients were referred to as EFCCs.\nThus far, although the MFCCs and EFCCs applied to\nMIR problems have made some attempt to address the ques-\ntion of perception in terms of frequency warping, no at-\ntempt has been made to demonstrate the major level-dependent\neffects of cochlear processing: (i) absolute threshold, (ii)\nspread of excitation, (iii) compression, and (iv) masking.\nIn other words, the major parameter of listening level has\nnot been investigated.\nAt present, MIR is usually based on recordings, which\nlisteners can reproduce at any listening level. Whilst we\nacknowledge the immediate practical difﬁculty that this\nimposes, we believe it is important to determine whether\nthe effects of listening level may be signiﬁcant. In this arti-\ncle, we use a psychoacoustic model to produce level depen-\ndent spectrograms, which incorporate the effects of (i) ab-\nsolute threshold, (ii) spread of excitation and (iii) compres-\nsion, and which can be used to evaluate level dependent\nsimilarity metrics. The similarity ratings are compared for\neach listening level to determine whether speciﬁc applica-\ntions of MIR, such as music playlist generation, may be\nlistening level dependent. This article also serves to begin\na more general discussion as to the relevance and impor-\ntance of listening level for other areas within MIR.\n2. MODELLING\nThe loudness models [7, 16] provide a means to predict\ntime and level dependent excitation patterns for time-varying\nacoustic stimuli. The outer and middle ear stages are mod-\nelled as a single FIR ﬁlter. Next, a bank of parallel ﬁl-\nters is used to calculate spectral magnitude over speciﬁc\nfrequency bands. The resulting excitation pattern is then\ntransformed into instantaneous speciﬁc loudness (ISL) ac-\ncording to a compressive nonlinearity designed to model\nthe action of the cochlea. The instantaneous speciﬁc loud-\nness is essentially a level dependent spectrogram with the\nfrequency axis in the ERB scale. We refer to it as an audi-\ntory spectrogram.\nWe collected a random subset of 500 recordings from\nthe Magnatagatune data set [10]. Magnatagatune is a col-\nlection of over 56,000, 30-second music clips from the\nMagnatune catalogue, with matching tags collected from\nLaw’s TagATune game. Our subset of 500 clips has ap-\nproximately the same proportion of genres as the full data\nset, including roughly 22% Classical, 17% each of Pop/Rock,\nElectronic and “Ethnic” or World music, and the rest from\nassorted genres. The clips, all 44.1kHz, 32kbps mono mp3\nﬁles, were obtained using the “Source Only” version of the\nMagnatagatune data set.\nUsing the auditory model, auditory spectrograms were\nestimated at three listening levels for each recording. A\n20 ms normalised Hanning window was used with a 50%\noverlap. The frequency axis was split into ERB bands,which gave 53 discrete frequency bins. The listening lev-\nels were characterised by peak sound pressure levels of 40,\n80 and 120 dB SPL. The input to the loudness model is a\nwaveform in Pascals (Pa), where a pressure of 1 Pa cor-\nresponds to 94 dB SPL. Therefore, in order to convert a\nnormalised digital recording (peak amplitude is 1), sd, into\na pressure signal spwith a peak level of X SPL, we use,\nsp= 10(X\u000094)\n20sd: (1)\nFigure 1 shows the auditory spectrograms for a ran-\ndomly selected recording played at each listening level. At\n40 dB SPL it becomes relatively narrow-band due to the\nhigh and low frequency energy falling below the absolute\nthresholds of audibility. At 80 dB the majority of the en-\nergy is above absolute threshold and the auditory spectro-\ngram is similar to the objective spectrogram. At 120 dB\nSPL the spread of excitation causes smearing of the energy\nacross the frequency range, and the recording becomes rel-\natively broadband.\nFigure 1. The auditory spectrograms of a randomly se-\nlected recodring with peak play-back intensity levels from\nleft to right of: 40, 80 and 120 dB SPL respectively.\n3. ANALYSIS\nAn acoustic model of musical timbre is often a core com-\nponent of content-based MIR systems. It is fundamental in\ntasks such as content-based music recommendation [13],\nplaylist generation [18], genre classiﬁcation [23] and in-\nstrument recognition [5]. In our experiments, we choose to\nfollow a deliberately simple, yet widely adopted method of\nmodelling the overall timbre of a recording ﬁrst by extract-\ning frame-wise cepstral coefﬁcients, and then modelling\nthe overall timbre distribution by ﬁtting a single Gaussian\nto the resulting coefﬁcient vectors [13]. In order to be able\nto take the effect of listening level into account, we use\na set of auditory spectra cepstral coefﬁcients (termed AS-\nCCs), computed from auditory spectra, calculated using\nthe method outlined in Section 2.\nSimilarly to MFCCs, the computation of this feature is\nderived from the computation of the real Cepstrum shown\nin Equation 2, where X(!)represents the Fourier trans-\nform of the analysed signal. The cepstrum separates the\nslowly varying components of a signal from superimposed\nhigher frequency and noise like components. It can be\nviewed as a rearranged spectrum, such that relatively few\ncoefﬁcients are sufﬁcient to characterise the spectral enve-\nlope; however, the higher the number of coefﬁcients, themore spectral detail is retained.\nc(n) =1\n2\u0019Z\u0019\n\u0000\u0019logjX(!)jej!nd! (2)\nIn many applications, including speech recognition and\naudio similarity analysis, it has become common to char-\nacterise short audio segments using a set of cepstral coefﬁ-\ncients, such that non-linear frequency warping is used to\nemphasise perceptually relevant frequencies correspond-\ning to auditory bands. Mel-scaling is the most widely adopted\nmethod for this purpose.\nOur feature extraction follows a common procedure of\ncomputing MFCCs [4]; however instead of using Mel-scaled\nmagnitude spectra, we use auditory spectra estimated at\nthree different listening levels. The auditory spectrograms\nare logarithmically compressed and then decorrelated us-\ning the Discrete Cosine Transform (DCT) given in Equa-\ntion 3.\nC(n) =MX\ni=1X(i) cosn(i\u00000:5)\u0019\nM;withn= 1;2:::;J;\n(3)\nwhereMis the number of auditory ﬁlters, Jis the num-\nber of ASCCs (typically J < M ), andX(i)is the log-\nmagnitude output of the i-th ﬁlter. These coefﬁcients are\nthen modelled using a single Gaussian characterising the\ndistribution of ASCCs over a song in our collection.\nThis method makes several simplifying assumptions. For\none, it ignores musical structure, and also the fact that the\ndistribution of timbre features is not necessarily Gaussian.\nA solution to these problems may be the use of Gaussian\nmixture models (GMM) or a sequence of Gaussians ﬁtted\non coherent segments, for instance, a single Gaussian rep-\nresenting each bar or each structural segment of the music,\nfor modelling a track. However, approaches to estimate\nsimilarity between these models such as Monte Carlo sam-\npling are computationally expensive. Detailed discussions\non timbre models and the effects of the above assumptions\ncan be found, for instance, in [1], [2] and [3]. Besides mod-\nelling recordings using a single Gaussian, a further simpli-\nfying assumption is introduced by using Gaussians with\ndiagonal covariance. Although modelling timbre using a\nsingle Gaussian is a very simple approach, it was shown\nin [14] that it can perform comparably to mixture models\nwhen computing similarity between recorded audio tracks.\nIt was also shown to be effective and computationally ef-\nﬁcient for ﬁnding similar songs in personal music collec-\ntions in [11]. An important advantage of using this model\nis that the similarity between two tracks can be estimated\nusing closed form expressions, such as the Jensen-Shannon\n(JS) or Kullback-Leibler (KL) divergences. Here, we use\nthe symmetrised KL divergence given in Equation 4, where\npandqare Gaussian distributions, with \u0016mean and \u0006co-\nvariance, and dis the dimensionality of the feature vectors.KLs(pkq) = 2KL(pkq ) + 2KL(qkp)\n=tr(\u0006\u00001\nq\u0006p+ \u0006\u00001\np\u0006q)\n+(\u0016p\u0000\u0016q)T(\u0006\u00001\nq+ \u0006\u00001\np)(\u0016p\u0000\u0016q)\n\u00002d (4)\nUsing this simple model, we calculate symmetric dis-\ntance matrices holding pair-wise KL-divergences (similar-\nity estimates) between all recordings in our collection. For\neach distance matrix computation, different sets of ASCCs\nare used that are calculated from the auditory spectra esti-\nmated for different listening levels. The distance matrices\nare then individually analysed using the methods described\nin Section 4.1 and 4.2, and the results produced at three\ndifferent levels are compared.\n4. RESULTS\nThe data set is analysed as per Section 3 to produce a KL\ndivergence rating per pair of recordings at each listening\nlevel. To illustrate the approach, 25 tracks from the set\n(n=500) were selected at random and KL divergence matri-\nces computed at each listening level (40, 80, 120 dB SPL).\nFigure 2 shows the matrices. Blue indicates low values\n(similar) and red indicates high values (dissimilar). Fig-\nure 3 shows a box-plot of the matrix data. Figs. 2 and 3\nclearly illustrate that the similarity ratings are strongly de-\npendent on the listening level. At low level, the set shows\na high mean similarity with relatively small variance. At\nhigh level the mean similarity is lower and the variance is\nlarger. At the medium level the variance lies between the\nlow and high listening levels.\nFigure 2. The normalised KL divergence matrices for a\nsubset of recordings with peak intensity levels from left\nto right of: 40, 80 and 120 dB SPL respectively. Blue\nindicates low values (similar) and red indicates high values\n(dissimilar).\nWhilst Fig. 2 shows that the similarity ratings are de-\npendent upon the listening level, it is important to deter-\nmine whether these differences are signiﬁcant in MIR ap-\nplications. The application we choose to study is music\nrecommendation. Music recommendation tools generate\nplaylists based on similarity ratings, typically derived from\nMFCCs. We compared the similarity data across the three\nintensity levels in two ways: (i) by comparing the ordering\nof distances within triples, and (ii) by comparing the mem-\nbers of playlists with different seed recordings, and with\ndifferent playlist sizes.40 dBSPL 80 dBSPL 120 dBSPL00.20.40.60.8KL DivergenceFigure 3. Boxplots of the KL divergence matrices (Fig. 2)\nat each listening level. Low values correspond to similar\nrecordings and high values to dissimilar recordings.\n4.1 Triple analysis\nWe analysed all subsets of 3 recordings from the dataset\nand the pair of recordings with minimum distance (in terms\nof the KL divergence feature space), was identiﬁed. The\ndata was compared across listening level, and changes in\nthe closest identiﬁed pairs were recorded. For example, if\na given triplet (I,J,K) showed that at 40 dB SPL recordings\nI and J were closest together, but that at 80 dB SPL I and K\nwere closest together, this was recorded as a change. The\npercentage changes were calculated across all triples and\nare shown in Table 1. We see around a 30% change in the\nordering of triples. This suggests that MIR applications\nthat use similarity metrics, such as playlist generation, will\nbe affected by listening level.\n% Change in Triplet Order\n40 vs 80 40 vs 120 80 vs 120\n32 29 27\nTable 1. The percentage change in the closest identiﬁed\npair within each set of triples. The column headers refer to\nthe listening levels between which the comparisons were\nmade, i.e. 40 vs 80 relates to comparison of triplet data\nfrom the 40 dB SPL and 80 dB SPL sets.\n4.2 Playlist generation\nPlaylists were generated by assigning a seed song, and then\nidentifying the (n\u00001)closest songs in the similarity space,\nwherenis the size of the playlist. The playlists were\ncompared across listening levels. For example, if a ﬁve\nsong playlist is generated for seed song A, where identi-\nﬁed songs are (T,U,S,X) at 40 dB SPL, but at 80 dB SPL\nare (W,T,U,S), the percentage change would be 25%. We\ndo not consider a playlist to have changed if the order of\nthe chosen songs is different.\nThe mean and 95% conﬁdence intervals are calculated\nfor playlist changes across all seed songs. The mean data\nare shown in Table 2 using the ﬁrst 20 ASCCs. Playlist\nchange data using ﬁrst 12, 20 and 29 ASCCs are plotted inFigure 4. The changes range from 80% for small playlists,\nto 50% for large playlists.\nIn order to verify the signiﬁcance of these changes, an\nequivalent process is followed but comparisons are made\nbetween playlists generated using different numbers of AS-\nCCs at each listening level. These data are shown in Figure\n5. The changes range from 50% for small playlists, to 10%\nfor large playlists. For a 10 song playlists, the average\nchange (in the songs added) is: 62% caused by listening\nlevel (Fig. 4), and 22% caused by the number of ASCCs\nused (Fig. 5).\nN. Songs Mean % Change in Playlist Members\n40 vs 80 40 vs 120 80 vs 120\n1 74 67 80\n2 69 64 78\n3 68 62 76\n4 66 59 75\n5 66 58 75\n6 65 57 74\n7 64 56 73\n8 63 55 73\n9 62 54 72\n10 61 53 71\n11 61 52 70\n12 60 52 70\n13 60 51 69\n14 59 51 68\n15 58 50 68\n16 58 49 67\n17 58 49 67\n18 57 48 66\n19 57 47 66\n20 57 47 65\n21 57 46 65\n22 56 46 65\n23 56 46 64\n24 55 45 64\nTable 2. The percentage change in the recommended\nplaylists using the ﬁrst 20 ASCCs. The column headers re-\nfer to: the length of playlist (excluding seed song), (n\u00001),\nand the listening levels between which the comparisons\nwere made, i.e. 40 vs 80 relates to comparison of playlists\nfrom the 40 dB SPL and 80 dB SPL sets.\n5. DISCUSSION\nWe have demonstrated that the effect of listening level is\nlarger than that of variation of the number of ASCCs used\nin the playlist generation. The large percentage change in\nplaylist members shown for the comparison between 40-\n80 dB SPL is perhaps most relevant to the typical MIR end\nuser - such variation in listening levels may be typical in\nthe home (e.g., for radio broadcast). The equally large per-\ncentage change shown in the results for the highest sound\npressure level (120 dB SPL) may be relevant for the live20406080(a) (b) (c)\n20406080(d)% Change in Members(e) (f)\n0 10 2020406080(g)\n0 10 20(h)\nSongs Added (n ï1)0 10 20(i)Figure 4. The percentage change in playlist members with\nlistening level as a function of the length of playlist (ex-\ncluding seed song). The data shown are the mean and 95%\nconﬁdence intervals across all seed songs. The square, cir-\ncle and triangle markers show comparisons between: 40 to\n80, 40 to 120 and 80 to 120 dB SPL respectively. Figs. (a)\nto (c) show comparisons using the ﬁrst 12 ASCCs, (d) to\n(f) use the ﬁrst 20, and (g)-(i) use the ﬁrst 29.\nsound (or disc jockey) context, where sound levels tend to\nbe higher.\nAnother conclusion that may be drawn from the anal-\nysis is that low listening levels may be considered to pro-\nduce a homogenization effect by limiting bandwidth (due\nto absolute thresholds). A similar effect is seen at high lev-\nels, where saturation and upward spread of excitation limit\nthe effective number of independent ASCCs. It is conceiv-\nable that, given a larger set from which playlist members\nare drawn, the trends shown in Figs. 4 and 5 would re-\nsolve to a more signal or method dependent function, for\nexample, it may be shown that the effect of listening level\nis more signiﬁcant on certain genre. Future work should\ninclude modelling with larger sets of data.\nAlthough demonstrated here using a music similarity\nstudy, the effect of listening level on auditory spectra may\nhave wide ranging implications for MIR theory and prac-\ntice in general, and initiating this debate was a primary aim\nof this article. It seems unlikely that changes in listening\nlevel will manifest changes in MIR properties relating to\nmusical score (e.g., notation) or structure (e.g., segmen-\ntation). However, where MIR methods rely on spectrum\n(e.g., timbre) some effects of listening level may be ex-\npected. For example, speech (or even speaker) recogni-\ntion in a high noise environment might be enhanced by the\nproper masking (noise suppression) effects of loud speech\nin the auditory model. In a more general sense, loudness it-\nself may be a useful perceptual feature for MIR problems.\nFor example, in the creation of a playlist, using a simi-\nlar procedure to that described in the present paper, loud-\n20406080(a) (b) (c)\n20406080(d)% Change in Members(e) (f)\n0 10 2020406080(g)\n0 10 20(h)\nSongs Added (n ï1)0 10 20(i)Figure 5. The percentage change in playlist members with\nthe number of ASCCs used as a function of the length of\nplaylist (excluding seed song). The data shown are the\nmean and 95% conﬁdence intervals across all seed songs.\nThe square, circle and triangle markers show comparisons\nbetween: 12 to 20, 12 to 29, and 20 to 29 ASCCs respec-\ntively. Figs. (a) to (c) show comparisons at 40 dB SPL, (d)\nto (f) at 80 dB SPL and (g)-(i) at 120 dB SPL.\nness and loudness dynamic range may be used to produce\na sequence of songs which is tailored for smooth loudness\ntransitions between tracks, and for similar loudness dy-\nnamics. Furthermore, incorporation of complete psychoa-\ncoustic listening conditions within listening tests designed\nto validate such perceptual similarity metrics may lead to\nmore meaningful ground truth data.\n6. CONCLUSIONS\nIn this paper we have presented a computational analysis of\nthe effect of listening level on a perceptual music spectrum\nsimilarity metric. The similarity matrices and statistical\ndata have shown that the metric is strongly level depen-\ndent. The playlist data shows similarly striking effects of\nlistening level. Some general discussion has been given\non the immediate implications of the use of listening-level\ndependent auditory models in MIR and loudness itself has\nbeen suggested as possible future similarity feature. The\nresults of this study suggest that more complete data about\nsound [22] and about music production [6] may be useful\nto future context speciﬁc MIR applications.\n7. REFERENCES\n[1] J. J. Aucouturier. Ten experiments on the modelling\nof polyphonic timbre. PhD thesis, University of Paris,\n2006.\n[2] M. Casey, and M. Slaney. The importance of sequencesin musical similarity.” Proc. IEEE Int. Conf. ASSP,\n2006.\n[3] M. A. Casey, R. Veltcamp, M. Goto, M. Leman, C.\nRhodes, and M. Slaney. “Content-based music infor-\nmation retrieval: Current directions and future chal-\nlenges.” Proceedings of the IEEE, V ol. 96, No. 4, pp.\n668–696, 2008.\n[4] S. B. Davis, and P. Mermelstein. “Comparison of para-\nmetric representations for monosyllabic word recog-\nnition in continuously spoken sentences.” Proc. IEEE\nTrans. on Acoustic, Speech and Signal Processing, V ol.\n28, No. 4, pp. 357–366, 1980.\n[5] A. Eronen, and A. Klapuri. “Musical instrument recog-\nnition using cepstral coefﬁcients and temporal fea-\ntures.” IEEE International Conference on Acoustics,\nSpeech and Signal Processing, pp. 753–756, 2000.\n[6] G. Fazekas, and M. Sandler. “The Studio Ontology\nFramework.” Proc. 12th Int. Soc. for Music Informa-\ntion Retrieval, USA., 2011.\n[7] B. R Glasberg, and B. C. J. Moore. “A model of loud-\nness applicable to time-varying sounds.” J. Audio Eng.\nSoc, V ol. 50, pp. 331–342, 2002.\n[8] P. I. M Johannesma. “The pre-response stimulus en-\nsemble of neurons in the cochlear nucleus.” Symp.\nHear. Theory, pp. 58–69, 1972.\n[9] K. Metha. “Robust front-end and back-end process-\ning for feature extraction for hinhi speech recognition.”\nProc. IEEE Int. Conf. ICCIC, 2010.\n[10] E. Law, and L. von Ahn. “Input-agreement: a new\nmechanism for collecting data using human computa-\ntion games.” Proc. 27th Int. Conf. on Human Factors\nin computing systems, pp. 1197–1206, 2009.\n[11] M. Levy, and M. Sandler. “Lightweight measures for\ntimbral similarity of musical audio.” Proc. 1st ACM\nWorkshop on Audio and Music Computing Multimedia.\n2006.\n[12] B. Logan, and A. Salomon. “A music similarity func-\ntion based on signal analysis.” Proc. IEEE Int. Conf.\nMultimedia and Expo, pp. 745–748, 2001.\n[13] B. Logan. “Mel frequency cepstral coefﬁcients for mu-\nsic modeling.” Proc. Int. Symp. on Music Information\nRetrieval, 2000.\n[14] M. I. Mandel, G. E. Poliner, and D. P. W. Ellis.\n“Support vector machine active learning for music re-\ntrieval.” Proc. Int. Conf. on Music Information Re-\ntrieval, 2005.\n[15] B. C. J. Moore. An Introduction to the Physiology of\nHearing. Academic Press, 1997.[16] B. C. J. Moore, B. R. Glasberg, and T. Baer. “A model\nfor the prediction of thresholds, loudness, and partial\nloudness.” J. Audio Eng. Soc, V ol. 45, pp. 224–240,\n1997.\n[17] B. C. J. Moore ,and B. R. Glasberg. “Suggested formu-\nlae for calculating auditory-ﬁlter bandwidths and exci-\ntation patterns.” The Journal of the Acoustical Society\nof America, V ol. 74, No. 3. pp. 750–753, 1983.\n[18] E. Pampalk. Computational models of music similar-\nity and their application to music information retrieval.\nPhD thesis.\n[19] J O. Pickles. An Introduction to the Physiology of\nHearing. 2008.\n[20] M. D. Skowronski, and J. G. Harris. “Improving the\nﬁlter bank of a classic speech feature extraction algo-\nrithm.” Proc. Int. Symp. Circuits and Systems, V ol. 4,\npp. 281–284, 2003.\n[21] S. S. Stevens, J. V olkmann, and E. B. Newman. “A\nscale for the measurement of the psychological mag-\nnitude pitch.” The Journal of the Acoustical Society of\nAmerica, V ol. 8, No. 3, pp. 185–190, 1937.\n[22] M. J. Terrell, A. J. R. Simpson, and M. Sandler.\n“Sounds not signals: A perceptual audio format.” Eng.\nBrief, AES 132nd Int. Convention, 2012.\n[23] G. Tzanetakis and P. Cook. “Musical genre classiﬁca-\ntion of audio signals.” IEEE Trans. Speech Audio Pro-\ncessing, V ol. 10, pp. 293–301, 2002."
    },
    {
        "title": "N-gram Based Statistical Makam Detection on Makam Music in Turkey Using Symbolic Data.",
        "author": [
            "Erdem Ünal",
            "Baris Bozkurt",
            "Mustafa Kemal Karaosmanoglu"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1417459",
        "url": "https://doi.org/10.5281/zenodo.1417459",
        "ee": "https://zenodo.org/records/1417459/files/UnalBK12.pdf",
        "abstract": "This work studies the effect of different score representa- tions and the potential of n-grams in makam classification for traditional makam music in Turkey. While makams are defined with various characteristics including a dis- tinct set of pitches, pitch hierarchy, melodic direction, typical phrases and typical makam transitions, such cha- racteristics result in certain n-gram distributions which can be used for makam detection effectively. 13 popular makams, some of which are very similar to each other, are used in this study. Using the leave-one-out strategy, makam models are created statistically and tested against the left out music piece. Tests indicate that n-gram based statistical modeling and perplexity based similarity metric can be effectively used for makam detection. However the main dimension that cannot be captured is the overall progression which is the most unique feature for classifi- cation of close makams that uses the same scale notes as well as the same tonic.",
        "zenodo_id": 1417459,
        "dblp_key": "conf/ismir/UnalBK12",
        "keywords": [
            "makam classification",
            "n-grams",
            "traditional makam music",
            "pitch hierarchy",
            "melodic direction",
            "typical phrases",
            "typical makam transitions",
            "leave-one-out strategy",
            "makam models",
            "perplexity based similarity metric"
        ],
        "content": "N-GRAM BASED S TATISTICAL  MAKAM DETECTION ON \nMAKAM MUSIC IN TURK EY USING SYMBOLIC DATA \nErdem Ü nal Barış Bozkurt  M. Kemal  Karaosmanoğlu  \nTÜBİTAK -BİLGEM  \nunal@uekae.tubitak.gov.tr \n Bahçeşehir University  \nbaris.bozkurt@bahcesehir.edu.tr Yildiz Technical Univer sity \nkkara@yildiz.edu.tr \nABSTRACT  \nThis work studies the effect of different score represent a-\ntions  and the potential of n -grams in makam classification \nfor traditional makam music in Turkey. While makams \nare defined with various characteristics including a di s-\ntinct set of pitches, pitch hie rarchy, melodic direction, \ntypical phrases and typical makam transitions, such ch a-\nracteristics result in certain n -gram distributions which \ncan be used for makam detection effectively. 13 popular \nmakams, some of which are very similar to each other, \nare us ed in this study. Using the leave- one-out strategy, \nmakam models are created statistically and tested against \nthe left out music piece. Tests indicate that n-gram based \nstatistical modeling and perplexity based similarity metric can be effectively used for  makam detection. However \nthe main dimension that cannot be captured is the overall progression  which is the most unique feature for classif i-\ncation of close makams that uses the same scale notes as \nwell as the same tonic .  \n1. INTRODUCTION  \nThe makam/maqam/muga m concept is very central to \nmusic of a very large geographical region from Balkans \nto Kazakhstan, Iran, and North Africa. Automatic class i-\nfication of makam is hence very important for music i n-\nformation retrieval technologies though not widely stu-died.  \nComputational studies on makam music can be very \nbroadly classified into two categories based on the type of data being processed: symbolic or audio. While  some \nwork s such as [1 ], [6], [11] propose systems for makam \nrecognition from audio data, works on symbolic data ap-\npear to be much more limited, probably due to lack of \nmachine readable data. In [16],  Şentürk and Chordia use \nVariable -Length Markov Models (VLMM) to predict the \nmelodies in the uzunhava  (long tune) form, a melodic \nstructure in Turkish folk music. In [2] (which is a shorter \nversion of [ 8]), Alpkoçak  and Gedik present the first and \nonly study on n -grams for makam recognition. Unfort u-\nnately, due to sever al deficiencies, reliability of their r e-sults is questionable. The paper presents classification \nresults without cross -validation, uses limited and que s-\ntionable data (20 pieces for each of 10 makam where data is represented with 12 notes in an octave while  today’s \nnotation uses 24 notes in an octave). Due to such def i-\nciencies, a new work needs to be conducted exploring the potential of n -grams in automatic makam recognition \nfrom symbolic data.  Our main contributions in this study \nare: in addition to the 12-TET (Tone Equal Temper a-\nment) representation used in [2 ], [9], we also used data \nrepresented using the official theory of makam music in Turkey (TMMT) which uses 24 tones (unequally spaced) \nin an octave, and holding a larger database, and challen g-\ning makam sets, we were able to test the potential of n -\ngram based statistical approach in makam recognition \nmore reliably . We also tested makam detection perfo r-\nmance using comma level intervallic movements, sho w-\ning how this system can be used in real life applications using audio data  only.   \nIn the MIR literature, makam recognition can be co n-\nsidered, to some level, as a key finding or a mode finding \nproblem. However, there appears to be important differ-\nences between the concepts of key, mode and makam (a detaile d discussion can be found in [3]). In the makam \nsystem, different makams can be constructed using the same set of pitches, the same set of tetrachord - penta-\nchord formulation and the same tonic. Two examples are presented in Figure 1; the scale for makam Hüseyni  and \nmakam Muhayyer  (top figure) and makam Uşşak and \nmakam Beyati  (bottom figure). Then, pitch hierarchy, \nmelodic direction, typical phrases and typical makam \ntransitions appear to be the discriminating features for makams having the same set of pitches and tonic.  \n \nFigure 1 . Scale used for makam Hüseyni and makam \nMuhayyer (top), makam Beyati and makam Uşşak  \n \nThe listed characteristics have important influences on \nthe pitch -class distribution of a given piece in a given \nmakam, as in the case o f key or mode in Western music  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval  [13]. For \nthat reason, the processing of pitch (class) hi s-\ntograms appears as the most common approach for co m-\nputational studies of makam music [8 ]. N-grams can be \nconsidered to be an extension to this approach, where di s-\ntributions of fixed length note -sequences are used in add i-\ntion to single note pitch distributions.  \nThe n- grams approach [ 10], from the text retrieval \ndomain, have been widely used in computational studies \non Western music. Various applications exist including \nindexing [ 6], query processing and music similarity co m-\nputation [7 ], [17]. This study targets filling the gap for \nmakam music in Turkey in the context of makam reco g-\nnition. We first present the data, then details of imple-\nmentation, results and discussions.  \n2. DATA  \nThe current notation system (The Arel Theory (not a-\ntion)) [4] used for TMMT  a s s u m es  2 4  n o t e s  i n  an  o c-\ntave. Although highly- criticized, almost all scores being \nused today are written in that system. While a large number of scanned scores are available o n internet, m a-\nchine readable data is very limited. Recently, we have \nannounced the largest symbolic database of TMMT  con-\ntaining 1700 pieces in 155 makams [12] .  D u e  t o  t h e  \navailability  at the time of the experiments,  this study uses \nthe following subset fro m [12]: \n \nMakam name  Total # of Songs  Total # of Notes  \nBeyati  39 16,172 \nHicaz  112 45,905 \nHicazkar  48 17,950 \nHüseyni  70 28,292 \nHüzzam  63 26,842 \nKürdilihicazkar  49 20,993 \nMahur  51 22,037 \nMuhayyer  51 21,718 \nNihavent  79 31,143 \nRast  83 32,636 \nSaba  42 17,255 \nSegah  75 26,757 \nUşşak  85 31,704 \nTOTAL  847 339,404 \nTable 1 . Makam coverage and note statistics  \nThe makam selection is based on three criteria: co m-\nmonness, similarity and having sufficient number of \nsamples in the database. For a classification study, it is \nbeneficial to include similar classes and study the effects of such similarities in the classification performance. We \nhave included in our set, makam couples which are \nstated to be differing only  in melodic progression na mely \nUşşak - B eyati  and Muhayyer  - H üseyni  [14]. These \ncouples share the same set of pitches, the same tonic and \ndominant (which is considered to be the boundary of t e-\ntrachord -pentachord division of the octave) as shown in \nFigure 1. As previous classificat ion work on audio data \nshowed, Hüseyni  is also confused with Uşşak [8]. Ther e-\nfore, the set includes challenging examples of classes.  2.1 Arel representation compared to 12 -TET \nIn this work we test our system with two different repr e-\nsentations of the symbolic data. The first one is the off i-\ncial theory of TMMT [4] and the second is the well -\nknown 12 -TET representation. The tonal space of the two \nsystems are compared in Figure 2.  \n \nFigure 2.  Tonal spaces of the Arel T heory Notation and \n12-TET.  \n Being based on Pythagorean tuning, the 24 tones of \nArel Theory are indeed close to 12 -TET tones. While b e-\ning a better representation (than 12 -TET) for TMMT, \nArel system is known to be insufficient in representing \nthe practice. In this work we use the Arel Theory repre-\nsentation due to its wide use and the 12 -TET represent a-\ntion, to be able to compare our results with [2 ],[9].  \nArel theory uses two different but close formulations \nto represent notes and musical intervals, the first one b e-\ning frequency ratios (such as 3/2, 9/ 8, etc.) and second \none being the interval in integer multiples of Holdrian -\ncommas (obtained by equal 53 divisions of an octave). The second is indeed a quantized version of the first and \nis more practical in explaining accidentals of the notation \nsystem a s in Figure 3.  \n \nFigure 3.  Accidentals used in the Arel Theory notation \nsystem  \nA s  s h o w n  i n  F i g u r e  3 ,  a  w h o le-tone is composed of 9 \nHold rian commas (will be referred as comma hereafter) \nin the Arel Theory system. In a machine readable format, \nit is convenient  to name the notes using the comma steps \nsuch as B4b1, which corresponds to B4 with a flat of si n-\ngle comma size, where B4b4 w o u l d  h a v e  a  f l a t  o f  4  \ncomma size. Alternatively each note can be represented using its distance to a reference note, for example C1, in \ncommas . Such a representation makes it  possible to easily \nobtain  interval sizes (by simply subtracting  the values \nassigned to each note as a distance in commas to a co m-\nmon reference) between consecutive notes which can fu r-\nther be used in modeling the progression (as in Section 4.4).    3.  STATISTIC\nAL MODELING  \n3.1 N-gram models  \nN-grams are widely used in computational linguistics, \nprobability, communication theory and computational \nbiology as well as music information retrieval [6], [7], \n[17]. N-grams predict X i based on X i-(n-1), ..., Xi-1. In \ntheory this is the information calculated by P(X i|Xi-(n-1) , \n..., Xi-1). Given sequences of a certain set, one can stati s-\ntically model this set by statistically counting the s e-\nquences that belong to it. In this study, according to the given note sequences that belong to the same makam, n -\ngrams will be used to statistically model the pitch and \nintervallic space, as well as short melodic motifs to d e-\nfine makams.  \nThe main hypothesis to be tested here is that, the short -\ntime melodic contour and the frequency of makam speci f-\nic notes are selective features for defining makams. This i s  w h y  n -gram models are selected for training makam \nmodels using the Arel Theory notation . Given a micr o-\ntonal notation  sequence, usin g perplexity, the system will \ndefine how well the input sequence can be generated by \nthe makam models in the database. The makam model \nthat has the maximum similarity score is selected as the \noutput of the system.   \n3.2 Smoothing  \nIn practice, it is necessary to  smooth the probability dis-\ntributions by assigning non -zero probabilities to unseen \nwords or n -grams. The reason is that models derived d i-\nrectly from the n -gram frequency counts have severe \nproblems when confronted with any n- grams that have \nnot been seen before which is called the \"zero frequency \nproblem\". Different smoothing techniques are introduced in order to solve this problem  [10].  Written -Bell smooth-\ning technique available in the SRILM toolkit is used in \nour experiments  [15].  \n3.3 Perplexity  \nPerplexity  is a metric that is widely used for compar-\ning probability distributions. The perplexity of a random \nvariable X can be stated as the perplexity of the distrib u-\ntion over its possible values of x. Given a proposed prob-\nability model q (in our case: a makam mo del), evaluating \nq by asking how well it predicts a separate test sequence \nor set x1, x2, ...,xN  (in our case: a microtonal note s e-\nquence) also drawn from p, can be performed by using \nthe perplexity of the model q, defined by :   \n2∑1\n𝑁𝑁𝑙𝑙𝑙𝑙𝑙𝑙 2𝑞𝑞(𝑥𝑥𝑖𝑖)𝑁𝑁\n𝑖𝑖=1   (1) \nFor the test events, we can see that better models will \nassign better probability scores thus a lower perplexity \nscore which means it has a better potential to compress \nthat data set. The exponent is the cross entropy per def i-\nnition:  𝐻𝐻(𝑝𝑝,𝑞𝑞)=−∑𝑝𝑝(𝑥𝑥)𝑙𝑙𝑙𝑙𝑙𝑙2𝑞𝑞(𝑥𝑥) 𝑥𝑥  (2) \nThe cros\ns entropy thus the perple xity is the similarity \nmeasure between  the test input  and the makam models in \nthe database. For each of the makam model s defined, the \nsystem calculates the similarity metric to evaluate which \nmakam is the mos t similar to the input sequence given.  \n4.  EXPERIMENTAL SETUP  \n4.1 Leave -one-out \nThe experimental  setup can be found in Figure 4, explai n-\ning how the leave -one-out strategy is inherited.  \n \nFigure 4. The leave- one-out experimental setup  \nThere are 13 makam classes. Each of them has u n-\nequal number of music pieces. Since our approach is st a-\ntistical, for each class, it is desirable to have a training set \nand a separate test set. This approach is feasible in case \nthere is enough data.  Since machine -readable microtonal \nnotation  is very hard to find in makam music,  the \" leave-\none-out\" strategy will be used in this experimental setup \nin order to avoid the negative effect of unequal set sizes. \nFor each iteration of the experiment, one music piece will be selected as the in put and the leftover music pieces will \nbe used for training the genuine and the imposter makam \nclasses. Using a probabilistic evaluation metric (perplexi-ty), the system will calculate a similarity measure b e-\ntween the input and already built makam models.  \n4.2 Evaluation  \nGiven a note sequence, the perplexity will estimate how well this sequence can be statistically generated by the makam models in our search space. Between each of the \nmakam models, and the input sequence, the system ca l-\nculates a similarity measu re, and the makam that produc-\nes the maximum similarity measure becomes the output \nof the system. The performance criteria of the exper i-\nmental procedure is binary, which is either a success or failure. The matching performance of the entire system, which is  the accuracy  (Recall) , will be given as a propor-\ntion of successes over the total test trials  in terms of T o-\ntal Average (Tot -Ave) and the Weighted Average (W -\nAve) .  \n \n \n \n \n \n  \n \n \n \n \n \n4.3 Tests wi\nth the Arel Theory  Notation  \nIn information retrieval, the output of such kind of sys-\ntems are evaluated given 2 different measures. Given a \nmusic piece, the Recall (Rcl) suggests, how many of the \nqueries for each of  the makams are correctly found . On \nthe other hand, precision is how many of the retrieved \nmakams belong to the correct reference makam class. \nPrecision becomes more meaningful when there is equal number of test trials from each ma kam classes. As seen \nfrom  Table  3, 4 makams has perfect recall rate which are \nHicaz, Hicazkar , Kü rdilihicazkar  and Mahur . The m a-\nkam which shows the wor st performance is Beyati  as the \nrecall rate is 61.5% and it is confused with Uşşak , the \nmost  (for n=3 ).  \nThe confusion matrix also suggests that there are co n-\ncrete similarities between these makam sets : Beyati -  \nUşşak  and Hüseyni - M uhayyer . In theory these makam \ncouples use exactly the same microtonal note sets  as well \nas the tonics  and the effect of this similarity can be pra c-\ntically seen in our experiments.  \nTable 3 shows the change in Recall metric when the \norder of the n- grams increased from 1 to 3. Also, the last \ncolumn  shows which n -gram shows the best performance \nwith respect to Recall. As seen from results , for the ma-\nkam, Hicazkar, Hüseyni , Rast and Segah, increasing the \norder of n -grams from 1 to 2 or 3, improves the makam \ndetection pe rformance  of the classifier.  For Hicaz, \nHüzzam, Mahur,  Nihavent and Saba increasing the order \nof the n -grams did not have any positive influence.  On \nthe other hand, increasing N has negative effect on the performance of the classifier for the makams Beyati , Mu-\nhayy er and Uşşak.  \nThere might be a number of reasons for performance \nfluctuation within different makams. The one that we b e-\nlieve the most important is the unequal number of notes for training each makam classes. Even though a smoot h-\ning technique is used, the  frequency of widely seen s e-\nquences become more dominant for makams that have \nfew training samples (such as Beyati, Hüzzam, Segah and \nUşşak), which makes these  makams harder to be disti n-\nguished from the ones that are similar .   \n \n \n  \n \n \n \n \n \n  \n \n \n \n \n \n \nTable 3 . Change in Recall w.r. t. n-gram order for data \nusing the Arel Theory notation   \n4.4 Tests with microtonal intervals  \nConsidering that the real world application of this system will operate with audio inputs, and it is known that a di-rect transcription of audio to the microtonal sequence \nused above is not easy, tests on data represented as s e-\nquence of  microtonal intervals are also applied. The i n-\nterval between consecutive notes are computed in co m-\nmas as explained in Section 2.1 This also ensures that the \nsystem functionality is independent of the starting note of the music piece, the type or the tuning  of the instrument \nthat plays the piece.   n=1 n=2 n=3 Best-N \nBeyati  64.1 56.4 61.5 1 \nHicaz 100 99.1 100 1,3 \nHicazkar  97.9 100 100 2,3 \nHüseyni  50 64.3 71.4 3 \nHüzzam  98.4 98.4 98.4 1,2,3  \nKürdilihicakar  100 100 100 1,2,3  \nMahur  100 100 100 1,2,3  \nMuhayyer  80.4 68.6 68.6 1 \nNihavent  98.7 98.7 98.7 1,2,3  \nRast  74.7 92.8 91.6 2 \nSaba  97.6 97.6 97.6 1,2,3  \nsegah  93.2 97.3 95.9 2 \nUşşak  68.2 62.4 57.6 1 \nTot-Avg 86.3 87.9 88.2 3 \nW-Avg 86.4 87.4 87.8 3 byati  hicaz  hczkr  hsyni  huzzm  krdhz  mahur  muhyr  nhvnt  rast saba segah  Ussak  Ref Rcl. \n24 0 0 3 0 0 0 1 0 0 0 0 11 byati  61.5 \n0 112 0 0 0 0 0 0 0 0 0 0 0 hicaz  100 \n0 0 48 0 0 0 0 0 0 0 0 0 0 hczkr  100 \n1 0 0 50 0 0 0 13 0 0 0 0 0 hsyni  71.4 \n0 0 0 0 62 0 0 0 0 0 0 1 0 huzzm  98.4 \n0 0 0 0 0 49 0 0 0 0 0 0 0 krdhz  100 \n0 0 0 0 0 0 51 0 0 0 0 0 0 mahur  100 \n2 0 0 11 0 0 0 35 0 3 0 0 0 muhyr  68.6 \n0 0 0 0 0 0 0 0 78 1 0 0 0 nhvnt  98.7 \n0 0 0 1 0 0 0 0 0 76 0 0 4 rast 91.6 \n0 0 0 1 0 0 0 0 0 0 41 0 0 saba  97.6 \n0 0 0 1 2 0 0 0 0 0 0 70 0 segah  95.9 \n16 0 0 11 0 0 0 4 0 3 0 2 49 ussak  57.6 \n55.8 100 100 64.1 96.9 100 100 63.6 100 91.6 100 95.9 70 Prc.  \n     \n   Table 2 . Confusion Matrix for  Arel Theory Notation  (n=3)  \nTable 4 . Change in Recall w .r.t. n-gram order with data \nrepresented as microtonal intervals  \nThe basic goal in this experimental setup  is to achieve \nat least a close performance to the test explained in 4.3 , \nand thus the cost of losing absolute note level inform a-\ntion can be tested  over the system performance. The m a-\nkam detection performance for the microtonal represen-\ntations can be seen in Tabl e 4. \n4.5 The Basel ine: 12 -TET Input Tests  \nFinally we evaluated the performance of our system on \ndata represented using 12 -TET since it is the representa-\ntion used in the only a vailable system in literature that \ndoes makam detection using n -grams [2], [9]. In addition \nto important differences in the implementation, modeling \nand evaluation, this study uses and compares different data representations, where in [2 ], [9] only the 12 -TET \nrepresentation is used. Since the basic strategy  is building \nn-grams for both systems, we ran our experimental setup \non the same database using the leave- one-out technique .  \nTable 5 shows the results with respect to increasing n -\ngrams per each makam in  the database. Since neither the \nevaluation nor the modeling technique is clearly ex-\nplained in [2 ], [9], the standard modeling and smoothing \ntechniques in our system was used when implementing the baseline (i.e. the system using the 12 -TET represent a-\ntion).   A s  s e e n  f r o m  t h e  r e s u l t s , the best performing n - \ng r a m  o r d e r  i s  3 ,  s i m i l ar to results gathered from Arel  \ntheory tests. However, the system using the Arel Theory notation outperforms the baseline for both the Weighted Average and the Total Average.  \n \nTable 5 . Change in Recall w .r.t. n-gram order for 12 \nTET \nThe overall comparison of the performance of all the \ntests can be seen in Table 6. For n=3 where the best per-\nformance for all the systems were achieved, we observe that by using the Arel Theory notation as opposed to 12 -\nTET, an improvement of 3.7% is achie ved. \n \nRecall  n=1 n=2 n=3 \nArel Theory  86.3 87.9 88.2 \n12-TET 81.7 82.8 84.5 \nDelta (in commas)  58.9 77.3 80.6 \n \nTable 6 . Overall Performance Comparison.  \n5. DISCUSSION AND CONCL USION  \nIn this work, we implemented a perplexity based makam \ndetection system on symbolic data of TMMT. N -gram \nbased statistical makam models were built using the \nSRILM toolkit. Necessary smoothing was performed in \norder to compensate the negative effect of unequal set sizes.  \nExperimental set up was designed using the leave -one-\nout approach . For each of the test trials, one song from 5060708090100\nn=1 n=2 n=3Arel Theory 12-TET Delta n=1 n=2 n=3 Best-N \nBeyati  56.4 64.1 59 2 \nHicaz  61.6 81.2 93.8 3 \nHicazkar  66.7 85.4 85.4 2,3 \nHüseyni  30 50 60 3 \nHüzzam  69.8 87.3 85.7 2 \nKürdilihicazkar  38.8 71.4 77.6 3 \nMahur  80.4 90.2 94.1 3 \nMuhayyer  51 47.2 43.1 1 \nNihavent  70.9 92.4 97.5 3 \nRast  69.9 89.2 88 2 \nSaba  97.6 97.6 97.6 1,2,3  \nSegah  69.9 94.5 94.5 2,3 \nUşşak  21.2 51.8 56.5 3 \nTot-Avg 58.9 77.3 80.6 3 \nW-Avg 60.3 77.1 79.4 3  n=1 n=2 n=3 Best-N \nBeyati  66.7 56.4 61.5 1 \nHicaz 98.2 100 100 2,3 \nHicazkar  91.7 97.9 97.9 2,3 \nHüseyni  42.9 58.6 72.9 3 \nHüzzam  98.4 98.4 98.4 1,2,3  \nKürdilihicakar  100 100 98 1,2 \nMahur  70.6 82.4 74.5 2 \nMuhayyer  80.4 70.6 66.7 1 \nNihavent  97.5 97.5 97.5 1,2,3  \nRast  72.3 75.9 80.7 3 \nSaba  97.6 97.6 97.6 1,2,3  \nsegah  75.3 84.9 89 3 \nUşşak  69.4 54.1 56.5 1 \nTot-Avg 81.7 82.8 84.5 3 \nW-Avg 81.6 82.6 83.9 3 the databas\ne was chosen as the input. The rest of the \npieces were used for modeling the makam classes.   \nThree different experimental setups were created. The \ntests with data represented using Arel Theory showed that \nthe overall recal l performance of the system is 88.2% . \nIncreasing the order of n- grams  boosted the  classification \nperformance as expected. However, the effect is different \nfor different makams. We observed that increasing the n -\ngram order did not help when trying to distinguish ma-\nkams that use the same scale such as Beyati -Uşşak  and \nMuhayyer -Hüseyni . The second experimental setup was \nfor a real application case,  where there is no direct note \nlevel transcription. For this test, the data is represented as \nintervals (in commas) between consecutive notes. This \nexperiment was designed to provide reference inform a-\ntion for research on makam detection directly from audio  \nwhere exact note level transcription is not available. For audio, due to different instrumentation, and tuning, the \nonly reliable information is the intervallic movement. The \nresult showed that the  makam detection accuracy is \n%80.6 using with n order 3. Note that, higher order n \ngrams did not improve the experimental results beyond n=3 because of data spars ity. \nFor comparison with a previous study [2 ], [9], both the \ndata representation in [2 ], [9] and additional represent a-\ntions (Arel and interval representation) are tested and compared. It is observed from tests using a large dataset, \nand challenging makam couple sets, that a system using Arel Theory representation outperforms a system using \nthe 12 -TET representation on average 3.7% percent. In-\ncreasing the n -gram order beyond 3 did not improve the \nperformance of the tests due to lack of data.  \nFuture work includes defining global tonal features \nthat could help distinguishing makams having the same \nmicrotonal scale and tonic such as Uşşak - Beyati  and \nHüseyni  - M uhayyer . Features related to global progre s-\nsion of the melody could give clues that cannot be cap-\ntured by n -grams that concentrate more on local short \nlength movements.  \n6. ACKNOWLEDGEMENT  \nThis work was funded in part by the European Research \nCouncil under the European Union’s Seventh Framework \nProgramme (FP7/2007- 2013) / ERC grant agreement \n267583 (CompMusic)  and in part by TÜBİTAK ARDEB \ngrant no:3501- 109E196.  Any opinions, findings and \nconclusions or recommendations expressed in this m a-\nterial are those of the authors, and do not necessarily r e-\nflect those of the European Union  or the TÜBİTAK.  \n7. REFERENCES  \n[1] S. Abdoli,. \"Iranian Traditional Music Dastgah \nClassification,\" ISMIR, Florida ,2011  \n[2]  A. Alpkoçak and A.  C. Gedik. \" Classification of \nTurkish songs according to makams by using n grams,\" In Proceedings of the 15. Turkish \nSymposium on Artificial Intelligence and Neural \nNetworks (TAINN) , 2006.  \n[3] T. Aoyagi \"Makam Rast: Intervallic Ordering, Pitch \nHierarchy, Performance and Perception of a Melodic Mode in Arab Music,\"  University of California , \n2001.  \n[4] H. S. Arel, \"Türk Musikisi Nazariyatı Dersleri, \nHazırlayan Onur Akdoğu,\" Kültür Bakanlığı \nYayınları /1347, Ankara,  p.70. 1991 , \n[5] N. Darabi, N. Azimi  and H. Nojumi,  \"Recognition \nof Dastgah and Makam for Persian Music with \nDetecting Skeletal Melodic Models,\" The second \nannual IEEE BENELUX/DSP Valley Signal \nProcessing Symposium   2006  \n[6] S. Doraisamy, \"Polyphonic Music Retrieval: The N -\ngram Approach,\" PhD Thesis, University of London,  \n2004. \n[7] S. Downie \"Evaluating a simple approach to music \ninformation retrieval: Conceiving melodic n -grams \nas text,\" PhD thesis, University of Western Ontario, \n1999.  \n[8] A .  C .  G e d i k ,  a n d  B .  B o z k u r t ,  \" P i t c h -frequency \nhistogram- based music information retrieval for  \nTurkish music,\" Signal Processing, 90(4), 1049-\n1063, 2010.  \n[9] A.C. Gedik, C. Işıkhan, A. Alpkoçak and Y. Özer, \n“Automatic Classification of 10 Turkish Makams ,” \nInternational Congress on Representation in Music & Musical Representation, İstanbul , 2005.  \n[10] H. S. Heaps \"Information Retrieval: Computational \nand Theoretical Aspects,\"  Academic Press , 1978.  \n[11] L. Ioannidis, E. Gómez, and P. Herrera,  \"Tonal -\nbased retrieval of Arabic and Middle- East music by \nautomatic makam description ,\" CBMI , 2011.  \n[12] M. K. Karaosmanoğ lu, \"A Turkish makam music \nsymbolic data base for music information retrieval: \nSymbTr ,\"  submitted to ISMIR , 2012.  \n[13] C.L. Krumhansl, \"Cognitive Foundations of Musical \nPitch ,\" Oxford University Press, NewYork , 1990.  \n[14] I. H. Özkan, \"Türk musikisi nazariyatı ve us ulleri: \nkudüm velveleleri ,\". Ötüken Neşriyat , 2006.  \n[15] A. Stolcke : “Srilm – an Extensible Language \nModeling Toolkit,” Proc eedings of the  International \nConference on Spoken Language Processing, 2002.  \n[16] S. Şentürk, and P. Chordia, “Modeling Melodic Improvisation  in Turkish Folk Music Using \nVariable -length Markov Models ,” ISMIR , Florida , \n2011.  \n[17] E .  U n a l ,  E .  C h e w ,  P .  G e o r g i o u  a n d  S. Narayanan, \n\"Perplexity based cover song identification System for short length queries ,\" ISMIR , Florida , 2011"
    },
    {
        "title": "How Significant is Statistically Significant? The case of Audio Music Similarity and Retrieval.",
        "author": [
            "Julián Urbano",
            "J. Stephen Downie",
            "Brian McFee",
            "Markus Schedl"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1418055",
        "url": "https://doi.org/10.5281/zenodo.1418055",
        "ee": "https://zenodo.org/records/1418055/files/UrbanoDMS12.pdf",
        "abstract": "The principal goal of the annual Music Information Re- trieval Evaluation eXchange (MIREX) experiments is to determine which systems perform well and which systems perform poorly on a range of MIR tasks. However, there has been no systematic analysis regarding how well these evaluation results translate into real-world user satisfac- tion. For most researchers, reaching statistical significance in the evaluation results is usually the most important goal, but in this paper we show that indicators of statistical sig- nificance (i.e., small p-value) are eventually of secondary importance. Researchers who want to predict the real- world implications of formal evaluations should properly report upon practical significance (i.e., large effect-size). Using data from the 18 systems submitted to the MIREX 2011 Audio Music Similarity and Retrieval task, we ran an experiment with 100 real-world users that allows us to explicitly map system performance onto user satisfaction. Based upon 2,200 judgments, the results show that abso- lute system performance needs to be quite large for users to be satisfied, and differences between systems have to be very large for users to actually prefer the supposedly better system. The results also suggest a practical upper bound of 80% on user satisfaction with the current definition of the task. Reflecting upon these findings, we make some rec- ommendations for future evaluation experiments and the reporting and interpretation of results in peer-reviewing.",
        "zenodo_id": 1418055,
        "dblp_key": "conf/ismir/UrbanoDMS12",
        "keywords": [
            "MIREX experiments",
            "determine system performance",
            "statistical significance",
            "practical significance",
            "user satisfaction",
            "real-world implications",
            "data analysis",
            "100 real-world users",
            "explicit mapping",
            "user preference"
        ],
        "content": "HOW SIGNIFICANT IS STATISTICALLY SIGNIFICANT?\nTHE CASE OF AUDIO MUSIC SIMILARITY AND RETRIEV AL\nJuli´an Urbano\nUniversity Carlos III of Madrid\nBrian McFee\nUniversity of California at San DiegoJ. Stephen Downie\nUniversity of Illinois at Urbana-Champaign\nMarkus Schedl\nJohannes Kepler University Linz\nABSTRACT\nThe principal goal of the annual Music Information Re-\ntrieval Evaluation eXchange (MIREX) experiments is to\ndetermine which systems perform well and which systems\nperform poorly on a range of MIR tasks. However, there\nhas been no systematic analysis regarding how well these\nevaluation results translate into real-world user satisfac-\ntion. For most researchers, reaching statistical signiﬁcance\nin the evaluation results is usually the most important goal,\nbut in this paper we show that indicators of statistical sig-\nniﬁcance (i.e., small p-value) are eventually of secondary\nimportance. Researchers who want to predict the real-\nworld implications of formal evaluations should properly\nreport upon practical signiﬁcance (i.e., large effect-size).\nUsing data from the 18 systems submitted to the MIREX\n2011 Audio Music Similarity and Retrieval task, we ran\nan experiment with 100 real-world users that allows us to\nexplicitly map system performance onto user satisfaction.\nBased upon 2,200 judgments, the results show that abso-\nlute system performance needs to be quite large for users\nto be satisﬁed, and differences between systems have to be\nvery large for users to actually prefer the supposedly better\nsystem. The results also suggest a practical upper bound of\n80% on user satisfaction with the current deﬁnition of the\ntask. Reﬂecting upon these ﬁndings, we make some rec-\nommendations for future evaluation experiments and the\nreporting and interpretation of results in peer-reviewing.\n1. INTRODUCTION\nEvaluation experiments are the main research tool in Infor-\nmation Retrieval (IR) to determine which systems perform\nwell and which perform poorly for a given task [1]. Sev-\neral effectiveness measures are used to assign systems a\nscore that estimates how well they perform. The assump-\ntion underlying these evaluations is that systems with bet-\nter scores are actually perceived as more useful by the users\nand therefore are expected to bring more satisfaction.\nResearchers are usually interested in the comparison be-\ntween systems: is system Abetter or worse than system\nB? After running an experiment with a test collection, re-\nsearchers have a numeric answer to that question that mea-\nsures the effectiveness difference between systems. Statis-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.tical procedures are then used to check whether that dif-\nference is statistically signiﬁcant or not. Statistical signif-\nicance is usually thought of as a sort of bulletproof evi-\ndence that one system really is better than another. Teams\nusually follow one or another research line based solely on\nstatistical signiﬁcance, and it has also become an essential\nrequirement for publication in peer-reviewed venues.\nHowever, there are several misconceptions regarding sta-\ntistical signiﬁcance [2, 11]. In the case of IR evaluation\nexperiments, null hypotheses about differences in perfor-\nmance are false by deﬁnition, so observing a small p-value\nto conclude signiﬁcance is just a matter of meeting certain\nconditions in the experiment. On the other hand, very little\nattention is paid to the effect-sizes and their implications\nin practical terms. In fact, even if statistical signiﬁcance is\npresent, the difference between two systems may very well\nbe so subtle that users do not note the difference.\nHowever, IR evaluations are traditionally focused on the\nalgorithmic aspect of the systems, and whether the results\ndo predict user satisfaction or not is very seldom stud-\nied [8]. Evaluation experiments make different assump-\ntions regarding the operational settings and the needs and\nbehavior of the users, so the extent to which results can be\nextrapolated should be questioned [9].\nIn this paper we focus on the evaluation of the Au-\ndio Music Similarity and Retrieval task (AMS), as carried\nout in the annual Music Information Retrieval Evaluation\neXchange (MIREX). AMS is one of the tasks that most\nclosely resemble a real-world music retrieval scenario, and\nit is also one of the tasks that receives most attention from\nthe research community. We carried out an experiment\nwith 100 users that allowed us to map system effective-\nness onto user satisfaction, providing a new perspective\nin the interpretation of evaluation results. We also argue\nthat researchers should not only focus on achieving statis-\ntical signiﬁcance in effectiveness differences, but also on\nthe size and practical implications of such differences.\n2. SYSTEM EFFECTIVENESS\nAND USER SATISFACTION\nIn the MIREX AMS evaluation experiments, the similar-\nity of a document to a query is assessed by humans and\nbased on two different scales. The Broad scale has three\nlevels: 0 (not similar), 1 (somewhat similar) and 2 (very\nsimilar). The Fine scale has 101 levels, from 0 (not similar\nat all) to 100 (identical to the query). Only one measure is\nreported to assess the effectiveness of the participating sys-\ntems:AG@5 (Average Gain after 5 documents retrieved):AG@k =1\nkkX\ni=1gaini\nwheregainiis the gain of the i-th document retrieved (the\nsimilarity score assigned). Two versions of AG@5 are ac-\ntually reported, following the Broad and Fine scales.\nAG@k assumes that a document retrieved at rank 3 is\nas useful as a document retrieved at rank 30. A measure\nwith a more realistic user model is nDCG@k (Normalized\nDiscounted Cumulated Gain after kretrieved) [3]:\nnDCG@k =Pk\ni=1gaini=log2(i+ 1)\nPk\ni=1gain\u0003\ni=log2(i+ 1)\nwheregain\u0003\niis the gain of the i-th document in the ideal\nranking (i.e.8i:gain\u0003\ni\u0015gain\u0003\ni+1). The gain contri-\nbution of a document is discounted with the logarithm of\nthe rank at which it is retrieved, thus penalizing late ar-\nrival of relevant documents. Also, the gain contribution of\ndocuments is divided by the ideal contribution, bounding\nthe measure between 0 and 1. Therefore, and for the sake\nof simplicity when comparing results across measures, we\nnormalizeAG@k between 0 and 1 too:\nnAG @k=1\nk\u0001l+kX\ni=1gaini\nwherel+is the maximum similarity score allowed by the\nscale: 2 in the Broad scale and 100 in the Fine scale.\n2.1 Interpretation of Effectiveness Scores\nAfter running an evaluation of AMS systems, researchers\ninterpret the results and make design decisions accordingly\n[9]. The ultimate goal is answering this question: what\nsystem would yield more user satisfaction? But we need to\nask something else ﬁrst: what measure and what similarity\nscale are better to predict user satisfaction?\nIntuitively, if a system obtained a nAG @5ornDCG@5\nscore of 1, our interpretation would be that an arbitrary\nuser would be 100% satisﬁed with the results of the sys-\ntem, or satisﬁed 35% of the times if the effectiveness score\nachieved were 0.35. On the other hand, if system Aob-\ntained an effectiveness score larger than the one obtained\nby system B, we should expect users to prefer A. By choos-\ning one or another measure, researchers make different as-\nsumptions as to the behavior and needs of the ﬁnal users,\nand by choosing one or another similarity scale they follow\ndifferent criteria to differentiate satisfying from unsatisfy-\ning results. To the best of our knowledge, none of these\nassumptions has been validated in the literature so far.\n3. EXPERIMENTAL DESIGN\nWe devised an experiment with actual users that allowed\nus to map system effectiveness onto user satisfaction. Sub-\njects were presented with a query clip and two ranked lists\nof ﬁve results each, as if retrieved by two different AMS\nsystems AandB[8]. They had to listen to the clips and\nthen select one of the following options: system Apro-\nvided better results, system Bdid, they both provided good\nFigure 1. Task template used in the experiment.\nresults, or they both returned bad results (see Figure 1).\nFrom these we can differentiate 4 judgments:\n\u000fPositive preference , if the subject selected the sys-\ntem whose results yield larger effectiveness.\n\u000fNegative preference, if the subject selected the sys-\ntem whose results yield smaller effectiveness.\n\u000fGood nonpreference, if the subject indicated both\nsystems are equally good.\n\u000fBad nonpreference, if the subject indicated both\nsystems are equally bad.\nSuch a design allows us to analyze the results from two\ndifferent approaches: evaluation of a single system and\ncomparison of two systems. Subjects indicating that both\nsystems are good suggest that they are satisﬁed with both\nranked lists. That is, their answer serves as an indication\nthat the effectiveness measured for those systems translates\ninto user satisfaction. If, on the other hand, they indicate\nthat both systems are bad, we can infer that those effec-\ntiveness scores do not translate into user satisfaction. Sub-\njects indicating a preference for one ranked list over the\nother one suggest that there is a difference between them\nlarge enough to be noted. That is, their answer serves as an\nindication that the difference in effectiveness between the\nsystems translates into users being more satisﬁed with one\nsystem than with the other.\n3.1 Data\nWe used the similarity judgments collected for the 2011\nedition of the MIREX AMS task: a total of 18 systems by\n10 research teams were evaluated with 100 queries, leading\nto a total of 6,322 unique similarity judgments. This is the\nlargest edition as of the writing of this paper1.\nAccording to the deﬁnition of nAG @kwith Broad judg-\nments, the difference between two systems is always a mul-\ntiple of 0.1. For each difference \u00012f0;0:1;:::; 1g, we se-\nlected 200 random queries and artiﬁcially created two ran-\ndom ranked lists of 5 documents such that their difference\ninnAG @5would be \u0001according to the Broad judgments\nmade for that query in MIREX 2011. Therefore, we have\na total of 2,200 examples. Note that for the extreme value\n\u0001 = 1 we need at least 5 very similar documents and 5 not\n1http://www.music-ir.org/mirex/wiki/2011:MIREX2011 ResultsDifference in effectiveness scoresFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nAG@5 Broad\nDifference in effectiveness scoresFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nAG@5 Fine\nDifference in effectiveness scoresFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nDCG@5 Broad\nDifference in effectiveness scoresFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nDCG@5 FineFigure 2. Distribution of effectiveness differences in all 2,200\nexamples, for nAG@5 (top) and nDCG @5(bottom), and Broad\n(left) and Fine (right) judgments.\nsimilar documents for the query. Due to this restriction, we\ncould actually use only 73 of the total 100 queries. Across\nall 2,200 examples, we had 2,869 unique ranked lists of\nresults, with 3,031 unique clips (including the 73 queries).\nFigure 2 shows the distributions of effectiveness differ-\nences in the 2,200 examples. As mentioned, differences\nfornAG @5with Broad judgments follow a uniform dis-\ntribution, but with the Fine judgments there are very few\nexamples with large differences. We note though that this\nis an artifact of the Fine scale itself and not a sampling\nﬂaw: for \u0001 = 0:9 we need 5 documents with very high\nsimilarity scores (90 to 100) and 5 documents with very\nlow scores (0 to 10); however, assessors very rarely assign\nsuch small and large scores. Therefore, it is very rare to\nobserve differences that large when using the Fine scale.\nThese 2,200 pairs of artiﬁcial ranked lists can also be\nevaluated as per nDCG@5. As Figure 2 shows, the dis-\ntributions of differences in nDCG@5 are very similar to\nnAG @5. Our examples do therefore cover the wide range\nof possible evaluation outcomes.\n3.2 Procedure\nAll 2,200 judgments were collected via crowdsourcing. Pre-\nvious work by Lee [6] and Urbano et al. [10] demonstrated\nthat music similarity judgments gathered through crowd-\nsourcing platforms are very similar to the ones collected\nwith experts, with fast turnaround and low cost. Another\nadvantage of using crowdsourcing for our experiment is\nthat it offers a large and diverse pool of subjects around\nthe globe. Using a controlled group of students or experts\nwould probably bias our results, but using a diverse pool\nof workers allows us to draw conclusions that should gen-\neralize to the wider population of users.\nHowever, using crowdsourcing has other issues. The\nquality of judgments via crowdsourcing can be questioned\nbecause some workers are known to produce spam answers\nand others provide careless answers to proﬁt without actu-\nally doing the task. We decided to use the platform Crowd-\nﬂower to gather the judgments, which delegates the work\nto other platforms such as Amazon Mechanical Turk. It\nalso provides a quality control layer at the process level\nthat separates good from bad workers by means of trap ex-\namples [5,8]: some of the examples shown to workers haveknown answers (provided by us) that are used to estimate\nworker quality. Workers that show low quality on the trap\nexamples are rejected, and those that show high agreement\nare allowed to participate. We provided Crowdﬂower with\n20 such trap examples (5 for each of the four answers), as-\nsigning each of them a subjective level of difﬁculty based\non the answers by two experts.\n3.3 Task Design\nFigure 1 shows the task template we used. A ﬁrst section\nlisted the task instructions, and then a Flash player permit-\nted subjects to listen to the query clip. Below, they could\nﬁnd the two ranked lists of 5 results each, followed by ra-\ndio buttons to select the answer. Finally, a textbox was pro-\nvided for workers to optionally leave feedback. All 3,031\naudio clips were uploaded to our servers, and served upon\nrequest. The order in which examples are shown to work-\ners is random, as is the assignment of the ranked lists as\nsystem Aor system B. Also, we limited the maximum\nnumber of answers by a single worker to 50, minimizing\nthe possible bias due to super-workers.\nWe collected all answers in four batches of 550 exam-\nples each. Lee collected similarity judgments paying $0.20\nfor 15 query-document pairs [6], while Urbano et al. col-\nlected preference judgments paying $0.02 for each query-\ndocument-document [10]. In both studies workers were\ntherefore paid approximately $0.007 per audio clip. Music-\nrelated tasks are known to be enjoyable by workers, and\ngiven that quality does not signiﬁcantly degrade when de-\ncreasing wages [7], we decided to pay $0.03 for each ex-\nample, leading to approximately $0.003 per clip. Adding\nthe corresponding feeds to Crowdﬂower, all 2,200 judg-\nments were collected for a grand total of $100.\n4. RESULTS2\nThe four batches were completed in less than 24 hours. We\ncollected answers from 881 unique workers from 62 coun-\ntries and 7 different crowdsourcing markets. These work-\ners provided a grand total of 6,895 answers, from which\nCrowdﬂower accepted 3,393 (49%) as trustworthy. Note\nthat the extra answers are due to repeatedly showing trap\nexamples to workers. Only 100 workers were responsible\nfor these trusted answers, so 781 workers (87%) were re-\njected. The average quality of these 100 workers, as com-\nputed by Crowdﬂower [5], ranges from 60% to 100%, with\nan average of 95%. In fact, 27 of our 2,200 examples con-\ntained the exact same documents, in the exact same order,\nin both ranked lists. Only twice did we not get, as should\nhave, an unsigned preference in these cases. Therefore, the\nresults reported herein comprise 2,200 answers by 100 dif-\nferent users who, apparently, provided honest responses.\n4.1 Evaluation of a Single System\nFor 884 of the 2,200 examples (40%) we received a non-\npreference (i.e. subjects judged both systems as equally\ngood or bad). Therefore, we have 1,768 ranked lists that\nsubjects considered equally satisfying. Figure 3 shows the\n2All data can be downloaded from http://julian-urbano.info.Absolute effectiveness scoreFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nAG@5 Broad\nAbsolute effectiveness scoreFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nAG@5 Fine\nAbsolute effectiveness scoreFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nDCG@5 Broad\nAbsolute effectiveness scoreFrequency\n0.0 0.2 0.4 0.6 0.8 1.00 100 200 300nDCG@5 FineFigure 3. Distribution of absolute effectiveness scores in the\n884 examples with unsigned preferences, for nAG@5 (top) and\nnDCG @5(bottom), and Broad (left) and Fine (right) judgments.\ndistributions of absolute effectiveness scores. As can be\nseen, a wide range of scores are covered, following a some-\nwhat uniform distribution as well. The number of good and\nbad nonpreferences was almost the same too: 440 vs. 444.\nFigure 4 shows the ratio of good nonpreferences ob-\nserved in these 884 examples as a function of absolute\neffectiveness. As expected, there is a very tight positive\ncorrelation between effectiveness and user satisfaction. In\nfact, the relationship appears to be nearly linear. There is\nno appreciable difference between measures, but the Fine\nscale seems to adhere better to the diagonal than the Broad\nscale does. Note that the deviations from the trend with the\nFine judgments (\u0001 <0:2and\u0001>0:8) are just an artifact\nof the very small number of observations in that range (see\nSection 3.1 and Figure 3).\nFigure 4 shows a pretty straightforward mapping be-\ntweennAG @kandnDCG@k scores and user satisfac-\ntion. However, the Broad scale seems to reveal a practi-\ncal lower bound of 20% and an upper bound of 80% on\nuser satisfaction. This could be merely due to noise in the\ncrowdsourced data or a fault in the measures or scales. But\ngiven the symmetry, we believe these bounds are due to the\nnatural diversity of users: some might consider something\na very good result while others do not [4]. This means that\neven if a system obtains a nAG @5score of 0, about 20%\nof the users will like the results (or dislike if nAG @5 = 1 ).\nThis is evidence of the room for improvement through\npersonalization. Therefore, the AMS evaluations should\ninclude a user factor, possibly through user proﬁles, so that\nsystems can attempt to reach 100% satisfaction on a per\nuser basis. Otherwise, the ﬁnal user satisfaction should not\nbe expected to pass 80% for arbitrary users.\n4.2 Evaluation of Two Systems\nFor 1,316 of the 2,200 examples (60%) we did receive a\npreference (i.e. subjects indicated that one system pro-\nvided better results than the other one). Whether those\nuser preferences were positive or negative (i.e. agreeing\nwith the effectiveness difference or not), depends on the\ncombination of measure and scale used. Figure 5 shows\nthe ratio of preference signs across all 2,200 examples.\nIn terms of positive preferences (left plot), ideally we\nwould want users to show a preference for the better sys-\nAbsolute effectiveness score% of good nonpreferences\nnAG@5 Broad\nnAG@5 FinenDCG@5 BroadnDCG@5 Fine0 10 20 30 40 50 60 70 80 90 100\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1User satisfactionFigure 4. Ratio of good and bad nonpreferences in 884 examples,\nas a function of absolute system effectiveness, for AG@5 and\nnDCG @5combined with the Broad and Fine judgments.\ntem whenever we observe an effectiveness difference in the\nevaluation, regardless of how large this difference is. But\nthere is a very tight positive correlation instead: the larger\nthe difference in effectiveness, the more likely for users to\nprefer the supposedly better system. The relationship is\nagain nearly linear, though this time we can observe a very\nclear difference between the Broad and Fine scales: for\nthe same magnitude of the difference, the Fine judgments\nare always closer to the ideal 100% of positive user pref-\nerences. In fact, the Broad scale seems to indicate once\nagain an upper bound of 80%. In addition, the plot shows\nthat for users to prefer the supposedly better system more\nthan the random 50% of the times, a difference of at least\n0.3 in the Fine scale is needed, or 0.5 in the Broad scale.\nNote that the deviations from the trend with the Fine judg-\nments (\u0001>0:8) are also here just an artifact of the very\nsmall number of observations in that range.\nAs a consequence, there is a very clear negative corre-\nlation in terms of nonpreferences (middle plot): the larger\nthe differences between systems, the more likely for users\nto prefer one of them. Again, the Fine scale seems to be-\nhave better than the Broad scale.\nAs the right plot shows, all four combinations of mea-\nsure and similarity scale yield very similar ratios of neg-\native preferences. There is a very slight negative correla-\ntion with difference in effectiveness, but in general about\n5-10% of the user preferences disagree with the sign of the\neffectiveness difference. That is, about 5-10% of the times\nusers prefer the supposedly worse system.\n5. UNDERSTANDING EV ALUATION RESULTS\nThe effectiveness of IR systems is assessed with different\nmeasures such as nAG @kandnDCG@k . These mea-\nsures are used to assign systems a score that represents how\nwell they would satisfy users. For an arbitrary system Aa\nmeasureMdeﬁnes a distribution of effectiveness scores\nYA, describing the effectiveness of the system for an arbi-\ntrary query. The goal of evaluation experiments is usually\nﬁnding the mean of that distribution: yA.\nComputing the parameter yAallows researchers to as-\nsess how well the system performs and what is the ex-\npected user satisfaction according to the user model un-Difference in effectiveness scores% of examplesnAG@5 Broad\nnAG@5 FinenDCG@5 BroadnDCG@5 Fine0 10 20 30 40 50 60 70 80 90 100\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Positive preferences\nDifference in effectiveness scores% of examplesnAG@5 Broad\nnAG@5 FinenDCG@5 BroadnDCG@5 Fine0 10 20 30 40 50 60 70 80 90 100\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Nonpreferences\nDifference in effectiveness scores% of examplesnAG@5 Broad\nnAG@5 FinenDCG@5 BroadnDCG@5 Fine0 10 20 30 40 50 60 70 80 90 100\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Negative preferencesFigure 5. Ratio of positive preferences (left), nonpreferences (middle) and negative preferences (right) observed in the 2,200 examples,\nas a function of system effectiveness difference, for nAG@5 andnDCG @5combined with the Broad and Fine scales.\nderlyingM. However computing this distribution would\nrequire running the system for the universe of all queries,\nwhich is clearly impossible. Instead, IR evaluation exper-\niments are run with a sample of queries Q, so they are\nused as estimators of the true yA. The average effective-\nness across queries, yA, is used as the estimate ^yA. Like\nany other estimate, ^yAbears some uncertainty, so statisti-\ncal techniques such as conﬁdence intervals should be em-\nployed to report the conﬁdence on the estimation.\nWhen comparing two systems, say AandB, one is usu-\nally interested in the distribution of the difference DAB,\nrepresenting the paired difference in effectiveness between\nAandBfor an arbitrary query. Again, a comparative IR\nevaluation experiment only provides an estimate ^d, whose\nsign indicates which system is expected to perform better.\n5.1 Statistical Signiﬁcance: p-values\nGiven thatdis an estimate, the immediate question is: how\nconﬁdent can we be of this difference? The observed d\ncould be just a random and rare observation due to the\nparticular sample of queries used. Again, statistical tech-\nniques are needed to compute some sort of conﬁdence on\nthe difference. The most popular is hypothesis testing.\nIn a statistical hypothesis testing procedure, a nullhy-\npothesisH0is deﬁned, such as H0:d= 0. The alterna-\ntive, or research hypothesis, is then deﬁned as the opposite:\nH1:d6= 0. All hypothesis testing procedures are based\non probability distributions, so there is always some de-\ngree of uncertainty when estimating parameters such as d.\nThus, researchers may commit one of two errors: a Type I\nerror if they conclude H0is not true when it actually is, or\na Type II error if they conclude H0is true when it is not.\nThe maximum probability of committing a Type I error is\nknown as the signiﬁcance level, usually \u000b= 0:05. The\nprobability of committing a Type II error is denoted with\nthe letter\f, and 1\u0000\fis known as the power of the test: the\nprobability of detecting a difference if there really is one.\nThe result of a hypothesis testing procedure is a proba-\nbility called p-value. These are usually mistaken with the\nprobability of H0being true [2, 11], but they are actually\nthe probability of observing the difference d(or one larger)\nunder the assumption that H0is true. That is, p-values are\nthe probability of the data given the hypothesis, not the\nprobability of the hypothesis given the data. If the reported\np-value is smaller than the signiﬁcance level \u000b, we then\nreject the null hypothesis in favor of the alternative, andsay that the difference is statistically signiﬁcant. But it is\nimportant to note that the test does nottell anything about\nH0being true or false: that dichotomous interpretation is\nmade by usbased on the p-value and \u000b, not by the test.\nThis is the ultimate goal of an IR evaluation: reaching\nsigniﬁcance. However, observing a statistically signiﬁcant\ndifference between two systems is usually misinterpreted\nas having high conﬁdence that one system really is bet-\nter than the other one because H0was rejected [2, 11]. In\nfact, all these null hypotheses are false by deﬁnition: any\ntwo different systems produce a distribution of differences\nwithd6= 0. What is important is the magnitude of d: dif-\nferences of 0.0001, for instance, are probably irrelevant,\nbut differences of 0.8 deﬁnitely are. However, a difference\nof just 0.0001 will always be statistically signiﬁcant under\ncertain experimental conditions, so focusing on statistical\nsigniﬁcance alone becomes, at some point, meaningless.\n5.2 Practical Signiﬁcance: effect-sizes\nThe most popular procedure to test such hypotheses about\npopulation means is the paired t-test. In IR evaluation, the\nhypotheses use to be H0:d\u00140andH1:d>0. The test\nstatistic is then computed as (note that in our case d= 0):\nt=d\u0000d\nsd=p\njQj(1)\nwheresdanddare the standard deviation and mean of the\nsample ofDABcomputed with the set of queries Qin the\ntest collection. Using the t-distribution’s cumulative distri-\nbution function, the p-value is then calculated as the area\nthat is to the right of t. If p-value < \u000b,wereject the null\nhypothesis and plainly conclude d>0.\nExamining Eqn. (1) we can see that the test is more\nlikely to come up signiﬁcant with larger observed differ-\nencesdand smaller deviations sd. But most important is\nto note that the power of the test is also directly propor-\ntional to the sample size jQj: the more queries we use to\nevaluate systems, the more likely to observe a signiﬁcant\ndifference. This shows that focusing on signiﬁcance alone\nis eventually meaningless: all a researcher needs to do in\norder to obtain signiﬁcance is evaluate with more queries.\nIncreasing the sample size (number of queries) increases\nthe power of the test to detect ever smaller differences be-\ncause the standard error on the mean, sd=p\njQj, decreases.\nThus, observing a statistically signiﬁcant difference doesnot mean that the systems really are different, in fact they\nalways are. It just means that the observed difference and\nthe sample size used were large enough to conclude with\nconﬁdence that the true difference is larger than zero.\nWhat really matters is how far apart from zero dis.\nThis is the effect-size, which measures the practical signif-\nicance of the result. As shown in Section 4.2, large differ-\nences in effectiveness scores (large effect-sizes) do predict\nmore user satisfaction, but small differences do not really.\nHowever, with a sufﬁciently large number of queries we\nmay be able to detect a statistically signiﬁcant difference\nwhose effect-size is extremely small, having no value for\nreal users. In such a case we would have statistical signiﬁ-\ncance, but no practical signiﬁcance at all.\n5.3 Reporting and Interpreting Results\nWe showed above that obtaining small p-values (statistical\nsigniﬁcance) should not be the sole focus of researchers\nwhen running evaluation experiments. The focus should\nreally be on obtaining large effect-sizes (practical signif-\nicance). The easiest way to report effect-sizes is just to\nreport the effectiveness difference between systems or the\nabsolute score of a single system. But these ﬁgures are just\nestimates of population means, and therefore subject to er-\nror. A better way to report effect-sizes is with conﬁdence\nintervals, computed as d\u0006t\u000b=2\u0001sd=p\njQj. Conﬁdence\nintervals for the absolute effectiveness of a single system\nare computed likewise, but using the yandsyestimates.\nAlong with the results in Section 4, these conﬁdence in-\ntervals can be used to interpret evaluation results from the\nultimate perspective of user satisfaction. For instance, the\nHKHLL1 system in MIREX AMS 2011 obtained a nAG @5\nscore of 0.422 for the Fine judgments, with a 95% conﬁ-\ndence interval ranging from 0.376 to 0.468. According to\nthe results in Figure 4, this system is expected to satisfy an\narbitrary user from about 35% to 45% of the times.\nOn the other hand, the difference between SSPK2 and\nDM2 was found to be statistically signiﬁcant. The magni-\ntude of the difference was just 0.082, with the 95% con-\nﬁdence interval ranging from 0.051 to 0.112. According\nto Figure 5 though, such difference is hardly ever noted by\nthe users. Indeed, substituting in Eqn. (1) we ﬁnd that any\ndlarger than 0.031 would have been deemed as statistically\nsigniﬁcant for these two systems. This is an example of a\nstatistically signiﬁcant difference that makes no practical\ndifference for arbitrary users.\nIn summary, we suggest to report not only the observed\nscores but also their conﬁdence intervals, and the actual\np-values rather than an indication of signiﬁcance. For in-\nstance, a proper report for a single system would read as\nnAG @5 = 0:584\u00060:023. For the difference between two\nsystems, we suggest \u0001nAG@k =0:037\u00060:031(p =0:02).\nBy reporting the p-value we leave the interpretation of sig-\nniﬁcance to the reader and his operational context: a large\neffect-size (e.g. d= 0:43), even if not statistically signiﬁ-\ncant (e.g. p-value = 0:06), is deﬁnitely worth implement-\ning. After all, the levels \u000b= 0:05 and\u000b= 0:01, despite\nwidely accepted, are completely arbitrary. People gener-\nally consider p-value = 0:054 as signiﬁcant, while othersrequest p-value <0:005. It depends on the context of the\nreader and factors such as the cost of committing a Type I\nerror or the cost of implementing one or another technique.\n6. CONCLUSIONS\nReaching statistical signiﬁcance in IR evaluation experi-\nments is usually the most important goal for researchers.\nA difference between systems is usually regarded as im-\nportant if signiﬁcance is involved, when in reality all sys-\ntems are different. With the development of ever larger test\ncollections, statistical signiﬁcance can easily be misunder-\nstood, suggesting large differences between systems when\nthey are actually very similar. To predict the real-world im-\nplications of these differences, researchers need to focus on\neffect-sizes as indicators of practical signiﬁcance. That is,\nit does not matter whether there is a difference or not (in\nfact, there always is), what matters is how large it is. Final\nuser satisfaction is only predicted with effect-sizes. Statis-\ntical signiﬁcance serves just as a measure of conﬁdence.\nHowever, even when reporting on the magnitude of ef-\nfectiveness differences, there is no established relationship\nwith ﬁnal user satisfaction. To ﬁll this gap we carried out a\nuser study with 100 real users in the context of the Au-\ndio Music Similarity and Retrieval task, where subjects\nindicated their preferences between different system out-\nputs. Our results allow researchers to map observed ab-\nsolute scores and relative effectiveness differences directly\nonto expected user satisfaction. In addition, they suggest\nroom for improvement if considering personalization, as\nwell as further work on the development of measures and\nevaluation criteria that more closely capture the user model\nunderlying the task.\n7. REFERENCES\n[1] D. Harman. Information retrieval evaluation. Synthesis Lec-\ntures on Information Concepts, Retrieval, and Services, 2011.\n[2] J. Ioannidis. Why most published research ﬁndings are false.\nPLoS Medicine, 2005.\n[3] K. J ¨arvelin and J. Kek ¨al¨ainen. Cumulated gain-based evalua-\ntion of IR techniques. ACM Trans. Inf. Systems, 2002.\n[4] M. Jones, J. Downie, and A. Ehmann. Human similarity judg-\nments: implications for the design of formal evaluations. In\nISMIR, 2007.\n[5] J. Le, A. Edmonds, V . Hester, and L. Biewald. Ensuring qual-\nity in crowdsourced search relevance evaluation: the effects\nof training question distribution. In ACM SIGIR Workshop on\nCrowdsourcing for Search Evaluation, 2010.\n[6] J. Lee. Crowdsourcing music similarity judgments using Me-\nchanical Turk. In ISMIR, 2010.\n[7] W. Mason and D. Watts. Financial incentives and the per-\nformance of crowds. In ACM SIGKDD Workshop on Human\nComputation, 2009.\n[8] M. Sanderson, M. Paramita, P. Clough, and E. Kanoulas. Do\nuser preferences and evaluation measures line up? In ACM\nSIGIR, 2010.\n[9] J. Urbano. Information retrieval meta-evaluation: challenges\nand opportunities in the music domain. In ISMIR, 2011.\n[10] J. Urbano, J. Morato, M. Marrero, and D. Mart ´ın. Crowd-\nsourcing preference judgments for evaluation of music simi-\nlarity tasks. In ACM SIGIR Workshop on Crowdsourcing for\nSearch Evaluation, 2010.\n[11] S. Ziliak and D. McCloskey. The cult of statistical signif-\nicance: how the standard error costs us jobs, justice, and\nlives. University of Michigan Press, 2008."
    },
    {
        "title": "Modeling Musical Mood From Audio Features and Listening Context on an In-Situ Data Set.",
        "author": [
            "Diane K. Watson",
            "Regan L. Mandryk"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415234",
        "url": "https://doi.org/10.5281/zenodo.1415234",
        "ee": "https://zenodo.org/records/1415234/files/WatsonM12.pdf",
        "abstract": "Real-life listening experiences contain a wide range of music types and genres.  We create the first model of mu- sical mood using a data set gathered in-situ during a us- er’s daily life. We show that while audio features, song lyrics and socially created tags can be used to successful- ly model musical mood with classification accuracies greater than chance, adding contextual information such as the listener’s affective state or listening context can improve classification accuracy. We successfully classify musical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context.",
        "zenodo_id": 1415234,
        "dblp_key": "conf/ismir/WatsonM12",
        "keywords": [
            "real-life listening experiences",
            "wide range of music types and genres",
            "first model of musical mood",
            "data set gathered in-situ",
            "user’s daily life",
            "audio features",
            "song lyrics",
            "socially created tags",
            "successful classification accuracy",
            "classification accuracies greater than chance"
        ],
        "content": "MODE LING MUSICAL MOOD FROM AUDIO FEATURES \nAND LISTENING CONTEXT  ON A N IN- SITU  DATA SET  \nDiane Watson Regan L. Mandryk \nUniversity of Saskatchewan \ndiane.watson@usask.ca  University of Saskatchewan  \nregan.mandryk@usask.ca  \nABSTRACT  \nReal-life listening experiences contain a wide range of \nmusic types and genres .  We create the first model of m u-\nsical mood using a data set gathered in -situ during a us-\ner’s daily life. We show that wh ile audio features, song \nlyrics and socially created tags can be used to successful-\nly model musical mood with classification accuracies \ngreater than chance, adding contextual information such \nas the listener’s affective state or listening context can impro ve classification accuracy. We successfully classify \nmusical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context. \n1. INTRODUCTION  \nMusical mood – the emotion expressed  by a piece of mu-\nsic – is conveyed to a listener through a variety of musi-\ncal cues in the form of auditory features. These auditory \nfeatures (e.g., mode, rhythm, articulation, intensity and timbre of a musical track) have previously been used to model musi cal mood with fairly good classification re-\nsults [1-2]. However, current high performing models of \nmusical mood have two main problems: first, mu sic is \nconstrained to a single genre (usually Western classical or \nWestern popular) ; and second, the data is collected and \nlabeled in laboratory contexts. Previous work has shown \nthat t he data sets  used in previous research modeling mu-\nsical mood do not correspond to real -life listening experi-\nences in a number of ways  [3]. First, people listen to m u-\nsic of various genres  in their daily life; second , music is \nlistened to  as part of  social activi ties or in a public venue ; \nthird, music is attended  to as a secondary activity while \ndriving, working, or ex ercising ; finally, people listen to \nmusic f or various reasons, such as to relax, to entertain, \nor to influence emotion  [3]. \n    Previous high- performing musical mood models, based \non data from a single genre and gathered in a laboratory \nsetting may fail when applied to data sets gathered in da i-\nly life. Systems implementing these previous models – \nsuch as music recommender systems – may also fail \nwhen using data collected from real -life listening experi-ences, which  may lead to negative user experience s. \nHowever, musical mood clas sifiers built on a broad data \nset, containing several genres and labeled during real -life \nactivities rather than in a laboratory, may be unusable in real systems if they yield weak classification results.  \n    To solve the problem of building good musical mood \nclassifiers that are effective for real -life listening experi-\nences , we include context -sensitive features in addition to \nthe previously used  auditory features.  Our data set of re-\nal-life listening experiences was gathered through an e x-\nperience -sampling study using smartphones . Participants \nwere handed phones for a period of two weeks. Phones would randomly poll the user about once per hour and ask them to fill out a survey collecting musical mood, the listener’s affective state and the context of the listening experience.  Genre, title and artist were optionally ca p-\ntured. Previous analysis of our data set  shows that real -\nlife listening experiences are far from the homogenous data sets used in current models, and cover a wide range of genres, artists, and  songs [3]. In the present paper, we \nused our naturally -gathered data set to model musical \nmood using Bayesian networks and feature sets including \nmusical features (audio features, song lyrics, socially -\ncreated tags), the affective state of the listener, a nd listen-\ning context. Listening context included reason for liste n-\ning, activity, location, social company, level of choice \nover the song  and mental associations . \n    In this paper we make two main contributions. First, \nwe successfully model musical mood fr om a data set \ngathered in -situ during a user’s daily life; we are the first \nto do so . Second, w e show that while musical features \n(audio features, song lyrics and socially created tags) can successful ly model musical mood with classification a c-\ncuracies bet ter than chance, adding contextual info r-\nmation, such as the listener’s affective state or the liste n-\ning context of the musical experience , can further i m-\nprove classification accuracies. We successfully classify musical arousal with a classification accurac y of 67% and \nmusical valence with a n accuracy of 75% when using \nboth musical features and listening context.  \n2. RELATED WORK \n2.1 Affective State  \nIt is well documented that music can induce specific a f-\nfective experiences in the listener. Affective state, or the emotion or mood a person is experiencing, can be d e-\nscribed using either a categorical or dimensional a p-\nproach. The categorical approach breaks emotions into \n \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full ci tation on the first page.   \n© 2012  International Society for Music Information Retrieval    \n \ndiscrete labeled categories (e.g., happiness, fear, joy) [4]. \nIn contrast, the dimensional approach, which we use in \nthis paper, represents affective state using two orthogonal dimensions: arousal and valence [5]. Arousal can be d e-\nscribed as the energy or ac tivation of an emotion. Low \narousal corresponds to feeling sleepy or sluggish while high arousal corresponds to feeling frantic or excited. V a-\nlence describes how positive or negative an emotion is. Low valence corresponds to feeling negative, sad or me l-\nancholic and high valence to feeling positive, happy or joyful. Most categorical emotions can be described by \nArousal -Valence  (A-V) space (e.g., angry in Figure 1).  \n \nFigure 1 shows A -V space labeled with several of the \ncategorical em otions.  \n2.2 Musical Mood  \nMusical mood, the emotion expressed by a piece of m u-\nsic, is to some degree perceived consistently across di f-\nferent listeners and even different cultures. Studies by \nJuslin and Sloboda have shown that listeners of different \nmusical trai ning classify musical mood into the same ca t-\negories [6]. Fritz et al. fou nd that the Mafa natives of A f-\nrica – without any exposure to Western music – categ o-\nrized music into the same three basic emotional categ o-\nries as Westerners [7]. Musical mood is frequently mea s-\nured in arousal and valence [8] and we have used this a p-\nproach in this paper. It should be noted that the affective \nstate induced in the listener is not necessarily the same as the musical mood of the music [9], [10] . For example, an \nindividual who is herself feeling frustrated (i.e., mood of the listener) can still perceive a piece of music as calm \n(i.e., musical mood).  \n2.3 Musical Mood Classification  \nMusical mood can be manually categorized by the listen-\ner, but researchers have also algorithmically classified musical mood us ing audio features extracted from the \nmusical track. Work by Juslin [11] has identified seven \nmusical features that are important in the interpretation of \nmusical mood.  He asked performers to play the same musical scores in such a way as to express four different \nmusical moods (anger, sadness, happiness and fear) and \nthen had listeners rate the strength of each mood. He found that performers and listener s used the same fe a-\ntures to identify each mood, but weighted their i m-\nportance differently. These features are:  \n• Mode:  Mode refers to the key of the music. (e.g. A -) • Rhythm:  Rhythm is the pattern of strong and weak \nbeat. It can be described through speed (t empo), \nstrength, and regularity of the beat.  \n• Articulation: Articulation refers to the transition and \ncontinuity of the music. It ranges from legato (co n-\nnected notes) to staccato (short abrupt notes).  \n• Intensity / Loudness:  Intensity is a measure of chan g-\nes in volume.  \n• Timbre / Spectrum:  Timbre describes the quality of \nthe sound. It is often defined in terms of features of the spectrum gathered from the audio signal.  \n   Musical mood has previously been modeled using only audio features. Lu et al. classified classical music into the \nfour quadrants of A -V space using a collection of audio \nfeatures with an accuracy of 86.3% [1]. Their algorithm \nalso detected places within the song where the mood changed. Experts labeled musical mood . Feng et al. cla s-\nsified Western popular music into four moods using only two features: tempo and articulation. They achieved a precision of 67% and a recall of 66% [2]. They do not \nspecify how they gathered music al mood.  \n    Some effort has been made to incorporate other mus i-\ncal context with audio features to improve classification. Yang et al., working with a set of Western Rock music, made small gains in their classification rates by adding lyrics to the audio f eatures (from 80.7% to 82.8%) [12]. \nMusical mood was gathered in a laboratory study.       Bischoff et al. integra ted socially created tags with audio \nfeatures, and while their classification rates were low due to problems with their ground truth data, they achieved better results using tags and audio features than audio fe a-\ntures alone [13]. Their poor results may be due to the fact \nthey were using a diverse, online, data set with multiple \ngenres. Musical mood was specified in this data set by \nusers of the AllMusic site.    \n2.4 Music Recommenders  \nMany commercial music recommender systems exist \n(e.g., Last.fm, Pandora, Apple’s Genius, StereoMoods). \nIn 2010, Han et al. created COMUS, a context -based mu-\nsic recommender system that accounts for mood, situ a-\ntion and musical features [14]. Their sys tem was limited \nto recommending music for only one listening purpose – \nto transition between emotional states – and assumed a \nprior explicit knowledge about how a specific individual changes their music habits depending on situation.  \n3. EXPERIENCE SAMPLING SOFTWARE \nTo gather an in -situ data set of musical mood and liste n-\ning context, we surveyed participants  using an exper i-\nence- sample methodology  [15]. We created an applic a-\ntion that ran on Android 2.1 smartphones , which genera t-\ned custom surveys from XML files. Participants were asked to carry the phone with them at all times. While it \nwould be possible to create a plug- in for an existing co m-\nputer media p layer such as iTunes, we wanted to capture \nlistening experiences in all contexts. For example, some \nactivities, such as exercising, do not usually occur simu l-\ntaneously with computer use. Participants were not r e-\nquired to use the phone as a media player as this would \n  \n \nfurther limit listening contexts (e.g., music playing in the \nbackground at a restaurant). The tradeoff is that we could not automatically capture song title, artist, or audio fea-\ntures such as tempo.  \n    The program would query the user randomly (approx-\nimately hourly) by vibrating the phone. A participant could fill out a survey or dismiss the program by indica t-\ning they were too busy. Surveys were completed in less than five minutes and were filled out regardless of whet h-\ner participants were listening to music. This was done to \nencourage survey completion. Participants were paid per \nnumber of surveys completed, between 5 and 40 CAD. \nTo obtain the maximum payout, 112 surveys were r e-\nquired, which is roughly 8 surveys per day.  A progress \nbar in the software provided feedback about how many \nsurveys had been completed.  \n    Four types of information were collected: musical mood, affective state, musical context and listening co n-\ntext. See Figure 2 for screenshots of the experience -\nsampling appli cation.  \n     Musical Mood:  Participants were asked to describe \nthe musical mood of the song they were listening to using two five -point differential scales. They were asked to rate \nthe arousal of the music by selecting one of five radio buttons between low  arousal and high arousal. Similarly, \nthey rated the valence of the music on a scale between sad and happy.  Definitions were given to participants b e-\nfore the study and available from a help menu.  \n    Affective State: Participants were asked to describe \ntheir personal arousal and valence using five -point diffe r-\nential scales similar to musical mood.  \n     Artist, Title and Genre: Artist and title could optional-\nly be entered in free -text fields that autocompleted to pr e-\nviously entered answers. A genre field was provided that autocompleted to a list of common genres taken from \nWikipedia, but also allowed participants to enter their \nown genre.  \n    Listening Context: Participants were asked questions \ndescribing their current listening context. Participants s e-\nlected their current activity from a list (waking up, bat h-\ning, exercising, working, doing homework, relaxing, eat-ing, socializing, romantic activities, reading, going to sleep, driving, travelling as a passenger, shopping, dan c-ing, getting drunk, other). These activities were taken \nfrom [8], which lists the most common activities to occur \nin conjunction with music. Participants also selected their location (home , work, public place, other) and social \ncompany (by myself, with people I know, with people I do not know). Participants selected their reason for listen-ing (to express or release emotion, to influence my em o-\ntion, to relax, for enjoyment, as background sound, other) as well as whether or not they choose the song (yes, yes as part of a playlist, no). A text field was provided for participants to enter any terms or phrases they associated with the song.  \n4. DATA SET GATHERED IN -SITU  \nTwenty participants, (14 male ) with an average age of 25, \nused the experience- sampling software for two weeks.  \n    For a full description of the data set, see [3]. Here we \nsummarize for the purposes of guiding the development \nof our musical mood classifiers. In total 1803 surveys \nwere filled out ; 610 of those surveys were completed \nwhen the participant was listening to music. Only the r e-\nsults of the music surveys are included in this pape r. \n    Participants had an average arousal of 2.28 (SD=0.92) on our 5 -pt scale (0 low, 2 neutral, 4 high) and average \nvalence of 2.64 (SD=0.90). The music they were listening to had an average arousal of 2.64 (SD=1.05) and average valence of 2.66 (SD=1.14) . \n     The most common activities while listening to music were working (37%)  and relaxing  (21%) . Users  also lis-\ntened to music while eating (6%), driv ing (5%), travelling \n(as a passenger)(5%), other (5%), and socializing (4%).  \nParticipants were by themselves 57% of the time, with people they knew 37% and with people they did not \nknow 6%. They were at work 39% of the time, at home \n38%, in a public place 21% and in other locations 2%.  \n     The most common reason for listening was to use the music as backgr ound sound (46%) or enjoyment (25%). \nParticipants chose the song 74% of the time; 50% of the time it was as part of a playlist.   \n     Participants entered 102 unique song genres a total of 486 times.  Genres were coded into their parent genre  and \nthe most common genres were pop (28%), rock (23%), \nFigure 2 shows screenshots of the experience -sampling software. Participants answered a short survey about their \naffective state, listening c ontext and the music they were listening too.     \n \nelectronic (14%), jazz (7%), hip -hop & rap (6%), other \n(5%), modern folk (4%) and country (3%). The remai n-\ning genres were  classical, traditional/indigenous music, \nsoundtrack, blues, easy listening and R&B.  \n       Participants entered musical associations  for 335 \nsongs. These were then coded into themes, from a list \npartially taken from [8]. Participants mostly described \nemotion s (45%),  lyrics or instruments (20%), i magery \n(15%), or specific people , location s or memor ies (7%). \n    Songs were not limited to Western genres or even the \nEnglish language. At least 14% of the songs with artist \nand title specified were non -English; however, all partici-\npants listened to at least some English music.  \n5. CLASSIFICATION  FEATURES \nTo create classifiers of musical mood, we included musi-\ncal features used in previous work, but also added co n-\ntext-based features  from our data set . \n5.1 Musical Features  \nSongs were downloaded from iTunes and other sources where possible using the arti st and title specified.  \n5.1.1 Audio Features  \nAudio features describing the mode, rhythm, articulation, \nand timbre of the music were extracted using MIRtoolbox  \n[16] and Matlab.   \n     Mode: These features included the most probable key \nof the music as well  as an estimation of whether the key \nwas major or minor.  \n    Rhythm : These features included an estimation of \ntempo (number of beats per minute ) and pulse clarity \n(relative strength of the beat, related to how easily a li s-\ntener can perceive the tempo [17]). \n    Articulation : These features included the attack slope, \n(an indicator of how aggressively a note is played) as well as the Average Silence Ratio (ASR)[2]. \n     Timbre:  These features were taken from the audio \nspectrum and include b rightness (amount of energy above \na cutoff point in the spectrum), rolloff  (the frequency \nsuch that 85% o f total energy is contained below that fr e-\nquency),  spectral flux (average distance between the \nspectrum of successive frames), spectral c entroid  (the \nfrequency around which the spectrum is centered ), \nMFCC (description of the sound separated into different bands), low energy  (percentage of frames with less than \naverage energy), and average sensory roughness . Sensory \nroughness corresponds to when several sounds of nearly the same frequency are heard, causing a “beating” phe-\nnomenon. High roughness corresponds to harsher music \nwith more “beating” oscillations.  \n5.1.2 Lyrics  \nLyrics were downloaded from various sources  using the \nartist and title. Some included mark -ups indicating non - \nword sounds or names of singers responsible for a section \nof lyrics . Only English l yrics were collected. Songs that \nwere mainly English but included a few fo reign words \nwere included. Some songs contained not ations indicat-\ning that a section was repeated (e.g., “x2”). These were manually removed and replaced with the r epeated text. \nLyrics were analyzed using  the Linguistic Inquiry Word \nCount Tool ( LIWC ) [18], a textual analysis tool that pr o-\nvides a word count in 80 categories and the output of LIWC was used as the feature set.  \n5.1.3 Tags  \nSocially created tags from the website Last.fm were downloaded and analyzed using  LIWC. This output was \nused as features.  \n5.2 Affective Features  \nThis included the personal arousal and valence of the li s-\ntener on a 5- point scale. \n5.3 Listening Context  \nListening context included: reason for listening, activity, \nlocation, social company, and level of choice over the \nsong . The associations were categorized (see section 4) \nand this category was included as a feature. Associat ions \nwere also analyzed using  LIWC . \n6. MODEL RESULTS  \n6.1 Feature Sets  \nWe used a number of feature combinations in creating \nour models , which can be summarized as three feature \nsets.  \n    Musical Features:  Our first feature set used audio fea-\ntures, lyrical features, and tag features, as these fe atures \nwere used in previous models based on laboratory -\ngathered data sets of a single genre . There were 198 di f-\nferent features in this set.  \n    Musical Features + Affective Features : Our second \nfeature set used all  the musical features but added perso n-\nal arousal and valence for a total of 200 different features.  \n    Musical Features + Listening Context : Our third fe a-\nture set combined musical features with the listening co n-\ntext collected in our study  for a total of 296 features.  \n6.2 Models of Musical Mood  \nDue to “in the wild” nature of the study, musical arousal and musical valence had an uneven distribution of re-sponses. Participants were much more likely to indicate that they were listening to songs with high arousal a nd \nhigh valence. To prevent over fitting of the model due to class skew, musical arousal and musical valence were binned into two levels, low, and high. Neutral instances  \nwere ignored.  Since only songs with song titles could be \ndownloaded and audio feature s extracted, instances wit h-\nout a song title were also ignored. Also, undersampling, a \ntechnique that selects a random number of instances to obtain an equal distribution, was used. This lowered the \ntotal number of instances from 610 to 122 when mode l-\ning mu sical arousal and 156 when modeli n\n g musical v a-\nlence. To avoid any effects caused by the specific set of random instances chosen, this process was completed five times, and the average accuracies of all runs are reported.  \n     All models were  created in Weka [19] using Bayes \nNet classifiers, Markov Estimation and tenfold cross va l-\nidation. We modeled musical arousal and musical valence   \n \nseparately, using each feature set. See Figure 3 for class i-\nfication accuracies.  \n    Musical Features:  Using only musical features ( audio \nfeatures, lyrics and tags), m usical arousal has a classific a-\ntion accuracy of 59.5% (SD= 3.1, kappa= 0.1984). Musical \nvalence has a n accuracy of 53.0% (SD= 6.8, ka p-\npa=0.0604).  While both models are higher than chance  \n(50%), a one sample t -test shows that only musical arou s-\nal (t4=6.74, p<0.0 1) was significantly higher . However, \nthe results are lower than previously reported classifica-\ntion accuracies of homogenous lab- based data sets.     \n    Musical Features  + Affective State:  When we co m-\nbined affective features  (personal arousal and valence) \nwith musical features, musical arousal has a classification \naccuracy of 60.3%  (SD= 4.6, kappa= 0.2121). Musical v a-\nlence has a n accuracy of 60.2% (SD= 4.6, kappa= 0.2074).  \nBoth musical arousal  (t4=4.99 p<0.01) and musical va-\nlence  (t4=4.70, p<0.01)  performed significantly better \nthan chance ( 50%) , and w e achieved improved classific a-\ntion accuracies of musical arousal and musical valence by \nusing a combination of affective and musical features . \n    Musical Features  + Listening Context : When we \ncombined musical features with listening context  fea-\ntures , musical arousal has a classification accuracy of \n67.4%  (SD= 1.7, kappa= 0.3437). Musical valence has a n  \naccuracy of  75.7%  (SD= 1.5, kappa= 0.5133).  Both  musi-\ncal arousal (t 4=23.54, p <0.0001) and musical valence \n(t4=38.75, p<0.0001) performed significantly better than \nchance ( 50%) , and we achieved gains in classification \naccuracy in both models over using only musical features \nor musi cal features and affective features combined. \n \nFigure 3 shows the classification accuracies for each \nfeature set. The dotted line shows chance (50%).  \n7.  DISCUSSION \nOur experience sampling study collected in -situ data  that \nreflects real -life listening experiences. Unlike previous \nmodels, our data included multiple genres and different \nlistening contexts.  \n     We have shown that musical mood can be successfully \nmodeled from in-situ data, although with a lower classif i-\ncation accuracy than previous attempts. Adding affective \nstate to the model resulted in an improvement in classif i-\ncation accuracy while modeling musical valence; adding \nlistening context to the model resulted in improvements in both musical arousal and musi cal valence.  Our results \nshow  that listening context is an important aspect of \nmodeling musical mood , when using real -life data .  7.1 Importance of Context  \nIt may be possible that context is important when mode l-\ning musical mood because participants rate musical mood differently depending on their context . For example, a \nuser may rate the same song differently depending on whether they are working alone or cooking with friends. We cannot confirm this with our data set, as one would \nneed the same songs played in  a variety of listening co n-\ntexts – in our study, songs and artists were only encou n-\ntered once on average.   \n    It is also possible that people listen to music with cer-\ntain musical moods based on their context. For example, \na user  may generally  choose to li sten to music with high \narousal when exercising  and low arousal when eating \ndinner.  In that case our model predict s the type of musical \nmood listeners want to listen to , based on context , which \nis useful for automatically generating playlists .  \n      Simil arly, participants may rate musical mood diffe r-\nently depending on their affective state. This is a tricky relationship to investigate as the music itself has a hand \nin inducing an affective state  in a listener. Any correl a-\ntion found between musical mood an d affective state does \nnot show directionality of the relationship.  \n       To examine the relationship s between listening con-\ntext, musical mood, and affective state, we could provide \nusers  with representative samples in a music library.  By \nlistening to (a nd rating) the same song in a variety of con-\ntexts and affective states, the relationship between these three factors might be made clear.   \n7.2 Limitations  \nThere are several limitations with our study. The first is that participants are unlikely to answer a sur vey during \nsome activities (e.g., driving ). Second , all categories in \nour data may not be mutually exclusive (e.g., reading while running on the treadmill) . Third, the number of pa r-\nticipants and length of the study may have been too small to collect a full y representative sample of listening con-\ntext. Finally, previous studies have assumed that people \nlisten to music with four emotional categories (happy, sad, fear, anger) [11]; however, in our study we found \nthat people tended to listen to happy music. The other \nthree emotions may not be equally represented when ca p-\nturing in -situ data  [3]. \n    W hile a cla s\nsification accuracy of 75% is much im-\nproved over  a random classifier , or one based on auditory \nfeatures, a music recommender suggesting songs with the \nwrong mood a quarter of the time may result in a negative \nuser experience. This can be circumvented in a few ways. First a music recommender can select tracks from a per-sonal music library ; users are more likely to enjoy their \nown music even if the recommendation is off. Second, a playlist rather than a single song could be recommended \nso that a majority of the music recommended is suitable. Third, combining this model with existing recommend a-\ntion systems that use clustering of similar genres and art-ists could further improve existing prediction rates.  Final-\nly, we could improve the classification rates and avoid \npossible overfitting  caused by the small number of in-\nstances  in our models by collecting a more comprehen-\nsive data set.  \n  \n \n8. FUTURE WORK  \nBased on the results of this work,  we will create  a con-\ntext-aware music recommender system. This system will \ntake in the context of the listening experience and use this  \ncontext to compile a playlist. Based on our models, the \nsystem will recommend a musical mood  listeners are \nlikely to enjoy , and will create playlist s of songs with this \nspecific musical mood , (based on a data set  labeled from \nmusical features ). The system  could also make sugges-\ntions of songs for purchase the user might enjoy. We will \nevaluate the predictions through a user study, conducted in-situ, to preserve the importance of context.  \n    To create the underlying model for this music reco m-\nmender, a larg er in -situ data set will be collected. The \nstudy will run for a longer time period (i.e. , month s) with \na larger pool of participants. Participants will receive b o-\nnuses for filling out genre, title and artist and will be asked to provide a copy of their mus ic library at the end \nof the study for audio feature processing.  This larger, \nmore comprehensive data set  will help improve classifi-\ncation accuracies.  \n9. CONCLUSIONS  \nWe successfully model musical mood from a data set gathered in -situ during a user’s daily li fe. We show that \nmusical features (audio features, song lyrics and socially created tags) can successful model musical mood with classification accuracies better than chance.  We succes s-\nfully classify musical arousal with a classification accur a-\ncy of 59% an d musical valence with an accuracy of 53% \nwhen using only musical features  on an in- situ data set.  \n    Adding contextual information, such as the listener’s affective state or the listening context of the musical e x-\nperience can further improve classification accuracies. We successfully classify musical arousal and musical va-\nlence with a classification accuracy of 60% when using \nboth musical features and affective state. We classify m u-\nsical arousal with a classification accuracy of 67% and musical valence wi th an accuracy of 75% when using \nboth musical features and listening context. \n10. ACKNOWLEDGEMENTS  \nThis work was funded in part by the GRAND NCE. \n11. REFERENCES  \n[1]  L. Lu, D. Liu, and H. J. Zhang, “Automatic mood \ndetection and tracking of music audio signals,” Au-\ndio, Speech, and Language Processing, IEEE Tran s-\nactions on , vol. 14, no. 1, pp. 5 – 18, Jan. 2006.  \n[2]  Y. Feng, Y. Zhuang, and Y. Pan, “Popular music \nretrieval by detecting mood,”  in Proceedings of the \n26th annual international ACM SIGIR conference on Research and development in informa tion retrieval , \nNew York, NY, USA, 2003, pp. 375– 376. \n[3] D. Watson and R.  L. Mandr yk, \"An In -Situ Study of  \nReal-Life Listening Context ,\" in SMC 2012 . \n[4]  P. Ekman, Basic Emotion . John Wiley & Sons, Ltd., \n2005.  [5]  J. A. Russell, “Core affect and the psychological \nconstruction of emotion ,” Psychological Review , \nvol. 110, no. 1, pp. 145– 172+, 2003.  \n[6]  P. Juslin and J. Sloboda, Music and Emotion: The o-\nry and Research . Oxford University Press, 2001.  \n[7]  T. Fritz, S. Jentschke, N. Gosselin, D. Sammler, I. \nPeretz, R. Turner, A. D. Friederici, and S. Koelsch, “Universal Recognition of Three Basic Emotions in Music,” Current Biology , vol. 19, no. 7, pp. 573 – \n576, 2009.  \n[8]  P. Juslin and P. Laukka, “Expression, Perception, \nand Induction of Musical Emotions: A Review and a Questionnaire Study of Everyday Listening - Journal \nof New Music Research,” Journal Of New Music \nResearch , vol. 33, no. 3, pp. 217– 238, 2004.  \n[9]  K. . Scherer and M. R. Zentner, “Emotional Effects \nOf Music: Production Rules,” in Music and Emo-\ntion:Theory and Research , New York, NY, USA: \nOxford University Press, 2001, pp. 361 –392. \n[10]  M. R. Zentner, S. Meylan, and K. Scherer, “Explo r-\ning ‘musical emotions’ across five genres of music.,” in ICMPC, Keeyle, UK, 2000.  \n[11]  P. N. Juslin, “Cue utilization in communication of \nemotion in music performance: Relating perfo r-\nmance to perception.,” Journal of Experimental Psy-\nchology: Human Perception and Performance , vol. \n26, no. 6, pp. 1797– 1812, 2000.  \n[12]  D. Yang and W. Lee, “Disambiguating Music Em o-\ntion Using Software Agents,” in ISMIR Barcelona  \n[13]  K. Bischoff, C. Firan, R. Paiu, W. Nejdl, C. Laurier, \nand M. Sordo, “Mus ic Mood and T heme Classifica-\ntion-a Hybrid Approach,” in ISMIR , Kobe, Japan, \n2009.  \n[14]  B.-J. Han, S. Rho, S. Jun, and E. Hwang, “Music \nemotion classification and context -based music re c-\nommendation,” Multimedia Tools Appl. , vol. 47, pp. \n433– 460, May 2010.  \n[15]  R. Larson and M. Csikszentmihalyi, “The Experi-\nence Sampling Method.,” New Directions for Met h-\nodology of Social & Behavioral Science , vol. 15, pp. \n41–56, 1983.  \n[16]  O. Lartillot and P. Toiviainen, “A Matlab Toolbox \nfor Musical Fea ture Extraction from  Audio,” in Pro-\nceedings of the 10th International Conference on Digital Audio Effects , Bordeaux, France, 2007. \n[17]  O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari, \n“Multi -Feature Modeling of Pulse Clarity: Design \nValidation and Optimization,” pres ented at the \nISMIR 2008, 2008.  \n[18]  J. W. Pennebaker, M. E. Francis, and R. J. Booth, \n“Linguistic Inquiry and Word Count LIWC 2001,” Word Journal  \nOf The International Linguistic Ass o-\nciation , pp. 1 –21, 2001.  \n[19]  M. Hall, E. Frank, G. Holmes, B. Pfahringe r, P. \nReutemann, and I. H. Witten, “The WEKA Data Mining Software: An Update,” SIGKDD Explora-\ntions , vol. 11, no. 1, 2009."
    },
    {
        "title": "The Role Of Music in the Lives of Homeless Young People: A Preliminary Report.",
        "author": [
            "Jill Palzkill Woelfer",
            "Jin Ha Lee 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415864",
        "url": "https://doi.org/10.5281/zenodo.1415864",
        "ee": "https://zenodo.org/records/1415864/files/WoelferL12.pdf",
        "abstract": "This paper is a preliminary report of findings in an on- going study of the role of music in the lives of homeless young people which is taking place in Vancouver, British Columbia and Seattle, WA. One hundred homeless young people in Vancouver took part in online surveys, 20 of these young people participated in interviews and 64 completed design activities. Surveys included demo- graphic and music questions. Interviews consisted of questions about music listening and preferences. In the design activities, participants envisioned a music device and provided a drawing and a scenario. Since the study is on-going, findings are limited to descriptive analysis of survey data supplemented with interview data. These findings provide initial insights into music listening be- haviors, social aspects of shared music interests, and pre- ferred music genres, bands and artists, and moods.",
        "zenodo_id": 1415864,
        "dblp_key": "conf/ismir/WoelferL12",
        "keywords": [
            "Homelessness",
            "Young People",
            "Music Listening Behaviors",
            "Social Aspects of Music",
            "Music Preferences",
            "Music Genres",
            "Music Devices",
            "Online Surveys",
            "Interviews",
            "Shared Music Interests"
        ],
        "content": "THE ROLE OF MUSIC IN THE LIVES OF HOMELESS \nYOUNG PEOPLE: A PR\nELIMINARY REPORT \nJill Palzkill Woelfer Jin Ha Lee \nThe Inform\nation School \nUniversity of Washington \nSeattle, WA 98195 \nwoelfj@uw.edu The Information School\n \nUniversity of Washington \nSeattle, WA 98195 \njinhalee@uw.edu \nABSTRACT \nThis paper i\ns a preliminary report of findings in an on-\ngoing study of the role of music in the lives of homeless \nyoung people which is taking place in Vancouver, British \nColumbia and Seattle, WA. One hundred homeless young \npeople in Vancouver took part in online surveys, 20 of \nthese young people participated in interviews and 64 \ncompleted design activities. Surveys included demo-\ngraphic and music questions. Interviews consisted of \nquestions about music listening and preferences. In the \ndesign activities, participants envisioned a music device \nand provided a drawing and a scenario. Since the study is \non-going, findings are limited to descriptive analysis of \nsurvey data supplemented with interview data. These \nfindings provide initial insights into music listening be-\nhaviors, social aspects of shared music interests, and pre-\nferred music genres, bands and artists, and moods. \n1. INTRODUCTION \nHomelessness is a pressing problem with lasting social \nand economic consequences. Experts estimate that in a \ngiven year, 3 million young people age 12-24 experience \nhomelessness in the U.S and 50,000-60,000 experience \nhomelessness in Canada [11,13]. The number of homeless \nyoung people and interest in their welfare has prompted \nresearch into their characteristics and circumstances. This \nextensive research with homeless young people has found \nthat these young people are a heterogeneous group rang-\ning from youth to young adulthood, with varying experi-\nences of abuse and neglect [11,13]. Indeed, much is \nknown about the psychological and social (psycho-social) \naspects of homeless young people, but far less is known \nabout their everyday lives, including interests in music, \nand associated experiences with technologies. \nSubsequently, since 2007, the first author has investi-\ngated the experiences that homeless young people, aged \nup to 30, have with technologies, including music players \n[15,16,17]. One finding arising from this work is that \nhomeless young people have a keen interest in music and use digital means to find and listen to music and share \nmusic with others. The current study builds on this prior \nwork by taking a general and exploratory stance, asking: \nWhat role does music play in the lives of homeless young \npeople? In response, this paper presents preliminary find-\nings from an on-going study in Vancouver, British Co-\nlumbia and Seattle, Washington, reporting on data col-\nlected in Vancouver in February and March 2012. \n2. LITERATURE REVIEW \nConsideration of the role that music might play in the \nlives of young people began in antiquity. Writing in \nB.C.E. 350, Aristotle proposed that music “might have \nsome influence over the character and the soul” and \nshould therefore “be introduced into the education of the \nyoung” [2]. In the 20th century, the music interests of \nyoung people living at home have been studied extensive-\nly in the psycho-social literature. For example, over 100 \nstudies since the 1970s have focused on possible associa-\ntions between preferences for particular genres of music \nor types of music-related media use and risk-taking be-\nhaviors, such as drug and alcohol use, high-risk sexual \nactivity, and so on [1]. Despite the high rates of risk-\ntaking behaviors among homeless young people [11,13] \nand the general lack of knowledge regarding homeless \nyoung people’s interests in and behaviors related to mu-\nsic, after extensive searching only a single study in the \npsycho-social literature was found that had investigated \nmusic and homeless young people [7].  \nIn a similar fashion, studies in the domain of music in-\nformation retrieval have investigated the music listening \nand sharing behaviors of young people living at home. \nWilliams [14] examined issues relating to popular music \naudiences by conducting unstructured small group discus-\nsions with teenagers in England. His subjects stated that \nmusic was important in their lives, but “interestingly, they \nframed its significance in terms of its practical use (of \nmusic) in their daily routines,” rather than identification \nor self-construction [14]. Laplante and Downie published \na series of studies examining music related behaviors of \nyoung adults in Montreal, Canada, specifically on music \nseeking in everyday life [9], relevance judgments [8], and \noutcomes of music seeking [10]. In one study, partici-\npants reported that informal channels such as friends, col-\nleagues or relatives played a significant role in obtaining  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fe e provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nmusic inform\nation [9]. In another study, participant’s rel-\nevance judgments were based on a combination of differ-\nent criteria, some pertaining to the music itself, but also \nexternal factors such as use, disposition, or personal \nknowledge [8]. Finally, further analysis revealed that par-\nticipant’s satisfaction with music depended on both he-\ndonic (i.e., experiencing pleasure) as well as utilitarian \noutcomes [10]. Carlisle [5] conducted in-depth online in-\nterviews with five young Australians, aged 18-22, and \nfound that each young person wanted digital music for \nmarkedly different reasons and had high personal stakes \nin their musical perspective. Taken together, these studies \nprovide important insights regarding the needs, uses, and \nmusic seeking behaviors of young adults in various re-\ngions. However, as in the psycho-social literature, search-\ning did not reveal studies with homeless young people. \nSubsequently, the current study aims to increase \nknowledge by providing empirical data on the role of mu-\nsic in the lives of homeless young people.     \n3. STUDY DESIGN \nWhen the current study is complete, all procedures will \nhave been carried out with equal numbers of participants \nin Vancouver, BC and Seattle, WA. This brief, prelimi-\nnary report only includes data from homeless young peo-\nple in Vancouver (collected in February and March 2012) \nsince data collection in Seattle is planned to begin in June \n2012. The study uses convenience sampling and is taking \nplace in collaboration with two youth service agencies, \none in each city, that provide assistance and shelter for \nhomeless young people. Data in the study is anonymous \nand names of participants are not collected. \nIn keeping with the exploratory nature of the goals of \nthe research, the study is a broadly conceptualized, mixed \nmethods design. The procedures with homeless young \npeople include three components: (1) an online survey \nwith up to 100 homeless young people in each city; (2) \nindividual, semi-structured interviews with up to 20 \nhomeless young people in each city; and (3) a self-\ndirected, individual design activity with up to 100 home-\nless young people in each city. Following approved hu-\nman subjects protocols, young people were recruited by \nstaff at a homeless youth service agency while attending \nagency programs. The first author obtained verbal consent \nfrom all participants, then introduced and conducted or \nmoderated the surveys, interviews and design activities. \nThe procedures took place sequentially so that a young \nperson first engaged in the survey and was then invited to \ntake part in an interview - until 20 interviews were com-\npleted. Finally, all young people who took part in the sur-\nveys, whether or not they had been interviewed, were in-\nvited to complete the design activity. Homeless young \npeople were compensated with gift cards, from $5-20 de-\npending on the number of procedures. In sum, 100 home-\nless young people in Vancouver, BC completed surveys, 20 of these people also completed interviews and 64 par-\nticipated in the design activity. \n4. FINDINGS AND DISCUSSION \n4.1 Overview \nThe data analyzed in this paper include self-reported sur-\nvey responses from 100 homeless young people to ten key \ndemographic and music questions (Table 1) with detail \nfrom four additional survey questions. Homeless young \npeople completed surveys on one of three laptop comput-\ners. Time needed to complete the surveys ranged from 15-\n65 minutes (M=35 minutes). Importantly, in order to min-\nimize stress due to the personal nature of the questions, \nresponses to survey questions other than age, gender, \nrace, and sexual orientation were voluntary, resulting in \nsome no responses. Survey participants were 16-24 years \nold (Mdn=22) and predominately male (63%). Partici-\npants identified as heterosexual (68%), bisexual (21%), \nhomosexual (6%), queer (4%), and unsure (1%). Most \nparticipants reported their race as White (38%), Aborigi-\nnal (27%), or Mixed Race (23%). Half of the participants \n(50%) had not completed high school (Table 1).  \nAdditionally, regarding homelessness, participants re-\nported that they first became homeless between 2 and 23 \nyears of age (Mdn=16). Participants also reported that \nthey had experienced a total between 0 and 132 months \n(11 years) of homelessness during their lifetimes. The “0 \nyears, 0 months” responses may indicate that, despite at-\ntending programs at an agency that provides services ex-\nclusively for homeless young people, some of the partici-\npants did not consider themselves to be homeless or they \nhad very recently become homeless. However, these 15 \nresponses should be interpreted cautiously since “0 years, \n0 months” was the default answer. Thus, these responses \nmay also indicate skipped questions. If these responses \nare eliminated, the number of months of homelessness \nreported by the remaining 85 respondents was 1 to 132 \nmonths during their lifetimes (M=24 months). \nRegarding music, 97 participants reported that they lis-\ntened to music on a daily basis and for many reasons with \nthe most frequent responses being related to emotional \nwelfare. Notably, for these questions, there were three “I \ndon’t listen to music” responses from two participants. \nThe first participant gave contradicting responses, indicat-\ning that she listened to music “2-4 hours a day” on one \nquestion and “I don’t listen to music” on the other. This \nmay indicate some change in her music listening behav-\nior, although this was not confirmed. The second partici-\npant, who took part in an interview, was a guitarist and \nsinger who performed on the street to make money. Ex-\nplaining her responses, she stated, “I don’t listen to music. \nI play music.” \nApproximately one-third of the young people indicated \nthat they listened to the same music as their parents, and   \n \nhalf indicat\ned other family members. Over half of the \nyoung people indicated that they listened to the same mu-\nsic as friends from home or the streets. Indeed, 17 partici-\npants also indicated that they made decisions about estab-\nlishing friendships based on music preferences. More than \nhalf of the participants listened most often to hip hop, \nrock, and rhythm and blues (R&B), although techno and \nmetal were also listened to by nearly half of the partici-\npants.  \n4.2 Discussion \nThe data presented so far indicate that music does play a \nrole in the lives of homeless young people. In order to \nelaborate this role, a discussion follows regarding: (1) \nmusic-listening behaviors; (2) social aspects of shared \nmusic interests; and (3) genres, bands/artists and moods. \nThe discussion is supplemented with evidence drawn \nfrom 20 interviews with homeless young people. Since \nnames of participants were not collected in order to pre-\nserve participant’s privacy, pseudonyms are used to iden-\ntify participants as needed. \nParticipants in the interviews were 14 young men and 6 \nyoung women, aged 18-24 (Mdn=21). Consistent with \nprior work [16], interview times ranged widely, from 9-70 \nminutes (M=45). However, despite homeless young peo-\nple’s general distrust of adults and strangers, with one ex-\nception the participants seemed at ease, speaking willing-\nly and at length. Of the 20 interview participants, 13 peo-\nple had personal music players, such as MP3 players and \nmobile phones, and 15 people reported that they had mu-\nsic collections ranging from “11-50” to “over 10,000” \nsongs (Mdn=101-500 songs). The size of these song col-\nlections may seem surprising. However, in the interviews, \nsome participants stated that since they did not have mon-\ney to pay for music, they “stole music” and used a variety \nof “pirate” websites to download songs.  \nThe interviews with homeless young people included \nquestions regarding music listening and preferences, as \nwell as questions related to the importance and influence \nof music. The interviews began with an activity where \nyoung people wrote responses on a 24-hour timeline. \nWhen writing on the timelines, participants were asked to \nindicate what music they listened to, where they were, \nwho they were with, what they were doing, and so on.  \nAfter the timeline was filled in, young people rated the \nimportance of music (from very low to very high) and the \ninfluence of music (from very negative to very positive) \non 5-point Likert scales. The importance of music was \nrated from low to very high with an average between high \nand very high (4.125 out of 5). The influence of music \nwas rated from very negative to very positive with an av-\nerage between neutral and positive (3.75 out of 5). As \npart of this rating, young people told stories about a time \nwhen music was important and a time when music had a \npositive or negative influence. Finally, young people \nshared at least one favorite song using a speaker to play songs from music players, or playing videos on YouTube, \nor singing songs that they had composed or memorized. \n4.2.1 Music-listening Behaviors \nIn the surveys and interviews, homeless young people re-\nported that they listened to music on a daily basis, more DEMOGRAPHICS  W HY DO YOU LISTEN  \n n TO MUSIC?* n \nAge \n  16-18 \n  19-21 \n  22-24   \nGender \n  Male \n  Female \n  Transgender \nEthnicity   \n  White \n  Aboriginal \n  Mixed Race \n  Black \n  Asian \n  Arab \n  Latino \n  Other \nDegree \n  None \n  High school \n  Trade school \n  2 yr  college \n  4 yr college \n  No response \nCurrent job? \n  Yes \n  No \n  No res\nponse  \n9 \n35 \n56 \n \n63 \n36 \n1 \n \n38 \n27 \n23 \n4 \n3 \n1 \n1 \n3 \n \n50 \n38 \n6 \n3 \n0 \n3 \n \n26 \n73 \n1      Calm down or relieve tension  \n    Help get through difficult times  \n    Relieve boredom  \n    Get rid of negative feelings/anger  \n    Express feelings or emotions  \n    Be creative/use imagination  \n    Wake myself up  \n    Reduce loneliness  \n    Separate myself from society \n    Get better playing/writing music  \n    Create an image \n    Be trendy or cool  \n    Please my friends  \n    Please my parents  \n    No response \n    I don’t listen to music \n  ON AVERAGE, HOW MANY HOURS      \n A DAY DO YOU LISTEN TO MUSIC? \n    1 hour o\nr less  \n    2-4 hours \n    5-8 hours \n    9 hours or more  \n    No response  \n    I don’t listen to music \n  FRIENDS WITH SOMEONE BASED \n  ON THE MUSIC HE OR SHE LIKES? \n     Yes \n    No \n    No\n response 77 \n74 \n72 \n68 \n55 \n54 \n51 \n48 \n47 \n34 \n33 \n16 \n12 \n3 \n3 \n2 \n \n \n26 \n34 \n19 \n18 \n2 \n1 \n \n \n17 \n80 \n3 \n  WHAT KINDS OF MUSIC DO YOU LISTEN TO?*  \n    Hip hop  \n    Rock  \n    R&B   \n    Techno  \n    Metal  \n    Pop \n    Reggae  \n    Punk  \n  Jazz \n  Other (includes Dubstep, Country, Rap, Celtic, etc.) 70 \n65 \n56 \n49 \n47 \n42 \n38 \n35 \n30 \n44 \n  WHO LISTENS TO THE SAME MUSIC AS YOU DO?*  \n    Friends or people you know from home  \n    Friends or people you know from the streets  \n    Brothers, sisters, cousins, or other family members   \n    Boyfriend, girlfriend, or sex partner  \n    Friends or people you met online  \n    Parents (including foster family or step family)  \n    Staff at youth agencies  \n    Boss or employer at your job  \n  No response  61 \n52 \n50 \n44 \n33 \n31 \n13 \n10 \n7 \nTable 1.  Self-reported participant characteristics \n(n=100). *Multiple responses were allowed. \n   \n \noften for pr\nactical purposes than for identification or self-\nconstruction echoing the findings in [14]. For example, on \nthe one hand, over 70 of the 100 participants reported that \nthey listened to music for practical purposes, such as to \n“calm down or relieve tension,” “help get through diffi-\ncult times,” or “relieve boredom.” On the other hand, \nfewer than 50 participants indicated that they listened to \nmusic in order to “separate myself from society,” “create \nan image,” “be trendy or cool,” or “please my friends \n(parents),” issues related to identity and self-construction.  \nAs further evidence of the practical aspects of listening \nto music, the timelines from the interviews indicated that \nall 20 respondents listened to music while engaging in \nactivities such as waking up and going to sleep, hanging \nout with friends, and looking for work. For example, \nSheila was 22 and listened to music from the time she \nwoke up to the time she went to sleep. Sheila spoke of the \nimportance and practical aspects of music while also \nhighlighting its impact on her emotions, saying: \nI’ve been through a lot of f**ked up shit in my life and it’s \nnice…to hea r\n people’s opinions…how they dealt with things \nin music … to kind of relate ‘I’m not the only one’…. I just \nthink music is part of my life and I don’t think there would \never be a point…where I would say, ‘I don’t want to listen to \nmusic,’ ‘cause I either want to cry to it or I want to be happy \nto it or I want to dance around to it, but there’s always a \nsong for no matter what emotion you’re experiencing.  \nIn another example, Brian, who was 21, listened to \nmusic throug\nhout the day. However, he said he did not \nlisten to music while studying for college classes where \nhe had recently begun to learn how to read and write. Bri-\nan rated the influence of music as both very positive and \nvery negative. Speaking of the positive influence, he said \nthat he liked to listen to 1990s rock music such as Bob \nSeger. Brian said that this music “brings you back to the \npositive times growing up,” particularly playing games \nwith his brother and friends. More recently, he found that: \nWhen I’m doing my art I like to listen to MP3. So it kind of \ncalms me so  \nI’m into the artistic zone. Art is my hobby. I just \nrecently discovered that after I quit drinking. \nOn the other hand, he found that even some of his favorite \nsongs by the\n Canadian musician, Matthew Good, could \nhave a negative influence on his emotions: \nI have post traumatic stress disorder,… some of the music \nthat I liste\nn to kind of triggers me… and makes me feel down. \nCause some of the songs I do enjoy are really deep and real-\nly sad,… so I get kind of saddened a bit…, so I then actually \nchange the song to try to get on a happier page. \n4.2.2 Social Aspects of Shared Music Interests  \nFindings in\n the current study indicate that friends or fami-\nly are sources of music information [9], and that shared \ninterests play some role in social relationships. In survey \nand interview responses, homeless young people reported \nthat they shared music interests with friends from home as \nwell as friends from the street. For some interview partic-ipants, shared interests with friends led to the desire to \nattend music concerts, which can be difficult for homeless \nyoung people due to their economic circumstances. How-\never, Arthur, age 21, a Dubstep fan who played a favorite \nsong by Flux Pavilion [6], spoke enthusiastically about \nplans he had to go to an upcoming concert. Arthur and his \nfriend Drew, age 20, another Dubstep fan, also talked in \ntheir individual interviews about going together to raves, \ndance parties where electronic music is played. \nAlthough for most respondents (80%), music was not a \ndetermining factor in establishing friendships, 17% re-\nsponded positively on the survey when asked, “Do you \ndecide whether to be friends with someone based on the \nmusic he or she likes?” Indeed, for some participants, not \nhaving shared music interests could be potentially isolat-\ning. For instance, Matthew, age 20, enjoyed death metal \nmusic, and chose a favorite video by Behemoth to play \nduring his interview [4]. Matthew expressed surprise \nwhen the first author was not put off by the video and said \nhe rarely listened to his favorite music with other people \nbecause they did not share his taste in music. \nParticipants also reported shared music interests with \nfamily. For example, Sheila, introduced above, spoke \nabout choosing the music for her mother’s funeral: \nAnd I had a lot of good stuff like you know like the classic \n“Arms of an  \nAngel” kind of thing, but I also had songs like \nme and her used to listen to that were in it [the funeral], so \nthey [the songs] may not have been like funeral appropriate \nbut they were what we’d listen to.  \nIn a second example, Amanda, age 24, said that her \nadoptive mot\nher had introduced her to music and that she \nliked to sing because it made her mother happy. Amanda \nhad been homeless for nine years, and earned money by \nsinging and playing guitar on the street. A long-time hero-\nin user, Amanda recounted how she had recently “got \nclean with my music” when she had been invited to sing \none of her own songs at a concert. Amanda shared her \nperformance via a video, and said she had been clean for \n22 days at the time of the concert, adding: \nShe [Amanda’s mother] was sitting right there [in the audi-\nence] and s h\ne was crying. Everyone was crying. It was a big \ndeal that I was clean. \n4.2.3 Music Genre, Bands/Artists and Mood \nHomeless yo\nung people reported their music preferences \nvia two different survey questions. One question asked \nwhat kinds of music were listened to the most (Table 1). \nHip hop emerged as the most preferred category followed \nby Rock and R&B. However, there is precedence in the \nliterature that music preferences may be gender specific \n[3]. When broken down by gender, the top three catego-\nries preferred by young men were Hip hop, Rock, and \nTechno, while the top three categories for young women \nwere R&B, Rock, and Hip hop. These differences are po-\ntentially noteworthy given that associations between Hip   \n \nhop (i.e., R\nap) music and risk-taking, and the emotional \nuse of music, such as R&B, have been investigated [1].  \nHowever, while genre has been used traditionally in \nstudies as a way of gauging young people’s music prefer-\nences, genre can only go so far in typifying music prefer-\nences. Subsequently, in a second question participants \nnamed their three favorite bands/artists, resulting in 192 \nunique responses (out of a possible 300). Table 2 lists the \ntop 17 responses, chosen by three or more participants, \nand includes mainstream artists, such as Eminem, as well \nas “underground” bands such as Insane Clown Posse. \n \nBand or Artist Style [12] Total M F \n Eminem  Hardcore Rap 11 7 4 \n 2Pac  Gangsta Rap 9 6 3 \n Marianas Trench   Punk-Pop 5 2 3 \n Wiz Khalifa  East Coast Rap 4 3 1 \n Lady Gaga  Pop 4 2 2 \n Dr. Dre  Gangsta Rap 3 3 0 \n AC/DC  Hard Rock 3 2 1 \n The Notorious B.I.G.  Gangsta Rap 3 2 1 \n Insane Clown Posse  Rap-Metal 3 2 1 \n Iron Maiden  Heavy Metal 3 2 1 \n Korn  Heavy Metal 3 2 1 \n Skrillex  Dubstep 3 2 1 \n Lil Wayne  Southern Rap 3 1 2 \n Nicki Minaj  Hardcore Rap 3 1 2 \n Nickleback  Heavy Metal 3 1 2 \n Adele  Pop/Rock 3 0 3 \n Deadmau5  Club/Dance 3 0 3 \nTable 2. Favorite bands and musical artists, by \nmale (M) and female (F).  \nGiven that homeless young people reported that listen-\ning to music can have an impact on emotional well-being, \nmoods associated with bands/artists were analyzed. Using \nallmusic.com, 176 unique mood labels were found for \n155 of the 192 bands/artists [12]. Of the top 20 moods \n(Table 3), aggressive  and confrontational  appear to have \nstrong negative valence, while energetic, confident, rous-\ning, brash, fun, playful, freewheeling , intense, par-\nty/celebratory  and boisterous  appear to indicate high lev-\nels of energy or intensity. While no claims can be made \nabout effects of these moods on homeless young people, \nit appears that the moods in music may be related to rea-\nsons for listening to music (Table 1). Recall that Sheila \nnoted how listening to songs with themes related to diffi-\ncult life experiences could be cathartic; reducing her feel-\nings of isolation and that Brian found that certain songs \ncould trigger his post-traumatic stress disorder symptoms. \nAdditionally, Marvin, age 24, explained a cathartic ef-\nfect of his favorite music, saying:   \nDo you ever get mad?, you kind of get pissed off and you just \ngo to your r\noom and you listen to music. It’s either that or \nyou punch your little brother out or something.  Table 3.  Top \n20 moods corresponding to 155 \nbands and artists. \nFinally, music with high energy/intensity moods may \nplay a relatively straightforward, practical role as home-\nless young people move through their daily lives. For in-\nstance, recall that on the surveys, participants indicated \nthat they listened to music in order to “wake myself up” \nand “reduce boredom,” and similar answers were given \nwhen interview participants completed timelines. \n5. CONCLUSION AND FUTURE WORK \nIn prior and on-going work, findings have revealed that \nhomeless young people have ordinary interactions with \ntechnology which are conditioned by the extraordinary \ncircumstances of homelessness [15,16,17]. In this report, \nwe have presented further evidence that indicates aspects \nof the ordinary and extraordinary. For homeless young \npeople who participated in this study in Vancouver, Brit-\nish Columbia, music appears to play a role in daily life \nthat may be fairly ordinary. Like most young people their \nage, homeless young people listen daily to a variety of \nmusic and music plays a part in their relationships with \nfriends and family. Yet, for homeless young people in this \nstudy, experiences with music were also extraordinary. \nSheila found comfort in songs that resonated with her dif-\nficult life experiences. Amanda played music to make \nmoney while living on the street and as part of overcom-\ning her drug addiction. Brian listened to music to regulate \naspects of post-traumatic stress disorder, a common result \nof problematic circumstances during childhood [13]. \nImportantly, as these results are preliminary, further \nwork is needed to fully elaborate the role of music in the \nlives of homeless young people. Once the study is com-\npleted in Seattle, a comprehensive thematic analysis of \nthe interview data, including the themes of music listening \nbehaviors and social aspects of shared musical interests, \nwith independent cross-coding will take place. Addition-\nally, associations between preferred music, bands and art-\nists, moods and risk-taking behaviors will be analyzed. \nFinally, results from the design activity where participants \nenvisioned a music device that could help homeless Mood Count Mood Co unt \n  Energetic 68   Intense 38 \n  Aggressive 66   Party/Celebratory 38 \n  Confident 63   Boisterous 36 \n  Rousing 63   Dramatic 36 \n  Brash 51   Stylish 36 \n  Fun 42   Earnest 35 \n  Playful 42   Reflective 34 \n  Confrontational 41   Passionate 34 \n  Freewheeling 41   Rebellious 33 \n  Fiery 39   Visceral 33   \n \nyoung people\n (Figure 1) will be analyzed and inde-\npendently cross-coded. Results from this design activity \nanalysis will provide context and additional evidence of \nthe role of music and associated technologies in the lives \nof homeless young people. \n \nSo I was sitting one day on granville + georgia st, chilling \nout after a \nlong day of walking. My bags sitting at my side \ntrying to get enough change for a bite to eat. When some la-\ndy dropped this thing that looked like an iPod. I ran to pick \nit up, gave it back to her + being the kickass lady she was \nshe gave the player to me + said it would be better use to me. \nI asked her what it was + why she was being so nice. She \ntold me she bought it brand new the day before + it wasn’t to \nher likeing [sic]. She said it has lists of shelters + places to \nget food + their phones numbers in it. So she wanted to help \nsomeone out.  \nI was so greatful [sic]. I was able to escape from reality \nwith beautiful music for a little while. Then it came time for \nme to find another tree to sleep under. All of a sudden I re-\nmembered about the shelter listings on the MP3. (the [sic] \nlady called it a Musik Monster) First place that came up was \n[the collaborating agency]. I called, did an intake + now am \nliving happily in my own home. Looking for the next person \nto help with my Musik Monster. \nFigure 1. Design activity drawing and scenario. \n6. ACKNOWLED G\nMENTS \nWe would like to thank the study participants and the col-\nlaborating service agency. Partial support for the first au-\nthor was provided by Fulbright Canada. \n7. REFERENCES \n[1] American Academy of Pediatrics: “Policy Statement \n– Impact of Music, Music Lyrics, and Music Videos \non Children and Youth,” Pediatrics , Vol. 124, No. 5, \npp. 1488-1494, 2009. \n[2] Aristotle: Politics – Part V. Trans. by B. Jowett. \nhttp://classics.mit.edu/Aristotle/politics.8.eight.html [3] J.J. Arnett: Metalheads: Heavy metal music and \nadolescent alienation , Westview Press, Boulder, \nCO, 1996. \n[4] Behemoth: Ov Fire and the Void , 2009. \nhttp://www.youtube.com/watch?v=sKjNk-sAS 7U \n[5] J. Carlisle: “Digital Music and Generation Y: \nDiscourse Analysis of the Online Music Information \nBehaviour Talk of Five Young Australians,” \nInformation Research , Vol. 12, No. 4, 2007.  \n[6] Flux Pavilion: Lines in Wax , 2012. \nhttp://www.youtube. com/watch?v=Dx2nH2RJEAA \n[7] M.D. Kipke, J.B. Unger, S. O'Connor, R.F. Palmer, \nand S.R. LaFrance: “Street Youth, their Peer Group \nAffiliation and Differences According to Residential \nStatus, Subsistence patterns, and Use of Services,” \nAdolescence , Vol. 32, No. 127, pp. 655-669, 1997. \n[8] A. Laplante: “Users’ Relevance Criteria in Music \nRetrieval in Everyday Life: An Exploratory Study,” \nProc. of ISMIR, pp. 601-606, 2010. \n[9] A. Laplante and J.S. Downie: “Everyday Life Music \nInformation-Seeking Behaviour of Young Adults,” \nProc. of ISMIR, pp. 381-382, 2006.  \n[10] A. Laplante and J.S. Downie: “The Utilitarian and \nHedonic Outcomes of Music Information-seeking in \nEveryday Life,” Library & Information Science \nResearch, Vol. 33, No. 3, pp. 202-210, 2011. \n[11] Public Health Agency of Canada: Street Youth in \nCanada: Findings from Enhanced Surveillance of \nCanadian Street Youth, 1999-2003 , Ottawa, 2006. \n[12] Rovi Corporation.: Allmusic, 2012. http://www. \nallmusic. com \n[13] L.B. Whitbeck: Mental health and emerging \nadulthood among homeless young people, \nPsychology Press, New York, 2009. \n[14] C. Williams: “Does it Really Matter? Young people \nand Popular Music,” Popular Music , Vol. 20, No. 2, \npp. 223-242, 2001. \n[15] J.P. Woelfer and D.G. Hendry: “Homeless Young \nPeople and Technology: Ordinary Interactions, \nExtraordinary Circumstances.” ACM interactions , \nVol. 28, No. 2, pp. 70-73, 2011. \n[16] J.P. Woelfer and D.G. Hendry: “Homeless Young \nPeople and Living with Personal Digital Artifacts,” \nProc. of CHI, pp. 1697-1706, 2011. \n[17] J.P. Woelfer and D.G. Hendry: “Homeless Young \nPeople’s Experiences with Information Systems: Life \nand Work in a Community Technology Center,” \nProc. of CHI, pp. 1291–1300, 2010."
    },
    {
        "title": "A Systematic Comparison of Music Similarity Adaptation Approaches.",
        "author": [
            "Daniel Wolff",
            "Sebastian Stober",
            "Andreas Nürnberger",
            "Tillman Weyde"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416600",
        "url": "https://doi.org/10.5281/zenodo.1416600",
        "ee": "https://zenodo.org/records/1416600/files/WolffSNW12.pdf",
        "abstract": "In order to support individual user perspectives and differ- ent retrieval tasks, music similarity can no longer be con- sidered as a static element of Music Information Retrieval (MIR) systems. Various approaches have been proposed recently that allow dynamic adaptation of music similarity measures. This paper provides a systematic comparison of algorithms for metric learning and higher-level facet dis- tance weighting on the MagnaTagATune dataset. A cross- validation variant taking into account clip availability is presented. Applied on user generated similarity data, its effect on adaptation performance is analyzed. Special at- tention is paid to the amount of training data necessary for making similarity predictions on unknown data, the num- ber of model parameters and the amount of information available about the music itself.",
        "zenodo_id": 1416600,
        "dblp_key": "conf/ismir/WolffSNW12",
        "keywords": [
            "Music similarity",
            "Dynamic adaptation",
            "MagnaTagATune dataset",
            "Metric learning",
            "Facet distance weighting",
            "Cross-validation",
            "User-generated similarity data",
            "Adaptation performance",
            "Training data",
            "Model parameters"
        ],
        "content": "A SYSTEMATIC COMPARISON OF\nMUSIC SIMILARITY ADAPTATION APPROACHES\nDaniel Wolff\u0003, Tillman Weyde\nMIRG, School of Informatics\nCity University London, UK\ndaniel.wolff.1@city.ac.ukSebastian Stober\u0003, Andreas N ¨urnberger\nData & Knowledge Engineering Group\nOtto-von-Guericke-Universit ¨at Magdeburg, DE\nstober@ovgu.de\nABSTRACT\nIn order to support individual user perspectives and differ-\nent retrieval tasks, music similarity can no longer be con-\nsidered as a static element of Music Information Retrieval\n(MIR) systems. Various approaches have been proposed\nrecently that allow dynamic adaptation of music similarity\nmeasures. This paper provides a systematic comparison of\nalgorithms for metric learning and higher-level facet dis-\ntance weighting on the MagnaTagATune dataset. A cross-\nvalidation variant taking into account clip availability is\npresented. Applied on user generated similarity data, its\neffect on adaptation performance is analyzed. Special at-\ntention is paid to the amount of training data necessary for\nmaking similarity predictions on unknown data, the num-\nber of model parameters and the amount of information\navailable about the music itself.\n1. INTRODUCTION\nMusical similarity is a central issue in MIR and the key\nto many applications. In the classical retrieval scenario,\nsimilarity is used as an estimate for relevance to rank a\nlist of songs or melodies. Further applications comprise\nthe sorting and organization of music collections by group-\ning similar music clips or generating maps for a collection\noverview. Finally, music recommender systems that fol-\nlow the popular “ﬁnd me more like. . . ”-idea often employ\na similarity-based strategy as well. However, music sim-\nilarity is not a simple concept. In fact there exist various\nframeworks within musicology, psychology, and cognitive\nscience. For a comparison of music clips, many interre-\nlated features and facets can be considered. Their individ-\nual importance and how they should be combined depend\nvery much on the user and her or his speciﬁc retrieval task.\nUsers of MIR systems may have various (musical) back-\ngrounds and experience music in different ways. Conse-\nquently, when comparing musical clips with each other,\nopinions may diverge. Apart from considering individual\n*The two leading authors contributed equally to this work.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.users or user groups, similarity measures also should be\ntailored to their speciﬁc retrieval task to improve the per-\nformance of the retrieval system. For instance, when look-\ning for cover versions of a song, the timbre may be less\ninteresting than the lyrics. Various machine learning ap-\nproaches have recently been proposed for adapting a music\nsimilarity measure for a speciﬁc purpose. They are brieﬂy\nreviewed in Section 2. For a systematic comparison of\nthese approaches, a benchmark experiment based on the\nMagnaTagATune dataset has been designed, which is de-\nscribed in Section 3. Section 4 discusses the results of the\ncomparison and Section 5 ﬁnally draws conclusions.\n2. ADAPTATION APPROACHES\nThe approaches covered in this paper focus on learning\na distance measure, which (from a mathematical perspec-\ntive) can be considered as a dual concept to similarity. The\nlearning process is guided by so-called relative distance\nconstraints. A relative distance constraint (s;a;b) demands\nthat the object ais closer to the seed object sthan objectb,\ni.e.,\nd(s;a )<d(s;b) (1)\nSuch constraints can be seen as atomic bits of information\nfed to the adaptation algorithm. They can be derived from\na variety of higher-level application-dependent constraints.\nFor instance, in the context of interactive clustering, as-\nsigning a song sto a target cluster with the prototype ct\ncan be interpreted by the following set of relative distance\nconstraints as proposed by Stober et al. [11]:\nd(s;ct)<d(s;c)8c2Cnfctg (2)\nwhereCis the set of cluster prototypes. Bade et al. de-\nscribe how relative distance constraints can be derived from\nexpert classiﬁcations of folk songs [1] or from an existing\npersonal hierarchy of folders with music ﬁles [2]. Alter-\nnatively, it is also possible to directly ask the users to state\nthe opinion for a triplet of songs as in the bonus round of\ntheTagATune game [7]. (Section 3.2 covers this in de-\ntail.) McFee et al. [8] use artist similarity triples collected\nin the web survey described in [5]. They also describe a\ngraph-based technique to detect and remove inconsisten-\ncies within sets of constraints such as direct contradictions.\nUsing relative distance constraints, the task of learning\na suitable adaptation of a distance measure can be formu-\nlated mathematically as constraint optimization problem.In the following, the two general approaches covered in\nthis comparison are brieﬂy reviewed.\n2.1 Linear Combinations of Facet Distances\nStober et al. model the distance d(a;b) between two songs\nas weighted sum of facet distances \u000ef1(a;b);:::;\u000e fl(a;b):\nd(a;b) =lX\ni=1wi\u000efi(a;b) (3)\nEach facet distance refers to an objective comparison of\ntwo music clips with respect to a single facet of music in-\nformation such as melody, timbre, or rhythm. Here, the\nfacet weights w1;:::;wl2R+serve as parameters of\nthe distance measure that allow to adapt the importance of\neach facet to a speciﬁc user or retrieval task. These weights\nobviously have to be non-negative so that the aggregated\ndistance cannot decrease where a single facet distance in-\ncreases. Furthermore, the sum of the weights should be\nconstant such as lX\ni=1wi=l (4)\nto avoid arbitrarily large distance values.\nThe small number of parameters somewhat limits the\nexpressivity of the distance model. However, at the same\ntime, the weights can easily be understood and directly ma-\nnipulated by the user. Stober et al. argue that this design\nchoice speciﬁcally addresses the users’ desire to remain\nin control and not to be patronized by an intelligent sys-\ntem that “knows better”. In [11], they describe various ap-\nplications and respective adaptation algorithms which they\nevaluate and compare in [12] using the MagnaTagATune\ndataset. Three of these approaches are covered by the com-\nparison in this paper.\n2.1.1 Gradient Descent\nHere, if a constraint is violated by the current distance mea-\nsure, the weighting is updated by trying to maximize\nobj(s;a;b) =lX\ni=1wi(\u000efi(s;b)\u0000\u000efi(s;a)) (5)\nwhich can be directly derived from Equation 1. This leads\nto the following update rule for the individual weights:\nwi=wi+\u0011\u0001wi;with (6)\n\u0001wi=@obj(s;a;b)\n@wi=\u000efi(s;b)\u0000\u000efi(s;a) (7)\nwhere the learning rate \u0011deﬁnes the step width of each it-\neration. As in [12], the optimization process is restarted 50\ntimes with random initialization and the best result is cho-\nsen to reduce the risk of getting stuck in a local optimum.\n2.1.2 Quadratic Programming\nOf the various quadratic programming approaches covered\nin [12], only the one minimizing the quadratic slack is con-\nsidered here because it was the best performing one in the\noriginal comparison. In this approach, an individual slack\nvariable is used for each constraint, which allows viola-\ntions. As optimization objective, the sum of the squared\nslack values has to be minimized.\nrelative distance constraints linear classification problem \nFigure 1. Transformation of a relative distance constraint\nfor linear combination models into two training instances\nof the corresponding binary classiﬁcation problem as de-\nscribed by Cheng et al. [3].\n2.1.3 Linear Support Vector Machine (LibLinear)\nThe third approach takes a very different perspective. As\ndescribed by Cheng et al. [3], the learning task can be re-\nformulated as a binary classiﬁcation problem, which opens\nthe possibility to apply a wide range of sophisticated clas-\nsiﬁcation techniques such as (linear) Support Vector Ma-\nchines (SVMs). Figure 1 illustrates this idea to rewrite\neach relative distance constraint d(s;a )<d(s;b)as\nmX\ni=1wi(\u000efi(s;b)\u0000\u000efi(s;a)) =mX\ni=1wixi=wTx>0(8)\nwherexiis the distance difference with respect to facet fi.\nThe positive training example (x;+1) then represents the\nsatisﬁed constraint whereas the negative example (\u0000x;\u00001)\nrepresents its violation (i.e., inverting the relation sign).\nFor these training examples, the normal vector of the hy-\nperplane that separates the positive and negative instances\ncontains the adapted facet weights. As in [12], the Lib-\nLinear library is used here, which ﬁnds a stable separating\nhyperplane but still suffers from the so far unresolved prob-\nlem that the non-negativity of the facet weights cannot be\nenforced.\n2.2 Metric Learning\nAlternative approaches to weighting predeﬁned facet dis-\ntance measures include direct manipulation of parametrized\nvector distance measures. All features are concatenated\nto a single combined feature vector per clip. We model\na clip’s feature vector by g(a) :N7!RN. This corre-\nsponds to assigning a single facet to each feature dimen-\nsion. Frequently, the mathematical form of Mahalanobis\nmetrics is used to specify a parametrized vector distance\nmeasure. In contrast to the approaches described in the pre-\nvious section, adaptation is performed in the (combined)\nfeature space itself: Given two feature vectors a=g(a),\nb=g(b)2RN, the family of Mahalanobis distance mea-\nsures can be expressed by\ndW(a;b) =q\n(a\u0000b)TW(a\u0000b); (9)\nwhere W2RN\u0002Nis a positive semideﬁnite matrix, para-\nmetrizing the distance function. Generic variants of theEuclidean metric, Mahalanobis metrics allow for linear trans-\nformation of the feature space when accessing distance.\nAn important property of this approach is that the number\nof adjustable parameters directly depends on the dimen-\nsionalityNof the feature space. As this number grows\nquadratically with N, many approaches restrict training to\ntheNparameters of a diagonal matrix W, only permitting\na weighting of the individual feature dimensions.\n2.2.1 Linear Support Vector Machine (SVMLight)\nThe SVM approach explained in Section 2.1.3 has been\nshown as well suited to learning a Mahalanobis distance\nmeasure: Schultz et al. [10] adapted a weighted kernelized\nmetric towards relative distance constraints. We follow the\napproach of Wolff et al. [13], where a linear kernel is used.\nThis simpliﬁes the approach of Schultz et al. to learning a\ndiagonally restricted Mahalanobis distance (Equation 9).\nLike the SVM for the facet distances, a large margin\nclassiﬁer is optimized to the distance constraints. Here,\nfor each constraint (s;a;b), we replace the facet distance\ndifference vector xinEquation 8 with the difference of the\npointwise squared1feature difference vectors x= (s\u0000\nb)2\u0000(s\u0000a)2:\nGiven the vector w=diag(W );wi\u00150and slack vari-\nables\u0018(s;a;b)\u00150, optimization is performed as follows:\nmin\nw;\u00181\n2wTw+c\u0001X\n(s;a;b)\u0018(s;a;b) (10)\ns.t.8(s;a;b) wTx(s;a;b)\u00151\u0000\u0018(s;a;b)\nHere,cdetermines a trade-off between regularization and\nthe enforcement of constraints. For the experiments below,\nthe SVMlightframework2is used to optimize the weights\nwi. As for LibLinear, wi\u00150cannot be guaranteed.\n2.2.2 Metric Learning to Rank\nMcFee et al. [9] developed an algorithm for learning a Ma-\nhalanobis distance from rankings.3Using the constrained\nregularization of Structural SVM, the matrix Wis opti-\nmized to an input of clip rankings and their feature vectors.\nGiven a relative distance constraint (s;a;b) (seeEquation 1),\nthe corresponding ranking assigns a higher ranking score\ntoathan tob, when querying clip s. For a setXof training\nquery feature vectors q2X\u001aRNand associated training\nrankingsy\u0003\nq, Metric Learning to Rank (MLR) minimizes\nmin\nW;\u0018tr(WTW) +c1\nnX\nq2X\u0018q; (11)\ns.t.8q2X;8y2Ynfy\u0003\nqg:\nHW\u0000\nq;y\u0003\nq\u0001\n\u0015HW(q;y) + \u0001(y\u0003\nq;y)\u0000\u0018q;\nwithWi;j\u00150and\u0018q\u00150. Here, the matrix Wis reg-\nularized using the trace. Optimization is subject to the\nconstraints creating a minimal slack penalty of \u0018q.cde-\ntermines the trade-off between regularization and the slack\npenalty for the constraints below. HW(q;y)4assigns a\n1(a2)i:= (a i)2\n2http://svmlight.joachims.org/\n3http://cseweb.ucsd.edu/˜bmcfee/code/mlr/\n4For simpliﬁcation, HW(q;y )substitutes the Frobenius product\nhW; (q;y )iFin [9].score to the validity of ranking ygiven the query qwith\nregard to the Mahalanobis matrix W. This enforces W\nto fulﬁll the training rankings y\u0003\nq. The additional ranking-\nloss term \u0001(y\u0003\nq;y)assures a margin between the scores of\ngiven training rankings y\u0003\nqand incorrect rankings y. The\nmethod is kept efﬁcient by selecting only a few possible\nalternative rankings y2Yfor comparison with the train-\ning rankings: A separation oracle is used for predicting the\nworst violated constraints (see [6]). In our experiments, an\nMLR variant DMLR restricts Wto a diagonal shape.\n3. EXPERIMENT DESIGN\n3.1 The MagnaTagATune Dataset\nMagnaTagATune is a dataset combining mp3 audio, acous-\ntic feature data, user votings for music similarity, and tag\ndata for a set of 25863 clips of about 30 seconds taken from\n5405 songs provided by the Magnatune5label. The bun-\ndled acoustic features have been extracted using version\n1.0 of the EchoNest API6. The tag and similarity data has\nbeen collected using the TagATune game [7]. TagATune\nis a typical instance of an online “Game With A Purpose”.\nWhile users are playing the game mainly for recreational\npurposes, they annotate the presented music clips. The tag\ndata is collected during the main mode of the game, where\ntwo players have to agree on whether they listen to identi-\ncal clips. Their communication is saved as tag data. The\nbonus mode of the game involves a typical odd one out\nsurvey asking two players to independently select the same\noutlier out of three clips presented to them. The triplets of\nclips presented to them vary widely in genre, containing\nmaterial from ambient and electronica, classical, alterna-\ntive, and rock.\n3.2 Similarity Data\nThe comparative similarity data in MagnaTagATune can be\nrepresented in a constraint multigraph with pairs of clips as\nnodes [8, 12]. The vote for an outlier kin the clip triplet\n(i;j;k )is transformed into two relative distance constraints:\n(i;j;k )and(j;i;k ). Each constraint (s;a;b) is represented\nby an edge from the clip pair (s;a)to(s;b). This results\nin 15300 edges of which 1598 are unique. In order to\nadapt similarity measures to this data, the multigraph has\nto be acyclic, as cycles correspond to inconsistencies in the\nsimilarity data. The MagnaTagATune similarity data only\ncontains cycles of length 2, corresponding to contradictive\nuser statements regarding the same triplet. In order to re-\nmove these cycles, the contradicting multigraph edge num-\nbers are consolidated by subtracting the number of edges\nconnecting the same vertices in opposite directions. The\nremaining 6898 edges corresponding to 860 unique rela-\ntive distance constraints constitute the similarity data we\nwork with.7\n5http://magnatune.com/\n6http://developer.echonest.com/\n7In [12], the authors report that the number of consistent constraints is\n674. This differing number was caused by a software bug in the ﬁltering\nalgorithm, which led to the removal of more constraints than necessary.3.3 Data Partitioning\nIn order to assess the training performance of the approaches\ndescribed in Section 2, we compare two cross-validation\nvariants to specify independent test and training sets.\nA straightforward method, randomly sampling the con-\nstraints into cross-validation bins and therefore into combi-\nnations of test and training sets has been used on the dataset\nbefore by Wolff et al. [13]. We use this standard method\n(sampling A) to perform 10-fold cross validation, sampling\nthe data into non-overlapping test and training sets of 86\nand 774 constraints respectively\nFor the second sampling, it is considered that two con-\nstraints were derived from each user voting, as such are\nrelated to the same clips. Assigning one of such two con-\nstraints to training and the remaining one to a test set might\nintroduce bias by referring to common information. In\nour second validation approach, (sampling B) it is assured\nthat the test and training sets also perfectly separate on the\nclip set. The 860 edges of the MagnaTagATune similar-\nity multigraph connect 337 components of three vertices\neach. These correspond to the initial setup of clip triplets\npresented to the players during the TagATune game.\nAs the removal of one clip causes the loss of all similar-\nity information (maximally 3 constraints) within its triplet,\nthe sampling of the test data is based on the triplets rather\nthan the constraints. On the 337 triplets, we use 10-fold\ncross validation for dividing these into bins of 33 or 34\ntriplets. Due to the varying number of 2-3 constraints per\ntriplet, the training set sizes vary from 770-779 constraints,\nleaving the test sets at 81-90 constraints.\nFor evaluation of generalization and general performance\ntrends, the training sets are analyzed in an expanding sub-\nset manner. We start with individual training sets of ei-\nther 13 constraints (sampling A) or 5 triplets (sampling B),\ncorresponding to 11-15 constraints. The size of the train-\ning sets is then increased exponentially, including all the\nsmaller training sets’ constraints in the larger ones. Con-\nstraints remaining unused for each of the smaller training\nset sizes are used for further validation, and referred to as\nunused training constraints. For both sampling methods,\nall test and training sets are ﬁxed, and referred to as sam-\npling A andsampling B.\n3.4 Features and Facets\nAs features, we use those deﬁned in [12] plus the genre\nfeatures used by Wolff et al. [13]. This results in the set of\nfeatures shown in Table 1.\nOf the 7 global features, “danceability” and “energy”\nwere not contained in the original clip analysis information\nof the dataset but have become available with a newer ver-\nsion of the EchoNest API. Furthermore, the segment-based\nfeatures describing pitch (“chroma”) and timbre have been\naggregated (per dimension) resulting in 12-dimensional vec-\ntors with the mean and standard deviation values. This has\nbeen done according to the approach described in [4] for\nthe same dataset. The 99 tags were derived from annota-\ntions collected through the TagATune game [7] by applyingfeature dim. value description\nkey 1 0to11(one of the 12 keys) or \u00001(none)\nmode 1 0(minor), 1(major) or\u00001(none)\nloudness 1 overall value in decibel (dB)\ntempo 1 in beats per minute (bpm)\ntime signature 1 3to7(3\n4to7\n4),1(complex), or\u00001(none)\ndanceability 1 between 0(low) and 1(high)\nenergy 1 between 0(low) and 1(high)\npitch mean 12 dimensions correspond to pitch classes\npitch std. dev. 12 dimensions correspond to pitch classes\ntimbre mean 12 normalized timbre PCA coefﬁcients\ntimbre std. dev. 12 normalized timbre PCA coefﬁcients\ntags 99 binary vector (very sparse)\ngenres 44 binary vector (very sparse)\nTable 1. Features for the MagnaTagATune dataset. Top\nrows: Globally extracted EchoNest features. Middle rows:\nAggregation of EchoNest features extracted per segment.\nBottom row: Manual annotations from TagATune game\nand the Magnatune label respectively.\nthe preprocessing steps described in [12]. The resulting bi-\nnary tag vectors are more dense than for the original 188\ntags but still very sparse. The genre labels were obtained\nfrom the Magnatune label as described by Wolff et al. [13].\nA total of 42 genres was assigned to the clips in the test set\nwith 1-3 genre labels per clip. This also results in very\nsparse binary vectors.\nFor the facet-based approaches described in Section 2.1,\ntwo different sets of facets are considered consisting of 26\nand 155 facets respectively. In both sets, the 7 global fea-\ntures are represented as individual facets (using the dis-\ntance measures described in [12]). As the genre labels\nare very sparse, they are combined in a single facet us-\ning the Jaccard distance measure. The set of 155 facets\nis obtained by adding 99 tag facets (as in [12]) and a sin-\ngle facet for each dimension of the 12-dimensional pitch\nand timbre features. For the set of 26 facets, the pitch\nand timbre feature are represented as a single facet each\n(combining all 12 dimensions). Furthermore, 14 tag-based\nfacets are added of which 9 refer to aggregated tags that\nare less sparse (solo, instrumental, voice present, male, fe-\nmale, noise, silence, repetitive, beat) and 5 compare binary\nvectors for groups of related tags (tempo, genre, location,\ninstruments, perception / mood). This results in a realis-\ntic similarity model of reasonable complexity that could\nstill be adapted manually by a user. The more complex\nmodel with almost six times as many facet weight param-\neters serves as the upper bound of the adaptability using a\nlinear approach for the given set of features.\n4. RESULTS\nFor the similarity data sampling A, Figure 2 shows all of\nthe algorithms to improve the baseline of 63% satisﬁed un-\nknown constraints by 7% to 10%. Plotted are the perfor-\nmance averages over 10-fold cross-validation as described\ninSection 3.3. Except for SVMlight, most of the ﬁnal gen-\neralization success is achieved within the ﬁrst 250 train-\ning constraints. Only diagonal MLR shows notable non-\nmonotonic behaviour for larger training sets >200 con-straints. Further tests on the unused training data repro-\nduce the results on the static test sets shown here. As\nshown in Figure 3, all algorithms are able to satisfy the\ninitial training constraints. With the exception of MLR,\n(see Section 4.2), the training performance decreases for\ngrowing training sets, asymptotically approaching the test\nset performance. Such effects have been shown in [13] not\nto contradict good generalization results.\n4.1 Impact of Model Complexity\nFor the facet-based linear approaches (Figure 3, left), a\nstrong impact of the number of facet weight parameters can\nbe observed. Whilst the performance for the model with\n155 facets is signiﬁcantly superior on the training data, it\nis generally worse on the test data. Only for a high number\nof training constraints, the simpler model with 26 facets\ncan be matched or slightly outperformed. This is a strong\nindicator for model overﬁtting. With its many parameters,\nthe complex model adapts too much to the training data at\nthe cost of a reduced ability to generalize. In contrast, the\nsimple model is able to generalize much quicker. This is\nespecially remarkable for the quadratic programming ap-\nproach with the quickest generalization of all approaches.\nIts adaptation performance on the test data also comes clos-\nest to the training performance, which can be seen as an\nupper bound. It appears as if this limit is increased by\nabout 5%, if 155 facets are used instead, but more train-\ning examples would be needed to get closer to this value.\nHere lies great potential for future research: By adapting\nthe model complexity (i.e., the number of parameters) de-\npending on the number of training examples and the per-\nformance on some unseen constraints, the ability of simple\nmodels to quickly generalize could be combined with the\nsuperior adaptability of more complex ones.\n4.2 Effects of Similarity Sampling\nFor most of the algorithms tested, the effect of choosing\nsampling A or sampling B is small. Best performing are\nMLR (samling A) and quadratic programming(m = 155)\nfor sampling B. Except for MLR, decrease in test set per-\nformance is limited to 1% when trained with the clip-sepa-\nrating sampling B. In the right column of Figure 3, the met-\nric learning algorithms are compared. The bottom black\ncurves represent the test set results for sampling A (dashed,\n\u0001–\u0001) and sampling B (solid, —). The training perfor-\nmances for these samplings are plotted on the top of the\ngraphs. While SVMlight(d) and DMLR (f) only loose 2-3%\nin performance, MLR (e) drops by more than 6%. Exclu-\nsively among the algorithms tested, the fully parametrized\nMLR(e) variant shows a 100% training performance for all\ntraining sizes. In line with results from Wolff et al. [13,14],\nthe algorithm generalizes well on the similarity data with\nsampling A. Even with further permutations of the data,\nthis capability to generalize reduces signiﬁcantly when us-\ning MLR with our sampling method B, possibly caused by\nthe lack of feature reoccurence in the training data.5. CONCLUSIONS\nThe results of the experiment show that all approaches can\nadapt a similarity model to training data and generalize the\nlearned information to unknown test data. Training perfor-\nmance curves can be used as an indicator for the maximal\ngeneralization outcome to expect, which depends on the\nnumber of facets and the features used. Sensitivity with re-\nspect to the sampling method of the test data was observed\nfor MLR, which requires further investigation. Another\npromising direction for future work is to dynamically adapt\nthe model complexity, e.g., by regularization. The feature\ndata and sampling information are available online8for\nbenchmarking of approaches developed in the future.\n6. REFERENCES\n[1] K. Bade, J. Garbers, S. Stober, F. Wiering, and\nA. N ¨urnberger. Supporting folk-song research by au-\ntomatic metric learning and ranking. In Proc. of IS-\nMIR’09, 2009.\n[2] K. Bade, A. N ¨urnberger, and S. Stober. Everything in\nits right place? learning a user’s view of a music collec-\ntion. In Proc. of NAG/DAGA’09, Int. Conf. on Acous-\ntics, 2009.\n[3] W. Cheng and E. H ¨ullermeier. Learning similarity\nfunctions from qualitative feedback. In Proc. of EC-\nCBR’08, 2008.\n[4] J. Donaldson and P. Lamere. Using visualizations for\nmusic discovery. Tutorial at ISMIR’09, 2009.\n[5] D. P. W. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence. The quest for ground truth in musical\nartist similarity. In Proc. of ISMIR’02, 2002.\n[6] T. Joachims, T. Finley, and C.-N.J. Yu. Cutting-plane\ntraining of structural SVMs. In Machine Learning,\n2009.\n[7] E. Law and L. von Ahn. Input-agreement: a new mech-\nanism for collecting data using human computation\ngames. In Proc. of CHI ’09, 2009.\n[8] B. McFee and G. Lanckriet. Heterogeneous embedding\nfor subjective artist similarity. In Proc. of ISMIR’09,\n2009.\n[9] B. McFee and G. Lanckriet. Metric learning to rank. In\nProc. of ICML’10, 2010.\n[10] M. Schultz and T. Joachims. Learning a distance metric\nfrom relative comparisons. In NIPS, 2003.\n[11] S. Stober. Adaptive distance measures for exploration\nand structuring of music collections. In Proc. of AES\n42nd Conference on Semantic Audio, 2011.\n[12] S. Stober and A. N ¨urnberger. An experimental com-\nparison of similarity adaptation approaches. In Proc. of\nAMR’11, 2011.\n[13] D. Wolff and T. Weyde. Adapting metrics for music\nsimilarity using comparative judgements. In Proc. of\nISMIR 2011, 2011.\n[14] D. Wolff and T. Weyde. Adapting Similarity on the\nMagnaTagATune Database In ACM Proc. of WWW’12,\n2012.\n8http://mi.soi.city.ac.uk/datasets/ismir2012/0 100 200 300 400 500 600 70060657075\navg. number of training constraints% of test constraints satisfied\n  \nMLR 75.58%\nSVMLIGHT 73.72%\nLIBLINEAR 72.21%\nGRADIENT 72.21%\nQUADRATIC 71.74%\nDMLR 69.53%Figure 2. Performance comparison of facet-based approaches (with 26 facets) and metric learning. Values are averaged\nover all 20 folds of sampling A. The baseline at 63% refers to the mean performance of random facet weights ( n= 1000).\n0 200 400 6006080100\navg. number of training constraints(c) LIBLINEAR\n  \n71.5\n69.10 200 400 6006080100(a) GRADIENT\n  \n71.6\n70.6\n0 200 400 6006080100% of constraints satisfied(b) QUADRATIC\n  \n72.7\n74.0\n0 200 400 6006080100(e) MLR\n  \n68.9\n75.6\n0 200 400 6006080100\navg. number of training constraints(f) DMLR\n  \n66.5\n69.50 200 400 6006080100(d) SVMLIGHT\n  \n71.2\n73.7\nFigure 3. Detailed performance of the individual approaches under different conditions. Top curves show training\nperformance, bottom curves and legend show test set performance. Left column (a, b, c): Performance of the facet-\nbased approaches using 26 facets (—) and 155 facets (– –). Comparison based on sampling B. Right column (d, e, f):\nPerformance of the metric-based approaches. Effects of sampling A(\u0001 –\u0001) and B(—) are compared."
    },
    {
        "title": "Unsupervised Learning of Local Features for Music Classification.",
        "author": [
            "Jan Wülfing",
            "Martin A. Riedmiller"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414782",
        "url": "https://doi.org/10.5281/zenodo.1414782",
        "ee": "https://zenodo.org/records/1414782/files/WulfingR12.pdf",
        "abstract": "In this work we investigate the applicability of unsuper- vised feature learning methods to the task of automatic genre prediction of music pieces. More specifically we evaluate a framework that recently has been successfully used to recognize objects in images. We first extract local patches from the time-frequency transformed audio signal, which are then pre-processed and used for unsupervised learning of an overcomplete dictionary of local features. For learning we either use a bootstrapped k-means cluster- ing approach or select features randomly. We further ex- tract feature responses in a convolutional manner and train a linear SVM for classification. We extensively evaluate the approach on the GTZAN dataset, emphasizing the in- fluence of important design choices such as dimensionality reduction, pooling and patch dimension on the classifica- tion accuracy. We show that convolutional extraction of lo- cal feature responses is crucial to reach high performance. Furthermore we find that using this approach, simple and fast learning techniques such as k-means or randomly se- lected features are competitive with previously published results which also learn features from audio signals.",
        "zenodo_id": 1414782,
        "dblp_key": "conf/ismir/WulfingR12",
        "keywords": [
            "unsupervised feature learning",
            "automatic genre prediction",
            "image recognition",
            "local patches",
            "time-frequency transformed audio",
            "overcomplete dictionary",
            "convolutional manner",
            "linear SVM",
            "GTZAN dataset",
            "dimensionality reduction"
        ],
        "content": "UNSUPERVISED LEARNING OF LOCAL FEATURES FOR MUSIC\nCLASSIFICATION\nJan W ¨ulﬁng and Martin Riedmiller\nUniversity of Freiburg\nfwuelfj,riedmillerg@tf.uni-freiburg.de\nABSTRACT\nIn this work we investigate the applicability of unsuper-\nvised feature learning methods to the task of automatic\ngenre prediction of music pieces. More speciﬁcally we\nevaluate a framework that recently has been successfully\nused to recognize objects in images. We ﬁrst extract local\npatches from the time-frequency transformed audio signal,\nwhich are then pre-processed and used for unsupervised\nlearning of an overcomplete dictionary of local features.\nFor learning we either use a bootstrapped k-means cluster-\ning approach or select features randomly. We further ex-\ntract feature responses in a convolutional manner and train\na linear SVM for classiﬁcation. We extensively evaluate\nthe approach on the GTZAN dataset, emphasizing the in-\nﬂuence of important design choices such as dimensionality\nreduction, pooling and patch dimension on the classiﬁca-\ntion accuracy. We show that convolutional extraction of lo-\ncal feature responses is crucial to reach high performance.\nFurthermore we ﬁnd that using this approach, simple and\nfast learning techniques such as k-means or randomly se-\nlected features are competitive with previously published\nresults which also learn features from audio signals.\n1. INTRODUCTION\nAutomatic categorization of music pieces into categories\nsuch as mood, artist or genre is a widely studied topic\nin music information retrieval. Those categorization tasks\nbasically consist of two steps: feature selection/extraction\nand classiﬁcation. Designing and selecting good features\nfor a certain task is demanding and requires expert knowl-\nedge about the domain at hand. Nonetheless, a wide range\nof those hand designed features have been proposed in the\npast. More recently there has been a growing interest in\nmethods that automatically learn features from data in an\nunsupervised fashion. Those methods have been very suc-\ncessful on a range of recognition benchmarks for images\nas well as audio data (see Section 2).\nIn this work, we investigate the applicability of a k-\nmeans based unsupervised feature learning framework that\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.has been used successfully for object recognition in RGB-\nD images [1] to the problem of genre classiﬁcation of mu-\nsic pieces.\nThis framework allows us to fast and ﬂexibly learn local\nfeatures of different sizes and shapes and to show whether\nfeatures that span the whole frequency range, only parts of\nit or even features that cover several consecutive points in\ntime perform best for the task of genre prediction.\nTo do so, we need to transform the raw audio signal\ninto the time-frequency domain, for which researches have\nused varying transformations in the past. We determine the\ninﬂuence of this choice by evaluating the feature learning\non two different transformations.\nAfter learning an overcomplete dictionary of local fea-\ntures, we extract feature responses in a convolutional man-\nner. Although computationally more expensive, we show\nthat convolutional extraction, together with the right pool-\ning scheme, improves recognition performance signiﬁcantly.\nThe paper is structured as follows: In Section 2 we give\na short review of approaches that also learn features from\naudio data, followed by a description of the learning frame-\nwork in Section 3 and 4. We extensively evaluate the pa-\nrameters of the learning framework and show its potential\non the GTZAN dataset [14] by reporting competitive re-\nsults in Section 5.\n2. RELATED WORK\nThere is a range of feature learning methods that have been\nused to tackle music information retrieval tasks e.g. sparse\ncoding [6], principal component analysis [5], deep belief\nnetworks [4,8], or a mean-covariance restricted Boltzmann\nmachine [11].\nAll those feature learning frameworks rely on a transfor-\nmation of the raw audio signal to the spectral domain. Fre-\nquently used is the short time Fourier transformation (with\nvarying window lengths), which can be mel-frequency scaled\n[5, 11]. Henaff et al. [6] apply the constant-Q transform\n[12].\nFeatures learned by those approaches differ in size and\nshape, e.g. some approaches rely on features that cover\nsingle time frames [4–6], only parts of the frequency range\n[6] or even learn time-frequency features that span several\nconsecutive points in time [11].\nLearning feature codebooks using simple k-means has\na long tradition and has also been applied to audio tasks\nin [11, 13].However, Coates et al. [3] just recently found that good\nimage features can be learned using k-means if state-of-\nthe-art image pre-processing and feature encoding is used.\nThis ﬁnding could be conﬁrmed and extended for RGB-D\nimages by Blum et al. [1] who used a convolutional boot-\nstrapped k-means procedure to successfully learn RGB-D\nfeatures for object recognition in “3D” images.\n3. LEARNING FEATURE RESPONSES\nOur goal is to learn a set of feature responses D2RN\u0002k\ngiven a set of input vectors X=fx(1);:::;x(m)gwith\nx(i)2RN. The input vectors are patches of size v\u0002w\nextracted from a training set represented as column vec-\ntors. Each value is represented using dchannels (e.g. with\nRGB images d= 3, with spectrograms d= 1) and hence\nN=v\u0001w\u0001d. Random patches of size v\u0002ware extracted\nto build the training set X. OnceXis known we apply a\npre-processing step followed by the unsupervised learning\nalgorithm.\n3.1 Pre-processing\nAs a pre-processing step we ﬁrst normalize all patches con-\ntained inXby subtracting their mean and dividing by the\nstandard deviation. Afterwards a whitening transformation\n[7] is applied to the patches. The purpose of the whiten-\ning transformation is to ensure that values are decorrelated\nand have unit variance. This step is crucial to ensure a\ngood quality of the learned feature responses as shown\nin [3]. We use PCA whitening, which allows us to drop\ninsigniﬁcant dimensions from the input data. This results\nin increased feature extraction speed and can improve fea-\nture quality as shown in [1]. If dimensionality reduction\nis used, we chose to keep the ﬁrst ncomponents thereby\nprojecting each extracted patch x2RNto a lower dimen-\nsional vector x02Rn.\n3.2 Unsupervised learning\nWe use a k-means approach to learn kcentroids build-\ning the feature response dictionary Dby clustering the ex-\ntracted patches X. Although k-means is a very simple un-\nsupervised learning algorithm that is easy to implement, it\nhas recently been shown that it is competitive to other un-\nsupervised learning algorithms when learning local, low-\nlevel features from pre-processed image data [3]. Apart\nfrom its simplicity the main advantage of using k-means\nover other algorithms is that it is very fast and scales well\nto a large amount of centroids. It can therefore be trivially\nparallelized on current computer hardware in a map-reduce\nmanner and allows us to learn large, over-complete feature\ndictionaries that can be expensive to learn using other un-\nsupervised learning approaches.\n3.2.1 Bootstrapping\nTo further improve upon the feature quality that can be\nachieved using standard k-means, as well as the required\nrun time until convergence, we use a bootstrapping learn-\ning scheme as proposed in [1] to train the kcentroids.\n(a) without bootstrapping\n (b) with bootstrapping\nFigure 1: Comparison of 16\u000216features learned on the\nGTZAN dataset using the CQT transform without and with\nbootstrapping. (a) Without bootstrapping several cluster\ncenters, marked in white, do not represent good feature\nresponses due to the high dimensional space in which k-\nmeans clustering is performed. (b) When bootstrapping is\nenabled all learned centroids correspond to nicely localized\nfeatures.\nWe ﬁrst cluster in the subspace spanned by the ﬁrst p\nprincipal components and ﬁll the learned centroids with\nzeros for all other n\u0000pdimensions. These centroids are\nthen used to warm start the clustering procedure in the n\ndimensional PCA whitened space.\nWithout bootstrapping some features are badly local-\nized, which is an artifact of clustering in a high dimen-\nsional space (e.g. 256 dimensions if patches of size 16\u000216\nare used). This effect is visible in Fig. 1 where affected\nfeatures are marked white. When the bootstrapping pro-\ncedure is used the features are well distributed over the\nwhole feature space by pre-training the centroids on the\nmajor principal components. The consecutive clustering\nprocedure in the complete feature space is thus simpliﬁed\nand the badly localized features disappear.\n4. FEATURE EXTRACTION\ns\nw\nvfrequencytime\nFigure 2: Schematic of the convolutional extraction\nscheme. Note that with a stride ssmaller than vorw,\nextracted patches overlap.\nAfter learning the dictionary, feature responses are ex-\ntracted from the input data. Employing a convolutional\nextraction scheme (see Fig. 2), we traverse the input data\nwith stridesand extract patches at all possible positions.\nInstead of using standard hard k-means where the feature\nresponsef(x)is a sparse vector indicating the closest cen-troid\nfi(x) =(\n1ifci= arg min ci2Dkci\u0000xk\n0else;(1)\nwe compute the triangular response to maximize the in-\nformation content of each feature response. It keeps the\ninformation about the distance of the current patch to all\ncentroidsci2Dthat are closer than the average distance\n\u0016(z) =1\nkPk\ni=1ziwherez2Rkwithzi=kci\u0000xk. In\nthis casef(x)can be deﬁned as\nfi(x) = max(0 ;\u0016(z)\u0000zi): (2)\n4.1 Pooling\nfrequencytime\nSplit into overlapping time windowsFrequency \npooling\nFigure 3: Illustration of the pooling scheme. In the ﬁrst\nstep, the time-frequency transformed audio signal is split\nup into overlapping windows. For each window features\nare extracted and pooled.\nSince using all feature responses for classiﬁcation is\ncomputationally expensive - we get a response of size kfor\neach patch that we extract convolutionally - it is common\npractice to use a pooling scheme to reduce the dimension-\nality of the feature vector. The term pooling here refers\nto placing a grid with ccells on the input data and com-\nputing a function (e.g. maximum or average) over all fea-\nture responses that fall into a grid cell. The dimension of\nthe resulting feature vector is reduced to c\u0002k. For ob-\nject recognition in images a frequently used grid structure\nis2\u00022. This is a reasonable choice for object recogni-\ntion tasks where objects are positioned at the center of the\nimage. Here the grid helps to roughly encode the spatial\nproperties of the presented objects in the resulting feature\nvector. Analogously for audio data, using more than one\npool on the time axis results in an encoding of the tempo-\nral properties of the feature response. This however is not\ndesired for the task of genre prediction, since characteristic\npatterns for a certain genre might not always occur at ex-\nactly the same time. Additionally we might have to predict\nthe genre based only on a fragment of the song (as it is the\ncase for the GTZAN dataset), underlining the problem that\nencoding timing may not help, but in fact impair the qual-\nity of our prediction. Invariance to timing can be achieved\nby pooling only once on the time axis (e.g. 2\u00021). This\nhowever may reduce the information content of the feature\nvector too drastically. To overcome this problem, we split\nthe input data into overlapping time windows of a certain\nlength (similar to [6] and [5]), compute feature responsesand pool on each window separately. Each window then\nserves as input to the classiﬁer and the ﬁnal result is deter-\nmined by voting over the classiﬁcation results of all win-\ndows. An illustration of this scheme is depicted in Fig. 3.\n5. EV ALUATION\nWe evaluate the performance of the learned features on the\nGTZAN dataset [14].\n5.1 Experimental Setup\n5.1.1 Dataset\nThe GTZAN dataset is organized into 10 distinct genres:\nBlues, Classical, Country, Disco, Hip hop, Jazz, Metal,\nPop, Reggae and Rock. Each genre is represented by 100\nsong fragments of 30 seconds length.\n5.1.2 Pre-processing of audio data\nThere are several transformations used in the literature to\ntransform the raw audio signal into the time-frequency do-\nmain (see Section 2). To determine the inﬂuence of this\npre-processing choice, we evaluate the feature learning for\ntwo different transformations. We apply a short time Fourier\ntransform, calculated on 1024 samples with 512 samples\noverlap (STFT) and, in a second setting, use the Constant\nQ-Transform (CQT) [12] spanning 8 octaves, using 64 bins\nper octave, to create spectrograms of the audio signal. Both\nspectrograms have exactly the same number of values on\nthe frequency axis (512 values). We also sub sample the\nCQT to have exactly the same time resolution as the STFT\n(1292 time frames). This way any possible advantage due\nto a larger representation can be ruled out.\n5.1.3 Classiﬁcation\nFor classiﬁcation we use a linear SVM in a 10-fold cross\nvalidation setting.\n5.2 Patch dimension and learning techniques\nIn a ﬁrst experiment, we show the inﬂuence of varying\npatch sizes and learning techniques on classiﬁcation accu-\nracy. We learned features using k-means with bootstrapped\nand randomly initialized cluster centers (chosen at random\nfrom the input data). We ran k-means until convergence.\nTable 1: Inﬂuence of varying patch dimensions and learn-\ning methods on classiﬁcation accuracy. Results are aver-\naged over ten runs of 10-fold cross validation to minimize\nthe inﬂuence of random partitioning. The standard devia-\ntions are all well below 1%.\nPatch size k-means (random) k-means (boot.)\n(freq.\u0002time ) STFT CQT STFT CQT\n64\u00021 64:81 70:91 64:72 70:57\n128\u00021 59:59 67:14 68:48 72:19\n256\u00021 65:79 62:12 67:86 70:8\n512\u00021 62:37 66:54 67:69 67:26\n16\u000216 75:2 74:2 75:11 77:7Features span parts of the frequency axis (64 \u00021,128\u00021,\n256\u00021), the whole frequency range (512 \u00021) or fre-\nquency and time (16 \u000216). Note that features are ex-\ntracted convolutionally (with stride 1) if possible which ex-\ncludes features of size 512\u00021. In contrast to the smaller\npatches those features cannot beneﬁt from introducing sev-\neral pools on the frequency axis, they already span the\nwhole frequency range. That is why we only use one pool\nin this experimental condition. If not mentioned otherwise,\nwe learn dictionaries of size 800 using PCA whitening and\nkeeping as many components necessary to explain 95% of\nthe variance. Table 1 shows the results of this experiment.\nFor a small patch size of 64\u00021the performance of\nk-means and bootstrapped k-means is almost equal. Here\nbootstrapping k-means is unnecessary, since in low dimen-\nsions random initialization of the cluster centers sufﬁces.\nWith growing feature size however (e.g. 256\u00021), random\nk-means suffers from the effect depicted in Fig. 1, where\nparts of the dictionary are wasted on ill localized features.\nBootstrapping k-means reduces the impact of this prob-\nlem and affects classiﬁcation accuracy signiﬁcantly (e.g.\n5:05% improvement with a feature size of 128\u00021).\nComparing the performance of varying feature sizes, we\nﬁnd that learning features on the whole frequency range\n(without convolution) has the lowest accuracy, compared\nto smaller frequency patches. The 16\u000216time-frequency\nfeatures outperform any other setting.\nIn all settings the STFT accuracies are worse than the\nresults on the CQT transformed audio signal. In addition\nto the advantages of the Constant-Q transform over the dis-\ncrete Fourier transform described in [12], we suspect that\nthis is due to the fact that the Constant-Q transform is much\nsparser and less noisy than the STFT and thereby facilitates\nlearning of good features.\n5.3 Pooling and time windows\nIn this experiment we evaluate the parameters of the pool-\ning scheme described in Section 4.1 used for feature ex-\ntraction. We employ average pooling in all experiments\nand vary the number of pools on the frequency axis. We\nperform experiments on the CQT transformed data. The\nresults for features of size 256\u00021and16\u000216(note\nthat both settings share the same number of components)\nlearned with bootstrapped k-means are are shown in Fig.\n4a). In all tested settings, increasing the number of fre-\nquency pools helps to improve the classiﬁcation accuracy.\nBest results are achieved using two to four pools.\nIn Fig. 4b) the results of varying the length of the time\nwindows are depicted. Accuracy increases with shorter\ntime windows. Depending on the patch size, the optimum\nis reached with a window length of 1 second (16 \u000216) or\n2 seconds (256\u00021). This ﬁnding is in agreement with [5].\n5.4 PCA dimensionality reduction and dictionary size\nWe show the effect of varying the number of principal\ncomponents kept in Fig. 4c). In the previous experiments,\nwe used exactly as many principal components needed to\nexplain 95 % of the variance, which translates to keepingClassiﬁer Features Accuracy (%)\nLinear SVMConvolutional K-means\n(16\u000216) (our)85:25 \u00063:5\nRBF SVM DBN [4] 84.3\nLinear SVM PSD on octaves [6] 83:4\u00063:1\nLinear SVMConvolutional K-means\n(128\u00021) (our)83:37 \u00062:54\nLinear SVM PSD on frames [6] 79:4\u00062:8\nTable 2: Our results (in bold) compared to previously pub-\nlished results that learn features on the GTZAN dataset.\nWe report the averaged accuracy and standard deviation\nafter one run of 10-fold cross validation.\n88 principal components in case of the 16\u000216features and\n133 for the 256\u00021features. We ﬁnd that for both feature\nsizes the highest accuracy can be achieved by setting the\nnumber of principal components to 100.\nFinally, we evaluate the effect of varying the size of the\ndictionary learned. In Fig. 4d) the results of varying this\nnumber are depicted. Increasing the size of the dictionary\nsteadily improves recognition performance.\n5.5 Overall performance\nTo compare our results with previously published results\non the GTZAN dataset we learned 1600 features, used 4\nfrequency pools, time windows of 2 seconds length and\nkept the ﬁrst 100 (16 \u000216) and 72 (128\u00021) principal com-\nponents. Results are shown in Table 2. With features that\nspan time and frequency, we reach the best result on the\nGTZAN dataset compared with other approaches that learn\nfeatures from audio data. There are however approaches\nthat do not learn features in an unsupervised fashion, but\nfocus on sophisticated classiﬁers and signiﬁcantly outper-\nform our results (92:7% [2] ,92:4% [9]).\n5.6 Additional experiment using random features\nRecently randomly selected features were found to per-\nform well on object recognition benchmarks. Saxe et al.\n[10] attribute the success of random features to the convo-\nlutional pooling architecture they are used in. To investi-\ngate the role of convolutional feature extraction for audio\ndata, we performed a similar, additional experiment. We\nchose the same parameters as described in Section 5.5, but\ninstead of learning features, we randomly selected PCA\nwhitened patches without any further clustering and used\nthese as features. Indeed, we found that classiﬁcation ac-\ncuracy did not suffer signiﬁcantly (85:09% \u00063:56), but the\ninterpretability of the features is lost. This result underlines\nthe importance of convolutional feature extraction.\n6. DISCUSSION\nOur experiments indicate that convolutional extraction of\nlocal feature responses is a viable approach to increase\nrecognition accuracy for the task of genre prediction.\nWith convolutional extraction there is a trade off be-\ntween computational complexity and accuracy. Extracting1 2 4 8707274767880\nNumber of pools on the frequency axis\n(a)Classﬁcation Accuracy (%)16\u000216- K-means (bootstrapping)\n256\u00021- K-means (bootstrapping)\n30 5 2 1747678808284\nLength of the time windows\n(b)Classﬁcation Accuracy (%)16\u000216- K-means (bootstrapping)\n256\u00021- K-means (bootstrapping)\n200 400 800 1;60068707274767880\nSize of the dictionary\n(c)Classﬁcation Accuracy (%)16\u000216- K-means (bootstrapping)\n256\u00021- K-means (bootstrapping)\n50 100 150 200 25074767880\nNumber of components kept\n(d)Classﬁcation Accuracy (%)16\u000216- K-means (bootstrapping)\n256\u00021- K-means (bootstrapping)\nFigure 4: Classiﬁcation accuracies averaged over ten runs of 10-fold cross validation with (a) varying number of frequency\npools, (b) varying time window lengths, (c) varying dictionary size and (d) varying number of components kept.\nfeatures convolutionally with a stride of 1 is computation-\nally more expensive than extracting non-overlapping fea-\nture responses. To speed up the feature extraction we tried\nto reduce the size of the spectrograms to a quarter of their\noriginal size (128\u0002323), which helps twofold. For one,\nthe number of patches that need to be extracted decreases\nand we are able to learn smaller patch dimensions, which\nspeeds up ﬁnding the closest centroids. We found that ac-\ncuracy did only decrease marginally to 84:77%\u00062:6, when\nlearning patches of size 8\u00028on the smaller input. Another\nway of speeding up the extraction is to increase the stride s.\nFigure 5: Example features learned on frequency patches\n32\u00021(enlarged for better visualization).This however has a stronger effect on accuracy, which re-\nduces to 83:45%\u00063:3(16\u000216, same setting as in Section\n5.5) with a stride of 4.\nAnother important ﬁnding is that time-frequency fea-\ntures perform better in terms of accuracy than frame level\nfeatures. Nonetheless, features that span the whole fre-\nquency range have the advantage of being easily interpretable\nin musical terms (see Fig. 5 for exemplary features learned\nonly on the frequency axis). This is not the case for lo-\ncal time-frequency patches since the exact frequency is not\nencoded in those patches. They do however represent pat-\nterns of energy distribution over time that can occur at any\nfrequency, e.g. energy remaining constant at one frequency\n(a)\n (b)\n (c)\n (d)\nFigure 6: Examples of learned time-frequency features\n(enlarged for better visualization).(Fig. 6b), energy spreading across frequencies (Fig. 6c)\nand note onsets (Fig. 6a and d).\nFinally, we show the confusion matrix of the result that\nwas achieved with our best performing features in Fig. 3.\nGenres that have a low confusion rate include classic, jazz\nand metal, problematic are rock and pop songs. We be-\nlieve that the confusion patterns that occur are plausible,\ne.g. confusing metal with rock songs is a reasonable mis-\ntake, since both genres are closely related.\nBl Ro Di Hi Ja Re Po Co Cl Me\nBl 827 61 1 9 0 20 12 48 0 4\nRo 12 650 55 25 7 25 51 49 0 32\nDi 31 36 853 11 2 36 45 12 0 9\nHi 8 5 21 866 0 32 33 0 0 0\nJa 19 6 0 4 961 10 0 12 9 0\nRe 48 16 10 22 0 824 15 20 0 0\nPo 0 36 33 41 0 18 775 24 0 0\nCo 45 110 18 1 0 35 16 830 0 7\nCl 0 5 9 0 30 0 10 1 991 0\nMe 10 75 0 21 0 0 43 4 0 948\nTable 3: The confusion matrix for ten runs of 10-fold cross\nvalidation using features of size 16\u000216. Genres that have\na low confusion rate include classic, jazz and metal, prob-\nlematic are rock and pop songs.\n7. CONCLUSION\nIn this work, we have presented an approach that predicts\nthe genre of a music piece. We have shown that learn-\ning local features using simple and fast techniques like\nk-means or even randomly selected features is competi-\ntive with other more complex learning approaches, if fea-\ntures are extracted convolutionally. We found that time-\nfrequency patches perform better than one dimensional fre-\nquency patches and that they reach the highest accuracy to\ndate compared with other learned features on the GTZAN\ndataset. Furthermore, we have shown that features learned\non the CQT transformed audio signal perform better than\nthose learned on the STFT spectrogram. We consider as\ninteresting future work to apply the feature learning to dif-\nferent tasks in the domain of music information retrieval,\ne.g. auto tagging and also to investigate the possibility of\nlearning a deeper representation on top of the low level\nfeatures learned so far.\n8. REFERENCES\n[1] Manuel Blum, Jost Tobias Springenberg, Jan W ¨ulﬁng,\nand Martin Riedmiller. A Learned Feature Descriptor\nfor Object Recognition in RGB-D Data. In IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), 2012.\n[2] Kaichun K. Chang, Jyh-Shing Roger Jang, and\nCostas S. Iliopoulos. Music genre classiﬁcation via\ncompressive sampling. In ISMIR, pages 387–392. In-\nternational Society for Music Information Retrieval,\n2010.\n[3] Adam Coates, Honglak Lee, and Andrew Y . Ng. An\nanalysis of single-layer networks in unsupervised fea-ture learning. In Proceedings of the 14th Interna-\ntional Conference on Artiﬁcial Intelligence and Statis-\ntics (AISTATS), 2011.\n[4] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR), pages 339–\n344, August 2010.\n[5] Philippe Hamel, Simon Lemieux, Yoshua Bengio, and\nDouglas Eck. Temporal pooling and multiscale learn-\ning for automatic annotation and ranking of music au-\ndio. In In Proceedings of the 12th International Confer-\nence on Music Information Retrieval (ISMIR11), 2011.\n[6] Mikael Henaff, Kevin Jarrett, Koray Kavukcuoglu, and\nYann LeCun. Unsupervised Learning of Sparse Fea-\ntures for Scalable Audio Classiﬁcation. In Proceedings\nof the International Society for Music Information Re-\ntrieval, 2011.\n[7] Aapo Hyv ¨arinen and Erkki Oja. Independent compo-\nnent analysis: algorithms and applications. Neural net-\nworks, 13(4-5):411–30, 2000.\n[8] Honglak Lee, Yan Largman, Peter Pham, and An-\ndrew Y . Ng. Unsupervised feature learning for au-\ndio classiﬁcation using convolutional deep belief net-\nworks. In Advances in Neural Information Processing\nSystems 22, pages 1096–1104, 2009.\n[9] Yannis Panagakis, Constantine Kotropoulos, and Gon-\nzalo R. Arce. Music genre classiﬁcation using local-\nity preserving non-negative tensor factorization and\nsparse representations. In ISMIR, pages 249–254. In-\nternational Society for Music Information Retrieval,\n2009.\n[10] Andrew Saxe, Pang Wei Koh, Zhenghao Chen, Ma-\nneesh Bhand, Bipin Suresh, and Andrew Ng. On ran-\ndom weights and unsupervised feature learning. In\nProceedings of the 28th International Conference on\nMachine Learning (ICML-11), ICML ’11, pages 1089–\n1096, June 2011.\n[11] Jan Schl ¨uter and Christian Osendorfer. Music Simi-\nlarity Estimation with the Mean-Covariance Restricted\nBoltzmann Machine. In Proceedings of the 10th Inter-\nnational Conference on Machine Learning and Appli-\ncations (ICMLA 2011), 2011.\n[12] A. Schoerkhuber, C. and Klapuri. Constant-Q trans-\nform toolbox for music processing. In 7th Sound and\nMusic Computing, 2010.\n[13] Klaus Seyerlehner, Gerhard Widmer, and Peter Knees.\nFrame level audio similarity - a codebook approach. In\nProc. of the 11th International Conference on Digital\nAudio Effects (DAFx-08), 2008.\n[14] George Tzanetakis and Perry Cook. Musical Genre\nClassiﬁcation of Audio Signals. IEEE Transactions on\nSpeech and Audio Processing, 10(5):293–302, 2002."
    },
    {
        "title": "Cross-cultural Music Mood Classification: A Comparison on English and Chinese Songs.",
        "author": [
            "Yi-Hsuan Yang",
            "Xiao Hu 0001"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1416666",
        "url": "https://doi.org/10.5281/zenodo.1416666",
        "ee": "https://zenodo.org/records/1416666/files/YangH12.pdf",
        "abstract": "Most existing studies on music mood classification have been focusing on Western music while little research has investigated whether mood categories, audio features, and classification models developed from Western music are applicable to non-Western music. This paper attempts to answer this question through a comparative study on English and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxon- omy developed for English songs. Six sets of audio fea- tures commonly used on Western music (e.g., timbre, rhythm) were extracted from both Chinese and English songs, and mood classification performances based on these feature sets were compared. In addition, experi- ments were conducted to test the generalizability of clas- sification models across English and Chinese songs. Re- sults of this study shed light on cross-cultural applicabil- ity of research results on music mood classification.",
        "zenodo_id": 1416666,
        "dblp_key": "conf/ismir/YangH12",
        "keywords": [
            "music mood classification",
            "Western music",
            "non-Western music",
            "English songs",
            "Chinese songs",
            "existing mood taxonomy",
            "audio features",
            "classification models",
            "generalizability",
            "cross-cultural applicability"
        ],
        "content": "CROSS-CULTURAL MUSIC MO OD CLASSIFICATION:  \nA COMPARISON ON ENGLIS H AND CHINESE SONGS \nYi-Hsuan Yang Xiao Hu \nAcademia Sinica  \nyang@citi.sinica.edu.tw  University of Denver  \n xiao.hu@du.edu  \nABSTRACT \nMost existing studies on music mood classification have \nbeen focusing on Western music while little research has investigated whether mood categories, audio features, and \nclassification models developed from Western music are \napplicable to non-Western music. This paper attempts to answer this question thro ugh a comparative study on \nEnglish and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxon-omy developed for English songs. Six sets of audio fea-\ntures commonly used on Western music (e.g., timbre, \nrhythm) were extracted from both Chinese and English \nsongs, and mood classification performances based on \nthese feature sets were compared. In addition, experi-ments were conducted to test the generalizability of clas-\nsification models across English and Chinese songs. Re-\nsults of this study shed light on cross-cultural applicabil-ity of research results on music mood classification. \n1. INTRODUCTION \nThere have been a number of studies on music mood classification in the Music Information Retrieval (MIR) \ncommunity in recent years [7][17]. However, most of ex-\nisting studies have focused on Western music, in particu-\nlar English songs. The two mood-related tasks in the Mu-\nsic Information Retrieval Evaluation eXchange (MIREX): \nAudio Mood Classification (AMC) and Audio Tag Clas-sification (mood tag set) have been using datasets consist-\ning of Western music [3]. A lthough such research activi-\nties have shown promising performances on classifying Western music by mood, there is little research on wheth-\ner and how the mood categories and techniques applied to Western music can be equally well applied to non-\nWestern music. This study aims to bridge the gap using \nChinese contemporary pop songs as a case of non-\nWestern music. In particular, three research questions are \nanswered in this study:  \n1) How well mood categories developed from English songs can be applied to Chinese songs and what are \nthe differences of mood distributions among Chinese \nand English songs?  2) Are audio features commonly used in mood classifica-\ntion of Western songs applicable to Chinese songs? \n3) Can prediction models built using English songs be reliably applied to Chinese songs?        \nAnswers to these questions will further our under-\nstanding on cross-cultural generalizability of music mood categories, audio features and classification models.  \n2. RELATED WORK \n2.1 Mood Categories in Western and Chinese Music \nIn building datasets for evaluating mood classification algorithms, MIR researchers have used a variety of mood \ncategories. Quite a number of studies have used four cat-\negories derived from the four quadrants of Russell’s va-\nlence-arousal  dimensional model [12]:\n contentment, de-\npression, exuberance, and anxious/frantic  (or similar \nones). It is noteworthy that these four categories have \nbeen used for both Western music [10][14] and non-Western (e.g., Chines e) music [4][14].  \nIn the industry, online music repositories may organ-\nize music by mood. For example, allmusic.com, a large and influential online repository for Western popular mu-\nsic, provides 182 mood labels that are applied to songs \nand albums by professional music editors. In fact, the mood categories used in the AMC task in MIREX (cf. \nTable 1) were based on the most popular mood labels on \nallmusic.com [3]. However, no research has been done to investigate whether these mood labels are suitable for \nnon-Western music. In this study, we take on this chal-\nlenge using Chinese pop music as a case (see Section 3).  \n2.2 Mood Classification of Western Music \nQuite a number of studies have been conducted on auto-\nmated mood classification on Western Music, as re-viewed in [7][17]. Most of these studies extracted acous-\ntic features from music audio files. Some studies com-\nbined acoustic features with features extracted from other \ninformation sources such as lyrics and social tags [2][9].  \nAs one of the first studies comparing mood classification \ntechniques on Western and no n-Western music, this pa-\nper focuses on acoustic features and leave it to future work to compare approaches using combined information modals.   \nThe classification models often used include neural \nnetwork, k-nearest neighbor (k-NN), maximum likeli-hood, decision tree, and support  vector machines (SVM). \nOf these, SVM seems the most  popular due to its reliable \nclassification performance. In this study, we use SVM as \n \nPermission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for prof it or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page.  \n© 2012 International Society for Music Information Retrieval    \n \nthe classification model for a ll the experiments as our fo-\ncus is on the generalizability of acoustic features.  \n2.3 Mood Classification  of Chinese Music \nThere have been few studies on mood classification of \nnon-Western music and they are predominately on Chi-\nnese music. Hu et al . [4] and Xia et al.  [15] classified \nChinese pop songs using lyric features. Yang et al. com-\nbined lyrics and audio features to classify Chinese pop songs [17].  It is noteworthy that all these previous stud-ies used four or fewer mood categories (e.g., two catego-\nries were used in [15], “lighthearted” and “heavy-\nhearted.”). Moreover, none of these studies compared Chinese music to Western music on either mood catego-\nries or mood classification techniques and performance.  \n2.4 Comparison between Mood Classification on \nWestern and Chinese Music \nTo the best of our knowledge, [14] is the only existing \nstudy that evaluated the same mood classification tech-niques on both Western and Chinese music. Wu et al.  [14] \nextracted three acoustic feature sets of pitch, rhythm and \ntimbre from 20 pieces of Chinese traditional instrumental music and 20 pieces of Western classical music. They \nused a Bayesian probabilistic structure to classify the \nmoods in these pieces into four mood categories based on the Russell model and the results indicated better classifi-\ncation performance on Western pieces. At the same time, \nthe authors found “different identification about the \nmoods between the Oriental and Western culture” (p.152).    \nOur study differs from this prior work in the following \naspects. First, the music we  focus on is popular music \nfrom Western and Chinese cultures. Second, while [14] \nused four mood categories, we compare more than 30 mood labels in terms of their distributions in Chinese and \nEnglish songs. Third, the dataset in [14] was evidently \ntoo small (20 pieces in each culture) to draw reliably con-clusions, whereas our study uses a dataset of 500 Chinese \npieces and even more English pieces. Fourth, in addition \nto the rhythm and timbre feat ures examined in [14], our \nstudy also examines dynamics, MFCCs, psychoacoustic and tonal features. Fifth, besides comparing classification performances on music in e ach culture, our study also \ninvestigates the generalizability of classification models built with music in one culture being applied to music in another culture (cf. Section 4.2). \n3. MOOD CATEGORIES IN CHINESE SONGS \nThis section describes how we determine the mood cate-gories and obtain the mood annotations of Chinese songs \nfrom the experts we recruited and those of English songs \nfrom allmusic.com. \n3.1 Allmusic.com Mood Labels and Translation \nTo answer our first research question, we chose mood \nlabels contained in the five mood clusters used in the AMC task in MIREX [3]. This is due to the following reasons. First, the AMC mood clusters were derived from \nthe most popular mood labels on allmusic.com, one of the \nmost popular sites for Western music, and thus they are representative to the mood of Western songs. Second, \nallmusic.com provides “top songs” for each of its mood \nlabels, which empowers us to obtain English songs with \nexpert-annotated mood labels. Third, the 29 mood labels \nin AMC mood clusters (see Table 1) are of finer granular-ity than the commonly used four categories based on \nRussel’s model [12]. Previous studies based on the four categories were not able to discern the difference of mood distributions among Western and Chinese songs \n[14]. Fourth, as pointed out by music psychology re-\nsearch [6], classical models like Russell’s may not reflect the social context of everyday music listening. Since \nallmusic.com serves a large quantity of listeners in real \nlife, its mood labels are closer  to the reality of music lis-\ntening at present time.   \nC1\nRowdy, rousing, confident, boisterous, passionate \nC2Amiable/good natured, sweet, fun, rollicking, cheerful \nC3Literate, wistful, bittersweet , autumnal, brooding, poignant\nC4Witty, humorous, wh imsical, wry, campy, quirky, silly \nC5Volatile, fiery, visceral, aggr essive, tense/a nxious, intense \nTable 1.  Mood categories used in MIREX AMC task [3]. \nIt is noteworthy that, in discussions with music experts \n(see below), seven additional labels were added in be-\ncause the experts believed they might be important to \nrepresent moods in Chinese pop songs: “calm/peaceful,” \n“dreamy”, “encouraging,” “nostalgic,” “relaxed,” “sooth-\ning,” and “tender,” making 36 mood categories in total. \nExcept for “encouraging” and “tender”, the other five la-\nbels had appeared on allmusic.com at various time points (as allmuic.com changes its mood labels from time to \ntime). \nThe mood categories were translated into Chinese for \nconsistent understanding among the annotators who are \nnative Chinese speakers. The translation was first done by one of the authors who is a native Chinese speaker and \nfluent in English. The translation and the original English \nterms were then examined by three expert annotators. The four of them discussed several difficult cases (e.g., \n“autumnal,” “visceral”) befo re reaching agreements on \nthe translations.  \n3.2 Selection of Chinese Songs \nThe Chinese pop songs were co llected from an in-house \ncollection which contains albums released in Taiwan, \nHong Kong and Mainland China during the years from \n1987 to 2010. To maximize the diversity of songs, we \nselected one song from each album to be included in this study. Songs with non-Chinese (mostly English, some in \nJapanese) titles were eliminated because they were not in \nChinese despite being sung by Chinese artists. This pro-cess resulted in 500 Chinese songs.     \n \nAn excerpt of 30 seconds was extracted from each \nsong, for the purposes of limiting the burden of human \nannotators and mitigating the cases where mood changes during the entire course of a song [7]. Using 30 second \nexcerpts rather than entire songs is a common practice in \nMIR, but it remains in debate which 30 second segment \nshould be chosen [17]. For the purpose of mood classifi-\ncation, we decided to choose the segment with strongest emotion from each song. Speci fically, we used a sliding \nwindow of 30 seconds to exhaustively extract all 30 se-cond segments from each song, and then used a regres-sion model to predict the valence  and arousal  values of \neach segment. The segment with highest (| valence |\n2 + \n|arousal |2) value was chosen to represent each song. \nPlease note that the regression model was built on an ex-\nternal dataset of Western pop music [16], as there were \nno Chinese songs with proper valence  and arousal  anno-\ntations that could be used to train the regression model.  \n3.3 Annotation of Chinese Songs \nTo ensure the quality of annotation and to reduce the var-\niance of annotations across annotators, this study adopts \nthe approach of expert annotation.  \nThree experts were recruited from a university in the \nUnited States. All of them were female, Chinese (Manda-rin) native speakers, raised in Mainland China, majored \nin music, and fans of Chinese pop music. Table 2 shows \ntheir demographic and background information.  \nID Age Special-\nty Year in \ncollege Years in \nthe US Freq. of listening to \nChinese pop music \n1 23 Theory Senior 4 Several times a week\n2 25 Violin Graduate 0.5 Daily \n3 25 Vocal Graduate 3 Several times a week\nTable 2.  Information about the experts . \nThe annotation was conducted through a web-based \nsurvey system. Figure 1 show s the interface of annotating \na piece. One or more mood labels could be applied to \neach music piece. This is more in accordance to the reali-ty where a song can expres s multiple moods [2].  \n \nFigure 1 . Screenshot of the annotation interface. \nBefore the annotation started, all three experts met to-\ngether with one of the authors for a training session. An-notation requirements were made clear during the session, including “focusing on the mood expressed by the pieces \ninstead of mood induced in yourselves;” “making use of the lyrics but without looking them up anywhere;” “ig-noring the order by which the mood labels are arranged.” \nAlso in the training session, the experts listened to eight 30 seconds long pieces and discussed which moods each \npiece expressed. Half of the eight pieces were English \nsongs with very different moods selected from a previous \nstudy [16] and the other half were randomly selected \nfrom the Chinese pieces to be annotated. Through the discussion, the experts reached better agreement on musi-\ncal meanings of the mood labels.     \nEach piece was assigned to one expert and the three \nexperts annotated 150, 200 and 150 pieces respectively. Using one human judge’s opinion as gold standard for \nevaluation has been verified to be effective in the domain of text information retrieval [13]. Admittedly, this has yet \nto be verified in MIR, which will be our future work. The \nexperts reported that each song took each of them about 1 minute to annotate and were paid for their work on an \nhourly basis.  \n3.4 Mood Categories in Chinese and English songs \nA total of 2,453 mood labels were applied to the 500 \npieces, with one piece getting 1 to 13 labels. On average, \neach piece had 5 labels (standard derivation: 2.08). Each of the 36 mood labels were applied to 6 to 202 pieces, \nwith an average of 68.14 pieces each label (standard deri-\nvation: 40.85). The most popular mood labels among the \n500 Chinese songs were “tender” (202), “wistful” (164), \nand “passionate” (128). The least popular mood labels were “volatile” (6), “wry” (6), and “nostalgic” (8). The \ndistribution of Chinese songs across mood categories is \nshown in the left panel of Figure 2.  \nAs comparison, we counted the number of “Top \nSongs” provided by allmusic.com for each of the mood labels, as shown in the right panel of Figure 2. Note that the numbers of Chinese songs in Figure 2 are limited to \nthe songs available to us. While allmusic.com has a much larger song pool, they only provided up to 100 top songs \nfor each mood label. Despite this, it is still clear from \nFigure 2 that there are more Chinese songs than English songs labeled with “relaxed,”  “wistful,” “passionate,” \n“brooding,” or “rousing.” On  the other hand, there are \nmuch fewer Chinese songs labeled with “wry,” “vola-tile,” “humorous,” “aggressive,” and “fiery.” Such differ-\nence might reflect the differences between the Chinese \nand Western cultures: the Chinese culture tends to re-strain the expression of feelings and Chinese people are \nmore introverted compared to Western people [11]. An-\nother possible reason for the absent of radical moods (e.g., \n“aggressive,” “fiery”) in Chinese songs might be that \nChinese popular music has a sh orter history comparing to \nWestern one and the whole mood spectrum is yet to be \ndeveloped. \nTo further illustrate the ap plicability of the mood cate-\ngories to Chinese songs, we also examined and compared the relative distance among mood categories based on the \ntwo datasets. The distance between a pair of mood cate-gories was calculated based on the common songs shared   \n \nby them. We then projected the mood labels based on \ntheir distances to a 2-D space using multidimensional \nscaling (MDS) for each of the two datasets, as shown in Figure 3. It can be found that the relative positions of \nmood labels for both sets share some similarity: “aggres-\nsive,” “fiery,” “intense” and “volatile” are clustered to-\ngether and away from low arousal moods such as “amia-\nble” or “wistful.” Another two clusters shared by both plots are also in line with common sense:  “rollicking,” \n“cheerful,” “passionate” and “r ousing”; “literate,” sooth-\ning” and “bittersweet”.  \n \nFigure 2 . Song distribution across categories. \nThe two plots in Figure 3 also differ in several ways. \n“Dreamy,” “brooding” and “c heerful” are close to one \nanother in the English dataset but are separated out in the Chinese dataset which seems more intuitive. As another \nexample, the English set puts “campy” and “witty” to-gether with “cheerful” and “amiable” while the Chinese \nset separates them apart. This difference might be related to the cultural contexts. The Chinese experts interpreted \nthe first two words as neutral and the last two as positive. \nHowever, an English native speaker said he would asso-ciate the four terms to “fun” although the first two terms \nwere more of a kind of “dark” fun. In sum, the relative \ndistance among mood labels in the Chinese set, although somewhat different from that in the English set, seems \nagreeable with the semantic  meanings of the terms. \n \n(a) Mood distance in the Chinese dataset \n \n(b) Mood distribution for the English dataset \nFigure 3 . Projection of mood categories to a 2-D space.  \n From the comparisons of song distributions across \nmood categories as well as the relative distance between \nthem, we can see that the mood categories used to de-\nscribe English songs are generally applicable to Chinese \nsongs with exceptions of radical moods such as “fiery” and “volatile.”   \n4. CLASSIFICATION EXPERIMENTS \nTo answer research questions 2 and 3, we conducted mood classification experiments on Chinese and English \nsongs. To build a dataset of English songs, we collected \nthe “Top song” lists from allmusic.com for mood labels used in this study and then obtained 30-second audio pre-\nviews from 7digital.com which boasts itself as “a leading \ndigital media delivery company.” A total of 1,520 Eng-\nlish song clips were collected. \nAll experiments were set up as binary classifications \nfor each mood category, without considering possible correlations between categories. Positive examples of a \nmood category are songs labelled with that category \nwhile negative examples are randomly selected from \nsongs labelled with other categories. Positive and nega-  \n \ntive examples are balanced both in training and test sets. \nThe classification is carried out by SVM, with RBF ker-\nnel and the two parameters C and γ tuned. Each experi-\nment was repeated 20 times and average accuracy values \nare reported. In our evaluation, we only used the mood \nclasses with more than 20 positive examples in both Chi-\nnese and English datasets. Throughout the experiments, \nthe datasets were split into 50% training and 50% test. \n4.1 Effectiveness of Acoustic Features \nIn view of the complexity of mood perception, it is diffi-\ncult to find a universal feature representation that well \ncharacterizes every mood. In addition, the perceptions of \ndifferent moods in music are usually associated with dif-\nferent patterns of acoustic cues [5]. We therefore extract-ed acoustic features that represent various perceptual di-\nmensions of music listening and trained classifiers using \nfeatures of each perceptual dimension separately. \nA total of six feature sets were examined in this study, \nas summarized in Table 3. They were extracted using the MIR toolbox [8] and the PsySound toolbox [1]. These features have been used extensively in previous work on \nmood classification [7][17].  \nFeature Type Dim Description \nRMS Energy 2 The mean and standard deviation o f \nroot mean square energy \nPHY Rhythm 5 Fluctuation pattern and tempo [8] \nPCP Pitch 12 Pitch class profile, the intensity o f \n12 semitones of th e musical octave \nin Western twelve-tone scale [8] \nTON Tonal 6 Key clarity, musical mode (ma-\njor/minor), and harmonic change \n(e.g., chord change) [8] \nMFCC Timbre 78 The mean and standard deviation o f \nthe first 13 MFCCs, delta MFCCs, \nand delta delta MFCCs \nPSY Timbre 36 Psychoacoustic features including the perceptual loudness, volume, \nsharpness (dull/sharp), timbre \nwidth (flat/rough), spectral and to-nal dissonance (disso-nant/consonant) of music [1] \nTable 3.  Feature representations adopted in our study.  \nFigure 4(a) shows the average classification accuracy \nof the binary classification tasks on the two datasets. The \nsix audio descriptors performed well in both sets and \neven better for the Chinese songs. The relative perfor-\nmances among the feature sets are similar in both datasets: timbre descriptors performed better than energy or \nrhythm related descriptors. The fact that PCP and TON features worked well for Chin ese songs reflects that the \ncomposition of contemporary Chinese pop songs is influ-enced by the Western twelve-t one scale. In addition, we \nfind that PSY performed the best for both datasets with \nan average accuracy of 74.7 % and 65.8%, respectively. \nThe performance differences between PSY and the first \nfour features sets are significant (pair-wise t -test, p < 0.001). This shows that the psychoacoustic features seem \nto be generally applicable to Chinese songs [1].  \nIt is interesting that significantly better performance \n(pair-wise t-test, p < 0.001) is obtained for Chinese songs \nthan for English songs. Although both datasets were an-\nnotated by experts (recruited by allmusic.com and by us, \nrespectively), the allmusic.com song lists contain “tier1” \n(more representative) and “tier2” (less representative) songs. Our inclusion of tier2 songs may have introduced \nnoise to the English dataset. In contrast, all the Chinese \nsongs were annotated with th e same criteria. The perfor-\nmance difference may also result from the fact that the \nexcerpts of the Chinese songs were segments with strong \nemotion, whereas the English excerpts were provided by 7digital which may not represent the full songs annotated \nby allmusic.com experts.  \n4.2 Cross-cultural\n Applicability  of Classifiers \nIn this subsection, we report the result of using classifiers \ntrained from English songs to classify Chinese songs, and \nvice versa. This set of experiments is designed to study the cross-cultural applicability of classification models \nwhich has rarely been addres sed in the literature. Figure  \n4(b) shows the performances across the six feature sets. \nPSY was again the best performing feature set, with \naverage accuracies of 63.3% and 59.4% for Chinese and \nEnglish test data, respectively. The performance differ-ences between PSY and other feature sets are significant \nwhen Chinese songs are used as the test set (pair-wise t-\ntest, p < 0.001). While the pe rformances are comparable \nto those in the literature [3][17], they are significantly worse than those of last ex periments where training and \ntesting data were drawn from the same dataset (pair-wise \nt-test, t = 5.92 for Chinese songs; t = 5.09 for English \nsongs, df = 25, p < 0.001). This means the datasets in the two cultures have significant difference. Whenever pos-\nsible, it is better to use songs in the same culture to train \nclassification model. However, in cases when training \ndata from the same culture as test data are not available, it \nis still an acceptable alternative to use classification mod-els built with data in the other culture.  \nUsing English songs as training data to classify Chi-\nnese songs performed significantly better than the other way around (pair-wise t -test, t = 3.70, df = 25, p = 0.001). \nThis may be because there were more English songs making larger training sets and/or mood classification of Chinese songs seemed to be easier (c.f. Section 4.1). \nTo further examine possible differences in acoustic \ncharacteristics between English and Chinese songs, we \napplied MDS to project the PSY features of the songs to a \n2-D space (Figure 5). The fact that the two datasets over-\nlap to a large extent indicates that Chinese songs and \nEnglish songs in our datasets have similar perceptual \ntimbre quality (as depicted by the PSY features [1]). This may partially explain the fact that PSY features per-\nformed well in both English and Chinese songs as well as \nthe cross-cultural experiments.   \n \n     \n  \n(a)                                                                                          (b) \nFigure 4 . Average accuracy of different feature sets for mood cla ssification of (a) intra-cultural classification using Chi-\nnese and English datasets and (b) inter-cultural classification using the other set as training data.  \n \nFigure 5 . Visualizing PSY features of the two datasets. \n5. CONCLUSIONS AND FUTURE WORK \nThis study investigates the cross-cultural applicability of \nmood categories, acoustic features and classification \nmodels in the case of English and Chinese songs. Results show that mood categories found in English songs are \ngenerally well applicable to Chinese songs except for several categories representing radical moods. It also seems feasible to apply feature descriptors developed for \nEnglish songs to represent the audio content of Chinese \nsongs, possibly due to the overlap of Psychoacoustic tim-\nbre features in both datasets . Our cross-cultural evalua-\ntion showed significant degradation of classification per-\nformance compared to the result of within-culture evalua-\ntion, although the absolute accuracy values are still com-\nparable to the state-of-the-art in the literature.  \nIn future work, we will examine the cross-cultural ap-\nplicability of audio features and classification models on individual mood categories. We also plan to explore the problem of predicting the valence and arousal values of \nChinese songs and investigate whether techniques that \nworked for Western music will work for Chinese music.   \n6. REFERENCES \n[1] D. Cabrera: “Psysound: A computer program for psycho-\nacoustical analysis,” in Proc. Australia Acoustic Society \nConf. , 1999.  \n[2] X. Hu and J. S. Dowine: “Improving Mood Classification \nin Music Digital Libraries by Combining Lyrics and \nAudio,” in Proc. ACM/IEEE Joint C onf. Digital Libraries \n(JCDL), pp. 159–168, 2010. [3] X. Hu, J. S. Dowine, C. Laurier, M. Bay, and A. F. \nEhmann: “The 2007 MIREX audio mood classification \ntask: Lessons learned,” in Proc. ISMIR , 462–467, 2008. \n[4] Y. Hu, X. Chen, and D. Yang, “Lyric-based song emotion detection with affective le xicon and fuzzy clustering \nmethod,” in Proc. ISMIR, 2009. \n[5] P. Juslin, “Cue utilization in communication of emotion in \nmusic performance: Relating performance to perception,” J. Experimental Psychology , vol. 16, no. 6, 2000. \n[6] P. N. Juslin and P. Laukka,  P.: “Expression,  perception, \nand induction of musical emotions: a review and a \nquestionnaire study of everyday listening,” J. New Music \nResearch , vol. 33, no. 3, pp. 217–238, 2004. \n[7] Y. E. Kim et al : “Music emotion recognition: A state of \nthe art review,” in Proc. ISMIR , 2010. \n[8] O. Lartillot and P. Toiviainen: “MIR in Matlab (II): A \ntoolbox for musical feature extraction from audio,” in \nProc. ISMIR, pp. 127–130, 2007. \n[9] C. Laurier, J. Grivolla, and P. Herrera, “Multimodal music mood classification using audio and lyrics,” in Proc. Int. \nConf. Machine Learning and Applications , pp. 1–6, 2008. \n[10] L. Lu, D. Liu, and H. J.  Zhang: “Automatic mood \ndetection and tracking of music audio signals,” IEEE \nTrans. Audio, Speech, and Language Processing , vol. 14, \nno. 1, pp. 5–18, 2006 \n[11] R. R. McCrae, P. T. Costa, a nd M. Yik: ”Universal aspects \nof Chinese personality structure,” in M. H. Bond (Ed.) The \nHandbook of Chinese Psychology , pp. 189–207. Hong \nKong: Oxford University Press, 1996. \n[12] J. A. Russell: “A circum spect model of affect,” J. \nPsychology and Social Psychology , vol. 39, no. 6, 1980. \n[13] E. M. Voorhees a nd D. K. Harman: TREC: Experiments in \nInformation R etrieval Evaluation , MIT Press, 2005. \n[14] W. Wu and L. Xie: “Discr iminating mood tax onomy\n of \nChinese traditional music and Western classical music with content feature sets,” in Proc. IEEE Congress on \nImage and Signal Processing , pp. 148–152, 2008. \n[15] Y. Xia, L. Wang, K. Wong, and M. Xu: “Sentiment vector space model for lyric-based s ong sentiment classification,” \nin Proc. ACL , pp. 133–136, 2008. \n[16] Y.-H. Yang and H.-H. Chen: “Prediction of the \ndistribution of perceived musi c emotions using discrete \nsamples,” IEEE Trans. Audio, Speech, and Language \nProcessing, vol. 19, no. 7, pp. 2184–2196, 2011. \n[17] Y.-H. Yang and H.-H. Chen: “Machine recognition of \nmusic emotion: A review,” ACM Trans. Intelligent \nSystems and Technology , vol. 3, no. 3, 2012."
    },
    {
        "title": "Infinite Composite Autoregressive Models for Music Signal Analysis.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1414992",
        "url": "https://doi.org/10.5281/zenodo.1414992",
        "ee": "https://zenodo.org/records/1414992/files/YoshiiG12.pdf",
        "abstract": "This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonpara- metric Bayesian extensions of nonnegative matrix factor- ization (NMF) based on the source-filter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and filters) and time-varying gains of source-filter pairs. In this study we model musical instruments as autoregressive sys- tems that combine two types of sources—periodic signals (comb-shaped densities) and white noise (flat density)— with all-pole filters representing resonance characteristics. One of the main problems with such composite autore- gressive models (CARMs) is that the numbers of sources and filters should be given in advance. To solve this prob- lem, we propose nonparametric Bayesian models based on gamma processes and efficient variational and multiplica- tive learning algorithms. These infinite CARMs (iCARMs) can discover appropriate numbers of sources and filters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database.",
        "zenodo_id": 1414992,
        "dblp_key": "conf/ismir/YoshiiG12",
        "keywords": [
            "polyphonic",
            "audio signals",
            "fundamental frequencies",
            "nonnegative matrix factorization",
            "source-filter paradigm",
            "amplitude spectrogram",
            "time-varying gains",
            "musical instruments",
            "autoregressive systems",
            "composite autoregressive models"
        ],
        "content": "INFINITE COMPOSITE AUTOREGRESSIVE MODELS\nFOR MUSIC SIGNAL ANALYSIS\nKazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industria l Science and Technology (AIST), Japan\n{k.yoshii, m.goto }@aist.go.jp\nABSTRACT\nThis paper presents novel probabilistic models that can be\nused to estimate multiple fundamental frequencies (F0s)from polyphonic audio signals. These models are nonpara-metric Bayesian extensions of nonnegative matrix factor-\nization (NMF) based on the source-ﬁlter paradigm, and in\nthem an amplitude or power spectrogram is decomposedas the product of two kinds of spectral atoms (sources andﬁlters) and time-varying gains of source-ﬁlter pairs. In thisstudy we model musical instruments as autoregressive sys-tems that combine two types of sources—periodic signals(comb-shaped densities) and white noise (ﬂat density)—\nwith all-pole ﬁlters representing resonance characteristics.\nOne of the main problems with such composite autore-\ngressive models (CARMs) is that the numbers of sources\nand ﬁlters should be given in advance. To solve this prob-lem, we propose nonparametric Bayesian models based ongamma processes and efﬁcient variational and multiplica-\ntive learning algorithms. These inﬁnite CARMs (iCARMs)\ncan discover appropriate numbers of sources and ﬁlters in\na data-driven manner. We report the experimental resultsof multipitch analysis on the MAPS piano database.\n1. INTRODUCTION\nMultiple fundamental frequency estimation (a.k.a. multip-\nitch analysis ) is the basis of various kinds of music content\nanalysis.\nRecently, nonnegative matrix factorization (NMF)\nhas gained a lot of popularity [1–13]. The standard NMFapproximates an amplitude or power spectrogram (nonneg-ative matrix) as the product of two nonnegative matrices,one of which is a compact set of spectral bases and the\nother of which is a set of the corresponding time-varying\ngains [15, 16]. Such low-rank approximation is well justi-ﬁed by the fact that each musical piece consists of only lim-ited kinds of sounds that repeatedly appear.\nIn addition ,a\npractical advantage of NMF is that the bases and gains canbe alternately optimized by using efﬁcient iterative algo-rithms called multiplicative update (MU) rules. The stan-\ndard NMF, however, has three fundamental limitations:\n1. The spectral bases are time-invariant, and only their\ngains vary over time. A large number of independent\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or c ommercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2012 International Society for Music Information Retrieval.Comb-shaped excitations\nWhite noise (flat density)⊗\nsourcesI filtersJ\n…… IJgainsIJbases\n⊕⊗Combinatorial \nproductsAll-pole transfer functions\n(latent variables) (latent variables) Observed spectrum\nFigure 1 . Overview of composite autoregressive models:\nThe combinatorial products of Isources and Jﬁlters yield\nIJspectral bases, which are activated according to the cor-\nresponding time-varying gains at each frame. We take the\ninﬁnite limit as both IandJdiverge to inﬁnity.\nbases are needed to fully represent the timbral vari-ations of instrument spectra (e.g., envelopes) even ifthese spectra share the same fundamental frequency\n(F0). Such an unconstrained increase of model com-\nplexity is likely to result in optimization algorithmseasily getting stuck in bad local optima.\n2. A post-processing step for estimating the F0s from\nindividual bases is required because the F0s are notparameterized for representing the spectral bases. If\nthe shapes of spectral bases are unconstrained, the\nresulting bases often deviate from natural harmonicspectra. This makes F0 estimation difﬁcult and weneed to judge the existence of an F0.\n3. The number of bases (model complexity) should be\ncarefully speciﬁed in advance because it has a strongimpact on the decomposition results. Note that thislimitation is closely related to the ﬁrst. A naive solu-tion is to exhaustively test all possible complexitiesand ﬁnd an optimal value, but such model selection\nis often computationally impractical.\nAs noted above, unconstrained NMF is too ﬂexible for aset of musically-meaningful bases to be induced automati-cally. Although these limitations have partially been dealtwith in previous studies [1–13], no study has overcome all\nof them simultaneously in a principled manner.\nIn this paper we propose inﬁnite composite autoregres-\nsive models (iCARMs) (Fig. 1) developed for fusing the\nfollowing techniques into a uniﬁed Bayesian framework:\n1.Source-ﬁlter factorization (inspired by [1])\nWe further factorize the spectral bases as the combi-\nnatorial products of sources and all-pole ﬁlters. Thisidea originates in the autoregressive (AR) modeling\nof speech signals: various vowels can be generatedby changing the shape of the vocal tract (ﬁlter) whilekeeping the same F0 (source). This factorization en-\nables us to represent a wide variety of instrumental\nsounds in terms of two separate aspects (timbre andF0) with reasonable complexity.\n2.Harmonicity modeling (inspired by [7] and [9])\nWe represent each source as a comb-shaped functionthat uses an F0 parameter for representing equally-spanned harmonic partials of the same weight. Sincesuch sources are multiplied by acoustically-inspiredAR ﬁlters, the relative weights of partials of the basesare constrained to take realistic values as natural har-\nmonic sounds. In addition, we can directly optimize\nthe values of the F0s jointly with decomposition.\nAs proposed in [7, 14], we additionally introduce a\nspecial source representing white noise (ﬂat density).This enables us to deal with percussive and transient\nsounds having widely distributed spectra. Their tim-\nbres (envelopes) are characterized by AR ﬁlters.\n3.Bayesian nonparametrics (inspired by [12])\nWe build nonparametric Bayesian models that canautomatically adjust the numbers of sources and ﬁl-ters needed to factorize a given spectrogram. Ratherthan these numbers being speciﬁed, the inﬁnite limitof the conventional source-ﬁlter NMF [1] is taken asthe numbers of sources and ﬁlters diverge to inﬁnity.We perform sparse learning by introducing inﬁnite-\ndimensional priors in such a way that only limited\nnumbers of sources and ﬁlters are actually activated.\nTo optimize the iCARMs, we propose a new class of itera-tive algorithms that integrates a variational Bayesian (VB)\ntechnique with standard MU rules [8, 9].\nThe rest of this paper is organized as follows: Section 2\ndiscusses the positioning of this study. Section 3 presents\nthe iCARMs. Section 4 describes the evaluation. Section 5concludes the paper with a mention of future work.\n2. RELATED WORK\nThis section introduces two machine-learning (ML) tech-niques, i.e., NMF and Bayesian nonparametrics.\n2.1 Nonnegative Matrix Factorization\nNMF is a powerful tool for sparse decomposition of non-\nnegative matrix data [15]. It was ﬁrst used for representingface images as linear combinations of a compact set of ba-sis images corresponding to “local parts” such as eyes andnoses. Such parts-based sparse representation is sponta-\nneously induced by the nonnegativity constraint that allows\nonly summation of basis images. Therefore, NMF ﬁts nat-urally into audio spectrogram decomposition because theenergy of harmonic sounds is concentrated at the discretefrequencies of harmonic partials.\n2.1.1 Optimization Criteria\nTo perform NMF, we need some criterion for evaluating\nthe “goodness-of-ﬁt” of reconstructed data (linear combi-nations of spectral bases) to observed data (a spectrogram).\nMethod\n Divergence\n Sources #\n Filters #\nKameoka [1]\n IS\n -I\n ARJ\nBadeau [2]\n IS\n HI\n MAJ\nDurrieu [3]\n IS\n (H)I\n -J\nVirtanen [4]\n KL\n -I\n -J\nCarabias-Orti [5]\n KL\n HI\n -J\nHeittola [6]\n KL\n -I∗\n-J\nYasuraoka [7]\n KL\n H+N I∗\nARJ\nHennequin [8]\n Beta (0.5)\n -I\n ARMA J∗\nProposed\n KL or IS\n H+N∞\n AR∞\n(H: harmonic sources, N: noise source, -: others, ∗: varying over time)\nTable 1 . Several variants of source-ﬁlter NMF\nAs shown in Table 1, for example, Kullback-Leibler (KL)[15] and Itakura-Saito (IS) [16] divergences have been usedintensively. Some studies used beta divergence [17], whichincludes KL and IS divergences as special cases.\nIn the context of audio modeling, although IS-NMF is\njustiﬁed in theory (see [16] and Section 3.2.1), KL-NMFoften yields better results in a maximum likelihood estima-tion setting. One main reason is that the nonconvexity of ISdivergence makes it difﬁcult for gradient-descent-type op-timization algorithms to ﬁnd global optima. Note that nocomparative tests have been conducted under a Bayesian\nestimation setting. In this paper we formulate two kinds of\niCARMs, i.e., KL-iCARM and IS-iCARM.\n2.1.2 Source-Filter Factorization\nOne extension is obtained with the source-ﬁlter paradigm,\nas listed in Table1. Kameoka and Kashino [1],\nfor example ,\noriginally proposed the idea of the composite autoregres-sive model (CARM) using ﬁxed numbers of unconstrainedsources and autoregressive (AR) ﬁlters (all-pole transferfunctions). Although similar models were devised by someresearchers [3–5], ﬁlters were not acoustically constrained.Badeau et al. [2] used moving-average (MA) ﬁlters (all-\nzero transfer functions) with harmonic sources.\nSome NMF variants allow sources or ﬁlters to vary over\ntime to richly capture temporal variations of spectral basesat the cost of increasing complexity. Heittola et al. [6] and\nYasuraoka and Okuno [7] used time-varying sources whilea ﬁxed number of ﬁlters was shared over time. Hennequin\net al. [8], on the other hand, used time-varying ARMA ﬁl-\nters that could be estimated by efﬁcient MU rules.\n2.1.3 Harmonicity Modeling\nAnother extension is based on harmonicity constraints on\nspectral bases or sources. For example, Vincent et al. [10]\nand Bertin et al. [11] assumed each basis as a weighted\nsum of narrowband template spectra consisting of a fewadjacent harmonic partials. In the source-ﬁlter paradigm,Badeau et al. [2] represented each source as a binary vector\nwhose elements are determined by independent Bernoullitrials, where particular elements corresponding to harmonicpartials are more likely to take the value of 1. Yasuraoka\nand Okuno [7] and Hennequin et al. [9] represented each\nsource as a parametric function based on a (weighted) sum\nof atomic functions (e.g., Gaussian functions) correspond-ing to harmonic partials. Carabias-Orti et al. [5] proposed\nto further factorize a set of partials’ weights as a weightedsum of several patterns. Efﬁcient MU rules for estimatingthe parameters of the function were proposed in [5, 9].A key feature of [7] is to consider an additional source\nhaving a ﬂat density. This idea was inspired by the speech\nproduction mechanism. Excitation signals produced by vo-cal cords are roughly categorized into periodic signals (har-\nmonic comb-shaped spectra) and white noise (ﬂat spectra).\nThese signals are then articulated by the vocal tract whoseresonance characteristics can be represented by AR ﬁlters.This assumption is widely accepted as reasonable to someextent for music signal modeling. In this study we modelthis generative process in a Bayesian framework.\n2.2 Bayesian Nonparametrics\nAnother emerging ML technique is Bayesian nonparamet-\nrics [18], which is a generalization of the classical Bayesian\ntechnique. In the typical Bayesian framework, we put prior\ndistributions on unknown random variables of interests andthen, given observed data, estimate a posterior distributionover those variables. However, this framework cannot beused for determining model complexities (the numbers ofsources and ﬁlters in this study) because these complexi-ties are simply treated as hyperparameters. We thus have to\nuse an expensive grid search for combinatorial model se-\nlection. Bayesian nonparametrics enables us to treat model\ncomplexities as random variables and estimate their opti-mal values jointly with posterior computation.\nBayesian modeling is being used in music signal analy-\nsis, and Bayesian extensions of NMF [19] have been usedwith great success for audio decomposition (source separa-tion). An especially important breakthrough was recentlymade by Hoffman et al. [12]. They proposed a nonpara-\nmetric Bayesian extension called the gamma-process NMF\n(GaP-NMF) that in theory allows an observed spectrogram\nto contain an inﬁnite number of bases. A limited effectivenumber of bases can be obtained by using an efﬁcient vari-ational inference algorithm. This extension is the basis ofa more elaborate model that can consider inﬁnite kinds oftemporal variations of each basis [13].\n3. PROPOSED MODELS\nThis section presents new nonparametric Bayesian modelscalled inﬁnite composite autoregressive models (iCARMs).The essential concept of these models is inspired by a com-posite autoregressive model (CARM) [1] that decomposesapower spectrogram into ﬁxed numbers of sources and AR\nﬁlters by using IS divergence as an optimization criterion.\nWe formulate another CARM that decomposes an ampli-\ntude spectrogram by using KL divergence. To enforce har-\nmonicity we explicitly represent each source—except for a\nsingle source that has a ﬂat spectral density (white noise)—as a parametric comb-shaped function as proposed in [7].Finally, both KL-CARM and IS-CARM are extended to in\ntheory contain inﬁnite numbers of sources and ﬁlters by\nusing gamma processes as suggested in [12].\n3.1 Overall Framework\nWe ﬁrst deﬁne mathematical symbols as shown in Table 2.\nLetXbe anM×Ncomplex-valued spectrogram, where\nMis the number of frequency bins and Nis the number\nof frames. Let Ibe the number of sources and Jbe the\nM\n Number of frequency bins\nN\n Number of frames\nI\n Number of sources (diverges to inﬁnity)\nJ\n Number of ﬁlters (diverges to inﬁnity)\nXmn\n Amplitude (power) at m-th bin and n-th frame\nYmn\n Reconstructed value at m-th bin and n-th frame\nθi\n Global gain of i-th source\nφj\n Global gain of j-th ﬁlter\nWim\n Amplitude (power) of i-th source at m-th bin\nAjm\n Gain ofj-th ﬁlter at m-th bin\nHijn\n Gain ofi-th source & j-th ﬁlter pair at n-th frame\nTable 2 . Deﬁnition of mathematical symbols\nnumber of ﬁlters, which are assumed to go to inﬁnity. Let\nthe lower-case letters m,n,i,a n djindicate the indices.\nIn this paper we aim to factorize a nonnegative rep-\nresentation of X(amplitude or power spectrogram) into\nthree kinds of “factors” W,A,a n dHas follows:\n|Xmn|or|Xmn|2≈I,J→∞/summationdisplay\ni,jθiφjWimAjmHijn (1)\nwhereWim,Ajm,a n dHijnrespectively indicate the am-\nplitude (power) of the i-th source at the m-th bin, the gain\nof thej-th ﬁlter at the m-th bin, and the gain of the i-th\nsource and j- t hﬁ l t e rp a i ra tt h e n-th frame. In addition,\ntwo kinds of variables, θiandφj, are introduced to respec-\ntively indicate the global gain of the i-th source and the\nglobal gain ofj-th ﬁlter over all Nframes. Even when I\nandJdiverge to inﬁnity, ﬁnite numbers of the elements of\nθandφare expected to be substantially greater than zero\nwhile all other elements are negligibly small. This makes it\npossible for the “effective” numbers of sources and ﬁlters,I\n+andJ+, to be estimated in a data-driven manner.\nOur goal is, given the spectrogram X, to compute a pos-\nterior distribution p(θ,φ,H|X;W,A)over random vari-\nables and estimate parameters that represent WandA.W e\nwill discuss concrete forms of priors p(θ),p(φ),p(H),\nlikelihood p(X|θ,φ,H;W,A), and parametric functions\nofWandAaccording to KL or IS divergence.\n3.2 Mathematical Formulation\nWe explain the different formulations of iCARMs based on\nKL and IS divergences.\n3.2.1 Observation Likelihoods for X\nWe use KL or IS divergence as an optimization criterion.\nLetYmnbe/summationtext\nijYij\nmn,w h e r eYij\nmn=θiφjWimAjmHijn.\nWe aim to optimize Ymnsuch that the KL or IS divergence\nbetweenXmnandYmnis minimized, as shown in Eq.(1).\nThis is known to be equivalent to maximum likelihood esti-mation of a Poisson or exponential distribution having Y\nmn\nas its parameter, given an observation Xmn[16]. We here\nintroduce a complex-valued latent variable Xij\nmnthat indi-\ncates the contribution of the i-th source and j-th ﬁlter pair\ninXmnsuch that Xmn=/summationtext\nijXij\nmnis satisﬁed.\nThe KL-iCARM is based on an amplitude-additivity as-\nsumption; i.e., |Xmn|=/summationtext\nij|Xij\nmn|. This is obviously in-\ncorrect but is useful in practice. If |Xij\nmn|∼Poisson(Yij\nmn),\nthe reproductive property of the Poisson distribution leadsto|X\nmn|∼Poisson(/summationtext\nijYij\nmn), which means\n|Xmn|∼Poisson (Ymn) (2)The IS-iCARM is based on a complex-domain additivity\nassumption (see Section 3.2.5). If Xij\nmn∼Nc(0,Yij\nmn),\nthe reproductive property of the complex Gaussian leads\ntoXmn∼Nc(0,/summationtext\nijYij\nmn). This assumption, however,\nmay be violated when the sources are notstationary Gaus-\nsian noise (see Section 3.2.4). We nonetheless assume\n|Xmn|2∼Exponential (Ymn) (3)\n3.2.2 Gamma Process Priors on θandφ\nWe put gamma process (GaP) priors on inﬁnite-dimensionalvectorsθandφ. More speciﬁcally, we introduce indepen-\ndent gamma priors on elements of θandφas follows:\nθ\ni∼Gamma/parenleftBigα\nI,α/parenrightBig\n,φj∼Gamma/parenleftBigγ\nJ,γ/parenrightBig\n(4)\nAs the truncation level Idiverges to inﬁnity, the vector θ\napproximates an inﬁnite sequence drawn from a GaP withshape parameter α. I ti sp r o v e nt h a tt h e effective number\nof elements, I\n+, such that θi>/epsilon1 for some number /epsilon1>0\nis almost surely ﬁnite. If we set Ito be sufﬁciently larger\nthanα, we can expect that only a few of the Ielements of θ\nwill be substantially greater than zero. This condensationproperty enables sparse learning in an inﬁnite space. Thesame reasoning can be applied to the GaP on φ.\n3.2.3 Gamma Chain Priors on H\nTo impose smooth transitions on H, we put a gamma chain\nprior [20] on a temporal sequence of gains of each source-ﬁlter pair. More speciﬁcally, H\nijis modeled as follows:\nHij1∼Gamma(β,β/d)\nGijn∼Gamma(β,βHijn−1)\nHijn∼Gamma(β,βGijn) (5)\nwhereβis a hyperparameter that controls the strength of\nthe priors (degree of smoothness) and Gijnis an auxiliary\nvariable that imposes a positive correlation between tem-porally adjacent gains H\nijn−1andHijn(Eprior[Gijn]=\nH−1\nijn−1andEprior[Hijn]=G−1\nijn). Marginalizing Gijn\nout, we obtain a positively correlated Markovian transition\nkernel as p(Hijn|Hijn−1)=Γ(2β)\n2Γ(β)(Hijn−1Hijn)β\n(Hijn−1+Hijn)2βH−1\nijn.\n3.2.4 Comb-shaped Functions for W\nWe represent each harmonic source Wias a comb-shaped\nfunction that is the sum of HGaussian functions, where H\nis the number of harmonic partials. Speciﬁcally,\nWim=H/summationdisplay\nh=1exp/parenleftbigg\n−(m−hμi)2\n2σ2/parenrightbigg\n(6)\nwhereμiindicates F01andσindicates an energy diffusion\naround the frequencies of partials. Note that only the last\nsource is reserved as white noise, i.e., WIm=1.\n3.2.5 All-pole Transfer Functions for A\nWe assume each basis signal xij≡{xij\nt}2M\nt=1in a frame to\nbe represented as a P-order AR process as follows:\nxij\nt=−P/summationdisplay\np=1aj\npxij\nt−p+si\nt (7)\n1When the value of F0 is given by ˜μi[Hz],μi=˜μi/(r/2M)[bins],\nwhereris a sampling rate and 2Mis a window size of frequency analysis.wheresi≡{si\nt}2M\nt=1is a signal of the i-th source and\naj≡{aj\n0,···,aj\np}Tis a coefﬁcient vector of the j-th ﬁlter\n(aj\n0=1). Letwi≡{wi\nt}2M\nt=1be the autocorrelation of si\nand{Wim}2M\nm=1be a complex (amplitude) spectrum den-\nsity obtained by discrete Fourier transform (DFT) of wi.\nLet{Xij\nm}2M\nm=1be a complex spectrum density obtained by\nDFT ofxij.If the source signal siis a stationary Gaussian\nnoise , eachXij\nmis independently distributed as a complex\nGaussian Nc(0,Σij\nm),w h e r eΣijm=WimAjmand\nAjm=1\n/vextendsingle/vextendsingle/vextendsingle/summationtextP\np=0aj\npe−2πm\n2Mpi/vextendsingle/vextendsingle/vextendsingle2=1\naT\njUmaj(8)\nUmis a(P+1)×(P+1) Toeplitz matrix with [Um]pq=\ncos(2πm\n2M(p−q)). This means that |Xij\nm|2is distributed\nas an exponential distribution having WimAjmas its scale\nparameter. In other words, maximum likelihood estimation\nofajforxijis equivalent to minimizing the IS divergence\nbetween{|Xij\nm|2}M\nm=1and{WimAjm}Mm=12\n.\nIn the iCARMs based on KL and IS divergences, the\nabove discussion leads to the following formulations:\nAKL\njm=/radicalBigg\n1\naT\njUmajorAIS\njm=1\naT\njUmaj(9)\nA reason for taking the “root” in the KL-iCARM is that we\nassume an “amplitude” spectrogram as observed data.\n3.3 Variational and Multiplicative Optimization\nThe posterior over random variables p(θ,φ,H|X;W,A)\nandWandA(parameters μ,σ,a n da) are determined\nsuch that the log-evidence logp(X;W,A)is maximized.\nSince this cannot be analytically computed, we use an ap-\nproximate method called variational Bayes (VB), which re-\nstricts the posterior to a factorized form given by\nq(θ,φ,H)=/productdisplay\niq(θi)/productdisplay\njq(φj)/productdisplay\nijnq(Hijn) (10)\nand iteratively updates this form by monotonically increas-ing a lower bound\n3of the log-evidence, L,g i v e nb y\nlogp(X;W,A)≥E[logp(X|θ,φ,H;W,A)]\n+E[logp(θ)]+E[logp(φ)]+E[logp(H)]\n−E[logq(θ)]−E[logq(φ)]−E[logq(H)]≡L (11)\nThe iterative update rules are\nq(θ)∝exp(Eq(φ,H)[logp(X,θ,φ,H;W,A)])\nq(φ)∝exp(Eq(θ,H)[logp(X,θ,φ,H;W,A)])\nq(H)∝exp(Eq(θ,φ)[logp(X,θ,φ,H;W,A)]) (12)\nTo optimize WandA(μ,σ,a n da), we use multiplica-\ntive update (MU) rules inspired by [8, 9]. A general rule is\nobtained from the partial derivative of a “cost” function,\ne.g.,−L. For example, if we can write the derivative as the\ndifference of two positive terms, i.e.,−∂L\n∂μi=Gμi−Fμi,a n\nupdate rule for μiis given by μi←μi×Fμi\nGμi. Note that μi\nbecomes constant if the derivative is zero, and is updated inthe opposite direction of the derivative. We omit detailedderivations and only describe update rules below.\n2In linear predictive coding (LPC), the source signal siis generally\nlimited to white noise ( Wim=1). This is a conventional assumption.\n3More speciﬁcally, a further lower bound of Lshould be computed.3.3.1 Variational Updates for KL-iCARM\nThe variational posterior of each random variable is set to\nbe the same family as its prior distribution as follows:\nq(θi)=Gamma(aθ\ni,bθi),q(φj)=Gamma(aφ\nj,bφj)\nq(Hijn)=Gamma(aH\nijn,bHijn) (13)\nThe variational parameters are given by\naθ\ni=α\nI+/summationtext\nmnj|Xmn|λmnij\nbθ\ni=α+/summationtext\nmnjE[φjWimAjmHijn]\naφ\nj=γ\nJ+/summationtext\nmni|Xmn|λmnij\nbφj=γ+/summationtext\nmniE[θiWimAjmHijn]\naH\nijn=2β+/summationtext\nm|Xmn|λmnij (14)\nbH\nijn=βE[Gijn+Gijn+1]+/summationtext\nmE[θiφjWimAjm]\nwhereλmnij is an auxiliary variable given by\nλmnij∝exp(E[log(θiφjWimAjmHijn)]) (15)\n3.3.2 Variational Updates for IS-iCARM\nAs proposed in [12], the variational posterior of each vari-\nable is given by a generalized inverse-Gaussian (GIG) dis-\ntribution (see the Appendix) as follows:\nq(θi)=GIG(aθ\ni,bθi,cθi),q(φj)=GIG(aφ\nj,bφj,cφj)\nq(Hijn)=GIG(aH\nijn,bHijn,cHijn) (16)\nThe variational parameters are given by\naθi=α\nI,bθi=α+/summationtext\nmnjE[φjWimAjmHijn]\nξmn\ncθ\ni=/summationtext\nmnj|Xmn|2η2\nmnijE/bracketleftbig1\nφjWimAjmHijn/bracketrightbig\naφ\nj=γ\nJ,bφj=γ+/summationtext\nmniE[θiWimAjmHijn]\nξmn\ncφj=/summationtext\nmni|Xmn|2η2\nmnijE/bracketleftbig1\nθiWimAjmHijn/bracketrightbig\naH\nijn=2β, cHijn=/summationtext\nm|Xmn|2η2\nmnijE/bracketleftbig1\nθiφjWimAjm/bracketrightbig\nbHijn=βE[Gijn+Gijn+1]+/summationtext\nmE[θiφjWimAjm]\nξmn(17)\nwhereηmnij andξmnare auxiliary variables given by\nηmnij∝E[1\nθiφjWimAjmHijn]−1s.t./summationtext\nijηmnij=1\nξmn=/summationtext\nijE[θiφjWimAjmHijn] (18)\n3.3.3 Multiplicative Updates for KL- and IS-iCARMs\nThe MU rules for μ,σ,a n daare given by μi←G−1\nμiFμiμi,\nσ2←G−1\nσ2Fσ2σ2,a n daj←G−1\najFajaj,w h e r e\nFμi=/summationtext\nmnjhh(mVF\nmnij+hμiVG\nmnij)exp/parenleftbig\n−(m−hμi)2\n2σ2/parenrightbig\nGμi=/summationtext\nmnjhh(mVG\nmnij+hμiVF\nmnij)exp/parenleftbig\n−(m−hμi)2\n2σ2/parenrightbig\nFσ2=/summationtext\nmnijhVF\nmnij(m−hμi)2exp/parenleftbig\n−(m−hμi)2\n2σ2/parenrightbig\nGσ2=/summationtext\nmnijhVG\nmnij(m−hμi)2exp/parenleftbig\n−(m−hμi)2\n2σ2/parenrightbig\nFKL\naj=/summationtext\nmniθiφjWimHijnA3\njmUm\nGKL\naj=/summationtext\nmni|Xmn|λmnijA2jmUm\nFIS\naj=/summationtext\nmniE[θiφjWimHijn]\nξmnA2jmUm\nGIS\naj=/summationtext\nmni|Xmn|2η2\nmnijE/bracketleftbig1\nθiφjWimHijn/bracketrightbig\nUm (19)\nVF\nmnij andVG\nmnij are given by VF\nmnij=E[θiφjAjmHijn]\nandVG\nmnij=|Xmn|λmnijW−1\nimin the KL-iCARM. On\nthe other hand, VF\nmnij=E[θiφjAjmHijn]\nξmnandVG\nmnij=\n|Xmn|2η2\nmnijE/bracketleftbig1\nθiφjW2\nimAjmHijn/bracketrightbig\nin the IS-iCARM.4. EV ALUATION\nWe report comparative experiments that were conducted to\nevaluate the performance of the iCARMs based on KL andIS divergences as multipitch analyzers.\n4.1 Experimental Conditions\nWe used thirty pieces of “ENSTDkCl” subset included in\nthe MAPS piano database [21]. We truncated each piece to30 s as in [5,11] and converted the original CD-quality sig-nals into monaural signals sampled at 16 [kHz]. The spec-trograms were obtained with short-time Fourier transform(STFT) with a window size of 2048 samples and a shifting\ninterval of 10 [ms], i.e., M= 1024 andN= 3000.T h e\namplitude or power spectrogram of each piece was scaled\nsuch that\n1\nMN/summationtext\nmn|Xmn|=1 ormaxmn|Xmn|2=1.\nThe hyperparameters were speciﬁed as I= 88+1 ,J=1 0 ,\nα=1,β=γ=0.1,H=2 0 ,P=4,a n dd=Eemp[|Xmn|]\norEemp[|Xmn|2]. Although J=1 0 was too small to accu-\nrately approximate the GaP, it was sufﬁciently large in our\nexperiments because the audio signals contain only piano\nsounds. We initialized {μi}88\ni=1as the frequencies corre-\nsponding to the 88 keys of the standard piano. The other\nparameters were initialized randomly.\nMultiple F0s were detected at each frame in a threshold-\ning process. If the gain of the i-th source,/summationtext\njθiφjHijn,\nwas larger than the threshold, we judged that the n-th frame\nincludes an F0 indicated by μi. The threshold was globally\ndetermined such that the frame-level precision and recallrates were balanced to yield the best average F-measure.\n4.2 Experimental Results\nWe ﬁrst tested our models on toy data obtained by extract-\ning the ﬁrst 4.9 s (490 frames) of the piece “alb\nse2,” which\ncontains ﬁve different F0s and a polyphony level that in-creases one by one up to ﬁve (D4, +C#4, +C4, +A3, +F#3).As shown in Fig. 2, the KL-iCARM could successfully dis-cover the correct number of sources (ﬁve harmonic sources\n+ one white-noise source) in a data-driven manner. In addi-\ntion, we could separate Xinto harmonic and noise compo-\nnents by computing E[Y\ni\nmn]=/summationtext\njE[θiφjWimAjmHijn],\nwhich represents the component of the i-th source at the\nm-th bin and n-th frame.\nAs shown in Fig. 3, on the other hand, the IS-iCARM\noverestimated the numbers of sources and ﬁlters and mademany octave errors (half-F0 errors). One reason is that ISdivergence permits a reconstructed power to exceed an ob-served power with a smaller penalty. It is therefore difﬁcultto reduce false alarms of harmonic partials.\nWe then used the 30 pieces for evaluation. The KL- and\nIS-iCARMs achieved the frame-level F-measures of 48.4%and 35.1% respectively.\nAlthough these preliminary results\nwere not really impressive compared with the state-of-the-\nart results [5,11], we consider our framework to be promis-\ning because of its elegant nature of sparse learning over aninﬁnite space. A main limitation of the KL-iCARM is thatwe still need to resort a thresholding process for temporalgains although limited numbers of sources and ﬁlters canbe obtained by using GaPs. One solution would be to intro-duce binary latent variables that indicate note existences.Observation X\n Reconstruction Y\nHarmonic components in Y\n Noise components in Y\nGlobal gains of sources E[θ]\nUnnecessary sources with\nsufficienly small gainsD4C#4\nC4A3F#3\nGlobal gains of ﬁlters E[φ]\nUnnecessary filters with\nvanishingly small gainsEffective AR ﬁlters in A\nFigure 2 . Decomposition results obtained by KL-iCARM\n5. CONCLUSION\nWe presented nonparametric Bayesian models called inﬁ-\nnite composite autoregressive models (iCARMs) that de-\ncompose an observed spectrogram into three kinds of fac-\ntors, i.e., sources, ﬁlters, and time-varying gains of source-ﬁlter pairs. The experimental results showed that appropri-ate numbers of sources and ﬁlters can be discovered in adata-driven manner by using gamma processes for sparselearning. To improve the accuracy of multipitch analysis,we are considering the use of log-frequency spectrograms\nobtained by constant-Q or wavelet transform. We also plan\nto use these models for “timbre-based” source separationby distinguishing different resonance characteristics of in-strument and vocal sounds by AR ﬁlters.\nAcknowledgment: This study was supported by JSPS\nKAKENHI 23700184 and JST OngaCREST project.\n6. REFERENCES\n[1]H. Kameoka and K. Kashino. Composite autoregressive sys-\ntemfor sparse source-ﬁlter representation of speech. ICASSP ,\npp. 2477–2480, 2009.\n[2]R. Badeau, V . Emiya, and B. David. Expectation-maximization\nalgorithm for multi-pitch estimation and separation of over-\nlapping harmonic spectra. ICASSP , pp. 3073–3076, 2009.\n[3] J.-L. Durrieu, G. Richard, B. David, and C. F ´evotte. Source/Filter\nmodel for unsupervised main me lody extraction from polyphonic\naudio signals. IEEE Trans. on ASLP, 18(3):564–575, 2010.\n[4] T. Virtanen and A. Klapuri. Analysis of polyphonic audio\nusing source-ﬁlter model and non-negative matrix factoriza-tion. NIPS Ws. on Adv. in Mod. for Acoust. Proc. , 2009.\n[5]\nJ. J. Carabias-Orti, T. Virtanen, P. Vera-Candeas, N. Ruiz-Reyes ,\nand F. J. Ca ˜nadas-Quesada. Musical instrument sound multi-\nexcitation model for non-negative spectrogram factorization.IEEE J. of Sel. Top. in Sig. Proc. , 5(6):1144–1158, 2011.\n[6] T. Heittola, A. Klapuri, and T. Virtanen. Musical instrument\nrecognition in polyphonic audio using source-ﬁlter model forsound separation. ISMIR , pp. 327–332, 2009.\n[7] N. Yasuraoka and H. G. Okuno. Musical audio signal model-\ning for joint estimation of harmonic, inharmonic, and timbralstructure and its application to source sepatation. SIG Tech-\nnical Reports , volume 2012-MUS-94, pp. 1–8, 2012.Observation X\nReconstruction Y\nHarmonic components in Y\n Noise components in Y\nGlobal gains of sources E[θ]\nOverestimated sources\nwith heavy-tailed gains\nGlobal gains of ﬁlters E[φ]\nOverestimated filterswith heavy-tailed gainsEffective AR ﬁlters in A\nFigure 3 . Decomposition results obtained by IS-iCARM\n[8] R. Hennequin, R. Badeau, and B. David. NMF with time-\nfrequency activations to mode l nonstationary audio events.\nIEEE Trans. on ASLP, 19(4):744–753, 2011.\n[9] R. Hennequin, R. Badeau, and B. David. Time-dependent\nparametric and harmonic templates in non-negative matrix\nfactorization. DAFx, pp. 1–8, 2010.\n[10] E. Vincent, N. Bertin, and R. Badeau. Adaptive harmonic\nspectral decomposition for multiple pitch estimation. IEEE\nTrans. on ASLP , 18(3):528–537, 2010.\n[11] N. Bertin, R. Badeau, and E. Vincent. Enforcing harmonicity\nand smoothness in Bayesian non-negative matrix factoriza-\ntion applied to polyphonic music transcription. IEEE Trans.\non ASLP , 18(3):538–549, 2010.\n[12] M. Hoffman, D. Blei, and P. Cook. Bayesian nonparametric\nmatrix factorization for recorded music. ICML , 2010.\n[13] M. Nakano et al. Bayesian nonparametric spectrogram mod-\neling based on inﬁnite factorial inﬁnite hidden Markov\nmodel. WASPAA , pp. 325–328, 2011.\n[14] B. Niedermayer. Improving accuracy of polyphonic music-\nto-score alignment. ISMIR , pp. 585–590, 2009.\n[15] D. Lee and H. Seung. Algorithms for non-negative matrix\nfactorization. NIPS , pp. 556–562, 2000.\n[16] C. F ´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix\nfactorization with the Itakura-Saito divergence: With appli-cation to music analysis. NECO , 21(3):793–830, 2009.\n[17] C. F ´evotte and J. Idier. Algorithms for nonnegative matrix\nfactorization with the beta-divergence. NECO , 23(9):2421–\n2456, 2011.\n[18] P. Orbanz and Y . W. Teh. Bayesian nonparametric models.\nEncyclopedia of Machine Learning . Springer, 2010.\n[19] A. T. Cemgil. Bayesian infe rence for nonnegative matrix fac-\ntorisation models. Computational Intelligence and Neuro-\nscience , 2009:Article ID 785152, 2009.\n[20]\nA. T. Cemgil and O. Dikmen. Conjugate gamma Markov random\nﬁelds for modelling nons tationary sources. ICA, 2007.\n[21] V . Emiya, R. Badeau, and B. David. Multipitch estimation of\npiano sounds using a new probabilistic spectral smoothnessprinciple. IEEE Trans. on ASLP, 18(6):1643–1654, 2010.\nA. PROBABILITY DISTRIBUTIONS\nGamma(x|a,b)=ba\nΓ(a)xa−1e−bxExponential (x|λ)=1\nλe−x\nλ\nPoisson(x|λ)=λx\nx!e−λGIG(x|a,b,c)=(b/c)a\n2xa−1\n2Ka(2√\nbc)e−(bx+c\nx)"
    },
    {
        "title": "Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets.",
        "author": [
            "José Ricardo Zapata",
            "Andre Holzapfel",
            "Matthew E. P. Davies",
            "João Lobato Oliveira",
            "Fabien Gouyon"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1415080",
        "url": "https://doi.org/10.5281/zenodo.1415080",
        "ee": "https://zenodo.org/records/1415080/files/ZapataHDOG12.pdf",
        "abstract": "In this paper we establish a threshold for perceptually ac- ceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the first step we use an ex- isting annotated dataset to show that mutual agreement can be used to select one committee member as the most reli- able beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to estab- lish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percent- age of trackable music of about 73%, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat track- ing. The proposed methods provide a means to automat- ically obtain a confidence value for beat tracking in non- annotated data and to choose between a number of beat tracker outputs.",
        "zenodo_id": 1415080,
        "dblp_key": "conf/ismir/ZapataHDOG12",
        "keywords": [
            "beat tracking",
            "mutual agreement",
            "committee of beat trackers",
            "Million Song Dataset",
            "listening test",
            "percentages",
            "trackable music",
            "data tags",
            "acceptable and problematic",
            "confidence value"
        ],
        "content": "ASSIGNING A CONFIDENCE THRESHOLD ON AUTOMATIC BEAT\nANNOTATION IN LARGE DATASETS\nJos´e R. Zapata1, Andr ´e Holzapfel1, Matthew E.P. Davies2, Jo˜ao L.Oliveira2;3and Fabien Gouyon2;3\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n2Sound and Music Computing Group, INESC TEC, Porto, Portugal\n3Faculty of Engineering of the University of Porto, Porto, Portugal\njoser.zapata@upf.edu, hannover@csd.uoc.gr, fmdavies, jmso, fgouyong@inescporto.pt\nABSTRACT\nIn this paper we establish a threshold for perceptually ac-\nceptable beat tracking based on the mutual agreement of a\ncommittee of beat trackers. In the ﬁrst step we use an ex-\nisting annotated dataset to show that mutual agreement can\nbe used to select one committee member as the most reli-\nable beat tracker for a song. Then we conduct a listening\ntest using a subset of the Million Song Dataset to estab-\nlish a threshold which results in acceptable quality of the\nchosen beat output. For both datasets, we obtain a percent-\nage of trackable music of about 73%, and we investigate\nwhich data tags are related to acceptable and problematic\nbeat tracking. The results indicate that current datasets are\nbiased towards genres which tend to be easy for beat track-\ning. The proposed methods provide a means to automat-\nically obtain a conﬁdence value for beat tracking in non-\nannotated data and to choose between a number of beat\ntracker outputs.\n1. INTRODUCTION\nBeat tracking can be considered one of the fundamental\nproblems in music information retrieval (MIR) research.\nThere have been numerous algorithms presented (e.g., [5,\n6, 10]) whose common aim is to “tap along” with musical\nsignals. Furthermore the inclusion of beat trackers within\nother music analysis tasks (such as harmony analysis [8],\nstructural segmentation [11]) has become common-place.\nHowever despite the somewhat automatic inclusion of beat\ntrackers as temporal processing components, beat tracking\nitself is not considered a solved problem. Recent compar-\native studies of beat trackers suggest there is often little to\nchoose between the best performing state of the art meth-\nods [4, 12]. Indeed the viewpoint could be taken that beat\ntracking performance is approaching a glass ceiling [9]\nwith the current algorithms stagnating at around the 80%\nmark when evaluated using the least stringent metrics on\ncommon datasets [4].\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2012 International Society for Music Information Retrieval.In previous work [9] we proposed that the presence of\nthis apparent glass ceiling was not the result of beat track-\ning algorithms having reached their full potential, but rather\nthe datasets on which beat trackers are evaluated not con-\ntaining a sufﬁcient proportion of challenging examples; and\nthat current beat trackers have over-learned the musical\nproperties of the “easier” songs within these datasets. To-\nwards the future advancement of beat tracking we presented\na technique to automatically identify challenging examples\nfor beat tracking without the need for ground truth annota-\ntions [9]. Our technique was based on measuring the mean\nmutual agreement (MMA) between a committee of state of\nthe art beat tracking algorithms, where low mutual agree-\nment (or put another way, high disagreement) between beat\noutputs was shown to be a good indicator of low perfor-\nmance against the ground truth. To this end we empirically\ndetermined an MMA “failure” threshold below which beat\ntracking performance was shown to be very poor, and cre-\nated a new database comprised of challenging songs with\nMMA below this threshold.\nIn this paper we address the opposite issue, where, in-\nstead of trying to ﬁnd where beat tracking algorithms fail,\nwe wish to identify when beat tracking has been success-\nful. When ground truth annotations are available this ques-\ntion can be easily answered, however the problem is non-\ntrivial when no ground truth exists, i.e., on the vast ma-\njority of music. The current implicit means for doing so\nis simply to extrapolate the performance on the limited\ndataset, for which a precise evaluation can be conducted,\nand assume this is representative of beat tracking perfor-\nmance on all music.\nIn light of our previous concerns about the make-up of\nthese annotated databases, we believe that extrapolating\nperformance in this way will be overly optimistic. There-\nfore when seeking to determine an unbiased measure of\nperformance we can either manually annotate more and\nmore music examples for evaluation, or instead attempt to\nestimate beat tracking performance without ground truth.\nDue to the impractical nature of the ﬁrst option, we pursue\nthe second. Furthermore, if no ground truth is required,\nthen performance can be estimated on very large (effec-\ntively unlimited) collections of music.\nWe extend our previous work to attempt to determine\nan MMA “success” threshold above which we can have\nhigh conﬁdence in the beat tracking output of a commit-tee of state of the art algorithms. We determine the success\nthreshold by means of a subjective listening test, where lis-\nteners are asked to rate the quality of the beat output given\nby the committee across a range of songs for which the\nMMA has been calculated. In each case the beat tracker\noutput chosen to represent the committee is selected auto-\nmatically as the one which most agrees with the remainder\nof the committee, i.e., the beat tracker output with the max-\nimal mutual agreement (MaxMA). We demonstrate that se-\nlecting between beat tracker outputs using MaxMA leads\nto improved performance over consistently picking any in-\ndividual algorithm from the committee.\nThrough the calculation of both MMA and MaxMA we\npresent a technique by which we can estimate the level\nof successful beat tracking on any dataset without ground\ntruth, and, for those songs with MMA above the threshold,\nautomatically annotate the beats in a way that exceeds the\nperformance of the state of the art. In light of the recently\npresented Million Song Dataset [1] we consider this work\nto be particularly timely.\nThe remainder of the paper is structured as follows:\nSection 2 gives an overview of the proposed method based\non mutual agreement and describes the chosen committee.\nSection 3 demonstrates the improvement in performance\nwhen selecting a beat tracker based on the MaxMA ap-\nproach on a manually annotated dataset. Section 4 applies\nthe technique to non-annotated data and describes the pro-\ncedure followed in the listening test and the main results.\nSection 5 concludes the paper with discussion of the results\nand areas for future work.\n2. MEASURING MUTUAL AGREEMENT\nThe measurement of Mean Mutual Agreement (MMA) is\ninspired by the Query by Committee concept [14] which\nselects the most informative set of samples from a database\nbased on the mutual (dis-)agreement between a designated\ncommittee of learners. In beat tracking, the MMA is com-\nputed using the beat outputs (or beat sequences) of a com-\nmittee of Nbeat trackers on a musical piece, by measuring\nthe mutual agreement MAi;jbetween every pair of esti-\nmated beat tracker outputs iandj, and retrieving the mean\nof allN(N\u00001)=2 mutual agreements. A graphical exam-\nple is shown in Figure 1.\nIn addition to calculating the MMA as a summary statis-\ntic, we can easily identify the mutual agreement, MAi, of\nthe beat tracker output iwhich most agrees with the re-\nmainder of the committee: MaxMA, and the beat tracker\noutput iwhich agrees the least: MinMA. In order to mea-\nsure the mutual agreement MAi;jbetween each pairfi; jg\nof beat tracker outputs, a beat tracking evaluation method\nmust be chosen. In [9] we reviewed the properties of ex-\nisting evaluation methods [2] and selected the Information\nGain approach [3] (InfGain) as the only one with a true\nzero value, able to match low MMA (measured in bits)\nwith unrelated beat tracker outputs:\nMAi;j=InfGain (i; j); i; j = 1; : : : ; N^i6=j:(1)\nThe Information Gain measure is determined by forming a\nFigure 1: Example calculation of the MMA and MaxMA\nfor a song with the beats estimated from a committee of\nfour beat trackers.\nbeat error histogram representing the timing error between\nbeat sequences. A numerical score is calculated as a func-\ntion of the entropy of the histogram. The range of values\nfor the Information Gain is 0 bits to approximately 5.3 bits,\nwhere the upper limit is log2(K)forK=40 histogram bins.\nFor further details see [3].\nTo form our committee we select ﬁve state of the art and\npublicly available beat trackers: Dixon (Dix.) [5], Degara\n(Deg.) [4], Ellis (Ell.) [6], IBT [13], and Klapuri (Kla.) [10].\nThese convey the performance and diversity necessary to\ncompute a reliable MMA [9].\n3. MUTUAL AGREEMENT ON EXISTING\nANNOTATED DATA\nIn order to assess if the mutual agreement among our com-\nmittee of beat trackers can reliably inform us about the\nbest estimated beat tracker output we computed and com-\npared the outputs of this committee on a manually anno-\ntated dataset containing 1360 song excerpts [5,7] (referred\nto as Dataset1360) which covers the following genres:\nAcoustic; Afro-American; Jazz/Blues; Classical; Choral;\nElectronic; Rock/Pop; Balkan/Greek; and Samba.\nSince we have shown in previous work that disagree-\nment among the committee indicates poor beat tracking\nperformance [9], we consider the potential positive effect\nof agreement within the committee. Our hypothesis is that\nthe beat tracker that best agrees with the rest of the com-\nmittee (the one with MaxMA) will be the most reliable al-\ngorithm for a speciﬁc musical piece. On this basis, we\ncompare the mean ground truth performance of the best\noverall beat tracker, Best Mean, (which was shown to be\nKlapuri [10] (Kla.) for Dataset1360 [9]) against the mean\nscores of the algorithms with the MaxMA and MinMA for\neach excerpt. To illustrate the upper limit on performance\nfor our committee we also compute the Oracle as the mean\nscore given by the best beat tracker per excerpt.\nFigure 2 compares the results of the described perfor-\nmance variants on Dataset1360. As described in Section 2,\nthe MaxMA and MinMA were computed using the Inf-\nGain1. In order to compare MinMA and MaxMA against\n1the InfGain and AMLt measures were computed using the\nthe beat tracking evaluation toolbox, available at http://code.\nsoundsoftware.ac.uk/projects/beat-evaluation100 98 95 88 82 73 65 52 42 29 20 556065707580859095100\nPercentage of kept filesAMlt %\n  0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3MMA\nOracle\nMaxMA\nBestMean\nMinMAFigure 2: AMLt scores of the beat tracker output with\nmaximum (MaxMA) and minimum (MinMA) agreement\nper song, compared with the single best beat tracker choice\n(BestMean), and the oracle score (Oracle) for various\nthresholds of MMA applied to Dataset1360.\nthe Best Mean and Oracle performances of the committee\non the same data, we used the least stringent continuity-\nbased measure, AMLt1(Allowed Metrical Level with no\ncontinuity required) [3], where beats are accurate when\nconsecutive falling within tempo-dependent tolerance win-\ndows around successive annotations. Beat tracker outputs\nare also considered accurate if beats occur on the off-beat,\nor are estimated at double or half the annotated tempo.\nThis performance measure provides a more intuitive scale\nof 0 to 100% than Information Gain and allows some am-\nbiguity in the choice of metrical level at which the beats\nare estimated.\nPerformance across these conditions was computed for\ndifferent amounts of data conﬁned by incremental values\nof MMA, in the range of [0-3] bits and varying in steps\nof 0.3 bits. These MMA values act as a threshold for the\nselection of excerpts from the dataset (e.g., for an MMA of\n2.1 bits we retain 52.1% of the song in the dataset).\nAs expected, the overall performance of the committee\nincreases with the MMA threshold. This conﬁrms the hy-\npothesis that the MMA is able to reliably detect difﬁcult\nsongs for beat tracking, and therefore can conﬁne the data\nto easier songs by removing those with low MMA. Across\nall MMA thresholds we can observe that the performance\nof MinMA is signiﬁcantly lower than all other conﬁgu-\nrations tested. Although lower than the Oracle, MaxMA\noutperforms the BestMean algorithm, and the difference\nbetween the two, around 3.3%, is statistically signiﬁcantly\n(p<0.01) for all songs with an MMA below 2.4 bits. Above\n2.4 bits this difference is no longer signiﬁcant however the\nperformance of the Oracle, BestMean and MaxMA are all\nvery high. This suggests that for very high MMA thresh-\nolds, where beat tracker outputs are highly consistent with\none another, any attempt to choose between the members\nof the committee offers little scope for improvement.4. AUTOMATICALLY BEAT-ANNOTATING A\nLARGE DATASET\nHaving illustrated the validity of using the MaxMA method\nto select a beat tracker output among a committee of al-\ngorithms on a manually annotated dataset, we now turn\nour attention to applying it to a large collection of non-\nannotated data. For very large collections it is impractical\nto expect there to be ground truth annotations on which\nto base the performance evaluation. Towards understand-\ning how well the state of the art in beat tracking can au-\ntomatically annotate beats in large collections we employ\nour MMA and MaxMA methods and attempt to determine\nthe proportion of songs for which the beat estimates are\nacceptable via a subjective listening test. We want to es-\ntablish a threshold on MMA above which the beat tracker\noutputs are perceptually acceptable. For each ﬁle, the beat\ntracker output will be chosen using the MaxMA method.\n4.1 Million Song Subset\nThe large collection we aim to automatically annotate is\ntheMillionSongSubset from the Million Song Dataset [1].\nThe subset is comprised of 10,000 songs without ground\ntruth for which audio previews were obtained. The major-\nity of audio previews were either 30 s or 60 s in duration,\nhowever to provide sufﬁciently long song excerpts for beat\ntracking we discarded any shorter than 20 s. This left a set\nof 9940 songs on which to automatically annotate beats. To\ncomplement the audio data, we obtained 31696 Last.fm2\ntags which covered a subset of 4638 songs.\nOnce all of the audio and meta data was collected we\nran the committee of beat tracking algorithms recording\nthe MMA value per excerpt and saving the MaxMA beat\ntracker output.\n4.2 Subjective Listening Test\nThe aim of our listening test was to determine an MMA\nthreshold above which the beat tracker output given by the\nMaxMA method was deemed acceptable to human listen-\ners. By subsequent inspection of the number of songs in\nthe dataset above this MMA threshold we could then esti-\nmate the proportion for which beat tracking can be consid-\nered successful.\nJust as it is not possible to hand annotate beats in nearly\n10,000 songs, it is equally impractical to ask participants to\nlisten and rate this large number. As alternative to the ex-\nhaustive rating of all audio songs, we selected 8 levels of\nMMA = [0.5, 1.0, 1.5, . . . , 4.0] bits and chose the 6 clos-\nest songs from the MillionSongSubset to each MMA level,\ngiving a total of 48 songs to summarize the dataset. To cre-\nate the musical stimuli for the listening test we constructed\nstereo audio ﬁles containing a mixture of source audio and\nthe MaxMA beat output synthesized as short click sounds.\nTo mitigate the effect of errors in beat tracking at the start\nof songs, which might bias the listener ratings, each mu-\nsical stimulus was formed out of the middle 15 s of each\n2http://labrosa.ee.columbia.edu/millionsong/\nlastfmsong. To allow listeners to hear the audio with and with-\nout click sounds, we panned the source audio on its own\non the left channel, and on the right channel we mixed the\nclick sounds conveying the beats with a quiet version of the\nsource audio. Through informal listening tests prior to the\nmain experiment, this was deemed an acceptable method\nfor creating the stimuli.\nTo take the listening test we recruited 25 participants\n(21 male, 4 female) with an age range of 23 to 41 (mean =\n31 years, std = 4.7 years). The participants’ level of music\ntraining ranged from 0 to 20 years (mean = 8.7 years, std =\n7.7 years). Each participant was instructed to perform the\ntest in a quiet environment with good quality headphones.\nPrior to starting the main test, the participants were given\nthree training examples (not in the main set of 48). The\ntraining phase was used for three reasons: i)to familiarise\nparticipants with the type of musical stimuli in the test, ii)\nfor the participants to understand the panning of the beats\nin the stimuli and iii)so the participants could set the play-\nback volume to a comfortable level. To prevent order ef-\nfects in the stimuli, each participant was given an individ-\nual playlist of songs in a different random order.\nIn taking the test, the participants were asked to answer\nthe following question: “How do you rate the overall qual-\nity of the given click as a beat annotation of the piece?”\nThe options for rating were: 1 - Bad, 2 - Poor, 3 - Fair, 4 -\nGood, 5 - Excellent.\n4.3 Results\n4.3.1 Listening Test\nFigure 3 presents a comparison between the human ratings\nand the MMA of our committee of beat trackers for the se-\nlected 48 pieces of the MillionSongSubset. The plot shows\nthat for an MMA equal to 1.5 bits the mean rating was 3.7\n(Good) with a standard deviation of 0.93. However, for\nMMA equal to 1 bit, the mean rating was much lower, at\naround 2.4 (Poor). Performing a t-test, we found the differ-\nence between the mean ratings at these MMA values to be\nhighly signiﬁcant (p <0.0001). On this basis we can eas-\nily identify an MMA threshold of 1.5 bits which separates\nperceptually acceptable beat tracking from inaccurate beat\ntracking.\n4.3.2 MMA Threshold\nBy selecting an MMA of 1.5 bits as a threshold of percep-\ntual conﬁdence for beat tracking we ﬁnd 996 songs (73%)\nin Dataset1360 and 7252 songs (coincidentally also 73%),\nin the MillionSongSubset above this limit (see Figure 4).\nTable 1 shows the AMLt scores for the Oracle, MaxMA,\nBest Mean, and MinMA for the two subsets of Dataset1360\nseparated by MMA = 1.5 bits, evaluated against the ground\ntruth. The beat tracking performance is consistently high\nfor songs with MMA >1.5 bits, with a mean MaxMA per-\nformance of\u001990%, which must be considered very accu-\nrate, and hence hints at a meaningful relationship between\nsubjective judgement of beat tracking and the AMLt scores\nobtained from the objective evaluation. While beat track-\ning performance is lower for MMA <1.5 bits this does not\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5012345Rating\nMMA level [=] bitsFigure 3: Listening test ratings vsMMA for the selected\n48 music excerpts, from the MillionSongSubset.\nName AMLt (%) MMA\nOracle 95.4\nMMA>1.5 MaxMA 89.9\nBest Mean 86.3\nMinMA 63.9\nOracle 70.9\nMMA<1.5 MaxMA 58.8\nBest Mean 54\nMinMA 50.1\nTable 1: Mean AMLt score of Oracle, MaxMA,\nBest Mean, and MinMA for the two subsets of\nDataset1360 divided by an MMA threshold of 1.5 bits.\nmean the MaxMA beat estimations cannot be perceptually\naccurate, merely that we do not have high conﬁdence in\nthem.\n4.3.3 Last.fm Tag Analysis\nGiven the MMA threshold and collected Last.fm meta-\ndata, we now look at the genre-related tags of the songs that\nappear signiﬁcantly more often (with p <0.0001) in the\nMillionSongSubset with MMA above and below 1.5 bits.\nThese are shown in Table 2. From inspection of the table\nwe can see that the genres above the MMA threshold are\nthose which we would typically associate with being “eas-\nier” for beat tracking where as those below the threshold\nappear more challenging. Seeing all genre labels related\nto metal music below the threshold was a surprising result\nsince this music is strongly percussive and is not charac-\nterised by wide tempo changes. The fact that metal music\nconsistently falls below the threshold indicates it might be\nthe “noisy” element of the music which causes it to be dif-\nﬁcult. To the best of our knowledge we are unaware of\nmany metal examples in existing beat tracking databases.\nThis suggests it is something of a forgotten genre for beat\ntracking.\nAnother important observation relates to the tag fre-\nquency for genre labels above and below the threshold.\nThere is a far higher proportion of songs tagged “Rock”\nand “Pop” compared to all the others, and in general the0 200 400 600 800 1000 120001234MMA value  [=]  bits73 %\n1360(a) Dataset 1360\n0 2000 4000 6000 8000 1000001234MMA value  [=]  bits73 %\n(b) MillionSongSubset\nFigure 4: Datasets sorted by MMA and the perceptual\nthreshold of 1.5 bits.\ntags used above the threshold appear much more frequently\nthan those below it. From this we can infer that, just as\nDataset1360 is biased towards easier cases for beat track-\ning [9], the same could be said of the MillionSongSubset.\nEvidence for this conclusion can be found in the descrip-\ntion of the MillionSongDataset itself [1] where the lack of\ndiversity is mentioned; in particular the small amount of\nclassical and world music.\nGiven the disproportionate number of easier songs for\nbeat tracking in this dataset, our estimate of 73% of songs\nfor which beat tracking is acceptable may still be an op-\ntimistic estimate of the true level of beat tracking perfor-\nmance across all music.\n4.3.4 MaxMA Choice of Beat Tracker\nHaving investigated the main results of applying MaxMA\nto automatically annotate beat locations, we now address\nthe properties of the committee. Figure 5 presents his-\ntograms for both evaluated datasets depicting the propor-\ntion of songs where each beat tracking algorithm is se-\nlected as the MaxMA beat output. Both histograms show\nsimilar shapes, indicating that there may be some similar\nproperties between the musical content of both datasets.\nThe two most chosen algorithms are those of Degara [4]\nand Klapuri [10]; both of which perform most accurately\nagainst the ground truth, and can be considered the best\namong the state of the art methods. As to why the Degara\nalgorithm is chosen more frequently than that of Klapuri,Tag Frequency MMA\nRock 1080\nMMA>1.5Pop 680\nDance 320\nHip-hop 271\nRap 193\nPop rock 154\nReggae 149\nJazz 227\nMMA<1.5Instrumental 199\nDeath metal 80\nBlack metal 74\nProgressive metal 59\nClassical 36\nGrindcore 28\nTable 2: Frequency of the genre-based occurrence of tags\nfor the two subsets of MillionSongSubset divided by an\nMMA threshold of 1.5 bits.\n0200400600800\nDix.Deg. Ell. IBT Kla.Number of Songs\n(a) Dataset 1360\n010002000300040005000\nDix. Deg. Ell.IBTKla.Number of Songs (b) MillionSongSubset\nFigure 5: Histograms with the number of times each algo-\nrithm is chosen with the MaxMA approach.\nresults in [4] indicate that the inter-quartile range of the De-\ngara algorithm is smaller than that of Klapuri (for a similar\nmedian), implying it is “wrong” in a lower proportion of\nsongs.\n5. DISCUSSION AND CONCLUSIONS\nTo estimate the conﬁdence of beat tracking without ground\ntruth annotations we have proposed the use of two meth-\nods based on the mutual agreement between a committee\nof beat tracking algorithms. The ﬁrst, the Mean Mutual\nAgreement, was used to estimate the level of consensus\nbetween the beat outputs of the committee. The second,\nthe Maximum Mutual Agreement, was used for selecting\nthe best beat tracking output from the committee of beat\ntrackers.\nThrough a subjective listening test we determined an\nMMA threshold between this committee of beat trackers\nof 1.5 bits above which we believe automatic beat track-\ning can be applied with high conﬁdence. Based on this\nperceptual conﬁdence, we demonstrate that around 73% of\nthe MillionSongSubset could be automatically annotated\nusing our committee of beat trackers. This proportion of\nsongs for which we can be conﬁdent in an automatic beat\nannotation was also veriﬁed in a second dataset with man-\nually annotated ground truth. Given the apparent bias inthese datasets towards easier genres for beat tracking, we\nconsider this value of 73% to be somewhat optimistic. We\nplan to verify this hypothesis in future work by measuring\nMMA in more diverse datasets.\nRegarding the types of music which formed the remain-\ning 27% of the MillionSongSubset (i.e., those below the\nthreshold) we found a high proportion of tags related to\nmetal and similar “noisy” styles of music. Beyond classi-\ncal music and jazz, which are known to be challenging for\nbeat tracking systems, we consider the difﬁculty of beat\ntracking in metal to be a new and unexpected result, and\nfurthermore an interesting area for the future development\nof beat tracking algorithms.\nIn addition to using MMA to determine successful beat\ntracking, we also presented a related technique, MaxMA,\nto select beat estimations among a committee of beat track-\ners. The fact that a simple approach of this kind was able\nto demonstrate a signiﬁcant improvement over using indi-\nvidual state of the art algorithms is encouraging. Yet, as\nour results indicate, performance of MaxMA falls some\nway below that of the Oracle system using our committee.\nThis suggests that there is still room for making a more ac-\ncurate selection among existing algorithms, and exploring\nnew selection methods will form a further area for future\nwork.\nOne limitation of our approach may have been the use\nof short song excerpts for the listening test. This was done\nto make the listening test as manageable as possible for a\nwide range of participants. However, to obtain a greater\nunderstanding of subjective ratings for longer musical ex-\ncerpts and a better understanding of perceptual difﬁculty\nin beat perception we plan to conduct more sophisticated\nsubjective listening experiments.\nWhile all the directions for future work have so far been\nrelated to beat tracking, we strongly believe that, given\nsuitable evaluation metrics, our framework based on MMA\nand MaxMA could be readily applied to other areas of\nMIR. We therefore encourage researchers to explore its us-\nage in problems such as onset detection, chord detection,\nstructural segmentation, and music transcription.\n6. ACKNOWLEDGEMENTS\nThis research received support from the Portuguese Foun-\ndation for Science and Technology through the “ShakeIt”\nproject (grants UTAustin/CD/ 0052/2008 and PTDC/ EAT-\nMMU/ 112255/ 2009) and through grants SFRH/BD/ 43704/\n2008 and SFRH/ BPD/ 51348/ 2011, and by Universidad\nPontiﬁcia Bolivariana (Colombia) and Colciencias, and by\nthe EU-funded project MIReS.\n7. REFERENCES\n[1] T. Bertin-Mahieux, D. P.W. Ellis, B. Whitman and P.\nLamere, “The Million Song Dataset,” in Proc. of the\n12th ISMIR conference, pp. 591–596, 2011.\n[2] M. E. P. Davies, N. Degara, and M. D. Plumbley,\n“Evaluation methods for musical audio beat trackingalgorithms,” Queen Mary University of London, Cen-\ntre for Digital Music, Tech. Rep. C4DM-TR-09-06,\n2009.\n[3] M. E. P. Davies, N. Degara and M. D. Plumbley, “Mea-\nsuring the performance of beat tracking algorithms us-\ning a beat error histogram,” IEEE Signal Processing\nLetters, vol. 18, no. 3, pp. 157–160, 2011.\n[4] N. Degara, E. Argones, A. Pena, S. Torres-Guijarro,\nM. E. P. Davies and M. D. Plumbley, “Reliability-\nInformed Beat Tracking of Musical Signals,” IEEE\nTransactions on Audio, Speech and Language Process-\ning,V ol. 20, pp. 290–301, 2012.\n[5] S. Dixon, “Evaluation of the audio beat tracking\nsystem BeatRoot,” Journal of New Music Research,\nV ol. 36, pp. 39–50, 2007.\n[6] D. P. W. Ellis, “Beat tracking by dynamic program-\nming,” Journal of New Music Research, vol. 36, no. 1,\npp. 51–60, 2007.\n[7] F. Gouyon, A Computational Approach to Rhythm De-\nscription — Audio Features for the Computation of\nRhythm Periodicity Functions and their use in Tempo\nInduction and Music Content Processing, PhD. Thesis.\nMTG, Universitat Pompeu Fabra, 2005.\n[8] A. Holzapfel and Y . Stylianou “Parataxis: Morpholog-\nical similarity in traditional music,” Proc. of the 11th\nISMIR Conference, pp. 453–458, 2010.\n[9] A. Holzapfel, M. E. P. Davies, J.R. Zapata, J.L.\nOliveira and F. Gouyon, “Selective sampling for beat\ntracking evaluation,” IEEE Transactions on Audio,\nSpeech and Language Processing, In press, 2012.\n[10] A. P. Klapuri, A. J. Eronen and J. T. Astola, “Analysis\nof the meter of acoustic musical signals,” IEEE Trans.\non Audio,Speech, and Language Processing, V ol. 14,\nNo. 1, pp. 342–355, 2006\n[11] M. Levy and M. Sandler, “Structural segmentation of\nmusical audio by constrained clustering,” IEEE Trans-\nactions on Audio, Speech and Language Processing,\nvol. 16, no. 2, pp. 318–326, 2008.\n[12] M. F. McKinney, D. Moelants, M. E. P. Davies, A. Kla-\npuri, “Evaluation of Audio Beat Tracking and Music\nTempo Extraction Algorithms,” Journal of New Music\nResearch, V ol. 36, pp. 1–16, 2007.\n[13] J. Oliveira, F. Gouyon, L. Martin, and L. Reis: “IBT:\nA realtime tempo and beat tracking system.,” in Proc.\nof the 11th ISMIR conference, pp. 291–296, 2010.\n[14] H. S. Seung, M. Opper and H. Sompolinsky “Query by\ncommittee,” in Proc. of the 5th Annual Workshop on\nComputational learning theory, pp. 287–294, 1992."
    },
    {
        "title": "Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012, Mosteiro S.Bento Da Vitória, Porto, Portugal, October 8-12, 2012",
        "author": [
            "Fabien Gouyon",
            "Perfecto Herrera",
            "Luis Gustavo Martins",
            "Meinard Müller"
        ],
        "year": "2012",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": "https://ismir2012.ismir.net/event/978-972-752-144-9.pdf",
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\nŞentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 - 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmanoğlu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., Şentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/2012"
    }
]