[
    {
        "title": "Score-Informed Analysis of Intonation and Pitch Modulation in Jazz Solos.",
        "author": [
            "Jakob Abeßer",
            "Estefanía Cano",
            "Klaus Frieler",
            "Martin Pfleiderer",
            "Wolf-Georg Zaddach"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416836",
        "url": "https://doi.org/10.5281/zenodo.1416836",
        "ee": "https://zenodo.org/records/1416836/files/AbesserCFPZ15.pdf",
        "abstract": "The paper presents new approaches for analyzing the char- acteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on con- textual parameters such as pitch, duration, and tempo as well as the performing artist.",
        "zenodo_id": 1416836,
        "dblp_key": "conf/ismir/AbesserCFPZ15",
        "keywords": [
            "intonation",
            "pitch modulation",
            "woodwind",
            "brass solos",
            "jazz recordings",
            "score-informed analysis",
            "source separation",
            "fundamental frequency tracking",
            "audio splitting",
            "fundamental frequency contour"
        ],
        "content": "SCORE-INFORMED ANALYSIS OF INTONATION AND PITCHMODULATION IN JAZZ SOLOSJakob Abeßer1,2Estefanía Cano2Klaus Frieler1Martin Pﬂeiderer1Wolf-Georg Zaddach11Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany2Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germanyjakob.abesser@idmt.fraunhofer.deABSTRACTThe paper presents new approaches for analyzing the char-acteristics of intonation and pitch modulation of woodwindand brass solos in jazz recordings. To this end, we usescore-informed analysis techniques for source separationand fundamental frequency tracking. After splitting theaudio into a solo and a backing track, a reference tuningfrequency is estimated from the backing track. Next, wecompute the fundamental frequency contour for each tonein the solo and a set of features describing its temporalshape. Based on this data, we ﬁrst investigate, whetherthe tuning frequencies of jazz recordings changed over thedecades of the last century. Second, we analyze whetherthe intonation is artist-speciﬁc. Finally, we examine howthe modulation frequency of vibrato tones depends on con-textual parameters such as pitch, duration, and tempo aswell as the performing artist.1. INTRODUCTIONThe personal styles of improvising jazz musicians can bedescribed from various musical perspectives. There areseveral structural or syntactical aspects of the improvisedmelodic lines, which could be idiosyncratic for a certainmusician, e.g., preferred pitches, intervals, scales, melodiccontours, rhythms or typical patterns, licks, and formu-las. These dimensions can be best explored using a sym-bolic representation, e.g., Western staff notation or MIDI.However, there are other important aspects, which deﬁnepersonal style and make it recognizable:timbre(soundcharacteristics such as roughness or breathiness),micro-timing(systematic deviations from the underlying metricstructure),dynamics(the changes in intensity of tones orphrases),intonation(the pitch accuracy with respect to agiven tone system),articulation(e.g., legato or staccatoplaying) andpitch modulation(the variation of the funda-mental frequency within the duration of a tone). Symbolicc\u0000Jakob Abeßer, Estefanía Cano, Klaus Frieler, MartinPﬂeiderer, Wolf-Georg Zaddach. Licensed under a Creative CommonsAttribution 4.0 International License (CC BY 4.0).Attribution:JakobAbeßer, Estefanía Cano, Klaus Frieler, Martin Pﬂeiderer, Wolf-GeorgZaddach. “Score-informed Analysis of Intonation and Pitch Modulationin Jazz Solos”, 16th International Society for Music Information RetrievalConference, 2015.representation does not reveal information about timbre,intonation, and pitch modulation. Therefore, audio-levelanalysis of recorded improvisations is necessary to charac-terize those non-syntactical, expressive dimensions in or-der to get a comprehensive and exhaustive description of apersonal style.2. GOALSPolyphonic music recordings exhibit strong spectral andtemporal overlaps between harmonic components of dif-ferent instrument sources. Hence, the transcription andanalysis of the individual sound sources remain one of themost challenging tasks in Music Information Retrieval(MIR). We approach this task by using high-quality melodytranscriptions provided by music experts as foundation fora score-informed audio analysis. In particular, we use scoreinformation for the source separation of the solo instru-ment from the audio mixture and for the frame-wise track-ing of the fundamental frequency of each tone. Our maingoal is to investigate, which intonation and modulation stra-tegies are applied by woodwind and brass instrument play-ers in jazz solos.3. RELATED WORKVarious MIR publications investigate theintonationandtuningof music recordings, ranging from historic solo harp-sichord recordings [5], over classical music recordings [11],to Non-Western music styles such as Carnatic and Hindus-tani music [17]. The tuning frequency of audio record-ings is commonly estimated based on pitch frequencies [6],high-resolution interval histograms [17], or adjustable ﬁl-terbanks [11]. Just intonation and equal temperament aregenerally used as reference tunings for the analysis of mu-sic performances. Lerch [11] points out that observed tun-ing deviations can have different reasons ranging from de-viation of harmonic frequencies from the equal temperedscale to deviations due to non-equal temperament.Most automatic music transcription algorithm aim ata symbolic representation of tone events, which are de-scribed by distinct onset times, durations, and constantpitches [15]. Some automatic melody extraction algorithmssuch as proposed in [16] and [6] include an estimation ofthe tone-wise contours of the fundamental frequency (f0)823as well, which is an essential pre-processing step for ana-lyzing the appliedfrequency modulation techniques. Thereare many studies onvibratodetection in audio recordings[14], particularly for singing voice [8,9,12]. Other publica-tions deal with analyzing the deviation off0contours fromthe target pitch [8] as well as with segmentingf0contoursbased on modulations such as vibrato andpitch glides[12]orbendings[10]. To the best knowledge of the authors,no publication so far analyzes intonation and modulationtechniques in recorded jazz solos.4. METHODFigure 1 gives an overview over our analysis approach,all processing steps are detailed in the following sections.Section 4.1 describes the dataset of jazz solo audio excerptsand transcriptions. Two separate score-informed analysistechniques are involved. At ﬁrst, asource separationalgo-rithm is performed (see Section 4.2), which separates theoriginal audio recording into a solo track containing theimprovising solo instrument and a backing track contain-ing the accompanying band, i.e., the rhythm section (mostoften piano, double bass, and drums). The backing track isused to estimate thereference tuning frequency(see Sec-tion 4.4). The second step is thetracking of frame-wisef0contoursfor each note played by the solo instrument (seeSection 4.3). Based on the extractedf0contours, we com-pute severalcontour features(see Section 4.5) to describetheir temporal shape. In the experiments reported in Sec-tion 5, we analyze how these features depend on contextualparameters such as tone duration and pitch and whetherthese might be speciﬁc for the personal style.\nFigure 1: Proposed algorithm for score-informed analysisof tuning and modulation in improvised jazz solos.4.1 Dataset & Melody AnnotationsThe dataset used in this publication is a subset of 207 jazzsolos taken from theWeimar Jazz Database1. Table 1 lists1http://jazzomat.hfm-weimar.de(last accessed Juli 10,2015)all musicians in the dataset with their instrument, the num-ber of solosNS, and the total number of tones andf0con-toursNN, respectively. The solos were manually anno-tated by musicology and jazz students based on excerptsfrom commercial audio recordings. The annotations in-clude score-level melody transcription (MIDI pitch, toneonset, and duration) as well as additional annotation layerswith respect to melody phrases, metric structure, chords,and modulation techniques. So far, the tone-wise anno-tations of modulation techniques are incomplete and onlyrepresent the most clear examples within the solos. Figure2 gives an overview over the number of annotated tones perartist. In total, 87643 tones andf0contours are included inthe dataset.\n0100200300Art PepperBen WebsterBenny CarterBenny GoodmanBix BeiderbeckeBob BergCannonball AdderleyCharlie ParkerChet BakerClifford BrownColeman HawkinsDavid LiebmanDavid MurrayDexter GordonDizzy GillespieDon ByasEric DolphyFats NavarroFreddie HubbardJ.J. JohnsonJoe HendersonJoe LovanoJohn ColtraneJoshua RedmanKenny DorhamLee KonitzLester YoungLouis ArmstrongMichael BreckerMiles DavisOrnette ColemanPaul DesmondRoy EldridgeSidney BechetSonny RollinsSonny StittStan GetzSteve ColemanSteve LacyWayne ShorterWoody Shaw\nNotes  Fall−offSlideVibrato\nFigure 2: Number of tones of each artist which are anno-tated with fall-off, slide, and vibrato.4.2 Score-informed Source SeparationTo separate the solo/lead instrument from the backing track,the method for pitch-informed solo and accompanimentseparation proposed in [4] was used. For this study, theautomatic pitch detection stage in the separation algorithmwas bypassed, and the manual melody transcriptions wereused as prior information. The separation method is basedon an iterative modeling of the solo instrument in the spec-tral domain. The model is constructed taking into accountcharacteristics of musical instruments such as common am-plitude modulation, inharmonicity, and enforcing magni-tude and frequency smoothness constraints in the estima-tion. The separation method has proven to be robust inthe extraction of a great variety of solo instruments, aswell as being particularly efﬁcient, with computation timesthat allow real-time processing. The complete dataset wasprocessed and independent signals for the solo instrumentsand the backing tracks were extracted.4.3 Score-informedf0trackingThe original audio recordings are processed at a samplingrate of22.05kHz. In order to track thef0contour of eachtone, the signal is analyzed between the annotated note on-set and offset time, for which areassigned magnitude spec-trogramM2RK⇥N+withKfrequency bins andNframes824 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Performer Inst.NSNNPerformer Inst.NSNNPerformer Inst.NSNNPerformer Inst.NSNNArt Pepper cl/as 4 2134 David Liebman ss/ts 4 3286 John Coltrane ts/ss 11 8969 Sidney Bechet ss 2 489Ben Webster ts 4 1497 David Murray ts 4 2295 Joshua Redman ts 5 2429 Sonny Rollins ts 10 4639Benny Carter as 3 1153 Dexter Gordon ts 4 3056 Kenny Dorham tp 6 2149 Sonny Stitt ts 4 1284Benny Goodman cl 7 1154 Dizzy Gillespie tp 4 967 Lee Konitz as 4 1839 Stan Getz ts 6 3253Bix Beiderbecke tp 4 519 Don Byas ts 7 2022 Lester Young ts 4 887 Steve Coleman as 3 1353Bob Berg ts 5 3275 Eric Dolphy as 2 1109 Louis Arm-strongtp 4 634 Steve Lacy ss 4 1437CannonballAdderleyas 5 2623 Fats Navarro tp 4 937 MichaelBreckerts 4 2605 Wayne Shorter ts 9 3013Charlie Parker as 6 1688 Freddie Hub-bardtp 6 2266 Miles Davis tp 7 2080 Woody Shaw tp 5 1822Chet Baker tp 6 1100 J.J. Johnson tb 2 754 Ornette Cole-manas 3 1782Clifford Brown tp 4 1676 Joe Henderson ts 6 3830 Paul Desmond as 8 2142ColemanHawkinsts 6 2613 Joe Lovano ts/ts-c2 1787 Roy Eldridge tp 6 1744Table 1: Overview over all artists in the dataset. For each artist, the number of solosNS, the total number of notesNN, aswell as the instrument is given (ts: tenor saxophone, ss: soprano saxophone, as: alto saxophone, cl: clarinet, tp: trumpet,cor: cornet, tb: trombone, ts-c: C melody tenor saxophone).is computed. We use a logarithmic frequency axis with ahigh resolution of 50 bins/semitone and a frequency rangeof±2 semitones around the annotated pitch. Based on aninitial short-time Fourier transform (STFT) with a block-size of 1024, a hopsize of 64, and a zero-padding factor of16, the magnitude values are mapped (reassigned) towardsthe frequency bins that correspond to their instantaneousfrequency values at the original frequency bins computedusing the method proposed by Abe in [1]. Two steps areperformed for each tone to estimate itsf0contour. First,we estimate a suitablestarting framewithin the tone’s du-ration with a prominent peak close to the annotated pitch.Second, we perform acontour trackingboth forwards andbackwards in time. Further details are provided in [3].4.4 Tuning Frequency EstimationThe oldest recordings in our dataset date back to the year1924, two years before the American music industry rec-ommended 440 Hz for A4 as standard tuning, and 12 yearsbefore the American Standards Association ofﬁciallyadopted it. Hence, we can not rely on the assumption ofa constant and ﬁxed overall tuning. Moreover, the techni-cal level of recording studios were rather low at this time,which might result in tuning deviations by speed ﬂuctu-ations of recording machines as well as from instrumentstuned to another reference pitch such as studio or live venuepianos. Hence, we estimate areference tuning frequencyfrefprior to the intonation analysis of the solo instrumentfrom the backing track of the rhythm section, which we ob-tain from the source separation process explained in Sec-tion 4.2. The reference tuning frequency corresponds tothe fundamental frequency of the pitch A4 in the backingtrack.In the Chroma Toolbox [13], a triangular ﬁlterbank isgenerated based on a given tuning frequency in such waythat its center frequencies are aligned to the chromatic scalewithin the full piano pitch range. For a given audio signal,the magnitude spectrogram is averaged over the full signalduration and processed using the ﬁlterbank. Bymaximizing the ﬁlterbank output energy over different tun-ing frequency hypotheses, a ﬁnal tuning frequency esti-mateˆfrefis derived. We modiﬁed the originally proposedsearch range forˆfrefto440Hz±0.5semitone (correspond-ing MIDI pitch range:69±0.5) and the stepsize to0.1cents. As will be shown in Section 5.1, the inﬂuence ofsource separation artifacts on the estimation accuracy ofthe reference tuning frequency can be neglected.4.5 Feature ExtractionBased on the estimated contourf0(n)of each tone, we ﬁrstperform a smoothing using a two-element moving aver-age ﬁlter in order to compensate for local irregularities andpossible estimation errors. The extracted audio featuresdescribe thegradientof thef0contour as well as its tem-poralmodulation. Table 2 lists all computed audio featuresand their dimensionality.Category Feature Label Dim.Gradient Linear slope 1Gradient Median gradients (ﬁrst half, second half, over-all)3Gradient Ratio of ascending frames 1Gradient Ratio of ascending / descending / constantsegments3Gradient Median gradient of longest segments 1Gradient Relative duration of longest segments 1Gradient Pitch progression 1Modulation Modulation frequency [Hz] 1Modulation Modulation dominance 1Modulation Modulation range [cent] 1Modulation Number of modulation periods 1Modulation Average relative / absolutef0deviation 2Modulationf0deviation inter-quartile-range 1Table 2: Summary of audio features to descript thef0con-tours.4.5.1 Gradient featuresBased on the gradient\u0000f0(n)=f0(n+1)\u0000f0(n), we ﬁrstdetermine frames and segments of adjacent frames with as-cending (\u0000f0(n)>0), descending (\u0000f0(n)<0), andconstant frequency. We use the relative duration (with re-spect to the note duration) of each segment class as fea-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 825tures. Also, we compute median gradients in the ﬁrst andsecond halves, over the whole note, as well as over thelongest segment. Overall pitch progression is measured bythe difference of averagef0values in the end and begin-ning of each tone. Furthermore, we use linear regressionto estimate the linear slope of thef0contour.4.5.2 Modulation featuresWe analyze the modulation of thef0contour by comput-ing the autocorrelation overf0(n). Fletcher [7] reportedfor woodwind instruments that a vibrato frequency rangebetween 5 and 8 Hz is comfortable for listeners and com-mon for players. We add a safety margin of 2 Hz andsearch for the lag position⌧maxof the highest local maxi-mum within the lag range that corresponds to fundamentalfrequency values offmod2[3,10]Hz and estimate themodulation frequency asˆfmod=1/⌧max. The differencebetween the maximum and median magnitude within thisfrequency band is used as dominance measure for the mod-ulation. Other applied features are the number of modula-tion periods and the frequency modulation range in cent.4.6 Analysis of Intonation and ModulationTechniquesWe distinguish three modulation techniquesfall-off,slide,andvibrato. Table 3 provides a description of the charac-teristicf0contour shape for each technique. The numberof tones in our dataset annotated with each technique isgiven in Table 3.Technique Description NotesFall-off Drop of thef0contour in the end of the tone after astationary part.146Slide Rise or drop of thef0in the beginning of the tonetowards a stationary part.708Vibrato Periodic modulation of thef0contour during the sta-tionary part of the tone.1380None No discernible modulation of thef0contour / Nomodulation technique annotated.83587Table 3: Frequency modulation techniques considered andnumber of annotated notes in the dataset for each tech-nique.5. RESULTS5.1 Inﬂuence of Source Separation on the ReferenceTuning EstimationAfter the application of source separation algorithms, partsof the isolated solo instrument often remain audible in thebacking track due to artifacts or interference. We ﬁrst in-vestigated the inﬂuence of the source separation step de-scribed in Section 4.2 on the reference tuning estimation.A subset of 13 solos was randomly selected from the dataset,covering various recording decades and solo instruments.For each solo, we took a 20s segment from the originalrecording, where only the rhythm section and no solo in-strument is playing. We used the tuning estimation methoddescribed in Section 4.4 on both this 20s segment as wellas on the backing track obtained from the source separationof the solo part (compare Section 4.2) to get two estimatesfNoSolorefandfBackingrefof the reference tuning frequency.The results show a very high sample correlation ofr=0.97(p<0.001)and a small root mean squared error ofRMSE = 1.05Hz between both estimates. These resultsindicate that the inﬂuence of source separation artifacts isnegligible for the tuning estimation process. Therefore, wewill useˆfref=fBackingrefas an estimate of the referencetuning frequency throughout the paper.5.2 Relationship between the Reference Tuning andthe Recording Year / DecadeHow did the tuning frequencyfrefof commercial jazzrecordings change during the 20th century? Figure 3 showsthe distribution of solos in the dataset over the from the1920s to the 2000s. Moreover, the inserted boxplots illus-trate the deviation\u0000f= 1200 log2fref440between the tuningfrequencyfrefand 440 Hz in cent.Absolute tuning deviation|\u0000f|and recording year ofeach solo are weakly negatively correlated (r=\u00000.33,p<0.001). Hence, the absolute deviation from the tun-ing frequency from 440 Hz decreased over the the courseof the 20th century, reﬂecting the spread of the 440 Hzstandard (1955 adopted by the International Standards Or-ganization), as well as the progress of studio technology.\n−60−40−200204060801920−19301930−19401940−19501950−19601960−19701970−19801980−19901990−2000Recording Year Decade∆ f [cent] | Number of solosFigure 3: The box plot shows the reference tuning de-viation from 440 Hz in cent for different recording yeardecades. The bars show the number of solos in the datasetfor each decade.5.3 Dependency of Intonation from Artist andInstrumentThe distribution of the absolute deviation of tone-wise fun-damental frequency values from the estimated tuning fre-quency as well as modulation range are shown for all mu-sicians in Figure 4, and for all instruments in Figure 5.According to Figure 4, the overall pitch intonation ofjazz musicians is astonishingly accurate. Some woodwindand brass players tend to play a bit sharp, few a bit ﬂat—but throughout in a range of less than 25 cent. There are826 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015few exceptions: Sidney Bechet, a traditional soprano saxo-phonist, has very high values; however, presumably this iscaused not by a sharp intonation but by the high percentageof pitch slides played by him (almost 15 % of the tones, cf.Figure 2).For most players, the range of frequency modulation,i.e., the size of vibrato, is around 25 cent. There are somebigger modulation ranges from 35 to 50 cent, predomi-nantly used by tenor saxophone players associated withswing style (Ben Webster, Coleman Hawkins, Don Byas,and Lester Young), but also by postbop tenor saxophonistJoe Lovano, and, again, by Sidney Bechet, showing thelargest variance of modulation ranges. Therefore, there aresome slight personal and stylistic peculiarities in the useof vibrato size. However, there are no obvious trends ofintonation according to different instruments (cf. Figure5), since for each instrument there seem to be players whoplay a bit sharp as well as players who play a bit low; notethat for trombone and c-melody sax there is only one mu-sician (J.J. Johnson resp. Joe Lovano) in our sample. Like-wise, there is no evidence for general trends of modulationranges with respect to instrument.\n−50−250255075100ArtPepBenCarBenGooBenWebBixBeiBobBerCanAddChaParCheBakCliBroColHawDavLieDavMurDexGorDizGilDonByaEriDolFatNavFreHubJ.JJohJoeHenJoeLovJohColJosRedKenDorLeeKonLesYouLouArmMicBreMilDavOrnColPauDesRoyEldSidBecSonRolSonStiStaGetSteColSteLacWayShoWooShaAvF0Dev [cent]\n0255075100ArtPepBenCarBenGooBenWebBixBeiBobBerCanAddChaParCheBakCliBroColHawDavLieDavMurDexGorDizGilDonByaEriDolFatNavFreHubJ.JJohJoeHenJoeLovJohColJosRedKenDorLeeKonLesYouLouArmMicBreMilDavOrnColPauDesRoyEldSidBecSonRolSonStiStaGetSteColSteLacWayShoWooShaModRange [cent]Figure 4: Absolute deviation of tone-wise fundamentalfrequency values from the estimated tuning frequency incent and modulation range in cent for all musicians (fortheir full names see Table 1).\n−50−250255075100asclcorsstbtptsts−cAvF0Dev [cent]0255075100asclcorsstbtptsts−cModRange [cent]Figure 5: Absolute deviation of note-wise fundamentalfrequency values from the estimated tuning frequency incent and modulation range in cent for all instruments.5.4 Context-dependency of the Modulation Frequencyof VibratoDoes the modulation frequency of vibrato depend on pitch,or duration of the vibrato tones, or on the tempo of thepiece? For the 1380 tones with vibrato notes (cf. Table3), we found no signiﬁcant correlations between modula-tion frequency and pitch (r=0.02,p=0.42), duration(r=0.02,p=0.5), nor tempo (r=0.0,p=0.83).The small effect size of the correlation indicates that de-spite the high variety of tempo values in the dataset (meantempo 154.52 bpm, standard deviation 68.16 bpm), themodulation frequency only slightly increases with increas-ing tempo.Furthermore, we investigated, whether and how the mo-dulation frequency of vibrato is connected to the under-lying metrical structure of a solo. We computed the ra-tior=Tmod/Tsolobetween the modulation tempo andthe average tempo of the solo. The modulation tempo iscomputed asTmod= 60fmod. Figure 6 shows the ratioragainst the average tempo of the solo. There is no ev-idence in our data for a strategy to adapt the modulationfrequency of vibrato to integer multiples of the tempo ofthe piece, e.g., to use a vibrato speed according to simplesubdivision of the beat (e.g. eighth notes or eighth triplets).As Figure 6 shows, for medium and fast tempos (100 to350 bpm) the vibrato frequency varies only between thebeat and the 16th note level. For slower tempos, the vi-brato tempo could be up to six or seven times as fast as thebeat—but rarely much faster.\n50100150200250300350024681012Tmod / Tsolo\nTsoloFigure 6: Ratio between the modulation frequency of vi-brato tones and the average tempo of a piece vs. the aver-age tempo.5.5 Artist-dependency of the Modulation Frequencyof VibratoAlthough there is no obvious correlation between modu-lation frequency and pitch, duration, or tempo, there aresome peculiarities of musicians according to vibrato modu-lation speed. In Figure 7 only those musicians are includedfor which more than twenty annotated vibrato tones couldbe found in our data set. All in all, there seems to be noclear correlation between vibrato speed and jazz style orinstrument, which indicates that modulation technique ismostly an idiosyncratic part of personal styles. Strikingly,several trumpet players can be found there (Louis Arm-strong, Kenny Dorham, Roy Eldridge) using vibrato to anconsiderable amount and size. This is in sharp contrast toProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 827246810ArtPepBenCarBenGooBenWebBixBeiChaParCheBakDavMurDonByaFreHubKenDorLeeKonLouArmPauDesRoyEldSidBecSonStiStaGetModulation frequency [Hz]Figure 7: Modulation frequency in Hz in vibrato notes fordifferent performers. Only performers with more than 20vibrato notes are shown.playing standards for brass instruments in classical music,where it is custom to play without any vibrato [7].5.6 Automatic Classiﬁcation of FrequencyModulation TechniquesUsing the set of features discussed in Section 4.5, we ex-tracted an 18-dimensional feature vector for each tone,which was used to automatically classify tones with re-spect to their modulation class. To this end, we only con-sidered tones annotated with fall-off, slide, and vibratosince all remaining tones were not explicitly annotated. Weused a Support Vector Machine (SVM) classiﬁer with a lin-ear kernel function as classiﬁcation algorithm and performa 10-fold cross-validation. Due to the imbalanced classsizes (cf. Table 3), we repeatedly re-sampled from the ex-isting class items such that all classes have the same num-ber of items as the largest class from the original dataset.The confusion matrix is shown in Table 4. The highestaccuracy of 92.25 % was achieved for vibrato tones. Theclasses fall-off and slide show lower accuracy values of48.04 % and 67.32 %, respectively. One might assume,that the similarf0contour shapes of fall-offs and the slide-downs causes part of the confusions between both classes.Correct ClassiﬁedFall-off Slide VibratoFall-off 48.0437.46 14.49Slide23.5567.329.13Vibrato4.06 3.792.25Table 4: Confusion matrix for the automatic classiﬁcationof frequency modulation techniques. All values are givenin percent.6. CONCLUSIONSIn this exploratory study, we proposed ascore-informed algorithm for the extraction ofnon-syntactical features in jazz solos played with wind andbrass instruments. This method allows for an analysis ofperformative and expressive aspects of jazz improvisationwhich are not captured by the traditional approaches ofjazz research such as transcriptions (even though some rudi-mentary notation forf0-modulations are used sometimes).Combining transcriptions with state-of-art MIR algorithmssigniﬁcantly enhances the methodical and analytical toolbox of jazz research (as well as other subﬁelds of mu-sicology and performance studies). In turn, this kind ofﬁne-structured analysis might be useful in guiding auto-matic transcription algorithms by providing relevant back-ground information on tone characteristics. Moreover, inthis study we demonstrated exemplarily that our methodcan be readily applied for a range of different researchquestions, from historical analysis of reference tuning in20th century jazz recordings to more general questions suchas intonation accuracy or differences inf0modulationswith respect to tempo, instrument class, stylistic trends, orpersonal style.As a case study, we investigated whether some these ex-pressive aspects, i.e., intonation, slides, vibrato speed andvibrato range, are correlated with structural features of thesolos (absolute pitch, tone duration, overall tempo, meter)and whether those aspects are characteristic for an instru-ment, a jazz style or the personal style of a musician. Whilethere is little evidence for a general correlation between in-tonation and pitch modulation (slide, vibrato) on the onehand, and structural features on the other hand, the issueof how intonation and pitch modulation contributes to theformation of a jazz style and personal style needs furtherexamination with more data and including listening testsfor style discrimination.For the future, we plan to complete and reﬁne thef0-modulation annotations for the dataset, with the overallgoal of the design of an automatedf0-modulation anno-tation algorithm. Finally, we aim at a complete descriptionof personal timbre characteristics, the so-called “sound” ofa player, which is an important dimension of jazz music,and not yet fully addressed. Dynamics [2], intonation, ar-ticulation, andf0-modulation are part of this “sound”, butother aspects such as breathiness, roughness and generalspectral characteristics (and their classiﬁcation) are still tobe explored.7. ACKNOWLEDGEMENTSThe Jazzomat research project is supported by a grant DFG-PF 669/7-1 (“Melodisch-rhythmische Gestaltung von Jazz-improvisationen. Rechnerbasierte Musikanalyse einstim-miger Jazzsoli”) by the Deutsche Forschungsgemeinschaft(DFG). The authors would like to thank all jazz and mu-sicology students participating in the transcription and an-notation process.828 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Toshihiko Abe, Takao Kobayashi, and Satoshi Imai.Harmonics tracking and pitch extraction based on in-stantaneous frequency. InProceedings of the 1995IEEE International Conference on Acoustics Speechand Signal Processing (ICASSP), pages 756–759, De-troit, USA, 1995.[2]Jakob Abeßer, Estefanía Cano, Klaus Frieler, and Mar-tin Pﬂeiderer. Dynamics in jazz improvisation - score-informed estimation and contextual analysis of tone in-tensities in trumpet and saxophone solos. InProceed-ings of the 9th Conference on Interdisciplinary Musi-cology (CIM), Berlin, Germany, 2014.[3]Jakob Abeßer, Martin Pﬂeiderer, Klaus Frieler, andWolf-Georg Zaddach. Score-informed tracking andcontextual analysis of fundamental frequency contoursin trumpet and saxophone jazz solos. InProceedingsof the 17th International Conference on Digital AudioEffects (DAFx-14), Erlangen, Germany, 2014.[4]Estefanía Cano, Gerald Schuller, and ChristianDittmar. Pitch-informed solo and accompaniment sep-aration: towards its use in music education applica-tions.EURASIP Journal on Advances in Signal Pro-cessing, 23:1–19, 2014.[5]Simon Dixon, Dan Tidhar, and Emmanouil Benetos.The temperament police: the truth, the ground truth,and nothing but the truth. InProceedings of the 12thInternational Society for Music Information RetrievalConference (ISMIR), pages 281–286, Miami, USA,2011.[6]Karin Dressler. Pitch estimation by the pair-wise evalu-ation of spectral peaks. InProceedings of the 42nd AESInternational Conference on Semantic Audio, pages 1–10, Ilmenau, Germany, 2011.[7]N. H. Fletcher. Vibrato in music – physics and psy-chophysics. InProceedings of the International Sym-posium on Music Acoustics, pages 1–4, 2010.[8]David Gerhard. Pitch track target deviation in naturalsinging. InProceedings of the 6th International Con-ference on Music Information Retrieval (ISMIR), pages514–519, London, UK, 2005.[9]Chao-Ling Hsu and Jyh-Shing Roger Jang. Singingpitch extraction by voice vibrato/tremolo estimationand instrument partial deletion. InProceedings of the11th International Society for Music Information Re-trieval Conference (ISMIR), pages 525–530, Utrecht,Netherlands, 2010.[10]Christian Kehling, Jakob Abeßer, Christian Dittmar,and Gerald Schuller. Automatic tablature transcriptionof electric guitar recordings by estimation of score- andinstrument-related parameters. InProceedings of the17th International Conference on Digital Audio Effects(DAFx-14), Erlangen, Germany, 2014.[11]Alexander Lerch. On the requirement of automatic tun-ing frequency estimation. InProceedings of the 7th In-ternations Conference on Music Information Retrieval(ISMIR), Victoria, Canada, 2006.[12]Sai Sumanth Miryala, Kalika Bali, Ranjita Bhagwan,and Monojit Choudhury. Automatically identifying vo-cal expressions for music transcription. InProceedingsof the 14th International Society for Music InformationRetrieval Conference (ISMIR), Curitiba, Brazil, 2013.[13]Meinard Müller and Sebastian Ewert. Chroma tool-box: Matlab implementations for extracting variantsof chroma-based audio features. InProceedings of the12th International Society for Music Information Re-trieval Conference (ISMIR), pages 215–220, Miami,USA, 2011.[14]T. Özaslan, Serra X., and Arcos J. L. Characteriza-tion of embellishments in ney performances of makammusic in turkey. InProceedings of the 13th Interna-tional Society for Music Information Retrieval Confer-ence (ISMIR), pages 13–18, 2012.[15]Matti P. Ryynänen and Anssi Klapuri. Automatic tran-scription of melody, bass line, and chords in poly-phonic music.Computer Music Journal, 32:72–86,2008.[16]Justin Salamon, Geoffroy Peeters, and Axel Röbel. Sta-tistical characterization of melodic pitch contours andits application for melody extraction. InProceedingsof the 13th International Society for Music InformationRetrieval Conference (ISMIR), Porto, Portugal, 2012.[17]Joan Serrá, Gopala K. Koduri, Marius Miron, andXavier Serra. Assessing the tuning of sung indian clas-sical music. InProceedings of the 12th InternationalSociety for Music Information Retrieval Conference(ISMIR), pages 157–162, Miami, USA, 2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 829"
    },
    {
        "title": "Emotion Based Segmentation of Musical Audio.",
        "author": [
            "Anna Aljanaki",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418201",
        "url": "https://doi.org/10.5281/zenodo.1418201",
        "ee": "https://zenodo.org/records/1418201/files/AljanakiWV15.pdf",
        "abstract": "The dominant approach to musical emotion variation detection tracks emotion over time continuously and usu- ally deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener’s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is nec- essary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of dif- ferent segmentation methods as applied to a task of emo- tional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have struc- tural annotations from the SALAMI dataset. We investi- gate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agree- ment on emotional segmentation. Lastly, we evaluate dif- ferent unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best.",
        "zenodo_id": 1418201,
        "dblp_key": "conf/ismir/AljanakiWV15",
        "keywords": [
            "time resolutions",
            "emotional variation",
            "listeners point of view",
            "progression of emotionally stable segments",
            "segmenting music at emotional boundaries",
            "formal evaluation",
            "structural segmentation",
            "emotional segmentation",
            "inter-annotator agreement",
            "unsupervised segmentation methods"
        ],
        "content": "EMOTION BASED SEGMENTATION OF MUSICAL AUDIOAnna AljanakiUtrecht UniversityA.Aljanaki@uu.nlFrans WieringUtrecht UniversityF.Wiering@uu.nlRemco C. VeltkampUtrecht UniversityR.C.Veltkamp@uu.nlABSTRACTThe dominant approach to musical emotion variationdetection tracks emotion over time continuously and usu-ally deals with time resolutions of one second. In this paperwe discuss the problems associated with this approach andpropose to move to bigger time resolutions when trackingemotion over time. We argue that it is more natural fromthe listener’s point of view to regard emotional variation inmusic as a progression of emotionally stable segments. Inorder to enable such tracking of emotion over time it is nec-essary to segment music at the emotional boundaries. Toaddress this problem we conduct a formal evaluation of dif-ferent segmentation methods as applied to a task of emo-tional boundary detection. We collect emotional boundaryannotations from three annotators for 52 musical piecesfrom the RWC music collection that already have struc-tural annotations from the SALAMI dataset. We investi-gate how well structural segmentation explains emotionalsegmentation and ﬁnd that there is a large overlap, thoughabout a quarter of emotional boundaries do not coincidewith structural ones. We also study inter-annotator agree-ment on emotional segmentation. Lastly, we evaluate dif-ferent unsupervised segmentation methods when appliedto emotional boundary detection and ﬁnd that, in terms ofF-measure, the Structural Features method performs best.1. INTRODUCTIONImproving automatic music emotion recognition (MER)methods is crucial to enhance accessibility of large musiccollections for both personal and commercial use. Drivenby this interest, the MER ﬁeld greatly expanded in the lastdecade. One of the fundamental MER problems is trackingemotion over time, or music emotion variation detection(MEVD). This problem is usually approached by a contin-uous approach to MER (dynamic MER), when the emotionof a piece of music is predicted on a second-by-second ba-sis. Though dynamic MER does not actually assume thatemotion in music should change every second, the currentmethods tend to work on very low time resolutions bothby choosing rather short excerpts where no serious mu-sical development could occur (e.g., 15 seconds) and byc\u0000Anna Aljanaki, Frans Wiering, Remco C. Veltkamp.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Anna Aljanaki, Frans Wiering, RemcoC. Veltkamp. “Emotion based segmentation of musical audio”, 16th In-ternational Society for Music Information Retrieval Conference, 2015.collecting the ground truth with certain task demands onthe annotators. It has been notoriously difﬁcult to collect aground truth for MEVD with a reasonable inter-annotatoragreement, and the reason may lie in the fact that musi-cal meaning is usually communicated during bigger timespans than several seconds, and it is therefore difﬁcult andunnatural for the listeners to evaluate their emotional re-sponse to music in such a way. Though it might still be in-teresting and important to track musical change over time,the question should be raised whether change on such ashort time scale is actually an expression of musical emo-tion orthe means of creatingemotional expression on ahigher level (e.g., accelerando or crescendo).A bordering MER ﬁeld (static MER) studies identiﬁ-cation of emotion in somewhat longer musical segments.Static MER methods usually deal with excerpts of 15 to30 seconds. It is natural for listeners to describe musicalcontent by applying emotional labels to musical excerptsor complete pieces. This kind of labels are used by mostmusic services to categorize their data. However, the realworld problem of MEVD requires music to be presegmen-ted into fragments with stable emotion. This problem isusually just neglected by static MER methods, which of-ten use ground-truth excerpts picked by randomly sam-pling the audio and ﬁltering out the excerpts that receivecontradictory ratings from experts. Also, sometimes theproblem is solved (or rather avoided) by trying to pick themost representative part of the song for classiﬁcation (e.g.,chorus).Hence, many questions about emotional segmentationof music remain unsolved. What is a typical length ofan emotionally stable fragment in music? (Ironically, bothstatic and dynamic MER methods usually deal with musi-cal excerpts of more or less the same lengths, ranging from15 to 45 seconds in an attempt to cover as much differentmusic as possible while reducing the annotation burden.)Is emotional segmentation explained by structural segmen-tation? How many emotional boundaries are there typi-cally in a piece of music? Which segmentation methodswork best when applied to emotional boundary detection?These are the questions that we are going to deal within this paper. For these purposes we assemble a dataset of52 double-annotated pieces from the RWC music database[6], which also have structural annotations in the SALAMIdataset [13]. We obtain a little under 2000 annotated emo-tional boundaries (around 630 from each of the annota-tors). We compare emotional and structural segmentationof music, analyze the inter-annotator agreement and the770average stable segment length. Then we apply four seg-mentation algorithms to emotional segmentation problemand benchmark them on our dataset. Though the datasetis not big, a formal evaluation of emotional segmentationperformance has never been conducted before.In this work, we are not going to deal with MER in a tra-ditional sense (predicting emotion from a musical excerpt).There already exist numerous state-of-the-art approachesto this problem [20]. Here we will address the questionhow to do the preprocessing step before static MER, i.e.,emotional segmentation of music.The rest of the paper is organized as follows. In section2 we describe related research. In section 3 we explainwhy dynamic MER methods, at least in their current form,might not produce a good solution to the MEVD problem.In section 4 we analyze the obtained emotional segmen-tation. In section 5 we compare different segmentationmethods when applied to a problem of detecting emotionalboundaries in music. Section 6 concludes the paper.2. RELATED WORKThough the problem of emotional boundary detection hasnot yet been addressed systematically, there exist MERmethods that can be applied to this problem, and we willreview them in this section. For a more general overviewof MER, [20] can be consulted.2.1 Static MER for MEVDThe most simple approach to MEVD when using a staticMER method is detecting emotion over time using a slid-ing window. This method would give a distorted resultwhen a sliding window has an emotional boundary in it.In [21], a sliding window of ten seconds and1/3overlapis used to segment a music piece, and a fuzzy classiﬁeris trained to detect the emotion of the segments. In [9]it is suggested that a homogeneous music segment is usu-ally around 16 seconds, and therefore a sliding window of16s is used to detect the boundaries by comparing featuredistributions from neighboring windows. This approach isshown to be viable, though many questions are left open.For instance, only two features — intensity and timbre —are tested, and the evaluation is conducted only on 9 pieces.A similar approach is attempted in [15] to solve a multi-label classiﬁcation problem (with two sliding windows of10s and 30s). It is concluded that a more sophisticatedemotional segmentation strategy is needed. Multi-labelclassiﬁcation approaches recognize that one musical piececan express a variety of emotions and several labels areapplicable to one piece. However, the music is often stillhandled in the same way as in the static MER approach. Ashort excerpt (e.g., 30s) is selected ( [16], [17]), and severallabels are applied to it, which addresses the problem of mu-sical ambiguity, but not musical change. As opposed to thisapproach, in [18] a multi-label classiﬁcation was appliedto whole musical pieces, which were pre-segmented us-ing aligned lyrics annotations on an assumption that mostoften emotion is stable within one sentence. Then, a hi-erarchical Bayesian model was applied to a task of multi-label classiﬁcation. Due to the absence of ground-truth onemotional boundaries in [18], it is left unclear how wellthe annotated sentences in the lyrics actually correspond toemotional structure of the musical piece.To answer the question of what is the typical length ofmusical segments that represent stable emotion, Xiao etal. tried to classify excerpts of different lengths by emo-tion and found that excerpts of 8 or 16 seconds have abetter classiﬁcation accuracy than excerpts of 4 or 32 sec-onds [19]. This experiment gives an indirect indication ofemotional segmentation resolution.2.2 Dynamic MERDynamic MER methods are usually trained on time-seriesof annotations, typically with a resolution of 1 or 2 Hz. InKorhonen et al. [7], musical emotion is modeled as a func-tion of musical features using system identiﬁcation tech-niques. In [11], conditional random ﬁelds were used tomodel continuous emotion with a resolution of11⇥11invalence-arousal space. A similar strategy was employedin [4], where dynamic texture models were trained corre-sponding to quadrants of resonance-arousal-valence modeland applied to predict musical emotion continuously. Whenseparate models are trained to predict different emotions,emotional boundary detection occurs naturally. This ap-proach might be problematic, however, due to lack of reso-lution in the emotional space. Also, for boundary detectionit might be more important to keep track of the local con-text and relative changes in musical attributes rather thanpredict an absolute rating at every moment. This is whyunsupervised methods might work very well in this case.3. MOTIVATIONWhile static MER methods cannot deal with emotionallynon-homogenous music, dynamic MER methods approachthis problem by taking the fragmentation to the extreme(the typical resolution of a dynamic MER method is 1 sec-ond), which might create even more problems than it solves.Firstly, the output (per-second emotion prediction) produ-ced by a dynamic MER method is not easily interpretableand useful. Secondly, it seems that musical emotion is notconceptualized in this way by listeners.3.1 Analyzing dynamic MER ground-truthDynamic MER relies on human ground-truth in the formof per-second emotional annotations, which are typicallyrecorded from an annotator continuously moving their cur-sor in a one or two-dimensional space [1, 14]. It seemsthat this task is extremely difﬁcult for humans, which is, inparticular, indicated by a very low inter-annotator agree-ment as compared to static annotations (where, due to tasksubjectivity, it is also not very high). For the MediaEvaldataset [1], the average Kendalls W is0.23±0.16forarousal and0.28±0.21for valence, and for the Mood-Swings Lite dataset [14] the mean Kendall’s W is0.21±0.14for arousal and0.23±0.17for valence. All theseProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 771Figure 1. Dynamic annotation of 45 seconds of audio from[1]. One third of the annotators react to every beat of slowmusic by a peak in arousal.numbers indicate weak agreement. There are several typi-cal problems arising when annotating music continuously:1.A dimensional annotation interface has an absolutescale. For instance, on an axis with a slider control-ling valence, the leftmost side represents the mostmiserable music imaginable, and the rightmost themost ecstatic one. Giving absolute ratings is rela-tively easy when evaluating music statically (com-paring a piece to all existing music). When compar-ing piece with itself over time, humans tend to thinkof occurring changes relatively. This leads to a hugedifference in magnitude of given ratings, though thedirection of change can be indicated uniformly (e.g.,see Figure 1).2.Though it is not explicitly requested from the anno-tators to move their cursor at all times, the task de-mands (short excerpt, necessity to track and respondcontinuously) lead to some of the annotators evalu-ating every single musical event (e.g., see Figure 1).This results in annotations on widely different ‘zoomlevel’.We argue that continuous annotation is so difﬁcult (al-beit through training in the lab and a careful selection ofcomplete music pieces it is possible to obtain satisfyingresults [3]) because it is unnatural for humans to evalu-ate their emotional response on a per-second basis, sinceemotional expression occurs on a much larger time-scale.Though through years of exposure to music listeners ac-quire an ability to associate certain timbres with genre andemotion, and a crude emotional interpretation is possibleeven from short sounds snippets of 300ms [8], we believethat real-life emotional interpretation of music is much morecomplex and happens during longer time spans, most cer-tainly when it concerns induced emotion.\nFigure 2. Histogram of segment durations for the threeannotators separately.4. ANALYSIS OF EMOTIONAL BOUNDARIES4.1 DataThe dataset consists of 52 complete pieces [6] from Pop,Jazz and Genre (the latter contains rock, soul, world etc.music) collections of RWC music database. We picked thepieces that already had SALAMI [13] annotations in orderto compare structural and emotional segmentation. TheSALAMI annotations for these pieces are single-keyed,our annotations are triple-keyed in order to enable mea-suring agreement.The three annotators received instructions to mark whenemotion of the piece changes. There were no explicit in-structions as to what could be interpreted as an emotionalboundary. They were also instructed to mark the transi-tions between stable emotional states as separate sections,in case those were long enough to be perceived as sepa-rate ‘transition states’. In practice, this meant for instancemarking long diminuendo (fade-out) at the end of a musi-cal piece as a separate section.In total, annotators marked 562, 602 and 746 emotionalboundaries, respectively. The dataset is available from thewebsiteosf.io/jpd5z.Evaluation metricA2!A1A3!A2A1!A3Precision @ 0.50.470.430.52Recall @ 0.50.480.330.55F-measure @ 0.50.460.370.67Precision @ 30.730.880.72Recall @ 30.760.790.88F-measure @ 30.730.770.78Table 1. Inter-annotator boundary retrieval with a toler-ance window of 0.5 and 3 seconds.4.2 Inter-annotator agreementThe mean number of boundaries per piece was 12.2 (me-dian = 11.5). The average segment length was19.5±18s.772 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2 shows the histograms of segment lengths fromthe three annotators. We can see that the distribution isskewed, 90% of intervals are shorter than 37 seconds. An-notators 1 and 3 have annotated more short segments thanannotator 2, which was caused mostly by their differentdecisions about short (1–3 seconds) transition segments inmusic (e.g., short pauses between verse and chorus).Unfortunately, segmentation tasks are not well-adaptedfor formal inter-annotator agreement calculation. We per-form the standard F-measure evaluation as is common inthe literature [13]:F1=2precision·recallprecision+recall.(1)Table 1 shows the F-measure at 0.5 and 3 seconds. Themetrics are similar to those obtained for the structural seg-mentation task, though a bit lower for a 0.5s window [13].It seems that 0.5s window is too strict for these particularannotations. This might be caused by the nature of the task.Though some emotional boundaries are rather abrupt, oth-ers are smeared by a transitional musical process necessaryfor an emotion to modulate from one state to another.4.3 Structural segmentation explaining emotionalsegmentationIn order to check how well emotional segmentation is ex-plained by structural segmentation we compared the emo-tional boundary annotations to structural boundaries in theSALAMI dataset. The SALAMI dataset contains hierar-chical annotations on multiple levels — musical function(verse, chorus, etc.), lead instrument, and musical similar-ity on large and small scale. Table 2 shows the precision,recall and F-measure obtained when predicting emotionalsegmentation from structure. From the table we can seethat about 69 to 80% of the emotional boundaries coin-cide with large section boundaries. More than a half ofthe boundaries coincide with the lead instrument change.Small-scale similarity was not included in the table be-cause of the abundance of small-scale boundaries (mean-ing close to 100% recall and very low precision). We alsodidn’t include the 0.5s time resolution, because emotionalsegmentation seems to be less precise than structural and0.5s time resolution is too detailed.It is important to note that, with regard to F-measure, theemotional annotations when retrieved from each other havea bigger score than with any of the structural segmentationannotations.5. SEGMENTATION METHODS EVALUATIONSegmentation methods are usually categorized into homo-geneity, novelty and similarity based methods. We arguethat for emotional boundary detection only the ﬁrst twocategories are relevant, because an emotional boundary isusually signiﬁed by changes in loudness, timbral proper-ties, harmony, instrumentation, etc., and though it mightcoincide with repetitive segments (i.e., chorus), there is nostraightforward connection between them. Hence, in thissection we are mostly going to evaluate homogeneity andnovelty based methods, namely Convex NMF [10], MoodTracking [9], the classic method by Foote [5] and Struc-tural Features [12]. We implemented the Mood Trackingmethod as described in the article, and adapted an imple-mentation1of the rest for our purposes (i.e., feature ex-traction, thresholds etc. as described below).All of these methods are unsupervised and take as in-put time-series of features extracted from audio. We ex-tract both low (mfcc, chroma, energy, dissonance and otherspectral features) and high-level (scale, tempo, tonal stabil-ity) beat-synchronized audio features using Essentia [2].Beats are determined using the Essentia BeatTracker algo-rithm. All the music ﬁles have 44100 Hz sampling rateand are converted to mono. To extract low-level timbralfeatures we use a half-overlapping window of 100ms, anda window of 3 seconds for high level features. The fea-tures are smoothed with median sliding window, normal-ized, and resampled according to detected beats (see Figure3a).We use the same feature set to evaluate all the algo-rithms. Many segmentation algorithms limit themselves tousing only MFCC or chroma features, but through experi-mentation with different feature sets we found that addingother spectral and high-level features signiﬁcantly improvesthe performance on our dataset.To combine the annotations, we decided to select onlythe boundaries which were marked by all the annotatorswith a tolerance window of 3 seconds. We will call themcommon. It can be assumed that the boundaries present inall the three annotations are the most prominent and im-portant ones.5.1 Summary of evaluated methods5.1.1 FooteFoote’s method [5] relies on a self-similarity matrix (com-posed using pairwise sample comparisons). A short-time1https://github.com/urinieto/SegmenterMIREX2014Evaluation metricFunctions Large scale InstrumentsA1 A2 A3A1 A2 A3A1 A2 A3Precision @ 30.61 0.68 0.670.63 0.63 0.670.52 0.50 0.51Recall @ 30.74 0.78 0.750.69 0.80 0.750.55 0.55 0.58F-measure @ 30.65 0.71 0.690.64 0.68 0.690.50 0.50 0.55Table 2. Retrieving emotional segmentation from structural segmentationProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 773C-NMF SF Foote MT (enh.)EvaluationmetricC A1 A2 A3C A1 A2 A3C A1 A2 A3C A1 A2 A3P@3.27 .35 .36 .47.33 .43 .49 .57.31 .38 .41 .50.18 .28 .27 .34R@3.71 .67 .69 .67.67 .61 .68 .61.72 .67 .72 .66.43 .47 .47 .41F@3.36 .43 .45 .52.41 .47 .55 .56.39 .45 .50 .53.23 .34 .33 .35Table 3. Performance of investigated methods on emotional segmentation task (F-measure).\nFigure 3. An illustration of the boundary detection pro-cess on theRadetzky Marchby J. Strauss Sr.. a) Beat-synchronized features. b) Annotations. c) Novelty curvesand detected boundaries.Gaussian checkerboard-shaped kernel is slided over the di-agonal of the matrix, resulting in a novelty curve. Theboundaries are detected by picking the peaks on the nov-elty curve. We experimented with different distance mea-sures to compute the SSM and found that standardized eu-clidean distance gave the best results, which is computedbetween two vectorsuandvas follows:qX(ui\u0000vi)2/V[xi],(2)where V is the variance vector; V[i] is the variance com-puted over all the ith components of the points. We set thesize of the checkerboard kernel to the size of the averageemotionally stable segment — 20 seconds.5.1.2 Convex NMFThe Convex non-negative matrix factorization method [10](Convex NMF) uses a convex variant of NMF in order todivide the audio features into meaningful clusters. Thisalgorithm focuses both on ﬁnding segments and groupingthem by similarity. If a NMF of input feature matrix X isFG, Convex NMF adds a constraint to the columns of thematrix F(f1,f2,. . . ,fn)that the columns should becomeconvex combinations of the features of X:fi=x1w1j+...+xpwpj=Xwj,j2[1 :r],(3)wherexpis a column of matrixX,ris a rank of decom-position, andwij\u00000,Pjwij=0. This makes columnsfiinterpretable as cluster centroids. We set the rank ofdecomposition to 2.5.1.3 Mood TrackingA method by Lu et al. [9] ﬁnds boundaries by comparingthe audio features extracted from the two consecutive win-dows of 16 seconds and computing a difference betweenthem. A novelty curve is formed using an obtained differ-ence feature, from which peaks are picked. The differencebetween the consecutive windows is computed using diver-gence shape measure:Di|i+1=12Tr⇥(Ci\u0000Ci+1)(C\u00001i+1\u0000C\u00001i)⇤,(4)whereCiandCi+1are the covariance matrices of featuresof windowsiandi+1. Then, conﬁdence of boundary iscomputed:Confi|i+1=e x p✓|Di|i+1\u0000Dmean|Dvar◆,(5)whereDmeanandDvarare the mean and variance of all di-vergence shapes for this song. From a list of boundaryconﬁdences the boundaries are retrieved by satisfying con-ditions of being a local maximum and exceeding a localadaptive threshold.We implemented the method as it was described in [9],but it didn’t work well in its original form on our data. Theconstraint of 16 seconds was too conservative and adaptivethreshold window was too narrow. We describe an opti-mized version below. The optimized version performs onaverage about 10% better than the original method, and weonly show the performance of the optimized version in Ta-ble 3.774 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20155.1.4 Enhanced Mood TrackingThe best results with Lu et al. method were obtained usinga window of 4 seconds to compute the divergence shapemeasure. We smoothed the boundary conﬁdence vectorwith a median ﬁlter before peak picking. To pick the peaks,we select a maximum in a neighbourhood of 10 beats incase it exceeds both of the two threshold – a moving aver-age and half of the global average.Though the performance of the method improved withmodiﬁcations, it still performed worse than other methodsin our evaluation.5.1.5 Structural FeaturesThe Structural Features (SF) method is both homogene-ity and repetition based. It uses a variant of lag matrix toobtain structural features. The SF are differentiated to ob-tain a novelty curve, on which peak picking is performed.The SF method calculates self-similarity between samplesiandjas follows:Si,j=⇥(\"i,j\u0000| |xi\u0000xj||),(6)where⇥(z)is a Heaviside step function,xiis a featuretime series transformed using delay coordinates,||z||is aEuclidean norm, and\"is a threshold, which is set adap-tively for each cell of matrixS. From matrixSstructuralfeatures are then obtained using a lag-matrix, and comput-ing the difference between successive structural featuresyields a novelty curve.5.2 Evaluation resultsTable 3 shows the results obtained in evaluation. We onlyuse a tolerance window of 3 seconds, because for our dataseta tolerance window of 0.5s is too strict. From the table wecan see that the SF method consistently shows the best re-sults in terms of F-measure. The method proposed in [9]consistently shows the worst results.6. DISCUSSIONIn this paper we discussed the problems associated withdynamic MER and argued that these problems originatefrom the unnaturally low time resolutions that dynamicMER is usually dealing with (Section 3). We proposedto move to bigger time resolutions by tracking emotionallystable segments over time and identifying transitions be-tween them. We call this problem emotion based segmen-tation, and conduct a formal evaluation procedure, whichhas not been done before for this task.We collected data on emotional segmentation of music;in total about 2000 emotional boundaries were annotated.In general, the annotators could agree rather well whenidentifying stable emotional segments, the inter-annotatorF-measure was comparable to the one obtained for, sup-posedly less ambiguous, structural segmentation task, ex-cept for the very high resolution level (0.5 s). In terms ofF-measure the emotional annotations coincide with eachother better than any of the structural segmentation lev-els. That means that there exist some robust and importantemotional boundaries which are not explained by structuralsegmentation.We compared emotional and structural segmentation andfound that emotional boundaries coincide with structuralboundaries very often. About half of the emotional bound-aries were accompanied by a lead instrument change. Ap-proximately 25% of the emotional boundaries did not coin-cide with the structural boundaries. For instance, an emo-tional change can occur within a structural section due to amodulation to a different tonality.We found that the average length of stable emotionalsegment is approximately 20 seconds. This ﬁnding couldbe used to calculate a suitable length of musical excerpts tobe employed for MEVD algorithms development and eval-uation. Namely, we believe that length of such excerptsshould be several times bigger than 20 seconds.We evaluated different unsupervised segmentation al-gorithms on the task of emotional segmentation and foundthat the SF method performed best. This segmentationmethod is different from the second best Foote’s method byincorporation of repetition-based criteria along with homo-geneity-based ones. This shows that sequences of emotion-ally stable segments, probably, repeat in the same way asstructural sequences, and therefore repetition-based cuesare useful for emotional boundary detection. This ﬁnd-ing goes against our initial intuition that novelty and ho-mogeneity cues must be the only ones important to de-tect emotional change. The Mood Tracking method wasdemonstrated to be least useful. This method only uses avery narrow local context to ﬁnd the discontinuities in afeature matrix, which appears to be not enough. We alsofound that employing higher level audio features, alongwith traditional chroma features and MFCCs, improves theperformance of the methods on emotional segmentationtask.Though SF’s method performed reasonably well, its per-formance was still much worse than the performance achie-ved by best methods for structural segmentation, which isa more mature area of research now. Developing betteremotional segmentation methods is a crucial task to enableapplying static MER algorithms to real world problems.We leave this task for future work, which can be facilitatedby the data provided in this study.7. ACKNOWLEDGEMENTSWe thank Kayleigh Hagen and Valeri Koort for assistancewith the data annotation. This research was supported byCOMMIT/.8. REFERENCES[1]Anna Aljanaki, Mohammad Soleymani, and Yi-HsuanYang. Emotion in music task at mediaeval 2014. InWorking Notes Proceedings of the MediaEval 2014Workshop, 2014.[2]D. Bogdanov, N. Wack, E. Gomez, S. Gulati, P. Her-rera, and O. Mayor. Essentia: an audio analysis libraryProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 775for music information retrieval. InInternational Soci-ety for Music Information Retrieval Conference, pages493–498, 2013.[3]Eduardo Coutinho and Angelo Cangelosi. Musicalemotions: Predicting second-by-second subjectivefeelings of emotion from low-level psychoacousticfeatures and physiological measurements.Emotion,11(4):921–937, 2011.[4]J. Deng and C. Leung. Dynamic time warping formusic retrieval using time series modeling of musicalemotions.IEEE Transactions on Affective Computing,PP(99), 2015.[5]J. Foote. Automatic audio segmentation using a mea-sure of audio novelty. InProceedings of the IEEE In-ternational Conference of Multimedia and Expo, pages452–455, 2000.[6]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.Rwc music database: Popular, classical, and jazz musicdatabases. InProceedings of the 3rd International Con-ference on Music Information Retrieval, pages 287–288, 2002.[7]M.D. Korhonen, D.A. Clausi, and M.E. Jernigan. Mod-eling emotional content of music using system identi-ﬁcation.IEEE Transactions on Systems, Man, and Cy-bernetics, 36(3):588–599, 2006.[8]C. L. Krumhansl. Plink: thin slices of music.MusicPerception: An Interdisciplinary Journal, 27(5):337–354, 2010.[9]L. Lu, D. Liu, and H.J. Zhang. Automatic mood detec-tion and tracking of music audio signals.IEEE Trans-actions on Audio, Speech, and Language Processing,14(1):5–18, 2006.[10]O. Nieto and T. Jehan. Convex non-negative matrix fac-torization for automatic music structure identiﬁcation.InProceedings of the 38th IEEE International Con-ference on Acoustics Speech and Signal Process- ing,pages 236–240, 2013.[11]E. M. Schmidt and Y.E. Kim. Modeling musical emo-tion dynamics with conditional random ﬁelds. InPro-ceedings of the 2011 International Society for MusicInformation Retrieval, 2011.[12]Joan Serra, Meinard Muller, Peter Grosche, andJosep Lluis Arcos. Unsupervised music structure an-notation by time series structure features and segmentsimilarity.IEEE Transactions on Multimedia, SpecialIssue on Music Data Mining, 2014.[13]J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. DeRoure, and J. S. Downie. Design and creation of alarge-scale database of structural annotations. InPro-ceedings of the International Society for Music Infor-mation Retrieval Conference, pages 555–560, 2011.[14]J. A. Speck, E.M. Schmidt, B.G. Morton, and Y.E.Kim. A comparative study of collaborative vs. tradi-tional annotation methods. InProceedings of the 2011International Society for Music Information RetrievalConference, 2011.[15]J.-H. Su, Y.-C. Tsai, and V. S. Tseng. Empirical analy-sis of multi-labeling algorithms for music emotion an-notation. InIEEE International Conference on Mul-timedia and Expo Workshops (ICMEW), pages 1–6,2013.[16]K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-havas. Multilabel classiﬁcation of music into emotions.InProc. 9th International Conference on Music Infor-mation Retrieval, pages 325–330, 2008.[17]A. Wieczorkowska, P. Synak, and Z. W. Ras. Multi-label classiﬁcation of emotions in music. InIntelligentInformation Processing and Web Mining, pages 307–315, 2006.[18]B. Wu, E. Zhong, A. Horner, and Q. Yang. Musicemotion recognition by multi-label multi-layer multi-instance multi-view learning. InProceedings of theACM International Conference on Multimedia, pages117–126, 2014.[19]Z. Xiao, E. Dellandrea, W. Dou, and L. Chen. Whatis the best segment duration for music mood analysis.InIn Proceedings of the IEEE International Workshopon Content-Based Multimedia Indexing, pages 17–24,2008.[20]Y.-H. Yang and H. H. Chen. Machine recognition ofmusic emotion: A review.ACM Transactions on Intel-ligent Systems and Technology, 3(3), 2012.[21]Y.-H. Yang, C.-C. Liu, and H. H. Chen. Music emo-tion classiﬁcation: A fuzzy approach. InProceedingsof the 14th Annual ACM International Conference onMultimedia, pages 81–84, 2006.776 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Real-Time Music Tracking Using Multiple Performances as a Reference.",
        "author": [
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417205",
        "url": "https://doi.org/10.5281/zenodo.1417205",
        "ee": "https://zenodo.org/records/1417205/files/ArztW15.pdf",
        "abstract": "In general, algorithms for real-time music tracking di- rectly use a symbolic representation of the score, or a syn- thesised version thereof, as a reference for the on-line align- ment process. In this paper we present an alternative ap- proach. First, different performances of the piece in ques- tion are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking al- gorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience.",
        "zenodo_id": 1417205,
        "dblp_key": "conf/ismir/ArztW15",
        "keywords": [
            "symbolic representation",
            "on-line alignment",
            "alternative approach",
            "multiple instances",
            "live performance",
            "output combination",
            "current position",
            "score",
            "robustness",
            "precision"
        ],
        "content": "REAL-TIME MUSIC TRACKING USING MULTIPLE PERFORMANCESAS A REFERENCEAndreas Arzt, Gerhard WidmerDepartment of Computational Perception, Johannes Kepler University, Linz, AustriaAustrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austriaandreas.arzt@jku.atABSTRACTIn general, algorithms for real-time music tracking di-rectly use a symbolic representation of the score, or a syn-thesised version thereof, as a reference for the on-line align-ment process. In this paper we present an alternative ap-proach. First, different performances of the piece in ques-tion are collected and aligned (off-line) to the symbolicscore. Then, multiple instances of the on-line tracking al-gorithm (each using a different performance as a reference)are used to follow the live performance, and their output iscombined to come up with the current position in the score.As the evaluation shows, this strategy improves both therobustness and the precision, especially on pieces that aregenerally hard to track (e.g. pieces with extreme, abrupttempo changes, or orchestral pieces with a high degree ofpolyphony). Finally, we describe a real-world application,where this music tracking algorithm was used to follow aworld-famous orchestra in a concert hall in order to showsynchronised visual content (the sheet music, explanatorytext and videos) to members of the audience.1. INTRODUCTIONReal-time music tracking (or, score following) algorithms,which listen to a musical performance through a micro-phone and at any time report the current position in themusical score, originated in the 1980s (see [8, 24]) andstill attract a lot of research [4, 6, 11, 15, 17, 21, 23]. Inrecent years this technology has already found use in real-world applications. Examples include Antescofo1, whichis actively used by professional musicians to synchronise aperformance (mostly solo instruments or small ensembles)with computer realised elements, and Tonara2, a musictracking application focusing on the amateur pianist andrunning on the iPad.A common approach in music tracking, and also forthe related task of off-line audio to score alignment (see1repmus.ircam.fr/antescofo2tonara.comc\u0000Andreas Arzt, Gerhard Widmer.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Andreas Arzt, Gerhard Widmer.“Real-time Music Tracking using Multiple Performances as a Reference”,16th International Society for Music Information Retrieval Conference,2015.e.g. [9, 19, 20]), is to start from a symbolic score represen-tation (e.g in the form of MIDI or MusicXML). Often, thisscore representation is converted into a sound ﬁle using asoftware synthesizer. The result is a ‘machine-like’, low-quality rendition of the piece, in which we know the time ofevery event (e.g. note onsets). Then, a tracking algorithmis used to solve the problem of aligning the incoming liveperformance to this audio version of the score – thus, theproblem of real-time music tracking can be treated as anon-line audio to audio alignment task.In this paper we follow a similar approach, but insteadof using the symbolic score directly, we propose to ﬁrstautomatically align a recording of another performance ofthe same piece to the score. Then, we use this automati-cally annotated ‘score performance’ as the new score rep-resentation for the on-line tracking process (for the relatedtask of off-line performance to performance alignment seee.g. [18]). Our motivation for this is twofold. First ofall, we expect the quality of the features to be higher thanif they were computed from a synthesised version of thescore. Also, in a performance a lot of intricacies are en-coded that are missing in the symbolic score, including(local) tempo and loudness changes. In this way we im-plicitly also take care of special events like trills, whichnormally are insufﬁciently represented in a symbolic scorerepresentation.As will be seen in this paper, this approach proves tobe promising, but the results also depend heavily on whichperformance was chosen as a reference. To improve therobustness we further propose a multi-agent approach (in-spired by [25], where a related strategy was applied to off-line audio alignment), which does not depend on a singleperformance as a reference, but takes multiple ‘score per-formances’ and aligns the live performance to all these ref-erences simultaneously. The output of all agents is com-bined to come up with the current position in the score. Aswill be shown in the evaluation, this extension stabilisesour approach and increases the alignment accuracy.The paper is structured as follows. First, in Section 2 wegive an overview on the data we use to evaluate our musictracker. For comparison, we then give results of the origi-nal tracking algorithm that our approach is based on in Sec-tion 3. In Section 4 we present a tracking strategy based onoff-line aligned performances, which shows promising butunstable results. Then, in Section 5 we propose a multi-agent strategy, which stabilises the tracking process and357IDComposerPiece Name# Perf.GroundtruthCEChopinEtude Op. 10 No. 3 (excerpt)22MatchCBChopinBallade Op. 38 No. 1 (excerpt)22MatchMSMozart1stMov. of Sonatas KV279, KV280, KV281,KV282, KV283, KV284, KV330, KV331,KV332, KV333, KV457, KV475, KV5331MatchRPRachmaninoffPrelude Op. 23 No. 53ManualB3BeethovenSymphony No. 31ManualM4MahlerSymphony No. 41ManualTable 1. The evaluation data set.ErrorCECBMZRPB3M40.050.330.330.550.450.420.230.250.960.920.970.900.840.710.500.990.960.980.960.910.830.7510.980.990.980.940.871.0010.980.990.980.950.91Table 2. Results for theoriginal on-line tracking algo-rithm. The results are shown as proportion of correctlyaligned pairs of time points (note times or downbeat times,respectively), for different error tolerances (in seconds).For instance, the ﬁrst number in the ﬁrst row means that forthe Chopin Etude the alignment was performed for 33% ofthe notes with an error smaller than or equal to 0.05 sec-onds.improves the results for all test pieces. Next, we comparethe results of the previous chapters to each other (Section6). Finally, we describe a real-life application of our algo-rithm at a world-famous concert hall, where it was used totrack Richard Strauss’Alpensinfonie(see Section 7).2. DATA DESCRIPTIONTo evaluate a real-time music tracking algorithm, a collec-tion of annotated performances is needed. Table 1 givesan overview on the data that will be used throughout thepaper. It is important to note that the dataset includes twoorchestral pieces (symphonies by Beethoven and Mahler),which in our experience are difﬁcult challenges for mu-sic tracking algorithms, due to their high polyphony andcomplexity. The table also indicates how the ground truthwas compiled. For the Chopin Ballade and Etude, and forthe Mozart piano sonatas we have access to accurate dataabout every note onset (‘matchﬁles’) that was played, asthese were recorded on a computer-monitored grand piano(see [12] and [26] for more information about this data).For the Prelude by Rachmaninoff as well as for the Sym-phonies by Beethoven and Mahler we have to rely on man-ually annotated performances (at the note level for the pre-lude and at the downbeat level for the two symphonies).Furthermore, we collected a number of additional per-formances of the pieces in our dataset. For these we donot have any annotations, and their sole purpose is to beErrorCECBMZRPB3M40.050.920.870.930.750.540.380.250.990.970.990.970.930.860.5010.9710.990.960.940.7510.9810.990.970.971.0010.98110.980.98Table 3. Results for theoff-line alignments. The resultsare shown as proportion of correctly aligned pairs of timepoints (note times or downbeat times, respectively), for dif-ferent error tolerances (in seconds). For instance, the ﬁrstnumber in the ﬁrst row means that for the Chopin Etudethe alignment was performed for 92% of the notes with anerror smaller than or equal to 0.05 seconds.processed fully automatically. These will act as replace-ments for the symbolic scores. We collected 7 additionalperformances for each piece in the dataset. We made anexception for the excerpts of the Ballade and the Etude byChopin, as we already have 22 performances of those. Wethus reused these performances accordingly, randomly se-lected 7 additional performances for each performance inthe evaluation set, and treated them in the same way asthe other additional data (i.e. we did not use any part ofthe ground truth, everything was computed automaticallywhen they were used as a ‘score performance’). We alsotook care not to use additional performances of the sameperformer(s) that occur in our evaluation set.3. STANDARD MUSIC TRACKING BASED ON ASYMBOLIC SCORE REPRESENTATIONOur approach to music tracking is based on the standarddynamic time warping (DTW) algorithm. In [10] exten-sions to DTW were proposed that made it applicable foron-line music tracking: 1) the path is computed in an in-cremental way, and 2) the complexity is reduced to beinglinear in the length of the input sequences. Later on, thisalgorithm was extended with a ‘backward-forward’ strat-egy, which reconsiders past decisions, increasing the ro-bustness [4], and a simple tempo model (see [3]), whichgreatly increases the ability of the algorithm to cope withtempo differences.To make music tracking possible, some internal repre-358 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Live Performance\nScoreMIDI RepresentationSynthesised MIDIOn-line Music TrackingLive Performance\nScoreMIDI RepresentationSynthesised MIDIPerformance RecordingOn-line Music TrackingOff-line Alignment\nFigure 1. Standard music tracking (left) vs. music trackingvia an off-line aligned reference performance (right).sentation of the musical score is needed. In this case westart with a MIDI version of the score, which is convertedinto an audio ﬁle using a software synthesizer. Thus we ac-tually treat this task as an audio-to-audio alignment prob-lem, with additional knowledge about the score audio ﬁle(i.e. the exact timing of each note). See Figure 1 (left) fora sketch of this setup. In our approach we use the features(a mix of chroma features and ‘semi-tone onset’ features)and the distance computation method presented in [5].For comparison, we re-evaluated this algorithm on ourdata. Each performance from our evaluation set was alignedto the symbolic score representation. The results are givenin Table 2. The goal of this paper is to improve on theseresults, both regarding tracking precision and, especially,robustness (i.e. reduce the amount of big mistakes madeby the music tracker). As can be seen, the algorithm worksparticularly well on the piano pieces, but shows problemswith the two symphonies. A reason for this is that it is rela-tively easy to synthesise piano pieces from MIDI in accept-able quality, but it is much harder to do this automaticallyfor orchestral pieces.4. MUSIC TRACKING VIA A SINGLEPERFORMANCE AS A REFERENCEAs we are effectively treating the task of music tracking asan on-line audio-to-audio alignment task, we can actuallyuse any annotated audio recording of a performance as ascore representation. Using a real performance as a ‘score’has some advantages.First of all, an audio ﬁle synthesised from a deadpanMIDI ﬁle may sound bad compared to a real performance,thus also the features are of relatively low quality (i.e. theydiffer sometimes quite heavily from the features computedfrom the live performance we want to track). Despite ob-vious differences between performances, their respectiveLive Performance\nScoreMIDI RepresentationSynthesised MIDIPerformance Recording 1Performance Recording 2Performance Recording N...On-line Music TrackingOff-line Alignment\nFigure 2. Multi-agent tracking based on off-line alignedperformances as a reference.features tend to be more similar to each other. This is es-pecially true for orchestral pieces, which often include in-struments that are hard to synthesise in high quality (or atleast this would demand for expensive sound fonts and alot of effort by a trained audio engineer).Secondly, a performance implicitly encodes a lot of in-formation that is missing in the symbolic score. This in-cludes detailed information about tempo, loudness and ar-ticulation. Again we want to stress that of course perfor-mances differ from each other quite heavily, but comparedto the differences between a performance and an audio syn-thesised from the MIDI, these differences are small.There is also one big disadvantage: the symbolic infor-mation linking time points in the audio to beat times inthe score, which we get for free when we use a MIDI ﬁleas the basis for the score audio, is missing. Thus, this in-formation needs to be generated. There are two possibleways to do that: (1) by manual annotation, which can bevery laborious, or (2) by automatic off-line alignment ofthe performance to the score – which is the option we de-cided on, as we are interested in an automatic method toimprove tracking results (see Section 4.1 below).Figure 1 shows a sketch of the intended setup. On theleft, ‘normal’ music tracking is shown, where the live per-formance is aligned to the symbolic score (via a synthe-sised audio). On the right, another performance is ﬁrstaligned to the symbolic score. This performance is thenused as the new reference in the on-line alignment process.4.1 Ofﬂine AlignmentTo use a performance as a ‘score’ we have to generate thenecessary symbolic information, linking time points in theaudio to beat times in the score. As we are interested in anautomatic way to improve the tracking results, we decidedto use off-line audio alignment to align the ‘score perfor-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 359ErrorCECBMZRPB3M40.050.390.350.520.250.350.270.250.980.960.970.870.850.800.500.990.970.990.970.930.920.7510.980.990.990.950.951.0010.98110.970.96Table 4. Results foron-line music trackingbased on asingle off-line aligned performance as a reference. Theresults are shown as proportion of correctly aligned pairsof time points (note times or downbeat times, respectively),for different error tolerances (in seconds). For instance,the ﬁrst number in the ﬁrst row means that for the ChopinEtude the alignment was performed for 39% of the noteswith an error smaller than or equal to 0.05 seconds.mance’ to the symbolic score, which gives us the neededmapping as a result. As off-line audio alignment is farmore accurate than on-line tracking, our intuition was thatthe increase in feature quality outweighs the introduced er-ror by the off-line alignment process.The off-line alignment is computed with the music track-ing algorithm from Section 3 above, with the only differ-ence being that in the end we compute the backward path,as it is done in the standard DTW algorithm. As this pathis based on more information (i.e. it is computed in a non-causal way), the results are generally much more accu-rate than in the on-line case. Of course any off-line audioscore alignment algorithm could be used for this task (seee.g. [16, 19, 20].Just to get a rough idea of how much error will be intro-duced by the off-line alignment, we ran an experiment onour test data and aligned it to the symbolic scores (later on,off-line alignments of the additional data will be used, butwe expect a similar behaviour). Unsurprisingly, the resultsshow that there is a gap between the results of the off-lineapproach (see Table 3) and the on-line music tracking ap-proach (see Table 2). As we will use the off-line algorithmduring data preparation, we strongly expect that the higherquality of the features and the additional information en-coded in the performances will outweigh the error that isintroduced during this step.Thus, we aligned all the additional performances fromSection 2 to the respective symbolic scores, resulting inperformances with linked symbolic information. In the fol-lowing sections, we will use these performances as newreferences (‘score performances’) for the music trackingalgorithm.4.2 Tracking based on an aligned PerformanceGiven the automatically computed ‘score performances’,we can now use them in the tracking process as shown inFigure 1. In this experiment, each performance from theevaluation set is aligned to the score via each respective‘score performance’, resulting in 7 on-line alignments foreach performance.The results are given in Table 4 and should be comparedErrorCECBMZRPB3M40.050.390.350.580.190.440.320.250.990.980.990.920.900.840.5010.98110.950.940.7510.98110.960.961.0010.99110.970.97Table 5. Results for themulti-agent trackingapproachbased on aset of off-line aligned performances as a ref-erence. The results are shown as proportion of correctlyaligned pairs of time points (note times or downbeat times,respectively), for different error tolerances (in seconds).For instance, the ﬁrst number in the ﬁrst row means that forthe Chopin Etude the alignment was performed for 39% ofthe notes with an error smaller than or equal to 0.05 sec-onds.to the numbers in Table 2. As can be seen, the generaltrend is an improvement in robustness, especially for thecomplex orchestral pieces (e.g. the percentage of aligneddownbeats with an error smaller than 250 ms increasedfrom 71% to 80% for the Mahler Symphony).Unfortunately, the results also proved to be unstable.Some performances are more similar (or at least easier toalign) to each other, which also results in good trackingresults – but the use of some of the ‘score performances’led to results that were worse than our basic approach. Acloser look at the positions where tracking errors occurredshowed that some of them happened at the same points intime over all alignments of the piece – basically showingthat some parts are harder to track than others. But therewere also many alignment errors that occurred only for oneor two of the ‘score performances’, but not for the others.This led us to the idea to combine individual on-line align-ments in such away, that it would smooth out these errors.5. MUSIC TRACKING VIA A SET OFPERFORMANCES AS REFERENCEThe analysis of the results from Section 4 above showedthat a combination of a number of on-line alignments mightfurther improve the tracking results. Here, we propose asimple multi-agent strategy (see Figure 2 for an illustra-tion). During a live concertntrackers run in parallel andeach tracker tries to align the incoming live performanceto its score representation, each producing its own, inde-pendent hypothesis of the current position in the score. Fi-nally, the hypotheses are combined to form one collectivehypothesis of the music tracking system.Many different ways of combining the hypotheses wouldbe possible, e.g. based on voting or on the current align-ment error of the individual trackers. Here, we decided ona very simple method: taking the median of the positionsthat are returned by the individual trackers. The reasoningbehind this is that trackers tend to make mistakes in bothdirections – i.e. ‘running ahead’ (reporting events to early),and ‘lagging behind’ (reporting events with some delay) –360 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015with about the same frequency. Thus, trackers that staysafely in the middle of the pack tend to give a robust esti-mate of the position in the score.Furthermore, using the median also means that as longasn2+1trackers stay close to the actual position, the sys-tem would still come up with a reasonable position esti-mate – while this is not directly reﬂected in the evaluationresults, this extra robustness is convenient when the track-ing algorithm is used in real-world applications. Furtherstrategies to increase the robustness are possible, like theautomatic replacement of trackers that got lost, but werenot used in our experiments.For the evaluation we setn=7, as this was a goodtrade-off between robustness and computation time (7 on-line alignments can still be easily computed in real-timeon a conventional consumer laptop). The results, given inTable 5, show that our approach is working well. Errors ofmore than 1 second are rare, and the multi-agent approacheven improved the alignment precision for all pieces (withthe exception of the Prelude by Rachmaninoff).6. DISCUSSIONThe main goal of our approach was to increase the robust-ness of the algorithm, i.e. to decrease the frequency of‘large’ errors and to make sure that the tracker does not getlost, even when following difﬁcult orchestral pieces. Forconvenience, we give a summary of the results (see Table6) based on a common measure in the evaluation of mu-sic tracking algorithms: the percentage of notes that werealigned with an error less than or equal to 250 ms (see [7]).As can be seen, the multi-agent approach based on au-tomatically aligned reference performances improves theresults heavily – in fact for CB the results of the on-linealignment even surpassed the off-line alignment. For theresults on the Chopin data (CE and CB) one has to takeinto account that we used 22 performances which wererecorded by different performers, but still on the same pi-ano and with the same recording setup, which will havea positive inﬂuence on the alignment results. Still, as theremaining results show, even when completely unrelatedperformances of the same piece were used as references,the alignment results improved drastically.Especially for the orchestral pieces (B3 and M4), wecan see that our intuition proved to be correct: the errorintroduced by the off-line alignment had a lot less impactthan the better quality of the features and the additionaltempo and loudness information provided by the perfor-mances. In addition, the multi-agent approach proved tobe very effective regarding the increase in robustness. Itsmooths out some of the bigger errors that occur when us-ing just a single performance as a score reference.7. REAL-LIFE SCENARIO: MUSIC TRACKING INTHE CONCERTGEBOUW AMSTERDAMThe multi-national European research project PHENICX3provided us with the unique opportunity (and challenge) to3http://phenicx.upf.eduPieceOfﬂineStandardVia 1Via 7CE99.06%95.62%97.92%98.78%CB97.13%92.10%96.00%97.93%MZ99.35%96.88%97.46%99.04%RP96.62%90.14%87.47%92.47%B392.88%83.67%85.04%89.55%M486.74%71.15%80.06%83.66%Table 6. Comparison of the results (error tolerance 250ms). The results are shown as percentage of matching pairsof time points (note times or downbeat times, respectively).For instance, the ﬁrst number in the ﬁrst row means that forthe Chopin Etude the off-line alignment was performed for99.06% of the notes with an error smaller than or equalto 0.25 seconds. The results of theofﬂinealignment algo-rithm are only shown for comparison.Standardrefers tothe basic on-line music tracker (see Section 3),Via 1to thetracker using a single ‘score performance’ as a reference,Via 7to the multi-agent approach based on 7 trackers.demonstrate our score following technology in the contextof a big, real-life symphonic concert (for a full descriptionof this experiment see [2], a similar study was presentedin [22]). The general goal of the project is to develop tech-nologies that enrich the experience of classical music con-certs. In the experiment to be described, this was done byusing the live performance tracker to control, in real timeand via WiFi, the transmission and display of additional vi-sual and textual information, synchronised to the live per-formance on stage. The user interface and the visualisa-tions were provided by our project partner Videodock4.Some impressions can be seen in Figure 3.The event took place on February 7th, 2015, in the Con-certgebouw in Amsterdam. The Royal Concertgebouw Or-chestra, conducted by Semyon Bychkov, performed theAlpensinfonie(Alpine Symphony) by Richard Strauss. Thisconcert was part of a series called ‘Essentials’, during whichtechnology developed within the project can be tested in areal-life concert environment. All the tests during this con-cert series have to be as non-invasive as possible. For thedemonstration during the concert in question, a test audi-ence of about 30 people was provided with tablet comput-ers and placed in the rear part of the concert hall.In contrast to the experiments presented in this paper sofar, we did not even have access to a symbolic score. In-stead, we annotated a single performance manually (on thelevel of downbeats) and used it as a score representation.Then, to add extra robustness, we aligned 6 more perfor-mances to this reference, resulting in 7 instances that canbe used for the tracking process.The event in the Concertgebouw was a big success. Thetracking went smoothly and there were no glitches, onlysome minor inaccuracies, and the accuracy was more thansufﬁcient to trigger the visualisation in time.After the event we annotated an audio recording of theconcert to be able to perform quantitative experiments (see4http://videodock.comProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 361Figure 3. Left: View from the control room onto the stage (during orchestra rehearsal); right: synchronised score displayin the audience during the concert.Err. (sec)SingleMulti-agent0.2578.25%81.80%0.5092.20%93.24%0.7595.57%96.44%1.0097.49%98.01%Table 7. Real-time alignment results for the single tracker(using only on manually annotated performance), and themulti-agent tracker, shown as percentages of correctlyaligned pairs of downbeats. For instance, the ﬁrst num-ber in the ﬁrst row means that the single tracker aligned78.25% of the downbeats with an error smaller than orequal to 0.25 seconds.Table 7). The ﬁrst column shows the results of the trackingusing only the manually annotated performance as a refer-ence. The second column shows the results of the multi-agent approach. Also in this case using multiple perfor-mances as a reference improved the tracking results: extrarobustness and a slight increase in accuracy were achievedwithout any extra manual efforts as the additional data wasprepared by automatic methods.8. CONCLUSIONIn this paper we presented an alternative approach to real-time music tracking. Instead of tracking directly on a sym-bolic score representation, we ﬁrst use off-line alignmentto match other performances of the piece in question tothe symbolic score. We then use these performances asour new score representation, which results in high qualityfeatures, and implicitly also adds extra information abouthow this piece generally is performed. Together with amulti-agent tracking strategy, which smooths out most ofthe major errors, we achieve increased robustness and alsoincrease the accuracy of the live tracking, especially forcomplex orchestral music. We also reported on a success-ful real-world test of our algorithm in a world-famous con-cert hall.In the future, we will also look at other options to com-bine tracking results of the individual trackers. While tak-ing the median seems like a natural choice, more sophis-ticated strategies also based on alignment costs might bepromising. A further problem which deserves a closer lookis the automatic selection strategy of the ‘score performan-ces’. For this paper we simply decided on 7 additionalperformances of the pieces based on availability. With abigger database, automatic selection of the ‘best score per-formances’ for an on-going live performance becomes aninteresting question, and a good selection strategy mightfurther improve the tracking results.A common problem of real-time music tracking and au-dio to score alignment are structural differences betweenthe score and the performance. For example, if a piecehas some repeated sections, the performers might decideto play the repetition or to leave it out. For the experi-ments in this data we chose the additional ‘score perfor-mances’ manually, such that they have the same structureas the piece we want to track, but in the future we will tryto cope with this automatically – in the preparation phasevia the technique used in [13] or [14] (maybe in combina-tion with the method described in [25], to bring the beneﬁtof using multiple performances also to the preprocessingstage), and in the live tracking phase with the approachpresented in [1], extended to orchestral music.9. ACKNOWLEDGEMENTSThis research is supported by the Austrian Science Fund(FWF) under project number Z159 and the EU FP7 ProjectPHENICX (grant no. 601166).10. REFERENCES[1]Andreas Arzt, Sebastian B¨ock, Sebastian Flossmann,Harald Frostel, Martin Gasser, and Gerhard Widmer.The complete classical music companion v0.9. InProc.of the AES Conference on Semantic Audio, London,England, 2014.[2]Andreas Arzt, Harald Frostel, Thassilo Gadermaier,Martin Gasser, Maarten Grachten, and Gerhard Wid-mer. Artiﬁcial intelligence in the concertgebouw. InProc. of the International Joint Conference on Arti-ﬁcial Intelligence (IJCAI), Buenos Aires, Argentina,2015.[3]Andreas Arzt and Gerhard Widmer. Simple tempomodels for real-time music tracking. InProc. of362 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015the Sound and Music Computing Conference (SMC),Barcelona, Spain, 2010.[4]Andreas Arzt, Gerhard Widmer, and Simon Dixon. Au-tomatic page turning for musicians via real-time ma-chine listening. InProc. of the European Conferenceon Artiﬁcial Intelligence (ECAI), Patras, Greece, 2008.[5]Andreas Arzt, Gerhard Widmer, and Simon Dixon.Adaptive distance normalization for real-time musictracking. InProc. of the European Signal ProcessingConference (EUSIPCO), Bucharest, Romania, 2012.[6]Arshia Cont. A coupled duration-focused architecturefor realtime music to score alignment.IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(6):837–846, 2009.[7]Arshia Cont, Diemo Schwarz, Norbert Schnell, andChristopher Raphael. Evaluation of real-time audio-to-score alignment. InProcedings of the InternationalConference on Music Information Retrieval (ISMIR),Vienna, Austria, 2007.[8]Roger Dannenberg. An on-line algorithm for real-timeaccompaniment. InProc. of the International Com-puter Music Conference (ICMC), Paris, France, 1984.[9]Roger Dannenberg and Ning Hu. Polyphonic audiomatching for score following and intelligent audio edi-tors. InProc. of the International Compter Music Con-ference (ICMC), Singapore, 2003.[10]Simon Dixon. An on-line time warping algorithm fortracking musical performances. InProc. of the Inter-national Joint Conference on Artiﬁcial Intelligence (IJ-CAI), Edinburgh, Scotland, 2005.[11]Zhiyao Duan and Bryan Pardo. A state space model foron-line polyphonic audio-score alignment. InProc. ofthe IEEE Conference on Acoustics, Speech and SignalProcessing (ICASSP), Prague, Czech Republic, 2011.[12]Sebastian Flossmann, Werner Goebl, MaartenGrachten, Bernhard Niedermayer, and Gerhard Wid-mer. The magaloff project: An interim report.Journalof New Music Research, 39(4):363–377, 2010.[13]Christian Fremery, Meinard M¨uller, and MichaelClausen. Handling repeats and jumps in score-performance synchronization. InProc. of the Interna-tional Conference on Music Information Retrieval (IS-MIR), Utrecht, The Netherlands, 2010.[14]Maarten Grachten, Martin Gasser, Andreas Arzt, andGerhard Widmer. Automatic alignment of music per-formances with structural differences. InProc. of theInternational Society for Music Information RetrievalConference (ISMIR), Curitiba, Brazil, 2013.[15]Filip Korzeniowski, Florian Krebs, Andreas Arzt, andGerhard Widmer. Tracking rests and tempo changes:Improved score following with particle ﬁlters. InProc. of the International Computer Music Conference(ICMC), Perth, Australia, 2013.[16]Marius Miron, Julio Jos´e Carabias-Orti, and JordiJaner. Audio-to-score alignment at note level for or-chestral recordings. InProc. of the InternationalConference on Music Information Retrieval (ISMIR),Taipei, Taiwan, 2014.[17]Nicola Montecchio and Arshia Cont. A uniﬁed ap-proach to real time audio-to-score and audio-to-audioalignment using sequential montecarlo inference tech-niques. InProc. of the IEEE Conference on Acous-tics, Speech and Signal Processing (ICASSP), Prague,Czech Republic, 2011.[18]Meinard M¨uller, Frank Kurth, and Michael Clausen.Audio matching via chroma-based statistical features.InProc. of the International Society for Music Infor-mation Retrieval Conference (ISMIR), London, GreatBritain, 2005.[19]Meinard M¨uller, Frank Kurth, and Tido R¨oder. To-wards an efﬁcient algorithm for automatic score-to-audio synchronization. InProc. of the International So-ciety for Music Information Retrieval Conference (IS-MIR), Barcelona, Spain, 2004.[20]Bernhard Niedermayer and Gerhard Widmer. A multi-pass algorithm for accurate audio-to-score alignment.InProc. of the International Society for Music In-formation Retrieval Conference (ISMIR), Utrecht, TheNetherlands, 2010.[21]Takuma Otsuka, Kazuhiro Nakadai, Toru Takahashi,Tetsuya Ogata, and Hiroshi G. Okuno. Real-timeaudio-to-score alignment using particle ﬁlter for co-player music robots.EURASIP Journal on Advancesin Signal Processing, 2011(2011:384651), 2011.[22]Matthew Prockup, David Grunberg, Alex Hrybyk, andYoungmoo E. Kim. Orchestral performance compan-ion: Using real-time audio to score alignment.IEEEMultimedia, 20(2):52–60, 2013.[23]Christopher Raphael. Music Plus One and machinelearning. InProc. of the International Conference onMachine Learning (ICML), Haifa, Israel, 2010.[24]Barry Vercoe. The synthetic performer in the contextof live performance. InProc. of the International Com-puter Music Conference (ICMC), Paris, France, 1984.[25]Siying Wang, Sebastian Ewert, and Simon Dixon. Ro-bust joint alignment of multiple versions of a piece ofmusic. InProc. of the International Society for Mu-sic Information Retrieval Conference (ISMIR), Taipei,Taiwan, 2014.[26]Gerhard Widmer. Discovering simple rules in com-plex data: A meta-learning algorithm and somesurprising musical discoveries.Artiﬁcial Intelligence,146(2):129–148, 2003.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 363"
    },
    {
        "title": "Corpus Analysis Tools for Computational Hook Discovery.",
        "author": [
            "Jan Van Balen",
            "John Ashley Burgoyne",
            "Dimitrios Bountouridis",
            "Daniel Müllensiefen",
            "Remco C. Veltkamp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415038",
        "url": "https://doi.org/10.5281/zenodo.1415038",
        "ee": "https://zenodo.org/records/1415038/files/BalenBBMV15.pdf",
        "abstract": "Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is op- timized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: au- dio bigram distributions, and the use of corpus-relative or “second-order” descriptors. To test the real-world applica- bility of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening his- tory and experience of our participants. We find that our corpus-based audio features are able to explain a compa- rable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable.",
        "zenodo_id": 1415038,
        "dblp_key": "conf/ismir/BalenBBMV15",
        "keywords": [
            "corpus analysis",
            "interpretable results",
            "audio bigram distributions",
            "corpus-relative descriptors",
            "song recognition data",
            "cognitive adequacy",
            "model interpretation",
            "listening history",
            "symbolic features",
            "new insights"
        ],
        "content": "CORPUS ANALYSIS TOOLS FOR COMPUTATIONAL HOOK DISCOVERYJan Van Balen1John Ashley Burgoyne2Dimitrios Bountouridis1Daniel M¨ullensiefen3Remco C. Veltkamp11Department of Information and Computing Sciences, Utrecht University2Music Cognition Group, University of Amsterdam3Department of Psychology, Goldsmiths, University of LondonJ.M.H.VanBalen@uu.nlABSTRACTCompared to studies with symbolic music data, advances inmusic description from audio have overwhelmingly focusedon ground truth reconstruction and maximizing predictionaccuracy, with only a small fraction of studies using audiodescription to gain insight into musical data. We presenta strategy for the corpus analysis of audio data that is op-timized for interpretable results. The approach brings twopreviously unexplored concepts to the audio domain: au-dio bigram distributions, and the use of corpus-relative or“second-order” descriptors. To test the real-world applica-bility of our method, we present an experiment in which wemodel song recognition data collected in a widely-playedmusic game. By using the proposed corpus analysis pipelinewe are able to present a cognitively adequate analysis thatallows a model interpretation in terms of the listening his-tory and experience of our participants. We ﬁnd that ourcorpus-based audio features are able to explain a compa-rable amount of variance to symbolic features for this taskwhen used alone and that they can supplement symbolicfeatures proﬁtably when the two types of features are usedin tandem. Finally, we highlight new insights into whatmakes music recognizable.1. INTRODUCTIONThis study addresses the scarcity of corpus analysis toolsfor audio data. Bycorpus analysis, we refer to any analy-sis of a collection of musical works in which the primarygoal is to gain insight into the music itself. Such analysesmakes up only a small fraction of the music computingﬁeld, with much more research being done on classiﬁca-tion, recommendation and retrieval [16], where the focusis often more on prediction accuracy than interpretability.Examples of corpus analysis studies include work on sum-marization and visualisation (e.g., [1]), hypothesis testing,(e.g., evidence for Western inﬂuence in the use of Africanc\u0000Jan Van Balen, John Ashley Burgoyne, Dimitrios Boun-touridis, Daniel M¨ullensiefen, Remco C. Veltkamp.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Jan Van Balen, John Ashley Burgoyne, Dim-itrios Bountouridis, Daniel M¨ullensiefen, Remco C. Veltkamp. “CorpusAnalysis Tools for Computational Hook Discovery”, 16th InternationalSociety for Music Information Retrieval Conference, 2015.tone scales in [11]), and discovery-based analysis (e.g., ofthe structural melodic features that predict performance ina music memory task [12]).Strikingly, while audio data is by far the most widelyresearched form of information in the community [16], abrief review suggests that only a minority of corpus analysisstudies used audio data. This includes the above work onvisualisation [1], tone scales analysis [11], and a numberof recent studies on the structure and evolution of popularmusic [10, 15, 18]. Symbolic corpus analysis, in contrast,includes Huron’s many studies [9], Conklin’s work on mul-tiple viewpoints and Pearce’s extensions [6, 14], corpusstudies of harmony [5, 7] as well as toolkits such as Hum-drum,1Idyom,2and FANTASTIC.3Although the music information retrieval community hasmade substantial progress in improving the transcription ofaudio to symbolic data, considerable hurdles remain [16].We therefore aim to further the resources for audio analysis. We present a set of audio corpus description features thatare founded on the use of three novel concepts. A newkind of melodic and harmonic interval proﬁles are usedto describe melody and harmony, extending the notion ofinterval bigrams to the audio domain. We then propose threeso-calledsecond-orderfeatures, a concept that has yet tobe applied to audio features. Finally, we deﬁne song-basedand corpus-based second-order features.We test our newly developed analysis pipeline in a casestudy on “hook discovery”.2. CORPUS-BASED AUDIO FEATURES2.1 Harmony and Melody DescriptionWe propose a novel set of harmony and melody descrip-tors. The purpose for these descriptors is to translate basicharmonic and melodic structures to a robust representationon which corpus statistics can be computed. They shouldbe relatively invariant to other factors such as tempo andtimbre, and have a ﬁxed size.In [17], the correlation matrix of the chroma featuresis used as a harmonic descriptor. The 144-dimensional1www.musiccog.ohio-state.edu/Humdrum/2code.soundsoftware.ac.uk/projects/idyom-project3www.doc.gold.ac.uk/isms/m4s/227‘chroma correlation features’ measure co-occurrence of har-monic pitch. They capture more detail than a simple chromapitch histogram, while preserving tempo and translation-invariance. The feature was shown to perform reasonablywell in a small-scale cover song experiment. In this studywe extend this and two related concepts to three new in-terval representations. Whereas pitch bigram proﬁles areexpected to strongly correlate with the key of an audio frag-ment, interval bigrams are key-invariant, which allows themto be compared across songs.TheHarmonic Interval Co-occurrence (HIC)is basedon thetriad proﬁle, which is deﬁned as the three-dimensionalco-occurrence matrix of three identical copies of the chromatime seriesct,i(tis time,iis pitch class):triads(c)i1,i2,i3=Xtct,i1ct,i2ct,i3.(1)The pitch class triplets in this feature can be converted tointerval pairs using the function:intervals(X)j1,j2=12Xi=0X(i\u0000j1) mod 12,i,(i+j2) mod 12.(2)This essentially maps each triad(i1,i2,i3)to a stack ofintervals(i2\u0000i1,i3\u0000i2). A major chord(0,4,7)wouldbe converted to(4,3), or a major third with a minor thirdon top. Applied to the triads matrix, the intervals functionyields theharmonic interval co-occurrencematrix,HIC(c)j1,j2=intervals(triads(ct,i)) (3)It measures the distribution of triads in an audio segment,represented by their interval representation. For example, apiece of music with only minor chords will have a strongactivation ofHIC3,4, while a piece with a lot of tritones willhave activations in HIC0,6and HIC6,0.The same processing can be applied to the melodic pitchto obtain theMelodic Interval Bigrams (MIB). We ﬁrstdeﬁne the three-dimensionaltrigram proﬁleas an extensionof the two-dimensional bihistogram in [17]:trigrams(m)i1,i2,i3=Xtmax⌧(mt\u0000⌧,i1)mt,i2max⌧(mt+⌧,i3),(4)with⌧=1...\u0000tandmthe melody matrix, a binary chroma-like matrix containing the melodic pitch activations. Theresult is a three-dimensional matrix indicating how oftentriplets of melodic pitches(i1,i2,i3)occur less than\u0000tsec-onds apart. The pitch trigram proﬁle can be converted to aninterval bigram proﬁle by applying the intervals function(Eqn 2). This yields themelodic interval bigramsfeature, atwo-dimensional matrix that measures which pairs of pitchintervals follow each other in the melody:MIB(X)j1,j2=intervals(trigrams(mt,i)).(5)Finally, theharmonisation featurein [17] measures whichharmonic pitches in the chromacco-occurr with the melodicpitches in the melodym. We derive aHarmonisation In-terval (HI)feature as:HI(m,h)j=Xt12Xi=0mt,iht,i+j(6)2.2 Second-order FeaturesOne of the contributions of the FANTASTIC toolbox is toincludesecond-orderfeatures. Second-order features arederivative descriptors that reﬂect, for a particular feature,how an observed feature value relates to a reference corpus.They help contextualize the values a feature can take. Isthis a high number? Is it a common result? Or if the fea-ture is multivariate: is this combination of values typicalor atypical, or perhaps representative of a particular style?Examples of second-order features in the FANTASTIC tool-box include features based on document frequencies, i.e.how many songs (documents) in a large corpus contain anobserved event or structure:mtcf.mean.log.DFcomputesthe mean log document frequency over all melodic motivesin a given melody.2.2.1 Second-Order Audio Features in One DimensionLike many audio features, most of the audio features dis-cussed in this paper are based on frequency-domain compu-tations, which are typically performed on short overlappingwindows. As a result, the features discussed here representcontinuous-valued, uncountable quantities. Symbolic fea-tures, on the other hand, operate on countable collectionsof events. This makes it impossible to apply the same oper-ations directly to both, and alternatives must be found forthe audio domain.After comparison of several alternatives, we propose anon-parametric measure of typicality based on log odds.The second-order log odds of a feature valuexcan formallybe deﬁned as thelog odds of observing a less extreme valuein the reference corpus. It is conceptually similar to ap-value, which measures the probability of observing amoreextreme value, but we look at its complement, expressed asodds, and take the log.We further propose a simple non-parametric approachto compute the above odds. By deﬁning ‘less extreme’ as‘more probable’, we can make use of density estimation(e.g., kernel density estimation) to obtain a probability den-sity estimatef(X)for the observed featureX, and lookat the rank of each feature value’s density in the referencecorpus. Normalizing this rank by the number of observa-tion gives us a pragmatic estimate of the probability we’relooking for, and applying the logit function gives us the logodds:Z(X)=logit\"rank (f(X))N#(7)whereNis the size of the reference corpus. SinceZisnon-parametric and based on ranks, the output always fol-lows the same logistic distribution, which is bell-shaped,symmetric, and general very similar to a normal distribu-tion. The feature can therefore be used out of the box for avariety of statistical applications.Some caution is warranted when usingZwhere there area limited number of observations. If the ﬁrst order featureXis one-dimensional, some form of density estimationis typically possible even if few data are available. Formultivariate features with independent dimensions (e.g.,228 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015MFCC features), each dimension can be treated as a one-dimensional feature, and a meaningful density estimatecan also be obtained. However, if the dimensions of amultidimensional feature are not de-correlated by design buthighly interdependent (as is the case for chroma features),density estimates require more data. For such cases, acovariance matrix must typically be estimated, increasingthe number of parameters to be estimated, and thereby thenumber of required data points for a ﬁt.2.2.2 Second-Order Audio Features indDimensionsFor higher-dimensional features, such asMIBandHIC, weturn to other measures of typicalness. After comparisonof distributions and correlations of several alternatives, weadopt two approaches. The ﬁrst measure, directly adoptedfrom the FANTASTIC toolbox, is Kendall’s rank-basedcorrelation⌧. The second measure isinformation(I), aninformation-theoretic measure of unexpectedness. Thismeasures assumes that the multidimensional ﬁrst order fea-ture itself can be seen as a frequency distributionFoverpossible observations in an audio excerpt (cf. term frequen-cies), and that a similar distributionFccan be found for thefull reference corpus (cf. document frequencies). We deﬁnetheI(F) as the average of\u0000logFc, weighted byF:I(F)=\u0000dXi=1F(i) logFc(i) (8)The assumptions hold for HIC, BIM and HI, and pro-duce well-behaved second-order feature values. The re-sult is similar tomean.log.TFDF,mtcf.mean.log.DFandmtcf.mean.entropyin the FANTASTIC toolbox and highlycorrelated withmtcf.mean.gl.weight. Information is alsoused as a measure of surprise by Pearce [14].2.3 Song- vs. Corpus-based Second-order FeaturesIn a statistical learning perspective, expectations arise fromstatistical inference by the listener, who draws on a lifetimeof listening experiences to assess whether a particular stim-ulus is to be expected or not. In [9], Huron comparesveridi-calandschematicexpectations, analogous to episodic andsemantic memory. Veridical expectations of a listener aredue to familiarity with a speciﬁc musical work. Schematicexpectations arise from the “auditory generalizations” thathelp us deal with novel, but broadly familiar situations.If, in a corpus study, the documents are song segmentsrather than entire songs, second-order features can be usedto incorporate a crude model of both layers of expecta-tion. By choosing the reference corpus to be a collectionof fragments spanning a large number of songs, the abovemeasures of typicality and surprise approximate schematicexpectations: values that are typical, representative of thereference corpus, are more expected. By choosing as thereference corpus the set of all segments belonging to thesame song, veridical expectations can be approximated.In the following section, we will refer to corpus-basedsecond-order features asconventionality. The second, song-based second-order features indicate how representative asegment is for the song, and to some extent, how much asegment is repeated. We will refer to this asrecurrence.3. HOOK DISCOVERY: A CASE STUDYWe tested the proposed approach to audio corpus analysisby examining data from the Hooked! experiment on long-term musical salience [3]. Using these data, we sought toaddress three questions: (i) how do the proposed audio fea-tures behave and what aspects of the music do they model,(ii) which attributes of the music, as measured by both anaudio feature set and a selection of symbolic features, pre-dict recognition rating differences within songs, and ﬁnally,(iii) how much insight do audio-based corpus analysis toolsadd when compared to the symbolic feature set?3.1 DataThe Hooked! experiment used a broad selection of Westernpop songs from the 1930s to the present. The experimenttested how quickly and accurately participants could recog-nise different segments from each song, based on the EchoNest segmentation algorithm.4For each song segment, thedata include an estimate of thedrift rate, the reciprocal ofthe amount of time it would take a median participant torecognize the segment, based on linear ballistic accumula-tion, a cognitive model for timed recognition tasks [2,4]. Toimprove reliability, we excluded song segments that fewerthan 15 serious participants had attempted to recognize(where a “serious” participant is deﬁned to be a participantwho attempted at least 15 segments). We further excludedall segments from songs from which fewer than 3 segmentsmet the previous reliability criteria. After these exclusions,1715 song segments remained, taken from 321 differentsongs, representing data from 973 participants. We wereunable to obtain symbolic transcriptions of all songs, andso for comparing audio and symbolic features, we used arestricted set of 99 transcribed songs (536 segments).3.2 Audio FeaturesFor timbre description, we used a feature set that is largelythe same as the one used in [18], where statistical analysisof an audio corpus is used to model pop songs choruses.Speciﬁcally, we computed the loudness (mean and standarddeviations) for each segment, mean sharpness and rough-ness, and the total variance of the MFCC features. Insteadof the pitch centroid feature, we obtained an estimate ofpitch height using theMelodiamelody extraction algorithmand computed the mean.5FOr chroma, HPCP were used.6For each of these one-dimensional features, we thencomputed the corpus-based and song-based second-orderfeatures as described in Section 2.2.1 using Python.7Fi-nally, we added song and corpus-basedZ(X)features basedon the mean of the ﬁrst 13 MFCC components. First-order4http://www.echonest.com/5http://mtg.upf.edu/technologies/melodia6http://mtg.upf.edu/technologies/hpcp7code will be made available athttp://github.com/jvbalenProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 229features based on the MFCC means were not included be-cause of their limited interpretability. All features werecomputed over 15-s segments starting from the beginningof each segment, as participants in the experiment weregiven a maximum of 15 s for recognition.For melody and harmony description, we used the fea-tures described in Section 2.1, and compute the entropyHas a ﬁrst-order measure of dispersion.The entropies werethen normalized as follows:H0=logHmax\u0000HHmax(9)As second-order features, Kendall’s⌧and the informationIwere computed, as proposed in Section 2.2.2.3.3 Symbolic featuresThe symbolic features used were a subset of 19 ﬁrst-orderand 5 second-order features from the FANTASTIC toolbox,computed for both melodies and bass lines. Second-orderfeatures were computed with both the song and the fulldataset as a reference, yielding a total of 58 symbolic de-scriptors.3.4 Principal Component AnalysisBefore going further with either the audio or the symbolicfeature sets, we used principal component analysis (PCA) asa way to identify groups of features that may measure a sin-gle underlying source of variance and as a way to reduce thedimensionality of the feature spaces to a more manageablenumber of decorrelated variables. Features were centeredand normalized before PCA, and the resulting componentswere transformed with a varimax rotation to improve inter-pretability. We selected the number of components to retain(12 in both cases) using parallel analysis [8].3.5 Linear Mixed Effects ModelIn order to ﬁt the extracted components to the drift rates, weused a linear mixed-effects regression model. Mixed-effectsmodels can handle repeated-measures data where severaldata points are linked to the same song and therefore have acorrelated error structure. The Hooked! data provide driftrates for individual sections within songs, and one would in-deed expect considerably less variation in drift rates withinsongs than between them: some pop songs are thought to bemuch “catchier” than others overall. Moreover, it is likelyimpossible to model between-song variation in recognis-ablity from content-based features alone: it may arise fromdifferences in marketing, radio play, or social appeal.Linear mixed-effects models have the further advantagethat they are easy to interpret due to the linearity and addi-tivity of the effects of the predictor variables. More com-plex machine-learning schemes might be able to explainmore variance and make more precise predictions for thedependent variable, but this usually comes at the cost of theinterpretability of the model.We ﬁt three models, one including audio componentsonly, one including symbolic components only, and oneincluding both feature types, and used a stepwise selectionprocedure at↵=.005 to identify the most signiﬁcant pre-dictors under each model. In all models, the dependentvariable was the log drift rate of a song segment and the re-peated measures (random effects) were handled as a randomintercept, i.e., we added a per-song offset to a traditionallinear regression (ﬁxed effects) on song segments, with theassumption that these offsets be distributed normally:logyij=\u00000xij+ui+✏ij(10)whereiindexes songs,jindexes segments within songs,yijis the drift rate for song segmentij,xijis the vector ofstandardized feature component scores for song segmentijplus an intercept term, theui⇠N(0,\u00002song), and the✏ij⇠N(0,\u00002residual). To facilitate comparison, we ﬁt theaudio-only model twice: once using the full set of 321songs and again using just the 99 songs with transcriptions.4. RESULTS AND DISCUSSION4.1 Audio ComponentsTable 1 displays the component loadings (correlation coefﬁ-cients between the extracted components and the originalfeatures) for audio feature set. The loadings tell a consis-tent story. The 12 components we retain break the audiofeature set down into three timbre components (ﬁrst order,conventionality, and recurrence) and three entropy compo-nents (idem), two features grouping conventionality andrecurrence for melody and harmony, respectively, and threemore detailed timbre components correlating with sharp-ness, pitch range and dynamic range.Component 9 is characterized by an increased dynamicrange and MFCC variance and a typical pitch height. Wehypothesize that this component correlates with the pres-ence and prominence of vocals. It is not unreasonable toassume that the most typical registers for the melodies in apop corpus would be the registers of the singing voice, andvocal entries could also be expected to modulate a section’stimbre and loudness. This hypothesis is also consistentwith our own observations while listening to a selection offragments at various points along the Component 9 scale.Overall, the neatness of the above reduction attests tothe advantage of using interpretable features, and to thepotential of this particular feature set.4.2 Recognizability PredictorsA look at the ﬁrst column of results for the linear mixedeffects model (Table 2) conﬁrms that the audio featuresare indeed meaningful descriptors for this corpus. Eightcomponents correlate signiﬁcantly, most of them relating toconventionality of features. This suggests a general patternin which more recognizable sections have a more typical,expected sound. Another component, timbral recurrence,points to the role of repetition: sections that are more rep-resentative of a song are more recognizable. Finally, thecomponent with the strongest effect is V ocal Prominence.230 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015ComponentFeature 1 2 3 4 5 6 7 8 9 10 11 12MIB|Song.31\u0000.10.12.08.05.66.05.08.23.08\u0000.01.14HI|Song\u0000.25\u0000.08.12.06.11.55.12.35\u0000.06.04.01\u0000.02MIB|Corpus.15\u0000.03\u0000.02.13.00.77\u0000.06.00.08\u0000.02\u0000.01.05HI|Corpus\u0000.28\u0000.09\u0000.05\u0000.01.10.55.11.42\u0000.15\u0000.02.08\u0000.05HIC|Song.04.13.22.04.00.13\u0000.04.58\u0000.03.06\u0000.02\u0000.03HIC|Corpus\u0000.23.11.04.32.08.15\u0000.07.66.03\u0000.06.07.00HIC Entropy.88.06.03\u0000.16.02.07\u0000.02\u0000.23\u0000.12.02\u0000.00\u0000.10MIB Entropy.83\u0000.15\u0000.00\u0000.19.04.04.08.26.26.03\u0000.02.20HI Entropy.85\u0000.06.02\u0000.20.01\u0000.01.04.15.12.02\u0000.02.16HIC Song Information.84.17.06.09.11.13\u0000.02\u0000.16\u0000.28\u0000.04.10\u0000.13MIB Song Information.79\u0000.21\u0000.03.01.07.05.13.25.29.07\u0000.02.21HI Song Information.90.18.01.11.07\u0000.07.00\u0000.17\u0000.03\u0000.02.00\u0000.03HIC Corpus Information.86.16.06.01.10.11\u0000.02\u0000.20\u0000.27\u0000.02.09\u0000.13MIB Corpus Information.79\u0000.19\u0000.01\u0000.03.07.02.14.26.31.07\u0000.02.21HI Corpus Information.90.15.02\u0000.01.03\u0000.12\u0000.01\u0000.24\u0000.03.00\u0000.02\u0000.03HIB Entropy|Song.03.11.42.08.03.00\u0000.08.15.08.19.01\u0000.06MIB Entropy|Song.01\u0000.01.07.10.03\u0000.01.03.02\u0000.01.82.00.05HI Entropy|Song.03.02.11.12.06.04\u0000.02\u0000.01.02.81\u0000.01.02HIB Entropy|Corpus\u0000.13.08.08.68.08.15\u0000.06.26\u0000.03\u0000.10.07\u0000.02MIB Entropy|Corpus\u0000.04\u0000.09\u0000.01.80.01.06.14\u0000.01.05.16.00.07HI Entropy|Corpus\u0000.03\u0000.07\u0000.02.84.04.04.06.04.05.19\u0000.02.04Loudness\u0000.04.92.07\u0000.06\u0000.05\u0000.05\u0000.07.06\u0000.04.02\u0000.07.04Roughness.14.78.14.01.15.09.31.06\u0000.08.07.06.01Melodic Pitch Height.13.66\u0000.05\u0000.03.09\u0000.24\u0000.16.09.22\u0000.06\u0000.06.00MFCC Variance.13\u0000.51\u0000.05.08\u0000.26.10.05\u0000.02.48.02\u0000.22\u0000.10Loudness|Song\u0000.03\u0000.05.67\u0000.01.06.01.07\u0000.04.10.03.11\u0000.03Roughness|Song.04.10.67\u0000.03\u0000.01.02.11.08\u0000.05\u0000.02\u0000.04\u0000.05Mel. Pitch Height|Song\u0000.01.02.46.03.13.14\u0000.12\u0000.15.29.07.16.03MFCC Mean|Song.07.07.61\u0000.04.21.12.10.10\u0000.07.16.11.11MFCC Variance|Song.00\u0000.04.54.03.01\u0000.06.10.08\u0000.10\u0000.09\u0000.06.17Loudness|Corpus.04\u0000.23.06.07.12.08.76\u0000.05.22.02.10\u0000.05Roughness|Corpus.12.34.15.03.00.01.71\u0000.07.05.04.03\u0000.07Mel. Pitch Height|Corpus.00.04.06.06.25.06.14\u0000.01.60.02.14\u0000.09MFCC Mean|Corpus.21.13.12.07.51.03.31.20\u0000.18.05.14.08MFCC Variance|Corpus\u0000.09\u0000.09.08.08.25\u0000.02.40.05\u0000.13\u0000.13\u0000.12.21Sharpness.23.11.03.08.72.04.29.13.08\u0000.01.10.05Sharpness|Song\u0000.02\u0000.07.24\u0000.04.50.06\u0000.14\u0000.07.04.15\u0000.08\u0000.04Sharpness|Corpus.08.10.03.06.75.03.03\u0000.02.14\u0000.01\u0000.10\u0000.01Loudness SD.10.38.09.06\u0000.06.06.22.02.40.03\u0000.61\u0000.03Loudness SD|Song.04.02.22.02\u0000.05.00\u0000.05.03.14.01.60.03Loudness SD|Corpus.03.05\u0000.02.05\u0000.03.04.19.02.04.00.78.02Mel. Pitch SD.21\u0000.10\u0000.02\u0000.05.04\u0000.19.21.18\u0000.27.12\u0000.07\u0000.28Mel. Pitch SD|Song.01.04.11.01.04.13.00\u0000.15.01.14.07.69Mel. Pitch SD|Corpus.13.03\u0000.02.06\u0000.01\u0000.02.01.11\u0000.08\u0000.04.00.74R2.16.06.05.05.05.04.04.04.04.04.04.03Note.MIB = Melodic Interval Bigram; HI = Harmonization Interval; HIC = Harmony Interval Co-occurrence. Loadings>.40 are inboldface. Collectively, these components explain 64 % of the variance in the underlying data. We interpret and name them as follows: (1)Melodic/Harmonic Entropy, (2) Timbral Intensity, (3) Timbral Recurrence, (4) Melodic/Harmonic Entropy Conventionality, (5) SharpnessConventionality, (6) Melodic Conventionality, (7) Timbral Conventionality, (8) Harmonic Conventionality, (9) V ocal Prominence, (10)Melodic Entropy Recurrence, (11) Dynamic Range Conventionality, and (12) Melodic Range Conventionality.Table 1. Loadings after varimax rotation for principal component analysis of corpus-based audio features.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 231AudioaAudiobSymbolicbCombinedbParameterˆ\u000099.5 % CIˆ\u000099.5 % CIˆ\u000099.5 % CIˆ\u000099.5 % CIFixed effectsIntercept\u00000.84 [\u00000.91,\u00000.77]\u00000.67 [\u00000.78,\u00000.56]\u00000.62 [\u00000.73,\u00000.51]\u00000.63 [\u00000.74,\u00000.53]AudioV ocal Prominence 0.14 [0.10,0.18] 0.11 [0.04,0.17] 0.08 [0.01,0.15]Timbral Conventionality 0.09 [0.05,0.13]Melodic Conventionality 0.06 [0.02,0.11]M/H Entropy Conventionality 0.06 [0.02,0.10]Sharpness Conventionality 0.05 [0.02,0.09]Harmonic Conventionality 0.05 [0.01,0.10]Timbral Recurrence 0.05 [0.02,0.08]Mel. Range Conventionality 0.05 [0.01,0.08] 0.07 [0.02,0.13] 0.07 [0.01,0.12]SymbolicMelodic Repetitivity 0.12 [0.06,0.19] 0.11 [0.05,0.17]Mel./Bass Conventionality 0.07 [0.01,0.13] 0.08 [0.01,0.14]Random effectsˆ\u0000song0.39 [0.34,0.45] 0.35 [0.26,0.45] 0.34 [0.25,0.44] 0.32 [0.24,0.42]ˆ\u0000residual0.48 [0.45,0.50] 0.40 [0.37,0.44] 0.39 [0.35,0.43] 0.38 [0.34,0.42]R2marginalc.10 .06 .07 .10R2conditionalc.47 .46 .47 .47\u00002⇥log likelihood 2765.61 699.81 576.74 558.11Note.Grouping by song, all models displayed are the optimal random-intercept models for the given feature types after step-wise selectionusing Satterthwaite-adjustedF-tests at↵=.005. Component scores – but not log drift rates – were standardized prior to regression.aComplete set of 321 songs (N=1715 segments).bReduced set of 99 songs with symbolic transcriptions (N=536 segments).cCoefﬁcients of determination following Nakagawa and Schielzeth’s technique for mixed-effects models [13]. The marginal coefﬁcientreﬂects the proportion of variance in the data that is explained by the ﬁxed effects alone and the conditional coefﬁcient the proportionexplained by the complete model (ﬁxed and random effects together).Table 2. Estimated prediction coefﬁcients and variances for audio and symbolic components inﬂuencing the relativerecognizability (log drift rate) of popular song segments.The model based on symbolic data only, in the third col-umn, has just two components. This is possibly due to the re-duced number of sections available for ﬁtting, as the audio-based model run on the reduced dataset also yields justtwo components. The top symbolic features that make upthe ﬁrst of the signiﬁcant components are melodic entropyand productivity, both negatively correlated, suggesting thatrecognizable melodies are more repetitive. The top featuresthat make up the second components aremtcf.mean.log.DF,for the melody (song-based and corpus-based), and negativemtcf.mean.productivity(song-based and corpus-based forboth bass and melody). This suggests that recognizablemelodies contain more typical motives (higher DF, lowersecond-order productivity).The last column shows how the combined model, inwhich both audio and symbolic components were used,retains the same audio and symbolic components that makeup the previous two models. The feature sets are, in otherwords, complementary: not only are all four componentsstill relevant at↵<.005, the marginalR2now reaches.10, as opposed to.06 and.07 for the individual models.This answers the last of the questions stated in Section 3:for the data in this study, the audio-based corpus analysistools contribute substantial insight, and make an excellentaddition to the symbolic feature set.5. CONCLUSIONS AND FUTURE WORKWe have presented a strategy for audio corpus descriptionthat combines a new kind of melodic and harmonic intervalproﬁles, three general-purpose second-order features,and the newly introduced notion of song-based andcorpus-based second-order features. Using these featuresto analyse the results of a hook discovery experiment,we show that all of the above contributions add new andrelevant layers of information to the corpus description. Weconclude that an audio corpus analysis as proposed in thispaper can indeed complement symbolic corpus analysis,which opens a range of opportunities for future work. Aspossible future directions we would like to perform moreexperiments on the Hooked! data, exploring more ﬁrst- andsecond-order descriptors and more powerful statistical ormachine-learning models, to see if allowing for interactionsand non-linearities helps to explain more of the variance indrift rates between sections. We also would like to extendthe feature set to explore rhythm description and chordestimation, especially as more reliable transcription toolsbecome available from the MIR community.AcknowledgementsJVB, JAB and DB are supported by COGITCH (NWOCATCH project 640.005.004) and FES project COMMIT/.232 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20156. REFERENCES[1]Mathieu Barthet, Mark Plumbley, Alexander Kachkaev,Jason Dykes, Daniel Wolff, and Tillman Weyde. Bigchord data extraction and mining. InProceedings of the9th Conference on Interdisciplinary Musicology, Berlin,Germany, 2014.[2]Scott Brown and Andrew Heathcote. The simplest com-plete model of choice response time: Linear ballistic ac-cumulation.Cognitive Psychology, 57(3):153–78, 2008.[3]John Ashley Burgoyne, Dimitrios Bountouridis, JanVan Balen, and Henkjan J. Honing. Hooked: A game fordiscovering what makes music catchy. InProceedingsof the 14th International Society for Music InformationRetrieval Conference, pages 245–50, Curitiba, Brazil,2013.[4]John Ashley Burgoyne, Jan Van Balen, DimitriosBountouridis, Themistoklis Karavellas, Frans Wiering,Remco C. Veltkamp, and Henkjan J. Honing. The con-tours of catchiness, or Where to look for a hook. Paperpresented at the International Conference on Music Per-ception and Cognition, Seoul, South Korea, 2014.[5]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-jinaga. Compositional data analysis of harmonic struc-tures in popular music. In Jonathan Wild, Jason Yust,and John Ashley Burgoyne, editors,Mathematics andComputation in Music, pages 52–63. Springer, Berlin,2013.[6]Darrell Conklin and Ian H. Witten. Multiple viewpointsystems for music prediction.Journal of New MusicResearch, 24(1):51–73, 1995.[7]Trevor de Clercq and David Temperley. A corpus analy-sis of rock harmony.Popular Music, 30(1):47–70, 2011.[8]John L. Horn. A rationale and test for the number offactors in factor analysis.Psychometrika, 30(2):179–85,1965.[9]David Huron.Sweet Anticipation: Music and the Psy-chology of Expectation. MIT Press, Cambridge, MA,2006.[10]Matthias Mauch, Robert M MacCallum, Mark Levy,and Armand M Leroi. The evolution of popular music:USA 1960–2010.Royal Society Open Science, In press.[11]Dirk Moelants, Olmo Cornelis, and Marc Leman. Ex-ploring African tone scales. InProceedings of the 10thInternational Society for Music Information RetrievalConference, pages 489–94, Kobe, Japan, 2009.[12]Daniel M¨ullensiefen and Andrea R Halpern. Therole of features and context in recognition of novelmelodies.Music Perception: An Interdisciplinary Jour-nal, 31(5):418–435, 2014.[13]Shinichi Nakagawa and Holger Schielzeth. A generaland simple method for obtainingR2from generalizedlinear mixed-effects models.Methods in Ecology andEvolution, 4(2):133–42, 2013.[14]Marcus Thomas Pearce.The Construction and Eval-uation of Statistical Models of Melodic Structure inMusic Perception and Composition. PhD thesis, CityUniversity London, England, 2005.[15]Joan Serr`a, Alvaro Corral, Mari´an Bogu˜n´a, Mart´ın Haro,and Josep Ll. Arcos. Measuring the evolution of con-temporary Western popular music.Scientiﬁc Reports,2(521), 2012.[16]Xavier Serra, Michela Magas, Emmanouil Benetos,Magdalena Chudy, S. Dixon, Arthur Flexer, EmiliaG´omez, F. Gouyon, P. Herrera, Sergi Jord`a, Oscar Pay-tuvi, G. Peeters, Jan Schl¨uter, H. Vinet, and G. Widmer.Roadmap for Music Information ReSearch. MIReS Con-sortium, 2013.[17]Jan Van Balen, Dimitrios Bountouridis, Frans Wier-ing, and Remco Veltkamp. Cognition-inspired descrip-tors for scalable cover song retrieval. InProceedingsof the 15th Interational Society for Music InformationRetrieval Conference, pages 379–384, Taipei, Taiwan,2014.[18]Jan Van Balen, John Ashley Burgoyne, Frans Wier-ing, and Remco C. Veltkamp. An analysis of chorusfeatures in popular song. InProceedings of the 14thInternational Society for Music Information RetrievalConference, Curitiba, Brazil, 2013.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 233"
    },
    {
        "title": "Predictive Power of Personality on Music-Genre Exclusivity.",
        "author": [
            "Jotthi Bansal",
            "Matthew Woolhouse"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417467",
        "url": "https://doi.org/10.5281/zenodo.1417467",
        "ee": "https://zenodo.org/records/1417467/files/BansalW15.pdf",
        "abstract": "Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this rela- tionship using a new methodology: genre dispersion among people’s mobile-phone music collections. By analyzing the download behaviours of genre-defined user subgroups, we investigated the following questions: (1) do genre-pre- ferring subgroups show distinct patterns of genre consump- tion and genre exclusivity; (2) does genre exclusivity re- late to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusiv- ity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be “open” to differ- ent musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people’s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wid- er musical tastes than those with low openness and agree- ableness. Our study corroborated previous research link- ing genre preference and personality, and revealed, in a novel way, the predictive power of personality on music- consumption.",
        "zenodo_id": 1417467,
        "dblp_key": "conf/ismir/BansalW15",
        "keywords": [
            "personality",
            "music preference",
            "genre dispersion",
            "mobile-phone music collections",
            "download behaviours",
            "Big Five personality factors",
            "genre exclusivity",
            "genre inclusivity",
            "openness",
            "agreeableness"
        ],
        "content": "PREDICTIVE POWER OF PERSONALITY ON MUSIC-GENREEXCLUSIVITYJotthi BansalMcMaster Universitybansalj@mcmaster.caMatthew WoolhouseMcMaster Universitywoolhouse@mcmaster.caABSTRACTStudies reveal a strong relationship between personalityand preferred musical genre. Our study explored this rela-tionship using a new methodology: genre dispersion amongpeople’s mobile-phone music collections. By analyzingthe download behaviours of genre-deﬁned user subgroups,we investigated the following questions: (1) do genre-pre-ferring subgroups show distinct patterns of genre consump-tion and genre exclusivity; (2) does genre exclusivity re-late to Big Five personality factors? We hypothesized thatgenre-preferring subgroups would vary in genre exclusiv-ity, and that their degree of exclusivity would be linearlyassociated with the openness personality factor (if peoplehave open personalities, they should be “open” to differ-ent musical styles). Consistent with our hypothesis, resultsshowed that greater genre inclusivity, i.e. many genresin people’s music collections, positively associated withopenness and (unexpectedly) agreeableness, suggesting thatindividuals with high openness and agreeableness have wid-er musical tastes than those with low openness and agree-ableness. Our study corroborated previous research link-ing genre preference and personality, and revealed, in anovel way, the predictive power of personality on music-consumption.1. INTRODUCTIONExisting music-personality studies have speciﬁcally exam-ines the relationship between music preference and BigFive personality factors [4, 13, 16]. The music peoplelisten to–their musical preferences–reveal aspects of theiridentity [12], to the point where music can be worn as a“badge” of honour [16].Big Five personality factors are designed to delineatebasic, measureable features of personality. Each factorconsists of various traits that describe behaviour, thoughtsand emotions; traits that co-vary with one-another are cat-egorized under one factor [3]. Factors in the current BigFive model are openness, conscientiousness, extraversion,agreeableness, and neuroticism. Each factor is deﬁned basedon terms from everyday language [7].c\u0000Jotthi Bansal, Matthew Woolhouse.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Jotthi Bansal, Matthew Woolhouse.“Predictive power of personality on music-genre exclusivity”, 16th Inter-national Society for Music Information Retrieval Conference, 2015.In detail, the Big Five personality factors are as fol-lows. Openness measures open-mindedness to new expe-riences, including traits such as creativity, insightfulness,and originality. Conscientiousness measures efﬁciency andorganization, including resourcefulness and intelligence.Extraversion measures sociability, including outgoingness,self-conﬁdence, and aggression. Agreeableness measuresfriendliness and compassion, including trustworthiness, com-pliance, and modesty. Lastly, neuroticism measures emo-tional vulnerability, including moodiness, hostility, self-consciousness, and impulsivity [11].In respect of individuals’ personalities, the Big Five arequantiﬁed using the NEO-PI psychometric inventory [3].A common methodology of music-personality studies as-sociates NEO-PI results with music-preference tests (e.g.for genres). Results from existing studies have revealedmany relationships between the Big Five and musical pref-erences, which will now be overviewed.Individuals with high openness typically prefer genressuch as blues and jazz, while avoiding pop and country[19]. They also enjoy a wider variety of musical genresoverall [15]. High conscientiousness has been linked tosoul and funk [19]. Extraverts prefer pop and rap [19],which commonly occur in social situations, and thus mayappeal to those high in extraversion [14, 15]. High agree-ableness is associated with soundtracks (e.g. of ﬁlms).The ﬁfth factor, neuroticism, predicts preference for genreswith exaggerated bass, such as dance [10].The current study examined music and personality interms of music-consumption patterns. The primary patternwe studied was genre exclusivity–a measure of the varietyof genres in users’ music collections. Genre exclusivitycan be thought of as a scale with two extremes. The lowerend contains homogenous music collections with very fewgenres (referred to as “genre exclusive”); the upper endcontains heterogeneous music collections with many well-represented, distinct genres (referred to as “genre inclu-sive”). We investigated the link between genre homogene-ity/heterogeneity, musical preference and factors within theBig Five, and in so doing evaluated the predictive power ofpersonality on genre exclusivity.1.1 MixRadio DatabaseThis study utilized a music-download database, the major-ity of which were made onto Nokia mobile phones. Thedata became accessible through a data sharing agreementbetween McMaster University and the Nokia Corporation652which begun in 2012. In January 2015 the Nokia divisionresponsible for music became a separate entity under thename MixRadio. Henceforth, we referred to the data ascoming from the MixRadio database.The MixRadio database contains downloads from 33countries across the globe1and spans from 2007 to Septem-ber of 2014. Currently, the database contains the metadataof 1.36 billion individual downloads from over 17 millionMixRadio users.2MixRadio users had free access to un-limited amounts of music on online music stores, meaningthey could explore musical genres without cost constraints.Each download’s metadata includes information such astrack name, artist, album, genre, user ID (anonymous),date, (local) time and country. Open source databases in-cluding MusicBrainz (the open music encyclopedia) [9]and The Echo Nest [6] are used to supplement downloadmetadata and enrich the database. Examples of supple-mented information from additional databases include track-release date, tempo, key, mode, time signature and instru-mentation. The data are arranged into a relational databasemanagement system and queried using the open-source My-SQL implementation of SQL [18], and the Python DatabaseAPI [9], enabling more extensive, iterative analyses to beundertaken.Our ﬁrst study used the MixRadio database to exploremusic consumption behaviours of genre-deﬁned subgroupsof users. We referred to these subgroups as “x-heads”,where “x” was a user’s most downloaded genre. As genreis the most commonly used musical classiﬁer [16], we as-sumed genre to be a reliable marker of musical interest.The second study examined the relationship betweengenre exclusivity of x-head subgroups and Big Five per-sonality factors. We correlated measures of genre exclu-sivity with measures from an existing study associating theBig Five with preference for particular genres. We hy-pothesized that openness values would positively correlatewith genre inclusivity (having a heterogeneous music col-lection). In other words, those high in openness shouldalso be open to numerous genres. Previous literature hasfound that those high in openness tend to prefer diversemusical genres [15]. We conjectured that the remainingBig Five factors–extraversion, neuroticism, agreeablenessand conscientiousness–would not correlate with genre ex-clusivity, due to lack of evidence of this in previous studies.1.2 Study ParametersAs existing music-personality study focused on Westernpopulations, we elected only to include user data from Eu-ropean countries (14 countries in total): Austria, Finland,France, Germany, Great Britain, Ireland, Italy, Netherlands,Norway, Poland, Portugal, Spain, Sweden and Switzer-land. Downloads were also limited to the ten most com-1Argentina, Australia, Austria, Brazil, Britain, Canada, Chile, China,Finland, France, Germany, India, Indonesia, Ireland, Italy, Malaysia,Mexico, Netherlands, Norway, Poland, Portugal, Russia, Saudi Arabia,Singapore, South Africa, Spain, Sweden, Switzerland, Thailand, Turkey,United Arab Emirates, United States of America, Venezuela2This represents only a portion of MixRadio’s total database, and isnot indicative of market share.monly used genres in existing music and personality stud-ies: classical, country, dance, folk, indie, jazz, metal, pop,rap and rock. Finally, to ensure robust measures of genreexclusivity, only users with between 10 and 5,000 down-loads were included; heuristically, we decided that fewerthan ten would be an insufﬁcient sample size; greater thanﬁve-thousand might indicate that a user was simply a mu-sical “stamp collector”.2. STUDY 1.1We used the MixRadio database to explore genre exclu-sivity in genre-deﬁned subgroups of users. Each user inthe study was categorized as an “x-head”, where x was themost popular genre within a user’s download collection.For example, if a user’s total collection contained 40 metaldownloads, and 10 dance, they were deﬁned as a “metal-head”, and placed within the metal-head subgroup. If nogenre was more popular than any other in a user’s collec-tion (e.g. 10 pop and 10 rock), the user was classiﬁed basedon whichever genre they downloaded ﬁrst. The raw countsper genre were obtained for each user, and a (normalized)level of genre exclusivity per user calculated by dividingthe SD of the genre counts by their total number of down-loads.So as to weigh each country’s contribution to genre ex-clusivity equally, users in x-head subgroups were then sub-divided based on user-country, and a median SD per x-headsubgroup per country was calculated; this value was called“x-med”. For each x-head subgroup the x-med was de-rived from fourteen SD values (one per country). X-headsubgroups were ranked based on their degree of genre ex-clusivity, i.e. x-med value. The lower the x-med, the moregenre inclusive the x-head subgroup; the higher the x-medvalue, the more genre exclusive the x-head subgroup.2.1 ResultsTable 1 displays x-med values for x-head subgroups frommost inclusive on left, to most exclusive on right. Indie-heads, who had the lowest x-med (0.137), were the mostgenre inclusive subgroup, while pop-heads who had thehighest x-med (0.200) were the most genre exclusive. Amore detailed look at x-head subgroups’ collections basedon genre is discussed in Study 1.2 below.3. STUDY 1.2This study examined how x-head subgroups consumed mu-sic from individual genres. Speciﬁcally, we looked at pairsof x-head subgroups and examined the degree to whichboth x-head subgroups consumed each other’s main, group-deﬁning genre. Equation (1) calculates the degree to whichx-head subgroups consumed each genre.\nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 653Table 1. Percentage of genres in each x-head subgroup’s collection compared to their main genre.\nFigure 1. Percentage of genres in each x-head subgroup’scollection compared to their main genre.Ci,j = count of genre i in x-head j’s collectionN = number of x-headsSj,i = the value of nthrow and ithcolumn (in particular,Sj,i is a measure of the average relative proportion ofgenre i in x-head j’s collection)Each value of Sj,i refers to a cell shown in Figure 1.3.1 ResultsFigure 1 shows the degrees to which x-head subgroupsconsumed other genres. The left-axis lists x-head sub-groups; the top-axis lists the genres they consumed. Thedarker the cell, the greater the degree of genre consump-tion. The x-head medians listed in the far right column arethe median percentages of the genres consumed by x-headsubgroups. The genre medians listed along the bottom arethe median percentages that each genre is consumed by thex-head subgroups. Figure 1 is symmetrical along its diago-nal axis (diagonal line of white cells). By comparing eachside of the diagonal axis, relationships between genre pairscan be explored. For example, rock-heads and pop-headsconsumed the greatest percentage of each another’s genres:rock-heads consumed 29.1% of pop, pop-heads consumed20.9% of rock.Various “classes” of relationships appeared based on thedegree of genre consumption by pairs of x-head subgroups.Some x-head subgroup pairs consumed equal amounts ofeach other’s main genre, and therefore had symmetricalrelationships (same-shaded cells across the diagonal axis,e.g. rap and metal). Some x-head subgroup pairs con-sumed unequal amounts of each other’s main genre, and\nFigure 2. H-H consumption relationship between pop-heads and rock-heads.therefore had asymmetrical relationships (differently shadedcells across the diagonal axis, e.g. indie and pop).Symmetrical relationships were also classiﬁed as “hot”or “cold” based on the volume of consumption betweentwo x-head subgroups. Symmetrically hot relationshipsoccurred when both x-head subgroups downloaded signif-icant amounts of each other’s main genre. Symmetricallycold relationships occurred when neither x-head subgroupdownloaded signiﬁcant amounts of each other’s main genre.Overall, three categories of x-head relationships were iden-tiﬁed and are deﬁned below using example pairs of x-headsubgroups.3.1.1 Symmetrical hot relationships (H-H)Pairs of x-head subgroups downloading signiﬁcant and ap-proximately equally amounts of one another’s main genre,e.g. rock-heads and pop-heads (Figure 2).Figure 2 shows the composition of rock-heads’ (grey)and pop-heads’ (black) collections when comparing onlythe proportion of rock and pop downloads they each con-sumed. The x-axis displays a series of bins which describethe proportion of rock and pop downloads in x-heads’ col-lections (totalling 100%). The y-axis is the percentage ofx-heads that ﬁt into the speciﬁcations of each bin on thex-axis. There are two sets of horizontal-axis labels: theupper labels (% Rock) show the proportion of rock down-loads represented in rock-heads’ collections. The remain-ing proportion consists of pop downloads. For example,the grey column in the % Rock bin marked “50-60” showsthe percentage of rock-heads whose collection containedapproximately 50-60% rock downloads and 40-50% popdownloads. The lower labels (% Pop) show the proportionof pop downloads represented in pop-heads’ collections.The remaining percentage consists of rock downloads.H-H relationships are represented in Figure 1 by diago-nally related dark-shaded squares. X-head subgroup pairswith H-H relationships can thought of as being mutually654 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 3. C-C consumption relationship between jazz-heads and metal-heads.\nFigure 4. H-C consumption relationship between pop-heads and country heads.inclusive, and vice versa for light-shaded squares.3.1.2 Symmetrical cold relationships (C-C)Pairs of x-head subgroups who downloaded roughly equal,but insigniﬁcant amounts of the each others’ main genre,e.g. jazz-heads and metal-heads (Figure 3).The axes in Figure 3 are the same as those in Figure2, but represent jazz-heads and metal-heads instead. Barheights in Figure 3 reveal that a majority jazz-heads andmetal-heads had a ratio of 90-100% of their main genreand 0-10% of the other. Very few jazz-heads or metal-heads downloaded equal amounts of both genres. C-C rela-tionships are represented in Figure 1 by diagonally relatedlight-shaded squares. X-head subgroup pairs with C-C re-lationships can be thought of as being mutually exclusive.3.1.3 Asymmetrical hot-cold relationships (H-C)Pairs of x-head subgroups who consumed each other’s maingenre unequally, e.g. pop-heads and country-heads (Figure4).The axes in Figure 4 are the same as those in Figures2 and 3, but represent pop-heads and country-heads. Barheights in Figure 4 revealed that many country-heads con-sumed large amounts of both pop and country music. How-ever, a majority of pop-heads did not consume signiﬁcantamounts of country music. H-C relationships are repre-sented in Figure 1 by diagonally related cells, betweenwhich there is a mismatch in shading, i.e. light grey todark grey.3.2 Study 1 ConclusionsIn Study 1.1, x-head subgroups ranked from genre exclu-sive to inclusive in the following order: pop, dance, rap,metal, rock, classical, country, folk, jazz, and indie. In-triguingly, this ranking is consistent with previous litera-ture indicating that individuals who prefer jazz and folkmusic rank highly in the Big Five factor of openness, whichhas been linked to genre inclusivity. Those who are highin openness also tend to avoid genres like pop; pop-headswere found to be the most genre exclusive. Therefore,study 1.1 results preliminarily hinted at links between genreexclusivity and aspects of personality.In Study 1.2, pairs of x-head subgroups were comparedbased on their consumption of one another’s main genre.Some x-head subgroup pairs were mutually inclusive ofone another (H-H), while others were mutually exclusive(C-C). Remaining x-head pairs consumed each other’s maingenres unequally (H-C).4. STUDY 2Study 2 examined links between genre exclusivity and theBig Five personality factors. Our measures of genre ex-clusivity (median SD per x-head subgroup per country)were correlated with measures of Big Five personality fac-tors that had previously been associated with certain genresfrom Zweigenhaft (2008) [19].Zweigenhaft had subjects complete the NEO-PI and aversion of the STOMP (Short Test of Music Preferences),[16]. Measures of Big Five personality and music pref-erence were then correlated. We used the correlation val-ues between Big Five factors and genres from Zweigenhaft(2008), and correlated them with levels genre exclusivityfrom Study 1.1 (14 country values per x-head subgroup).4.1 ResultsA signiﬁcant, negative correlation existed between genreexclusivity and genres associated with openness (Figure 5:n = 140; r = -0.37; two-tailed, p<0.001) and agreeableness(Figure 6: n = 140; r = -0.32; two-tailed, p<0.001). Thatis, genre-openness associations and genre-agreeableness as-sociations in Zweigenhaft (2008) predicted genre inclusiv-ity in x-head subgroups. There were no signiﬁcant correla-tions between extraversion, conscientiousness and neuroti-cism with genre exclusivity.Figures 5 and 6 show relationships between opennessand agreeableness with genre exclusivity. The horizontal-axes display degree of genre exclusivity for x-head sub-groups (median SD of x-heads’ music collections basedon genre). Each x-head subgroup (listed down the rightlegend) is represented with a different shade of grey. Hor-izontally positioned markers with the same shade are themedian SDs per x-head subgroup for each of the 14 coun-tries included in the study. The height of the markers cor-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 655Figure 5. X-head genre exclusivity against genre-openness associations in Zweigenhaft (2008).\nFigure 6. X-head genre exclusivity against genre-agreeableness associations in Zweigenhaft (2008).responds to the degree of openness and agreeableness foreach genre in Zweigenhaft (2008), shown on the y-axes.4.2 Study 2 ConclusionsGenre-openness and -agreeableness associations from Zwei-genhaft (2008) predicted genre inclusivity in x-head sub-groups; if you score high in openness and/or agreeablenessyou are likely to have more genres within your music col-lection. Conscientiousness, extraversion and neuroticismare not predictors of genre exclusivity.5. DISCUSSIONStudy 1 explored overall genre exclusivity of x-head sub-groups. Study 1.2 revealed the pairwise relationships be-tween x-head subgroups. Some of these relationships wereone-sided; only one of the two x-head subgroups consumedmusic from the-other’s main genre. While others weremore equitable; both x-head subgroups consumed each-other’s main genre equally.Study 2 revealed links between genre exclusivity andpersonality; openness and agreeableness predicted prefer-ence for a wide range of genres. Breaking down open-ness and agreeableness based on their traits reveals pos-sible reasons for their relationship with genre exclusivity.Openness is a general willingness to encounter new experi-ences, and different musical styles certainly constitute newexperiences. If someone is open to new experiences, theyalso seem to be open to new musical genres. Those high inopenness tend to break from the rules of social boundaries[5] and may not fear venturing outside of Western-culturedmusical norms. Those high in openness often dislike ubiq-uitous genres like pop [19], tending, instead, to exploreless commercial musical styles. Moreover, they use mu-sic for cognitive and rational purposes, such as intellectualstimulation, and focus more on the quality, complexity andperformance [1]. Exploration of numerous genres may sat-isfy their desire for these musical properties.The ability of agreeableness to predict genre exclusiv-ity was unanticipated–few studies have found this factorto be a reliable predictor of musical preference. However,agreeableness encompasses traits such as compliance [19],so perhaps those who are agreeable may also be “compli-ant” to various musical genres. To test this theory, associa-tions between traits of agreeableness and genre exclusivitywould have to be examined.5.1 LimitationsGiven that our data were derived predominantly from mobile-phone users, it may be problematic to generalize our ﬁnd-ings to those who acquire music from other sources. More-over, Studies 1 and 2 were restricted to European countries,again, limiting result generalizability. Since personality[17] and musical preferences [16] vary between countries,our results may not be globally consistent.A second population-based limitation relates to socioe-conomic variance between individuals and countries. Theusers in the MixRadio database are biased to those whocan afford a Nokia mobile phone. Despite this, Nokia hashistorically made a range of models to appeal to differentmarket sectors. Therefore, although the self-selected usersin our study may not be fully representative, it is assumedthat they are relatively widely distributed throughout thepopulations of the countries within our study.A third limitation arises when associating genre-person-ality correlations from Zweigenhaft (2008) with measuresof genre exclusivity: the subject group tested in Zweigen-haft (2008) are not the same as the MixRadio user pop-ulation. However, without gathering personality informa-tion directly from MixRadio users, genre-personality cor-relations were the most suitable measure to associate withgenre exclusivity.Additionally, given that pop is the commonest genre, itis perhaps not surprising that most pairwise relationshipswith pop are asymmetrical and that pop is the most pop-ular genre for non-pop-heads. However, despite this limi-tation the method adopted (as shown in Figure 1) does atleast indicate instances where x-head subgroups consumedifferent amounts of another genre relative to one another.For example, relatively speaking, pop-heads consume lesscountry than jazz-heads.A possible methodological complication relates to theway in which x-heads are deﬁned based upon most down-loaded genre. That is, we assume that users’ genre distri-656 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015butions represent genuine musical preferences, which, al-though likely to be the case, is not known for certain. Inother words, our notion of genre popularity could be a mis-representation of musical tastes.5.2 ImplicationsInformation about x-head genre exclusivity is a valuableresource in music marketing and recommender systems.For example, a MixRadio user purchased a large quan-tity of country songs. For example, based on results fromStudy 1.2, country-heads would appear to be susceptibleto pop, although, given the asymmetrical relationship be-tween these genres, the reverse seems not to be the case(country-heads consume pop, but pop-heads do not con-sume country). Understanding each side of x-head rela-tionships could be useful in avoiding misguided recom-mendations.Moreover, understanding the link between personalityand genre consumption may prove useful in music mar-keting. If a user were to complete a Big Five personalityquestionnaire upon signing up with a music service, infor-mation concerning openness and agreeableness could befactored into recommendations; e.g. wide range of obscuregenres for those open and/or agreeable, and vice versa.5.3 Future StudiesThe reasons underpinning genre inclusivity or exclusivitycan be examined further. For example, perhaps certaingenres are downloaded in tandem due to similar acousticproperties such as tempo, key, instrumentation, or metricalstructure. Feature analysis and genre preference will be atarget of future studies.Our new-found links between the Big Five and genreexclusivity mark the beginning of explorations on person-ality and music consumption. Other types of exclusivityrelationships may also be linked to personality traits, in-cluding artist exclusivity (the number of artists in a user’scollection), tempo exclusivity (variety of tempos in a user’scollection), or release-date exclusivity (the era from whicha musical collection stems). We hope to examine these fac-tors, other factors, and their possible links to personality.6. CONCLUSIONBy analyzing a subset of mobile phone music-downloaddata, the current study revealed information concerningmusical-genre consumption. Genre-deﬁned subgroups ofusers acquired music in unique and distinctive ways, withvarying degrees of acceptance for other musical styles. Over-all, genre exclusivity was most consistently associated withthe Big Five personality factor of openness, which sup-ports similar research in existing music-personality stud-ies. Genre exclusivity was also linked to agreeableness,adding a new ﬁnding to the music-personality literature.Overall, the more open or agreeable you are, the moregenre inclusive, or heterogeneous, your musical tastes.The current study introduced a novel big-data method-ology to music-personality studies, which we will continueto utilize. With access to ever-growing music-downloaddatabases, the predictive power of personality on genreexclusivity is an exciting and expanding ﬁeld of music-consumption research.7. REFERENCES[1]T. Chamorro-Premuzic, and A. Furnham. Personalityand music: Can traits explain how people use musicin everyday life?British Journal of Psychology, 98(2):175–85, 2007.[2]P. T. Jr. Costa, and R. R. McCrae.The NEO Person-ality Inventory Manual, Psychological Assessment Re-sources, Odessa, 1985.[3]P. T. Jr. Costa, and R. R. McCrae. Four ways the ﬁvefactors are basic.Personality and Individual Differ-ences, 13(6): 653–665, 1992.[4]S. J. Dollinger. Research note: Personality and mu-sic preference: Extraversion and excitement seeking oropenness to experience?Psychology of Music, 21(1):73–77, 1993.[5]S. J. Dollinger, L. A. Orf, and A. E. Robinson. Person-ality and campus controversies: Preferred boundariesas a function of openness to experience.The Journal ofPsychology, 125(4): 399–406, 1991.[6]T. Jehan, and B. Whitman. The Echo Nest. [Data set].Retrieved from http://the.echonest.com/, 2005.[7]O. P. John and S. Srivastava. Big ﬁve trait taxonomy:History, measurement, and theoretical perspectives. InL. Pervin & O. P. John (Eds.).Handbook of person-ality: Theory and research (2nd ed.)(pp. 102–138),Guilford, Berkely, 1999.[8]R. Kaye. MusicBrainz Database [Data set]. Retrievedfrom http://musicbrainz.org/doc/MusicBrainzDatabase, 2000.[9]M. A. Lemburg. Python database API speciﬁcation v2.0.Python Enhancement Proposal, 249, 2008.[10]W. McCown, R. Keiser, S. Mulhearn, and D.Williamson. The role of personality and gender in pref-erence for exaggerated bass in music.Personality andIndividual Differences, 23(4): 543–547, 1997.[11]R. R. McCrae, and P. T. Costa, P. T. Validation of theﬁve-factor model of personality across instruments andobservers.Journal of Personality and Social Psychol-ogy, 52(1): 81–90, 1987.[12]A. C. North, and D. J. Hargreaves. Music and adoles-cent identity.Music Education Research, 1(1): 75–92,1999.[13]E. Payne. Musical taste and personality.The BritishJournal of Psychology, 58(1): 133–138, 1967.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 657[14]J. L. Pearson, and S. J. Dollinger. Music preferencecorrelates of Jungian types.Personality and IndividualDifferences, 36(5): 1005–1008, 2004.[15]D. Rawlings, and V. Ciancarelli. Music preference andthe ﬁve-factor model of the NEO personality inventory.Psychology of Music, 25(2): 120–132, 1997.[16]P. J. Rentfrow, and S. D. Gosling. The do re mi?s ofeveryday life: The structure and personality correlatesof music preferences.Journal of Personality and SocialPsychology, 84(6): 1236–1256, 2003.[17]D. P. Schmitt, J. Allik, R. R. McCrae, and V. Benet-Martinez. The geographic distribution of big ﬁvepersonality traits: Patterns and proﬁles of humanself-description across 56 nations.Journal of Cross-Cultural Psychology, 38(2): 173–212, 2007.[18]P. Weinberg, J. Groff, A. Oppel, and A. Davenport.SQL, the Complete Reference. McGraw-Hill, NewYork, 2010.[19]R. L. Zweigenhaft. A do re mi encore.Journal of Indi-vidual Differences, 29(1), 45–55, 2008.658 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Benford&apos;s Law for Music Analysis.",
        "author": [
            "Isabel Barbancho",
            "Lorenzo J. Tardón",
            "Ana M. Barbancho",
            "Mateu Sbert"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417012",
        "url": "https://doi.org/10.5281/zenodo.1417012",
        "ee": "https://zenodo.org/records/1417012/files/BarbanchoTBS15.pdf",
        "abstract": "Benford’s law defines a peculiar distribution of the lead- ing digits of a set of numbers. The behavior is logarith- mic, with the leading digit 1 reflecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the applica- tion of Benford’s law to many different fields. In this paper, a novel exploitation of Benford’s law for the analysis of au- dio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain au- dio dataset to Benford’s law are presented. These new pro- posed features are succesfully tested in two concrete audio tasks: the detection of artificially assembled chords and the estimation of the quality of the MIDI conversions.",
        "zenodo_id": 1417012,
        "dblp_key": "conf/ismir/BarbanchoTBS15",
        "keywords": [
            "Benford’s law",
            "peculiar distribution",
            "leading digits",
            "logarithmic",
            "probability of occurrence",
            "audio signals",
            "evaluation of agreement",
            "audio tasks",
            "artificially assembled chords",
            "estimation of MIDI conversions"
        ],
        "content": "BENFORD’S LAW FOR MUSIC ANALYSISIsabel Barbancho1Lorenzo J. Tard´on1Ana M. Barbancho1Mateu Sbert21Universidad de M´alaga, ATIC Research Group, ETSI Telecomunicaci´on,Dpt. Ingenier´ıa de Comunicaciones, Campus Teatinos, 29071 M´alaga, Spain2University of Girona, Institute of Informatics and Applications,Campus Montilivi, 17003 Girona, Spainibp@ic.uma.es, lorenzo@ic.uma.es, abp@ic.uma.es, mateusbert@mac.comABSTRACTBenford’s law deﬁnes a peculiar distribution of the lead-ing digits of a set of numbers. The behavior is logarith-mic, with the leading digit 1 reﬂecting largest probabilityof occurrence and the remaining ones showing decreasingprobabilities of appearance following a logarithmic trend.Many discussions have been carried out about the applica-tion of Benford’s law to many differentﬁelds. In this paper,an o v e le x p l o i t a t i o no fB e n f o r d ’ sl a wf o rt h ea n a l y s i so fa u -dio signals is proposed. Three new audio features based onthe evaluation of the degree of agreement of a certain au-dio dataset to Benford’s law are presented. These new pro-posed features are succesfully tested in two concrete audiotasks: the detection of artiﬁcially assembled chords and theestimation of the quality of the MIDI conversions.1. INTRODUCTIONBenford’s law, also known as the ‘ﬁrst-digit law’, describesa peculiar distribution of the leading digits of datasets ofnumbers, especially those related to the measure of ‘real-life phenomena’. Unlike thecentral limit theorem,B e n -ford’s law states that the typical distribution of the lead-ing digits of a large number of datasets, derived from themeasure of several common variables follows a logarithm-shaped law.Most of the measures from real-life (tax returns, streetaddresses, population number or length of rivers) seem topresent this peculiar distribution. Many works have beenpublished on Benford’s law, mixing the empirical evidencewith some more mathematical formalism.Benford’s law has been widely proposed as a discrimi-nating tool for ‘naturally-shaped’ datasets [6] and even em-ployed [8] or criticized [5] as a somewhat reliable diagnos-tic tool to detect a large variety of frauds.In this paper, Benford’s law is evaluated as a discrimina-tor for audio signals. In particular it is employed to detectc\u0000Isabel Barbancho, Lorenzo J. Tard´on, Ana M. Barban-cho, Mateu Sbert. Licensed under a Creative Commons Attribution 4.0International License (CC BY 4.0).Attribution:Isabel Barbancho,Lorenzo J. Tard´on, Ana M. Barbancho, Mateu Sbert. “Benford’s Lawfor Music Analysis”, 16th International Society for Music InformationRetrieval Conference, 2015.differences between natural and artiﬁcially created chordsand real music and MIDI-generated music.The article is organized as follows: in Section 2, Ben-ford’s law is discussed and its probabilistic framework isdetailed. In Section 3, the three new audio features basedon the evaluation of the degree of agreement of a certaindataset to Bendford’s law are deﬁned. These descriptorsare widely employed in Section 4 for the aforementionedtasks, as part of the audio signals analysis. Finally, in Sec-tion 5, some conclusions are drawn.2. BENFORD’S LAWBenford’s law afﬁrms that the frequency of occurrence ofthe leading signiﬁcative digit of a large dataset comingfrom real-life measurements, presents a peculiar histogramin which the height of the bars follows a logarithmic scale(see Figure 1).\n12345678905101520253035\nLeading digitProbability30.117.612.59.77.96.75.85.14.6Figure 1. The logarithm-shaped distribution of the leadingdigits, following Benford’s law.More speciﬁcally, the probability value of thed-th digitis computed as follows:P(d)=log10✓1+1d◆(1)wheredis the digit number.Simon Newcomb [11]ﬁrst described this peculiar be-havior after the observation of the pages of the tables ofcommon logarithms. He noticed that the logarithms begin-ning with the digit 1 were more frequently browsed thanthe others. In his two-page paper, he brieﬂyd e s c r i b e dt h eempirical evidence of such observation, extending it to allthe digits.735However, his work remained unknown for several years.In 1938, theGeneral Electricphysicist Frank Benford,apparently unaware of Newcomb’s paper, formalized thesame observations with a more consistent article publishedby the American Philosophical Society [4]. He includedthe formalization of the same law and a large amount ofobservations of real-life phenomena gathered during sev-eral years of research.The rigorous mathematical discussion of the law wastackled several years after, and it is currently a matter ofquestion. In 1976, the mathematician Ralph Raimi wroteabout the mathematical explanation of the law, citing the‘scale-invariance’ as one of the possible keys for inter-pretation of the phenomenon [14]. Theodore Hill [7], in1995, described the statistical derivation of the law, whilein 1997, Stephen Smith [15], in his book “The Scientist andEngineer’s Guide to Digital Signal Processing” presentedar i g o r o u sc o m p l e t ed e s c r i p t i o n ,u n d e rt h ep o i n to fv i e wo fthe signal processing.Nowadays, Benford’s law is a well deﬁned probabilisticproblem, and it has been demonstrated that it is based onan intrinsic property of a large number of real-life datasets.According to the central limit theorem [13], the distribu-tion of a certain measure of a quantity follows a normal dis-tribution. The larger the amount of data, the closer theﬁtof the sample histogram to the Gaussian distribution. Nev-ertheless, when a single measure is iteratively repeated, itsvariance tends to be steady and to robustly deﬁne the rangeof variability of the quantity measured. Usually, it limitsthe width of the distribution to few orders of magnitude.In fact, it is infrequent that a series of iterative measures ofthe same variable could span across a wide range of values.Also, if we multiply groups of random numbers, eachfollowing a normal distribution, we will obtain a newdataset following the so called ‘log-normal’ distribution[9]. Its name derives from the dome-shaped histogram thatthis kind of distribution shows, when it is represented on alogarithmic scale. In log-normal distributions, 95% of thevalues are distributed within the meanµminus twice thestandard deviation\u0000and the meanµplus twice the stan-dard deviation\u0000, on the logarithmic scale. This leads to anaccumulation of values on the left edge of the distribution,on the linear scale [15]. Actually, in log-normal distribu-tions the median is lower than the mean and they presentlarge positive values of skewness [9] (see Figure 2).The fact that the log-normal distribution usually de-rives from the combination of normally distributed vari-ables, leads one to assume that, in nature, it is as commonas the normal distribution [15]. Most of the datasets ofreal-life variables are log-normally distributed, especiallythose with only-positive values, where the intrinsic limita-tion leads to an increase of probability around the smallestvalues. Most of these datasets follow Benford’s law. In en-vironmental pollutants datasets, for instance, most of themeasures are typically very low and only few of them arelarger than their mean. Moreover, these variables are typ-ically only positive, but they usually show very low val-ues, very close to zero. This leads to a compression of the024681012141618x 1040100200300400500600700\nxNumber of occurrenceHistogramMedianMean\n(a) The histogram of a log-normal shaped dataset (linear axis).\n3.43.63.844.24.44.64.855.25.4050100150200250300350\nLog10(x)Number of occurrenceHistogramMedianMean\n(b) The histogram of a log-normal shaped dataset (logarithmic axis).Figure 2.A ne x a m p l eo fl o g - n o r m a ls h a p e dd a t a s e ti nl i n -ear and logarithmic axis. The median and the mean arerepresented with continuous and dashed line, respectively.Note that the median and the mean coincide when the his-togram is spaced on the logarithmic axis (the distribution isnormally-shaped). The histogram bins have been equallyspaced on the logarithmic axis, such to deﬁne a constantwidth of the bars.histogram toward the minimum, resulting in a typical log-normal distribution.Nevertheless, the shape of the histogram is not sufﬁcientto be an index of the degree ofﬁt to Benford’s law. Usually,the log-normal distributions derived from the combinationof multiple normal distributions (with different widths) arebroader than them, because of the larger range of variabil-ity they present. In fact, it is the width of these kinds ofdistributions, that is key to understand their relation to Ben-ford’s law. Smith [15] shows how the degree ofﬁtt ot h elaw of a certain dataset is a mere question of distributionwidth. The broader the distribution of the data, the moreaccurate theﬁtt ot h et h e o r e t i c a ll a w .This is a very important issue, related to the data ma-nipulation by humans. The most common way to system-atically extract the leading digit of a number is to multiplyor divide it by ten, until it reaches a value between1and9.9periodic. In particular, the number must be divided by10,i fi ti sh i g h e rt h a n10and multiplied by10,i fi ti sl o w e rthan1.Thus, for instance, the number0.00567will be multi-plied by10three times to obtain the number5.67,w h o s einteger part (5)i st a k e ni n t oa c c o u n ta st h el e a d i n gd i g i t .Similarly, for the number7865,i th a st ob ed i v i d e db y10three times to obtain7.865,a n dt h ec o r r e s p o n d e n tl e a d i n gdigit (7).This ’human-driven’ mechanism is primarily responsi-ble for dependence of the distribution of the leading digits736 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015on the logarithmic law [15]. Hence, the amount of depen-dence, namely the degree ofﬁtt oB e n f o r d ’ sl a w ,d e p e n d son the broadness of the original data distribution. If thedata span across a large number of orders of magnitudes,with respect to unity, they will need several steps of mul-tiplications/divisions to be scaled to range between1and9.9periodic. Conversely, if the dataset ranges from1to9,t h en u m b e r sw i l ln o tr e q u i r ea n yo p e r a t i o n .T h ei m p a c tof these kinds of manipulations is directly related to thedegree of agreement to Benford’s law.3. BENFORD’S LAW BASED AUDIO FEATURESIn order to evaluate the degree of agreement of a certaindataset to Benford’s law, several approaches can be em-ployed. The task is to obtain new features to be used ascomparative measure among the different audio elementsto be classiﬁed. In this section three new features will beextracted: the one-scaling-test, the Fourier-based methodand the goodness-of-ﬁtt e s t .3.1 The one-scaling-testRaimi [14], still speaking about a “universal law”, intro-duced the scale-invariance principle to deﬁne the validityof the law. He afﬁrmed that “...since God is not knownto favor either the metric system or the English system...”,Benford’s law must be scale-invariant. Smith [15] formal-ized a test based on the scale-invariance of the law, by mea-suring the variation of the probability of occurrence of theleading digit1, when the dataset is iteratively multipliedby a constant.The theoretical probability of occurrence, by Benford’slaw, of theﬁrst leading digit is0.301.I ft h ee m p i r i c a lp r o b -ability of the digit1of a certain dataset is close to thisvalue, we can suppose that the dataset follows (potentially)Benford’s law. This is obviously not sufﬁcient. Recallingthe concept about the scale-invariance, we can afﬁrm thatif the dataset follows the law, the empirical probability ofoccurrence of its leading digits (the digit1in this case)should not vary, or vary very weakly, if the dataset is itera-tively multiplied or divided. The so calledone-scaling-testproposed by Smith [15] exploits this property to evaluatethe agreement of a certain dataset to Benford’s law.If we take two log-normally distributed datasets withequal mean,10,a n dd i f f e r e n ts t a n d a r dd e v i a t i o n ,0.5and3, respectively, and we multiply them iteratively by a con-stant (e.g.:1.01), we will observe a certain variation of theprobability of occurrence of theﬁrst leading digit aroundthe value0.301(Figure 3).Ab r o a d e rd i s t r i b u t i o np r e s e n t sam u c hw e a k e rv a r i a -tion of the probability of occurrence of theﬁrst leadingdigit around the value0.301, than a narrower distribution.The means of the distribution of the probability values are0.3010and0.3013,f o rt h eb r o a d e ra n dt h en a r r o w e rd i s -tribution, respectively. That is, they both follow Benford’slaw, showing a value close to the expected theoretical prob-ability (0.301). However, their standard deviations0.0069and0.1450reveal a much larger variation around the mean0501001502002503003504004500.10.20.30.40.5\n# iterationProbability of leading digit1ProbabilityMean\npdf(a) The variability of the probability of occurrence of the leading digit1for a broad log-normally distributed dataset (\u0000=3).\n0501001502002503003504004500.10.20.30.40.5\n# iterationProbability of leading digit1ProbabilityMean\npdf(b) The variability of the probability of occurrence of the leading digit1for a narrow log-normally distributed dataset (\u0000=0.5).Figure 3.A ne x a m p l eo ft h ee f f e c to ft h ew i d t ho ft h e( l o g -normal) distribution on the one-scaling-test. Means arerepresented with thick lighter line. The equivalent PDFsare displayed on the right side of the plots.for the dataset with the narrower distribution. Althoughboth datasets seem to follow Benford’s law, the broaderone required a heavier manipulation of the original datato extract the leading digits and it emphasized the loga-rithmic pattern attributed to their distribution, leading it toapproach the theoretical law closer.Note that in both cases, the variation of the probabilityshows a periodic pattern due to the factor chosen for themultiplication. The leading digit is unchanged when thenumbers are multiplied by10.I no u re x a m p l e ,t h i so c c u r severy232times (1.01232⇡10).The one-scaling-test presents a main drawback relatedto the high computational cost derived from the iterativemultiplication of the whole dataset. If we consider onesingle minute of an audio signal recorded at a samplingfrequency of44.1kHz, we have to handle with a vector ofmore than2.5millions samples. If we want to multiply thisdataset at least232+1times (to observe at least one wholeperiod), we must do more than600millions of operations.In the case of exploiting Benford’s law in a classiﬁer toolfor music genres, we should have to handle hundreds ofsongs, each of them with a length of several minutes. Thiswould become an unfeasible task from the point of view ofthe computational cost.3.2 The Fourier transform-based methodSmith [15] reinterprets the problem from the point of viewof signal processing. He proves that the degree of agree-ment of a certain dataset to Benford’s law, can be estimatedby evaluating the behavior of the Fourier transform (FT) ofthe normalized histogram in logarithmic axes. In particu-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 737lar, the measure of how fast the transform falls, from itsmaximum value (1at frequency0) to its minimum value(zero at some frequency higher than zero), is directly re-lated to the width of the distribution measured with thenormalized histogram and, consequently, with the degreeof correspondence with the law.Ideally, in order to follow perfectly Benford’s law, theFourier transform should present a unitary value at fre-quency zero and a zero value at all the remaining frequen-cies. This would occur if the distribution was uniform from\u00001to+1[12].In real-life, this does not occur. Hence, the faster theFourier transform drops to zero, the closer the agreementof the dataset to Benford’s law. In particular, Smith de-ﬁnes the value at frequency1as the threshold to discrim-inate between the agreement or not of the dataset to thelaw [15]. If the transform of the histogram in logarithmicaxes (denoted as PDF) falls to zero before frequency1Hz,the correspondent dataset follows Benford’s law. If it doesnot occur, the dataset does not follow the law. In practice,the value of the PDF atf=1Hzis a reliable index of thedegree of agreement with the law.In Figure 4, an example of the application of the Fouriertransform to the histograms of the dataset tested in the pre-vious section, is shown.\n−20246810012345x 10−3\nLog10 (x)Number of occurrence0123400.20.40.60.81\nFrequency (Hz)Amplitude(a) Broad log-normally distributed dataset. Left: distribution on a loga-rithmic axis. Right: Fourier transform of the distribution (PDF).\n−20246810012345x 10−3\nLog10 (x)Number of occurrence0123400.20.40.60.81\nFrequency (Hz)Amplitude(b) Narrow log-normally distributed dataset. Left: distribution on a loga-rithmic axis. Right: Fourier transform of the distribution (PDF).Figure 4.E x a m p l eo ft h ea p p l i c a t i o no ft h eF o u r i e rt r a n s -form for the estimation of the agreement of the data to Ben-ford’s law. The transform of the broader distribution dropsto zero faster than the narrower one, revealing a closer cor-relation with the law.The distribution of the data shown in Figure 4(a) is broa-der than the one in Figure 4(b). Actually the two datasetsare the same that were previously analyzed in Figure 3,with standard deviation3and0.5,r e s p e c t i v e l y . T h eP D Fof the broader distribution falls to zero much faster thanthe narrower one. In particular, the amplitude of the PDFat frequency1Hzis0.0023and0.4184,f o rt h eb r o a d e rand the narrower distribution, respectively. This issue re-veals a closer agreement to Benford’s law of the broaderdistribution, as observed previously.Note that unlike the one-scaling-test, the method basedon the Fourier transform has a reasonable computationalcost. Furthermore, this method returns a higher discrim-inant range for the two datasets: The ratio between thetwo standard deviations of the one-scaling-test is about20,while the ratio between the two values of the transforms atfrequency1Hzis about180.I ft h ea i mo ft h ea p p l i c a t i o nof Benford’s law is a boolean discrimination of the data,then the Fourier transform-based method is efﬁcient.3.3 The\u00002divergence and the goodness-of-ﬁtt e s tAn alternative to the the two empirical methods proposedso far, is the well known\u00002test [9]. It is called thegoodness-of-ﬁttest and it returns a measure of how wellan empirical distributionﬁts a theoretical one.The divergence is calculated as follows:D=(f(d)\u0000P(d))2P(d)(2)wheref(d)is the empirical relative frequency of the digitdandP(d)stands for its theoretical probability deﬁned, inour case, by Benford’s law, detailed in equation (1).The null hypothesisH0is veriﬁed if its associated prob-ability (thep-value) does not exceed the signiﬁcance levelﬁxed a priori. This probability value, when the test ispassed, can be employed as additional information for themeasure of the agreement to Benford’s law.The goodness-of-ﬁtt e s ta p p l i e dt ot h et w od a t a s e t sa n -alyzed before, returns a divergence value of0.2484and0.0009,f o rt h en a r r o w e ra n dt h eb r o a d e rd i s t r i b u t i o n ,r e -spectively. Actually, the narrower distributed dataset didnot pass the test. In Figure 5, the two empirical distribu-tions of the leading digits are shown.\n12345678900.050.10.150.20.250.30.350.4\nLeading digitProbabilityTheoretical probability (Benford)Broad datasetNarrow dataset\nFigure 5. The distribution of the leading digits for the twodatasets of the previous example. Both empirical distribu-tions are compared against the theoretical values.Once again, the broader distribution reveals a larger cor-relation with Benford’s law, than the narrower one. Notethat the ratio between the two divergences (about280)i seven larger than the one measured between the two valuesof the transform in the previous example.Nevertheless, the approach based on the Fourier trans-form does not need to extract the samples in the dataset andit is, therefore, more efﬁcient.738 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20154. EVALUATION OF BENFORD’S LAWPERFORMANCE IN PATTERN RECOGNITIONTASKS FOR MUSIC SIGNALSIn this section, the performance of the proposed featuresbased on quantitative measurements of agreement to Ben-ford’s law is evaluated in two concrete audio tasks.4.1 Real and artiﬁcially assembled chordsOften, the methods employed in the evaluation of the al-gorithms for multi-pitch estimation are based on the usageof ground-truth datasets of artiﬁcially assembled chords,i.e. made up by the summation of individual waveforms ofthe single notes that compose the chords. In this context,this procedure leads to cleaner spectra that can be moreeasily analysed. Benford’s law based audio features areemployed to discriminate between real and artiﬁcially as-sembled chords. A set of230chords has been examined.The half of them are real chord [3] and the other half are thesame chords but artiﬁcially assembled adding single notes.The two sets of chords do not reveal any kind of signiﬁ-cant difference when submitted to a perceptual evaluation.They sound practically the same.Using this data set, the descriptors to evaluate the agree-ment of the data to Benford’s law have been calculatedfor each pair of chords (real and artiﬁcially assembled).The ones-scaling test has not been performed because ofits high computational costs. In order to evaluate the per-formance of the new Bendford’s law based features, theyhave been compared against a set of time and frequencyfeatures commonly used for the music classiﬁcation task(RMS, ZCR, CER, SPF) [16].The descriptors deﬁned in this context reﬂect a notablediscrepancy between the two classes of chords. Surpris-ingly, an average value of the30%of the samples (12outof115for the artiﬁcial chords and56out of115of thereal chords) did not pass the\u00002test. The signals showedrather skewed distributions on the logarithmic axis with theconsequent decrease of the level of agreement to Benford’slaw.Aknnclassiﬁer has been adopted here to perform theclassiﬁcation of the chords using both the set of singlefeatures selected and the two groups of features with andwithout the two Benford’s law-based descriptors. As itis shown in Table 1, Benford’s law-based features behaverather well when compared against the typical features foraudio classiﬁcation. Also, the multidimensional set of de-scriptors improves its performance with the inclusion ofthe two Benford’s law-based features. It is interesting tonote that the artiﬁcial chords returned smaller values ofPDF(f=1Hz)than the real chords (see Figure 6).4.2 Quality of MIDI conversionRecalling the ‘Nature-dependence’ of Benford’s law, weformulate the hypothesis that the agreement of MIDI [10]audio to Benford’s law, could be used as a ranking measurefor the quality of automatic MIDI converters. Two soft-ware tools for automatic MIDI conversion were tested: theFeature Success rate (%)Benford’s law-based featuresPDF(f=1Hz)80.87\u00002divergence 71.74Time and Frequency featuresRoot mean square 82.61Zero crossing rate 68.70Cepstrum residuals 58.26Spectralﬂux 72.61Grouped-feature setTime and Frequency features set 79.57Benford’s law-based features added 82.17Table 1.R e a l a n d a r t iﬁcial chord classiﬁcation accuracyof the single-feature tests and the grouped-feature tests.\nArtificialReal0.020.040.060.080.10.120.140.16Fourier transform amplitude at f=1Chords1.5 x interquartile rangeInterquartile rangeMedianOutliers\nFigure 6.B o x - w h i s k e r s p l o t o f t h e a m p l i t u d e o fPDF(f=1Hz),f o ra r t iﬁcial and real chords. The non-overlapping notches, indicating the95%conﬁdence inter-val of the two medians, reveal good discrimination powerof the analyized feature.freeware software AMAZING MIDI (v1.70) by arakisoft-ware [2] and the shareware software AKoff Music Com-poser (v2.0) by the AKoff Sound Labs [1]. Both of thempresent at least two different conﬁguration sets. In particu-lar, the AKoff software has been run with and without theapplication of the ‘overtonesﬁltering’, a utility toﬁlter thehighest harmonics of the spectrum, while the AMAZINGMIDI software has been executed with and without a timeand an amplitudeﬁlter (to reduce the range of amplitudeand note duration).The term “quality of a MIDI conversion” is a rather sub-jective concept, i.e., it may depend on the person who isevaluating that quality. Therefore, the sounds of the au-tomatic conversion tools have been listened carefully by ateam of ten expert musicians who have evaluated person-ally both the similarity between the converted track andthe original one, and the overall quality of the MIDI audio.Each listener had to rank the MIDI converters with a scorein the range0(the worst quality) to100(the best quality).Table 2 shows the mean of the subjective test scores ob-tained by each tool/conﬁguration.In Figure 7, an example of the test performed, appliedto the song ‘Come sei veramente’ by the pianist G. Allevi,is shown. The original track returned the smallest value inthe ones-scaling test,PDF(f=1Hz)and the\u00002diver-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 739Software/conﬁguration Mean scoreAKoff with overtone control 27/100AKoff without overtone control 48/100AmazingMIDI withﬁlters 75/100AmazingMIDI withoutﬁlters 80/100Table 2.M e a ns u b j e c t i v er a n k so ft h ef o u rc o m b i n a t i o no ftool and conﬁguration employed in the MIDI-quality test.gence, with respect to the other four MIDI versions. Thetwo outcomes of the AKoff software returned the largestvalues of each descriptor, revealing the lowest accordanceto Benford’s law. Note the relation between features ex-tracted and the subjective ranks in Table 2. Therefore, theaccordance to Benford’s law provide us with a measure ofthe quality of the MIDI converters.\n00.010.020.030.040.050.060.070.08w/ filtersAmazingMIDIAmazingMIDIw/o filtersovertone controlAKoff w/oAKoff w/overtone controlOriginal track\nAmplitude/Divergence/Standard deviationPDF (f=1)χ2 divergence (*10)Ones−scaling test\nFigure 7.T h e t h r e e B e n f o r d ’ s l a w - b a s e d f e a t u r e s c a l c u -lated for the track ‘Come sei veramente’ by the Italian pi-anist Giovanni Allevi. Divergence values are multiplied by10 for displaying purposes.5. CONCLUSIONSIn this paper, it has been shown how Benford’s law can beconveniently exploited to extract useful features that canbe successfully used in different audio pattern recognitiontasks. Three new Benford’s law based audio features basedon different measurement of the agreement to Benford’slaw have been proposed.Two concrete tasks have been addressed to highlightthis novel context of application of Bendford’s law for au-dio signal. For chord analysis, the new proposed featuresare rather compelling as good discriminators when com-pared against other typical features for speech and audioclassiﬁcation and also the results obtained for the determi-nation of the quality of the automatic MIDI conversionsare promising.Therefore, through this paper it has been illustrated howBenford’s law, that substantially arises as a matter of shapeand width of the distribution of the leading digits of thedata, can be conveniently exploited for audio classiﬁcationproblems.6. ACKNOWLEDGEMENTThis work has been funded by the Ministerio de Econom´ıayC o m p e t i t i v i d a do ft h eS p a n i s hG o v e r n m e n tu n d e rP r o j e c tNo. TIN2013-47276-C6-2-R and Project No. TIN2013-47276-C6-1-R. This work has been partially done at Uni-versidad de M´alaga, Campus de Excelencia Internacional(CEI) Andaluc´ıaT e c h .7. REFERENCES[1] AKoff-Sound-Labs. AKoof music composer, wav-to-midi converter.http://www.akoff.com/music-composer.html,2 0 1 1 .[2] Arakisoftware. AmazingMIDI, wav to midi converterfor music transcription.http://www.pluto.dti.ne.jp/˜araki/amazingmidi,2 0 1 1 .[3] Ana M. Barbancho, Isabel Barbancho, Lorenzo J.Tard´on, and Emilio Molina.Database of PianoChords. An Engineering View of Harmony.S p r i n g e r -Verlag, New York, NY, USA, 2013.[4] Frank Benford. The law of anomalous numbers.Proceedings of the American Philosophical Society,78(4):551–572, 1938.[5] Joseph Deckert, Mikhail Myagkov, and Peter C. Or-deshook. The irrelevance of benfords law for detect-ing fraud in elections.Caltech/MIT Voting TechnologyProject,1 ( 9 ) ,2 0 1 0 .[6] Esteve del Acebo and Mateu Sbert. Benford’s lawfor natural and synthetic images. InComputationalAesthetics,p a g e s1 6 9 – 1 7 6 .E u r o g r a p h i c sA s s o c i a t i o n ,2005.[7] Theodore P. Hill. A statistical derivation of thesigniﬁcant-digit law.Statistical Science,1 0 ( 4 ) : 3 5 4 –363, 1995.[8] Brian A. Jacob and Steven D. Levitt. Rotten apples:An investigation of the prevalence and predictors ofteacher cheating.The Quarterly Journal of Economics,118(3):843–877, 2003.[9] Averill M. Law and W. David Kelton.Simulation Mod-eling and Analysis. McGraw Hill, Inc., 1991.[10] Robert A. Moog. Midi: Musical instrument digitalinterface.Journal of the Audio Engineering Society,34(5):394–404, 1986.[11] Simon Newcomb. Note on the frequency of use of thedifferent digits in natural numbers.American Journalof Mathematics,4 ( 1 ) : 3 9 – 4 0 ,1 8 8 1 .[12] Alan V . Oppenheim and Ronald W. Schafer.Discrete-Time Signal Processing, 3/E.P r e n t i c eH a l l ,2 0 1 0 .[13] Athanasios Papoulis and S. Unnikrishna Pillai.Prob-ability, Random Variables and Stochastic Processes.McGraw Hill, 2002.740 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[14] Ralph A. Raimi. Theﬁrst digit problem.The AmericanMathematical Monthly,8 3 ( 7 ) : 5 2 1 – 5 3 8 ,1 9 7 6 .[15] Steven W. Smith.The scientist and engineer’s guideto digital signal processing.C a l i f o r n i aT e c h n i c a lP u b -lishing, San Diego, CA, USA, 1997.[16] Lorenzo J. Tard´on, Simone Sammartino, and IsabelBarbancho. Design of an efﬁcient music-speech dis-criminator.The Journal of the Acoustical Society ofAmerica,1 2 7 : 2 7 1 – 2 7 9 ,2 0 1 0 .Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 741"
    },
    {
        "title": "An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.",
        "author": [
            "Emmanouil Benetos",
            "Tillman Weyde"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418017",
        "url": "https://doi.org/10.5281/zenodo.1418017",
        "ee": "https://zenodo.org/records/1418017/files/BenetosW15.pdf",
        "abstract": "In this paper, an efficient, general-purpose model for multi- ple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component anal- ysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computa- tional efficiency is achieved by supporting the use of pre- extracted and pre-shifted sound state templates. Two vari- ants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the ap- pearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models out- perform the state-of-the-art for multiple-instrument tran- scription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch de- tection performance compared with constant-Q represen- tations.",
        "zenodo_id": 1418017,
        "dblp_key": "conf/ismir/BenetosW15",
        "keywords": [
            "efficient",
            "general-purpose",
            "multiple instrument",
            "polyphonic music transcription",
            "probabilistic latent component analysis",
            "sound state spectral templates",
            "variable-Q transform",
            "pre-extracted and pre-shifted sound state templates",
            "hidden Markov model-based constraints",
            "benchmark transcription datasets"
        ],
        "content": "AN EFFICIENT TEMPORALLY-CONSTRAINED PROBABILISTICMODEL FOR MULTIPLE-INSTRUMENT MUSIC TRANSCRIPTIONEmmanouil BenetosCentre for Digital MusicQueen Mary University of Londonemmanouil.benetos@qmul.ac.ukTillman WeydeDepartment of Computer ScienceCity University Londont.e.weyde@city.ac.ukABSTRACTIn this paper, an efﬁcient, general-purpose model for multi-ple instrument polyphonic music transcription is proposed.The model is based on probabilistic latent component anal-ysis and supports the use ofsound statespectral templates,which represent the temporal evolution of each note (e.g.attack, sustain, decay). As input, a variable-Q transform(VQT) time-frequency representation is used. Computa-tional efﬁciency is achieved by supporting the use of pre-extracted and pre-shifted sound state templates. Two vari-ants are presented: without temporal constraints and withhidden Markov model-based constraints controlling the ap-pearance of sound states. Experiments are performed onbenchmark transcription datasets: MAPS, TRIOS, MIREXmultiF0, and Bach10; results on multi-pitch detection andinstrument assignment show that the proposed models out-perform the state-of-the-art for multiple-instrument tran-scription and is more than 20 times faster compared to aprevious sound state-based model. We ﬁnally show that aVQT representation can lead to improved multi-pitch de-tection performance compared with constant-Q represen-tations.1. INTRODUCTIONAutomatic music transcription is deﬁned as the process ofconverting an acoustic music signal into some form of mu-sical notation [16] and is considered a fundamental prob-lem in the ﬁelds of music information retrieval and mu-sic signal processing. The core problem of automatic mu-sic transcription is multi-pitch detection (i.e. the detectionof multiple concurrent pitches), which despite recent ad-vances is still considered an open problem, especially for alarge polyphony level and multiple instruments.Al a r g es u b s e to fm u s i ct r a n s c r i p t i o na p p r o a c h e su s espectrogram factorizationmethods such as non-negativematrix factorization (NMF) and probabilistic latent com-ponent analysis (PLCA), which decompose an input time-frequency representation into a series of note templatesc\u0000Emmanouil Benetos, Tillman Weyde.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Emmanouil Benetos, Tillman Weyde.“An efﬁcient temporally-constrained probabilistic model for multiple-instrument music transcription”, 16th International Societyf o rM u s i cI n -formation Retrieval Conference, 2015.and note activations. Several variants of the above meth-ods propose more complex formulations compared to theoriginal NMF/PLCA models, and also add musically- andacoustically-meaningful constraints. Such spectrogram fac-torization methods include amongst others [4,8,10,13,15,18, 24]. Issues related to spectrogram factorization meth-ods include: the choice of an input time-frequency rep-resentation, the ability to recognize instruments, the sup-port of tunings beyond twelve-tone equal temperament, thepresence or absence of a pre-extracted dictionary, the in-corporation of any constraints, as well as computationalefﬁciency (given ever-expanding collections and archivesof music recordings).In this paper, a model for multiple-instrument transcrip-tion is proposed, which uses a 5-dimensional dictionary ofsound statespectral templates (sound states correspond tothe various states in the evolution of a note, such as theattack, sustain, and decay states). The proposed model isbased on PLCA and decomposes an input time frequencyrepresentation (in this case, a variable-Q transform spec-trogram) into a series of probability distributions for pitch,instrument, tuning, and sound state activations. This modelis inspired by a convolutive model presented in [4] thatused a 4-dimensional dictionary and was able to transcribear e c o r d i n ga t6 0⇥real-time. This model uses pre-shiftedspectral templates across log-frequency, thus introducinganew dimension in the dictionary and eliminating the needfor convolutions. Thus, tuning deviations from equal tem-perament are supported and at the same time this modelonly uses linear operations that result in a system that ismore than 20 times faster compared to the system of [4].In addition, temporal constraints using pitch-wise hiddenMarkov models (HMMs) are incorporated, in order to modelthe evolution of a note as a sequence of sound states. Ex-periments are performed on several transcription datasets(MAPS, MIREX multiF0, Bach10, TRIOS) and experi-mental results for the multi-instrument datasets using theproposed system outperform the state-of-the-art. Finally,we show that a VQT representation leads to an improve-ment in transcription performance compared to the morecommon constant-Q transform (CQT) representation, es-pecially on the detection of lower pitches. Code for theproposed model is also supplied (cf. Section 4).The outline of this paper is as follows. The proposedsystem is presented in Section 2. The employed trainingand test datasets, evaluation metrics, and experimental re-701AUDIOVQTMODEL POSTPROCESSINGMIDI\nTEMPLATESFigure 1.D i a g r a mf o rt h ep r o p o s e ds y s t e m .sults are shown in Section 3. Finally, a discussion on theproposed system followed by future directions is made inSection 4.2. PROPOSED SYSTEM2.1 MotivationThe overall aim of the proposed work is the creation ofas y s t e mf o ra u t o m a t i ct r a n s c r i p t i o no fp o l y p h o n i cm u -sic, that supports the identiﬁcation of instruments alongwith multiple pitches, supports tunings beyond twelve-toneequal temperament along with frequency modulations, isable to model the evolution of each note (as a temporalsuccession ofsound states), and is ﬁnally computation-ally efﬁcient. The proposed system is based on work car-ried out in [4], which relied on a convolutive PLCA-basedmodel and a 4-dimensional sound state dictionary. Theaforementioned model was able to transcribe recordings atapproximately 60⇥real-time (i.e. for a 1min recording,transcription took 60min). This paper proposes an alterna-tive linear model able to overcome the computational bot-tleneck of using a convolutive model, which is supportedby the use of a 5-dimensional dictionary of pre-extractedand pre-shifted sound state spectral templates, at the sametime providing the same beneﬁts with the model of [4]. Fi-nally, this paper proposes the use of a variable-Q transform(VQT) representation, in contrast with the more commonconstant-Q transform (CQT) or linear frequency represen-tations (a detailed comparison is made in Section 3). Onrelated work, a linear model that used a 4-dimensional dic-tionary which did not support sound state templates or tem-poral constraints was proposed in [3].In Fig. 1, a diagram for the proposed system can beseen. As motivation on the use of sound state templates,two log-frequency representations for a G1 piano note areshown in Fig. 2; it is clear that the note evolves froman attack/transient state to a steady state, and ﬁnally to adecay state. Fig. 3 shows 3 spectral templates extractedfor the same note, which correspond to the 3 sound states(the lower corresponds to the attack state, the middle to thesteady state and the top to the decay state).2.2 PLCA-based modelThe ﬁrst variant of the proposed system takes as input anormalised log-frequency spectrogramV!,t(!is the log-frequency index andtis the time index) and approximatesit as a bivariate probability distributionP(!, t).I n t h i swork,V!,tis a variable-Q time-frequency representationwith a resolution of 60 bins/octave and minimum frequency\n(b)\nt!(a)!\n50 100 150 200 250 30050 100 150 200 250 300\n100200300400500100200300400500\nFigure 2.( a ) T h e C Q T s p e c t r o g r a m o f a G 1 p i a n o n o t e .(b) The VQT spectrogram for the same note.sound state index (q)\n!01 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0123\nFigure 3.S o u n d s t a t e s p e c t r a l t e m p l a t e s f o r a G 1 p i a n onote (extracted using a VQT representation).of 27.5Hz, computed using the method of [22]. As dis-cussed in [22], a variable-Q representation offers increasedtemporal resolution in lower frequencies compared with aconstant-Q representation. At the same time, a log-frequencytransform represents pitch in a linear scale (where inter-harmonic spacings are constant for all pitches), thus allow-ing for pitch changes to be represented by shifts across thelog-frequency axis.In the model,P(!, t)is decomposed into a series oflog-frequency spectral templates per sound state, pitch, in-strument, and log-frequency shifting (which indicates de-viation with respect to equally tempered tuning), as well asprobability distributions for sound state, pitch, instrument,and tuning activations. As explained in [4], a sound staterepresents different segments in the temporal evolution ofan o t e ;e . g . f o rap i a n o ,d i f f e r e n ts o u n ds t a t e sc a nc o r r e -spond to the attack, sustain, and decay.702 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015The model is formulated as:P(!, t)=P(t)Xq,p,f,sP(!|q, p, f, s)Pt(f|p)Pt(s|p)Pt(p)Pt(q|p)(1)whereqdenotes the sound state,pdenotes pitch,sde-notes instrument source, andfdenotes log-frequency shift-ing.P(t)is the energy of the log-spectrogram, which is aknown quantity.P(!|q, p, f, s)is a 5-dimensional tensorthat represents the pre-extracted log-spectral templatespersound stateq,p i t c hpand instruments,w h i c ha r ea l s op r e -shifted across log-frequencyf.T h ep r o p o s e dp r e - s h i f t i n goperation is made in order to account for pitch deviations,without needing to formulate a convolutive model acrosslog-frequency, as in [4].Pt(f|p)is the time-varying log-frequency shifting distribution per pitch,Pt(s|p)is the in-strument source contribution per pitch over time,Pt(q|p)is the time-varying sound state activation per pitch, and ﬁ-nallyPt(p)is the pitch activation, which is essentially theresulting multi-pitch detection output.In the proposed model,f2[1,...,5],w h e r ef=3isthe ideal tuning position for the template (using equal tem-perament). Given that the input time-frequency represen-tation has a resolution of 5 bins per semitone, this meansthat all templates are pre-shifted across log-frequency ona±20and±40cent range around the ideal tuning position,thus accounting for small tuning deviations or frequencymodulations. The proposed model also uses 3 sound statesper pitch; more information on the extraction of the soundstate spectral templates is given in subsection 3.1.The unknown model parameters (Pt(f|p),Pt(s|p),Pt(p),Pt(q|p))c a nb ei t e r a t i v e l ye s t i m a t e du s i n gt h ee x p e c t a t i o n -maximization (EM) algorithm [9]. For theExpectationstep, the following posterior is computed:Pt(q, p, f, s|!)=P(!|q, p, f, s)Pt(f|p)Pt(s|p)Pt(p)Pt(q|p)Pq,p,f,sP(!|q, p, f, s)Pt(f|p)Pt(s|p)Pt(p)Pt(q|p)(2)For theMaximizationstep, unknown model parametersare updated using the posterior from (2):Pt(f|p)=P!,s,qPt(q, p, f, s|!)V!,tPf,!,s,qPt(q, p, f, s|!)V!,t(3)Pt(s|p)=P!,f,qPt(q, p, f, s|!)V!,tPs,!,f,qPt(q, p, f, s|!)V!,t(4)Pt(p)=P!,f,s,qPt(q, p, f, s|!)V!,tPp,!,f,s,qPt(q, p, f, s|!)V!,t(5)Pt(q|p)=P!,f,sPt(q, p, f, s|!)V!,tPq,!,f,sPt(q, p, f, s|!)V!,t(6)Eqs. (2)-(6) are iterated until convergence; typically 15-20 iterations are sufﬁcient. No update rule for the soundstate templatesP(!|q, p, f, s)is included, since they areconsidered ﬁxed in the model. As in [4], we also incor-porated sparsity constraints onPt(p)andPt(s|p)in orderto control the polyphony level and the instrument contribu-tion in the resulting transcription. The resulting multi-pitchdetection output is given byP(p, t)=P(t)Pt(p),w h i l eatime-pitch representationP(f0,t)can also be derived fromthe model, as in [4] (this representation has the same pitchresolution as in the input representation, i.e. 20 cent reso-lution).2.3 Temporally-constrained modelThis model variant proposes a formulation that expressesthe evolution of each note as a succession of sound states,following work carried out in [4]. These temporal con-straints are modelled using pitch-wise hidden Markov mod-els (HMMs). This also follows the work done by Mysorein [17] on the non-negative HMM (a spectrogram factor-ization framework where the appearance of each templateis controlled by an HMM).As discussed, one HMM is created per pitchp,w h i c hhas as hidden states the sound statesq(assuming 88 pitchesthat cover the entire note range of a piano, 88 HMMs areused). Thus, the basic elements of this pitch-wise HMMare: the sound state priorsP(q(p)1),t h es o u n ds t a t et r a n s i -tionsP(q(p)t+1|q(p)t),a n dt h eo b s e r v a t i o n sP(¯!t|q(p)t).F o l -lowing the notation of [17],¯!corresponds to the sequenceof observed spectra from all time frames, and¯!tis the ob-served spectrum at thet-th time frame. Also,q(p)tis thevalue of the hidden sound state at thet-th frame for pitchp.In this paper, the model formulation is the same as in(1), where the following assumption is made:Pt(q|p=i)=Pt(q(p=i)t|¯!)(7)which means that the sound state activations are assumedto be produced by the posteriors (also calledresponsibili-ties)o ft h eH M Mf o rp i t c hp.F o l l o w i n g[ 1 7 ] ,t h eo b s e r v a -tion probability is calculated as:P(¯!t|q(p)t)=Y!tP(!t|q(p)t)V!,t(8)whereP(!t|q(p)t)is the approximated spectrum for a givensound state and pitch. The observation probability is cal-culated as above since in PLCA-based models,V!,trep-resents the number of times!has been drawn at thet-thtime frame [17].In order to estimate the unknown parameters of this pro-posed temporally-constrained model, the EM algorithm isalso used, which results in a series of iterative update rulesthat combine PLCA-based updates as well as the HMMforward-backward algorithm [20]. For the Expectation step,the HMM posterior per pitch is computed as:Pt(q(p)t|¯!)=Pt(¯!, q(p)t)Pq(p)tPt(¯!, q(p)t)=↵t(q(p)t)\u0000t(q(p)t)Pq(p)t↵t(q(p)t)\u0000t(q(p)t)(9)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 703where↵t(q(p)t)and\u0000t(q(p)t)are the forward and backwardvariables for thep-th HMM, respectively, and can be com-puted using the forward-backward algorithm [20]. Theposterior for the transition probabilitiesPt(q(p)t+1,q(p)t|¯!)isalso computed as in [4]. Finally, the model posterior iscomputed using (2) and (7).For the Maximization step, unknown parametersPt(f|p),Pt(s|p),a n dPt(p)are computed using eqs. (3)-(5). Fi-nally, the sound state priors and transitions per pitchpareestimated as:P(q(p)1)=P1(q(p)1|¯!)(10)P(q(p)t+1|q(p)t)=PtPt(q(p)t,q(p)t+1|¯!)Pq(p)t+1PtPt(q(p)t,q(p)t+1|¯!)(11)In our experiments, it was found that an initial estimationof the pitch and source activations using the PLCA-onlyupdates in the Maximization step leads to a good initialsolution. In the ﬁnal iterations (set to 3 in this case), theHMM parameters are estimated as well, which leads to anestimate of the sound state activations, and an improved so-lution over the non-temporally constrained model of sub-section 2.2.2.4 Post-processingFor both the non-temporally constrained model of subsec-tion 2.2 and the temporally-constrained model of subsec-tion 2.3, the resulting pitch activationP(p, t)=P(t)Pt(p)(which is used for multi-pitch detection evaluation) as wellas the pitch activation for a speciﬁc instrumentP(s, p, t)=P(t)Pt(p)Pt(s|p)(which is used for instrument assign-ment evaluation) need to be converted into a binary repre-sentation such as a piano-roll or a MIDI ﬁle. As in the vastmajority of spectrogram factorization-based music transcrip-tion systems (e.g. [10, 15]), thresholding is performed onthe pitch and instrument activations, followed by a processfor removing note events with a duration less than 80ms.3. EVALUATION3.1 Training dataSound state templates are extracted for several orchestralinstruments, using isolated note samples from the RWCdatabase [14]. Speciﬁcally, templates are extracted for bas-soon, cello, clarinet, ﬂute, guitar, harpsichord, oboe, pi-ano, alto sax, and violin, using the variable-Q transform asat i m e - f r e q u e n c yr e p r e s e n t a t i o n[ 2 2 ] . T h ec o m p l e t en o t erange of the instruments (given available data) is used. Thesound state templates are computed in an unsupervised man-ner, using a single-pitch and single-instrument variant ofthe model of (1), with the number of sound states set to 3.3.2 Test dataSeveral benchmark and freely available transcription datasetsare used for evaluation (all of them contain pitch groundtruth). Firstly, thirty piano segments of 30s duration areused from the MAPS database using the ‘ENSTDkCl’ pi-ano model. This test dataset has in the past been used forSystemFPR§2.270.08%76.78%65.27%§2.371.56%77.95%66.89%Table 1.M u l t i - p i t c h d e t e c t i o n r e s u l t s f o r t h e M A P S -ENSTDkCl dataset using the proposed models.multi-pitch evaluation (e.g. [7,18], the latter also citingr e -sults using the method of [24]).The second dataset consists of the woodwind quintetrecording from the MIREX 2007 multiF0 development data-set [1]. The multi-track recording has been evaluated in thepast either in its complete duration [4], or in shorter seg-ments (e.g. [19, 24]).Thirdly, we employ the Bach10 dataset [11], a multi-track collection of multiple-instrument polyphonic music,suitable for both multi-pitch detection and instrument as-signment experiments. It consists of ten recordings of J.S.Bach chorales, performed by violin, clarinet, saxophone,and bassoon.Finally, the TRIOS dataset [12] is also used, which in-cludes ﬁve multi-track recordings of trio pieces of classi-cal and jazz music. Instruments included in the dataset are:bassoon, cello, clarinet, horn, piano, saxophone, trumpet,viola, and violin.3.3 MetricsFor assessing the performance of the proposed system interms of multi-pitch detection we utilise the onset-basedmetric used in the MIREX note tracking evaluations [1]. Anote event is assumed to be correct if its pitch correspondsto the ground truth pitch and its onset is within a±50 msrange of t ground truth onset. Using the above rule, pre-cision (P), recall (R), and F-measure (F)m e t r i c sc a nb edeﬁned:P=NtpNsys,R=NtpNref,F=2·R·PR+P(12)whereNtpis the number of correctly detected pitches,Nsysis the number of detected pitches, andNrefis the numberof ground-truth pitches. For comparison with other state-of-the-art methods, we also use frame-based multiple-F0estimation metrics, deﬁned in [2], denoted asPf,Rf,Ff.For the instrument assignment evaluations with the Bach-10 dataset, we use the pitch ground-truth of each instru-ment and compare it with the instrument-speciﬁc output ofthe system. As for the multi-pitch metrics, we deﬁne thefollowing note-based instrument assignment metrics:Fv,Fc,Fs,Fb,c o r r e s p o n d i n gt ov i o l i n ,c l a r i n e t ,s a x o p h o n e ,and bassoon, respectively. We also use a mean instrumentassignment metric, denoted asFins.3.4 ResultsExperiments are performed using the two proposed modelvariants from Section 2: the non-temporally constrainedversion of subsection 2.2 and the HMM-constrained ver-sion of subsection 2.3. In both versions, the post-processing704 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015SystemFPR§2.271.75%68.78%74.98%§2.372.50%73.31%71.71%Table 2.M u l t i - p i t c hd e t e c t i o nr e s u l t sf o rt h eM I R E Xm u l -tiF0 recording using the proposed models.SystemFPR§2.264.43%56.99%74.16%§2.365.01%57.35%75.11%Table 3.M u l t i - p i t c h d e t e c t i o n r e s u l t s f o r t h e B a c h 1 0dataset using the proposed models.steps are the same. For the HMM-constrained model, theHMMs are initialized as ergodic, with uniform priors andstate transition probabilities.In terms of multi-pitch detection evaluation, results forthe MAPS, MIREX, Bach10, and TRIOS datasets are shownin Tables 1, 2, 3, and 4, respectively. In all cases, theHMM-constrained model outperforms the non-temporallyconstrained model. The difference over the two modelsin terms of F-measure is more prominent for the MAPSdataset (1.48%) and the TRIOS dataset (1.81%) comparedto the MIREX (0.75%) and Bach10 (0.58%) datasets. Thiscan be attributed to the presence of piano in the MAPS andTRIOS datasets, compared to the woodwind/string instru-ments present in the other two datasets; since the pianois a pitched percussive instrument with a clear attack andtransient state, the incorporation of temporal constraintso nsound state evolution can be considered more importantcompared to bowed string and woodwind instruments, thatdo not exhibit a clear decay state. As an example of thetranscription performance of the proposed system, Fig. 4shows the resulting pitch activation for the MIREX mul-tiF0 recording along with the corresponding ground truth.Instrument assignment results for the Bach10 datasetare presented in Table 5. As can be seen, the performanceof the proposed system regarding instrument assignment ismuch lower compared to multi-pitch detection, which thiscan be attributed to the fact that instrument assignment is amuch more challenging problem, since it not only requiresac o r r e c ti d e n t i ﬁ c a t i o no fan o t e ,b u ta l s oac o r r e c tc l a s -siﬁcation of that detected note to a speciﬁc instrument. Itis worth noting however that a clear improvement is re-ported when using the temporally-constrained model overthe model of subsection 2.2. That improvement is consis-tent across all instruments.3.4.1 Comparison with state-of-the-artOn comparison of the proposed system with other state-of-the art multi-pitch detection methods, for MAPS the pro-posed HMM-constrained method outperforms the spectro-gram factorization transcription methods of [18] and [24]by 13.2% and 2.5% in terms ofF,r e s p e c t i v e l y .I ti sh o w -ever outperformed by the transcription system of [7] (4.9%difference); it should be noted that the system of [7] isSystemFPR§2.257.55%64.60%54.04%§2.359.36%60.18%59.45%Table 4.M u l t i - p i t c h d e t e c t i o n r e s u l t s f o r t h e T R I O Sdataset using the proposed models.SystemFvFcFsFbFins§2.210.55%39.99%33.87%40.80%31.30%§2.312.28%41.55%34.53%42.33%32.67%Table 5.I n s t r u m e n t a s s i g n m e n t r e s u l t s f o r t h e B a c h 1 0dataset using the proposed models.developed speciﬁcally for piano, in contrast with the pro-posed multiple-instrument system.Regarding comparison on the MIREX recording, theproposed method outperforms the method of [6] by 3.9%in terms ofF.I nt e r m so fFf,t h eﬁ r s t3 0 s e co ft h eM I R E Xrecording were evaluated using the systems of [24] and[19], leading toFf=6 2.5%andFf=5 9.6%,r e s p e c -tively. The proposed HMM-constrained method reachesFf=7 0.35%,t h u so u t p e r f o r m i n gt h ea f o r e m e n t i o n e dsystems.For the Bach10 dataset, a comparison is made usingthe accuracy metric deﬁned in [11]. The proposed HMM-constrained method reaches an accuracy of 72.0%, whereasthe method of [11] reaches 69.7% (the latter results arewith unknown polyphony level, for direct comparison withthe proposed method).Finally, for the TRIOS dataset, multi-pitch detection re-sults were reported in [6], withF=5 7.6%.T h e p r o -posed method reaches for the HMM-constrained caseF=59.3%,t h u so u t p e r f o r m i n gt h es y s t e mo f[ 6 ] .3.4.2 Comparing time-frequency representationsIn order to evaluate the use of the proposed input VQTtime-frequency representation, a comparative experimentis made using the proposed system and having as input aconstant-Q representation (using the method of [21], with a60 bins/octave log-frequency resolution as with the VQT).For the comparative experiments, the MAPS-ENSTDkCldataset is employed and both the non-temporally constrainedand HMM-constrained models are evaluated. The post-processing steps are exactly the same as in the proposedmethod. Results show that when using the constant-Q rep-resentationF=6 3.98%for the non-temporally constrainedmodel andF=6 5.51%for the temporally-constrainedmodel, which are both signiﬁcantly lower when comparedto using a VQT representation as input (cf. Table 1).In order to show the improved detection performanceof a VQT representation with respect to lower pitches, thetranscription performance for the MAPS dataset was com-puted when only taking into account notes below or aboveMIDI pitch 60 (middle C in the piano). Using the VQT,F=6 5.18%for the lower pitches andF=7 4.98%for the higher pitches. In contrast when using the CQT,Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 705t(sec)MIDI pitch(b)MIDI pitch(a)\n123456789 1 0123456789 1 0\n406080100406080100\nFigure 4.( a ) T h e p i t c h a c t i v a t i o n o u t p u tP(p, t)for theﬁrst 10 sec of the MIREX multiF0 recording. (b) The cor-responding pitch ground truth.F=5 1.17%for the lower pitches andF=7 4.58%for the higher pitches. This result clearly demonstratesthe beneﬁt of using a VQT representation with respect totemporal resolution in lower frequencies, and by extension,to detecting lower pitches. As an example, Fig. 2 showsthe CQT and VQT spectrograms for a G1 piano note, withthe VQT exhibiting better temporal resolution in lower fre-quencies.3.4.3 Sound state templates vs. note templatesHere, a comparison is performed between the use of theproposed 5-dimensional dictionary of sound state templatesagainst the use of a 4-dimensional note template dictio-nary (which contains one template per pitch, instrument,and log-frequency shifting); the latter is supported by themethod of [3]. In order to have a direct comparison, themethod of [3] (for which the source code is publicly avail-able) is modiﬁed as to use the same input VQT represen-tation as well as post-processing steps with the proposedmethod, and is compared against the non-temporally con-strained model of subsection 2.2.When using a 4-dimensional dictionary, multi-pitch de-tection performance for the MAPS dataset reaches 64.65%,in contrast to 70.1% when using the 5-dimensional soundstate dictionary. This shows the importance of using soundstate templates, which are able to model the transient partsof the signal in contrast to simply using one (typically har-monic) note template for each pitch and instrument.3.4.4 RuntimesOn computational efﬁciency, the proposed model requireslinear operations like matrix/tensor multiplications in theEM steps; on the contrary, the previous model of [4] re-quired the computation of convolutions which signiﬁcantlyslowed down computations. Regarding runtimes, the orig-inal HMM-constrained convolutive model of [4] runs atabout 60⇥real-time using a Sony V AIO S15 laptop. Us-ing the proposed method, the runtime is approximately 1⇥real-time for the non-temporally constrained model, and2.5⇥real-time for the HMM-constrained model (i.e. fora1 m i nr e c o r d i n g ,r u n t i m e sa r e1 m i na n d2 . 5 m i n ,r e s p e c -tively). Thus, the proposed system is signiﬁcantly fastercompared to the model of [4], making it suitable for large-scale MIR applications.4. CONCLUSIONSIn this paper, we proposed a computationally efﬁcient sys-tem for multiple-instrument automatic music transcription,based on probabilistic latent component analysis. The pro-posed model employs a 5-dimensional dictionary of soundstate templates, covering different pitches, instruments,a n dtunings. Two model variants were presented: a PLCA-only method and a temporally constrained model that usespitch-wise HMMs in order to control the order of the soundstates. Experiments were performed on several transcrip-tion datasets; results show that the temporally-constrainedmodel outperforms the PLCA-based variant. In addition,the proposed system outperforms several state-of-the-artmultiple-instrument transcription systems using the MIREXmultiF0, Bach10, and TRIOS datasets. We also showedthat a VQT representation can yield improved results com-pared to a CQT representation. Finally, the non-temporallyconstrained variant of the model is able to transcribe arecording at 1⇥real-time, thus making this method use-ful for large-scale applications. The Matlab code for theHMM-constrained model can be found online1in the hopethat this model can serve as a framework for creating tran-scription systems useful to the MIR community.This system can also be extended beyond the proposedformulations, by exploiting recent developments in spec-trogram factorization-based approaches for music and au-dio signal analysis. Thus, the proposed model can alsoincorporate prior information in various forms (e.g. instru-ment identities, key information, music language models),following the PLCA-based approach of [23]. It can alsouse alternate EM update rules to guide convergence [8] orcan use additional temporal continuity and sparsity con-straints [13]. Drum transcription can also be incorporatedinto the system, in the same way as in [5]. In the future,we will also incorporate temporal constraints on note tran-sitions and polyphony level estimation and will continuework on instrument assignment by combining timbral fea-tures with PLCA-based models.5. ACKNOWLEDGEMENTEB is supported by a Royal Academy of Engineering Re-search Fellowship (grant no. RF/128).6. REFERENCES[1] Music Information Retrieval Evaluation eX-change (MIREX).http://music-ir.org/mirexwiki/.1https://code.soundsoftware.ac.uk/projects/amt_plca_5d706 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[2] M. Bay, A. F. Ehmann, and J. S. Downie. Evalua-tion of multiple-F0 estimation and tracking systems. In10th International Society for Music Information Re-trieval Conference,p a g e s3 1 5 – 3 2 0 ,K o b e ,J a p a n ,O c -tober 2009.[3] E. Benetos, S. Cherla, and T. Weyde. An efﬁcient shift-invariant model for polyphonic music transcription. In6th International Workshop on Machine Learning andMusic,P r a g u e ,C z e c hR e p u b l i c ,S e p t e m b e r2 0 1 3 .[4] E. Benetos and S. Dixon. Multiple-instrument poly-phonic music transcription using a temporally-constrained shift-invariant model.Journal of theAcoustical Society of America,1 3 3 ( 3 ) : 1 7 2 7 – 1 7 4 1 ,March 2013.[5] E. Benetos, S. Ewert, and T. Weyde. Automatic tran-scription of pitched and unpitched sounds from poly-phonic music. InIEEE International Conference onAcoustics, Speech, and Signal Processing,p a g e s3 1 3 1 –3135, Florence, Italy, May 2014.[6] E. Benetos and T. Weyde. Explicit duration hiddenMarkov models for multiple-instrument polyphonicmusic transcription. In14th International Society forMusic Information Retrieval Conference,p a g e s2 6 9 –274, Curitiba, Brazil, November 2013.[7] T. Berg-Kirkpatrick, J. Andreas, and D. Klein. Unsu-pervised transcription of piano music. InAdvances inNeural Information Processing Systems,p a g e s1 5 3 8 –1546, 2014.[8] T. Cheng, S. Dixon, and M. Mauch. A deterministicannealing em algorithm for automatic music transcrip-tion. In14th International Society for Music Informa-tion Retrieval Conference,C u r i t i b a ,B r a z i l .[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-mum likelihood from incomplete data via the EM algo-rithm.Journal of the Royal Statistical Society,3 9 ( 1 ) : 1 –38, 1977.[10] A. Dessein, A. Cont, and G. Lemaitre. Real-timepolyphonic music transcription with non-negative ma-trix factorization and beta-divergence. In11th Inter-national Society for Music Information Retrieval Con-ference,p a g e s4 8 9 – 4 9 4 ,U t r e c h t ,N e t h e r l a n d s ,A u g u s t2010.[11] Z. Duan, B. Pardo, and C. Zhang. Multiple fundamen-tal frequency estimation by modeling spectral peaksand non-peak regions.IEEE Transactions on Audio,Speech, and Language Processing,1 8 ( 8 ) : 2 1 2 1 – 2 1 3 3 ,November 2010.[12] J. Fritsch. High quality musical audio source separa-tion. Master’s thesis, UPMC / IRCAM / Tel´ecom Paris-Tech, 2012.[13] B. Fuentes, R. Badeau, and G. Richard. Harmonicadaptive latent component analysis of audio and ap-plication to music transcription.IEEE Transactions onAudio, Speech, and Language Processing,2 1 ( 9 ) : 1 8 5 4 –1866, September 2013.[14] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.RWC music database: music genre database and musi-cal instrument sound database. InInternational Con-ference on Music Information Retrieval,B a l t i m o r e ,USA, October 2003.[15] G. Grindlay and D. Ellis. Transcribing multi-instrument polyphonic music with hierarchicaleigeninstruments.IEEE Journal of Selected Topics inSignal Processing,5 ( 6 ) : 1 1 5 9 – 1 1 6 9 ,O c t o b e r2 0 1 1 .[16] A. Klapuri and M. Davy, editors.Signal Process-ing Methods for Music Transcription.S p r i n g e r - V e r l a g ,New York, 2006.[17] G. Mysore.An o n - n e g a t i v ef r a m e w o r kf o rj o i n tm o d -eling of spectral structure and temporal dynamics insound mixtures.P h Dt h e s i s ,S t a n f o r dU n i v e r s i t y ,U S A ,June 2010.[18] K. O’Hanlon and M.D. Plumbley. Polyphonic pianotranscription using non-negative matrix factorisationwith group sparsity. In2014 IEEE International Con-ference on Acoustics, Speech and Signal Processing,pages 3112–3116, May 2014.[19] P.H. Peeling and S.J. Godsill. Multiple pitch esti-mation using non-homogeneous poisson processes.IEEE Journal of Selected Topics in Signal Processing,5(6):1133–1143, October 2011.[20] L. R. Rabiner. A tutorial on hidden Markov models andselected applications in speech recognition.Proceed-ings of the IEEE,7 7 ( 2 ) : 2 5 7 – 2 8 6 ,F e b r u a r y1 9 8 9 .[21] C. Sch¨orkhuber and A. Klapuri. Constant-Q transformtoolbox for music processing. In7th Sound and MusicComputing Conf.,B a r c e l o n a ,S p a i n ,J u l y2 0 1 0 .[22] C. Sch¨orkhuber, A. Klapuri, N. Holighaus, andM. D¨orﬂer. A Matlab toolbox for efﬁcient perfectreconstruction time-frequency transforms with log-frequency resolution. InAES 53rd Conference on Se-mantic Audio,p a g e8p a g e s ,L o n d o n ,U K ,J a n u a r y2014.[23] P. Smaragdis and G. Mysore. Separation by “hum-ming”: user-guided sound extraction from mono-phonic mixtures. InIEEE Workshop on Applications ofSignal Processing to Audio and Acoustics,p a g e s6 9 –72, New Paltz, USA, October 2009.[24] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-monic spectral decomposition for multiple pitch esti-mation.IEEE Transactions on Audio, Speech, and Lan-guage Processing,1 8 ( 3 ) : 5 2 8 – 5 3 7 ,M a r c h2 0 1 0 .Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 707"
    },
    {
        "title": "Melody Extraction by Contour Classification.",
        "author": [
            "Rachel M. Bittner",
            "Justin Salamon",
            "Slim Essid",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416322",
        "url": "https://doi.org/10.5281/zenodo.1416322",
        "ee": "https://zenodo.org/records/1416322/files/BittnerSEB15.pdf",
        "abstract": "Due to the scarcity of labeled data, most melody extrac- tion algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch con- tour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data- driven melodic contour selection. Specifically, a discrim- inative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track’s extracted contours, and these scores are decoded to generate a single melody out- put. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the gen- erative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements.",
        "zenodo_id": 1416322,
        "dblp_key": "conf/ismir/BittnerSEB15",
        "keywords": [
            "scarcity of labeled data",
            "careful engineering",
            "pitch contour selection",
            "heuristics",
            "discriminative model",
            "binary classifier",
            "melodic contours",
            "generative model",
            "contour classification accuracy",
            "melody output"
        ],
        "content": "MELODY EXTRACTION BY CONTOUR CLASSIFICATIONRachel M. Bittner1, Justin Salamon1,2, Slim Essid3, Juan P. Bello11Music and Audio Research Lab, New York University2Center for Urban Science and Progress, New York University3T´el´ecom Paris-Techrachel.bittner@nyu.eduABSTRACTDue to the scarcity of labeled data, most melody extrac-tion algorithms do not rely on fully data-driven processingblocks but rather on careful engineering. For example, theMelodia melody extraction algorithm employs a pitch con-tour selection stage that relies on a number of heuristicsfor selecting the melodic output. In this paper we explorethe use of a discriminative model to perform purely data-driven melodic contour selection. Speciﬁcally, a discrim-inative binary classiﬁer is trained to distinguish melodicfrom non-melodic contours. This classiﬁer is then usedto predict likelihoods for a track’s extracted contours, andthese scores are decoded to generate a single melody out-put. The results are compared with the Melodia algorithmand with a generative model used in a previous study. Weshow that the discriminative model outperforms the gen-erative model in terms of contour classiﬁcation accuracy,and the melody output from our proposed system performscomparatively to Melodia. The results are complementedwith error analysis and avenues for future improvements.1. INTRODUCTIONMelody extraction has a variety of applications in musicretrieval, classiﬁcation, transcription and analysis [15]. Aprecise deﬁnition of melody that takes into account all pos-sible scenarios has proven elusive for the MIR commu-nity. In this paper we consider two different deﬁnitionsof melody [1]: Thef0curve of the predominant melodicline drawn from a single source (melody type 1), and thef0curve of the predominant melodic line drawn from mul-tiple sources (melody type 2).Some approaches to melody extraction are sourceseparation-based [4, 18], ﬁrst isolating the melodic sourcefrom the background and then tracking the pitch of the re-sulting signal. The most common approaches are basedon the notion of salience [3, 7, 13, 14], and are variants ofthe following steps (1) audio pre-processing, (2) saliencec\u0000Rachel M. Bittner, Justin Salamon, Slim Essid, Juan P.Bello. Licensed under a Creative Commons Attribution 4.0 InternationalLicense (CC BY 4.0).Attribution:Rachel M. Bittner, Justin Salamon,Slim Essid, Juan P. Bello. “Melody Extraction by Contour Classiﬁca-tion”, 16th International Society for Music Information Retrieval Confer-ence, 2015.function computation, (3)f0tracking, and (4) voicing de-cisions. Steps (3) and (4) for these methods are each basedon a series of carefully chosen heuristic steps, and are lim-ited to the data they were designed for. A recent trend inMusic Information Retrieval research is to combine do-main knowledge with data driven methods [8], using do-main informed feature representations as input to data-driven models. To the best of our knowledge, only onemelody extraction approach [5] has been proposed to dateusing a fully data driven method. However, the featuresemployed were poor for the task (magnitude Fourier co-efﬁcients), and used only limited temporal modeling viaHMM smoothing. Additionally, at the time, only a smallamount of data was available. The recent availability of an-notated melody data allows for new exploration into datadriven methods for melody extraction.In this paper, we present a system for melody extractionwhich replaces the common series of heuristic steps witha data-driven approach. We propose a method for scoringextracted contours (short, continuous pitch sequences) us-ing a discriminative classiﬁer, and a Viterbi-based methodfor decoding the output melody. We show that our methodperforms competitavely with Melodia [14]. The imple-mentation of the proposed method and the code used foreach experiment is available on Github1. The remainderof this paper is organized as follows: in Section 2 we givean overview of Melodia; Section 3 describes our proposedmethod for melody extraction; in Section 4 we present ex-periments evaluating the effectiveness of our method, in-cluding a comparison with Melodia, and in Section 5 wediscuss the conclusions and avenues for future work.2. MELODIAMelodia [14], a salience-based melody extraction algo-rithm, has proved to perform particularly well. The algo-rithm is comprised of four processing blocks: sinusoid ex-traction, salience function computation, contour creationand characterization, and ﬁnally melody selection. In theﬁrst block, spectral peaks are detected, and precise fre-quencies of each component are estimated using their in-stantaneous frequency. In the second stage a harmonicsummation-based salience function is computed. In thethird block, the peaks of the salience function are tracked1www.github.com/rabitt/contour_classification500into continuous pitch contours using auditory streamingcues. Additionally, a number of features are computed foreach contour:•Duration (seconds)•Pitch mean and standard deviation (in cents)•Salience mean, standard deviation, and sum•Vibrato presence (binary), rate (Hz), extent (cents),coverage (fraction of contour with vibrato)The melodyf0trajectory is obtained in the fourth blockby ﬁltering out non-melodic contours based on their fea-tures in combination with an iterative estimation of theglobal melodic trend. This step exploits the contour fea-ture distributions to perform the ﬁltering, but does so in aheuristic fashion. For further details the reader is referredto [14].Recently, Melodia was evaluated on the MedleyDB [1]dataset which contains considerably more variety in musi-cal style than previous datasets. The results were shown tobe substantially lower than for the existing datasets. In par-ticular, it was reported that Melodia’s performance on mu-sic with vocal melodies was better than on music with in-strumental melodies. This indicates that the heuristic stepsat the contour selection stage may be well tuned for singingvoice, but less so for instrumental melodies. Since theseheuristics are hard coded, the algorithm cannot be adjustedfor different kinds of data or different concepts of melody.We will show that these steps can be replaced by a datadriven approach.In [16], Salamon, Peeters, and R¨obel proposed to re-place the heuristic melodic selection block of Melodia witha generative classiﬁcation model. The contour featureswere used to estimate two multivariate Gaussian distribu-tions, one for melodic and another for non-melodic con-tours. These distributions were used to deﬁne the “melodi-ness” score, computed as the likelihood ratio of the twodistributions.The ﬁnalf0sequence was obtained by taking thef0ofthe contour with the highest “melodiness” at each frame.The authors showed that the generative model could pro-duce similar (albeit not equally good) results in terms ofpitch accuracy, but the model lacked a voicing detectionstep. This was addressed by combining the model with thevoicing detection ﬁlter of the original Melodia algorithm.Finally, in [17] the authors combined Melodia’s contourfeatures with additional features to train a discriminativemodel for classifying different musical genres. Their ex-periments showed that the contour features carry discrim-inative melodic information. This outcome, together withthat of [16] and the release of MedleyDB, gives compellingmotivation for the exploration of discriminative modelsusing pitch contour features for solving the problem ofmelodic contours selection.3. METHODThe proposed system uses the pitch contours and contourfeatures generated by Melodia2. The method consists of2These are taken from intermediate steps in the Vamp plugin’s imple-mentationa contour labeling stage, a training stage where a classiﬁeris ﬁt to discriminate melody from non-melody contours,and a decoding stage which generates a ﬁnalf0sequence.Melody output is computed using a trained classiﬁer asshown in Figure 1.3.1 Contour LabelingTo generate contours for a musical audio signal, we usethe ﬁrst three processing blocks of the Melodia algorithmdirectly (see [14] for details). Each contour is representedby a sequence of tuples (time, frequency, salience). Asdescribed in Section 2, the third block also computes a setof descriptive features for each contour, which we use totrain the model in Section 3.2.During training, extracted contours are assigned binarylabels: 1 if the contour should be considered as a part ofthe melody and 0 otherwise. The labels are chosen bycomparing the amount of overlap between each contourand the ground truth annotation. Given an annotationa(t)with0tT, a contourc(t)spanning the time intervalt1tt2is compared witha(t)over the time ranget1tt2. The amount of overlap between these twosequences is computed using “Overall Accuracy” [15], de-ﬁned as:Accov=1LL\u00001Xi=0viT[|ˆ'i\u0000'i|]+( 1\u0000vi)(1\u0000ˆvi)(1)where L is the number of reference/estimate examples,viandˆviare the (binary) voicings of the reference and esti-mate respectively,'iandˆ'iare thef0values in cents ofthe reference and estimate respectively, andTis a thresh-old function equal to 1 if the argument is less than 0.5,and 0 otherwise. Given a minimum overlap threshold↵, ifAccov>↵the contour is labeled as melody. Note that if↵=1, because of the strict inequality, all contours wouldbe labeled as non-melody. Despite containing extraneousinformation, a contour with a small degree of overlap stillcontains part of the melody. Labeling it as non-melody re-moves any possibility of the melody-portion of the contourending up in the ﬁnal extracted melody (i.e., lower recall).On the other hand, labeling it as melody potentially resultsin having non-melody information included in the melody(i.e., lower precision). Thus, there is an inherent trade-offbetween melody precision and recall based on the value ofthe overlap threshold↵.3.2 Contour ClassiﬁcationWe normalize the features per track to remove variancecaused by track-level differences. The salience features areeach divided by the maximum salience value in the trackto remove differences based on overall track salience. Theduration feature is normalized so that across the track theminimum value is 0 and the maximum value is 1. The fea-ture “total salience” is additionally re-scaled to reﬂect thenormalized duration.These features and the computed labels are used to traina random forest classiﬁer [2]. We use the random forest im-plementation inscikit-learn[11] with 100 trees andProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 501Discriminative ClassifierTraining DataContour ExtractionViterbi DecodingThresholdFigure 1. Block diagram of the proposed system (left to right): pitch contours are extracted from an audio signal, a classiﬁeris used to score the contours and remove those below a threshold, the ﬁnalf0sequence is obtained using Viterbi decoding.choose the maximum depth parameter by cross validatingover the training set. In our experiments, the classiﬁer wastrained with roughly 11,000 examples for melody 1 androughly 15,000 for melody 2. Because our class distribu-tions tend to be biased towards non-melody examples, theclassiﬁer is trained with class weights inverse to the classdistributions. Once the classiﬁer is trained, we use it topredict the probability that a given contour is melody. Inthe case of a random forest, the melody likelihood is com-puted as the fraction of trees that classify a given exampleas melody.3.3 Melody DecodingWe create an output melody by ﬁrst removing contourswhose likelihood falls below a threshold\u0000and then de-coding to generate a ﬁnal melody. The thresholding stepis necessary because there may be regions of time whereonly non-melody contours are present. Since decodingonly chooses the best path through available contours, hav-ing regions with contours which are all non-melody wouldresult in false positives. Aside from the contour extraction,the choice of this threshold is the single most importantstep for determining the voicing of the output melody.This raises the question: what is the best way to de-termine the likelihood threshold\u0000? A natural choice is\u0000=0.5, as this is the threshold that has been opti-mized by the machine for maximum classiﬁcation accu-racy. While this threshold gives us nearly perfect preci-sion for the melody class, the recall is extremely low. Weinstead choose the threshold that yields the highest class-weighted F1 score on a validation set. The chosen valueof\u0000in this manner is consistently much lower than 0.5(typically\u0000⇡0.1), resulting in higher recall at the costof lower precision. It is interesting to note that for our endgoal – selecting a single melody sequence – we do not nec-essarily need perfect precision because false positives canbe removed during decoding.After this ﬁltering step, contours that do not overlapwith any other contour are immediately assigned to themelody. The remaining contours have partial overlap withat least one other contour, requiring the melody line tobe chosen from within the overlapping segments. Thus,we divide these remaining contours into groups: contours{C1[t],...,Cn[t]}each spanning some time interval areassigned to the same group if the union of their intervalsforms a contiguous interval.The path over time through each group of contours iscomputed using Viterbi decoding [6]. Given a group ofncontours, our state space is the set of contour numbers{1,2,...,n}. We create a matrixYof emission probabili-ties using each contour’s likelihood score[p1,p2,...,pn]:Yit=⇢piifCiis active at timet0otherwise(2)The transition matrixA, deﬁned to encourage continu-ity in pitch space, is computed for each group as:Aij=Pnk6=j|log2(fi)\u0000log2(fk)|(n\u00001)Pnk=1|log2(fi)\u0000log2(fk)|(3)wherefiis the average frequency (in Hz) of contouri.This transition matrix, simply put, assigns a high transi-tion probability between contours whose (log) frequenciesare near one another, and a lower transition probability be-tween contours which are far from one another in log fre-quency. The prior distribution is set to be uniform. Giventhe sequence of contour statesS[t]computed by Viterbi,for each time pointt, the frequencyCS[t][t]is assigned tothe melody.4. EXPERIMENTSFor each of the following experiments we use the Med-leyDB Dataset [1]. Of the 122 tracks in the dataset, weuse the 108 that include melody annotations. We createtrain/test splits using an artist-conditional random parti-tion (i.e., tracks from the same artist cannot be in boththe train and test set). The complete training set is fur-ther split randomly into a training and validation. A giventrain, validate, and test split contains roughly 78%, 7%,and 15% respectively of the 108 tracks. We repeat eachexperiment with ﬁve different randomized splits to get asense of the variance of the results when using differentdata. In Figures 2, 3, and 4, vertical lines indicate thestandard deviation over the ﬁve splits. Recall that we con-sider two deﬁnitions of melody (Section 1). Consequently,when we report scores for melody type 1, the classiﬁer wastrained using the melody 1 annotations, and likewise formelody type 2. All evaluation metrics were computed us-ingmireval[12].4.1 Experiment 1: Generative vs. DiscriminativeContour ClassiﬁcationBefore evaluating components of the proposed system, weﬁrst examine the recall of Melodia’s contour extraction onthis dataset. That is, given all extracted contours, what isthe percentage of the reference melody that is covered bythe contours (in terms of pitch overlap)? We tested this byselecting the “oracle” (i.e., best possible)f0curve from the502 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2. Maximum F1 score achieved per overlap thresh-old↵by the generative and discriminative models.contours. The oracle output yielded an average Raw PitchAccuracy of 0.66 (\u0000=0.22) for melody type 1, and 0.64(\u0000=0.20) for melody type 2. Thus, the best raw pitchaccuracy we (or Melodia) could hope to achieve on thisdataset is upper bounded by these numbers.Acknowledging the existence of this glass ceiling, wecompare the generative model for scoring contours withthe proposed discriminative model. The features used forthe discriminative model are those described in Section 2,while the generative model used only the continuous fea-tures (i.e., none of the vibrato features)3. The features forthe multivariate Gaussian were transformed using the box-cox transformation as in [16], where the transformation’sparameter\u0000was estimated from the training set and ap-plied to the testing set.To evaluate the effectiveness of these two methods, wecompare the F1 scores achieved by selecting the optimallikelihood threshold\u0000. Figure 2 shows the best achievedF1 scores on the validation set for the two models. We seethat the random forest classiﬁer obtains a better F1 scorefor all values of↵. Interestingly, the F1 score achievedby the multivariate Gaussian is less affected by↵than theRandom Forest, which decreases as↵increases. Note thatneither classiﬁer achieves an F1 score above 75%. Thissuggests that either the models are not complex enough orthat the classes are not completely discriminable using thisset of features. Since our feature set is relatively small, thelatter is likely, and the performance of both of these mod-els would likely beneﬁt from a larger feature set. However,ﬁtting a high dimensional multivariate Gaussian requires alarge amount of data. Thus, another advantage of using arandom forest classiﬁer is that increasing the dimensional-ity of the feature space does not necessarily require moredata.One might argue that the difference in performanceof the two methods could be due to the fact that the vi-brato features are not used in the multivariate Gaussianmodel. However, a post-hoc analysis of the importanceof the vibrato features within the random forest classiﬁer(for melody 1 with↵=0.5) showed that they were by a3We initially included the vibrato features for the generative model,but the results were extremely poor.Melody Type OA RPA RCA11.6 2.5 2.522.4 4.2 2.1Table 1. Percentage point difference between Viterbi de-coding and taking a simple maximum.large margin the least important features in the set. In fact,the presence of vibrato contributed to discriminating only⇡0.03% of the training samples. The most discriminativefeatures for the random forest were the salience standarddeviation, followed by pitch mean, followed by pitch stan-dard deviation.Overall, we see that the random forest consistently out-performs the multivariate Gaussian, and has the additionalbeneﬁt of scalability to a much larger feature set.4.2 Experiment 2: Decoding MethodOur second experiment examines the effect of our Viterbidecoding strategy. First, we compare it with an approachbased on the one used in [16], where thef0value at eachpoint in time was chosen by taking a simple maximum overthe “melodiness” score. For our comparison, we take themaximum over the likelihoods produced by the classiﬁerafter thresholding.We found that Viterbi decoding consistently showed animprovement in the melody evaluation metrics on eachtrack. For some particular tracks, Viterbi decoding im-proved the output by up to 10 percentage points. Ta-ble 1 shows the average percentage point increase per trackby using Viterbi over the simple maximum. The metricsshown are the Overall Accuracy (OA), Raw Pitch Accu-racy (RPA), and Raw Chroma Accuracy (RCA) [15]. Wesee a particularly good improvement for melody 2, whereViterbi decoding increases the average raw pitch accuracyby more than 4 percentage points.Figure 3 shows each melody evaluation metrics acrossthe different overlap thresholds↵. The values plotted areaverages over each of the 5 experiments, where the errorbars indicate the standard deviation. Surprisingly, we seevery little difference in any of the metrics for both melodytypes. We saw in Figure 2 that the F1 score decreased as↵increased, which implies that unlike what we might ex-pect, the ﬁnal melody output is not strongly affected by theF1 score. Note, however, that the F1 score is computed ona different set of labels for each value of↵. The resiliencemay be due to the fact that the labels that change as wesweep↵are the “noisier” labels, and thus the hardest toclassify, whereas the contours that are not affected by thevalue of↵(i.e., very high overlap or no overlap with the an-notation) are easier to classify. We conjecture that for eachvalue of↵the classiﬁers are probably performing equallypoorly on the noisy contour examples and equally well onthe clean examples.All in all, the deviations in metrics are minor across val-ues of↵, and we conclude that the value of↵does not havea strong impact on the ﬁnal melody accuracy. The valuesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 503Figure 3. Melody metrics for each overlap threshold↵andmelody type.of↵that yield the highest scores on the validation set (by asmall margin) were↵=0.5for melody 1 and↵=0.4formelody 2, and these values are used for our ﬁnal system.4.3 Experiment 3: Melodia vs. Proposed MethodAs a ﬁnal experiment, we compare the proposed methodwith Melodia. This experiment essentially evaluates thetwo different contour selection methods, since both meth-ods begin with the same set of contours. Melodia’s param-eter⌫controls the voicing decision, and is the parameterwith the largest impact on the ﬁnal output. The scores re-ported for Melodia in this experiment use the value of⌫that achieved the best overall accuracy on the training set.The ﬁnal scores are reported for the test set.The results for each algorithm are shown in Figure 4.The proposed method performs quite competitively withMelodia. In particular, Melodia only outperforms our sys-tem in overall accuracy by 4 percentage points for melody1 and 2 percentage points for melody 2. The primary met-ric where the algorithms differ is in the voicing recall andvoicing false alarm rates. Our system has signiﬁcantly bet-ter recall than Melodia (33 percentage points higher formelody 1, 9 for melody 2), but also a much higher falsealarm rate (34 percentage points higher for melody 1, 14for melody 2) - in other words, our system assigns con-tours to the melody much more often than Melodia does.An interesting example to this point is shown in Fig-ure 5. Both methods achieve the same overall accuracyof⇡0.50, but their output is quite different. Our outputgets almost all of the voiced frames correct, but has spu-rious mistakes outside of the annotation as well. In con-trast, Melodia has nearly perfect precision, but misses largesegments. This example is characteristic of the differencebetween the two algorithms – our approach over-predictsmelody, and Melodia under-predicts it. Notice that the pro-posed method produces spurious frequency values, causedby slight differences in contour start and end points withincontour groups. These values could be removed in a futurepost processing stage.\nFigure 5. Segment of melody output for “Clara Berry andWooldog: The Bad Guys”.This difference is especially signiﬁcant for tracks con-taining instrumental melodies. Figure 6 shows a segmentfrom a track with a ﬂute melody. Our approach worksquite well for this example, while Melodia misses mostof the melodic line. In Figure 4 we also report the over-all accuracy for the portions of the data containing vocal(OA-V) and instrumental melodies (OA-I). We see that forinstrumental melodies, our method matches Melodia’s per-formance for melody 1 and slightly outperforms Melodiafor melody 2. Conversely, we see the opposite trend forvocals, with Melodia outperforming our method for bothmelody types. This trend can be largely attributed again tothe differences in voicing statistics – vocal melodies in thisdataset tend to have many more unvoiced frames than in-strumental melodies, so our method’s high false alarm ratehurts our performance for vocal tracks.Despite the slight difference in metrics, the two algo-rithms perform similarly, with inversely related pitfalls. Itis interesting to note that when the current approach com-pletely fails, so does Melodia. This ﬁrst and foremost oc-curs when output from the contour extraction stage is poor,which dooms both methods to failure.\nFigure 6. Segment of melody output for “Phoenix: Larkon the Strand/Drummond’s Castle”.Overall, we see that the proposed method is quite goodat correctly choosing melody examples, but the high false504 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 4. Final melody output scores for the proposed method and Melodia. The metrics are abbreviated on the x-axisas: VR = V oicing Recall, VFA = V oicing False Alarm, RPA = Raw Pitch Accuracy, RCA = Raw Chroma Accuracy, OA =Overall Accuracy, OA-V = Overall Accuracy – vocal tracks, and OA-I = Overall Accuracy – instrumental tracks.alarm rates hurt its overall scores. This speaks to the clas-siﬁer’s need for better discrimination between melody andnon melody examples. To do this, we need more/betterfeatures, a more powerful classiﬁer, or both. This ties backto Ellis and Poliner’s observation in [5]: a large percentageof contours are very easy to distinguish, and the remainingcontours are difﬁcult for data driven and heuristic methodsalike. This is likely due to the lack of longer time scale fea-tures describing the relationship between observations. Weas humans are able to distinguish melody from non-melodyin a song, but in ambiguous cases, we make our distinctionbased on what we heard earlier in the song [10].As a ﬁnal illustration, Figure 7 shows the output of bothalgorithms for melody 1 (top) and melody 2 (bottom) fora segment containing a ﬂute and a trumpet. The melody iscarried by the ﬂute for most of the track, but in this segmentis carried by the trumpet. For melody 1, both methods trackthe ﬂute, matching the annotation, whereas for melody 2both methods still track the ﬂute whereas the trumpet lineis annotated. Without long-term context giving the algo-rithm information about which lines have happened previ-ously as background or melody, there is no way for eitherof these methods to choose the “correct” line.5. CONCLUSIONS AND FUTURE WORKIn this paper, we have shown that replacing Melodia’sheuristic decisions with a series of fully data driven de-cisions can nearly match Melodia’s overall performance,and there many open avenues for improving our results.In particular, we have shown that a discriminative modelcan outperform a generative model for labeling contours,and we have provided a detailed evaluation of how eachstep in the proposed approach inﬂuences the ﬁnal results.Compared to Melodia, we noted that the proposed methodhas better melody recall, but a considerably worse voicingfalse alarm rate. To improve the discrimination ability ofthe classiﬁer, future iterations of this method will ﬁrst in-corporate a wider set of features, including features that de-scribe neighboring contours (octave duplicates, etc.), andfeatures that describe a contour’s relationship with the restof the track on a longer time scale, potentially includingtimbre similarity. Additionally, since we are using a rel-\nFigure 7. Outputs for melody 1 (top) and melody 2 (bot-tom) for a segment of “Music Delta: Latin Jazz”.atively small training set, we would like to explore aug-menting our training data through sets of time and pitchdeformations.With a slight adjustment to the evaluation metrics, ourmethod can be easily extended to be trained on and pre-dict melody type 3 [1] annotations, which give all feasiblemelody candidates at each time point, and is the most in-clusive melody deﬁnition for MedleyDB. A limitation ofthe current method is that it assigns a single likelihood toeach contour. Since the extracted contours virtually neveroverlap completely with the annotation, it would be de-sirable to be able to assign time-varying scores to eachcontour. To do this, we plan to explore the use of Condi-tional Random Fields [9] for assigning scores to contoursbecause of their ability to incorporate temporal informa-tion. Finally, to raise the glass ceiling on performance,future work will include revisiting the contour extractionstage.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 5056. REFERENCES[1]R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,C. Cannam, and J. P. Bello. MedleyDB: a MultitrackDataset for Annotation-Intensive MIR Research. InIn-ternational Society for Music Information RetrievalConference, July 2014.[2]L. Breiman. Random forests.Machine Learning,45(1):5–32, 2001.[3]K. Dressler. An Auditory Streaming Approach forMelody Extraction from Polyphonic Music. InInter-national Society for Music Information Retrieval Con-ference, 2011.[4]Jean-Louis Durrieu, Ga¨el Richard, Bertrand David,and C´edric F´evotte. Source/ﬁlter model for unsuper-vised main melody extraction from polyphonic audiosignals.IEEE Trans. on Audio, Speech, and LanguageProcessing, 18(3):564–575, March 2010.[5]D. P. W. Ellis and G. Poliner. Classiﬁcation-BasedMelody Transcription.Machine Learning Journal,65(2-3):439–456, December 2006.[6]G. D. Forney Jr. The Viterbi algorithm.Proceedings ofthe IEEE, 61(3):268–278, 1973.[7]M. Goto. A Real-Time Music-Scene-Description Sys-tem: Predominant-F0 Estimation for Detecting Melodyand Bass Lines in Real-World Audio Signals.SpeechCommunication, 43(4):311–329, September 2004.[8]E. Humphrey, J. P. Bello, and Y . Lecun. Feature Learn-ing and Deep Architectures: New Directions for Mu-sic Informatics.Journal of Intelligent Information Sys-tems, 41(3):461–481, December 2013.[9]John Lafferty, Andrew McCallum, and Fernando CNPereira. Conditional random ﬁelds: Probabilistic mod-els for segmenting and labeling sequence data. InPro-ceedings of the Eighteenth International Conference onMachine Learning, pages 282–289, 2001.[10]Eugene Narmour.The Analysis and Cognition ofMelodic Complexity: The Implication-RealizationModel. University of Chicago Press, November 1992.[11]F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V . Dubourg, et al. Scikit-learn: Machinelearning in python.The Journal of Machine LearningResearch, 12:2825–2830, 2011.[12]C. Raffel, B. McFee, E. Humphrey, J. Salamon, O. Ni-eto, D. P. W. Ellis, and D. Liang. mir eval: A Trans-parent Implementation of Common MIR Metrics. InInternational Society for Music Information RetrievalConference, 2014.[13]M. Ryyn¨anen and A. Klapuri. Automatic Transcriptionof Melody, Bass Line, and Chords in Polyphonic Mu-sic.Computer Music Journal, 32(3):72–86, September2008.[14]J. Salamon and E. G´omez. Melody extraction frompolyphonic music signals using pitch contour charac-teristics.IEEE Transactions on Audio, Speech, andLanguage Processing, 20(6):1759–1770, Aug. 2012.[15]J. Salamon, E. G´omez, D. P. W. Ellis, and G. Richard.Melody Extraction From Polyphonic Music Signals:Approaches, Applications, and Challenges.IEEE Sig-nal Processing Magazine, 31(2):118–134, 2014.[16]J. Salamon, G. Peeters, and A. R¨obel. Statistical char-acterisation of melodic pitch contours and its applica-tion for melody extraction. In13th Int. Soc. for MusicInfo. Retrieval Conf., pages 187–192, Porto, Portugal,Oct. 2012.[17]J. Salamon, B. Rocha, and E. G´omez. Musical genreclassiﬁcation using melody features extracted frompolyphonic music signals. InIEEE International Con-ference on Acoustics, Speech and Signal Processing,pages 81–84, Kyoto, Japan, Mar. 2012.[18]H. Tachibana, T. Ono, and S. Sagayama. MelodyLine Estimation in Homophonic Music Audio SignalsBased on Temporal-Variability of Melodic Source. InIEEE International Conference on Acoustics, Speechand Signal Processing, pages 425–428. IEEE, 2010.506 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.",
        "author": [
            "Sebastian Böck",
            "Florian Krebs",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416026",
        "url": "https://doi.org/10.5281/zenodo.1416026",
        "ee": "https://zenodo.org/records/1416026/files/BockKW15.pdf",
        "abstract": "In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike ex- isting (comb filter based) approaches, we do not use hand- crafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as in- put to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing differ- ent accent bands, modelling metrical relations, categoris- ing the excerpts into slow / fast or any other advanced pro- cessing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator’s histogram peak.",
        "zenodo_id": 1416026,
        "dblp_key": "conf/ismir/BockKW15",
        "keywords": [
            "tempo estimation",
            "bank of resonating comb filters",
            "determining dominant periodicity",
            "recurrent neural network",
            "beat-level representation",
            "comb filter bank",
            "state-of-the-art performance",
            "nine of ten datasets",
            "advanced processing",
            "resonators histogram peak"
        ],
        "content": "Accurate Tempo Estimation based on Recurrent Neural Networks andResonating Comb FiltersSebastian B¨ock, Florian Krebs and Gerhard WidmerDepartment of Computational PerceptionJohannes Kepler University, Linz, Austriasebastian.boeck@jku.atABSTRACTIn this paper we present a new tempo estimation algorithmwhich uses a bank of resonating comb ﬁlters to determinethe dominant periodicity of a musical excerpt. Unlike ex-isting (comb ﬁlter based) approaches, we do not use hand-crafted features derived from the audio signal, but rather leta recurrent neural network learn an intermediate beat-levelrepresentation of the signal and use this information as in-put to the comb ﬁlter bank. While most approaches applycomplex post-processing to the output of the comb ﬁlterbank like tracking multiple time scales, processing differ-ent accent bands, modelling metrical relations, categoris-ing the excerpts into slow / fast or any other advanced pro-cessing, we achieve state-of-the-art performance on nineof ten datasets by simply reporting the highest resonator’shistogram peak.1. INTRODUCTIONTempo estimation is one of the most fundamental musicinformation retrieval (MIR) tasks. The tempo of musiccorresponds to the frequency of the beats, i.e. the speed atwhich humans usually tap to the music.In this paper, we only deal with global tempo estima-tion, i.e. report a single tempo estimate for a given musi-cal piece, and do not consider the temporal evolution oftempo. Possible applications for such algorithms includeautomatic DJ mixing, similarity estimation, music recom-mendation, playlist generation, and tempo aware audio ef-fects. Finding the correct tempo is also vital for many beattracking algorithms which use a two-folded approach ofﬁrst estimating the tempo of the music and then aligningthe beats accordingly.Many different methods for tempo estimation have beenproposed in the past. While early approaches estimated thetempo based on discrete time events (e.g. MIDI notes or asequence of onsets) [6], almost all of the recently proposedalgorithms [4, 7, 8, 17, 23, 28] use some kind of continuousinput. Generally, they follow this procedure: they trans-c\u0000Sebastian B¨ock, Florian Krebs and Gerhard Widmer. Li-censed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Sebastian B¨ock, Florian Krebs and Ger-hard Widmer. “Accurate Tempo Estimation based on Recurrent NeuralNetworks and Resonating Comb Filters”, 16th International Society forMusic Information Retrieval Conference, 2015.form the audio signal into a down-sampled feature, esti-mate the periodicities and ﬁnally select one of the period-icities as tempo.As a reduction function, the signal’s envelope [26], bandpass ﬁlters [8, 17, 28], onset detection functions [4, 8, 23,28] or combinations thereof are commonly used. Popu-lar choices for periodicity detection include Fast FourierTransform (FFT) based methods like tempograms [3, 28],autocorrelation [6, 8, 23, 25] or comb ﬁlters [4, 17, 26]. Fi-nally, post-processing is applied to chose the most promis-ing periodicity as perceptual tempo estimate. These post-processing methods range from simply selecting the high-est periodicity peak to more sophisticated (machine learn-ing) techniques, e.g. hidden Markov models (HMM) [17],Gaussian mixture model (GMM) regression [24] or sup-port vector machines (SVM) [9, 25].In this paper, we propose to use a neural network toderive a reduction function which makes complex post-processing redundant. By simply selecting the comb ﬁlterwith the highest summed output, we achieve state-of-the-art performance on nine of ten datasets in theAccuracy 2evaluation metric.2. RELATED WORKIn the following, we brieﬂy describe some important worksin the ﬁeld of tempo estimation. Gouyon et al. [12] givean overview of the ﬁrst comparative algorithm evaluationwhich took place for ISMIR 2004, followed by anotherstudy by Zapata and G´omez [29].The work of Scheirer [26] was the ﬁrst one to processthe audio signal continuously rather than working on aseries of discrete time events. He proposed the use ofresonating comb ﬁlters, which are one of the main tech-niques used for periodicity estimation since then. Periodic-ity analysis is performed on a number of band pass ﬁlteredsignals and then the outputs of this analysis are combinedand a global tempo is reported.Dixon [6] uses discrete onsets gathered with the spectralﬂux method to build clusters of inter onset intervals whichare in turn processed by a multiple agent system to ﬁnd themost likely tempo. Oliveira et al. [23] extend this approachto use a continuous input signal instead of discrete timeevents and modiﬁed it to allow causal processing.Klapuri et al. [17] jointly analyse the musical piece atthree time scales: the tatum, tactus (which corresponds to625the beat or tempo) and measure level. The signal is splitinto multiple bands and then combined into four accentbands before being fed into a bank of resonating combﬁlters similar to [26]. Their temporal evolution and therelation of the different time scales are modelled with aprobabilistic framework to report the ﬁnal position of thebeats. The tempo is then calculated as the median of thebeat intervals during the second half of the signal.Instead of a multi-band approach as used in [17, 26],Davies and Plumbley [4] process an autocorrelated versionof a complex domain onset detection function with a shiftinvariant comb ﬁlter bank to get the beat period. Althoughthis method uses only a single dimensional input feature,it performs almost as good as the competing algorithmsin [12] but has much lower computational complexity.Gainza and Coyle [8] use a multi-band decompositionto split the audio signal into three frequency bands and thenperform a transient / onsets detection (with different onsetdetection methods). These are transformed via autocor-relation into periodicity density functions, combined, andweighted to extract the ﬁnal tempo.Gkiokas et al. [9] utilise harmonic / percussive sourceseparation on top of a constant-Q transformed signal inorder to extract chroma features and ﬁlter bank energiesfrom the separated signal respectively. Periodicity is esti-mated for both representations with a bank of resonatingcomb ﬁlters for overlapping windows of 8 seconds lengthand the resulting features are combined before a metricallevel analysis is performed to report the ﬁnal tempo. In aconsecutive work [10] they use a support vector machine(SVM) to classify the music into tempo classes to betterpredict the tempo to be reported.Elowsson et al. [7] also use harmonic / percussive sourceseparation to model the speed of music. They derive var-ious features like onset densities (for multiple frequencyranges) and strong onset clusters and use a regression modelto predict the tempo of the signal.Percival and Tzanetakis [25] use a “traditional” approachby ﬁrst generating a spectral ﬂux onset strength signal, fol-lowed by a stage which detects the beat period in overlap-ping windows of approximately 6 seconds length (via gen-eralised autocorrelation with harmonic enhancement) anda ﬁnal accumulating stage which gathers all these tempoestimates and uses a support vector machine (SVM) to de-cide which octave the tempo should be in.Wu and Jang [28] ﬁrst derive an unaltered and a lowpass ﬁltered version of the input signal. Then they obtain atempogram representation of a complex domain onset de-tection function for both signals to obtain tempo pairs. Aclassiﬁer is then used to report the ﬁnal most salient tempo.3. ALGORITHM DESCRIPTIONScheirer [26] found it beneﬁcial to compute periodicitiesindividually on multiple frequency bands and then subse-quently combine them to estimate a single tempo. Klapuriet al. [17] followed this route but Davies and Plumpley ar-gued that is is enough to have a single – musically mean-ingful – feature to estimate the periodicity of a signal [4].Given the fact that beats are the musically most relevantdescriptors for the tempo of a musical piece, we take thisapproach one step further and do not use the pre-processedsignal directly – or any representation that is strongly cor-related with it, e.g. an onset detection function – as an inputfor a comb ﬁlter, but rather process the signal with a neuralnetwork which is trained to predict the positions of beatsinside the signal. The resulting beat activation function isthen fed into a bank of resonating comb ﬁlters to determinethe tempo.\nNeural NetworkCombFilter BankSignalTempoSignalPreprocessingFigure 1: Overview of the new tempo estimation system.Figure1gives general overview over the different stepsof the tempo estimation system, which are described intomore detail in the following sections.3.1 Signal Pre-ProcessingThe proposed system processes the signal in a frame-wisemanner. Therefore the audio signal is split into overlappingframes and weighted with a Hann window of same lengthbefore being transferred to a time-frequency representa-tion by means of the Short-time Fourier Transform (STFT).Two adjacent frames are located 10 ms apart, which corre-sponds to a rate of 100 fps (frames per second). We omitthe phase portion of the complex spectrogram and use onlythe magnitudes for further processing. To reduce the di-mensionality of the signal, we process it with a logarith-mically spaced ﬁlter which has three bands per octave andis limited to the frequency range[30,17000]Hz. To bet-ter match the human’s perception of loudness, we scale theresulting frequency bands logarithmically. As the ﬁnal in-put features for the neural network, we stack three spec-trograms and their ﬁrst order difference calculated withdifferent STFT sizes of 1024, 2048 and 4096 samples, avisualisation is given Figure2b.3.2 Neural Network ProcessingAs a network we chose the system presented in [1], whichis also the basis for the current state-of-the-art in beat track-ing [2, 18]. The output of the neural network is a beatactivation function, which represents the probability of aframe being a beat position. Instead of processing the beatactivation function to extract the positions of the beats, weuse it directly as a one-dimensional input to the bank ofresonating comb ﬁlters.Using this continuous function instead of discrete beatsis advantageous since the detection is never 100% effec-tive und thus introduces errors when inferring the tempodirectly from the beats. This is in line with the observationthat recent tempo induction algorithms use onset detectionfunctions or other continuously valued inputs rather thandiscrete time events.626 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015(a) Input audio signal\n(b) Input to the neural network\n(c) Neural network output (beat activation function)\n(d) Resonating comb ﬁlter bank output\n(e) Maxima of the resonating comb ﬁlter bank\n(f) Weighted histogram with summed maximaFigure 2: Signal ﬂow of a 6 second pop song excerpt:(a)input audio signal,(b)pre-processed input to the neuralnetwork,(c)its raw (dotted) and smoothed (solid) output,(d)corresponding comb ﬁlter bank response,(e)the max-ima thereof,(f)resulting raw (dotted) and smoothed (solid)weighted histogram of the summed maxima. The beat po-sitions and the tempo are marked with vertical red lines.We believe that the learned feature representation (atleast to some extent) incorporates information that other-wise would have to be modelled explicitly, either by track-ing multiple time scales [17], processing multiple accentbands [26], modelling metrical relations [9], dividing theexcerpts into slow / fast categories [7] or any other advancedprocessing. Figure2cshows an exemplary output of theneural network. It can be seen that the network activationfunction has strong regular peaks that do not always coin-cide with high energies in the network’s inputs.3.2.1 Network TrainingWe train the network on the datasets described in Section4.2which are marked with an asterisk (*) in an 8-foldcross validation setting based on a random splitting of thedatasets. We initialise the network weights and biases witha uniform random distribution with range[\u00000.1,0.1]andtrain it with stochastic gradient decent with a learning rateof10\u00004and a momentum of0.9. We stop training if no im-provement of the cross entropy error of the validation setcan be observed for 20 epochs. All adjustable parametersof the system are tuned to maximise the tempo estimationperformance on the validation set.3.2.2 Activation Function SmoothingThe beat activation function of the neural network reﬂectsthe probability that a given frame is a beat position. How-ever, it can happen that the network is not sure about theexact position of the beat if it falls close to the border be-tween two frames and hence splits the reported probabilitybetween these two frames. Another aspect to be consideredis the fact that the ground truth annotations used as targetsfor the training are sometimes generated via manual tap-ping and thus deviate from the real beat position by up to50 ms. This can result also in blurred peaks in the beat acti-vation function. To reduce the impact of these artefacts, wesmooth the activation function before being processed withthe ﬁlter bank by convolving it with a Hamming windowof length 140 ms.13.3 Comb Filter Periodicity EstimationWe use the output of the neural network stage as input toa bank of resonating comb ﬁlters. As outlined previously,comb ﬁlters are a common choice to detect periodicitiesin a signal, e.g. [4, 17, 26]. The advantage of comb ﬁltersover autocorrelation lays in the fact that comb ﬁlters alsoresonate at multiples, fractions and simple rationales of theﬁlter lag. This behaviour is in line with the perception ofhumans, which do not necessarily consider double or halftempi wrong. We use a bank of resonating feed backwardcomb ﬁlters with different time lags (⌧), deﬁned as:y(t, ⌧)=x(t)+↵⇤y(t\u0000⌧,⌧).(1)Each comb ﬁlter adds a scaled (by factor↵) and delayed(with lag⌧) version of its own outputy(t)to the input sig-nalx(t)withtdenoting the time frame index.1Because of this smoothing the beat activations do not reﬂect proba-bilities any more (and they may exceed the value of 1), but this does notharm the overall interpretation and usefulness.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6273.3.1 Lag Range DeﬁnitionFor the individual bands of the comb ﬁlter bank we use alinear spacing of the lags with the minimum and maximumdelays calculated as:⌧min=b60⇤fps/bpmmaxc⌧max=d60⇤fps/bpmmine(2)withfpsrepresenting the frame rate of the system given inframes per second and the minimum and maximum tempibpmminandbpmmaxgiven in beats per minute. We foundthe tempo range of[40,250]bpm to perform best on thevalidation set.3.3.2 Scaling Factor DeﬁnitionScheirer [26] found it beneﬁcial to use different scalingfactors↵(⌧)for the individual comb ﬁlter bands. He de-ﬁnes them such that the individual ﬁlters have the samehalf-energy time. Klapuri [17] also uses ﬁlters with ex-ponentially decaying pulse response, but sets the scalingfactor such that the response decays to half after a deﬁnedtime of 3 seconds.Contrary to these ﬁndings, we use a single value for allﬁlter lags, which is set to↵=0.79. The reason that a sin-gle value works better for this system may lay in the factthat we sum all peaks of the ﬁlters. With a ﬁxed scalingfactor, the resonance of ﬁlters with smaller lags tend to de-cay faster, but they also produce more peaks, hence leadingto a more “balanced” histogram.3.3.3 Histogram BuildingAfter smoothing the neural network output and process-ing it with the comb ﬁlter, we build a weighted histogramH(⌧)from the outputy(t, ⌧)by simply summing the ac-tivations of the individual comb ﬁlters (over all frames)if this ﬁlter produced the highest peak at the given timeframe:H(⌧)=TXt=0y(t, ⌧)⇤I(⌧,arg max⌧y(t, ⌧))I(a, b)=(1ifa⌘b0otherwise(3)withtdenoting the time frame index,Tthe total numberof frames, and⌧the ﬁlter delays.The bins of the weighted histogram correspond to thetime lags⌧and the bin heights represent the number offrames where the corresponding ﬁlter has a maximum atthis delay, weighted by the activations of the comb ﬁl-ter. This weighting has the advantage that it favours ﬁl-ters which resonate at lags which correspond to intervalswith highly probable beat positions (i.e. high values of thebeat activation function) over those which are less proba-ble. Figure2dillustrates the output of the comb ﬁlter bank,Figure2ethe weighted maxima which are used to build theweighted histogram shown as the dotted line in Figure2f.3.3.4 Histogram SmoothingMusic almost always contains tempo ﬂuctuations – at leastwith regard to the frame rate of the system. Even stabletempi result in weights being split between two or morehistogram bins. Therefore we combine bins before report-ing the ﬁnal tempo.Our approach simply smooths the histogram by con-volving it with a Hamming window with a width of sevenbins, similar to [25]. Depending on the bin index (corre-sponding to the ﬁlter lag⌧), a ﬁxed width results in differ-ent tempo deviations, ranging from\u00007%to+8%for a lagof⌧= 24(corresponding to 250 bpm) to\u00002%to+2.9%for a lag of⌧= 40(i.e. 40 bpm). Although this allows agreater deviation for higher tempi, we found no improve-ment over choosing the size of the smoothing window asa function of the tempo. Figure2fshows the smoothedhistogram as the solid line.3.3.5 Peak SelectionThe histogram shows peaks at the different tempi of themusical piece. Again, previous works put much effort intothis stage to select the peak with the strongest perceptualstrength, ranging from simple rules driven by heuristics[25] over GMM regression based solutions [24] to utilis-ing a support vector machine (SVM) [10, 25] or decisiontrees [25]. In order to keep our approach as simple as pos-sible, we simply select the highest peak of the smoothedhistogram as our ﬁnal tempo.4. EVALUATIONTo assess the performance of the proposed system we com-pare it to an autocorrelation based tempo estimation methodas described in [1], which operates on the same beat activa-tion function obtained with the neural network described inSection3.2. The algorithms of Gkiokas [9], Percival [25],Klapuri [17], Oliveira [23], and Davies [4] were chosen asadditional reference systems based on their availability andoverall performance.For a short description of these algorithms, please referto Section2.All of the algorithms were used in their default con-ﬁguration, except the system of Oliveira [23], which weoperated in ofﬂine mode with an induction length of 100seconds, because it yielded signiﬁcantly better results.2It should be noted however, that this mode results in a re-duced tempo search range of 81-160 bpm, which can leadto biased results in favour of datasets in this tempo range.Following [29] and [25] we perform statistical tests ofour results compared to the others with McNemar’s testusing a signiﬁcance value ofp<0.01.4.1 Evaluation MetricsSince humans perceive tempo and rhythm subjectively,there is no single best tempo estimate. For example, theperceived tempo can be a multiple or fraction of the tempogiven by the score of the piece. This is also known as2This corresponds to:ibt -off -i auto-regen -t 100628 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015the tempo octave problem. Therefore, two evaluation mea-sures are used in the literature:Accuracy 1considers onlythe single annotated tempo for the evaluation, whereasAc-curacy 2also includes integer multiples or fractions of theannotated tempo. Since the data that we use also containsmusic in ternary meter, we do not only add double andhalf tempo annotations, but also triple and third tempo. Inline with most other publications we report accuracy valueswhich denote the algorithms’ ability to correctly estimatethe tempo of the musical piece with less than 4% deviationform the annotated ground truth.4.2 DatasetsWe use a total of ten datasets to evaluate the performanceof our algorithm. Table1lists some statistics of the datasets.Datasets marked with an asterisk (*) were used to train theneural networks with 8-fold cross validation as describedin Section3.2.1.For all sets with beat annotations available (Ballroom,Hainsworth, SMC, Beatles, RWC, HJDB), we generatedthe tempo annotations as the median of the inter beat in-tervals. For the HJDB set (which is in 4/4 meter), we ﬁrstderived the beat positions from the downbeat annotationsbefore inferring the tempo ground truth. For all other setswe use the provided tempo annotations and – where appli-cable – the corrected annotations from [25].# ﬁles length annotationsDatasetBallroom [12, 19] * 68535h 57m beatsHainsworth [13] * 222 3h 19m beatsSMC [16] * 217 2h 25m beatsKlapuri [17] 474 7h 22m beatsGTZAN [25, 27] 999 8h 20m tempoSongs [12] 465 2h 35m tempoBeatles [5] 180 8h 9m beatsACM Mirum [21, 24] 1410 15h 5m tempoRWC Popular [11] 100 6h 47m beatsHJDB [15] 235 3h 19m downbeatstotal 4987 63h 17mTable 1: Overview of the datasets used for evaluation.4.3 Results & DiscussionTable2lists the results of the proposed algorithm com-pared to the reference systems. The results (of our algo-rithm) reported on the Ballroom, Hainsworth and SMCset are obtained with 8-fold cross-validation, since thesedatasets were used to train the neural network. Althoughthis is a technically correct evaluation, it can lead to biasedresults, since the system knows, e.g. about ballroom musicand its features in general and thus has an advantage overthe other systems. It is thus no surprise that the proposedsystem outperforms the others on these sets.3We removed the 13 duplicates identiﬁed by Bob Sturm:http://media.aau.dk/nullspacepursuits/2014/01/ballroom-dataset.htmlNonetheless, the new system outperforms the autocor-relation based tempo estimation method operating on thevery same neural network output in almost all cases. Thisclearly shows the advantage of the resonating comb ﬁlters,which are less prone to single missing or misaligned peaksin the beat activation function, due to their recurrent na-ture and the fact that they also resonate on fractions andmultiples of the dominant tempo.The results for the other datasets reﬂect the algorithm’sability to estimate the tempo of a completely unknown sig-nal without tuning any of the parameters. It can be seenthat no single system performs best on all datasets. Ourproposed system performs state-of-the-art (i.e. no other al-gorithm is statistically signiﬁcantly better) in all but theHJDB set w.r.t.Accuracy 2. We even outperform most ofthe other methods inAccuracy 1, which highlights the al-gorithm’s ability to not only capture a meaningful tempo,but also choose the correct tempo octave.An inspection of incorrectly detected tempi in the HJDBset showed that the algorithm’s histogram usually has apeak at the correct tempo but that this peak is not the high-est. The reason lays in the fact that this set contains mu-sic with breakbeats and strong syncopation. Unfortunately,the neural network often identiﬁes these syncopated notesas beats. Contrary to single or infrequently misalignedbeats, the comb ﬁlter is not able to correct regularly recur-ring misalignments. E.g. in drum & bass music, where thebass drum usually falls on the offbeat between the third andfourth beat, this leads to additional peaks in the histogramcorresponding to 0.5 and 1.5 times the beat interval, and amuch lower peak at the correct position. Since we do notperform intelligent clustering of the histogram peaks, of-ten the rate of the downbeats is reported, which results ina tempo which is not covered by theAccuracy 2measureany more.4.4 MIREX EvaluationWe submitted the algorithm to last year’s MIREX evalua-tion.4Performance is tested on a hidden set of 140 ﬁleswith a total length of 1 hour and 10 minutes. The tempoevaluation used for MIREX is different, because for eachsong the two most dominant tempi are annotated. MIREXuses the following three evaluation metrics:P-Score[22]and the percentage of ﬁles for whichat least oneorbothofthe annotated tempi was identiﬁed correctly within a max-imum allowed deviation of±8%from the ground truth an-notations. Since MIREX requires the algorithms to reporttwo tempi with a relative strength, we adopted the peak-picking strategy outlined in Section3.3.5to simply reportthe two highest peaks.Table3gives an overview of the ﬁve best performingalgorithms (of different authors) over all years the MIREXtempo estimation task is run, together with results for al-gorithms also used for evaluation in the previous section.Our algorithm ranked ﬁrst in last year’s MIREX eval-uation and achieved the highestP-Scoreandat least onetempo reported correctlyperformance ever. The best per-4http://nema.lis.illinois.edu/nemaout/mirex2014/results/ate/Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 629NEWB¨ock [1] Gkiokas [9] Percival [25] Klapuri [17] IBT [23] Davies [4]Accuracy 1Ballroom [12, 19]0.950†0.639†\u00000.625\u00000.653\u00000.642\u00000.651\u00000.709\u0000Hainsworth [13]0.847†0.541†\u00000.667\u00000.721\u00000.752\u00000.698\u00000.739\u0000SMC [16]0.512†0.442†0.346\u00000.267\u00000.189\u00000.166\u00000.152\u0000Klapuri [17]0.7890.502\u00000.741 0.732 0.768 0.724\u00000.692\u0000GTZAN [25]0.668 0.601\u00000.716\u00000.754+0.704+ 0.599\u00000.582\u0000Songs [12]0.477 0.570+ 0.570+0.611+0.585+ 0.486 0.424Beatles [5]0.8500.700\u00000.778 0.811 0.789 0.767 0.761\u0000ACM Mirum [21, 24]0.7410.540\u00000.725 0.733 0.679\u00000.621\u00000.646\u0000RWC Popular [11]0.600 0.4500.900+0.810+ 0.770 0.750 0.770+HJDB [14]0.796 0.434\u00000.783 0.285\u00000.494\u00000.911+0.706Dataset average0.7210.543 0.563 0.638 0.636 0.637 0.617Total average0.7340.560\u00000.685\u00000.677\u00000.658\u00000.623\u00000.618\u0000Accuracy 2Ballroom [12, 19]1.000†0.997†0.981 0.953\u00000.921\u00000.921\u00000.974Hainsworth [13]0.941†0.910†0.887 0.901 0.869 0.802\u00000.878SMC [16]0.673†0.599†0.512\u00000.438\u00000.438\u00000.359\u00000.415\u0000Klapuri [17]0.937 0.907\u00000.9540.937 0.918 0.880\u00000.924GTZAN [25]0.9500.942 0.938 0.925\u00000.923\u00000.841\u00000.922\u0000Songs [12]0.9330.918 0.910 0.865\u00000.910 0.791\u00000.875\u0000Beatles [5]0.983 0.967 0.9780.9890.928 0.883 0.978ACM Mirum [21, 24]0.976 0.958\u00000.9790.972 0.967 0.915\u00000.975RWC Popular [11]0.950 0.9401.000 1.0000.990 0.9801.000HJDB [14]0.868 0.851 0.9111.000+0.864 0.991+1.000+Dataset average0.9190.899 0.916 0.896 0.871 0.837 0.893Total average0.9460.929\u00000.935\u00000.923\u00000.909\u00000.861\u00000.923\u0000Table 2:Accuracy 1andAccuracy 2results for different datasets and algorithms, with best results marked in bold and+and\u0000denoting statistical signiﬁcance compared to our results.†denote values obtained with 8-fold cross validation.P-Score\u00001 tempo both tempiAlgorithmNEW 0.876 0.9930.629Elowsson [7] 0.857 0.9430.693Gkiokas [9] 0.829 0.943 0.621Wu [28] 0.826 0.957 0.550Lartillot [20] 0.816 0.921 0.571Klapuri [17] 0.806 0.943 0.614B¨ock [1] 0.798 0.957 0.564Davies [4] 0.776 0.929 0.457Table 3: Results on the McKinney test collection used forthe MIREX evaluation.forming algorithm for theboth tempi correctevaluationwas the one submitted by Elowsson [7] in 2013, which ex-plicitly models the speed of the music and thus has a muchhigher chance to report the two annotated tempi which areinferred from human beat tapping.5. CONCLUSIONThe presented tempo estimation algorithm based on recur-rent neural networks and resonating comb ﬁlters is able toperform state-of-the-art or outperforms existing algorithmson all but one datasets investigated. Based on the highAc-curacy 2score, which also considers integer multiples andfractions of the annotated ground truth tempo, it can beconcluded that the system is able to capture a meaningfultempo in almost all cases.Additionally, we outperform many existing algorithmsw.r.t.Accuracy 1which suggests that it is advantageous touse a musically more meaningful representation than justthe onset strength of the signal – even if split into multipleaccent bands – as an input for a bank of resonating combﬁlters.In future, we want to investigate methods of perceptu-ally clustering the peaks of the histogram to report the mostrelevant tempo, as this has been identiﬁed to be the mainproblem of the new algorithm when dealing with very syn-copated music. We believe that this should increase theAccuracy 1performance considerably.The source code and additional resources can be foundat:http://www.cp.jku.at/people/boeck/ISMIR2015.html.6. ACKNOWLEDGMENTSThis work is supported by the European Union SeventhFramework Programme FP7 / 2007-2013 through theGiantSteps project (grant agreement no. 610591) and theAustrian Science Fund (FWF) project Z159. We wouldlike to thank the authors of the other algorithms for shar-ing their code or making it publicly available.630 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]S. B¨ock and M. Schedl. Enhanced Beat Tracking withContext-Aware Neural Networks. InProc. of the 14th Inter-national Conference on Digital Audio Effects (DAFx), pages135–139, Paris, France, 2011.[2]S. B¨ock, F. Krebs, and G. Widmer. A multi-model approachto beat tracking considering heterogeneous music styles. InProc. of the 15th International Society for Music InformationRetrieval Conference (ISMIR), Taipei, Taiwan, 2014.[3]A. T. Cemgil, B. Kappen, P. Desain, and H. Honing. Ontempo tracking: Tempogram Representation and Kalman ﬁl-tering.Journal of New Music Research, 28:4:259–273, 2001.[4]M. E. P. Davies and M. D. Plumbley. Context-dependentbeat tracking of musical audio.IEEE Transactions on Audio,Speech, and Language Processing, 15(3):1009–1020, 2007.[5]M. E. P. Davies, N. Degara, and M. D. Plumbley. Evalu-ation methods for musical audio beat tracking algorithms.Technical Report C4DM-TR-09-06, Centre for Digital Mu-sic, Queen Mary University of London, 2009.[6]S. Dixon. Automatic extraction of tempo and beat fromexpressive performances.Journal of New Music Research,30:39–58, 2001.[7]A. Elowsson, A. Friberg, G. Madison, and J. Paulin.Modelling the speed of music using features from har-monic/percussive separated audio. InProc. of the 14th Inter-national Society for Music Information Retrieval Conference(ISMIR), Curitiba, Brazil, 2013.[8]M. Gainza and E. Coyle. Tempo detection using a hybridmultiband approach.IEEE Transactions on Audio, Speech,and Language Processing, 19(1):57–68, 2011.[9]A. Gkiokas, V. Katsouros, G. Carayannis, and T. Stafylakis.Music tempo estimation and beat tracking by applying sourceseparation and metrical relations. InProc. of the 37th Interna-tional Conference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 421–424, Kyoto, Japan, 2012.[10]A. Gkiokas, V. Katsouros, and G. Carayannis. ReducingTempo Octave Errors by Periodicity Vector Coding AndSVM Learning. InProc. of the 13th International Societyfor Music Information Retrieval Conference (ISMIR), pages301–306, Porto, Portugal, 2012.[11]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.RWC Music Database: Popular, Classical, and Jazz MusicDatabases. InProceedings of the 3rd International Confer-ence on Music Information Retrieval (ISMIR 2002), pages287–288, Paris, France, 2002.[12]F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis,C. Uhle, and P. Cano. An experimental comparison of au-dio tempo induction algorithms.IEEE Transactions on Audio,Speech, and Language Processing, 14(5):1832–1844, 2006.[13]S. Hainsworth and M. Macleod. Particle ﬁltering applied tomusical tempo tracking.EURASIP Journal on Applied SignalProcessing, 15:2385–2395, 2004.[14]J. Hockman and I. Fujinaga. Fast vs slow: Learning tempooctaves from user data. InProc. of the 11th International So-ciety for Music Information Retrieval Conference (ISMIR),pages 231–236, Utrecht, Netherlands, 2010.[15]J. Hockman, M. E. Davies, and I. Fujinaga. One in the jungle:Downbeat detection in hardcore, jungle, and drum and bass.InProc. of the 13th International Society for Music Infor-mation Retrieval Conference (ISMIR), pages 169–174, Porto,Portugal, 2012.[16]A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. L. Oliveira,and F. Gouyon. Selective sampling for beat tracking evalu-ation.IEEE Transactions on Audio, Speech, and LanguageProcessing, 20(9):2539–2548, 2012.[17]A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis of themeter of acoustic musical signals.IEEE Transactions on Au-dio, Speech, and Language Processing, 14(1):342–355, 2006.[18]F. Korzeniowski, S. B¨ock, and G. Widmer. Probabilistic ex-traction of beat positions from a beat activation function. InProc. of the 15th International Society for Music InformationRetrieval Conference (ISMIR), pages 513–518, Taipei, Tai-wan, 2014.[19]F. Krebs, S. B¨ock, and G. Widmer. Rhythmic pattern model-ing for beat and downbeat tracking in musical audio. InProc.of the 14th International Society for Music Information Re-trieval Conference (ISMIR), pages 227–232, Curitiba, Brazil,2013.[20]O. Lartillot, D. Cereghetti, K. Eliard, W. J. Trost, M.-A. Rap-paz, and D. Grandjean. Estimating tempo and metrical fea-tures by tracking the whole metrical hierarchy. InProc. of the3rd International Conference on Music & Emotion (ICME),Jyv¨askyl¨a, Finland, 2013.[21]M. Levy. Improving perceptual tempo estimation with crowd-sourced annotations. InProc. of the 12th International So-ciety for Music Information Retrieval Conference (ISMIR),pages 317–322, Miami, USA, 2011.[22]M. F. McKinney, D. Moelants, M. E. P. Davies, and A. Kla-puri. Evaluation of Audio Beat Tracking and Music TempoExtraction Algorithms.Journal of New Music Research,36(1):1–16, 2007.[23]J. Oliveira, F. Gouyon, L. G. Martins, and L. P. Reis. IBT:a real-time tempo and beat tracking system. InProc. of the11th International Society for Music Information RetrievalConference (ISMIR), Utrecht, Netherlands, 2010.[24]G. Peeters and J. Flocon-Cholet. Perceptual tempo estimationusing GMM-regression. InProceedings of the second inter-national ACM workshop on Music information retrieval withuser-centered and multimodal strategies, pages 45–50, 2012.[25]G. Percival and G. Tzanetakis. Streamlined tempo estimationbased on autocorrelation and cross-correlation with pulses.IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing, 22(12):1765–1776, 2014.[26]E. D. Scheirer. Tempo and beat analysis of acoustic musi-cal signals.The Journal of the Acoustical Society of America,103(1):588–601, 1998.[27]G. Tzanetakis and P. Cook. Musical genre classiﬁcation ofaudio signals.IEEE Transactions on Speech and Audio Pro-cessing, 10(5):293–302, 2002.[28]F.-H. F. Wu and J.-S. R. Jang. A supervised learning methodfor tempo estimation of musical audio. In22nd Mediter-ranean Conference of Control and Automation (MED), pages599–604, Palermo, Italy, 2014.[29]J. Zapata and E. G´omez. Comparative evaluation and com-bination of audio tempo estimation approaches. In A. E. So-ciety, editor,AES 42nd Conference on Semantic Audio, Il-menau, Germany, 2011. Audio Engineering Society, AudioEngineering Society.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 631"
    },
    {
        "title": "An Audio to Score Alignment Framework Using Spectral Factorization and Dynamic Time Warping.",
        "author": [
            "Julio José Carabias-Orti",
            "Francisco J. Rodríguez-Serrano",
            "Pedro Vera-Candeas",
            "Nicolás Ruiz-Reyes",
            "Francisco J. Cañadas-Quesada"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418371",
        "url": "https://doi.org/10.5281/zenodo.1418371",
        "ee": "https://zenodo.org/records/1418371/files/Carabias-OrtiRV15.pdf",
        "abstract": "In this paper, we present an audio to score alignment framework based on spectral factorization and online Dy- namic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factoriza- tion (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposi- tion method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be inter- preted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance.",
        "zenodo_id": 1418371,
        "dblp_key": "conf/ismir/Carabias-OrtiRV15",
        "keywords": [
            "audio",
            "score",
            "alignment",
            "spectral",
            "factorization",
            "online",
            "Dynamic",
            "Time",
            "Warping",
            "NMF"
        ],
        "content": "AN AUDIO TO SCORE ALIGNMENT FRAMEWORK USING SPECTRALFACTORIZATION AND DYNAMIC TIME WARPINGJ.J. Carabias-Orti1F.J. Rodriguez-Serrano2P. Vera-Candeas2N. Ruiz-Reyes2F.J. Ca˜nadas-Quesada21Music Technology Group (MTG), Universitat Pompeu Fabra, Spain2Polytechnical School of Linares, Universidad de Jaen, Spainjulio.carabias@upf.eduABSTRACTIn this paper, we present an audio to score alignmentframework based on spectral factorization and online Dy-namic Time Warping (DTW). The proposed frameworkhas two separated stages: preprocessing and alignment.In the ﬁrst stage, we use Non-negative Matrix Factoriza-tion (NMF) to learn spectral patterns (i.e. basis functions)associated to each combination of concurrent notes in thescore. In the second stage, a low latency signal decomposi-tion method with ﬁxed spectral patterns per combination ofnotes is used over the magnitude spectrogram of the inputsignal resulting in a divergence matrix that can be inter-preted as the cost of the matching for each combinationof notes at each frame. Finally, a Dynamic Time Warping(DTW) approach has been used to ﬁnd the path with theminimum cost and then determine the relation between theperformance and the musical score times. Our frameworkhave been evaluated using a dataset of baroque-era piecesand compared to other systems, yielding solid results andperformance.1. INTRODUCTIONIn this work, we address the problem of audio-to-scorealignment (or score matching), which is the task of syn-chronizing an audio recording of a musical piece with thecorresponding symbolic score. There are two approachesto this problem, often called “ofﬂine” and “online” align-ment. In ofﬂine alignment, the whole performance is ac-cessible for the alignment process, i.e. it allows us to “lookinto the future” while establishing the matching. This isinteresting for applications that do not require the real-time property such as Query-by-Humming, intelligent au-dio editors and as a front-end for many music informationretrieval (MIR) systems. Online alignment, also knownas score following, processes the data in realtime as thec\u0000J.J. Carabias-Orti, F.J. Rodriguez-Serrano, P. Vera-Candeas, N. Ruiz-Reyes, F.J. Ca˜nadas-Quesada. Licensed under a Cre-ative Commons Attribution 4.0 International License (CC BY 4.0).At-tribution:J.J. Carabias-Orti, F.J. Rodriguez-Serrano, P. Vera-Candeas,N. Ruiz-Reyes, F.J. Ca˜nadas-Quesada. “An Audio to Score AlignmentFramework using Spectral Factorization and Dynamic Time Warping”,16th International Society for Music Information Retrieval Conference,2015.signal is acquired. This tracking is very useful for appli-cations such as automatic page turning, automated com-puter accompaniment of a live soloist, synchronization oflive sound processing algorithms for instrumental electroa-coustic composition or the control of visual effects syn-chronized with the music (e.g. stage lights or opera super-titles).Audio-to-score alignment is traditionally performed intwo steps: feature extraction and alignment. On the onehand, the features extracted from the audio signal charac-terize some speciﬁc information about the musical content.Different representations of the audio frame have beenused such as the output of a short-time Fourier transform(STFT) [1], auditory ﬁlter bank responses [2], chroma or”chroma-like” vectors [3, 4], multi-pitch analysis infor-mation [5–8]. On the other hand, the alignment is per-formed by ﬁnding the best match between the feature se-quence and the score. In fact, most systems rely on costmeasures between events in the score and in the perfor-mance. Two methods well known in speech recognitionhave been extensively used in the literature: statistical ap-proaches (e.g. HMMs) [6–11], and dynamic time warping(DTW) [3, 12, 13].In this paper we propose an audio to score frameworkbased on two stages: preprocessing and alignment. Onthe ﬁrst stage, we analyze the provided MIDI score to de-ﬁne the set of combinations of concurrent notes and thetransitions between them (i.e. the different states of theprovided MIDI). Then the score is converted into a ref-erence audio signal using a synthesizer software and weuse a method based on Non-Negative Matrix Factorization(NMF) with Beta-divergence to learn spectral patterns (i.e.basis functions) for each combination of notes. A simi-lar approach was used by Fritsch and Plumbey in [14], butthey use one component per instrument and note plus someextra-components to model the residual sounds. NMF wasalso used by Cont [8] as a multi-pitch estimator which de-ﬁnes the observation model. Joder et al. [10] also deﬁned aset of template vectors for each combination of concurrentnotes but directly from the score (i.e. without using audiosynthesis). The combination templates are obtained as alinear mapping of individual notes trained patterns usingseveral representations. On the second stage, alignmentis performed in two steps. First, the matching measure be-tween events in the score and in the performance is deﬁned.742Concretely, a divergence (i.e. cost) matrix is estimated us-ing a low latency signal decomposition method previouslydeveloped by the authors in [15] that uses the spectral pat-terns ﬁxed from the previous stage. Finally a DTW strat-egy has been used to ﬁnd the path with the minimum costand then determine the relation between the performanceand the musical score times. Both, ofﬂine and online DTWapproaches are implemented as in [23] and [13], respec-tively.The structure of the rest of the paper is as follows. InSection 2, we brieﬂy review the DTW principles. In Sec-tions 3 the proposed audio to score framework is explained.In Section 4, the evaluation set-up is presented and, in Sec-tion 5 the proposed method has been tested and comparedwith other reference systems. Finally, we summarize thework and discuss future perspectives in Section 6.2. DTW BACKGROUNDDTW is a technique for aligning two time series or se-quences. The series are represented by2vectors of fea-turesU=u1,. . . ,ui,. . . uIandV=v1,. . . ,vj,. . . ,vJwhereiandjare the point indices in the time series.IandJrepresent the length of time seriesUandV, respectively.As a dynamic programming technique, it divides the prob-lem into several sub-problems, each of which contribute incalculating the distance (or cost function) cumulatively.The ﬁrst stage in the DTW algorithm is to ﬁll a localdistance matrix (a.k.a cost matrix)Das follows:D(i, j)= (ui,vj)(1)where matrixDhasIxJelements which represent thematch cost between every two points in the time series.The cost function could be any cost function that returnscost0for a perfect match, and a positive value otherwise(e.g. euclidean distance).In the second stage (forward step), a warping matrixCis ﬁlled recursively as:C(i, j)=m i n8<:C(i, j\u0000cj)+D(i, j)C(i\u0000ci,j)+D(i, j)C(i\u0000ci,j\u0000cj)+\u0000D(i, j)9=;(2)whereciandcjare step size at each dimension and rangefrom1to↵iand1to↵j, respectively.↵iand↵jarethe maximum step size at each dimension. Parameter\u0000controls the bias toward diagonal steps.C(i, j)is thecost of the minimum cost path from(1,1)to(i, j), andC(1,1) =D(1,1).Finally, in the last stage (traceback step), the minimumcost pathw=w1,. . . ,wk,. . . ,wKis obtained by tracing therecursion backwards fromC(I,J). Eachwkis an orderedpair(ik,jk)such that(i, j)2wmeans that the pointsuiandvjare aligned. Moreover, the path has to satisfy thefollowing three conditions: i)wis bounded by the ends ofboth sequences, ii)wis monotonic and iii)wis continu-ous.\nFigure 1:Block diagram of the proposed system3. SYSTEM OVERVIEWThe proposed framework for audio-to-score alignment ispresented in Figure 1. As can be seen, the framework hastwo stages. First, the preprocessing stage must be com-puted beforehand and only the MIDI score is required.Then, once the parameters are learned, alignment can becomputed in realtime.3.1 Preprocessing StageIn this stage, the parameters for the alignment are learnedfrom the score, which must be provided beforehand usingMIDI representation. This stage is performed in two suc-cessive steps: states deﬁnition and spectral patterns learn-ing, as detailed below.3.1.1 States DeﬁnitionThe aim of this step is to adequately organize the informa-tion given by the score to be used for alignment purposes.First of all, the binary ground-truth transcription ma-trixGT(n, ⌧)(see Figure 2(a)) is inferred from the MIDIscore, where⌧is the time in frames referenced to the score(MIDI time) andnare the notes in MIDI scale. In Fig-ure 2(a) the MIDI score involves just one instrument (apiano) but more instruments can be deﬁned in a score. Forthose cases thenindex refers to each note of the differentinstruments. Consequently, the number of total notes fora composition,N, is obtained as the sum of the numberof different notes per instrument. The score can be inter-preted as a consecutive sequence ofMstates. Each statemis deﬁned by its combination of concurrent notes in thescore (for all instruments). Also, the score informs aboutthe time changes from one state to the next state. In fact,a score follower must determine the time (referenced tothe input signal) of all transitions between states. Thereare onlyKunique combination of notes in a score whereKMbecause some states represent the same combina-tion of notes.From the ground-truth transcription matrixGT(n, ⌧),Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 743we obtain the following decomposition of binary matrixesGT(n, ⌧)=Q(n, k)R(k,⌧)(3)whereQ(n, k)is the notes-to-combination matrix,ktheindex of each unique combination of notes andR(k,⌧)represents the activation of each combination in MIDItime. In Figure 2(b), the note-to-combination matrixQ(n, k)is represented. This matrix contains the notesbelonging to each combination but no information aboutMIDI time. Conversely,R(k,⌧)matrix retains the MIDItime activation per combination but no information aboutthe notes active per combination, as can be seen in Figure2(d).In order to obtain the information for states requiredto perform the alignment, the notes-to-combination matrixQ(n, k)is further decomposed asQ(n, k)=S(n, m)H(m, k)(4)whereS(n, m)is the notes-to-state matrix,mthe indexfor the states,Mthe number of states andH(m, k)rep-resents the unique combinationkof notes active at eachstatem. In Figure 2(c), the notes-to-state matrixS(n, m)is represented, this matrix contains the notes belonging toeach state, whileH(m, k)matrix informs about the com-binations active at each state, as can be seen in Figure 2(e).The matrixes here deﬁned will be used in the next stagesto perform the alignment and are computed from the MIDIscore.3.1.2 Spectral Patterns LearningWhen a signal frame is given to a score follower, the ﬁrststep should be the computation of a similarity measure be-tween the current frame and the different combinations ofnotes deﬁned by the score. Our approach is to compute adistance (or divergence) between the frequency transformof the input and just one spectral pattern per combinationof notes. A spectral pattern is here deﬁned as a ﬁxed spec-trum which is learned from a signal with certain charac-teristics. The use of only one spectral pattern per com-bination allows us to compute the divergence with a lowcomplexity signal decomposition method. This means thatour method must learn in advance the spectral pattern as-sociated to each unique combination of notes for the score.To this end, a state-of-the-art supervised method basedon Non-Negative Matrix Factorizacion (NMF) with Beta-divergence and Multiplicative Update (MU) rules [15] isused, but in this work, we propose to apply it on syntheticsignal generated from the MIDI score1instead of the realaudio performance.First of all, let us deﬁne the signal model asY(f,⌧)⇡ˆY(f,⌧)=B(f,k)G(k,⌧)(5)whereY(f,⌧)is the magnitude spectrogram of thesynthetic signal,ˆY(f,⌧)is the estimated spectrogram,G(k,⌧)matrix represents the gain of the spectral pattern1MIDI synthetic signals are generated using Timidity++ with the Flu-idR3 GM soundfont on Mac OSfor combinationkat frame⌧, andB(f,k)matrix, fork=1,. . . ,K, represents the spectral patterns for all thecombinations of notes deﬁned in the score.When the parameters are restricted to be non-negative,as it is the case of magnitude spectra, a common way tocompute the factorization is to minimize the reconstructionerror between the observed spectrogram and the modeledone.The most popular cost functions are the Euclidean(EUC) distance, the generalized Kullback-Leibler (KL)and the Itakura-Saito (IS) divergences.Besides, the Beta-divergence (see eq. 6) is another com-monly used cost function that includes in its deﬁnition thethree previously mentioned EUC (\u0000=2), KL (\u0000=1) andIS (\u0000=0) cost functions.D\u0000(x|ˆx)=8><>:xlogxˆx\u0000x+ˆx\u0000=1xˆx+l o gxˆx\u00001\u0000=01\u0000(\u0000\u00001)\u0000x\u0000+(\u0000\u00001)ˆx\u0000\u0000\u0000xˆx\u0000\u00001\u0000otherwise,(6)In order to obtain the model parameters that minimizethe cost function, Leeet al.[18] proposes an iterative algo-rithm based on MU rules. Under these rules,D\u0000(Y(f,⌧)|ˆY(f,⌧))is shown to be non-increasing at each iterationwhile ensuring non-negativity of the bases and the gains.Details are omitted to keep the presentation compact, forfurther information please read [18, 19]. For the model ofeq. (5), multiplicative updates which minimize the Beta-divergence are deﬁned asB(f,k) B(f,k)\u0000⇣Y(f,⌧)\u0000ˆY\u0000\u00002(f,⌧)⌘GT(⌧,k)⇣ˆY\u0000\u00001(f,⌧)⌘GT(⌧,k)(7)G(k, ⌧) G(k, ⌧)\u0000B(f,k)⇣Y(f,⌧)\u0000ˆY\u0000\u00002(f,⌧)⌘B(f,k)⇣ˆY\u0000\u00001(f,⌧)⌘(8)where operator\u0000indicates Hadamard product (orelement-wise multiplication), division and power are alsoelement-wise operators and(·)Tdenotes matrix transposi-tion.Finally, the method to learn the spectral patterns foreach state is described in Algorithm 1.Algorithm 1Method for learning spectral patterns combina-tions1InitializeG(k, ⌧)as the combinations activation matrixR(k, ⌧)andB(f,k)with random positive values.2Update the bases using eq. (7).3Update the gains using eq. (8).4Normalize each spectral pattern ofB(f,k)to the unit\u0000-norm.5Repeat step 2 until the algorithm converges (or maximumnumber of iterations is reached).As explained in Section 3.1.1,R(k,⌧)is a binary com-bination/time matrix that represents the activation of com-binationkat frame⌧of the training data. Therefore, ateach frame, the active combinationkis set to one and therest are zero. Gains initialized to zero will remain zero,and therefore the frame becomes represented with the cor-rect combination.744 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Time (MIDI)MIDI Note500100015002000344454(a) MIDI Ground-Truth TranscriptionGT(n, ⌧)\nNotes CombinationsMIDI note102030344454(b) Notes-to-combination matrixQ(n, k)\nMIDI StatesMIDI Note10203040344454(c) Notes-to-state matrixS(n, m)\nTime (MIDI)Notes Combinations500100015002000102030(d) Combinations activ. matrixR(k, ⌧)\nNotes CombinationsMIDI States1020302040(e) States-to-combination matrixH(m, k)Figure 2:Music signal from the test database in Section4(“01-AchGottundHerr”). (a) MIDI Ground-Truth TranscriptionGT(n, ⌧).(b) Notes-to-combination matrixQ(n, k). (c) Notes-to-state matrixS(n, m). (d) Combinations activation matrixR(k, ⌧). (e) States-to-combination matrixH(m, k).3.2 Alignment StageIn this stage, the alignment between the score and the au-dio performance is accomplished in realtime using the in-formation from the preprocessing stage.3.2.1 Observation ModelAs explained in Section 3.1.2, the spectral patternsB(f,k)for theKdifferent combinations of notes are learned in ad-vance using a MIDI synthesizer and kept ﬁxed. Each spec-tral pattern models the spectrum of a unique combination.Now, the aim is to compute the gain matrixG(k,t)andthe cost matrixD(⌧,t)that measures the suitability of eachcombination of notes belonging to each MIDI time⌧tobe active at each framet(referenced to the signal input)by analyzing the similarity between the spectral patternsB(f,k)and the input signal spectrogram2. From the costmatrixD(⌧,t), a classical DTW approach can be appliedto compute the alignment path.In this work, we propose to perform the factorization us-ing the realtime single-pitch constrained method proposedin [15]. Although this method was designed to address mu-sic transcription of monophonic signals, it can be adaptedfor audio to score alignment of polyphonic signals becauseonly one combination will be active at a time. In this tran-scription method, the optimum combinationkoptis cho-sen to minimize the Beta-divergence function at frametunder the assumption that only one gain is non-zero ateach frame. Taking the combinations as the index of gainsG(k,t), this assumption is fair because only a unique com-binationkof notes is active at each time (at least whenproducing the audio signal).Thus, the signal model with the single-combinationconstraint for the signal input vector at timet,xt(f), isdeﬁned as follows.xt(f)⇡ˆxkopt,t(f)=gkopt,tbkopt(f)(9)2Note that we are usingXandtinstead ofYand⌧to represent thesignal magnitude spectrogram and the time frames to distinguish betweenreal world and synthetic signals.whereˆxkopt,t(f)is the modeled signal for the optimumcombinationkoptat framet.kopt(t) = arg mink=1,...,KD\u0000(xt(f)|gk,tbk(f))(10)The signal model in eq. (9) assumes that when combi-nationkis active all other combinations are inactive and,therefore, the gaingk,tis just a scalar and represents thegain of thekcombination. The model of eq. (10) al-lows the gains to be directly computed from the input dataX(f,t)and the trained spectral patternsB(f,k)withoutthe need of an iterative algorithm and thus, reducing thecomputational requirements. To obtain the optimum com-bination at each frame, we must ﬁrst compute the diver-gence obtained by the projection of each combination ateach frame and then select the combination that achievesthe minimum divergence as the optimum combination ateach frame.In the case of Beta-divergence, the cost function forcombinationkand frametcan be formulated asD\u0000(xt(f)|gk,tbk(f)) =Xf1\u0000(\u0000\u00001)(x\u0000t(f)+(\u0000\u00001)(gk,tbk(f))\u0000\u0000\u0000xt(f)(gk,tbk(f))\u0000\u00001)(11)The value of the gain for combinationkand frametis then computed by minimizing eq. (11). Conveniently,this minimization has a unique non-zero solution due tothe scalar nature of the gain for combinationkand framet(see more details in [15]).gk,t=Pfxt(f)bk(f)(\u0000\u00001)Pfbk(f)\u0000(12)Finally, the divergence matrix for each combination ateach frame is deﬁned as:\u0000(k,t)=D\u0000(xt(f)|gk,tbk(f))(13)where\u0000can take values in the range2[0,2].Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 745As can be inferred, the divergence matrix\u0000(k,t)pro-vides us information about the similitude of each combina-tionkspectral pattern with the real signal spectrum at eachframet. Using this information, we can directly computethe cost matrix between the MIDI time⌧and the time ofthe input signaltasD(⌧,t)=RT(⌧,k)\u0000(k,t)(14)whereR(k,⌧)is the combinations activation matrix de-ﬁned in Section 3.1.1 and superscript “T” stands for matrixtransposition. The process is detailed in Algorithm 2.Algorithm 2Divergence matrix computation method1InitializeB(f,k)with the values learned in Section 3.1.2.2fort=1 to Tdo3fork=1 to Kdo4Compute the gainsgk,tusing eq. (12).5Compute the current value the divergence matrix\u0000(k, t)using eq. (13).6end for7end for8Compute the cost matrixD(⌧,t)between MIDI time and in-put signal time using (14).To resume, we propose the use the divergence matrixD(⌧,t)as the input of the DTW algorithm in order to per-form the alignment.3.2.2 Path ComputationWe here propose to use a DTW based method to per-form the alignment using the cost matrixD(⌧,t)obtainedin eq. (14). This cost matrix is computed from the in-put signalX(f,t)and the “synthetic” spectral patterns percombinationB(f,k)explained in Section 3.1.2. The term“synthetic” comes from the fact that the spectral patternsB(f,k)are computed from the score using a MIDI syn-thesizer.a)Ofﬂine approach: This approach represents the clas-sical ofﬂine alignment using DTW. To this end, we haveused the code from [23]. The forward step is computed asin the classical DTW (see eq. (2)). In this experiment,ciandcjrange from1to4in order to allow4times fasterspeed of interpretation. Finally the optimum path is ob-tained by tracing the recursion backwards fromC(I,J)asin the original formulation of DTW (see Section 2).b)Online approach: The online algorithm differs froman standard (i.e. ofﬂine) DTW algorithm in some points.Firstly, the signal is partially unknown (or the future ofthe signal is not known when making the alignment de-cisions), so the global path constraints cannot be directlyimplemented, in other words, the recursion backwards cannot be traced from the last frameTof the signal. Secondly,if some latency (i.e. delay in the decision) is permitted,the recursion backwards can be traced in equally spacedframes of the input signal making the latency equal to thedifference in time of the frame when the backtracking isdone and the input signal frame. Finally, in order to runin realtime, the complete algorithm should not increase thecomplexity with the length of the signal.In this work, we used the online scheme proposed byDixon in [13]. In fact, Dixon’s algorithm calculates an“adaptive diagonal” through the cost matrix by seeking thebest path considering a searching band with a ﬁxed width.Here, we propose an online algorithm with a ﬁxed latencyof just one frame. In order to obtain this low latency, nobacktracking is allowed, taking the decision directly fromthe forward information at each framet. As a consequenceof the low latency of online algorithms (apart from thecomplexity reduction), the obtained results are degradedfrom their ofﬂine counterparts. In fact, for those situationsin which a higher latency can be supported, delaying thedecision in time using a limited traceback can improve theobtained results of the online algorithms.4. EXPERIMENTAL SETUPa)Time-Frequency representation: In this paper we use alow-level spectral representation of the audio data whichis generated from a windowed FFT of the signal. A Han-ning window with the size of128ms, and a hop size of10ms is used (for both synthetic and real-world signals).Here, we use the resolution of a single semitone as in [21].In particular, we implement the time-frequency represen-tation by integrating the STFT bins corresponding to thesame semitone.b)Evaluation metrics: We have used the same evaluationmetrics as in the MIREX Score Following task. Detailedinformation can be found in [22]. For each piece, alignedrate (AR) or precision is deﬁned as the proportion of cor-rectly aligned notes in the score and ranges from0to1.Anote is said to be correctly aligned if its onset does not de-viate more than a threshold (a.k.a tolerance window) fromthe reference alignment. Missed notes are events that arepresent in the reference but not reported. Recognized noteswhose onsets are far from the given threshold are consid-ered misaligned notes.c)Dataset: The dataset used to evaluate our method iscomprised of excerpts from10human played J.S. Bachfour-part chorales. The audio ﬁles are sampled from realmusic performances recorded at44.1kHz that are30sec-onds in length per ﬁle. Each piece is performed by a quartetof instruments: violin, clarinet, tenor saxophone and bas-soon. Each musician’s part was recorded in isolation. In-dividual lines were then mixed to create10performanceswith four-part polyphony. Ground-truth alignment is pro-vided for both, individual sources and mixture, the latterassuming constant tempo between annotated beats and aperfect synchronization between the musicians. More in-formation about this dataset can be found in [6].5. RESULTSTo analyze the performance of the proposed (ofﬂine andonline) methods in Section 3. Evaluation has been per-746 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Threshold (ms - Log scale)50 200 1000 2000Precision\n0.30.40.50.60.70.80.91\nProposed OfflineEllis Offline DTWProposed OnlineDixon Online DTWSoundprismOracleFigure 3:Precision values in function of the tolerance windowformed using the metrics detailed in Section 4. The pro-posed systems are compared with four reference methodsthat are detailed below: a) Ellis’ Ofﬂine DTW [23] [24], b)Dixon’s Online DTW [13], c) Soundprism [6] and d) Ora-cle. Note that the latter is not a score following system butthe provided aligned MIDI score assuming constant tempobetween annotated beats and perfect synchronization be-tween musicians as explained in Section 4. The evalua-tion of this oracle information is very interesting to ana-lyze the deviation of the different instrument performancesbetween themselves and also to measure the best perfor-mance that can be obtained by score followers that are onlycapable of aligning on a global level (i.e., cannot detect theonset positions of individual notes). Note that there areseveral works that attempt to reﬁne the alignment by syn-chronizing the onsets/offsets individually for each note ina post-processing stage [25–27].In a ﬁrst experiment, we have evaluated the precision ofthe analyzed methods as a function of the onset deviationthreshold. To this end, the threshold value was varied from50to2000ms in50ms steps. Obtained results are plottedin Figure 3. As can be seen, the Oracle method, which canbe considered as the upper bound for score followers per-forming global alignment, requires around200ms thresh-old to obtain a perfect alignment. This value comes fromthe difference between the ground-truth alignment for eachinstrument played in isolation and the global ground-truthof the whole mixture, obtained by interpolating the anno-tated beat times of each audio.In general, our ofﬂine approach obtain the best resultsin terms of precision. In fact, our ofﬂine approach clearlyoutperforms Ellis’ ofﬂine approach, mainly due to the fac-torization based feature extraction stage. Regarding the on-line methods, our online approach and Soundprism obtainsimilar results on average than the Ellis’ ofﬂine approachand clearly outperform Dixon’s online approach. Sound-prism seems to perform better when using lower thresholdvalues while our online approach allows convergence tothe optimum alignment as the threshold is increased.In a second experiment (see Table 1), we evaluate theperformance of the proposed methods as a function of thepolyphony. A ﬁxed threshold (a.k.a tolerance window) of200ms is used because, as illustrated in Figure 3, this valuerepresents the difference between isolated instruments andPolyPrecision Miss Missalign Av offset Av|offset|Std Offset294,590,00 5,41 -11,2733,4344,38Prop. Ofﬂine394,750,00 5,25 -11,7834,2544,89494,500,00 5,50 -11,0935,1646,51290,57 0,00 9,42 66,90 71,28 47,30Ellis Ofﬂine390,39 0,00 9,60 65,67 71,53 50,24489,80 0,00 10,19 66,97 72,62 49,93288,440,00 11,56 -41,18 57,78 56,40Prop. Online390,130,00 9,86 -42,96 60,56 57,65490,70 0,00 9,30 -44,15 62,97 58,58281,69 0,00 18,31 53,93 70,51 67,02Dixon Online383,17 0,00 16,83 53,88 70,65 67,02483,94 0,00 16,06 51,29 71,64 70,26283,01 0,00 16,99 -25,9048,0855,36Soundprism388,81 0,00 11,19 -21,2343,7351,54493,500,00 6,50 -22,0542,9149,132100,00 0,00 0,00 6,15 32,7942,47Oracle3100,00 0,00 0,00 6,09 32,6743,084100,00 0,00 0,00 6,07 32,5843,38Table 1:Audio-to-score results as a function of polyphony interms of piecewise precision (%). Offset values in ms. The boldpercentage shows the best result for each measure.mixture ground-truth alignment.As explained in the previous section, ofﬂine methodsperform better in general than the online ones. The pro-posed ofﬂine method obtains the best results among thecompared methods and has demonstrated to be robustagainst polyphony in the analyzed dataset (polyphony 2 to4). Regarding the online methods, our online approach andSoundprism obtain similar results on average and clearlyoutperforms Dixon’s online approach, although the formerseems to be more robust against the polyphony.In relation to the offset, the oracle solution exhibits theminimum possible std offset due to the differences in start-ing times for the same states between musicians. More-over, our ofﬂine approach and the online Soundprism havethe lower average offset values which means that bothmethods are more responsive and thus provide better re-sults when dealing with lower thresholds.6. CONCLUSIONSIn this paper we present a score following framework basedon spectral factorization and DTW. Spectral factorizationis used to learn spectral patterns for each combination ofconcurrent notes in the MIDI score. Then, a cost matrixis computed using the divergence matrix obtained using anon-iterative signal decomposition method previously de-veloped by the authors in [15] that has been tuned to per-form the projection of each combination of notes. Finally,a DTW strategy is performed in an ofﬂine and online man-ner. The proposed ofﬂine and online approaches have beentested using a dataset with different polyphony levels (from2to4) and compared them with other reference methods.On average, our approaches (ofﬂine and online) obtain thebest results in terms of precision within the compared of-ﬂine and online approaches, respectively, and has demon-strated to be robust agains the analyzed polyphony.In the future we plan to track the tempo changes in or-der to enforce a certain degree of continuity in the onlinedecisions. Besides, we will extend the evaluation of ourmethod using a lager dataset of a varied range of instru-ments, dynamics and different styles.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7477. REFERENCES[1]A. Cont, “A coupled duration-focused architecture forreal-time music-to-score alignment,”IEEE Trans. Pat-tern Anal. Mach. Intell., vol. 32, no. 6, pp. 974987, Jun.2010.[2]N. Montecchio and N. Orio, “A discrete ﬁlterbank ap-proach to audio to score matching for score following,”in Proc. ISMIR, 2009, pp. 495-500.[3]N. Hu, R. B. Dannenberg, and G. Tzanetakis, “Poly-phonic audio matching and alignment for music re-trieval,”in Proc. IEEE WASPAA, 2003, pp. 185188.[4]O. Izmirli and R. Dannenberg. “Understanding featuresand distance functions for music sequence alignment”.In Proceedings of ISMIR, 411- 416. 2010[5]M. Puckette, “Score following using the sung voice,”in Proc. ICMC, 1995, pp. 175178.[6]Z. Duan and B. Pardo, “Soundprism: An Online Sys-tem for Score-informed Source Separation of MusicAudio,”IEEE Journal of Selected Topics in Signal Pro-cess., vol. 5, no. 6, pp. 1205-1215, 2011.[7]P. Cano, A. Loscos, and J. Bonada, “Score-performance matching using HMMs,”in Proc. ICMC,1999, pp. 441-444.[8]A. Cont, “Realtime audio to score alignment for poly-phonic music instruments using sparse non-negativeconstraints and hierarchical hmms,”In Proceedings ofIEEE ICASSP, Toulouse. France, 2006.[9]C. Raphael, “Automatic segmentation of acoustic mu-sical signals using hidden Markov models,”IEEETrans. Pattern Anal. Machine Intell.,vol. 21, no. 4, pp.360370, Apr. 1999.[10]C. Joder, S. Essid, and G. Richard. “Learning opti-mal features for polyphonic audio-to-score alignment.”IEEE Transactions on Audio, Speech, and LanguageProcessing, 21, 10, 2118-2128, 2013[11]P. Cuvillier and A. Cont. “Coherent time modelingof Semi-Markov models with application to realtimeaudio-to-score alignment”.Proceedings of the 2014IEEE International Workshop on Machine Learningfor Signal Processing. 16, 2014.[12]N. Orio and D. Schwarz, “Alignment of monophonicand polyphonic music to a score,”in Proc. Interna-tional Computer Music Conference (ICMC), 2001.[13]S. Dixon, “Live tracking of musical performancesusing on-line time warping,”in Proc. InternationalConference on Digital Audio Effects (DAFx), Madrid,Spain, 2005, pp. 92-97.[14]J. Fritsch and M. Plumbley. “Score Informed Au-dio Source Separation using Constrained NonnegativeMatrix Factorization and Score Synthesis,”In Proc.ICASSP, Vancouver, Canada, 2013.[15]J.J. Carabias-Orti et al., “Constrained non-negativesparse coding using learnt instrument templates for re-altime music transcription,”Engineering Applicationsof Artiﬁcial Intelligence, April 2013[16]F. Itakura, ”Minimum prediction residual principle ap-plied to speech recognition,”IEEE Transactions onAcoustics, Speech and Signal Processing,vol. 23, pp.5272, 1975.[17]H. Sakoe and S. Chiba, “Dynamic programming algo-rithm optimisation for spoken word recognition,”IEEETransactions on Acoustics, Speech and Signal Process-ing, vol. 26, pp. 4349, 1978.[18]D. D. Lee and H. S. Seung, “Algorithms for Non-negative Matrix Factorization,”in Proc. of Neural In-formation Processing Systems, Denver, USA, 2000.[19]C. F´evotte and J. Idier “Algorithms for Nonnega-tive Matrix Factorization with the Beta-Divergence,”Neural Computation, vol. 23, no. 9, pp. 2421-2456,September 2011.[20]J. F. Gemmeke et al., “Exemplar-based sparse repre-sentations for noise robust automatic speech recogni-tion,”IEEE Trans. Audio, Speech and Language Pro-cessing, Volume: 19, Issue: 7, 2011.[21]J.J. Carabias-Orti et al., “Musical Instrument SoundMulti-Excitation Model for Non-Negative Spectro-gram Factorization,”IEEE Journal of Selected Topicsin Signal Processing, Vol. 5, no. 6, pp. 1144 - 1158,October 2011[22]A. Cont et al., “Evaluation of real- time audio-to-scorealignment,”in Proc. International Conference on Mu-sic Information Retrieval (ISMIR), 2007.[23]D. Ellis. Dynamic Time Warp (DTW)in Matlab, Web resource, available:http://www.ee.columbia.edu/⇠dpwe/resources/matlab/dtw/.[24]R. Turetsky and D. Ellis “Ground-Truth Transcriptionsof Real Music from Force-Aligned MIDI Syntheses”,in Proc. International Conference on Music Informa-tion Retrieval (ISMIR), 2003.[25]B. Niedermayer, and G. Widmer. “A multi-pass algo-rithm for accurate audio-to-score alignment”in Proc.International Conference on Music Information Re-trieval (ISMIR), 2010.[26]J. Devaney, “Estimating Onset and Offset Asyn-chronies in Polyphonic Score-Audio Alignment”Jour-nal of New Music Research43 (3), 266-275, 2014[27]M. Miron, J. Carabias-Orti and J. Janer. “ImprovingScore-Informed Source Separation Through Audio ToScore Note-Level Reﬁnement ”in Proc. InternationalConference on Music Information Retrieval (ISMIR),2015.748 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC) Model for Pop Cover Song Recognition.",
        "author": [
            "Ning Chen 0007",
            "J. Stephen Downie",
            "Haidong Xiao",
            "Yu Zhu",
            "Jie Zhu 0006"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416096",
        "url": "https://doi.org/10.5281/zenodo.1416096",
        "ee": "https://zenodo.org/records/1416096/files/ChenDXZZ15.pdf",
        "abstract": "Most of the features of Cover Song Identification (CSI), for example, Pitch Class Profile (PCP) related features, are based on the musical facets shared among cover versions: melody evolution and harmonic progression. In this work, the perceptual feature was studied for CSI. Our idea was to modify the Perceptual Linear Prediction (PLP) model in the field of Automatic Speech Recognition (ASR) by (a) introducing new research achievements in psychophysics, and (b) considering the difference between speech and music signals to make it consistent with human hearing and more suitable for music signal analysis. Furthermore, the obtained Linear Prediction Coefficients (LPCs) were mapped to LPC cepstrum coefficients, on which liftering was applied, to boost the timbre invariance of the resultant feature: Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC). Experimental results showed that both LPC cepstrum coefficients mapping and cepstrum lif- tering were crucial in ensuring the identification power of the MPLPLC feature. The MPLPLC feature outperformed state-of-the-art features in the context of CSI and in re- sisting instrumental accompaniment variation. This study verifies that the mature techniques in the ASR or Compu- tational Auditory Scene Analysis (CASA) fields may be modified and included to enhance the performance of the Music Information Retrieval (MIR) scheme.",
        "zenodo_id": 1416096,
        "dblp_key": "conf/ismir/ChenDXZZ15",
        "keywords": [
            "Cover Song Identification",
            "Pitch Class Profile",
            "Perceptual Feature",
            "Perceptual Linear Prediction",
            "Psychophysics",
            "Automatic Speech Recognition",
            "Speech and Music Signals",
            "LPC cepstrum coefficients",
            "Liftering",
            "Music Information Retrieval"
        ],
        "content": "MODIFIED PERCEPTUAL LINEAR PREDICTION LIFTEREDCEPSTRUM (MPLPLC) MODEL FOR POP COVER SONG RECOGNITIONNing Chen1J. Stephen Downie2Haidong Xiao3Yu Zhu1Jie Zhu41Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech., CHN2Graduate School of Library and Information Science, UIUC, USA3Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, CHN4Dept. of Electronic Engineering, Shanghai Jiao Tong University, CHNnchen@ecust.edu.cnABSTRACTMost of the features of Cover Song Identiﬁcation (CSI),for example, Pitch Class Proﬁle (PCP) related features, arebased on the musical facets shared among cover versions:melody evolution and harmonic progression. In this work,the perceptual feature was studied for CSI. Our idea wasto modify the Perceptual Linear Prediction (PLP) model inthe ﬁeld of Automatic Speech Recognition (ASR) by (a)introducing new research achievements in psychophysics,and (b) considering the difference between speech andmusic signals to make it consistent with human hearingand more suitable for music signal analysis. Furthermore,the obtained Linear Prediction Coefﬁcients (LPCs) weremapped to LPC cepstrum coefﬁcients, on which lifteringwas applied, to boost the timbre invariance of the resultantfeature: Modiﬁed Perceptual Linear Prediction LifteredCepstrum (MPLPLC). Experimental results showed thatboth LPC cepstrum coefﬁcients mapping and cepstrum lif-tering were crucial in ensuring the identiﬁcation power ofthe MPLPLC feature. The MPLPLC feature outperformedstate-of-the-art features in the context of CSI and in re-sisting instrumental accompaniment variation. This studyveriﬁes that the mature techniques in the ASR or Compu-tational Auditory Scene Analysis (CASA) ﬁelds may bemodiﬁed and included to enhance the performance of theMusic Information Retrieval (MIR) scheme.1. INTRODUCTIONCover Song Identiﬁcation (CSI) refers to the process ofidentifying an alternative version, performance, rendition,or recording of a previously recorded musical piece [26]. Ithas a wide range of applications, such as music collectionsearch and organization, music rights management and li-c\u0000Ning Chen, J. Stephen Downie, Haidong Xiao, Yu Zhu,Jie Zhu. Licensed under a Creative Commons Attribution 4.0 Interna-tional License (CC BY 4.0).Attribution:Ning Chen, J. StephenDownie, Haidong Xiao, Yu Zhu, Jie Zhu. “Modiﬁed Perceptual Lin-ear Prediction Liftered Cepstrum (MPLPLC) Model for Pop Cover SongRecognition”, 16th International Society for Music Information RetrievalConference, 2015.censes, and music creation aids. Inspired by the actualapplication requirements and researchers’ growing interestin identifying near-duplicated versions, CSI has become adynamic area of study in the Music Information Retrieval(MIR) community over the past decades. As a result, forthe ﬁrst time in 2006, the CSI task was included by the Mu-sic Information Retrieval Evaluation eXchange (MIREX),an international community-based framework for the for-mal evaluation of MIR systems and algorithms [6].Since there are many different formats of cover ver-sion, such as remastering, instrumental, mashup, live per-formance, acoustic, demo, remix, quotation, medley, andstandard, the cover version may differ from the originalin timbre, tempo, timing, structure, key, harmonization,lyrics and language, and noise [24]. What remain almostinvariable among cover versions are melody evolution andharmonic progression, which form the basis of most exist-ing CSI feature extraction algorithms. Among these fea-tures, the Pitch Class Proﬁle (PCP) (or chroma) [9] andrelated descriptors [3, 7, 19, 25, 26, 31, 33]–which can rep-resent harmonic progression directly–are robust to noise(e.g. ambient noise or percussive sounds) and indepen-dent of timbre, played instruments, loudness, and dynam-ics, have become the most widely-used features for CSI.In [7], the beat-synchronous chroma for two tracks werecross-correlated, from the results of which the sharp peaksindicating good local alignment were looked for to deter-mine the distance between them. This CSI scheme per-formed the best in the audio CSI task contest of the 2006MIREX. The Harmonic Pitch Class Proﬁle (HPCP) featureproposed in [12] shared the common properties of PCP, butsince it was only based on the peaks of the spectrum withina certain frequency band, it reduced the inﬂuence of noisyspectral components. It also took the presence of harmonicfrequencies into account and was tuning independent. TheCSI scheme based on the HPCP andQmaxsimilarity mea-sure [26,27] achieved the highest identiﬁcation accuracy inthe audio CSI task contest of the 2009 MIREX. In [19], thelower pitch-frequency cepstral coefﬁcients were discardedand the remaining coefﬁcients were projected onto chromabins to obtain the Chroma DCT-Reduced log Pitch (CRP)feature. The CRP feature achieved high degree of timbre598invariance and, thus, outperformed conventional PCP inthe context of music matching and retrieval applications.We observed that despite the promising achievements ofthe CSI technique over the last decade, the available CSIschemes cannot perform as well as the human ear does.One possible reason is that the available CSI schemes payattention solely to the musical facets (e.g. melody evo-lution and harmonic progression) that are shared amongcover versions and do not resemble the way humans pro-cess music information at all [24]. In this paper, we pro-pose a perceptually inspired model called the MPLPLCmodel to process music signals based on the PerceptualLinear Prediction (PLP) model [13] in the ASR ﬁeld. Inthe proposed scheme, we will consider equally the variousattributes of human auditory processing, the difference be-tween speech and music signals, and the requirements ofrepresenting the musical facets shared among cover ver-sions. First, the MPLPLC model uses the Blackman win-dow but not the Hamming window to weight each frameto maintain the harmonic information of the music. Sec-ond, it replaces frequency warping on the bark scale witha real ﬁlter bank equally spaced on the Equivalent Rectan-gular Bandwidth (ERB) scale to model the time and fre-quency resolution of human ears. Third, it substitutes aﬁxed equal loudness curve for a loudness model suitablefor time-varying sounds (speech or music) [11]. Fourth,the hair cell transduction model [17] takes the place ofcubic-root intensity-loudness compression to replicate thecharacteristics of auditory nerve responses, including rec-tiﬁcation, compression, spontaneous ﬁring, saturation ef-fects, and adaptation [32]. Last and most important, tomake the resulted feature (MPLPLC) suited for the CSItask, the LPCs are transformed into LPC cepstrum coef-ﬁcients to reduce the correlation between them and theirunnecessary sensitivity, the result of which is liftered toachieve some degree of timbre invariance [1, 14].The identiﬁcation power and robustness to the varia-tion in instrumental accompaniments of MPLPLC weretested on two different collections. The ﬁrst was com-posed of 502 songs and 212 cover sets and the secondconsisted of 85 cover sets whose cover versions have beenperformed by the same artist with different instrumentalaccompaniments. We observed that MPLPLC achievedhigher identiﬁcation accuracy, in terms of the Mean of Av-erage Precision (MAP), the total number of identiﬁed cov-ers in the top ﬁve (TOP-5), the mean rank of the ﬁrst iden-tiﬁed cover (RANK), and the Mean averaged ReciprocalRank (MaRR) [23]. It also achieved a higher degree of in-variance to instrumental accompaniments than the conven-tional PLP feature [13] and different PCP-related features:the beat-synchronous chroma [7], the HPCP [12, 26], andthe CRP [19]. Experimental results also veriﬁed that boththe LPC cepstrum coefﬁcients mapping and the cepstrumliftering are crucial in ensuring the identiﬁcation power ofMPLPLC.The rest of this paper is organized as follows. The signalprocessing steps involved in the proposed MPLPLC modelhave been described in detail in Section 2. The perfor-mances of the MPLPLC feature in the CSI task in com-parison with PLP and other state-of-the-art features havebeen evaluated and discussed in Section 3. Conclusionsand prospects on future work have been given in Section 4.2. MPLPLC MODELA block diagram of the MPLPLC model is shown in Figure1. The signal processing steps involved in this model arediscussed in detail as follows.2.1 Pre-processingThe input music signal is ﬁrst converted to mono, 8 kHzand 16 bits per sample version to reduce both the compu-tation time and memory requirements. Then, it is ﬁlteredby a preemphasis ﬁlter of the formH(z)=1\u0000µz\u00001(1)where the coefﬁcientµis chosen between 0.95 and 0.99.The preemphasis is needed because ﬁrst, it weakens theinﬂuence of low-frequency noise and strengthens the high-frequency signal; second, it reduces the dynamic range ofthe spectrum to make autoregressive modelling easier [4];and third, it has been proven helpful in maintaining har-monic information in audio signals [22].2.2 EnframingThe pre-processed signal is segmented into overlappingframes, denoted as{si|i=1,···,N}, and each frame iswindowed by the Blackman window [20] to get{swi|i=1,···,N}.We chose the Blackman window but not the Hammingwindow because the Blackman window has a wider main-lobe and lower highest side-lobe than the Hamming win-dow [28]. As described in the open courseAudio SignalProcessing for Music Applications1, this characteristic ofthe Blackman window helps to maintain and smooth thepeaks in the spectrum corresponding to the harmonics inthe music signal.2.3 Equal Loudness PredictingTo compensate for the frequency-dependent transmissioncharacteristics of the outer ear (pinna and ear canal), thetympanic membrane, and the middle ear (ossicular bones),each windowed frameswiis ﬁltered by an equal loudnessmodel to simulate the transfer function from the sound ﬁeldto the oval window of the cochlea [2] to getswli. In PLP,a ﬁxed equal-loudness curve is combined [13]. However,since a music signal is time-varying and has both short-term loudness (the loudness of a speciﬁc note) and long-term loudness (the loudness of a musical phase) [18], theﬁxed loudness curve is not suited to it. So, Glasberg andMoore’s [11] loudness model, which can be applied di-rectly to the sound and works for time-varying sounds, isapplied to the MPLPLC model.1https://class.coursera.org/audio-001/lecture/53Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 599Figure 1. The comparison between the PLP model (left) and MPLPLC model (right).2.4 Auditory Filter Bank ModelingTo obtain the auditory spectrum, PLP does a critical-bandintegration after a Fourier Transform (FT) [13]. The prob-lem is that frequency bin in FT is linear, so it has a constantspectral resolution, while the human ear has high spectralresolution at low frequency and low spectral resolution athigh frequency. Therefore, in the proposed scheme, a realﬁlter-bank composed ofNfchannels equidistantly spacedon the ERB [10] scale was applied to imitate the frequencyresolution of human hearing. The bandwidths of the chan-nels in the ﬁlter bank are proportional to the center fre-quencies (see Figure 2). The real ﬁlter bank can obtaina good spectral resolution at low frequencies and a goodtemporal resolution at high frequencies (like the humanear) [15]. Another advantage of the ﬁlter bank approachis that each bandpass channel is treated essentially inde-pendently, i.e., there are no global spectral constraints onthe ﬁlter bank outputs [14]. In this speciﬁc case, a Han-ning window on the frequency side was chosen2and theexperimental results showed that the type of ﬁlter has lit-tle inﬂuence on the obtained cepstral feature. The outputof thej-th channel in the ﬁlter bank for the inputswliisdenoted ass(j)wlai.2.5 Hair Cell TransductionIn PLP [13], the cubic-root amplitude compression is com-bined to approximate the power law of hearing and sim-ulate the nonlinear relation between the intensity of the2http://ltfat.sourceforge.net/doc/ﬁlterbank/erbﬁlters.php\nFigure 2. Frequency responses of the ﬁlters in the auditoryﬁlter bank, with center frequencies equally spaced between131 Hz and 3400 Hz on the ERB-rate scale.sound and its perceived loudness [29]. Meddis’s hair celltransduction model [17] is incorporated in the MPLPLCmodel to simulate the rectiﬁcation, compression, sponta-neous ﬁring, saturation effects, and adaptation characteris-tics of auditory nerve responses [32]. This operation alsohelps to reduce the spectral amplitude variation of the au-ditory spectrum, which makes it possible to do the all-polemodeling by a relative low model order [13]. The hair celltransduced version ofs(j)wlaiis denoted asˆs(j)wlai.2.6 Filter Bank Based Energy CalculationTo represent the energy distribution of the music signal oneach channel, the energy of thej-th channel for thei-thframe, denoted asgi(j), is calculated as follows:600 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015gi(j) = logLwXn=1⇣ˆs(j)wlai(n)⌘2(2)Here,ˆs(j)wlai(n),n=1,···,Lwis the element of the vec-torˆs(j)wlai. Then, the ﬁlter bank based energy of thei-thframe isgi=[gi(1),···,gi(Nf)].2.7 Autoregressive ModelingTo represent the spectral envelope of the ﬁlter bank basedenergy in a compressed form, the ﬁlter bank based en-ergygi,i=1,···,Nare modelled by apth-order allpole spectrum\u0000/Ai(z), where\u0000is constant andAi(z)=1+ai1z\u00001+···+aipz\u0000p, using the autocorrelation method[16]. Then, the LPCs of theith frame are denoted asai=[ai(1),···,ai(p)].2.8 LPC Cepstrum Coefﬁcients MappingTo reduce the correlation between them [5], the LPCsaiare further transformed into (real) LPC cepstrum coefﬁ-cients, denoted asci=[ci(1),···,ci(p)], with the follow-ing recursion formula [14]:ci(n)=\u0000ai(n)\u00001nn\u00001Xk=1(n\u0000k)ai(k)ci(n\u0000k)(3)Figure 3(a) and 3(b) show the comparison between thespectrum of ﬁlter bank based energy and its LPC smooth-ing result, and that between the spectrum of ﬁlter bankbased energy and its cepstrum smoothing result, respec-tively. It can be seen that ﬁrst, both the LPC and the cor-responding LPC cepstrum can represent the rough changetrend of the spectral envelop of the ﬁlter bank based energy,and second, the LPC smoothing does not follow the slowvariations of the ﬁlter bank based energy as well as LPCcepstrum smoothing does. This means that the LPC cep-strum mapping helps to reduce the unnecessary sensitivitythat exists in LPC smoothing results.2.9 Cepstrum LifteringIt has been proven that the variability of low quefrencyterms is primarily due to variation in transmission, speakercharacteristics, and vocal efforts of the human voice [14].As for the music, the lower quefrency is closely relatedto the aspect of timbre [19, 21, 30]. So, to boost the de-gree of timbre invariance of the proposed feature, the lif-tering window proposed in [14] [see Eq.(4)] is applied tothe LPCs ﬁrst; then, the lowerqelements of the resultare truncated to get the liftered LPCs denoted asˆci={ˆci(1),···,ˆci(p\u0000q)}.WL(n)=⇢1+p2sin(⇡np),n=1,2,···,p0,o t h e r w i s e(4)\n(a)\n(b)Figure 3. Comparison of spectral smoothing methods.3. EVALUATION3.1 Evaluation PreparationTo test the effectiveness of the MPLPLC feature in thepop CSI task, the enhancedQmaxmethod [27] (denotedasˆQmaxin this paper) was used to measure the distancebetween the MPLPLC time series of two pieces of mu-sic. The parameters chosen to calculate cross recurrenceplots [34] were embedding dimensionm= 15, time delay(in units)⌧=2and the maximum percentage of neigh-bours=0.1. Furthermore, the parameters used to com-pute a cumulative matrixQ[26] are the penalty for a dis-ruption onset\u0000o=5and the penalty for a disruption ex-tension\u0000e=0.5.Two music collections were used. The ﬁrst one (de-noted as Collection1) comprised 502 pop songs of variousstyles and genres and 212 cover sets. The average numberof covers in each cover set is 2.4, and the distribution ofthe cover set cardinality has been presented in Figure 4.Western songs and Chinese songs occupy one half of thiscollection. The second one (denoted as Collection2) is in-dependent of Collection1 and comprised 175 songs and85 cover sets. The cover versions of each cover set in Col-lection2 were pop songs performed by the same artist butwith different instrumental accompaniments. The materi-als were obtained from a personal music collection. Theidentiﬁcation accuracy and robustness against variation ininstrumental accompaniments of the MPLPLC was testedon Collection1 and Collection2, in comparison with theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 601Figure 4. Distribution of the cover set cardinality.PLP feature [13], CRP feature [19]3, Ellis’s cover songscheme [7]4, and Serr`a’s cover song scheme [27]5. Theparameters of the MPLPLC model have been listed in Ta-ble 1, and those of PLP, CRP, Ellis’s scheme, and Serr`a’sscheme are the same as those in [13], [19], [7], and [27],respectively.Table 1. Parameter setting of MPLPLC featureDescriptionValuePreemphasis parameterµ0.97Frame length464msFrame overlap116msMinimum central frequency of auditory ﬁlter133HzMaximum central frequency of auditory ﬁlter6856HzNumber of channels in auditory ﬁlter bankNf41LPC orderp16Number of cepstrum16Cepstrum truncate numberq33.2 Identiﬁcation AccuracyWe used each of the 502 songs in Collection1 as a queryand calculated the distance [27] between each query andthe remaining 501 songs based on different features. Theidentiﬁcation accuracy, in terms of TOP-5, MAP, RANK,and MaRR, obtained from the distance matrices (see Ta-ble 2) demonstrated that MPLPLC performed better thanthe conventional features in the CSI task over Collection1.One possible explanation for this result is that Collection1was composed of pop songs that included a singing voice,and due to the MPLPLC’s background in speech recog-nition, it outperformed the musical facet based featuresin representing the singing voice. As an example, westudied two versions of the songWishing We Last For-everas performed by Teresa Teng and Faye Wong, re-spectively. In these two versions, the singing voice isdominant, the instrumental accompaniments are different,and the rhythm is smoothing. The version performedby Teresa Teng includes a national instrument accompa-niment, which doesn’t conform to the twelve-tone equaltemperament. The cross recurrence plots for these twoversions based on MPLPLC, CRP [19], beat-synchronouschroma [7] and HPCP [27] have been presented in Fig-ure 5(a)-(d), respectively. We observe that the extendedpattern in Figure 5(a), which corresponds to similar sec-tions in two versions, is much more distinct and longer3http://resources.mpi-inf.mpg.de/MIR/chromatoolbox/4http://labrosa.ee.columbia.edu/projects/coversongs/5http://joanserra.weebly.com/publications.htmlthan those in Figure 5(b)-(d). This indicates that ﬁrst,MPLPLC may outperform the other features in represent-ing the singing voice characteristics, and second, the differ-ence in harmonic information resulting from the differencein instrumental accompaniment affects the performance ofPCP-based features.Table 2. The identiﬁcation accuracy comparison amongMPLPLC and conventional features over Collection1.SystemIdentiﬁcation accuracyTOP-5MAPRANKMaRRMPLPLC+ˆQmax7380.94463.790.4387PLP [13]+ˆQmax3860.478358.520.2392CRP [19]+ˆQmax5250.671956.480.3237Ellis’s [7]6000.748928.320.3507Serr`a’s [27]5580.726628.280.3507\n(a)\n(b)\n(c)\n(d)Figure 5. Cross recurrence plot for two versions ofWish-ing We Last Foreveras performed by Teresa Teng and FayeWong based on different features: (a) MPLPLC (ˆQmax=464.5), (b) CRP (ˆQmax= 21), (c) Beat-synchronouschroma (ˆQmax= 61.5), and (d) HPCP (ˆQmax= 47.5)3.3 Robustness against Variation in InstrumentalAccompanimentsWhen compared with classical music, popular music canpresent a richer range of variation in style and instrumen-tation [8]. To test the robustness of MPLPLC against vari-ation in style and instrumentation, the identiﬁcation accu-racy in terms of MAP achieved by MPLPLC and by theconventional features were tested and compared with Col-lection2. The experimental results shown in Figure 6 in-dicate that the MPLPLC feature achieves a higher degreeof invariance against instrumental accompaniment than thePLP feature [13], CRP feature [19], Ellis’s scheme [7],and Serr`a’s scheme [27]. This phenomenon may also re-sult from the MPLPLC’s ability of representing the singingvoice.602 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 6. Comparison of robustness against variation ininstrumental accompaniments over Collection2.3.4 Effect of Cepstrum Mapping and LifteringTo demonstrate the inﬂuence of the step LPC cepstrum co-efﬁcients mapping and cepstrum liftering on the identiﬁ-cation power of of the MPLPLC feature, the identiﬁcationaccuracy based on the MPLP feature, which is obtainedby the MPLPLC model without LPC cepstrum coefﬁcientsmapping and cepstrum liftering steps; the MPLPC feature,which is generated by the MPLPLC model without cep-strum liftering step; and the MPLPLC feature, have beencompared in terms of TOP-5, MAP, RANK, and MaRRover Collection1 in Figure 7. It can be seen that both LPCcepstrum coefﬁcients mapping and cepstrum liftering helpto enhance the identiﬁcation power of the MPLPC feature.\n(a)\n(b)\n(c)\n(d)Figure 7. Identiﬁcation accuracy comparison amongMPLP feature, MPLPC feature, and MPLPLC feature, interms of (a) TOP-5, (b) MAP, (c) RANK, and (d) MaRRover Collection1.4. CONCLUSIONWe present a new approach, the MPLPLC model, to ex-tract perceptually relevant features from the music signalsfor pop cover song identiﬁcation. Here, our main idea isto modify the PLP model, which is a mature technique inthe ASR ﬁeld, by introducing the newest research achieve-ments in psychophysics, such as the time-varying loudnessmodel, auditory ﬁlter bank model, and hair cell transduc-tion model, and by taking the difference between speechand music signals into consideration. Furthermore, LPCcepstrum mapping and cepstrum liftering are combined inthe proposed model to boost the resulting feature towardstimbre invariance. Experimental results over two musiccollections show that MPLPLC achieves higher identiﬁca-tion accuracy and degree of invariance against instrumen-tal accompaniment than the conventional PLP feature andstate-of-the-art music theory based features [7, 19, 27] inthe CSI task. This means that the mature techniques inASR may be modiﬁed and used in CSI or other MIR ﬁelds.Despite these achievements, there still exists a lot ofroom for improvement. Since the MPLPLC feature isbased on the modiﬁcation of PLP, which has been suc-cessful in the ASR ﬁeld, it is good at representing singingvoice characteristics. As a result, the MPLPLC-basedCSI scheme can identify cover versions with a prominentsing voice very well but not those with only instrumen-tal sounds. To solve this problem, in the near future, wewill study the SCI scheme, which is based on the fusion ofthe MPLPLC feature and the musical facet based features(e.g. PCP-based features), which are good at analyzingharmony-based western music. Furthermore, we plan tolook into the application of the MPLPLC feature for otherMIR tasks, such as structure analysis, cross-domain musicmatching, and music segmentation.5. ACKNOWLEDGEMENTThis work is supported by the National Natural ScienceFoundation of China (No. 61271349) and the Natural Sci-ence Foundation of Shanghai, China (12ZR1415200).6. REFERENCES[1]J. Benesty, M.M. Sondhi, and Y .T. Huang.SpringerHandbook of Speech Processing. Springer Science &Business Media, 2008.[2]S. Bleeck, T. Ives, and R.D. Patterson. Aim-mat: theauditory image model in matlab.Acta Acustica unitedwith Acustica, 90(4):781–787, 2004.[3]T.M. Chang, E.T. Chen, C.B. Hsieh, and P.C. Chang.Cover song identiﬁcation with direct chroma featureextraction from aac ﬁles. In2nd Global Conference onConsumer Electronics, pages 55–56. IEEE, 2013.[4]P.J. Clemins and M.T. Johnson. Generalized perceptuallinear prediction features for animal vocalization anal-ysis.The Journal of the Acoustical Society of America,120(1):527–534, 2006.[5]J.R. Deller, J.G. Proakis, and J.H.L. Hansen.Discrete-Time Processing of Speech Signals. IEEE New York,NY , USA:, 2000.[6]J.S. Downie. The music information retrieval evalua-tion exchange (2005-2007): A window into music in-formation retrieval research.Acoustical Science andTechnology, 29(4):247–255, 2008.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 603[7]D.P.W. Ellis and G.E. Poliner. Identifying ’coversongs’ with chroma features and dynamic program-ming beat tracking. InInternational Conference onAcoustics, Speech and Signal Processing, pages IV–1429. IEEE, 2007.[8]D.P.W. Ellis and B.M. Thierry. Large-scale cover songrecognition using the 2d fourier transform magnitude.InThe 13th International Society for Music Informa-tion Retrieval Conference, pages 241–246, 2012.[9]T. Fujishima. Realtime chord recognition of musicalsound: A system using common lisp music. InPro-ceedings of the International Symposium on Music In-formation Retrieval, pages 464–467, 1999.[10]B.R. Glasberg and B.C.J. Moore. Derivation of audi-tory ﬁlter shapes from notched-noise data.Hearing re-search, 47(1):103–138, 1990.[11]B.R. Glasberg and B.C.J. Moore. A model of loudnessapplicable to time-varying sounds.Journal of the Au-dio Engineering Society, 50(5):331–342, 2002.[12]E. G´omez.Tonal Description of Music Audio Signals.PhD thesis, Universitat Pompeu Fabra, 2006.[13]H. Hermansky. Perceptual linear predictive (plp) anal-ysis of speech.Journal of the Acoustical Society ofAmerica, 87(4):1738–1752, 1990.[14]B.H. Juang, L. Rabiner, and J.G. Wilpon. On the use ofbandpass liftering in speech recognition.IEEE Trans-actions on Acoustics, Speech and Signal Processing,35(7):947–954, 1987.[15]J.C. Junqua, J.P. Haton, and H. Wakita.Robustness inAutomatic Speech Recognition: Fundamentals and Ap-plications. Kluwer Academic Publishers Boston, USA,1996.[16]J. Makhoul. Spectral linear prediction: Properties andapplications.IEEE Transactions on Acoustics, Speechand Signal Processing, 23(3):283–296, 1975.[17]R. Meddis, M.J. Hewitt, and T.M. Shackleton. Imple-mentation details of a computation model of the in-ner hair-cell auditory-nerve synapse.The Journal ofthe Acoustical Society of America, 87(4):1813–1816,1990.[18]B.C.J. Moore. Development and current status of thecambridge loudness models.Trends in hearing, 18:1–29, 2014.[19]M. M¨uller and S. Ewert. Towards timbre-invariant au-dio features for harmony-based music.IEEE Trans-actions on Audio, Speech, and Language Processing,18(3):649–662, 2010.[20]A.V . Oppenheim, R.W. Schafer, J.R. Buck, and other.Discrete-Time Signal Processing, volume 2. Prentice-hall Englewood Cliffs, 1989.[21]F. Pachet and J.J. Aucouturier. Improving timbre simi-larity: How high is the sky.Journal of Negative Resultsin Speech and Audio Sciences, 1(1):1–13, 2004.[22]A. Pˇribilov´a. Preemphasis inﬂuence on harmonicspeech model with autoregressive parameterization.Radioengineering, 12(3):33–36, 2003.[23]J. Salamon.Melody Extraction from Polyphonic MusicSignals. PhD thesis, Universitat Pompeu Fabra, 2013.[24]J. Serr`a.Identiﬁcation of Versions of the Same MusicalComposition by Processing Audio Descriptions. PhDthesis, Universitat Pompeu Fabra, 2011.[25]J. Serr`a, E. G´omez, P. Herrera, and X. Serra. Chromabinary similarity and local alignment applied to coversong identiﬁcation.IEEE Transactions on Audio,Speech, and Language Processing, 16(6):1138–1151,2008.[26]J. Serr`a, X. Serra, and R.G. ANDRZEJAK. Cross re-currence quantiﬁcation for cover song identiﬁcation.New Journal of Physics, 11(9):111–222, 2010.[27]J. Serr`a, M. Zanin, and R.G. Andrzejak. Cover song re-trieval by cross recurrence quantiﬁcation and unsuper-vised set detection.MIREX Extended Abstract, 2009.[28]J.O. Smith.Mathematics of the Discrete Fourier Trans-form (DFT): with Music and Audio Applications. JuliusSmith, 2007.[29]S.S. Stevens. On the psychophysical law.Psychologi-cal Review, 64(3):153, 1957.[30]H. Terasawa, M. Slaney, and J. Berger. The thirteencolors of timbre. InProc. IEEE WASPAA, New Paltz,NY, USA, pages 323–326, 2005.[31]T.C. Walters, D.A. Ross, and R.F. Lyon. The inter-valgram: An audio feature for large-scale cover-songrecognition. InFrom Sounds to Music and Emotions,pages 197–213. Springer, 2013.[32]D.L. Wang and G.J. Brown.Computational AuditoryScene Analysis: Principles, Algorithms, and Applica-tions. Wiley-IEEE Press, 2006.[33]C. Xiao. Cover song identiﬁcation using an enhancedchroma over a binary classiﬁer based similarity mea-surement framework. InInternational Conference onSystems and Informatics, pages 2170–2176. IEEE,2012.[34]J.P. Zbilut, A. Giuliani, and C.L. Webber. Detecting de-terministic signals in exceptionally noisy environmentsusing cross-recurrence quantiﬁcation.Physics LettersA, 246(1):122–128, 1998.604 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Electric Guitar Playing Technique Detection in Real-World Recording Based on F0 Sequence Pattern Recognition.",
        "author": [
            "Yuan-Ping Chen",
            "Li Su 0002",
            "Yi-Hsuan Yang"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414806",
        "url": "https://doi.org/10.5281/zenodo.1414806",
        "ee": "https://zenodo.org/records/1414806/files/ChenSY15.pdf",
        "abstract": "For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two- stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment.",
        "zenodo_id": 1414806,
        "dblp_key": "conf/ismir/ChenSY15",
        "keywords": [
            "playing techniques",
            "bend detection",
            "vibrato detection",
            "melody contour",
            "timbre features",
            "pitch features",
            "two-stage framework",
            "audio track",
            "accompaniment",
            "F-score"
        ],
        "content": "ELECTRIC GUITAR PLAYING TECHNIQUE DETECTION INREAL-WORLD RECORDINGS BASED ON F0 SEQUENCE PATTERNRECOGNITIONYuan-Ping Chen, Li Su, Yi-Hsuan YangResearch Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwanqoo0972@hotmail.com, lisu@citi.sinica.edu.tw, yang@citi.sinica.edu.twABSTRACTFor a complete transcription of a guitar performance, thedetection of playing techniques such as bend and vibratois important, because playing techniques suggest how themelody is interpreted through the manipulation of theguitar strings. While existing work mostly focused onplaying technique detection for individual single notes, thispaper attempts to expand this endeavor to recordings ofguitar solo tracks. Speciﬁcally, we treat the task as a timesequence pattern recognition problem, and develop a two-stage framework for detecting ﬁve fundamental playingtechniques used by the electric guitar. Given an audiotrack, the ﬁrst stage identiﬁes prominent candidates byanalyzing the extracted melody contour, and the secondstage applies a pre-trained classiﬁer to the candidates forplaying technique detection using a set of timbre and pitchfeatures. The effectiveness of the proposed frameworkis validated on a new dataset comprising of 42 electricguitar solo tracks without accompaniment, each of whichcovers 10 to 25 notes. The best average F-score achieves74% in two-fold cross validation. Furthermore, we alsoevaluate the performance of the proposed framework forbend detection in ﬁve studio mixtures, to discuss how it canbe applied in transcribing real-world electric guitar soloswith accompaniment.1. INTRODUCTIONOver the recent years there has been a ﬂourishing numberof online services such as Chordify1and Riffstation2thatare dedicated to transcribing the chord progression of real-world guitar recordings [10]. As manual transcription de-mands on musical training and time, such services, despitenot being perfect, make it much easier for music loversand novice guitar learners to comprehend and appreciate1http://chordify.net/(accessed: 2015-7-15)2http://play.riffstation.com/(accessed: 2015-7-15)c\u0000Yuan-Ping Chen, Li Su, Yi-Hsuan Yang.Licensed under a Creative Commons Attribution 4.0 InternationalLicense (CC BY 4.0).Attribution:Yuan-Ping Chen, Li Su,Yi-Hsuan Yang. “ELECTRIC GUITAR PLAYING TECHNIQUEDETECTION IN REAL-WORLD RECORDINGS BASED ON F0SEQUENCE PATTERN RECOGNITION”, 16th International Societyfor Music Information Retrieval Conference, 2015.\nFigure 1. The spectrogram and tablature of a guitar phrasethat contains the following techniques: bend (a, b, c, g),vibrato (d, h), hammer-on & pull-off (e) and slide (f, i).music, thereby creating valuable educational, recreationaland even cultural values.For solo guitar recordings, a note-by-note transcriptionof the pitches and the playing techniques associated witheach note is needed. While the sequence of notes con-stitutes a melody, playing techniques such as bend andvibrato determine how the notes are played and accord-ingly inﬂuence the expression of the guitar performance.As shown by the guitar tablature in Figure 1, a completetranscription of a guitar performance should contain thenotations of the playing techniques.3Unlike pitch estimation or chord recognition, researchon playing technique detection is still in its early stages.Most of the existing work, if not all, is only concernedwith audio recordings of pre-segmented individual singlenotes. For example, Abeßeret al.[1] collected around4,300 bass guitar single notes to investigate audio basedmethods to distinguish between 10 bass guitar playingtechniques. Reboursi`ereet al.[20] evaluated a number ofaudio features to detect 6 playing techniques over 1,416samples of hexaphonic guitar single notes. More recently,Suet al.[18] recorded 11,928 electric guitar single notesand investigated features extracted from the cepstrum andphase derivatives to detect 7 playing techniques. It is,3Fretted stringed instruments such as the guitar usually employ thetablature as the form of musical notation. Various arrows and symbols areused in a guitar tablature to denote the playing techniques. To “generate”the tablature from an audio recording, one would also need to predict theﬁnger positions on the guitar fret, which is beyond the scope of this paper.708however, not clear how these methods can be applied todetect playing techniques in a real-world guitar solo track,such as the one shown in Figure 1.The only exception, to our best understanding, is thework presented by Kehlinget al.[16], which extended thework presented in [1] and considered playing techniquedetection in 12 phrases of guitar solo. They proposedto use onset and off detection ﬁrst to identify each noteevent in a guitar solo track, after which the statistics (e.g.minimum, maximum, mean, or median) of frame-levelspectral features over the duration of each note event areextracted and fed to a pre-trained classiﬁer for playingtechnique detection. Using the multi-class support vectormachine (SVM) with radial basis function (RBF) kernel,they obtained 83% average accuracy in distinguishingbetween the following 6 cases:normal,bend,slide,vibrato,harmonics, anddead notes. It appears that lowerrecall rates are found for slide, vibrato, and bend: the recallrates are 50.9%, 66.7%, and 71.3%, respectively.Although Kehlinget al.’s work represented an impor-tant step forward in playing technique classiﬁcation, theirapproach has a few limitations. First, using the wholenote event as a fundamental unit in classiﬁcation cannotdeal with techniques that are concerned with the transitionbetween successive notes, such as pull-off and hammer-on,which are also widely used in guitar. Second, extractingfeatures from the whole note may include information notrelevant to techniques that are related to only the beginningphase of note events, such as bend and slide. Third,existing techniques for onset and offset detection may notbe robust to timbre variations commonly seen in guitarperformance [2, 14], originating from the predominant useof sound effects such as distortion or delay [9]. Onsetand offset detection would be even more challenging in thepresence of accompaniments such as bass and drums.In light of the above challenges, we propose in this worka new approach to playing technique detection in guitar,by exploiting the time sequence patterns over the melodycontour. Given a guitar recording, our approach ﬁrstlyemploys a melody extraction algorithm to estimate themelody contour,i.e.sequence of fundamental frequency(F0) estimates. Then, we apply a number of signal pro-cessing methods to analyze the estimated melody contour,from which candidate regions of target playing techniquesare identiﬁed. Because the candidates are identiﬁed fromthe melody contour, the proposed approach can deal withtechniques employed during the transition or the beginningphase of notes. The candidate selection algorithms aredesigned in such a way that emphasizes more on recallrates. Finally, we further improve the precision rates byextracting spectral and pitch features from the candidateregions and using SVM for classiﬁcation.The effectiveness of the proposed approach is validatedon a new dataset comprising of 42 electric guitar solostaken from the teaching material of the textbook,RockLead Basics: Techniques, Scales and Fundamentals forGuitar, by Danny Gill and Nick Nolan [13]. While theguitar phrases employed in Kehlinget al.’s work are notassociated with any sound effect [16], the phrases we takefrom this book are recorded with distortion sound effectand are perceptually more melodic and realistic. Moreover,according to the data from the book, we consider thefollowing ﬁve playing techniques in this work:slide,vibrato,bend,hammer-on, andpull-off, which are viewedas the most frequently used and fundamental techniques inrock lead guitar by the textbook authors.The guitar solos collected from the book are not ac-companied by any other instruments. To examine howthe proposed approach can be applied to real-world record-ings with accompaniment, we also conduct a preliminaryevaluation using 5 well-known guitar solo tracks withdifferent tones and accompaniments. The use of a sourceseparation algorithm as a pre-processing step to suppressthe accompaniments is also investigated.2. DATASETS AND PLAYING TECHNIQUESTwo datasets are employed in this work. The ﬁrst one iscomposed of 42 tracks of unaccompanied electric guitarsolo obtained from the CD of the textbook by Danny Gilland Nick Nolan. The duration of the tracks is about 15–20 seconds, summing up to about 10 minutes. The tracksare recorded by a standard tuned electric guitar with cleantone and distortion sound effect, covering 10–25 notes pertrack. For evaluation purposes, we have the timestampsof the playing techniques employed in each track carefullyannotated by an experienced electric guitar player, with thehelp of the corresponding guitar tablature. In total, we have143 pull-offs, 70 hammer-ons, 143 bends, 74 slides, and 61vibratos. While the audio tracks are copyright protected,we have made the annotations publicly available with theresearch community through a project webpage.4The ﬁrst dataset contains a variety of different possiblerealizations of the techniques in real-world performances.To illustrate this, we combine a few passages of differentphrases and show in Figure 1 the spectrogram and guitartab. The ﬁve playing techniques and their possible varia-tions are described below.•Bendrefers to stretching the string with left hand toincrease the pitch of the bended note either graduallyor instantly. The region (a) in Figure 1 shows anotefull-bendedfromA4toB4gradually. In (b),the note ispre-bendedtoB4,i.e.bend the notewithout sounding it, and thenreleasedtoA4withthe hitting of string. Region (c) shows ahalf-stepbend commonly seen in Blues. Agrace notebend iswhen you strike the string and at the same time bendthe note to the target, as shown in (g).•Vibratorepresents minute and rapid variations inpitch. Regions (d) and (h) of Figure 1 show a verysubtle vibrato with smaller extent and a wide vibratowith larger extent, respectively.4http://mac.citi.sinica.edu.tw/GuitarTranscription. Note that we label the instant of transitionbetween two notes for pull-off and hammer-on, the middle of theemployment of bend and slide, and the whole duration (including thebeginning and end timestamps) for vibrato.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 709Figure 2. Flowchart of the proposed approach to guitarplaying technique detection.•Hammer-onis when a note is sounded, a left handﬁnger is used to quickly press down a fret that is onthe same string while the ﬁrst note is still ringing.•Pull-offis when you have strummed one note andliterally pull off of the string to a lower note. Rapidand successive use of pull-of and hammer-on is oftenreferred to astrill, which is illustrated in (e).•Sliderefers to the action of slide left hand ﬁngeracross one or more frets to reach another note. Aslide betweenB3andD4is shown in (f). There areshiftslides andlegatoslides. A guitar solo usuallybegins or ends with another variant known asslidefrom/into nowhere,” which is illustrated in (i).The second dataset, on the other hand, consists of 5excerpts of real-world guitar solo (with accompaniment)clipped from the following famous recordings: segments1’48”–2’39” and 2’51”–3’23” fromBold as Loveby JimiHendrix, segments 0’17”–1’26” and 3’50”–4’33” fromComing Back to Lifeby Pink Floyd, and segment 4’22”–5’04” fromWet Sandby Red Hot Chili Peppers. The ﬁrsttwo are both played in fuzzy tone (akin to overdrive), thethird one with reverb effect in clean tone, the fourth onein overdrive, and the ﬁfth one is played with the distortioneffect. The excerpts last 3 minutes 57 seconds in total. Wealso manually label the playing techniques for evaluation.3. PROPOSED APPROACH3.1 OverviewKehlinget al.[16] employs a two-stage structure in detect-ing playing techniques in audio streams. The ﬁrst stageuses onset and offset detection to identify each note eventfrom the given audio track, and the second stage appliesa pre-trained classiﬁer to the note events for multiclassclassiﬁcation. A similar two-stage structure is also adoptedin the proposed approach, but in our ﬁrst stage we make useof the melody contour extracted from the given audio track,and employ a number of algorithms to identify candidatesof playing techniques from the melody contour. Differentcandidate selection algorithms are speciﬁcally designedfor the 5 playing techniques. Depending on the targetplaying technique, the input to the second-stage classiﬁercan be temporal segments falling between note events orfragments of whole note events. In this way, the proposedapproach can deal with techniques such as hammer-on andpull-off, while Kehlinget al.’s approach cannot.Figure 2 shows the ﬂowchart of the proposed ap-proach, which includes source separation as an optionalpre-processing step to cope with instrumental accompani-ments. We provide the details of each component below.3.2 Source SeparationIn real-world guitar performance, the guitar solo is usuallymixed with strong bass line, percussion sounds, or others.Due to the accompaniments, the performance of estimatingthe melody contour of the lead guitar may degrade.We experiment with the robust principal componentanalysis (RPCA) algorithm [6, 7, 15] to separate the soundof the lead guitar from the accompaniments, before ex-tracting the melody. Given the magnitude spectrogramD2Rt⇥mof the mixture, wheretdenotes temporallength andmthe number of frequency bins, RPCA seeksto decomposeDinto two matrices of the same size, alow-rank matrixAand a sparse matrixE, by solving thefollowing convex optimization problem:minA,E:D=A+EkAk⇤+\u0000kEk1,(1)where the trace normk·k⇤andl1normk·k1areconvex surrogate of the rank and the number of nonzeroentries of a matrix, respectively [6], and\u0000is a positiveweighting parameter. As thebackgroundcomponent of asignal is usually composed of repetitive elements in timeor frequency, its spectrogram is likely to have a lowerrank comparing to that of theforeground. RPCA has beenapplied to isolating the singing voice (foreground) from theaccompaniment (background) [15]. We use the same idea,assuming that the guitar solo is the foreground (i.e. inE).3.3 Melody ExtractionMelody extraction has been an active ﬁeld of research inthe music information retrieval society for years [5, 8, 19].It is concerned with the F0 sequence of only the mainmelody line in a polyphonic music recording. Therefore,it consists of a series of operations for creating candidatepitch contours from the F0 estimates and for selecting oneof the pitch contours as the main melody. We employthe state-of-the-art melody extraction algorithm proposedby Salamon and G`omez [21], for its efﬁciency and well-demonstrated effectiveness. Speciﬁcally, we employ theimplementation of the MELODIA algorithm developed bythe authors for an open-source library called Essentia [3].It is easy to use and the estimate is in general accurate.3.4 Candidate Selection (CS)We propose to mine the melody contour for the followingtime sequence patterns speciﬁc to each playing technique.Following this process of pattern ﬁnding, we can ﬁndcandidates of the playing techniques scattered in the timeﬂow of a music signal. We refer to this process as candidateselection, or CS for short.•Bend: arc-like or twisted trajectories.•Vibrato: sinusoidal patterns.•Slide: upward or downward stair-like patterns.•Hammer-on, pull-off: two adjacent parallel horizon-tal lines resulting from two notes of different F0s.Clearly, such patterns may not necessarily correspondto true instances (or, true positives) of the playing tech-niques. For example, sounding two notes with pick picking710 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 3. The procedure of candidate selection (best seen in color). (a) The raw melody contour of a bend (red segment)and a vibrato (yellow segment). (b) The processed melody contour by median ﬁlter, note tracking and mean ﬁlter. Fourlocal extrema of pitch value create a window to determine vibrato. (c) The candidate segment for vibrato (blue). (d) Thecandidate segments for bend (blue). (e) The candidate segments for bend, after excluding candidates of vibrato (blue).(f) The raw melody contour of a pull-off and a hammer-on. The red vertical lines show the groundtruth instants of theplaying techniques. (g) The processed melody contour by note tracking and quantization, and the blue vertical lines denotethe candidates instants. (h) The raw melody contour of a “slide into nowhere” (red segment). (i) The processed melodycontour by quantization, and the selected candidates for slide (blue segments).also results in a pitch trajectory of two parallel horizontallines akin to the case of hammer-on or pull-off. Theremight also be errors in the estimate of the melody contour(e.g.when the lead instrument is silent, the estimatedmelody contour may correspond to the sounds of otherinstruments). Therefore, the purpose of the CS process isactually to identify the candidates with high recall rates(i.e.not missing the true positives) and moderate precisionrates (i.e.it is ﬁne to have false positives). In the nextstage, we will use SVM classiﬁers that are discriminativelytrained to distinguish between true positives and falsepositives by exploiting both timbre and pitch features com-puted from these candidates. Because the CS process onlyconsiders pitch information, the additional use of timbreinformation in the classiﬁcation stage has the potential toboost the precision rates.As described below, the CS process is accomplishedwith a few simple signal processing methods for simplicityand efﬁciency. The methods are illustrated in Figure 3.3.4.1 Vibrato and BendWe use similar procedures to select the candidates ofvibrato and bend, because the two techniques share thesame arc-like trajectories. Indeed, a vibrato can be viewedas succession of bend up and then releasing down. Thetwo techniques mainly differ in the number of the cycles.The following operations are ﬁrstly employed to processthe (raw) melody contour estimated by MELODIA [3].•First, we ﬂatten the rugged raw contour and removethe outliers produced by the melody extraction algo-rithm by a 10 points (100ms in 44.1 kHz samplingrate)median ﬁlter, whose length is approximatelyshorter than a period of vibration. The median ﬁlteralso slightly corrects octave errors made by melodytracking.•Second, we perform a simplenote trackingstep bygrouping adjacent F0s into the same note if the pitchdifference between them is smaller than 80 cents,according to the auditory streaming cues [4]. Thestep leads to a number of segments corresponding todifferent note events, from which segments shorterthan 80ms are discarded, assuming that the a singlenote should last at least 80ms, approximately thelength of a semiquaver in 180 BPM.•Finally, we convolve each segment with a 5 points(50ms)mean ﬁlterwith hop of 10ms for smoothing.The segments are then considered as possible note events.We then use different ways to detect vibrato and bend.For vibrato, we search for all the local maxima andminima in each note [12]. A temporal fragment of fourconsecutive extrema within a note is considered as avibrato candidate if the following conditions meet: 1)the temporal distance between two neighboring extremashould fall within 30ms and 400ms for valid vibrato rate,i.e., the modulation frequency from 1.25Hz to 16.67Hz; 2)the pitch difference between neighboring extrema shouldProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 711be smaller than 225 cents, which is slightly larger thana whole note; 3) dividing the fragment into three shorterfragments of pitch sequence by the four extrema, thevariance in the logarithmic pitch of each short fragmentshould be larger than an empirical threshold. Please seeFigure 3(c) for an example.On the other hand, we consider a temporal fragment asa bend candidate if the following conditions meet: 1) it isnot a vibrato candidate; 2) the pitch sequence continuouslyascends or descends for more than 80ms; 3) the pitchdifference between two neighboring frames is smaller than50 cents. An example can be found in Figure 3(e).3.4.2 Pull-off and Hammer-onWhile bend and vibrato can last a few frames, pull-off andhammer-on are considered as the temporal instance (i.e.aframe) during the transition of notes. Therefore, withoutusing either a median or mean ﬁlter, we perform the notetracking procedure described in Section 3.4.1, and thenquantizeeach F0 to its closest semitone in terms of cent.After this, we consider all the temporal instances in themiddle of two notes as a candidate for both pull-off andhammer-on, as long as the following conditions meet: 1)the gap between the note transition is shorter than 20ms;2) the pitch cannot be away from its closest semitone by35 cents. The former condition is set, because it is knownthat the contact of pick (or right hand ﬁnger) and the stringwould temporarily stop the vibration of the string when anote is sounded by plucking the string, thereby creating thegap in the note transition [20]. The latter condition is setbecause there might be such gaps within the employmentof a vibrato or a bend due to the F0 quantization.Because each candidate for pull-off or hammer-on onlylasts one frame, to characterize the temporal moment, weuse a 100ms fragment centering at the candidate frame forthe feature extraction step described in Section 3.5.3.4.3 SlideTo recognizing the ladder-like pitch sequence pattern,we simply quantize all the F0s into its closest semitonewithout any pre-processing, in order not to falsely removethe transition notes of a bend (which is usually aroundtens of milliseconds). After quantization, we search forthe ladders in the melody contour with the following rules:1) the number of steps should be at least three (i.e.slideacross at least three frets); 2) the length of transitional steps(notes) should fall within 10 to 70ms, according to ourempirical observation from the data; 3) the pitch differencebetween neighboring steps should be exactly one semitone(i.e.a fret). Please refer to Figure 3(i) for an example.3.5 Feature Extraction and ClassiﬁcationAfter applying CS, we would have candidates of the 5playing techniques spreading over the input guitar track.As we have mentioned, our design of the signal processingmethods and the setting of some parameter values havebeen informed by the need of reaching high recall rate. It isthen the job of the classiﬁer to identify false positives of thetechniques and improve precision rates.The candidates arerepresented by the following three sets of audio features.•TIMBRE(T) includes the statistics of the followingfeatures: spectral centroid, brightness, spread, skew-ness, kurtosis, ﬂux, roll-off, entropy, irregularity,roughness, inharmonicity, zero-crossing rate, low-energy ratio, and their 1st-order time difference. Weuse the mean, standard deviation (STD), maximum,minimum, skewness, kurtosis as the statistics mea-sure, so there are 13⇥8⇥2=208 features in total.•MFCC(M) contains mean and STD of the 40-DMel-frequency cepstral coefﬁcients and its 1st-ordertime difference, totalling 160 features. Both theTIMBRE and MFCC sets are computed by the open-source library MIRtoolbox [17].•Pitch(P) is computed from the log scale F0 se-quence on the processed (instead of the raw) melodycontour. Except for vibrato, we use the following6 features for all the playing techniques: skewness,kurtosis, variance, the difference between the max-imum and minimum, and the mean and STD of the1st-order time difference. For vibrato, as there are3 short temporal fragments for each candidate (seeSection 3.4.1), we calculate the 6 features for eachof the fragment, and additionally use the varianceof difference between the four pitch extrema inlog scale and the variance of the temporal distancebetween the four pitch extrema, totalling 20 features.4. EXPERIMENT4.1 Experimental SetupFor short-time Fourier transform, we use the Hammingwindow of 46ms and 10ms overlap under the sampling rateof 44.1 kHz. For MELODIA, we set the lowest and highestpossible F0 to 77Hz (E2b) and 1400 (F6) respectively,considering the frequency range of a standard-tuned guitarplus additionally half step tolerance of inaccurate tuning.We train 5 binary linear kernel SVMs [11], one for eachtechnique,5and employ z-score normalization for the fea-tures. The parameterCof SVM is optimized by an insidecross validation on the training data. We conduct trainingand testing 10 times under a two-fold cross validationscheme and report the average result, in terms of precision,recall and F-score. An estimate of bend or slide is deemedcorrect as long as the ground truth timestamp falls betweenthe detected bend or slide segment. An estimate of pull-off or hammer-on is deemed correct if the detected instantof employment falls between the interval of ground truthinstant with a tolerance time-window of 50ms. Vibrato isevaluated in the frame level,e.g.the recall of vibrato is theproportion of frames labeled as vibrato which are detectedas vibrato. For evaluation on the studio mixtures, the SVMis trained over the 42 unaccompanied phrases. Sourceseparation is only performed for the 5 studio mixtures.5It would have been better if a multi-class classiﬁcation schemeis adopted to avoid possible overlaps of the estimates of differenttechniques. We leave the issue as a future work.712 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015BendVibratoPull-offHammer-onSlideRecall94.494.294.494.385.1Precision53.163.030.124.715.0F-score68.075.545.739.225.5(a)BendVibratoPull-offHammer-onSlideRecall86.279.573.665.758.6Precision89.389.175.366.756.8F-score87.784.074.466.357.7(b)Table 1. Recall, precision, and F-scores (in %) of playingtechnique detection in the unaccompanied set using (a) CSonly and (b) CS+SVM{MFCC,TIMBRE,Pitch}.4.2 Expriment Result4.2.1 Evaluation on Unaccompanied Guitar SolosTable 1 shows the per-class result of playing techniquedetection over the 42 unaccompanied guitar solos, usingeither (a) only candidate selection (CS) or (b) both CS andSVM. The following observations can be made.•Except for slide, the proposed CS methods can leadto recall rates higher than 94% for the consideredplaying techniques. Slide appears to be the mostchallenging one, as its detection can be affected byoctave errors from the melody extraction step.•By comparing Tables 1(a) and (b), we see that thesecond-stage SVM can remarkably improve the pre-cision rates, and accordingly the F-scores, for all theplaying techniques. This validates the effectivenessof the proposed approach.•Bend and vibrato appear to be easier to detect, withF-scores 87.7% and 84.0%, respectively. Althoughit is not fair to compare the numbers with the onesreported in [16] due to differences in settings anddatasets, the performance of the proposed approachseems to be promising. Interestingly, slide appearsto be the most challenging case in our study andthe one presented by [16], with comparable F-scores(57.7% versus 50.9%).Figure 4 provides the F-scores of using different fea-tures in the SVM. Although not shown in the ﬁgure, MFCCappears to be the best performing individual feature setamong the three. Better result is seen by concatenating thefeatures (i.e.early fusion). Pitch features contribute moreto the detection of hammer-on but less for others, possiblybecause pitch information has been exploited in CS.4.2.2 Evaluation on Real-World Studio MixturesAs bend detection is found to be promising, we focuson bend detection for our evaluation over the 5 studiomixtures, which include in total 85 bends. Figure 5 com-pares the F-score of bend detection of various methods,including the case when RPCA is employed before melodyextraction. It is not surprising that the F-scores are lowerthan that obtained for the unaccompanied tracks. However,it is interesting to note that the best result can be obtainedby CS only, regardless of whether RPCA or SVM is\n0%25%50%75%100%\nBendVibratoPull-offHammer-onSlideF-score CSCS+SVM{M}CS+SVM{M,P}CS+SVM{M,T}CS+SVM{M,T,P}Figure 4. F-scores of playing technique detection in 42unaccompanied guitar solos using various methods.\n0%25%50%75%F-score \nFigure 5. F-scores of bend detection of 5 accompaniedguitar solos, without (left) or with (right) RPCA.used. Actually, the result of using CS+SVM degrades a lotcomparing to the case of CS only, except for the case thatpitch features are considered in SVM. The performance ofCS+SVM can be improved by using RPCA, but the resultis still inferior to the result of CS only. We conjecturethat the inferior result of CS+SVM can be attributed tothe difference between the data used for training the SVM(i.e.the unaccompanied tracks) and the data for testing(i.e.the mixtures). The result might be better if we have afew training data that are with accompaniment. However,if such data are not available, it seems to be advisable touse the CS process only for the bend detection in mixtures.5. CONCLUSIONIn this paper, we have presented a two-stage approachfor detecting 5 guitar playing techniques in guitar solos.The proposed approach is characteristic of its use of timesequence patterns mined from the melody contour of thelead guitar for candidate selection in the ﬁrst stage, andthen using classiﬁers to reﬁne the result in the secondstage. The F-scores for the unaccompanied set range from57.7% to 87.7% depending on the playing techniques. Theaverage F-score across the techniques reaches 74%. Wehave also evaluated the case of bend detection for a fewguitar solos with accompaniment, and shown that the bestF-score 67.3% is obtained by candidate selection alone.6. ACKNOWLEDGMENTThis work was supported by the Academia Sinica CareerDevelopment Program.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7137. REFERENCES[1]J. Abeßer, H. Lukashevich, and G. Schuller. Feature-based extraction of plucking and expression stylesof the electric bass guitar. InProc. IEEE Int. Conf.Acoustics, Speech, and Signal Processing, pages 2290–2293, 2010.[2]S. Bock and G. Widmer. maximum ﬁlter vibratosuppression for onset detection. InProc. of the 16thInt. Conf. on Digital Audio Effects (DAFx), 2013.[3]D. Bogdanov, N. Wack, E. G`omez, S. Gulati,P. Herrera, O. Mayor, G. Roma, J. Salamon, J. Zapata,and X. Serra. Essentia: an audio analysis library formusic information retrieval. InProc. Int. Soc. MusicInformation Retrieval Conf., pages 493–498, 2013.[Online]http://essentia.upf.edu.[4]A. S. Bregman, editor.Auditory scene analysis. MITPress, 1990.[5]P. M. Brossier.Automatic Annotation of Musical Audiofor Interactive Applications. PhD thesis, Queen Mary,University of London, 2006.[6]E. J. Cand`es, X. Li, Y . Ma, and J. Wright. Robustprincipal component analysis?Journal of the ACM,58(3):1–37, 2011.[7]T.-S. Chan, T.-C. Yeh, Z.-C. Fan, H.-W. Chen, L. Su,Y .-H. Yang, and R. Jang. V ocal activity informedsinging voice separation with the iKala dataset. InProc. IEEE Int. Conf. Acoust., Speech and SignalProcess., pages 718–722, 2015.[8]A. De Cheveign´e and H. Kawahara. Yin, a fundamentalfrequency estimator for speech and music.The Journalof the Acoustical Society of America, 111(4):1917–1930, 2002.[9]J. Dattorro. Effect design, part 2: Delay linemodulation and chorus.J. Audio engineering Society,45(10):764–788, 1997.[10]W. B. de Haas, J. P. Magalh˜aes, and F. Wiering.Improving audio chord transcription by exploitingharmonic and metric knowledge. InProc. Int. Soc.Music Information Retrieval Conf., pages 295–300,2012.[11]R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,and C.-J. Lin. LIBLINEAR: A library for large linearclassiﬁcation.J. Machine Learning Research, 2008.http://www.csie.ntu.edu.tw/˜cjlin/liblinear/.[12]A. Friberg and E. Schoonderwaldt. Cuex: Analgorithm for automatic extraction of expressivetone parameters in music performance from acousticsignals.Acta Acustica united with Acustica, 93(3):411–420, 2007.[13]D. Gill and N. Nolan.Rock Lead Basics: Techniques,Scales and Fundamentals for Guitar. MusiciansInstitute Press, Los Angeles, California, 1997.[14]A. Holzapfel, Y . Stylianou, A. C. Gedik, andB. Bozkurt. Three dimensions of pitched instrumentonset detection.IEEE Trans. Audio, Speech, andLanguage Processing, pages 1517–1527, 2010.[15]P.-S. Huang, S. D. Chen, P. Smaragdis, andM. Hasegawa-Johnson. Singing-voice separation frommonaural recordings using robust principal componentanalysis. InProc. IEEE Int. Conf. Acoustics, Speech,and Signal Processing, pages 57–60, 2012.[16]C. Kehling, J. Abeßer, C. Dittmar, and G. Schuller.Automatic tablature transcription of eletric guitarrecordings by estimation of score-and instrument-related parameters. InProc. Int. Conf. Digital AudioEffects, 2014.[17]O. Lartillot and P. Toiviainen. MIR in Matlab (II):A toolbox for musical feature extraction from audio.InProc. Int. Soc. Music Information Retrieval Conf.,pages 127–130, 2007. [Online]http://users.jyu.fi/˜lartillo/mirtoolbox/.[18]L.Su, L.-F. Yu, and Y .-H. Yang. Sparse cepstral andphase codes for guitar playing technique classiﬁcation.InProc. Int. Soc. Music Information Retrieval Conf.,pages 9–14, 2014.[19]M. M¨uller, D. P. W. Ellis, A. Klapuri, and G. Richard.Signal processing for music analysis.IEEE J. Sel.Topics Signal Processing, 5(6):1088–1110, 2011.[20]L. Reboursi`ere, O. L¨ahdeoja, T. Drugman, S. Dupont,C. Picard, and N. Riche. Left and right-hand guitarplaying techniques detection. InProc. Int. Conf. NewInterfaces for Musical Expression, 2012.[21]J. Salamon and E. G`omez. Melody extraction frompolyphonic music signals using pitch contour charac-teristics.IEEE Trans. Audio, Speech, and LanguageProcessing, 20(6):1759–1770, 2012.714 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Hybrid Long- and Short-Term Models of Folk Melodies.",
        "author": [
            "Srikanth Cherla",
            "Son N. Tran",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417315",
        "url": "https://doi.org/10.5281/zenodo.1417315",
        "ee": "https://zenodo.org/records/1417315/files/CherlaTWG15.pdf",
        "abstract": "In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melod- ies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are com- bined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connec- tionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our exper- iments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hy- brid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of mono- phonic chorale and folk melodies.",
        "zenodo_id": 1417315,
        "dblp_key": "conf/ismir/CherlaTWG15",
        "keywords": [
            "dynamic models",
            "predicting sequences",
            "musical pitch",
            "melodies",
            "long-term model",
            "short-term model",
            "ensemble",
            "relative entropies",
            "connectionist LTMs",
            "Recurrent Temporal Discriminative Restricted Boltzmann Machine"
        ],
        "content": "HYBRID LONG- AND SHORT-TERM MODELS OF FOLK MELODIESSrikanth Cherla1,2Son N. Tran2Tillman Weyde1,2Artur d’Avila Garcez21Music Informatics Research Group, Department of Computer Science, City University London2Machine Learning Group, Department of Computer Science, City University London{srikanth.cherla.1, son.tran.1, t.e.weyde, a.garcez}@city.ac.ukABSTRACTIn this paper, we present the results of a study on dynamicmodels for predicting sequences of musical pitch in melod-ies. Such models predict a probability distribution over thepossible values of the next pitch in a sequence, which isobtained by combining the prediction of two components(1) a long-term model (LTM) learned ofﬂine on a corpus ofmelodies, as well as (2) a short-term model (STM) whichincorporates context-speciﬁc information available duringprediction. Both the LTM and the STM learn regularitiesin pitch sequences solely from data. The models are com-bined in an ensemble, wherein they are weighted by therelative entropies of their respective predictions. Going byprevious work that demonstrates the success of Connec-tionist LTMs, we employ the recently proposed RecurrentTemporal Discriminative Restricted Boltzmann Machine(RTDRBM) as the LTM here. While it is indeed possiblefor the same model to also serve as an STM, our exper-iments showed thatn-gram models tended to learn fasterthan the RTDRBM in an online setting and that the hy-brid of an RTDRBM LTM and ann-gram STM gives usthe best predictive performance yet on a corpus of mono-phonic chorale and folk melodies.1. INTRODUCTIONIn the present work, our interest is in learning a model topredict a probability distribution over the possible valuesof the pitch of a musical note in a melody given the se-quence of notes leading up to it. The motivation for thisstems from theoretical work in musicology and music cog-nition which attempts to explain various musical phenom-ena (such as style, genre and mood) in terms of patternsof fulﬁlment, prolongation and violation of musical expec-tation [10, 15, 19], i.e., that our perception of music is in-ﬂuenced by how its evolution in time conforms to, or de-viates from our expectations. There exists empirical evi-dence suggesting that these expectations are shaped by anunderlying mechanism of statistical learning [9], the con-sequences of which have also been observed in languagec\u0000Srikanth Cherla, Son N. Tran, Tillman Weyde, Arturd’Avila Garcez. Licensed under a Creative Commons Attribution 4.0 In-ternational License (CC BY 4.0).Attribution:Srikanth Cherla, Son N.Tran, Tillman Weyde, Artur d’Avila Garcez. “Hybrid Long- and Short-Term Models of Folk Melodies”, 16th International Society for MusicInformation Retrieval Conference, 2015.[24]. This apparent commonality between the two domainshas inspired the adoption of statistical models for word se-quences in language and character sequences in text, topitch sequences in melody [4, 6, 21, 31]. Previous work in-terpreting information theoretic concepts such as entropyand mutual information (which play a key role in languageand text modelling) in the context of music [5, 16] con-tributed towards the adoption of these quantities in evaluat-ing suchmelody models. Time-varyingentropy proﬁlesofpredictions made by such models on musical pieces havebeen used for explaining stylistic implications of salientmusical structures [7]. They have also been used to gener-ate melodic stimuli in music cognition research [20]. Pre-dictive models of music have also been used as Music Lan-guage Models in music transcription [26]. The reader isreferred to [23] for a recent review on predictive machinelearning models used in music research.The melody models considered here contain two com-ponents - a long-term model (LTM), and a short-termmodel (STM) [6]. The parameters of each model arelearned through exposure to appropriate data. From a ma-chine learning perspective, the LTM is a model whose pa-rameters are learned ofﬂine from a dataset of melodies. Itrepresents more global stylistic characteristics acquired bya listener over a longer time-span. The parameters of theSTM are learned online while making predictions on thetest data, without any sequence learning occurring in it be-forehand. The STM highlights the importance of context-speciﬁc information, available in a melody while it is be-ing processed by the listener, in the generation of expec-tations. Predictions (in the form of probability distribu-tions) made by each model about a certain musical eventin a sequence are combined using ensemble methods, andthis has been shown to improve the quality of predictionsover individual models in the past [6,21]. The idea of com-bining corpus-based long-term and context-sensitive short-term predictions from different models was originally afeature of cache-based language models [12]. It was in-troduced in the context of music in [6], further extendedin [21], and adopted in [7, 31].To address the prediction task, we employ a recentlyproposed Connectionist model known as the RecurrentTemporal Discriminative Restricted Boltzmann Machine[3]. This model has been shown to have a predictiveperformance better thann-gram models and other stan-dard Connectionist models on a corpus of monophonicmelodies when used as an LTM. We begin by evaluating584its utility as an STM by carrying out online learning init, which has not been done previously. Experiments re-vealed that, while learning did indeed take place, it didnot progress quickly enough (as a function of the numberof data-points presented to the RTDRBM) to outperformexisting state-of-the-art dynamic models based purely onn-grams [22]. On adopting the wisdom of previous workwhich demonstrated thatn-gram models are indeed an ef-fective choice as STMs, we found here that a hybrid pre-diction model which combines the predictions of an RT-DRBM LTM and ann-gram STM achieves better predic-tive performance, and this also outperforms the state-of-the-art, purelyn-gram based dynamic melody models on acorpus of8melody datasets. In this paper, we present theresults of various LTM-STM combinations that we exper-imented with to arrive at this result and discuss our obser-vations.In the next section we formally introduce the taskof melody modelling, and entropy-weighted combinationstrategies for LTMs and STMs. This is followed by a briefoverview of the two types of prediction models involvedin the present work, in Section 3. Various experiments incombining these models that led to the above mentionedoptimal predictive performance are described in Section 4,followed by the conclusions in Section 5.2. MELODY MODELLINGOur interest is in modelling musical pitch sequencesthrough prediction. The task of music prediction addressedhere has strong parallels with previous work in languagemodelling [14]. Thus, the analogy to natural language isused here to explain it. In statistical language modelling,the goal is to build a model that can estimate the joint prob-ability distribution of subsequences of words occurring ina languageL. A statistical language model (SLM) can berepresented by the conditional probability of the next wordw(T)given all the previous ones[w(1),...,w(T\u00001)](writ-tenw(1:T\u00001)), asP(w(1:T))=TYt=1P(w(t)|w(1:t\u00001)).(1)The present work treats notes in a monophonic melodyanalogous to words in the above language example. Thisis inspired by [6] where a similar analogy was made be-tween sequences of characters in the English language andnotes in music. We use an event-based representation ofmusic, where the occurrence of each note is treated asamusical event. Much in the same way as an SLM, asystem for music prediction models the conditional dis-tributionP(s(t)|s(1:t\u00001))given a sequences(1:T)of mu-sical events [4,6,22] from a musical languageS, such thats(t)2[S], where[S]is the set of symbols (musical pitchvalues) inS. For each prediction, context information isobtained from the eventss(1:t\u00001)precedings(t). Althougha range of musical features (such as musical pitch, noteduration, inter-onset interval, etc.) may be extracted fromeach musical event as explained in [6], we limit our atten-tion to sequences of musical pitch. And the symbols thatmake up these sequences are MIDI values of the pitcheswhich occur in a particular dataset.2.1 Long- and Short-term ModelsIn the present work, we make a distinction between twotypes of prediction models, as introduced previously in thecontext ofMultiple Viewpoints for Music Prediction[6].The ﬁrst is known as a Long-Term Model (LTM). Thismodel is learned ofﬂine on a corpus of melodies (train-ing data), its parameters thus being ﬁnalized beforehandand kept constant during the prediction stage. It repre-sents more global stylistic characteristics acquired by a lis-tener over a longer time-span. And the second is what isknown as the Short-Term Model (STM). It highlights theimportance of context-speciﬁc information, available in amelody while it is being processed by the listener, in thegeneration of expectations. The distinction between thelong- and short-term models is also akin to the that madein [11] between “schematic” (LTM) and “veridical” (STM)knowledge in a modular view on music processing. A vari-ant of the LTM which is also considered here was intro-duced in [22]. This is the LTM+, and in addition to beinglearned ofﬂine on a corpus of melodies like the LTM, it isalso updated while making predictions just like the STM.Another distinction between the LTM+ and the STM is thatthe former is continuously updated across melodies, whilethe latter is re-initialized after each melody in the test set.2.2 Combining the LTM & STMIt was demonstrated in [6, 21] that an entropy-weightedcombination of the predictions of two or moren-grammodels typically results in ensembles with better predictiveperformance than any of the individual models. As it is thepredicted distributions which are combined, this approachis independent of the types of models involved. Here, webrieﬂy describe two rules for creating such ensembles. LetMbe a set of models andPm(s)be the probability as-signed to symbols2[S]by modelm. The ﬁrst involvestaking a weighted arithmetic mean of their respective pre-dictions. This is theMeancombination rule, deﬁned asP(s)=Pm2MwmPm(s)Pm2Mwm(2)where each of the weightswmdepends on the entropy ofthe distribution generated by the corresponding modelmin the combination such that greater entropy (and henceuncertainty) is associated with a lower weight [6]. Theweights are given by the expressionwm=Hrel(Pm)\u0000b,where the relative entropyHrel(Pm)isHrel(Pm)=(H(Pm)/Hmax(Pm),ifHmax([S])>01,otherwise(3)The best value of the combination biasb\u00000is determinedthrough cross-validation. Whenb=0, all the combinedmodels have the same weight. The quantitiesHandHmaxProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 585are respectively the entropy of the prediction and the maxi-mum entropy of predictions over the symbol space[S], andare deﬁned asH(P)=\u0000Xs2[S]P(s) log2P(s).(4)Hmax(P) = log2|S|.whereP(X=s)is the probability mass function of arandom variableXdistributed over the discrete alphabet[S]such that the individual probabilities are independentand sum to1.The second — theProductcombination rule, is com-puted similarly as the weighted geometric mean of theprobability distributions. This is given byP(s)=1R Ym2MPm(s)wm!1Pm2Mwm(5)whereRis a normalisation constant which ensures that theresulting distribution overSsums to unity. The weightswmin this case are obtained in the same manner as in thecase of the Mean combination rule. It was observed in aprevious application of these two combination methods tomelody modelling [21], that the Product rule resulted in agreater improvement in predictive performance.3. PREDICTION MODELSBefore moving on to the experiments carried out on differ-ent LTM-STM combinations in the next section, here weprovide a quick overview of the two classes of predictionmodels that have been employed for this purpose. The ﬁrstis the Recurrent Temporal Discriminative Restricted Boltz-mann Machine, and the other is then-gram Model.3.1 Recurrent Temporal Discriminative RBMThe Recurrent Temporal Discriminative Restricted Boltz-mann Machine (RTDRBM) [3] was proposed by theauthors as the discriminative equivalent of the Recur-rent Temporal Restricted Boltzmann Machine (RTRBM)[28]. Both models are identical in structure, andare composed of a sequence of Restricted BoltzmannMachines (RBM) [27], where the visible and hid-den layers of the RBM at time-steptare condi-tioned on the mean-ﬁeld values of the hidden layerof that at(t\u00001)through a set of time-dependentmodel parameters. The RTDRBM learns the distribu-tionP(y(1:T)|x(1:T))over a sequence of input-label pairs{x(1:T),y(1:T)}, in contrast to the RTRBM whichlearns the joint probability of the entire sequenceP(y(1:T),x(1:T))[1].The RTDRBM (Figure 1) is obtained by carrying outdiscriminative learning and inference as put forward in theDiscriminative RBM (DRBM) [13], in a temporal settingby incorporating the recurrent structure of the RTRBM wh-ich was originally proposed as a generative model for high-v(1)v(2)h(0)h(1)h(2)...x(1)y(1)x(2)y(2)...Whhc(1)Whvb(1)Whhc(2)Whvb(2)WUWUFigure 1: The architecture of the RTDRBM, in which thebiases of the visible and hidden layersb(t)andc(t)re-spectively at time-steptare conditioned on the mean-ﬁeldvalues of the hidden layer of the RBMˆh(t\u00001)at time-step(t\u00001). This is also a feature of the RTRBM.dimensional sequences. This results in the following ex-pression for the posterior probabilities at time-stept:P(y(t)|x(1:t))=P(y(t)|x(t),ˆh(t\u00001))(6)It takes into account temporal information carried forwardfrom the previous time-step through the mean-ﬁeld valuesof the hidden unitsˆh(t\u00001)[3]. This can be extended to anentire sequence ofTevents as follows:P(y(1:T)|x(1:T))=TYt=1p(y(t)|x(t),ˆh(t\u00001))(7)One can thus learn the model by maximizing the log-likelihood function:O= logP(y(1:T)|x(1:T))=TXt=1logP(y(t)|x(t),ˆh(t\u00001)).(8)Learning here involves updating the model’s parametersas dictated by the Backpropagation Through Time (BPTT)algorithm [30]. It was demonstrated in [3] that the RT-DRBM outperformed the RTRBM,n-grams and a set ofstandard Connectionist models on a corpus of8differentdatasets of chorale and folk melodies of varying sizes andcomplexities when learned ofﬂine. In the pitch predic-tion task of Section 2, the one-hot encoding of the musicalevents(t)(which is to be predicted) substitutes the labely(t)in (6), whereas that of the most recent event from thecontexts(t\u00001)substitutes the inputx(t).3.2 n-gram ModelThen-gram model is a statistical model of sequences thatrelies on the simplifying assumption that the probability ofan event (or in the present case, a musical event) in a se-quence depends only on the(n\u00001)immediately precedingevents [14]. This is known as the Markov assumption, andis applied to model an event sequences(1:T)asP(s(1:T))=TYt=1P(s(t)|s(t\u0000n+1:t\u00001)).(9)586 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015wherenis known as the order of then-gram. The modelcan be represented by a state transition graph, or by atran-sition matrix. Maximum-Likelihood Estimation can becarried out to estimate the parameters of then-gram model(its transition probabilities) asP(s(t)|s(t\u0000n+1:t\u00001))=N(s(t\u0000n+1:t))N(s(t\u0000n+1:t\u00001))(10)whereN(s(t1:t2))is the number of occurrences of a se-quences(t1:t2)in the data. As we shall see in Section4, this simple learning rule is advantageous in an online-learning scenario where the model needs to be constantlyupdated as it encounters new data. Asn-grams rely ex-plicitly on the occurrence frequencies of sequences, it isoften the case that the model comes across a never-before-encountered context on which to predict the future event,and this is more common in higher order models. Thisissue has been dealt with by usingsmoothedn-grams[2]that use lower-order transition probabilities for generatingapproximations (through interpolation with or scaling of)higher-order probabilities. This also applicable to eventsthat lack a valid context, i.e.{s(t)|1t(n\u00001)}.The present work employs two of the best variants ofthen-gram model evaluated for melody modelling in [22]exclusively as STMs, as an alternative to the RTDRBMwhich performs poorly in this role (Table 3). Both variantsare of unbounded order, wherein they take into account thelongest available matching context (of immediately pre-ceding musical events) in order to make a prediction. Theﬁrst of these (referred to asC⇤I) uses the interpolatedsmoothing method proposed in [18] to account for unfa-miliar contexts. The second (referred to asX⇤UI) uses aPoisson process based interpolated smoothing method [18]with update exclusion [17]. We refer the interested readerto [22] for further details on these two models.4. EXPERIMENTAL RESULTSWe evaluate six different LTM-STM combinations. Theseare listed in Table 1. Also,C⇤IandX⇤UIare the names(a)LTM:RTDRBMSTM:n-gram (X*UI)(b)LTM:RTDRBMSTM:n-gram (C*I)(c)LTM:RTDRBMSTM:RTDRBM(d)LTM+:RTDRBMSTM:n-gram (X*UI)(e)LTM+:RTDRBMSTM:n-gram (C*I)(f)LTM+:RTDRBMSTM:RTDRBMTable 1: Various LTM-STM combinations evaluated here.of the two best STMs evaluated in a previous study ofn-gram based melody models [22].Each of the combined models was evaluated on8melody datasets of different sizes and styles. Predictioncross-entropy was used as the evaluation measure. It wasfound that combination (b) had the best predictive perfor-mance. Furthermore, each case involving an LTM wasDataset No. events|X|Yugoslavian folk songs 2691 25Alsatian folk songs 4496 32Swiss folk songs 4586 34Austrian folk songs 5306 35German folk songs 8393 27Canadian folk songs 8553 25Chorale melodies 9227 21Chinese folk songs 11056 41Table 2: Melody datasets used for evaluation with theirrespective total number of musical events and number ofprediction categories.better than its LTM+ counterpart. And ﬁnally, then-grams consistently proved to be a better choice than theRTDRBM as STMs when combined with the same LTM.4.1 DataEvaluation was carried out on a corpus of8datasets ofmonophonic MIDI melodies from the Essen Folk SongCollection1[25]. The corpus covers a range of musicalstyles and was previously used in [4, 22] to evaluate theirrespective prediction models. It contains folk melodies of7different traditions, and chorale melodies (Table 2). Allmelodies are encoded in the**kernformat in each dataset,and were parsed using theMusic21Python library [8]. Mu-sical pitch, which occurs as sequences of integer values, istreated as a discrete random variableX, which can assumeany of|X|distinct values (or prediction categories).4.2 Evaluation MeasureGiven that the models predict a probability distributionoverXat every time-step, their goal may be viewed asone of minimizing the distance between this predicted dis-tribution and that representing the correct class label (thevalue of the next pitch). An obvious choice of evaluationmeasure in this case would be the information theoreticquantity which calculates this distance: relative entropy.Here we use a measure derived from it known as cross-entropy (Hc), in order to compare our results with previ-ous work [22]. This gives us the mean divergence betweenthe entropy calculated from the predicted distribution andthat of the correct prediction label (and can be interpretedas the distance between these two distributions) for everysample in some given data. It can be computed over allthe events belonging to different sequences in the test dataDtest, asHc(Pmod,Dtest)=\u0000Ps2DtestPTst=1log2Pmod(s(t)|s(1:t\u00001))Ps2DtestTs(11)1Website: http://kern.ccarh.org/browse?l=essenProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 587wherePmodis the probability assigned by the model to thepitch of the events(t)in the melodys2Dtestgiven itspreceding context, andTsis the length ofs. Cross-entropyapproaches the true entropy as the number of test samples,i.e., the denominator in (11) increases.4.3 MethodologyThe models are evaluated using10-fold cross-validation.We use randomised folds identical to those used in previ-ous work [4, 22] to facilitate fair comparison2. A smallpart of the training set (5%) in each fold is extracted asthe validation set for model selection over the various hy-perparameters described below. This procedure is repeatedindependently for each of the8datasets in the corpus.The RTDRBM LTMs were learned (ofﬂine) up to amaximum of250epochs using mini-batch gradient descenton the training set, and that with the best validation setscore was chosen for evaluation on the test set. A gridsearch was carried out to determine the best set of hyper-parameters for each model. These constitute the learningrate⌘, theL1andL2regularization (\u00001and\u00002respec-tively) and the number of hidden unitsnhid. For eachof the models,⌘was varied as{0.01,0.05}, andnhidas{10,25,50,100,200}. BothL1andL2decay were setto identical values\u00001=\u00002=\u0000which was either on(\u0000=0.0001) or off (\u0000=0.0000). Learning rate was madeto decay according to the schedule⌘t=⌘init/(1 +t/⌧),where⌧= 50.The RTDRBM LTM+s and STMs were learned (online)using stochastic gradient descent, where model parameterswere updated after each time-step during prediction on thetest set, with the only distinction between the two beingthat the parameters of the former are initialized to thoseof the best LTM learned ofﬂine on the dataset. As ex-plained in Section 2.1, the LTM+ is continuously updatedacross melodies, while the STM is re-initialized after eachmelody in the test set. Since each of the STMs is expectedto learn a smaller number of patterns than its correspondingLTM, we decided to extend the model selection with muchsmaller models as well (nhid2{2,5,10,20,100,200}),with the remaining hyperparameters kept the same, and aconstant learning rate i.e.,⌘t=⌘init=0.01.The combination bias parameterbfor computing theentropy-based weightswmwas varied asb={0,1,2,3,4,5,6,7,8,16,32}, as in [21]. This range was used for bothcombination rules, following the example of [21].4.4 Results & DiscussionTable 3 shows the predictive performance of variousLTM-STM combination rules evaluated here together withthe corresponding combination bias value used, averagedacross all8datasets. The bottom row of this table cor-responds to the performance of the purelyn-gram basedmelody model in [22], which we compare the models eval-uated here with.2Information about the training/test split in the10folds was obtainedfrom the authors of [22]Model LTM STM Mix.bmProd.bp(a) 2.712 3.053 2.480 3 2.496 1(b) 2.712 3.0462.421 4 2.487 1(c) 2.712 3.363 2.674 5 2.703 1(d) 2.756 3.053 2.574 2 2.563 1(e) 2.756 3.046 2.540 2 2.581 1(f) 2.756 3.363 2.749 5 2.773 1n-gram 2.614 3.147 2.479 2 N/A N/ATable 3: Predictive performance of various model com-binations listed in Table 1, in comparison with a purelyn-gram based melody model (bottom row). Each row ofthe table contains the prediction cross-entropies of the con-stituent LTM (or LTM+), STM, and the combination ofthese two using the Mean and Product rules together withthe respective biases. A lower value of cross-entropy re-ﬂects more accurate predictions.In each case, the RTDRBM LTM has100hidden units(found to be the best in the model selection procedure).Despite the extended grid search for the STMs, it wasfound that the optimal number of hidden units was100inthat case as well.The ﬁrst thing to note is that combining the models (us-ing either of the two combination rules) results in an im-provement in predictive performance over each of the con-stituent models. Furthermore, the Mean combination ruleresults in slightly better prediction cross-entropies thanProduct rule. This can be explained by considering the ba-sic properties of the two rules, as concluded by a previousstudy comparing them [29]. The Mean combination rule isuseful in case of identical or very highly correlated featurespaces (which holds true in the present case) in which clas-siﬁers make independent errors. Furthermore, this rule isgenerally more fault tolerant in the case of poor posteriorprobability estimates (which is indeed the case here withthe STM being learned afresh at the start of each melody),whereas the Product rule emphasizes the points of agree-ment between the two models and is apt where classiﬁersmake small estimation errors. The best combined model(RTDRBM LTM;n-gram (C⇤I) STM) performs slightlybetter than the best purelyn-gram based melody modelin [22]. In the case of both the Mean and Product rules, itwas found that smaller values of the combination bias pa-rameter were preferred over larger ones, with a value of1being consistently optimal in the case of the latter.Another observation is regarding the LTM and LTM+,where the latter performs slightly worse when comparedto the former. This contrasts what has been previouslyobserved when usingn-gram models, where there wasan improvement from the LTM to the LTM+ [22]. Onepossible reason for this could be the absence of any newsequential regularities in the test data to update the al-ready optimized LTM with, since both the training andtest sequences have been sampled from the same data dis-tribution. Alternatively, the gradient-based optimizationprocedure employed here for online learning (stochastic588 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015gradient-descent) might not be the ideal choice for updat-ing the model quickly enough to facilitate an improvementin the predictions. The latter reason could also explain therelatively poor performance of the RTDRBM STMs whencompared to the STMs based onn-grams. This requiresfurther investigation.5. CONCLUSIONS & FUTURE WORKThis paper presented a study on models for melody predic-tion with a long-term and a short-term component (LTMand STM respectively). While all the LTMs explored hereare based on the Recurrent Temporal Discriminative RBM(RTDRBM), the STMs are based both on the RTDRBMandn-gram models. It was found that, while the RT-DRBMs are indeed a suitable choice when learned ofﬂineas LTMs [3], they fail to achieve a predictive performanceas good as that of then-gram models considered here in anonline setting (as in the case of the LTM+ and the STM).The best model in the present work is a combination ofan RTDRBM LTM and ann-gram STM which performsbetter than the state-of-the-art model based purely onn-grams. Among the two combination rules - Mean andProduct - it was found that the former rule works betterwith the models and data used here.One issue that remains unresolved in the present work,and requires investigation in the future, is the lack of im-provement in predictions during online learning in the RT-DRBM LTM. Another extension to the models employedhere is to incorporate additional melodic features as inputs,as detailed inMultiple Viewpoints for Music Prediction[6],and to examine how this would improve or worsen the pre-dictive performance over the existing models. And ﬁnally,previous work with LTMs and STMs based purely onn-gram models has found the predictions made by these mod-els to reﬂect the musical expectations of human subjects.This is also relevant to the models explored here, and is ofinterest in the future.6. ACKNOWLEDGEMENTSSrikanth Cherla and Son N. Tran are supported by Ph.D.studentships from City University London. The authorswould like to thank Marcus Pearce (Queen Mary Univer-sity of London) and Senanayak Sesh Kumar Karri (INRIARocqencourt) for their advice and helpful discussions re-lated to parts of this paper.7. REFERENCES[1]Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent. Modeling Temporal Dependencies inHigh-Dimensional Sequences: Application to Poly-phonic Music Generation and Transcription. InIn-ternational Conference on Machine Learning, pages1159–1166, 2012.[2]Stanley Chen and Joshua Goodman. An empiricalstudy of smoothing techniques for language modeling.Computer Speech & Language, 13(4):359–393, 1999.[3]Srikanth Cherla, Son Tran, Artur d’Avila Garcez, andTillman Weyde. Discriminative Learning and Inferencein the Recurrent Temporal RBM for Melody Mod-elling. InInternational Joint Conference on NeuralNetworks, 2015.[4]Srikanth Cherla, Tillman Weyde, Artur d’Avila Garcez,and Marcus Pearce. A Distributed Model for Multiple-Viewpoint Melodic Prediction. InInternational Soci-ety for Music Information Retrieval Conference, pages15–20, 2013.[5]Joel E Cohen. Information Theory and Music.Behav-ioral Science, 7(2):137–163, 1962.[6]Darrell Conklin and Ian Witten. Multiple ViewpointSystems for Music Prediction.Journal of New MusicResearch, 24(1):51–73, 1995.[7]Greg Cox. On the Relationship Between Entropy andMeaning in Music: An Exploration with RecurrentNeural Networks. InAnnual Conference of the Cog-nitive Science Society, pages 429–434, 2010.[8]Michael Cuthbert and Christopher Ariza. music21: AToolkit for Computer-aided Musicology and SymbolicMusic Data. InInternational Society for Music Infor-mation Retrieval Conference, pages 637–642, 2010.[9]Tuomas Eerola, Petri Toiviainen, and CarolKrumhansl. Real-Time Prediction of Melodies:Continuous Predictability Judgements and DynamicModels. InInternational Conference on MusicPerception and Cognition, pages 473–476, 2002.[10]David Huron.Sweet Anticipation: Music and the Psy-chology of Expectation. MIT Press, 2006.[11]Timothy Justus and Jamshed Bharucha. Modularity inMusical Processing: The Automaticity of HarmonicPriming.Journal of Experimental Psychology: HumanPerception and Performance, 27(4):1000–1011, 2001.[12]Roland Kuhn and Renato De Mori. A Cache-basedNatural Language Model for Speech Recognition.Pat-tern Analysis and Machine Intelligence, IEEE Trans-actions on, 12(6):570–583, 1990.[13]Hugo Larochelle and Yoshua Bengio. Classiﬁcationusing Discriminative Restricted Boltzmann Machines.InInternational Conference on Machine Learning,pages 536–543, 2008.[14]Christopher Manning and Hinrich Schütze.Founda-tions of Statistical Natural Language Processing. MITPress, 1999.[15]Leonard Meyer.Emotion and Meaning in Music. Uni-versity of Chicago Press, 1956.[16]Leonard Meyer. Meaning in Music and InformationTheory.Journal of Aesthetics and Art Criticism, pages412–424, 1957.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 589[17]Alistair Moffat. Implementing the PPM data compres-sion scheme.Communications, IEEE Transactions on,38(11):1917–1921, 1990.[18]Alistair Moffat, Radford Neal, and Ian Witten. Arith-metic Coding Revisited.ACM Transactions on Infor-mation Systems (TOIS), 16(3):256–294, 1998.[19]Eugene Narmour.The Analysis and Cognition ofMelodic Complexity: The Implication-RealizationModel. University of Chicago Press, 1992.[20]Diana Omigie, Marcus Pearce, Victoria Williamson,and Lauren Stewart. Electrophysiological Correlates ofMelodic Processing in Congenital Amusia.Neuropsy-chologia, 2013.[21]Marcus Pearce.The Construction and Evaluation ofStatistical Models of Melodic Structure in Music Per-ception and Composition. PhD thesis, City UniversityLondon, 2005.[22]Marcus Pearce and Geraint Wiggins. Improved Meth-ods for Statistical Modelling of Monophonic Music.Journal of New Music Research, 33(4):367–385, 2004.[23]Martin Rohrmeier and Stefan Koelsch. Predictive In-formation Processing in Music Cognition. A CriticalReview.International Journal of Psychophysiology,83(2):164–175, 2012.[24]Jenny Saffran, Elizabeth Johnson, Richard Aslin, andElissa Newport. Statistical Learning of Tone Se-quences by Human Infants and Adults.Cognition,70(1):27–52, 1999.[25]Helmut Schaffrath and David Huron. The Essen Folk-song Collection in the Humdrum Kern Format. 1995.[26]Siddharth Sigtia, Emmanouil Benetos, NicolasBoulanger-Lewandowski, Tillman Weyde, Arturd’Avila Garcez, and Simon Dixon. A Hybrid Re-current Neural Network For Music Transcription. InInternational Conference on Acoustics Speech andSignal Processing, 2015.[27]Paul Smolensky. Parallel Distributed Processing: Ex-plorations in the Microstructure of Cognition, Vol. 1.chapter Information Processing in Dynamical Systems:Foundations of Harmony Theory, pages 194–281. MITPress, 1986.[28]Ilya Sutskever, Geoffrey Hinton, and Graham Taylor.The Recurrent Temporal Restricted Boltzmann Ma-chine. InAdvances in Neural Information ProcessingSystems, pages 1601–1608, 2009.[29]David Tax, Martijn Van Breukelen, Robert Duin, andJosef Kittler. Combining Multiple Classiﬁers by Av-eraging or by Multiplying?Pattern Recognition,33(9):1475–1485, 2000.[30]Paul Werbos. Backpropagation Through Time: WhatIt Does and How to Do It.Proceedings of the IEEE,78(10):1550–1560, 1990.[31]Raymond Whorley.The Construction and Evaluationof Statistical Models of Melody and Harmony. PhDthesis, Goldsmiths, University of London, 2013.590 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Singing Voice Separation from Monaural Music Based on Kernel Back-Fitting Using Beta-Order Spectral Amplitude Estimation.",
        "author": [
            "Hye-Seung Cho",
            "Jun-Yong Lee",
            "Hyoung-Gook Kim"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417044",
        "url": "https://doi.org/10.5281/zenodo.1417044",
        "ee": "https://zenodo.org/records/1417044/files/ChoLK15.pdf",
        "abstract": "Separating  the  leading singing  voice  from  the  musical background  from  a  monaural  recordingis  a  challenging task  that  appears  naturally  in  several  music  processing applications.Recently,   kernel   additive modelingwith generalized  spatial  Wiener  filtering  (GW)  was  presented for  music/voice  separation. In this paper, an adaptive  au-ditory filteringbased  on β-order minimum  mean-square error spectral  amplitudeestimation  (bSA)is  applied  to the kernel additivemodeling for improving  the  singing voice separation performance  frommonaural  music sig-nal. The proposed algorithm is composed of five modules: short   time   Fourier   transform,   music/voice   separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, theSingular Value  Decomposition (SVD)-based factorized  spectral  amplitude  exponent βfor  each  kernel component isadaptively  calculated  for  effective  bSA-based  auditory  filtering  performance  during  kernel  back-fitting.  Using a back-fitting  threshold, the kernel  back-fitting process canautomatically be iteratively performed until  convergence.  Experimental  results  show  that  the proposed  method  achieves  better  separation  performance than GW based on kernel additive modeling",
        "zenodo_id": 1417044,
        "dblp_key": "conf/ismir/ChoLK15",
        "keywords": [
            "Singing Voice Separation",
            "Monaural Recording",
            "Music Processing",
            "Kernel Additive Modeling",
            "Generalized Spatial Wiener Filtering",
            "Auditory Filtering",
            "Spectral Amplitude Estimation",
            "Singular Value Decomposition",
            "Back-Fitting",
            "Fourier Transform"
        ],
        "content": "SINGING VOICE SEPARATION FROM MONAURAL MUSIC BASED ON KERNEL BACK-FITTING USING BETA-ORDER SPECTRAL AMPLITUDE ESTIMATION Hye-Seung Cho, Jun-Yong Lee, Hyoung-Gook Kim Kwangwoon University, Seoul, Rep. of Korea {hye_seung401,jasonlee88,hkim}@kw.ac.kr ABSTRACT Separating the leading singing voice from the musical background from a monaural recording is a challenging task that appears naturally in several music processing applications. Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive au-ditory filtering based on β-order minimum mean-square error spectral amplitude estimation (bSA) is applied to the kernel additive modeling for improving the singing voice separation performance from monaural music sig-nal. The proposed algorithm is composed of five modules: short time Fourier transform, music/voice separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, the Singular Value Decomposition (SVD)-based factorized spectral amplitude exponent β for each kernel component is adaptively calculated for effective bSA-based auditory filtering performance during kernel back-fitting. Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until convergence. Experimental results show that the proposed method achieves better separation performance than GW based on kernel additive modeling. 1. INTRODUCTION A singing voice in a music signal contains useful infor-mation for a song, as it embeds the singer, the lyrics, and the emotion of the song. Therefore, vocal or singing voice separation from monaural music signal is an important task in many applications, such as automatic karaoke [1], instrument/vocalist identification [2], music/voice tran-scription, music remixing [3] and audio restoration. So far, numerous vocal separation algorithms have been proposed with various approaches, such as non-negative matrix factorization [4], adaptive Bayesian mod-eling [5], and pitch-based interference [6-7]. These meth-ods usually first map signals onto a feature space, then detect singing voice segments, and finally apply source separation. Recently, a relatively promising approach using kernel additive modeling (KAM) was proposed [8], wherein the spectrogram of each source is modeled only locally. This approach encompasses a large number of recently pro-posed methods for source separation [9-14]. KAM per-mits the use of different proximity kernels for different sources, with separation using an iterative kernel back-fitting (KBF) algorithm. In the kernel back-fitting, gener-alized Wiener filtering (GW) is used for the step of mixed music signal separation, and two-dimensional median fil-tering is applied to the power spectrogram of each source estimate for kernel spectrogram model fitting at each iter-ation. The GW requires good models of the spectrograms of each proximity source along with its spatial character-istics and permits very good separation provided these parameters are well estimated. In spoken speech enhancement, one source may be the target voice, while others correspond to background noise which must be filtered out. Among the vast amount of single channel speech enhancement algorithms based on minimum mean-square error (MMSE) estimation of short-time spectral amplitude (STSA) published in the literature, it is well-known that the Bayesian STSA esti-mation methods [15] outperform the Wiener filtering, spectral-subtraction, and subspace approaches. In addi-tion, among the Bayesian STSA estimation methods, β-order MMSE spectral amplitude estimation [15-17] achieved better enhancement performance than the exist-ing Bayesian estimators, such as those based on the MMSE of the short-time spectral amplitude [15-17], and the MMSE of the logarithm of the STSA (LSA) [15-17]. In this paper, an advanced music/voice separation method is proposed, in which β-order MMSE spectral amplitude estimation and kernel spectrogram back-fitting are combined for improvement of the separation perfor-mance. In addition, the parameter β concerned in β-order MMSE spectral amplitude estimation is adaptively esti-mated according to the masking mechanism of human auditory system, the compressive nonlinearities of the cochlea and the critical sub-band SNR. The proposed method has the following four ad-vantages: (1) In the separation step, β-order MMSE estimation (bSA) of the factorized spectral amplitude © Hye-Seung Cho, Jun-Yong Lee, Hyoung-Gook Kim. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Hye-Seung Cho, Jun-Yong Lee, Hyoung-Gook Kim. “Singing Voice Separation from Monaural Music Based on Kernel Back-fitting Using Beta-Order Spectral Amplitude Estimation”, 16th International Society for Music Information Retrieval Conference, 2015. \n639   was used instead of GW for the kernel back-fitting procedure to achieve better separation performances. (2) The Singular Value Decomposition (SVD)-based factorized spectral amplitude βj were adaptively calcu-lated for effective bSA estimation performance. (3) In the back-fitting step, an SVD-based factorization pro-cedure was applied to the power spectrogram filtered by median filter to achieve efficient compression be-fore processing of the next proximity source. (4) Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until con-vergence. This paper is organized as follows. Section 2 describes the proposed method, while Section 3 discusses the ex-perimental results. Finally, the conclusion is presented in Section 4. 2. PROPOSED MUSIC/VOICE SEPARATION ALGORITHM The proposed algorithm is composed of five modules: short time Fourier transform (STFT), music/voice separa-tion based on β-order MMSE spectral amplitude estima-tion (bSA), determination of back-fitting, back-fitting, and inverse short time Fourier transform (ISTFT). Figure 1 denotes the overall procedure of the proposed music/voice separation algorithm. \nShort-time Fourier transform (STFT)\nMusic/Voice separation based on bSA\nInverse Short-time Fourier transform (ISTFT)\nBack-fitting\nMonaural music signal\nSeparated music and voice signal\nx(n)\nOj(ω,t)\nX(ω,t)\nSj(ω,t)\nYesNo\nIs the music/voice separation sufficient?)(noj Figure 1. Overall flow chart of proposed music/voice separation algorithm. We assume that the mixture music signal, x(n), is tak-en as the sum of j underlying sources that are composed of some of percussive elements, one of the stable har-monic elements, and one of the singing voice. Let a real-valued monaural music signal in discrete-time domain x(n) be assumed as: \u000b\f¦  Jjjnonx1)(                             (1) where j (= 1, 2, … J) is index of each objective sources, n is sample index, and oj(n) denotes an objective source in mixture music signal. First, an input monaural music signal x(n) is trans-formed into the complex spectrogram X(ω,t) using the short-time discrete Fourier transform (STFT), as shown: \u000b\f\u000b\f\u000b\f¸¹·¨©§\u0010\u000e ¦\u0010 NninwnRtxtNnSZZ2exp,X10           (2) where R denotes the frame shift, t is the frame index, w(n) indicates a window function, N is size of window, and ω is the frequency bin index, which is related to the normal-ized center frequency. From the input complex spectrogram X(ω,t), complex spectrogram Oj(ω,t) for each objective sources is estimat-ed by β-order MMSE spectral amplitude estimation. Each current estimated spectrogram is compared with each previous estimated complex spectrogram. If the dif-ference between the current and previous estimated spec-trograms is not larger than the back-fitting threshold val-ue, each complex spectrogram is converted back to the time domain using an inverse STFT. Conversely, if the difference between the two is larger than back-fitting threshold value, the kernel back-fitting process is iterated until convergence. During the back-fitting processes, the power spectro-gram of the estimated spectrogram is filtered by a simple two dimensional median filter with source-specific binary kernels. The source-specific binary kernels are explained in detail in next sub-section. This kernel back-fitting proceeds in an iterative fash-ion, with alternate performance of separation and re-estimation (back-fitting) of the parameters to obtain new spectrogram estimates for each source. 2.1 Re-estimation using back-fitting The re-estimation using back-fitting permits one to use different proximity kernels for each source and to sepa-rate them in order to perform the estimation. It assumes that vertical lines in a spectrogram correspond to percus-sive events; horizontal lines are typically associated with harmonics of pitched instruments, while cross-like forms correspond to singing voice events. In this case, peaks due to pitched harmonics can be regarded as outliers on the vertical lines associated with percussive events. Simi-larly, peaks due to the percussion events can be regarded as outliers on the horizontal lines associated with pitched harmonic instruments. Median filters used extensively in image processing are good at eliminating outliers. That is, 640 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   median filtering each time frame will suppress harmonics in this frame resulting in a percussion enhanced frame, while median filtering each frequency slice will suppress percussion events. This brings to the concept of using median filters individually in the horizontal, vertical, and cross-like directions to separate harmonic, percussive and vocal events. The process is as follows: (Step 1) Using the estimated complex spectrogram Oj(ω,t), the power spectrogram of the complex spectro-gram is calculated as:  ),(),( 2tOtVjjZZ                         (3) (Step 2) A simple two dimensional median filter is ap-plied to the power spectrogram Vj(ω,t) of the complex spectrogram with source-specific binary kernels, vocal, harmonic, and percussive. The different three proximity kernels [8] used for the median filter are as follows: (1) For a percussive and a repeating source, the vertical ker-nel is chosen; (2) For a harmonic source, the horizontal kernel is chosen; (3) Finally, for a source with only a spectral smoothness assumption, the cross-like kernel is chosen as vocals. The detailed three kernels are explained in the source separation using kernel additive models [8]. The median filtered kernel spectrogram is given by: )],(K |),([),( ttVmediantMjjjZZZ           (4) where Kj(ω,t) is a kernel which includes percussive ele-ments of periodic components (j = 1, 2, ... J-2), the stable harmonic elements (j = J-1), and the singing voice (j = J),  respectively. In effect, the original sample of the power spectrogram Vj(ω,t) of the complex spectrogram is re-placed with the middle value obtained from a sorted list of the samples in neighborhoods of the original sample according to each kernel. (Step 3) Kernel back-fitting using Wiener filtering or the β-order spectral amplitude estimator comes with an important drawback: it requires the full-resolution spec-trogram, and storage of a huge amount of parameters in each iteration, and for each source. To reduce the memory usage and improve the separation performance while maintaining computational efficiency, Singular Value Decomposition (SVD) is applied to the full-resolution spectrogram Mj(ω,t): >@),(SVD),(tMCDtSjjjjjZZ 6               (5) where Mj(ω,t) is factored into the matrix product of three matrices: the M × M row basis Dj matrix, the M × L diag-onal singular value matrix Σj and the L × L transposed column basis functions Cj. 2.2 Separation using β-order MMSE spectral ampli-tude estimation In the separation step, β-order MMSE spectral amplitude estimation of the factorized spectral amplitude is used instead of GW for the kernel back-fitting procedure to achieve better music/voice separation performances. In the β-order MMSE spectral amplitude estimation, the spectral amplitude order β is quite important for singing voice enhancement or separation from monaural music signal. For the different β values, the gain values are dif-ferent, and noise or other source reduction obtained is al-so different. In this way, the appropriated gain can be ob-tained by adaptively choosing right β. However, the traditional calculation method about β is based on overall Signal-to-Noise Ratio (SNR) of each frame. That is, their values are fixed and not vary with frequency in each frame. Furthermore, the human audito-ry system has different sensitivity for different frequency components. Therefore, the b-th critical sub-band SNR is employed to calculate β values. For more effective bSA estimation performance, the Singular Value Decomposi-tion (SVD)-based factorized spectral amplitude order βj(b,t) is adaptively calculated. Using adaptive β values and Singular Value Decomposition (SVD)-based factor-ized spectral amplitude, we can yield effective mu-sic/voice separation and obtain a good enhancement per-formance. 2.2.1 β-order MMSE spectral amplitude estimation The β-order MMSE spectral amplitude estimation is composed of following four modules: sum of all Sj(ω,t), calculation of a priori SNR and a posteriori SNR, calcula-tion of adaptive βj(b,t), and bSA-based gain function. Figure 2 shows the β-order MMSE spectral ampli-tude estimation. \nSVD-based factorized spectrogram\nInput complex spectrogram\nβ-order MMSE spectal amplitude estimation\n βj(b,t)\nCalculation of a priori SNR anda posteriori SNR\nSum of all Sj(ω,t)\nCalculation of adaptive βj(b,t)\nGain functoin\nX\nSj(ω,t)\nX(ω,t)\nξj(ω,t) γj(ω,t)\nGj(ω,t)\nW(ω,t)\nOj(ω,t)\nW(ω,t)\n Figure 2. Overall flow chart of the β-order MMSE spec-tral amplitude estimation. Before to obtain the estimated complex spectrum Oj(ω,t) from SVD-based factorized Sj(ω,t), the sum W(ω,t) of all Sj(ω,t) is defined by: \u000b\f\u000b\f\u000b\f\u000b\ftStStStWJ,...,,,21ZZZZ\u000e\u000e\u000e              (6) Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 641   Then, the a priori SNR ξj(ω,t) and the a posteriori SNR γj(ω,t) of each objective proximity sources are calculated as follows:    ;),(),(),(),(tStWtStjjjZZZZ[\u0010                  (7) ;),(),(),(),(2tStWttjjZZZZJ\u0010&                    (8)  ; ),(),(1),(),(ttttjjjjZJZ[Z[ZF\u000e                 (9) where χj(ω,t) is the function of ξj(ω,t) and γj(ω,t). The gain function Gj(ω,t) for the bSA is given by: ),(1),(;1,2),(12),(),(),(),(tbjjjjjjjttbtbtttGEZXEEZJZFZ»»¼º««¬ª¸¸¹·¨¨©§\u0010)¸¸¹·¨¨©§\u000e* (10) where Γ(•) is the gamma function, Φ(•) is the confluent hypergeometric function. And βj(b,t) denotes the parame-ter based on the human auditory system. To calculate βj(b,t), we employ the critical sub-band SNR. The b critical bands are divided for each speech frame, where a non-linear mel-frequency scale is used, which approximates the behavior of the auditory sys-tem. The mel-scale is a scale of pitches judged by lis-teners to be equal in distance one from another. The reference point between this scale and normal frequen-cy measurement is defined by equating a 1000 Hz tone, 40 dB above the listener’s threshold, with a pitch of 1000 mels. To convert a frequency ω in hertz into its equivalent in mel, the following formula is used: \u000b\f\u000b\f¸¹·¨©§\u000e 7001log0148.1127HzmelpitchZ       (11) The spectrum is then processed by a mel-filter bank. The signal energy of the spectrum within b-th critical frequency sub-bands by means of a series of triangular filters whose center frequency are spaced according to the mel-scale. Thereafter, the critical sub-band SNR Zj(b,t) is calculated in the b-th band. Finally, the estimated complex spectrogram from the gain function is defined as: \u000b\f\u000b\f\u000b\fttGtOjj,,,ZZZ&                       (12) 2.2.2 Calculation of adaptive βj(b,t) Since the spectral amplitude order βj(b,t) is based on characteristics of the human auditory system, including the compressive nonlinearities of the cochlea, and the perceived loudness, the choosing of adequate value for βj(b,t) can result in better enhancement or separation per-formance. First, using W(ω,t) and Sj(ω,t), the sub-band SNR Zj(b,t) is calculated as: \u000b\f\u000b\f\u000b\f\u000b\f\u000b\f¦¦  \u0010\u0010\u0010 bupBblowBjbupBblowBjjtStWtStWtWtbZZZZZZZZ),(),(),(),(),(log10),(210  (13) where b ∊ [0, 23] denotes the index of critical band. Bup(b) and Blow(b) denote the upper and lower frequency bound of the b-th critical band, respectively. To obtain βj(b,t), the compression rate ),(tbβj at inter-mediate frequencies can be calculated through linear in-terpolation between βlow and βhigh. That is, \u000b\f\u000b\fJjtbdtblowhighhighjdd\u0010\u0010 1for    ,),(EEEE      (14) using \u000b\f\u000b\f\u000b\f\u000b\f\u000b\f¦ ¿¾½¯®­¸¹·¨©§\u000e\u0010 bupBblowBlowuplAfbBbBtbdZZK10log11,      (15) where d(b,t) is the frequency-position function to the crit-ical band, βhigh = 0.2 and βlow = 1 denote the low-frequency and high-frequency of the compression rate, η = 0.06 mm, l = 1, and A = 165.4 Hz are the parameters set in paper [18], and fω is the frequency in Hz corresponding to spectral component ω, i.e., fω = ωFs/N, where Fs is the sampling frequency. By limiting the range of ),(tbjE as [βmin, βmax] in order to obtain a better trade-off between target source en-hancement and other source reduction,),(tbjE can be cal-culated through the following relationship: >@^` ,,),(maxmin),(maxminEEOP\u000e tbZtbβjj    (16) where μ = 0.45, λ = 1.3, βmin = 0.4, and βmax = 4.0. According to sub-band SNR, the compressive nonline-arities of the cochlea, and perceived loudness, a parame-ter βj(b,t) is given as follows: \u000b\f),(1),(),( tbβ-qtbβqtbβjjj\u000e            (17) where q (0 < q < 1) is a smoothing parameter. 3. EXPERIMENTAL RESULTS In this section, the performance of the proposed bSA-KBF algorithm is evaluated for the separation of back-ground music and singing voice. For experiments, 100 full-length song tracks were used (50 songs from the ccMixter database containing many different musical genres, 50 songs from a self-recording studio music database), where all singing voices and mu-sic accompaniments were recorded separately. All of the song data were stored in PCM format with mono, 16-bit depth, and 44.1 kHz sampling rate. For each track, the accompaniment of 6 repeating pat-terns along with a 2 second steady harmonic source was determined. Vocals were modeled using a cross-like ker-642 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   nel with a height of 15 Hz and width of 20 ms. The frame length was set to 90 ms, with 80% overlap. Six to eight iterations were performed for the back-fitting algorithm (approximately until convergence). For the performance measures, performance was eval-uated in terms of Normalized Source-to-Interference Ra-tio (NSIR) and Normalized Source-to-Distortion Ratio (NSDR) by Blind Source Separation Evaluation (BSS Eval) metrics [19]. NSDR and NSIR for singing voice are defined as: \u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\fvxvvxvvvxvvxvvrrrr,SIR,SIR,,NSIR,SDR,SDR,,NSDR\u0010 \u0010            (18) where vr is the synthesized singing voice, v is the original clean singing voice, and x is the mixture. NSDR is for estimating the improvement of the SDR between the pro-cessed mixture x and the separated singing voice vr. Higher values indicate better separation. The performance of the proposed bSA algorithm was compared with those of GW, LSA based on KAM. Table 1 presents the experimental results of compara-tive performance for music/voice separation of the four methods: • STFT-GW-KAM: As a basic KAM algorithm, the generalized Wiener filter was applied to the power spectrogram based on STFT. • SVD-GW-KAM: SVD was performed on the power spectrogram based on STFT. To the SVD-based de-composed power spectrogram, the generalized Wie-ner filter was applied. • SVD-LSA-KAM: The MMSE of the logarithm of the STSA was applied to the SVD-based decomposed power spectrogram. • SVD-bSA-KAM: β-order MMSE STSA was applied to the SVD-based decomposed power spectrogram.  Methods Separation Per-formance for Music Separation Per-formance for Voice NSDR NSIR NSDR NSIR STFT-GW-KAM 6.37 9.18 1.89 5.76 SVD-GW-KAM 6.83 9.65 2.35 6.23 SVD-LSA-KAM 7.36 10.48 2.87 6.74 SVD-bSA-KAM 8.25 12.13 3.12 6.88 Table 1. Comparative performance for music/voice sepa-ration. As shown in Table 1, the best separation performance of the music from the mixed music signal is obtained with the proposed method, SVD-bSA-KAM, in terms of NSDR and NSIR. Compared to the other three methods, the basic method, STFT-GW-KAM, attains the worst re-sults. And the proposed bSA delivers high performance result in the separation of vocal components. 4. CONCLUSIONS In this paper, we proposed a β-order MMSE spectral am-plitude estimation method based on kernel back-fitting for music/voice separation. The proposed algorithm en-hances the basic kernel back-fitting algorithm through application of β-order MMSE spectral amplitude estima-tion considering the perceptual properties of human audi-tory system. The experimental results show that the pro-posed method obtained better results compared to other existing methods. In future work, we will apply the method to spatial au-dio reproduction applications running on smart phones. 5. ACKNOWLEDGMENTS This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education, Sci-ence and Technology(NRF-2013R1A1A2007601). And this research was supported by the MSIP(Ministry of Sci-ence, ICT and Future Planning), Korea, under the ITRC(Information Technology Research Center) support program (IITP-2015-H8501-15-1016) supervised by the IITP(Institute for Information & communications Tech-nology Promotion) 6. REFERENCES [1] Z. Rafii and B. Pardo: “REpeating Pattern Extraction Technique (REPET): A Simple Method for Music/Voice Separation,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 21, No. 1, pp. 73–84, 2012. [2] N. C. Maddage, C. Xu, and Y. Wang: “Singer identification Based on Vocal and Instrumental Models,” in Proceedings of the 17th International Conference on Pattern Recognition, Vol. 2, pp. 375–378, 2004. [3] S. Marchand et al: “DReaM: A Novel System for Joint Source Separation and Multi-Track Coding,” in Proceedings of the 133rd Audio Engineering Society Convention, 2012. [4] S. Vembu and S. Baumann: “Separation of Vocals from Polyphonic Audio Recordings,” in Proceedings of the International Society for Music Information Retrieval Conference, pp. 337–344, 2005. [5] A. Ozerov, P. Philippe, F. Bimbot, and R. Gribonval: “Adaptation of Bayesian Models for Single-Channel Source Separation and its Application to Voice/Music Separation in Popular Songs,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 15, No. 5, pp. 1564–1578, 2007. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 643   [6] Y. Li and D. Wang: “Separation of Singing Voice From Music Accompaniment for Monaural Recordings,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 15, No. 4, pp. 1475–1487, 2007. [7] C. -L. Hsu and J. -S. R. Jang: “On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 18, No. 2, pp. 310–319, 2009. [8] A. Liutkus, D. Fitzgerald, Z. Rafii, B. Pardo, and L. Daudet: “Kernel Additive Models for Source Separation,” IEEE Transactions on Signal Processing, Vol. 62, No. 16, pp. 4298–4310, 2014. [9] Z. Rafii and B. Pardo: “Repeating Pattern Extraction Technique (REPET): A Simple Method for Music/Voice Separation,” IEEE Transaction on Audio, Speech and Language Processing, Vol. 21, No. 1, pp. 73\u001084, 2013. [10] D. Fitzgerald: “Harmonic/Percussive Separation Using Median Filtering,” in Proceedings of the 13th International Conference on Digital Audio Effects (DAFx-10), 2010. [11] Z. Rafii and B. Pardo: “A Simple Music/Voice Separation Method Based on the Extraction of the Repeating Musical Structure,” in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 221–224, 2011. [12] A. Liutkus et al: “Adaptive Filtering for Mu-sic/Voice Separation Exploiting the Repeating Musical Structure,” in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 53–56, 2012. [13] Z. Rafii and B. Pardo: “Music/Voice Separation Using the Similarity Matrix,” in Proceedings of the International Society for Music Information Retrieval Conference, pp. 583–588, 2012. [14] O. Yilmaz and S. Rickard: “Blind Separation of Speech Mixtures via Time-Frequency Masking,” IEEE Transaction on Signal Processing, Vol. 52, No. 7, pp. 1830–1847, 2004. [15] E. Plourde and B. Champagne: “Auditory-Based Spectral Amplitude Estimators for Speech Enhancement,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 16, No. 8, pp. 1614–1623, 2008. [16] F. Deng, F. Bao and C. -C. Bao: “Speech Enhancement Using Generalized β-Order Spectral Amplitude Estimator,” in Proceedings of the Speech Communication, Vol. 59, pp. 55–68, 2014. [17] C. H. You, S. N. Koh, and S. Rahardja: “β-Order MMSE Spectral Amplitude Estimation for Speech Enhancement,” IEEE Transactions on Speech and Audio Processing, Vol. 13, No. 4, pp. 475–486, 2005. [18] D. D. Greenwood: “A Cochlear Frequency-Position Function for Several Species-29 Years Later,” Journal of Acoustic Society America, Vol. 87, No. 6, pp. 2592–2605, 1990. [19] E. Vincent, R. Gribonval, and C. Fevotte: “Performance Measurement in Blind Audio Source Separation,” IEEE Transactions on Audio, Speech and Language Processing, Vol. 14, No. 4, pp. 1462–1469, 2006. 644 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Analysis of Intonation Trajectories in Solo Singing.",
        "author": [
            "Jiajie Dai",
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417169",
        "url": "https://doi.org/10.5281/zenodo.1417169",
        "ee": "https://zenodo.org/records/1417169/files/DaiMD15.pdf",
        "abstract": "We present a new dataset for singing analysis and mod- elling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: “Edelweiss”, “Do-Re- Mi” and “My Favourite Things”. 39 participants sang three repetitions of each excerpt without accompaniment, result- ing in a dataset of 21762 notes in 117 recordings. To ob- tain pitch estimates we used the Tony software’s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers’ pitch accuracy correlates signifi- cantly with self-reported singing skill and musical train- ing. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a sig- nificant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal ref- erence deviation (TRD). The data and software are freely available. 1",
        "zenodo_id": 1417169,
        "dblp_key": "conf/ismir/DaiMD15",
        "keywords": [
            "new dataset",
            "pitch accuracy",
            "pitch trajectories",
            "Tony software",
            "automatic transcription",
            "manual correction",
            "Singing analysis",
            "modelling",
            "self-reported singing skill",
            "musical training"
        ],
        "content": "ANALYSIS OF INTONATION TRAJECTORIES IN SOLO SINGINGJiajie Dai, Matthias Mauch, Simon DixonCentre for Digital Music, Queen Mary University of London, United Kingdom{j.dai, m.mauch, s.e.dixon}@qmul.ac.ukABSTRACTWe present a new dataset for singing analysis and mod-elling, and an exploratory analysis of pitch accuracy andpitch trajectories. Shortened versions of three pieces fromThe Sound of Musicwere selected: “Edelweiss”, “Do-Re-Mi” and “My Favourite Things”. 39 participants sang threerepetitions of each excerpt without accompaniment, result-ing in a dataset of 21762 notes in 117 recordings. To ob-tain pitch estimates we used theTonysoftware’s automatictranscription and manual correction tools. Pitch accuracywas measured in terms of pitch error and interval error.We show that singers’ pitch accuracy correlates signiﬁ-cantly with self-reported singing skill and musical train-ing. Larger intervals led to larger errors, and the tritoneinterval in particular led to average errors of one third of asemitone. Note duration (or inter-onset interval) had a sig-niﬁcant effect on pitch accuracy, with greater accuracy onlonger notes. To model drift in the tonal centre over time,we present a sliding window model which reveals patternsin the pitch errors of some singers. Based on the trajectory,we propose a measure for the magnitude of drift: tonal ref-erence deviation (TRD). The data and software are freelyavailable.11. INTRODUCTIONSinging is common in all human societies [2], yet the fac-tors that determine singing proﬁciency are still poorly un-derstood. Many aspects are important to singing, includingpitch, rhythm, timbre, dynamics and lyrics; here we fo-cus entirely on the pitch dimension. Music psychologistshave studied singing pitch [4, 6, 18], and engineers havedeveloped advanced software for automatic pitch track-ing [5, 11, 21], but the process of annotating and analysingthe pitch of singing data remains a laborious task. In thispaper, we present a new extensive dataset for the analy-sis of unaccompanied solo singing, complete with audio,pitch tracks, and hand-annotated note tracks matched tothe scores of the music. In addition, we provide an anal-ysis of the data with a focus on intonation: pitch errors,1see Data Availability, Section 7c\u0000Jiajie Dai, Matthias Mauch, Simon Dixon.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Jiajie Dai, Matthias Mauch, SimonDixon. “Analysis of Intonation Trajectories in Solo Singing”, 16th Inter-national Society for Music Information Retrieval Conference, 2015.interval errors, pitch drift, and the factors that inﬂuencethese phenomena.Intonation, deﬁned as “accuracy of pitch in playing orsinging” [23], or “the act of singing or playing in tune”[12], is one of the main priorities in choir rehearsals [9] andin choral practice manuals (e.g. [3]). Good intonation in-volves the adjustment of pitch to maximise the consonanceof simultaneous notes, but it also has a temporal aspect,particularly in the absence of instrumental accompaniment,where the initial tonal reference can be forgotten over time[15]. A cappella ensembles frequently observe a change intuning over the duration of a piece, even when they are un-able to detect any local changes. This phenomenon, calledintonation driftor pitch drift [22], usually exhibits as alowering of pitch, or downward drift [1]. Several studiespresent evidence that drift is induced by harmonic progres-sions as singers negotiate the tradeoff between staying intune and singing in just intonation [7,10,24]. Yet this is notthe only cause of drift, since drift is also observed in solosinging, such as unaccompanied solo folk songs [17] andeven queries to query-by-humming systems [20]. A factorthat has received relatively little attention in the singing re-search community is the effect of note duration on singingaccuracy [8], so one of our aims in this paper is to explorethe effect of duration.The deﬁnitions of intonation given above imply the ex-istence of a reference pitch, which could be provided by ac-companying instruments or (as in the present case) couldexist solely in the singer’s memory. This latter case al-lows for the reference to change over time, and thus explainthe phenomenon of drift. We introduce a novel method tomodel this internal reference as the pitch which minimisesthe intonation error given some weighted local context,and we compare various context windows for parametris-ing our model. Using this model of reference pitch, wecompute pitch error as the signed pitch difference relativeto the reference pitch and score, measured in semitones onan equal-tempered scale. Interval error is measured on thesame scale, without need of any reference pitch, and pitchdrift is given by the trajectory of score-normalised refer-ence pitch over time.In this paper we explore which factors may explain into-nation error in our singing data. The effects of four singerfactors, obtained by self-report, were tested for signiﬁ-cance. Most of the participants in this study were amateursingers without professional training. Their musical back-ground, years of training, frequency of practice and self-reported skill were all found to have a signiﬁcant effect on420Figure 1: Score of piece Do-Re-Mi, with some intervalsmarked (see Section 3)Table 1: Summary details of the three songs used in thisstudy.TitleTempo (BPM) Key NotesEdelweiss80 B[54Do-Re-Mi120 C 59My Favourite Things132 Em 73intonation errors. We then considered as piece factors threemelodic features, note duration, interval size and the pres-ence of a tritone interval, for their effect on intonation. Allof these features had a signiﬁcant effect on both pitch andinterval error. Finally we consider the pitch drift trajecto-ries of individual singers. Our model tracks the directionand magnitude of cumulative pitch errors and captures howwell participants remain in the same key. Some trajectorieshave periodic structure, revealing systematic errors in thesinging.2. MATERIALS AND METHODS2.1 Musical materialWe chose three songs from the musical “The Sound of Mu-sic” as our material: “Edelweiss”, “Do-Re-Mi” (shown inFigure 1) and “My Favourite Things.” Despite originatingfrom one work, the pieces were selected as being diverse interms of tonal material and tempo (Table 1), well-known tomany singers, and yet sufﬁciently challenging for amateursingers. The pieces were shortened so as to contain a singleverse without repeats, which the participants were askedto sing to the syllable “ta”. In order to observe long-termpitch trends, each song was sung three times consecutively.Each trial lasted a little more than 5 minutes.2.2 ParticipantsWe recruited 39 participants (12 male, 27 female), mostof whom are members of our university’s music society orour music-technology focused research group. Some par-ticipants took part in the experiments remotely. The ageof the participants ranged from 20 to 27 years (mean 23.3,median 23 years). We asked all participants to self-assesstheir musical background with questions loosely based onthe Goldsmiths Musical Sophistication Index [16].2Ta-ble 2 shows the results, suggesting a range of skill levels,with a strong bias towards amateur singers.Table 2: Self-reported musical experienceMusical Background Instrumental TrainingNone 5 None 5Amateur 27 1–2 years 15Semi-professional 5 3–4 years 7Professional 2 5+ years 12Singing Skill Singing PracticePoor 2 None 4Low 25 Occasionally 22Medium 9 Often 12High 3 Frequently 12.3 Recording procedureParticipants were asked to sing each piece three times onthe syllable ‘ta’. They were given the starting note but nosubsequent accompaniment, except unpitched metronomeclicks.2.4 AnnotationWe used the softwareTony3to annotate the notes in theaudio ﬁles [13]: pitch track and notes were extracted usingthe pYIN algorithm [14] and then manually checked and,if necessary, corrected. Approximately 28 corrections perrecording were necessary; detailed correction metrics onthis data have been reported elsewhere [13].2.5 Pitch metricsTheTonysoftware outputs the median fundamental fre-quencyf0for every note. We relate fundamental frequencyto musical pitchpas follows:p= 69 + 12 log2f0440 Hz(1)This scale is chosen such that a difference of 1 correspondsto 1 semitone. For integer values ofpthe scale coincideswith MIDI pitch numbers, with reference pitch A4 tunedto 440 Hz (p= 69).2.5.1 Interval ErrorA musical interval is the difference between two pitches[19] (which is proportional to the logarithm of the ratioof the fundamental frequencies of the two pitches). UsingEquation 1, we deﬁne the interval from a pitchp1to thepitchp2asi=p2\u0000p1and hence we can deﬁne the intervalerror between a sung intervaliand the expected nominalintervalin(given by the musical score) as:eint=i\u0000in(2)2The questions were: How do you describe your musical background?How many years do you have instrument training? How do you describeyour singing skills? How often do you practice your singing skills?3https://code.soundsoftware.ac.uk/projects/tonyProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 421Hence, for a piece of music withMintervals{eint1,...,eintM}, the mean absolute interval error (MAIE)is calculated as follows:MAIE=1MMXi=1|eiint|(3)2.5.2 Tonal reference curves and pitch errorIn unaccompanied singing, pitch error is ill-deﬁned, sincesingers use intonation with respect to their internal refer-ence, which we cannot track directly. If it is assumed thatthis internal reference doesn’t change, we can estimate itvia the mean error with respect to a nominal (or given)reference pitch. However, it is well-known that unaccom-panied singers (and choirs) do not maintain a ﬁxed internalreference (see Section 1). Previously, this has been ad-dressed by estimating the singer’s reference frequency us-ing linear regression [15], but as there is no good reasonto assume that drift is linear, we adopt a sliding windowapproach in order to provide a local estimate of tuning ref-erence.The ﬁrst step is to take the annotated musical pitchespiof a recording and remove the nominal pitchsigivenby the score,t⇤i=pi\u0000si, which we adjust further bysubtracting the mean:ti=t⇤i\u0000¯t⇤. The resulting raw tonalreference estimatestiare then used as a basis for our tonalreference curves and pitch error calculations.The second step is to ﬁnd a smooth trajectory based onthese raw tonal reference estimates. For each note, we cal-culate the weighted mean oftiin a context window aroundthe note, obtaining the reference pitchci, from which thepitch error can be calculated:ci=nXk=\u0000nwkti+k,(4)wherePnk=\u0000nwk=1. Any window functionW={wk}can be used in Equation 4. We experimented with sym-metric windows with two different window shapes (rect-angular and triangular) and seven window sizes (3, 5, 7,9, 11, 15 and 25 notes) to arrive at smooth tonal referencecurves. The rectangular windowWR,N={wR,Nk}cen-tred at theithnote is used to calculate the mean of its N-note neighbourhood, giving the same weight to all notes inthe neighbourhood, but excluding theithnote itself:wR,Nk=⇢1N\u00001,1|k|N\u0000120,otherwise.(5)The triangular windowWT,N={wT,Nk}gives moreweight to notes near theithnote (while still excluding theithnote itself). For example, if the window size is 5, thenthe weights are proportional to 1, 2, 0, 2, 1. More gener-ally:wT,Nk=⇢2N+2\u00004|k|N2\u00001,1|k|N\u0000120,otherwise.(6)0.270.280.290.300.310.32\nwindow sizemean absolute pitch error3579111525rectangular windowtriangular window\nFigure 2: Pitch error (MAPE) for different sliding win-dows.The smoothed tonal reference curveciis the basis for cal-culating the pitch error:epi=ti\u0000ci,(7)so for a piece withMnotes with associated pitch errorsep1,...,epM, the mean absolute pitch error (MAPE) is:MAPE=1MMXi=1|epi|.(8)2.5.3 Tonal reference deviationThe tonal reference curvescican also be used to calculate anew measure of the extent of ﬂuctuation of a singer’s refer-ence pitch. We call this measure tonal reference deviation(TRD), calculated as the standard deviation:TRD=vuut1M\u00001MXi=1(ci\u0000¯cM)2.(9)3. RESULTSWe ﬁrst compare multiple choices of window for the cal-culation of the smoothed tonal reference curvesci(Sec-tion 2.5.2), which provide the local tonal reference es-timate used for calculating mean absolute pitch error(MAPE). We assume that the window that gives rise to thelowest MAPE models the data best. Figure 2 shows thatfor both window shapes an intermediate window sizeNof 5 notes minimises MAPE, with the triangular windowworking best (MAPE=0.276semitones, computed overall singers and pieces). Hence, we use this window for allfurther investigations relating to pitch error, including tonalreference curves, and for understanding how pitch error islinked to note duration and singers’ self-reported skill andexperience.422 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●050100150−4−2024\nnote numbersemitonesMAPE: 0.171TRD: 0.070\n(a) Edelweiss, singer 11●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●050100150−4−2024\nnote numbersemitonesMAPE: 0.538TRD: 0.624\n(b) Do-Re-Mi, singer 39●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●050100150200−4−2024\nnote numbersemitonesMAPE: 0.297TRD: 0.635\n(c) My Favourite Things, singer 31Figure 3: Examples of tonal reference trajectories. Dashed vertical lines delineate the three repetitions of the piece.3.1 Smoothed tonal reference curvesThe smoothed curves exhibit some unexpected behaviour.Figure 3 shows three examples of different participants andpieces. Several patterns emerge. Figure 3a shows a perfor-mance in which pitch error is kept within half a semitoneand tonal reference is almost completely stable. This isreﬂected in very low values of MAPE (0.171) and TRD(0.070), respectively. However, most singers’ tonal refer-ence curves ﬂuctuate. For example, Figure 3b illustrates atendency of some singers to smoothly vary their pitch ref-erence in direct response to the piece. The trajectory showsa periodic structure synchronised with the three repetitionsof the piece. The ﬂuctuation measure TRD is much higheras a result (0.624). This is a common pattern we haveobserved. The third example (Figure 3c) illustrates thatstrong ﬂuctuations are not necessarily periodic. Here, TRD(0.635) is nearly identical, but originates from a mostlyconsistent downward trajectory. The singer makes signif-icant errors in the middle of each run of the piece, mostlikely due to the difﬁcult interval of a downward tritoneoccurring twice (notes 42 and 50; more discussion below).Comparing Figures 3b and 3c also shows that MAPE andTRD are not necessarily related. Despite large ﬂuctuations(TRD) in both, pitch error (MAPE) is much smaller in Fig-ure 3c (0.297).Turning from the trajectories to pitch error measure-ments, we observe that the three pieces show distinct pat-terns (Figure 4). The ﬁrst piece, Edelweiss, appears to bethe easiest to sing, with relatively low median pitch errors.In Do-Re-Mi, the third quarter of the piece appears muchmore difﬁcult than the rest. This is most likely due to fasterruns and the presence of accidentals, taking the singer outof the home tonality. Finally, My Favourite Things ex-hibits a very distinct pattern, with relatively low pitch er-rors throughout, except for one particular note (number50), which is reached via a downward tritone, a difﬁcultinterval to sing. The same tritone (A-D]) occurs at note42, where the error is smaller and notably in the oppo-site direction (this D]is ﬂat, while note 50 is over a semi-tone sharp on average). It appears that singers are drawntowards the more consonant (and more common) perfectﬁfth and fourth intervals, respectively.Estimate Std. Err.tp(intercept) 0.374 0.012 32.123 0.000nominal duration -0.073 0.004 -17.487 0.000prev. nom. IOI -0.021 0.004 -4.646 0.000abs(nom. interval) 0.016 0.001 13.213 0.000abs(next nom. interval) 0.010 0.001 8.471 0.000tritone 0.370 0.019 19.056 0.000quest. score -0.011 0.001 -9.941 0.000(a) MAPEEstimate Std. Err.tp(intercept) 0.481 0.015 33.124 0.000nominal duration -0.076 0.005 -14.570 0.000prev. nom. IOI -0.050 0.006 -8.984 0.000abs(nom. interv.) 0.030 0.002 19.700 0.000abs(next nom. interv.) -0.006 0.002 -3.826 0.000tritone 0.373 0.024 15.404 0.000quest. score -0.012 0.001 -8.665 0.000(b) MAIETable 3: Effects of multiple covariates on error for a linearmodel.tdenotes the test statistic. Thepvalue rounds tozero in all cases, indicating statistical signiﬁcance.3.2 Duration, interval and proﬁciency factorsThe observations on pitch error patterns suggest that noteduration and the tritone interval may have signiﬁcant im-pact on pitch error. In order to investigate their impact wemake use of a linear model, taking into account further-more the size of the intervals sung and singer bias via con-sidering the singers’ self assessment.Table 3a lists all dependent variables, estimates of theireffects and indicators of signiﬁcance. In the following wewill simply speak of how these variables inﬂuence, re-duce or add to error, noting that our model gives no in-dication of true causation, only of correlation. We turnﬁrst to the question of whether note duration inﬂuencespitch error. The intuition is that longer notes, and noteswith a longer preparation time (previous inter-onset inter-val, IOI), should be sung more correctly. This is indeedthe case. We observe a reduction of pitch error of 0.073semitones per added second of duration. The IOI betweenprevious and current note also reduces pitch error, but bya smaller factor (0.021 semitones per second). Conversely,absolute nominal interval size adds to absolute pitch error,by about 0.016 semitones per interval-semitone, as doesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 423●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●01020304050−1.5−1.0−0.50.00.51.01.5semitones(a) Edelweiss\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●0102030405060−1.5−1.0−0.50.00.51.01.5semitones(b) Do-Re-Mi\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●010203040506070−1.5−1.0−0.50.00.51.01.5semitones(c) My Favourite ThingsFigure 4: Pitch errors by note for each of the three pieces. The plots show the median values with bars extending to theﬁrst and third quartiles.the absolute size of the next interval (0.010 semitones).The intuition about the tritone interval is conﬁrmed here,as the presence of any tritone (whether upward or down-ward) adds 0.370 semitones—on average—to the absolutepitch error. The last covariate, questionnaire score, is thesum of the points obtained from the four self-assessmentquestions, with values ranging between 5 and 14. The re-sult shows that there is correlation between the singers’self-assessment and their absolute pitch error. For everyadditional point in the score their absolute pitch error isreduced by 0.012 semitones. The picture is very similaras we do the same analysis for absolute interval error (Ta-ble 3b): the effect directions of the variables are the same.4. DISCUSSIONWe have investigated how note length relates to singing ac-curacy, ﬁnding that notes are sung more accurately as thesinger has more time to prepare and sing them. Yet it is notentirely clear what this improvement is based upon. Dolonger notes genuinely give singers more time to ﬁnd thepitch, or is part of the effect we observe due to measure-ment or statistical artefacts? To ﬁnd out, we will need toexamine pitch at the sub-note level, taking vibrato and notetransitions into account. Conversely, studying the effect ofmelodic context on the underlying pitch track could shedlight on the physical process of singing, and could be usedfor improved physical modelling of singing.Overall, the absolute pitch error of singers (mean: 28cents; median: 18; std.dev.: 36) and the absolute inter-val error (mean: 34 cents; median: 22; std.dev.: 46) areslightly higher than those reported elsewhere [15], but thismay reﬂect the greater difﬁculty of our musical material incomparison to “Happy Birthday”. We also did not excludesingers for their pitch errors, although the least accuratesingers had MAPE and MAIE values of more than half asemitone, i.e. they were on average closer to an erroneousnote than to the correct one. That the values of MAIE andMAPE are similar is to be expected, as interval error is thelimiting case of pitch error, using a minimal window con-taining only the current and previous note.We used a symmetric window in this work, but thiscould easily be replaced with a causal (one-sided) win-dow [15], which would also be more plausible psycholog-ically, as the singer’s internal pitch reference in our modelis based equally on past sung notes and future not-yet-sungnotes. However, for post hoc analysis, the fuller contextmight reveal more about the singer’s internal state (whichmust inﬂuence the future tones) than the more restrictedcausal model.Figure 4 shows how the three pieces in our data differin terms of pitch accuracy. It is interesting to see that ac-cidentals (which result in a departure from the establishedkey), and the tritone as a particular example, seem to havea strong adverse impact on accuracy. To compile more de-tailed statistical analyses like the ones in Table 3 one couldconduct singing experiments on a wider range of intervals,isolated from the musical context of a song. In future workwe also intend to explore the interaction between singersas they negotiate a common tonal reference.424 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Finally, we would like to mention that some singerstook prolonged breaks between runs in a three-run rendi-tion of a song. The recording was stopped, but no newreference note was played, so the singers resumed with thememory of what they last sung. As part of the reproduciblecode package (see Section 7) we provide information onwhich recordings were interrupted and at which break. Wefound that the regression coefﬁcients (Tables 3b and 3a)did not substantially change as a result of these interrup-tions.5. CONCLUSIONSWe have presented a new dataset for singing analysis, in-vestigating the effects of singer and piece factors on theintonation of unaccompanied solo singers. Pitch accuracywas measured in terms of pitch error and interval error. Weintroduced a new model of tonal reference computed usingthe local neighbourhood of a note, and found that a win-dow of two notes each side of the centre note provides thebest ﬁt to the data in terms of minimising the pitch error.The temporal evolution of tonal reference during a piecerevealed patterns of tonal drift in some singers, others ap-peared random, yet others showed periodic structure linkedto the score. As a complement to errors of individual notesor intervals, we introduced a measure for the magnitude ofdrift, tonal reference deviation (TRD), and illustrated howit behaves using several examples.Two types of factors inﬂuencing pitch error were inves-tigated, those related to the singers and those related to thematerial being sung. In terms of singer factors, we foundthat pitch accuracy correlates with self-reported singingskill level, musical training, and frequency of practice.Larger intervals in the score led to larger errors, but onlyaccounted for 2–3 cents per semitone of the mean absoluteerrors. On the other hand, the tritone interval accountedfor 35 cents of error when it occurred, and in one case ledto a large systematic error across many of the singers. Wehypothesised that note duration might also have an effecton pitch accuracy, as singers make use of aural feedbackto regulate their pitch, which results in less stable pitch atthe beginnings of notes. This was indeed the case: a smallbut signiﬁcant effect of duration was found for both thecurrent note, and the nominal time taken from the onset ofthe previous note; longer durations led to greater accuracy.Many aspects of the data remain to be explored, such as thepotential effects of scale degree, consonance, modulation,and rhythm.6. ACKNOWLEDGEMENTSMatthias Mauch is funded by a Royal Academy of Engi-neering Research Fellowship. Many thanks to all the par-ticipants who contributed their help during this project.7. DATA A VAILABILITYAll audio recordings analysed here (and correspondingtrajectory plots) can be obtained fromhttp://dx.doi.org/10.6084/m9.figshare.1482221. The code andthe data needed to reproduce our results (note annotations,questionnaire results, interruption details) are provided inan open repository athttps://code.soundsoftware.ac.uk/projects/dai2015analysis-resources.8. REFERENCES[1] P. Alldahl.Choral Intonation. Gehrman, Stockholm,Sweden, 2006. p. 4.[2] D.E. Brown.Human Universals. Temple UniversityPress, Philadelphia, 1991.[3] D. S. Crowther.Key Choral Concepts: Teaching Tech-niques and Tools to Help Your Choir Sound Great.Cedar Fort, 2003.[4] S. Dalla Bella, J. Gigu`ere, and I. Peretz. Singing proﬁ-ciency in the general population.Journal of the Acous-tical Society of America, 121(2):1182, 2007.[5] A. de Cheveign´e and H. Kawahara. YIN, a fundamen-tal frequency estimator for speech and music.Jour-nal of the Acoustical Society of America, 111(4):1917–1930, 2002.[6] J. Devaney and D. P. W. Ellis. An empirical approachto studying intonation tendencies in polyphonic vocalperformances.Journal of Interdisciplinary Music Stud-ies, 2(1&2):141–156, 2008.[7] J. Devaney, M. Mandel, and I. Fujinaga. A study of in-tonation in three-part singing using the automatic mu-sic performance analysis and comparison toolkit (AM-PACT). In13th International Society of Music Infor-mation Retrieval Conference, pages 511–516, 2012.[8] J. Fyk. V ocal pitch-matching ability in children as afunction of sound duration.Bulletin of the Council forResearch in Music Education, pages 76–89, 1985.[9] C. M. Ganschow. Secondary school choral conduc-tors’ self-reported beliefs and behaviors related to fun-damental choral elements and rehearsal approaches.Journal of Music Teacher Education, 20(10):1–10,2013.[10] D. M. Howard. Intonation drift in a capella soprano,alto, tenor, bass quartet singing with key modulation.Journal of Voice, 21(3):300–315, May 2007.[11] H. Kawahara, J. Estill, and O. Fujimura. Aperiodic-ity extraction and control using mixed mode excita-tion and group delay manipulation for a high qual-ity speech analysis, modiﬁcation and synthesis systemSTRAIGHT. InProceedings of the Workshop on Mod-els and Analysis of Vocal Emissions for Biomedical Ap-plications (MAVEBA), pages 59–64, 2001.[12] M. Kennedy.The Concise Oxford Dictionary of Mu-sic. Oxford University Press, Oxford, United King-dom, 1980. p. 319.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 425[13] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J. Sala-mon, J. Bello, J. Dai, and S. Dixon. Computer-aidedmelody note transcription using the Tony software: Ac-curacy and efﬁciency. InProceedings of the First Inter-national Conference on Technologies for Music Nota-tion and Representation (TENOR 2015), pages 23–30,2015.[14] M. Mauch and S. Dixon. pYIN: A fundamental fre-quency estimator using probabilistic threshold distribu-tions. InProceedings of the IEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP 2014), pages 659–663, 2014.[15] M. Mauch, K. Frieler, and S. Dixon. Intonation in un-accompanied singing: Accuracy, drift, and a model ofreference pitch memory.Journal of the Acoustical So-ciety of America, 136(1):401–411, 2014.[16] D. M¨ullensiefen, B. Gingras, and L. Stewart. Pilotinga new measure of musicality: The Goldsmiths’ Musi-cal Sophistication Index. Technical report, Goldsmiths,University of London, 2011.[17] M. M¨uller, P. Grosche, and F. Wiering. Automatedanalysis of performance variations in folk song record-ings. InProceedings of the International Conferenceon Multimedia Information Retrieval, pages 247–256,2010.[18] P. Q. Pfordresher and S. Brown. Poor-pitch singingin the absence of “tone deafness”.Music Perception,25(2):95–115, 2007.[19] E. Prout.Harmony: Its Theory and Practice. Cam-bridge University Press, 2011.[20] M. P. Ryyn¨anen. Probabilistic modelling of note eventsin the transcription of monophonic melodies. Master’sthesis, Tampere University of Technology, Finland,2004. pp. 27–30.[21] J. Salamon, E. G´omez, D. P. W. Ellis, and G. Richard.Melody extraction from polyphonic music signals: Ap-proaches, applications, and challenges.IEEE SignalProcessing Magazine, 31(2):118–134, 2014.[22] R. Seaton, D. Pim, and D. Sharp. Pitch drift in A Cap-pella choral singing.Proceedings of the Institute forAcoustics Annual Spring Conference, 35(1):358–364,2013.[23] J. Swannell.The Oxford Modern English Dictionary.Oxford University Press, USA, 1992. p. 560.[24] H. Terasawa. Pitch Drift in Choral Mu-sic, 2004. Music 221A ﬁnal paper, URLhttps://ccrma.stanford.edu/˜hiroko/pitchdrift/paper221A.pdf.426 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "The MIR Perspective on the Evolution of Dynamics in Mainstream Music.",
        "author": [
            "Emmanuel Deruty",
            "François Pachet"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417995",
        "url": "https://doi.org/10.5281/zenodo.1417995",
        "ee": "https://zenodo.org/records/1417995/files/DerutyP15.pdf",
        "abstract": "Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called “loudness war” has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track’s year of release, rather than to its genre.",
        "zenodo_id": 1417995,
        "dblp_key": "conf/ismir/DerutyP15",
        "keywords": [
            "Mainstream music",
            "MIR perspective",
            "Dynamics",
            "Genre",
            "Loudness war",
            "Peak year",
            "Track release",
            "Impact of genre",
            "Overriding dynamics",
            "Diachronic analysis"
        ],
        "content": "THE MIR PERSPECTIVE ON THE EVOLUTION OF DYNAMICS IN MAINSTREAM MUSIC Emmanuel Deruty   François Pachet Sony Computer Science Laboratory / Akoustic Arts Paris, France emmanuel.deruty@gmail.com   Sony Computer Science Laboratory Paris, France  ABSTRACT Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called “loudness war” has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track’s year of release, rather than to its genre. 1. INTRODUCTION Mainstream popular music is in constant evolution. There may be more differences than common points between progressive rock albums from the 1970’s such as Pink Floyd’s best-selling “Dark Side of the Moon” and con-temporary rap albums such as Nicki Minaj’s platinum-certified “Roman Reloaded”. Studies tracking down the yearly evolution of signal descriptors are useful to char-acterize this diversity.  In 1982, Moller [1] established that recent recordings feature a larger dynamic excursion than older ones. More recently, Tardieu [2] studied the evolution of stereo, dy-namic and spectral features on pop/rock songs, and showed that decade classification accuracies using spec-tral and dynamic features are equal. Pestana [3] focused on spectral features and found that while spectra are de-pendent on genre, they also follow the yearly evolution of production standards. Serrà [4] performed a systematic analysis of more than 400,000 tracks and concludes that popular music “show[s] no considerable changes in more than fifty years” other than becoming louder, a result challenged by Mauch [5]. Deruty [6] focused on the changes in loudness and dynamics over the same period, and provided a characterization of the phenomenon re-ferred to as the “loudness war”. The loudness war, or loudness race, is a trend in popular music production that affects mainstream music dynamics [7]. It has been de-scribed as a contest between bands and record companies, in which music is engineered to be louder than the com-petition’s [8, pp. 237–292]. Starting at the end of the 80’s [4], [6], [9], its effects have been spectacular enough to reach the general media [10]–[11]. A distinction is made between dynamics occurring on different time-scales. The large-scale variations are known as macrodynamics, whereas the faster variations are referred to as micrody-namics [12]–[13]. The loudness war favors high loudness tracks with reduced microdynamics [4], [6], [9], although some authors claim it has also reduced macrodynamics [14]. Efforts have been made to reverse the trend, through measurement protocols [15]–[16], integrated loudness-leveling engines such as iTunes’ Sound Check [17], or public communications [18]–[19]. In this paper, we perform a diachronic analysis on 7200 mainstream tracks released between 1967 and 2014, and present two results. First, we show that the evolution towards louder and less dynamic content peaked in 2007, and then started to decrease. If this trend continues, pre-loudness war values for most descriptors of music dy-namics may be observed sometimes between 2017 and 2026. Second, we demonstrate that the loudness war’s impact supersedes the influence of music genre on dy-namics. In mainstream music, a piece’s dynamics are more typical of a given year than they are of a given gen-re. 2. METHOD 2.1 Music corpus The music corpus we rely on is a revision and extension of the corpus used in [6]. It includes 7200 tracks released between 1967 and 2014, 150 tracks per year. Track selection is based on Besteveralbums.com, a review aggregator. For each year, we choose the albums with the best ratings. If a given artist is the author of more than three well-rated albums, we select the artist’s complete discography. While this method does not lead to a random sampling, it ensures that the corpus is based on music that is popular. We choose to start the corpus at the end of the sixties because these years can be considered as the advent of the contemporary pop/rock era, characterized by the creative use of the recording studio [8, p. 157] along with mass media availability [20]. 2.2 Signal descriptors We use the signal descriptors defined in [6]. The track’s physical level is measured using the RMS power of the signal after normalization. Track loudness evaluation is \n © Emmanuel Deruty, François Pachet. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Emmanuel Deruty, François Pachet. “The MIR perspective on the evolution of dynamics in mainstream music”, 16th International Society for Music Information Retrieval Conference, 2015. 722   performed using the EBU3341 integrated loudness [21], which has been shown to be as robust as more complex measures such as detailed perceptual models [22]. Microdynamics are measured using a variation of the crest factor, as defined in [6]. For macro-dynamics, we rely on the EBU3342 Loudness Range [23], which is, to our knowledge, the only normative descriptor to quantify dynamics in a musical sense (piano, forte…) [9]. We evaluate the overall amount of dynamic processing using the Peak to RMS Regression Coefficient (PRRC). PRRC values below 1 indicate usage of dynamic compression, values above 1 usage of dynamic expansion [6]. Finally, we estimate the amount of limiting applied to the tracks using the High Level Sample Density (HLSD) [6]. HLSD can be linked to the practice of limiting [6], which is suspected to have a decisive impact on mainstream music production during the last 30 years [8, pp. 237-292], [9], [14], [24]–[27]. Using relations between limiting and HLSD as shown in [6], we indeed find that a significant amount of limiting (> 3dB) seems to have been applied on 33% of all tracks from our corpus, and on 65% of tracks released after 1994. For each descriptor, we provide a projection based on the current trend by fitting the descriptor’s smoothed median values using a second-degree polynomial, starting from the year for which the loudness war is observed to peak. As illustrated in Figure 1 (black dot at the right of the graphs), estimation of the return to pre-loudness war values is obtained using the crossing of the projected values with the median of the pre-1990 descriptor values. 2.3 Genre labels Following [28]–[30], we draw the music genre labels from AllMusic, a website that provides “unoptimized expert annotated ground truth dataset for music genre classification” [30] in the form of a database of commercial music annotated in terms of “genres”, “meta-styles” and “styles”. Whereas AllMusic provides only 21 “genres”, album information also comes with 905 “styles” and “meta-styles” that can be interpreted as sub-genres to refine the major genre labels. In this paper, while relying on the “styles” provided by Allmusic, we designate them as “genres”, “a conventional category that identifies pieces of music as belonging to a shared tradition or set of conventions” [31]. Under this terminology, the 7500 tracks from the corpus correspond to 272 distinct mainstream music genres, each track being associated with a mean of 4 genres, the minimum being 1 and the maximum 11. Conversely, each genre is represented with a mean of 110 tracks, the minimum being 3 and the maximum 2482. Issues linked to the pertinence of the results regarding this diversity of representation are discussed in Section 4.3. 3. DIACHRONIC STUDY OF DYNAMICS Figure 1 illustrates the descriptors’ behavior over time. The boxes’ upper and lower limits indicate the 25th and 75th percentiles of the distribution. The darker box indi-cates the peak of the loudness war for the descriptor, i.e. the year for which the median value is maximal. The small horizontal lines inside the boxes indicate the medi-an. The outer whiskers stand for the 5th and 95th percen-tiles. The solid, thick black curve is the smoothed medi-an, on which the projection is based. The projection itself is represented by a dashed gray line. The thin horizontal line indicates the median pre-1990 descriptor values.  \n  Figure 1. Descriptor evolution over the years. From top to bottom RMS power, EBU3341 integrated loudness, crest factor, PRRC, HLSD and EBU3342 Loudness Range.  The loudness war may be characterized by a change towards previously unobserved descriptor values that starts around 1990 and indicates the use of more dynamic compression [6]. Table 1 summarizes the loudness war timeline depending on the descriptor. It took ca. 15 years 197019801990200020102020−25−20−15−10RMS power (dB)\n19701980199020002010202011.522.53log(1+ EBU3342Loudness Range) (LU)197019801990200020102020−6−5−4−3−2HLSD19701980199020002010202000.250.50.7511.25PRRC197019801990200020102020−20−17−14−11EBU3341Loudness (LU)\n197019801990200020102020810121416\nYear of releaseCrest Factor (dB)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 723   for the loudness war to peak. The return to pre-loudness war values could take between 10 and 20 years. Figure 1 shows that macrodynamics are not affected by the loud-ness war. No significant change of values starting around 1990 and pointing toward more dynamic compression can be observed. The loudness war has increased music level and micro-dynamics, but has not decreased macro-dynamics.  Descriptor Corresponding phenomenon Peak Estimated re-turn to pre-loudness war values RMS power Physical level 2007 2018 EBU3341 Loudness [21] 2007 2020 Crest factor Microdynamics [6], [12] 2008 2026 PRRC Overall amount of dynamic processing [6] 2008 2017 HLSD Amount of lim-iting [6] 2006 2023 Table 1. Loudness war timeline summary.  Since 2006, macrodynamics have increased consist-ently, and are higher in 2014 than they have ever been during the time-span covered by the corpus. This increase can be put in relation with a demand for more dynamics combined with the confusion that’s often made between micro- and macrodynamics [6], [10]–[11], [14], [32]. Musicians and producers may be trying to counter the ef-fects of the loudness war by raising macrodynamics, whereas raising microdynamics would be more produc-tive in that respect. However, examination of Figure 1 shows that macrodynamics follow relatively shorter trends than other descriptors, and a reversal of the present tendency towards less macrodynamics could be witnessed as soon as 2015. 4. DYNAMICS AND MAINSTREAM GENRES 4.1 Dependency of dynamics on genres and trends In this section, we show that dynamics of mainstream music are more typical of a given year than they are of a given genre. Figure 2 illustrates the distribution of RMS power values depending of the music genre of the track. On first approach, it suggests that music genre and RMS power are related. However, as illustrated in Figure 1, RMS power is also related to the year of the album release. Figure 3 provides more details, by illustrating RMS power evolution for the four most represented genres in the corpus (Alternative Pop/Rock, Alternative/Indie Rock, Album Rock and Contemporary Pop/Rock). It indicates that genres follow the year’s trend in terms of RMS power. This phenomenon, previously mentioned in [32], suggests that RMS values may be primarily related to the year of the track release, rather than to its genre. We use two methods to confirm the tendency: a standard ANOVA and a variance evaluation. The second method possesses the advantage of providing results formulated using the original descriptor’s unit, and therefore being easier to interpret than the ANOVA’s results. It involves the evaluation of the RMS distribution’s variance for each genre and for each year, followed by the computation of the weighted arithmetic means of the variances, taking into account genre and year representativeness. The process is illustrated in Figure 4. The weighted mean variance for each year is 9.03dB, whereas the weighted mean variance for each style is 14.19dB. This shows that RMS values primarily originate from the track’s year of release. In other words, particular physical levels are more typical of a given year than they are of a given genre. As shown in Table 2, this result is confirmed by the ANOVA’s F-statistic. We repeat the experiment using the other descriptors described in Section 2.2. Results are similar. With the exception of the EBU3342 LRA, descriptors are clearly more related to the year’s trend than to the piece’s genre. \n Descriptor Mean variance for each year Mean variance for each genre ANOVA’s F-statistic (years as classes) ANOVA’s F-statistic (genres as classes) RMS power 9.03dB 14.2dB 107.7 6.4 EBU3341 4.57LU 7.41.LU 104.9 6.5 Crest factor 1.35dB 2.25dB 110.4 5.4 PRRC 0.04 0.06 77.5 4.9 HLSD 0.79 2.08 274.7 8.6 EBU3342 14.5LU 14.3LU 7.3 4.7 Table 2. Comparison of the weighted mean arithmetic means of the descriptor variances for each year and each genre, as well as comparison of the ANOVA’s F-statistics, show that dynamics in mainstream music are primarily linked to the piece’s year of release, rather than to its genre. 724 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   \n Figure 3. In gray, RMS power values corresponding to the music genres most represented in the corpus. Lighter gray sections indicate years with fewer tracks. The three black lines represent the 25th, 50th and 75th percentiles. 4.2 The particular cases of HLSD and LRA As shown in Table 2, a particularly high dependence to trends is clear in the case of the HLSD, with an F-statistic being higher than in the case of the other descriptors. As seen in Section 2.2, it implies that the amount of limiting applied by audio engineers during mastering can be con-sidered as independent from genre. Therefore, main-stream genres cannot be said to sound more or less “hot\". This is an important information in the context of main-stream music mastering: it can help engineers choose and argue the output level with their client, which is often a critical debate [33]. On the other hand, dependency to trends is much lower in the case of the EBU3342 LRA. As a result, macrodynamics can be considered as relative-ly independent from both genre and year of release.  Figure 4. Distribution of RMS power variances. Top, by year. Middle, by genre. The dashed vertical line represents the weighted mean of the distribution. Bar hues indicates style representativeness. Bottom, style representativeness displayed quantitatively.   197019751980198519901995200020052010−20−15−10\nRelease dateRMS (dB)051015202530354005Variance (dB)Number of years051015202530354001020Variance (dB)Number of genres0510152025303540025005000Variance (dB)Number of tracks\nFigure 2. Distribution of RMS power values depending on the tracks’ genres. Darker shades of gray indicate higher levels of distribution. The black rectangles indicate the median. This Figure is restricted to styles corresponding to more than 50 tracks. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 725   \n  Figure 5. Top, distance between each genre and the all-genre median value for the RMS descriptor, against genre representation. Bottom, result of the same process using random values between 0 and 1. The horizontal line rep-resents the linear regression.  4.3 Discussion It may seem counter-intuitive to conclude that dynamics are more dependent on trends than they are on genres. Indeed, genres such as Euro-Pop exhibit high micro-dynamics and low overall loudness, whereas other genres such as Trip-Hop are associated with low micro-dynamics and high overall loudness. However, the Euro-Pop genre is most represented in the 1970s and 1980s [34], at a period when music was produced to feature high microdynamics and low overall loudness [6]. Trip-Hop is mainly a mid-1990s trend [35], a moment when low microdynamics and high overall loudness were common in music production [6]. Conversely, all genres that span several decades follow the trend of the year of production. As mentioned in Section 2.3, not all genres are equal-ly represented. This may bring the suspicion that dynam-ics are only dependent on the trends followed by the most represented genres, such as the subgenres of rock repre-sented in Figure 3, but independent from the trends fol-lowed by most other genres, in which case our conclusion would not stand. To discard this suspicion, we evaluate the distance between each genre and the all-genre median value for the descriptors over the years. This distance is then matched against the genre’s number of occurrences. Figure 5, top, illustrates the case of the RMS descriptor. A few well-represented genres are indeed closer to the median than most other genres. However, Figure 5, bot-tom, illustrates the same process using 1000 sets of 7500 random values in place of the 7500 RMS values. Both graphs are similar, and the few well-represented genres are closer to the median in both cases. Therefore, a par-ticular dependency to a few genres is not a property of the present corpus. This discards the suspicion according to which the dependency to trends we found is only valid as far as a few genres are concerned.  5. CONCLUSION Mainstream music dynamics are thought to be conditioned by genre, in terms of overall track loudness [36], microdynamics [9], [37], macrodynamics [15], [38] or amount of dynamic processing applied to music pieces during the production stage [7], [39, p. 121]. However, using a MIR perspective, we have shown that dynamics and overall loudness depend more on the track’s year of release than on its genre. We have also found, as suspected by [40], that the loudness war has influenced all mainstream genres indiscriminately. A notable exception lies in macrodynamics as measured by the EBU3342 Loudness Range, which are more independent from both genre and year of release. In other words, dynamic range in the musical sense (pianissimo to fortissimo) is only marginally dependent on either mainstream genre or trend.  According to mastering engineer Bob Katz, the loudness wars were over in 2013 [41]. We have shown that the loudness war has peaked in 2007, and that a return to pre-loudness war dynamics may be reached in about ten years. As an exception, macrodynamics, which have not been significantly influenced by the loudness war, appear to increase since the loudness war’s peak, and are currently reaching very high values. This is useful knowledge in several situations. Many artists and producers ask sound engineers to increase loudness during mastering [33], arguing that the music genre to which their tracks belong is well suited to a “hot”, loud and compressed sound. The present study provides objective data to challenge this claim. Loudness war activists argue for more important dynamics [32], [41]. We have shown that this concerns only microdynamics. Automatic mixing and mastering rely on constraints to be applied on initial audio content [42]–[44]. The present study has demonstrated that constraints relative to dynamics in mainstream music may be derived from trends rather than genres. More generally, we suggest that the present method could be used for other audio descriptors, in order to establish their dependency to either diachronic trends, genre, or to any other musical dimension. 6. ACKNOWLEDGEMENTS This research is supported by the project Lrn2Cre8 which is funded by the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET grant number 610859. 7. REFERENCES [1] L.G. Moller, “How Much Has Amplitude Distribu-tion of Music Changed?,” in AES 71st Conv., Montreux, Switzerland, 1982. [2] D. Tardieu et al., “Production Effect: Audio Fea-tures For Recording Technique Description and Decade Prediction,” in 14th Int. Conf. on Digital Audio Effects (DAFX 11), Paris, France, 2011. 0123456700.511.5distance tomedian (dB)\n0123456700.050.10.15\nln(number of tracks per genre)distance tomedian726 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   [3] P.D. Pestana et al., “Spectral Characteristics of Popular Commercial Recordings 1950-2010,” in AES 135th Conv., New-York, NY, 2013. [4] J. Serrà et al., “Measuring the evolution of contem-porary western popular music,” Scientific Reports, Jul. 2012. [5] M. Mauch et al. (2015). The evolution of popular music: USA 1960–2010 [Online]. Available: http://rsos. royalsocietypublishing.org/content/2/5/150081. Royal Society Open Science, DOI: 10.1098/rsos.150081. [6] E. Deruty and D. Tardieu, “About Dynamic Pro-cessing in Mainstream Music,” J. Audio Eng. Soc. 62(1/2), pp. 42-55, Jan. 2014. [7] E. Vickers, “The loudness war: Background, specu-lation and recommendations,” in AES 129th Conv., San Francisco, CA, 2010. [8] G. Milner, Perfecting sound forever: an aural his-tory of recorded music. London, UK: Faber & Faber, 2009.  [9] E. Deruty, “’Dynamic range’ and the loudness war,” Sound on Sound, Sep. 2011. [10] E. Smith, “Even heavy-metal fans complain that today’s music is too loud!!!,” The Wall Street Journal, Sep. 2008.  [11] K. Matersonn, “Loudness war stirs quiet revolu-tion by audio engineers,” Chicago Tribune, Jan. 2008. [12] E. Skovenborg, “Measures of Microdynamics,” in AES 137th Conv., Los Angeles, CA, 2014. [13] B. Katz and R. Katz, Mastering audio: the art and the science. Focal Press, 2007 [14] S. Sreedhar. “The future of music,” IEEE Spec-trum, Aug. 2007.  [15] E. Skovenborg, “Loudness Range (LRA), Design and Evaluation,” in AES 132th Conv., Budapest, Hungary, 2012. [16] European Broadcasting Union. (2015). Loudness [Online]. Available: https://tech.ebu.ch/loudness. [17] T. Lund. (2013). Audio for Mobile TV, iPad and iPod [Online]. Available: http://www.tcelectronic.com/ media/2040040/mobile-test-paper-2013. [18] J. V. Serinus. (2012). Winning the Loudness Wars [Online]. Available: http://www.stereophile.com/ con-tent/winning-loudness-wars. [19] R. Archer. (2014). Have the Loudness Wars Peaked? [Online]. Available: http://www.cepro.com/ ar-ticle/excuse_the_pun_have_the_loudness_wars_peaked/. [20] The Cambridge companion to recorded music. Cambridge, UK: Cambridge University Press, 1999. [21] European Broadcasting Union. (2011). EBU - TECH 3341 [Online]. Available: https://tech.ebu.ch/ docs/tech/tech3341.pdf. [22] E. Skovenborg and S. Nielsen, “Evaluation of dif-ferent loudness models with music and speech material,” in AES 117th Conv., 2004, San Francisco, CA, 2004. [23] European Broadcasting Union. (2011). EBU-TECH3342 [Online]. Available: https://tech.ebu.ch/docs/tech/tech3342.pdf. [24] E. Vickers, “The Non-flat and Continually Chang-ing Frequency Response of Multiband Compressors,” in AES 129th Conv., San Francisco, CA, 2010. [25] P. Kraght, “Aliasing in Digital Clippers and Com-pressors,” J. Audio Eng. Soc. 48 (11), Nov. 2000. [26] A. Travaglini, “Broadcast Loudness: Mixing, Monitoring and Control,” in 122nd AES Conv., Vienna, Austria, 2007. [27] E. Skovenborg and T. Lund, “Loudness de-scriptors to characterize programs and music tracks,”  in AES 125th Conv., San Francisco, CA, 2008. [28] J. Bergstra et al., “Predicting genre labels for art-ists using FreeDB,” Proc. of the 7th Int. Conf. on Music Information Retrieval, 2006, pp. 85-88. [29] Y. Hu and M. Ogihara, “Nextone Player: A Music Recommendation System Based On User Behavior,” Proc. of the 12th Int. Conf. on Music Information Retriev-al, 2011, pp. 103-108. [30] A. Schindler et al., “Facilitating Comprehensive Benchmarking Experiments On The Million Song Da-taset”, Proc. of the 13th Int. Conf. on Music Information Retrieval, 2012, pp. 469-474.  [31] J.Samson. (2001). Grove Music Online: Genre [Online]. Available: http://www.oxfordmusic online.com /subscriber/article/grove/music/40599. [32] I. Shepherd. (2014). What is the Loudness War? [Online]. Available: http://dynamicrangeday.co.uk/about. [33] T. Woodhead. (2015). What is Mastering? [Online]. Available: http://www. hippocraticmaster-ing.com/whatismastering.html. [34] AllMusic. (2015). Euro-Pop [Online]. Available: http://www.allmusic.com/style/euro-pop-ma0000004446. [35] AllMusic. (2015). Trip-Hop [Online]. Available: http://www.allmusic.com/ style/trip-hop-ma0000002906. [36] E. Skovenborg et al., “Loudness Assessment of Music and Speech,” in AES 116th Conv., Berlin, Germa-ny, 2004. [37] M. Walsh et al., “Adaptive Dynamics Enhance-ment,” in AES 130th Conv., London, UK, 2011. [38] H. Robjohns, “The End Of The Loudness War?” Sound on Sound, Feb. 2014. [39] B. Katz and R. Katz, Mastering audio: the art and the science. Focal Press, 2007. [40] A.v. Ruschkowski, “Loudness war,” in Systematic and comparative musicology : concepts, methods, find-ings. Albrecht Schneider (ed.), pp.  213-230, 2008. [41] B. Katz. (2013). The Loudness War Has Been Won: Press Release [Online]. Available: http://www.digido.com/forum/ announcement/id-6.html. [42] Z. Ma et al., “Implementation of an intelligent equalization tool using Yule-Walker for music mixing and mastering,” in AES 134th Conv., Rome, Italy, 2013. [43] E. Perez-Gonzales and J.D. Reiss, “Improved con-trol for selective minimization of masking using inter-channel dependancy effects,” Proc. of the 11th Int. Conf. on Digital Audio Effects (DAFX-08), pp. 75-81, 2008. [44] E. Deruty et al., “Human Rock Mixes Exhibit Tight Relations Between Spectrum And Loudness.” J. Audio Eng. Soc., 62 (10), Oct. 2014. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 727"
    },
    {
        "title": "Theme And Variation Encodings with Roman Numerals (TAVERN): A New Data Set for Symbolic Music Analysis.",
        "author": [
            "Johanna Devaney",
            "Claire Arthur",
            "Nathaniel Condit-Schultz",
            "Kirsten Nisula"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417497",
        "url": "https://doi.org/10.5281/zenodo.1417497",
        "ee": "https://zenodo.org/records/1417497/files/DevaneyACN15.pdf",
        "abstract": "The Theme And Variation Encodings with Roman Nu- merals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the ef- fectiveness of automatic analysis algorithms in generaliz- ing across different musical textures. The pieces are en- coded in standard **kern format, with analyses jointly en- coded using an extension to **kern. The harmonic con- tent of the music was analyzed with both Roman numer- als and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, al- lowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level har- monic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase).",
        "zenodo_id": 1417497,
        "dblp_key": "conf/ismir/DevaneyACN15",
        "keywords": [
            "The Theme And Variation Encodings with Roman Numerals (TAVERN) dataset",
            "27 complete sets of theme and variations",
            "piano compositions by Mozart and Beethoven",
            "harmonic structures realized in different ways",
            "evaluation of automatic analysis algorithms",
            "kern format",
            "jointly encoded analyses",
            "harmonic content analysis",
            "Roman numerals and function labels",
            "musical phrases"
        ],
        "content": "THEME AND VARIATION ENCODINGS WITH ROMAN NUMERALS(TA VERN): A NEW DATA SET FOR SYMBOLIC MUSIC ANALYSISJohanna Devaney, Claire Arthur, Nathaniel Condit-Schultz, and Kirsten NisulaSchool of Music, Ohio State University, USA{devaney.12, arthur.193, condit-schultz.1, nisula.1}@osu.eduABSTRACTThe Theme And Variation Encodings with Roman Nu-merals (TA VERN) dataset consists of 27 complete sets oftheme and variations for piano composed between 1765and 1810 by Mozart and Beethoven. In these theme andvariation sets, comparable harmonic structures are realizedin different ways. This facilitates an evaluation of the ef-fectiveness of automatic analysis algorithms in generaliz-ing across different musical textures. The pieces are en-coded in standard **kern format, with analyses jointly en-coded using an extension to **kern. The harmonic con-tent of the music was analyzed with both Roman numer-als and function labels in duplicate by two different expertanalyzers. The pieces are divided into musical phrases, al-lowing for multiple-levels of automatic analysis, includingchord labeling and phrase parsing. This paper describesthe content of the dataset in detail, including the typesof chords represented, and discusses the ways in whichthe analyzers sometimes disagreed on the lower-level har-monic content (the Roman numerals) while converging atsimilar high-level structures (the function of the chordswithin the phrase).1. INTRODUCTIONThere are a wealth of musical scores in digitized form cur-rently available. While the vast majority exist as images,a combination of hand encoding of the visual data andadvances in optical music recognition (OMR) technologyhave increased the amount of symbolic music data avail-able. Unfortunately, most of this data is unlabeled, limitingits utility in developing predictive systems for analyzingsymbolically represented music. Accurately segmentingand labeling symbolic music data requires a higher level ofmusical expertise than can be reasonably obtained throughcrowd-sourcing platforms, like Mechanical Turk1. Evenwith expert-annotators, there is the challenge of ensuring1http://www.mturk.com/c\u0000Johanna Devaney, Claire Arthur, Nathaniel Condit-Schultz, and Kirsten Nisula.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Johanna Devaney, Claire Arthur,Nathaniel Condit-Schultz, and Kirsten Nisula. “Theme And VariationEncodings with Roman Numerals (TA VERN): A new data set for sym-bolic music analysis”, 16th International Society for Music InformationRetrieval Conference, 2015.that they all conform to the same conventions in label-ing the data. In this regard, conforming to the analyticapproach in a published textbook provides a measure ofconsistency for analyzing classical music.This paper presents the Theme And Variation Encod-ings with Roman Numerals (TA VERN) datase2, a newdataset of segmented and analyzed symbolic classical mu-sic. TA VERN consists of 27 theme and variations sets byMozart and Beethoven, segmented into phrases and ana-lyzed in terms of both Roman numeral chord labels andchord function. All of the pieces were analyzed in du-plicate by different PhD-level music theory students andboth the notes and analyses were encoded in Humdrum-related formats [9]. The dataset focuses on pieces in themeand variation form where the underlying harmony remainsrelatively constant across variations, while rhythmic andtextural aspects of the music change. The utility of themeand variations in symbolic music analysis has been demon-strated in the case of folk songs [27, 28] and for both har-mony [8,14] and melody [5] in classical themes and varia-tions. This is the ﬁrst such dataset, however, that includesharmonic and functional data, facilitating the developmentof algorithms of automatic symbolic chord recognition andsymbolic similarity, through a deeper understanding of theimpact of texture on both of these tasks. This paper beginswith a survey of existing symbolic music datasets, both an-notated and unannotated, before describing in detail the an-notation process and the contents of the dataset.2. EXISTING DATASETSAs noted above, there is a growing number of unannotatedsymbolic music datasets available, many items of whichare available in several collections. The most popular inMIR research are those that are hand-encoded and, to a cer-tain degree, curated. This includes the KernScores dataset[22], which has more than 100,000 ﬁles in **kern for-mat [9] from a range of styles from folk [23] to classi-cal. A number of the kern score pieces are available inother datasets, such as the music21 corpus [3], which con-tains ﬁles in MusicXML [6] and **kern format. The mu-sic21 corpus also includes the Yale Classical Archives Cor-pus [29], which contains almost 9000 pieces/movementsdivided into vertical slices. The Yale corpus is also part ofthe ELVIS database [1] along with the Josquin Research2http://getTAVERN.org728Project3and a number of smaller corpora of other Re-naissance composers. While some datasets are focused onmaking printed versions of the musical scores available,they often supply symbolic data. For example, the Mu-topia Project4contains not only PDFs of the scores butalso hand-encoded Lilypond5and MIDI ﬁles. The Peach-note dataset [26] provides similar access to the PetrucciMusic Library6by running OMR on the scanned scores,which typically has a higher error rate than hand encoding.Researchers have also made use of publicly available Bandin a Box lead sheets, e.g, [4], and MIDI ﬁles, e.g., [15].There is a much smaller number of harmonically an-notated datasets. Temperley encoded the analyses fromtheTonal Harmonytextbook by Kostka and Payne [11]for his work on key ﬁnding [24] and examined statisticalproperties of harmony [25]. These encodings have beenused by other researchers for evaluating symbolic chordrecognition systems [12, 18]. The note data and annota-tions are available both in a format Temperley deﬁned as“note ﬁles”7and as MIDI ﬁles (with the chord annotationinserted as lyrics).8The KSN harmonic annotations [10]provide Roman numeral labels with duration and inver-sion information for the Real World Computing (RWC)dataset [7] and have been used for modeling pitch struc-tures in polyphonic music [19].3. ANALYTIC APPROACHTA VERN comprises 27 sets of theme and variations, 10by Mozart and 17 by Beethoven (listed in Table 1). TheBeethoven set is nearly complete, with 18 of his 20 themeand variation sets included (Opus 35 was excluded becauseof the inclusion of a fugue in the piece and Wo0 79 wasexcluded because it included only 5 variations, which wasbelow our 6 variation minimum). The Mozart set is lesscomplete: due to time and resource restrictions, we tempo-rally sampled variations across his career (leaving out K.24, 54, 180, 264, 352, 460, 500). Going forward we planto analyze and include these variations in the dataset onceadditional resources become available.The pieces have been analyzed in duplicate by multi-ple expert-annotators using the hierarchical model of har-mony deﬁned in [13] that includes both Roman numeraland function labels, speciﬁcally a variant of functional anal-ysis known as the ‘Phrase Model’. Section 3.1 providessome background on the ‘Phrase Model’ in general andSection 3.2 describes the annotation process.3.1 Phrase ModelPhrases are complete musical statements built from an or-dered presentation of three harmonic functions and end-ing with a cadence. One way of analyzing phrases is in3http://jrp.ccarh.org/4http://www.mutopiaproject.org5http://www.lilypond.org6http://imslp.org7http://theory.esm.rochester.edu/temperley/kp-stats/index.html8http://www.cs.northwestern.edu/˜pardo/kpcorpus.htmComposer Piece # VariationsMozart K.25 7K.179 12K.265 12K.353 12K.354 12K.398 6K.455 10K.501 12K.573 9K.613 8Beethoven WoO 63 9WoO 64 6WoO 65 24WoO 66 13WoO 68 12WoO 69 9WoO 70 6WoO 71 12WoO 72 8WoO 73 10WoO 75 7WoO 76 8WoO 77 6WoO 78 7WoO 80 32Opus 34 6Opus 76 6Table 1. Summary of the sets of themes and variations inthe data set.terms of functions. The tonic function at the beginningof a phrase serves to establish the tonal centre, and at theend of a phrase to signal its return. The pre-dominantfunction prepares for the arrival of the dominant function,which sets up an opposition to tonic. The tension createdby the movement to the dominant is ultimately resolvedby a return to tonic. A phrase typically contains all threeharmonic functions, but may contain just tonic and dom-inant. The cadences may close with the dominant func-tion (termed a half cadence) or return to the tonic func-tion (termed an authentic or deceptive cadence, depend-ing on the chords used). Ideas about functional harmonycan be found in Rameau [20], although the speciﬁcationof the terms tonic, pre-dominant, and dominant were notdeﬁned until the late nineteenth century by Riemann’ [21].We have included function labels in addition to the Ro-man numeral labels because we believe that they are essen-tial in developing and testing hierarchical models of har-mony. Since function harmony has some limitations formusic outside of the Classical era, we focused this dataseton Mozart and early-mid career Beethoven pieces.The ‘Phrase Model’ is a contemporary adaption of Rie-mann’s thinking and is deﬁned in several textbooks. Forthe purposes of this project, we followed the speciﬁcs laidout inThe Complete Musicianby Steven Laitz [13]. Gen-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 729Figure 1. Example of a theme and variation from the dataset with harmonic analyses marked, note the similarity in theharmonic structure and the differences in the texture.erally, the majority of I and iii chords (i and III in the minormode) have a tonic function, although inversions of thesechords may have other function, such asI64functioning asdominant, depending on their harmonic context. vi (or VI)chords may have either a tonic or pre-dominant function,while ii or IV (iioor iv) chords are typically pre-dominant.V andviiochords are typically assigned a dominant func-tion, except for when their inversions occur in passing orneighbor contexts with I or vi chords in a tonic function.An example of the ‘Phrase Model’ analytical approach isshown in Figure 1. In the Theme, the Roman numerals I-V7-I-I6are assigned a tonic function, with theV7in theﬁrst bar functioning as a ornamentation of the surroundingI chords, rather than having a dominant function. Theii6chord has a pre-dominant function and the V chord has adominant function. Since the phrase ends on the dominantfunction, rather than returning to the tonic function, it endswith a half cadence. The variation has a similar structure,with the ﬁrst 2.5 measures having a tonic function, the sec-ond half of the third measure having a pre-dominant func-tion (albeit with a IV chord instead ofii6chord), and thefourth measure having a dominant function.3.2 AnnotatorsThe annotators are three PhD-level music theory students,who each have spent at least two years teaching the har-monic analysis technique described in Section 3.1 to un-dergraduate students within the same curricular framework.Thus the annotators are intimately familiar with the work-ings of Laitz’s version of the ‘Phrase Model’ and its ana-lytic conventions, ensuring a common interpretation acrossthe annotators on these conventions. Each of the themeand variations sets was analyzed by two annotators, withthe annotators analyzing 18 theme and variations sets each.The annotators worked independently, dividing each of thethemes and variations into phrases on their own and ana-lyzing each phrase both in terms of Roman numerals andphrase-level function. In cases where there was disagree-ment between the annotators, a third annotator reviewedthe analyses and sided with one interpretation. The adju-dicated version of the analysis was then joined with thenote data, as described in Section 4.1. On occasion, theanalyzers would disagree on the Roman numerals whilestill agreeing about the function of the chords, an exam-ple of which is discussed in Section 4.2 We believe thatpoints of disagreement between the trained annotators arean interesting source of information, particularly if chordrecognition algorithms run into accuracy issues in the samesituations, and so we are also releasing the individual an-notations in addition to the adjudicated data.4. DATASET DETAILS4.1 Encoding FormatThe musical scores of pieces were converted from pub-licly available MIDI ﬁles sourced online. The MIDI ﬁleswere less error-prone than running OMR on printed scoresof the pieces, but still required some manual correction.In the correction process, the MIDI ﬁles were ﬁrst con-verted into **kern format after which the errors were hand-730 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2. Example of the encoding format for the themein Figure 1. The leftmost column contains the function la-bels, the second one contains the harmonic labels, and theremaining columns contain the notes. Dots indicate that alabel is continued from a previous row while elements ofanother spine change.corrected in reference to public domain scores available inthe Petrucci Music Library (namely 19th century publica-tions from Breitkopf & Hartel [2,16,17]). In the correctionprocess, ornamentation and grace notes were removed inorder to simplify the data. In addition to pitch and durationinformation, **kern format allows for information aboutslurs and stem directions to be encoded. Where this in-formation was encoded in the MIDI data, it was convertedinto the **kern data.The analyses were encoded as separate spines and thenjoined with the **kern data. For the Roman numeral anal-ysis the existing **harm representation9was used. In thisformat, the labels are the same as standard Roman numerallabels except that the inversions are marked with the lettersa (for ﬁrst inversion, typically notated as6for triads or65for seventh chords), b (for second inversion,64or43), and c(for third inversion,42) in order to maintain consistency forthe number of character used to indicate inversions. Wedeveloped a new format, named **func, for the functionencoding, which simply consists of the labels T (tonic), P(pre-dominant), and D (dominant). Thus each ﬁle consistsof one **func spine, one **harm spine and a number of9http://www.humdrum.org/Humdrum/representations/harm.rep.html**kern spines, each of which corresponds to one staff inthe piano score. An example of a ﬁle, corresponding to theupper scores in Figure 1, is shown in Figure 2. Each ﬁle inthe dataset represents one phrase, with measure numbersmarked in reference to the entire piece. This allows for thephrases across the corresponding theme and variations tobe easily recombined into a single piece while at the sametime providing an indication of where each phrase beginsand ends. The ﬁles are readable by Humdrum, a MATLABparser for the ﬁles is currently available on github10andextensions to the music21 Humdrum parser will be avail-able shortly. We have also generated audio versions of eachﬁle from the symbolic data via MIDI.4.2 Theme and Variations FormAll 27 of theme and variations sets in TA VERN are in‘sectional’ form, meaning that all of the themes and varia-tions are tonally-closed distinct units. Within the sets, theharmony remains relatively constant across the theme andvariations, while the theme’s melody is embellished in thevariations. Additional musical interest is created throughchanges in rhythm, tempo, texture, key, and mode. Thereare some inconsistencies in the harmonies across relatedthemes and variations, but these are typically substitutionsof different chords with the same harmonic function. Anexample of this is present in Figure 1, where theii6chordin the penultimate measure of the theme is substituted witha IV chord in the variation. However,ii6and IV share twocommon notes (the 4th and 6th scale degrees) and a com-mon function (P), meaning that this substitution has verylittle harmonic impact.In total, the dataset consists of 1060 phrases. Of these,66 phrases occur as codas to isolated variations, so forthese phrases there is no corresponding phrase in the re-lated theme or variations. These have been included forthe purposes of completeness. Of the 1060 phrases, 917 ofthe phrases are in the major mode, with the remaining 143being in the minor mode. Seven different major and minorkeys are occur in the dataset: A, B ﬂat, C, D, E ﬂat, F, G).Within the phrases there are 290 unique sonorities (count-ing each inversion as a separate sonority), this includesboth diatonic chords and applied chords. A tally of the top40 unique chords with the highest number of occurrences(at least 25) is shown in Figure 3, along with the number oftimes that each chord occurs in each function. In additionto highlighting the large number of chords that are anno-tated in the dataset, Figure 3 also demonstrates the utilityof annotating function labels by showing that most of thechord inversions have two if not three possible functions(depending on the context in which they occur). This high-lights the need for such labelled data in order to learn thesecontexts, rather than simply relying on rule-based systems.The relatively large proportion of non-standard tonicchords with a tonic function in Figure 3 (e.g., ii, IV, V,viio)are a result of “embedded phrases” within the tonic func-tion in some of phrases [13]. An example of this is shownin the **comments spine of Annotator Two’s analysis of10https://github.com/jcdevaney/TAVERNProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 731Figure 3. A tally of the number of times each of the top forty unique chords occurs in the dataset in regards to thefunction (Tonic, Pre-dominant, Dominant) in which they occur. The data for the I and V chords are shown the number ofoccurrences per 1000, scaled from their total number of occurrences (2133 and1239 occurrences, respectively). This wasdone to facilitate the readability of the ﬁgure. The chords are grouped, from top to bottom, by the scale degree of their rootnote (or in the case of applied chords, the diatonic scale degree which functions as their relative tonic). Within each chordgroup, the chords are ordered by inversion followed by occurrences of applied dominant chords on that scale degree.732 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 4. An example of a phrase where the two annotators disagreed on speciﬁc chord labels. In the third measure(marked with a box), Annotator 1 analyzed the measure as ‘I-I64-V65-I’ while Annotator 2 analyzed the measure as ‘I-viio6-I’. The adjudicating annotator sided with Annotator 2 because in this context ‘viio6’ label is a complete chord. ’V65’,despite being technically correct, is less desirable because the root of the chord (E) is missing. Annotator 2’s analysis alsodemonstrates the nomenclature of ‘embedded phrases’, which are marked when there is a low-level ‘T-P-D-T’ or ‘T-D-T’pattern within the main T function that does not result in a cadence. Where applicable, ‘embedded phrase’ analyses areavailable in the individual annotators’ ﬁles in the **comments spine.musical phrases reproduced in Figure 4. Instances of em-bedded phrases are not included in the main database ﬁles,but are available in the individual annotator’s ﬁles that arealso released as part of TA VERN. Figure 4 also providesan example where the two annotators agreed on the over-all harmonic function, but disagreed on the speciﬁc Romannumerals (as seen in the different analyses for measure 3).Ultimately, in this case, a third annotator determined thesecond annotator’s analysis to be superior both becausethe chord labels described complete chords and because itbetter mirrored the harmonic activity in the correspondingphrases in the related theme and variations.5. CONCLUSIONSThis paper has presented TA VERN, a new dataset of 27harmonically annotated theme and variations piano piecesby Mozart and Beethoven that will facilitate research onsymbolic chord recognition and similarity in symbolic mu-sic. Each musical phrase in the dataset is encoded as a sep-arate ﬁle. The note information is encoded in **kern for-mat, the Roman numerals in **harm format, and the har-monic function of each Roman numeral label in the newlydeﬁned **func format.This dataset will be useful for systematically evaluatingthe effect of textural changes on symbolic chord recogni-tion algorithms since the consistency of harmonic mate-rials and melodic frame across each theme and variationsset occurs against a wide range of musical textures. Also,the segmentation of the pieces into phrases can facilitatethe development and evaluation of algorithms for musicalstructure analysis. In addition to the symbolic music data,MIDI-generated audio ﬁles are available. In the future, weplan to use score-audio alignment to generate a mappingbetween the symbolic data and public-domain recordingsof real piano performances, extending the utility of thisdataset to include audio chord recognition research.6. ACKNOWLEDGMENTSThis work was supported by the Google Faculty ResearchAward program.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7337. REFERENCES[1]Christopher Antila and Julie Cumming. The vis frame-work: Analyzing counterpoint in large datasets. InPro-ceedings of ISMIR, pages 71–6, 2014.[2]Ludwig van Beethoven.Variationen f¨ur das Pi-anoforte, volume Serie 17 ofLudwig van BeethovensWerke. Breitkopf & H¨artel, Leipzig, DE, 1862-90.[3]Michael Scott Cuthbert and Christopher Ariza. mu-sic21: A toolkit for computer-aided musicology andsymbolic music data. InProceedings of ISMIR, pages637–42, 2010.[4]W. Bas De Haas, Martin Rohrmeier, Remco CVeltkamp, and Frans Wiering. Modeling harmonicsimilarity using a generative grammar of tonal har-mony. InProceedings of the ISMIR, pages 549–54,2009.[5]Mathieu Giraud, Ken D´eguernel, and Emilios Cam-bouropoulos. Fragmentations with pitch, rhythm andparallelism constraints for variation matching. InSound, Music, and Motion, pages 298–312. Springer,2014.[6]Michael Good. MusicXML for notation and analysis.The virtual score: representation, retrieval, restora-tion, 12:113–24, 2001.[7]Masataka Goto, Hiroki Hashiguchi, Takuya Nishi-moto, and Ryuichi Oka. Rwc music database: Popular,classical, and jazz music databases. InProceedings ofISMIR, pages 287–288, 2002.[8]Keiji Hirata, Satoshi Tojo, and Masatoshi Hamanaka.Cognitive similarity grounded by tree distance from theanalysis of k. 265/300e. InSound, Music, and Motion,pages 589–605. Springer, 2014.[9]David Huron.The Humdrum Toolkit: Reference Man-ual. CCARH, Menlo Park, California, 1995.[10]Hitomi Kaneko, Daisuke Kawakami, and ShigekiSagayama. Functional harmony annotation databasefor statistical music analysis. InProceedings of the IS-MIR (Late Breaking Demo), 2010.[11]S. Kostka and D. Payne.Tonal Harmony: With an In-troduction to Twentieth Century Music. McGraw-Hill,New York, NY, 2008.[12]Pedro Kr¨oger, Alexandre Passos, Marcos Sampaio, andGivaldo De Cidra. Rameau: A system for automaticharmonic analysis. InProceedings of the InternationalComputer Music Conference, pages 273–281, 2008.[13]Steven G. Laitz.The Complete Musician. Oxford Uni-versity Press, Oxford, 3rd edition edition, 2011.[14]Alan Marsden. Recognition of variations using auto-matic schenkerian reduction. InProceedings of ISMIR,pages 501–506, 2010.[15]Matthias Mauch and Simon Dixon. A corpus-basedstudy of rhythm patterns. InProceedings of ISMIR,pages 163–168, 2012.[16]Wolfgang Amadeus Mozart.F¨ur ein und zwei Pi-anoforte zu vier H¨anden. Wolfgang Amadeus MozartsWerke, Serie XIX. Breitkopf & H¨artel, Leipzig, DE,1878.[17]Wolfgang Amadeus Mozart.Variationen f¨ur das Pi-anoforte. Wolfgang Amadeus Mozarts Werke, SerieXXI. Breitkopf & H¨artel, Leipzig, DE, 1878.[18]B. Pardo and W. Birmingham. Algorithms for chordalanalysis.Computer Music Journal, 26(2):27–49, 2002.[19]Stanislaw Andrzej Raczynski, Emmanuel Vincent, andShigeki Sagayama. Dynamic bayesian networks forsymbolic polyphonic pitch modeling.IEEE Transac-tions on Audio, Speech, and Language Processing,21(9):1830–1840, 2013.[20]Jean-Phillipe Rameau.Treatise on Harmony. Dover,Toronto, ON, 1722.[21]Hugo Riemann.Harmony Simpliﬁed; or, the Theoryof the Tonal Functions of Chords.Augener, London,1896.[22]Craig Stuart Sapp. Online database of scores in thehumdrum ﬁle format. InProceedings of ISMIR, pages664–665, 2005.[23]Helmut Schaffrath and David Huron.The Essen folk-song collection in the humdrum kern format. CCARH,Menlo Park, CA, 1995.[24]David Temperley.A Bayesian Approach to Key-Finding, volume 2445 ofLecture Notes in ComputerScience, pages 195–206. Springer Berlin Heidelberg,2002.[25]David Temperley. A uniﬁed probabilistic model forpolyphonic music analysis.Journal of New Music Re-search, 38(1):3–18, 2009.[26]Vladimir Viro. Peachnote: Music score search andanalysis platform. InProceedings of ISMIR, pages359–362, 2011.[27]Anja Volk, WB Haas, and P Kranenburg. Towardsmodelling variation in music as foundation for simi-larity. InProceedings of the International Conferenceon Music Perception and Cognition, pages 1085–1094,2012.[28]Anja Volk and Peter van Kranenburg. Melodic sim-ilarity among folk songs: An annotation study onsimilarity-based categorization in music.Musicae Sci-entiae, 16(3):317–339, 2012.[29]Christopher White and Ian Quinn. The Yale-ClassicalArchives Corpus. InProceedings of the InternationalConference on Music Perception and Cognition, page320, 2014.734 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Cross-Version Singing Voice Detection in Classical Opera Recordings.",
        "author": [
            "Christian Dittmar",
            "Bernhard Lehner",
            "Thomas Prätzlich",
            "Meinard Müller",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416958",
        "url": "https://doi.org/10.5281/zenodo.1416958",
        "ee": "https://zenodo.org/records/1416958/files/DittmarLPMW15.pdf",
        "abstract": "In the field of Music Information Retrieval (MIR), the au- tomated detection of the singing voice within a given mu- sic recording constitutes a challenging and important re- search problem. In this study, our goal is to find those seg- ments within a classical opera recording, where one or sev- eral singers are active. As our main contributions, we first propose a novel audio feature that extends a state-of-the- art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to im- prove the results in the case that the test data is not reflected well by the training data. Third, we show that a cross- version approach can help to stabilize the results even fur- ther. 1",
        "zenodo_id": 1416958,
        "dblp_key": "conf/ismir/DittmarLPMW15",
        "keywords": [
            "Music Information Retrieval",
            "singing voice detection",
            "classical opera recording",
            "audio feature",
            "state-of-the-art feature set",
            "singing voice detection in popular music recordings",
            "bootstrapping procedure",
            "cross-version approach",
            "stabilize the results",
            "improve the results"
        ],
        "content": "CROSS-VERSION SINGING VOICE DETECTION IN CLASSICAL OPERARECORDINGSChristian Dittmar1Bernhard Lehner2Thomas Pr¨atzlich1Meinard M¨uller1Gerhard Widmer21International Audio Laboratories, Erlangen, Germany2Johannes Kepler University, Linz, Austriachristian.dittmar@audiolabs-erlangen.de, bernhard.lehner@jku.atABSTRACTIn the ﬁeld of Music Information Retrieval (MIR), the au-tomated detection of the singing voice within a given mu-sic recording constitutes a challenging and important re-search problem. In this study, our goal is to ﬁnd those seg-ments within a classical opera recording, where one or sev-eral singers are active. As our main contributions, we ﬁrstpropose a novel audio feature that extends a state-of-the-art feature set that has previously been applied to singingvoice detection in popular music recordings. Second, wedescribe a simple bootstrapping procedure that helps to im-prove the results in the case that the test data is not reﬂectedwell by the training data. Third, we show that a cross-version approach can help to stabilize the results even fur-ther.1 IntroductionIn classical opera, singing voice is considered to be one ofthe most important musical aspects. Locating vocal seg-ments in an opera recording is an important prerequisitefor applications such as singing voice separation or musicstructure analysis. The task of singing voice detection(also known as vocal detection) comprises automaticsegmentation of a music recording into vocal (one ormore singers) and non-vocal (accompaniment or silence)parts. A typical example of such a temporal segmentationis shown in Figure 1, where the black rectangles beloweach plot are ground truth segments and the red rectan-gles show automatically detected segments. The mainchallenge in automatic vocal detection comes both fromthe huge variety of singing voice characteristics as wellas the simultaneous presence of other pitched musicalinstruments in the accompaniment. Especially in opera,the singers are often accompanied by instruments playingthe same sequence of notes. Since the singers voice shoulddominate over the accompaniment, expressive techniques© Christian Dittmar, Bernhard Lehner, Thomas Pr¨atzlich,Meinard M¨uller, Gerhard Widmer. Licensed under a Creative CommonsAttribution 4.0 International License (CC BY 4.0).Attribution:Chris-tian Dittmar, Bernhard Lehner, Thomas Pr¨atzlich, Meinard M¨uller, Ger-hard Widmer. “Cross-Version Singing Voice Detection in Classical OperaRecordings”, 16th International Society for Music Information RetrievalConference, 2015.such as pronounced vibrato and the so called singer’sformant [18] are often used. Moreover, the pitch anddynamic range of professional opera singers goes wellbeyond singing voices in popular music.There has been quite some research on the problem ofsinging voice detection. The majority of previous contri-butions employ some sort of machine learning approachin combination with the extraction of audio features (seeSection 2). When using machine learning, two majoraspects need to be considered. First, appropriate audiofeatures have to be designed that are suitable for thesinging voice detection task. A delicate trade-off betweenelaborate, but error-prone extraction steps on the one hand,and undirected low-level features on the other hand has tobe made. In this context, we introduce a novel extensionto a previously proposed feature set and show that it isappropriate for singing voice detection.Second, a supervised machine-learning algorithm usuallylearns from training data. It is well known that the per-formance of an optimized classiﬁer can drop signiﬁcantlyif the “closed world” of the training data does not matchthe “open world” of the target data. A typical exampleis found in speech processing where systems trainedwith clean speech usually fail under noisy or reverberantconditions. One possibility to approach this challengeis so-called bootstrapping [14, 19]. As a second maincontribution, we show how bootstrapping can help toimprove singing voice detection by adapting classiﬁersto the speciﬁc recording under analysis. Furthermore,we describe a cross-version fusion approach [8] that canimprove the results in case several versions of a musicpiece are available, which is a realistic assumption foropera and classical music in general.2 Related WorkAlthough singing voice detection seems to be a task thatis not so hard for human listeners, automatic singing voicedetection remains difﬁcult due to expressive characteris-tics of the singing voice and the diversity of accompani-ment music playing simultaneously. These speciﬁc chal-lenges have already been brought up in early works on thetopic [2]. Given an unknown music recording, automaticsinging voice detection is usually performed as a frame-wise estimation of singing voice activity. Even though this618poses a binary classiﬁcation problem with just two classes,the acoustical variance within each class is so large thatit is necessary to train the classiﬁer with a wide range oftraining data.Bootstrapping, i.e., the idea of using training data takenfrom the target recording itself, was proposed before as un-supervised [14] and user assisted [19] strategy for improv-ing classiﬁcation performance. One of the ﬁrst attempts toseparate the singing voice from the accompaniment priorto the feature extraction stage was described in [20]. Post-processing of the so-called posterior probabilities obtainedduring classiﬁcation was described in [12].A large set of low-level features was used in conjunctionwith a Support Vector Machine (SVM) classiﬁer in [15].Furthermore, the authors published singing voice annota-tions for training, validation and test subsets of the JA-MENDO corpus, enabling reproducible comparisons be-tween different methods (see Section 5.2). The same testcorpus was used for evaluation in [16], where the featureextraction focused on vibrato and tremolo properties. Astudy on the effect of accompaniment music in singing vs.rap discrimination was presented in [6]. Very promisingresults in singing voice detection and related tasks werereported in [13]. However, the proposed signal process-ing chain was quite elaborate and involved an estimationof the predominant pitch, which can lead to substantial er-ror propagation to all the feature extractors depending onit.Lehner et al. [10] focused on achieving comparable resultsusing a light-weight approach. In a follow-up work, theyimproved the achievable precision by introducing novel au-dio features tailored to the singing voice detection scenario[11]. A recent paper [4] showed that two cross-versionpost-processing strategies can improve the singing voicedetection performance achievable with the light-weightfeature set of [10,11].So far, the best classiﬁcation performance on the JA-MENDO data set was reported in [9], using a BidirectionalLong Short-Term Memory Recurrent Neural Network asmachine learning scheme that inherently takes the tempo-ral context of low-level feature sequences into account.However, it reads as if the authors selected the optimalnetwork architecture according to the best results obtainedw.r.t. the test set instead of the validation set. Thus, wethink that their results might be overly optimistic.3 Baseline Singing Voice DetectionOur baseline system for singing voice detection closely fol-lows the approach proposed in [10, 11]. The extraction ofdescriptive audio features is performed by splitting the au-dio signals into frames and transforming each frame to thespectral domain. Low-level and mid-level audio featuresare computed from each resulting spectral frame, form-ing a feature vector by concatenation. Supervised machinelearning is employed to train a classiﬁer for discriminat-ing the feature vector assigned to each frame into the twoclasses vocal and non-vocal. Note that the vocal class usu-Feature name and reference Abbrev. Dim.Mel-frequency Cepstral Coefﬁcients [10] MFCC 30Vocal Variance [11] VOCV AR 5Fluctogram Variance [11] FLUCT 17Spectral Contraction Variance [11] NSD 17Spectral Flatness Mean [11] FLAT 17Polynomial Shape Spectral Contrast [1,7] PSSC 24Table 1. Feature names, abbreviations, and dimensionalityof the low-level and mid-level audio features used.ally comprises singing voice plus accompaniment, whichmakes the task more intricate.3.1 Feature Extraction and ProcessingTable 1 lists the complete set of features that is used in ourapproach. Since most of our descriptors are wellknown inthe MIR literature, we only highlight a few aspects here.Mel-Frequency Cepstral Coefﬁcients (MFCC)are oneof the most common audio features widely used in diverseaudio classiﬁcation tasks. They are designed to capture thespectral envelope of an audio signal using only a few co-efﬁcients in the so-called Cepstral domain. As describedin [10], we use an optimized parametrization with a dif-ferent time-frequency resolution and a higher number ofcoefﬁcients than usual. A strongly related feature is theVocal Variance, which basically captures the variance inthe ﬁrst5MFCCs across a number of consecutive frames.The mid-level featuresFluctogram, Spectral Contrac-tion, and Spectral Flatnessare the most important contri-butions from [11]. All three are extracted in17overlappingfrequency bands, where each band covers two octaves andneighboring bands are spaced three semitones apart. TheFluctogram encodes the relative frequency ﬂuctuation ofsalient tonal components in each band, without the needfor an actual estimation of a predominant pitch. SpectralContraction and Flatness are designed to complement theFluctogram, encoding whether there are reliable harmoniccomponents with clear sinusoidal peaks or rather a noise-like distribution of the spectrum within the current bandboundaries.Spectral Contrastencodes the relation of peaks to val-leys of the spectral magnitude in several sub bands. Theband boundaries have been speciﬁed for the Octave-BasedSpectral Contrast (OBSC) [7] and the Shape-Based Spec-tral Contrast (SBSC) [1]. In general, both variants can beinterpreted as harmonicity or tonality descriptor. We sug-gest a modiﬁcation of the already existing methods, bothof which were successfully used for music genre classiﬁ-cation tasks. In the previous approaches, the spectral mag-nitude values in each sub band are sorted and the relationbetween the lowest and highest fraction is encoded via sta-tistical measures. In our modiﬁcation, we propose to ﬁta third-order polynomial to the ordered magnitude valuesand store the three polynomial coefﬁcients together withthe offset as descriptors. Therefore, we refer to this fea-ture asPolynomial Shape Spectral Contrast (PSSC). It isProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 61920406080100120140      0  0.51  \n20406080100120140      0  0.51  \n10203040506070      0  0.51  (a) \n(b) \n(c)          Baseline          Bootstrap          Threshold          Classification           Ground truth \n              Time (measures)                                  Time (seconds)                                  Time (seconds)                    Figure 1. Illustration of the cross-version post-processing strategies as described in Section 4.1 and Section 4.2. Thecurves and annotations are based on an excerpt corresponding to the ﬁrst80measures of the duet No.6(Agathe and¨Annchen): “Schelm! halt fest” from the opera “Der Freisch¨utz” by Carl Maria von Weber. For each case, the decisionfunctions of the baseline (blue thin curve) and bootstrap (red bold curve) classiﬁer are shown. The colored time-linesbelow the decision curves show the automatically detected singing voice activity (red segments, derived from bootstrapdecision) vs. the ground truth (black segments).(a):Recording of the performance conducted by Karl-Heinz Bloemecke(2013).(b):Recording of the performance conducted by Carlos Kleiber (1973).(c):Cross-version results based on threeperformances (including Bloemecke and Kleiber) after temporal alignment to a common, measure-based time axis andsubsequent averaging across the individual decision functions.computed for each of the6sub bands (0-200 Hz, 200-400Hz, 400-800 Hz, 800-1600 Hz, 1600-3200 Hz, and 3200-8000 Hz), yielding a feature vector with24attributes. Incontrast to the procedure in [1, 7], we do not apply anydecorrelation procedure to the raw features, hence reduc-ing the computational complexity. Compared to the beforementioned versions of spectral contrast, our modiﬁcationresulted in better accuracy on our internal data set (PSSC:80.2%, OBSC: 73.4%, and SBSC: 72.3%).In total, the concatenation of all features listed in Table 1results into a110-dimensional feature vector per spectralframe. The set of all feature vectors makes up our featurematrix which is split into appropriate training and test setsand used for machine learning in the following.3.2 Classiﬁcation and Decision FunctionAgain following [10,11], we employ Random Forests (RF)[3] as classiﬁcation scheme. RF are an instance of the so-called Bootstrap Aggregation (Bagging) concept appliedto Classiﬁcation and Decision Trees (CART) [21] classi-ﬁers. This machine learning ensemble meta algorithm wasdesigned to improve the stability and accuracy by averag-ing over a set of weak classiﬁers trained from random sub-spaces of the complete feature matrix. In RF, random setsof CARTs are trained by introducing randomness at2lev-els: in the subset of features as well as in the subset oftraining data [3]. The generalization error of RF dependson the classiﬁcation strength of the individual CARTs aswell as their mutual correlation. As changes in the featureselection cause drastic changes in the tree structure, the in-dividual trees are expected to be uncorrelated. Averagingtheir individual decisions in the RF leads to decreased vari-ance of the classiﬁer model, which is in general a desirableproperty.RFs deliver a frame-wise score value per class that can beinterpreted as conﬁdence measure for the classiﬁer deci-sion. In our binary classiﬁcation scenario, the two scorefunctions are inversely proportional. We pick the one cor-responding to our target vocal class and refer to it as deci-sion function in the following. A decision function valueclose to1indicates a very reliable assignment to the vocalclass, whereas a value close to0points to the non-vocalclass. In order to binarize the decision function, we com-pare it to a threshold. Only frames where the decision func-tion value exceeds the threshold will be classiﬁed as vocal.Prior to that, the decision function is smoothed using a me-dian ﬁlter. The ﬁlter width given in seconds is an importantparameter. Median ﬁltering of the decision function is jus-tiﬁed by the observation that singing voice activity usuallyexhibits a certain continuity. So this step helps to stabilizethe detection result and to prevent unreasonably short gapsin the decision function, where the classiﬁcation rapidlyﬂips from vocal to non-vocal or vice versa.620 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20154 Post-processing of Singing Voice DetectionIn this section, we describe two approaches suitable forpost-processing of intermediate singing voice detection re-sults. First, we describe our approach to unsupervisedbootstrap training of a classiﬁer adapted to the recordingunder analysis. Second, we describe how to perform a latefusion of decision functions by means of time alignmentbetween different versions.4.1 Bootstrap TrainingInspired by the ideas in [14, 19], we propose to perform asecond, specialized RF classiﬁcation subsequent to the ini-tial singing voice detection stage. The rationale is to rem-edy the “closed world” vs. “open world” training problemdiscussed before (see Section 1). We do so by creating anadapted classiﬁer model that is trained with feature vectorsexclusively taken from the current recording under anal-ysis. However, this recording does usually not come to-gether with an annotation of its frames to the two classes.So how to assign the feature vectors automatically to thetraining sets of the vocal respective non-vocal class?Our idea is to base this assignment on the shape of the deci-sion function generated by the initial RF classiﬁer. Look-ing at the course of this decision function, we see someextreme values for frames, where the observed feature vec-tors match very well to either the vocal or non-vocal classof the initial classiﬁer model. However, many values re-side in the middle of the range of decision function values,where an assignment to either side is questionable. If wenow select two subsets of the feature vectors, each corre-sponding to an upper and lower fraction (e.g.,20%) of therange of decision function values, we can use these to traina small RF classiﬁer that is adapted to the feature spacespanned by the recording under analysis. Before we do so,we stratify the training set, meaning that we randomly se-lect the same number of feature vectors for each class fromthe subset corresponding to the upper and lower decisionvalues.In Figure 1, we observe that the new decision functions(red curve) generated by classifying the current song withthe adapted RF classiﬁer exhibits a more desirable shapethan the decision function generated by the initial RF clas-siﬁer (blue curve). In Figure 1(a), it can be seen, that thebootstrap decision function can close small gaps, where theinitial decision function dipped below the decision thresh-old (e.g., at around80s).4.2 Cross-Version FusionIn [8], Konz et al. introduced the intuitive yet effectiveidea to exploit the availability of different recordings ofthe same piece of music for stabilizing automatic chordrecognition results. We pursue the same idea here in or-der to perform a late fusion of decision functions obtainedfrom the initial singing voice detection. This is achieved byAuthors and Reference Accuracy F-measureBiased Guess (all frames vocal) 46.3 0.64Vembu and Baumann 2005 [20] 77.4 0.77Ramona et al. 2008 [15] 82.2 0.84Regnier and Peeters 2009 [16] — 0.77Lehner et al. 2013 [10] 84.8 0.85Lehner et al. 2014 [11] 88.2 0.87Leglaive et al. 2015 [9] 91.5 0.91Proposed feature set 88.2 0.87Table 2. Singing voice detection results achievable withour novel feature set in comparison to other authors. Thebasis of all measurements is a publicly available subset ofthe JAMENDO corpus [15].warping the individual decision functions obtained for dif-ferent versions of the same piece to a version-independentrepresentation with a musical time axis given in measures(respective sub-divisions thereof) instead of seconds. Forthe moment, we assume that the required temporal positionof measure boundaries is given. In Section 5.3, we sketchhow to retrieve the measure boundaries automatically.In general, the procedure described above yields a set oftime-aligned decision functions that we use to derive afused, overall decision function. To this end, we use themost straightforward approach and just take the arithmeticmean of the decision values of all aligned decision func-tions. The averaging is intended to compensate for noiseand artifacts that might occur in the individual decisionfunctions. Figure 1(c) presents the resulting decision func-tion on the measure-related time axis. We show the fuseddecision function derived from baseline singing voice de-tection (thin blue curve) overlayed with the fused deci-sion function derived from bootstrap training (bold redcurve). It can be seen that the averaging leads to a slightlymore stable decision function. Comparison of the fusedbootstrap decision function against the decision threshold(dashed black line) yields our estimated singing voice seg-ments (black rectangles). In general, the estimated seg-ments exhibit improved agreement to the ground truth seg-mentation in comparison to Figure 1(a) and 1(b).5 EvaluationIn this section, we assess the performance of our proposedmethods. First, we validate our novel feature set on a pub-lic benchmark data set. Second, we show that bootstrap-ping and cross-version fusion can help to improve the re-sults for classical opera recordings.5.1 Experimental SettingsFor our experiments, we are going to ﬁx the following pa-rameters: For the majority of features in Table 1, the hop-size between consecutive analysis frames is200ms (fea-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 621ture rate of5Hz), the analysis windows have a length of800ms. The raw ﬂuctogram, ﬂatness and contraction fea-tures are extracted on a ﬁner temporal level, with a hop-size of20ms and a window size of100ms. We aggregate40consecutive frames of these raw features and use theirvariance as descriptor for ﬂuctogram and contraction, andtheir means as descriptor for ﬂatness. In the RF classiﬁer,we use128individual CART classiﬁers, each trained witha randomly selected subset of5feature dimensions, fromthe originally110-dimensional feature space. For post-processing of the decision functions, we employ a medianﬁlter with a width of1.4s. The decision function thresh-old is set to0.5. In the next sections, we keep these settingsﬁxed for the evaluation of our baseline system as well asour proposed post-processing strategies.5.2 Performance on a Common BenchmarkIn order to benchmark our novel feature set against thestate-of-the-art, we used a subset of the publicly availableJAMENDO music corpus [15]. Each recording in thatdata set was manually annotated into vocal and non-vocalsections by the original author. Since human annotatorscan have difﬁculties in determining singing segmentboundaries, the segmentation allowed some uncertainty,i.e., very short instrumental breaks were not labeled assuch. The exact split into training, validation and test setis speciﬁed in [15]. Table 2 lists our results in comparisonto previously published works. The used metrics arethe frame-wise F-measure and the accuracy which arecomputed by evaluating all frames across the16testsongs. According to the ground truth annotation, themajority of frames belongs to the non-vocal class. We alsoreport theBiased Guess, where all frames of a test itemare assigned to the vocal class, because in classical opera,the vocal class usually occurs more often. As can be seen,the performance of our proposed feature set is on par withthe state-of-the-art. Only the accuracy and F-measurereported in [9] surpass our results, but the comparisonmight not be entirely fair as discussed in Section 2.5.3 Opera Case-StudyThe opera “Der Freisch¨utz” by Carl Maria von Weber, awork of high relevance for opera studies, was chosen forthe further evaluation. For this opera, there exists a largenumber of historical sources, including a multitude of au-dio recordings. In the project “Der Freisch¨utz Digital”1,musicologists and computer scientists cooperate to exploreopportunities for new and digital ways of research, analy-sis and presentation of music related data in critical edi-tions [17].From the corpus used in the project, we had three differentversions of this opera available for the purpose of cross-version singing voice detection. The respective conductors1www.freischuetz-digital.deOpera Conductor Year“Carmen” Lorin Maazel 1984“Die Zauberﬂ¨ote” Nikolaus Harnoncourt 1988“Pelleas et Melisande” Claudio Abbado 1992“La Cenerentola” Riccardo Chailly 1993“La Traviata” Carlo Rizzi 2005“Tristan und Isolde” Daniel Barenboim 1995“Der Freisch¨utz” Karl Elmendorff 1944“Der Freisch¨utz” Carlos Kleiber 1973“Der Freisch¨utz” Karl-Heinz Bloemecke 2013Table 3. Overview over the used opera recordings. Theupper half speciﬁes the operas available as training set, thelower half gives the operas used as test set.and recording years are shown in Table 3. All numbersin the three versions have orchestral accompaniment andvarying number of soloist singers. We picked the numbers6, 8, and 9 as test cases of different musical complexity, aduet, a solo aria and a trio, respectively.For evaluation purposes, we ﬁrst had to generate referenceannotations of the singing voice activity in these pieces.This was achieved semi-automatically by means of align-ing a MIDI version of each piece to the recording and tak-ing the note onsets and offsets of the singing voice as ref-erence. Details about this procedure can be found in [5].Furthermore, each recording had its measures (i.e., the be-ginning of each bar) manually annotated to facilitate thealignment between corresponding versions of the samenumber. The manually annotated bar positions are usedto warp the individual decision functions to a commontime axis regardless of their original tempo and variationsthereof.5.4 Results and DiscussionThe diagrams in Figure 2 illustrate the beneﬁt of applyingbootstrap training (see Section 4.1), cross-version fusion(see Section 4.2), as well as a combination of both in twodifferent training scenarios. The bar plots in both (a) and(b) show the F-measures obtained per test item as well asthe average F-measure value. The following singing voicedetection and post-processing strategies were tested.Ran-dom Guessrefers to randomly assigning the frames of ourtest data to either the vocal or non-vocal class with equalprobability. Since the vocal class occurs more frequentlyin our test data, the resulting F-Measure is slightly abovechance.Biased Guessrefers to assigning the singing voiceclass to each frame of a test recording. It can be seen thatthe resulting F-measure is already quite high, again a con-sequence of the dominance of the vocal class in our testset.Baseline Detectionrefers to the results obtained bythe baseline singing voice detection system as described inSection 3.Bootstrap Detectionrefers to the results ob-tained by a second classiﬁcation run with an adapted RFclassiﬁer using the bootstrapping strategy as described inSection 4.1.Cross-version Fusionrefers to the results of622 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015060809Average0.60.650.70.750.80.850.90.951\n060809Average0.60.650.70.750.80.850.90.951\n  Random GuessBiased GuessBaseline DetectionBootstrap DetectionCross-version FusionBootstrap FusionMusical numbers F-measure (a) (b) Figure 2. The average F-measures obtained in two different training scenarios and four post-processing strategies. The testset consisted of three versions of the numbers 6, 8, and 9 from the opera “Der Freisch¨utz.” (a): Results obtained by trainingthe initial RF classiﬁer with popular music recordings from the RWC and JAMENDO data sets. (b): Results obtained bytraining the initial RF classiﬁer with classical opera recordings not including “Der Freisch¨utz.”fusing the initial RF decision functions of all available ver-sions of each test recording as described in Section 4.2.Finally,Bootstrap Fusionrefers to the results obtainedby combining both the bootstrap training and the cross-version fusion.The results in Figure 2(a) were obtained by training theinitial RF classiﬁer with a combined data set comprisingboth the JAMENDO [15] and RWC [13] subsets that areannotated for singing voice. Both corpora are dominatedby recordings of popular music. Obviously, this kind oftraining material differs from the music content in the testset. The average singing voice detection performance stayseven below the biased guess. However, this rather poor ini-tial estimate for the vocal frames can be used for bootstraptraining. Consequently, the bootstrap training leads to asubstantial performance gain, surpassing the bias results.Cross-version fusion of the imperfect initial decision func-tions leads to similar improvements as the bootstrap train-ing. The combination of both bootstrap training and cross-version fusion of decision functions delivers the best re-sults in this training scenario.The results in Figure 2(b) were obtained when trainingthe initial RF classiﬁer with recordings of classical opera.Speciﬁcally, we used the operas listed in the upper halfof Table 3. In total, the playtime of our training materialamounts to approximately4h. As can be seen from the F-measure of the baseline RF classiﬁer, this kind of trainingdata gives a considerable performance boost. This is notsurprising, since the orchestral timbre as well as the pro-nounced use of vibrato singing in these opera recordings isvery similar to our test items. The remaining F-measuresshow that the proposed post-processing strategies at bestlead to marginal improvements since the performance isalready saturated.From our comparison, we infer that bootstrap trainingcould be recommended as standard post-processing strat-egy for singing voice detection in classical opera record-ings. This is especially true if the initial classiﬁcation de-livers reasonable results that can be surpassed if more ap-propriate training data would be available. However, boot-strap training does not seem to help much if there exists nocombination of feature set, training set, and classiﬁer thatcan obtain good singing voice detection for the recordingunder analysis. Moreover, bootstrap training has the draw-back that it will likely produce erroneous decision func-tions when there is no singing voice activity at all through-out a recording. If these cases can not be ruled out frombootstrap training, singing voice detection results couldeven deteriorate in comparison to the baseline system.6 Conclusions and Future WorkIn this paper, we made two contributions to advancing thestate-of-the-art in automatic singing voice detection. First,we proposed a novel extension to a state-of-the-art au-dio feature set for singing voice detection and validatedit on a public benchmark set. Second, we proposed boot-strap training and cross-version fusion as post-processingstrategies applicable to intermediate results from a ma-chine learning system. In our case study, involving mul-tiple recordings of Carl Maria von Webers opera “DerFreisch¨utz,” we have shown that a combination of boot-strap training and cross-version fusion can help to improvethe classiﬁcation performance if the training data is verydifferent from the test data. While bootstrap fusion mightbe applicable to improve singing voice detection in variousmusic genres, cross-version fusion can only help if we havemultiple, sufﬁciently similar versions of the same piece ofmusic available. Future work will be directed towards fur-ther reﬁnements and applications of these techniques forvarious kinds of music genres.7 AcknowledgmentsThis work has been supported by the BMBF projectFreisch¨utz Digital (Funding Code 01UG1239A to C),and by the Austrian Science Fund (FWF) under grantsTRP307-N23 and Z159. The International Audio Lab-oratories Erlangen (AudioLabs) is a joint institutionof the Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg(FAU) and Fraunhofer IIS.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6238 References[1] Vincent Akkermans and Joan Serr´a. Shape-based spec-tral contrast descriptor. InProc. of the Sound and MusicComputing Conf. (SMC), pages 143–148, Porto, Portu-gal, July 2009.[2] Adam L. Berenzweig and Daniel P. W. Ellis. Locatingsinging voice segments within music signals. InProc.of the IEEE Workshop on the Applications of SignalProcessing to Audio and Acoustics (WASPAA), pages119–122, New Paltz, New York, USA, October 2001.[3] Leo Breiman. Random forests.Machine learning,45(1):5–32, 2001.[4] Christian Dittmar, Thomas Pr¨atzlich, and MeinardM¨uller. Towards cross-version singing voice detec-tion. InProc. of the Jahrestagung f¨ur Akustik (DAGA),Nuremberg, Germany, March 2015.[5] Sebastian Ewert, Meinard M¨uller, and Peter Grosche.High resolution audio synchronization using chromaonset features. InProc. of the IEEE Int. Conf. onAcoustics, Speech, and Signal Processing (ICASSP),pages 1869–1872, Taipei, Taiwan, April 2009.[6] Daniel G¨artner and Christian Dittmar. Vocal character-istics classiﬁcation of audio segments: An investiga-tion of the inﬂuence of accompaniment music on low-level features. InProc. of the Int. Conf. on MachineLearning and Applications (ICMLA), pages 583–589,Miami, Florida, USA, December 2009.[7] Daning Jiang, Lie Lu, Hong-Jiang Zhang, Jian-HuaTao, and Lian-Hong Cai. Music type classiﬁcation byspectral contrast feature. InProc. of the IEEE Int. Conf.on Multimedia and Expo (ICME), volume 1, pages113–116, Lausanne, Switzerland, August 2002.[8] Verena Konz, Meinard M¨uller, and Rainer Kleinertz.A cross-version chord labelling approach for exploringharmonic structures—a case study on Beethoven’s Ap-passionata.Journal of New Music Research, 42(1):1–17, January 2013.[9] Simon Leglaive, Romain Hennequin, and RolandBadeau. Singing voice detection with deep recurrentneural networks. InProc. of the IEEE Int. Conf. onAcoustics, Speech and Signal Processing (ICASSP),pages 121–125, Brisbane, Australia, April 2015.[10] Bernhard Lehner, Reinhard Sonnleitner, and Ger-hard Widmer. Towards lightweight, real-time-capablesinging voice detection. InProc. of the Int. Conf. onMusic Information Retrieval (ISMIR), pages 53–58,Curitiba, Brazil, November 2013.[11] Bernhard Lehner, Gerhard Widmer, and ReinhardSonnleitner. On the reduction of false positives insinging voice detection. InProc. of the IEEE Int.Conf. on Acoustics, Speech, and Signal Processing(ICASSP), pages 7480–7484, Florence, Italy, May2014.[12] Hanna Lukashevich and Christian Dittmar. Effectivesinging voice detection in popular music using armaﬁltering. InProc. of the Int. Conf. on Digital AudioEffects (DAFx), pages 165–168, Bordeaux, France,September 2007.[13] Matthias Mauch, Hiromasa Fujihara, KazuyoshiiYoshii, and Masataka Goto. Timbre and melody fea-tures for the recognition of vocal activity and instru-mental solos in polyphonic music. InProc. of the Int.Conf. on Music Information Retrieval (ISMIR), pages233–238, Miami, Florida, USA, October 2011.[14] Tin Lay Nwe and Ye Wang. Automatic detection of vo-cal segments in popular songs. InProc. of the Int. Conf.on Music Information Retrieval (ISMIR), pages 138–144, Barcelona, Spain, October 2004.[15] Mathieu Ramona, G¨ael Richard, and Bertrand David.Vocal detection in music with support vector machines.InProc. of the IEEE Int. Conf. on Acoustics, Speech,and Signal Processing (ICASSP), pages 1885–1888,Las Vegas, Nevada, USA, March 2008.[16] Lise Regnier and Geoffroy Peeters. Singing voice de-tection in music tracks using direct voice vibrato de-tection. InProc. of the IEEE Int. Conf. on Acoustics,Speech and Signal Processing (ICASSP), pages 1685–1688, Taipei, Taiwan, April 2009.[17] Daniel R¨owenstrunk, Thomas Pr¨atzlich, Thomas Bet-zwieser, Meinard M¨uller, Gerd Szwillus, and JoachimVeit. Das Gesamtkunstwerk Oper aus Datensicht - As-pekte des Umgangs mit einer heterogenen Datenlageim BMBF-Projekt ”Freisch¨utz Digital”.Datenbank-Spektrum, 15(1):65–72, 2015.[18] Zheng Tang and Dawn A. A. Black. Melody extractionfrom polyphonic audio of western opera: A methodbased on detection of the singer’s formant. InProc.of the Int. Society for Music Information RetrievalConf. (ISMIR), pages 161–166, Taipei, Taiwan, Octo-ber 2014.[19] George Tzanetakis. Song-speciﬁc bootstrapping ofsinging voice structure. InProc. of the IEEE Int. Conf.on Multimedia and Expo (ICME), volume 3, pages2027–2030, Taipei, Taiwan, June 2004.[20] Shankar Vembu and Stefan Baumann. Separation ofvocals from polyphonic audio recordings. InProc. ofthe Int. Conf. on Music Information Retrieval (ISMIR),pages 337–344, London, UK, September 2005.[21] Kristopher West and Stephen Cox. Features and clas-siﬁers for the automatic classiﬁcation of musical audiosignals. InProc. of the Int. Conf. on Music InformationRetrieval (ISMIR), Barcelona, Spain, October 2004.624 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Automated Estimation of Ride Cymbal Swing Ratios in Jazz Recordings.",
        "author": [
            "Christian Dittmar",
            "Martin Pfleiderer",
            "Meinard Müller"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418149",
        "url": "https://doi.org/10.5281/zenodo.1418149",
        "ee": "https://zenodo.org/records/1418149/files/DittmarPM15.pdf",
        "abstract": "In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drum- mers’ swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and eval- uation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical con- tribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for esti- mating swing ratios, circumventing the error-prone detec- tion of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research. 1",
        "zenodo_id": 1418149,
        "dblp_key": "conf/ismir/DittmarPM15",
        "keywords": [
            "microtiming",
            "drummers",
            "jazz recordings",
            "automatic analysis",
            "swing ratio",
            "RC onsets",
            "log-lag autocorrelation function (LLACF)",
            "jazz database",
            "experiment",
            "implications"
        ],
        "content": "AUTOMATED ESTIMATION OF RIDE CYMBAL SWING RATIOS IN JAZZRECORDINGSChristian DittmarInternational AudioLaboratories Erlangenchristian.dittmar@audiolabs-erlangen.deMartin PﬂeidererThe Liszt University of Music Weimarmartin.pfleiderer@hfm-weimar.deMeinard M¨ullerInternational AudioLaboratories Erlangenmeinard.mueller@audiolabs-erlangen.deABSTRACTIn this paper, we propose a new method suitable for theautomatic analysis of microtiming played by drummers injazz recordings. Speciﬁcally, we aim to estimate the drum-mers’ swing ratio in excerpts of jazz recordings taken fromthe Weimar Jazz Database. A ﬁrst approach is based onautomatic detection of ride cymbal (RC) onsets and eval-uation of relative time intervals between them. However,small errors in the onset detection propagate considerablyinto the swing ratio estimates. As our main technical con-tribution, we propose to use the log-lag autocorrelationfunction (LLACF) as a mid-level representation for esti-mating swing ratios, circumventing the error-prone detec-tion of RC onsets. In our experiments, the LLACF-basedswing ratio estimates prove to be more reliable than theones based on RC onset detection. Therefore, the LLACFseems to be the method of choice to process large amountsof jazz recordings. Finally, we indicate some implicationsof our method for microtiming studies in jazz research.1IntroductionJazz drummers usually keep time by using the ride cymbal(RC) and hi-hat (HH), especially in styles with so-called“swing feel” [2]. They commonly emphasize the “back-beat,” i.e., the metric-harmonically unaccented beat, onthe HH while playing typical patterns on the RC. Accord-ing to [21, p. 248], this supports the “light” character ofjazz rhythm. Instead of playing the beat in a steady man-ner, variations and additional “offbeat” strokes are usuallyadded on the RC as well as on other drum parts. Thesevariations differ from drummer to drummer and from per-formance to performance [2, pp. 617-629].The most common time-keeping pattern played on the RCis shown in Figure 1. In addition to conventional drumnotation in the top row, we show a corresponding time-domain signal at240BPM with overlaid amplitude en-velope (bold black curve) and the so-called novelty curve© Christian Dittmar, Martin Pﬂeiderer, Meinard M¨uller.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Christian Dittmar, Martin Pﬂeiderer,Meinard M¨uller. “Automated Estimation of Ride Cymbal Swing Ratiosin Jazz Recordings”, 16th International Society for Music InformationRetrieval Conference, 2015.(thin black curve). We color-code the relevant beats andsubdivisions thereof as follows. The sequence starts withthe so-called “downbeat” quarter note (light blue), followedby the backbeat eighth note (light green), and the offbeateighth note (light red) before starting over again with thedownbeat. We will refer to this prototype sequence of on-sets as RC pattern.The so-called swing ratio expresses the beat subdivisionand relates to the phrasing of the eighth notes in the RCpattern. Swinging eighth notes are typically played in dif-ferent ratios, ranging continuously from straight eighths(1 : 1), over triplet eighths(2 : 1), to dotted eighths(3 : 1), or more extreme ratios. The swing ratio is reportedto be tempo dependent [4, 9, 15], cf. Section 2.1. In Fig-ure 1, the color-coded tone durations show how the back-beat duration grows with increasing swing factor, while thecomplementing offbeat duration shrinks. In Figure 1(a),backbeat and offbeat have equal duration, correspondingto straight eighths as given in the drum notation. In Fig-ure 1(b), the RC pattern is notated as tied-triplets. In Fig-ure 1(c), the backbeat duration equals a dotted eighth. Con-sequently, the offbeat duration equals that of a sixteenthnote as shown in the drum notation.There are several case studies concerning the swing ratio injazz (cf. [21, pp. 262-273], and Section 2.1). While mostof the studies examine swing ratios of soloists, it is widelyacknowledged that the swing ratio of the RC pattern cru-cially contributes to the “swinging” character of the music.Most of the studies are based on manual transcription ofonsets, often by visual inspection of the amplitude enve-lope of jazz excerpts. Few studies speciﬁcally examine theRC pattern [15] and its interaction with the soloist’s tim-ing [9]. This inspired us to develop and to evaluate meth-ods for automated swing ratio estimation from RC patternsin jazz recordings. For sure, an automated generation oflarge amounts of reliable swing ratio data is essential formeaningful and more differentiated research on microtim-ing in jazz. Besides onset-based swing ratio estimation,our main approach is a log-lag variant of a local autocor-relation function (ACF) applied to onset-related noveltyfunctions (see Sections 3.3). We refer to this representa-tion as log-lag ACF (LLACF) and show its applicability toswing ratio estimation in Section 3.4.27100.250.50.751-101\n960480240120 6000.20.40.60.800.250.50.751-101\n960480240120 6000.20.40.60.800.250.50.751-101\n960480240120 6000.20.40.60.8\nAmplitude Envelope Ride  Cymbal Hi-hat 𝑠r=0.1250.125≈1 𝑠r=0.1670.083≈2 𝑠r=0.1880.063≈3 (a) (b) (c) Periodicity Salience Time (Seconds) Log-Lag (BPM) downbeat backbeat offbeat Figure 1. Illustration of prototypical RC patterns as drum notation (top), time-domain signal (mid), and LLACF (bottom).(a):Swing factor ofsr=1corresponding to straight eighth notes.(b):Swing factor ofsr=2corresponding to theidealized “tied-triplet feel”.(c):Swing factorsr=3, where the duration of the backbeat equals a dotted eighth note.2Related WorkA number of papers are concerned with systematic stud-ies on swing ratio in jazz music. Since most of the studiesuse comparably small data sets and manual annotation, wethink that swing ratio estimation is a suitable task to ap-ply automatic methods from Music Information Retrieval(MIR) research in order to enable analysis of larger musicdata sets.2.1Jazz Microtiming AnalysisAn early attempt to analyze swing ratios in jazz recordingsis described in [17]. The author relies on visual inspectionof spectrograms but does not report quantitative results.In [22], the swing ratios in the analyzed jazz recordingsare reported to range from1.48to1.82. Rose [23] reportsan average swing ratio of2.38measured from amplitudeenvelopes. In [7], an average swing ratio of1.75is mea-sured using a MIDI wind controller played by saxophon-ists. In [19], the analysis focuses on the RC and swingfactors between1.0and3.3are reported without detail-ing the measurement method. In [6], an average swing ra-tio of1.6is measured using amplitude envelopes. Fribergand Sundstr¨om [9] annotated RC onsets in spectrograms ofjazz excerpts. They report trends indicating a high negativecorrelation between the tempo and the swing ratio whichseems to be valid across different drummers. In [3], an av-erage swing ratio of2.45is measured in the performancesof pianists playing a MIDI piano. In [1], comparably lowswing ratios in the range between0.9to1.7are measuredfrom amplitude envelopes. Honing and de Haas [15] con-ducted experiments with professional jazz drummers per-forming on a MIDI drum kit. Besides further evidence forthe tempo dependency of swing ratios, the results show thatjazz drummers have enormous control over their timing.2.2Rhythmic Mid-Level FeaturesMotivated by the need to design specialized mid-level fea-tures for music similarity estimation, several authors pro-posed conceptually similar, tempo-independent represen-tations of rhythmic patterns. The basic observation is, thatrhythmic patterns that are perceived as similar by humanlisteners may not be judged as similar by automatic meth-ods. One of the main reasons is that the patterns are typi-cally played in different tempi, which makes them unsuitedfor direct comparison. Therefore, Peeters [20] used temponormalized spectral rhythm patterns to automatically clas-sify ballroom dance styles. Holzapfel and Stylianou [13,14] proposed to apply the scale transform to periodicityspectra to enable the use of conventional distance mea-sures between rhythmic patterns despite tempo differences.Around the same time, the LLACF was proposed in [12] aswell as the tempo-insensitive representation used for clas-siﬁcation of ballroom dances in [16]. The LLACF was re-ported to be favorable over the scale transform for classiﬁ-cation of Latin American rhythm patterns in [24]. The tem-pogram as described in [11] is based on similar ideas andadditionally features a cyclic post-processing to remedythe problem of octave ambiguity. Marchand and Peeters[18] revisited the scale transform and applied it to modula-tion spectra as tempo-independent feature, again for clas-siﬁcation of ballroom dances. Eppler et al. [8] used peakratios in the LLACF as features for detecting the swing feelbut did not explicitly try to estimate swing ratios.3MethodIn this section, we describe our approaches to automaticswing ratio estimation from excerpts of jazz recordingswith swing feel. The ﬁrst variant relies on peak-pickingin an onset-related novelty curve (Section 3.1). The sec-272 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Time (Seconds) 𝛼∙𝛿 𝛿b 𝛿o Relative Magnitude Figure 2. A four seconds excerpt from the1979recording of “Anthropology”, performed by Art Pepper playing soloclarinet, with Charlie Haden on bass and Billy Higgins on drums. The bold black curve depicts the novelty function\u0000, thethin black curve shows the RC related thresholdH. Automatically detected RC onsets are marked by the bold black crosses,colored crosses represent the four onset triples accepted for swing ratio estimation. The note durations are color-coded inthe same way as in Figure 1.ond approach relies on computation of the LLACF fromthe novelty curve (Section 3.3) and comparison to proto-type LLACFs. As will be explained in Section 4.1, we havea rough tempo estimate⌧e2R>0available for each jazzexcerpt. Let\u0000b,\u0000o2R>0be the tone duration of the back-beat and the offbeat in an RC pattern as shown in Figure 1.They relate to the tempo by⌧e⇡(\u0000b+\u0000o)\u00001⇡\u0000\u00001, withthe beat (quarter note) duration\u00002R>0. The targetedswing ratio is given by:sr=\u0000b\u0000o(1)Consequently,\u0000b=\u0000·sr·(1 +sr)\u00001yields the tone du-ration of the backbeat and\u0000o=\u0000·(1 +sr)\u00001yields thetone duration of the offbeat.3.1Ride Cymbal Onset DetectionWith regard to Eqn (1), we aim to measure\u0000band\u0000ofrom the jazz excerpts under analysis. One possibility isto search for RC onsets and use the time differences be-tween consecutive onsets as estimate for note durations.To this end, we compute a time-frequency (TF) represen-tation of an excerpt using the short-time Fourier trans-form (STFT) with blocksizewand hopsizergiven in sec-onds. LetX(m, k)withm2[1 :M],k2[0 :K]be acomplex-valued STFT coefﬁcient at themthtime frameandkthspectral bin. Here, the interval[1 :M]repre-sents the time axis andKcorresponds to the Nyquist fre-quency. Following the approaches in [10, 11], we com-pute a novelty curve\u0000:[ 1 :M]!Ras follows. First,we derive the logarithmically compressed magnitude spec-trogramY(m, k) := log (1 +\u0000·| X(m, k)|)for a suitableconstant\u0000\u00001. Then, the novelty function is given as\u0000(m): =KXk=0|Y(m+1,k)\u0000Y(m, k)|\u00000,(2)where|·|\u00000denotes half-wave rectiﬁcation. The resulting\u0000exhibits salient peaks at frames corresponding to toneonsets. Inevitably, spurious peaks may occur in\u0000thatcould be mistaken for RC onsets. Thus, we derive an RCrelated threshold function asH(m): =KXk=k0|X(m, k)|,(3)where the bink0corresponds to the lower cutoff frequency.Figure 2 shows an example of\u0000as bold black curve andthe correspondingHas thin black curve. For the sake ofvisibility, both curves are normalized to unit maximum inthe plot. We take the average value ofHas threshold crite-rion and only accept peaks from\u0000in frames whereHex-ceeds this value (indicated by the white background). TheN= 18local maxima accepted as RC onsets are markedby bold crosses. Multiplication of the corresponding frameindices with the hopsizeryields a set of strictly mono-tonically increasing onset timesB={b1,b2,...,bN}foronset-based swing ratio estimation.3.2Onset-Based Swing Ratio EstimationOnce we obtained a sequenceBof RC onsets, we esti-matesrin a tempo-informed manner. Assuming a roughlyconstant tempo⌧ethroughout the excerpt, the time interval\u0000=⌧\u00001ebetween two consecutive beats should be closeto\u0000b+\u0000o. To account for small deviations from the idealbeat period\u0000, we introduce a tolerance↵\u00001. Now, wego through every previously detected RC onset and test thehypothesis that it could be the ﬁrst in a series of three con-secutive onsets (backbeat, offbeat, downbeat). We denotethis sub-sequence asBn={bn,bn+1,bn+2},Bn⇢Band refer to it as onset triple. From all possible triplesBn,n2[1 :N\u00002]we accept the ones that fulﬁll the cri-terion(bn+2\u0000bn)<↵·\u0000(4)as instances of triples embedded in an RC pattern. Theswing ratio is estimated from a valid onset triple by setting\u0000b=bn+1\u0000bnand\u0000o=bn+2\u0000bn+1in Eqn (1). InFigure 2, we illustrate this procedure. All RC onset candi-dates are marked by black crosses but only the triples thatfulﬁll the constraint in Eqn (4) are marked with differentcolors. Above the third triple (blue note symbols) we de-pict the extent of the search range↵·\u0000that covers both\u0000band\u0000o. As indicated in the plot, we try to ﬁnd multipleProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 273occurrences of the RC pattern triples per excerpt, so wecan obtain a more robust estimate for the swing ratio byaveraging over the individualsr-values computed for eachtriple. For that reason, we also accept variations of the RCpattern where the offbeat impulse occurs in succession tothe downbeat instead of the backbeat. As will be explainedin Section 4.4, there are situations where estimation ofsrfrom RC onsets may deliver erroneous results. To obtainmore robust estimates, we introduce LLACF-based swingratio estimation in the next two sections.3.3LLACF Mid-Level RepresentationWe propose to employ the LLACF as a tempo-normalizedmid-level representation capturing the swing ratio that isimplicitly encoded in the peaks of\u0000. Using the LLACF,we can circumvent the selection of onset candidates andinstead transform the complete\u0000into a phase-invariant,tempo-normalized representation. Swing ratio estimationthen boils down to matching this representation to LLACFswith known swing ratios (see Section 3.4). To this end, weﬁrst compute a normalized ACF from the novelty function\u0000as:R\u0000\u0000(`)=PM\u0000`m=1\u0000(m)\u0000(m\u0000`)PMm=1\u0000(m)2,(5)where we only consider the positive lags`2[0 :M\u00001].Note thatR\u0000\u0000(`)=R\u0000\u0000(\u0000`)due to symmetry. More-over,R\u0000\u0000(0) = 1andR\u0000\u0000(`)<1for`2[1 :M\u00001].Each lag can be expressed as tempo value by the relation⌧=60r·`. We now deﬁne a logarithmically spaced tempo(log-tempo) axis, that has equal distanceqbetween tempooctaves and has the reference tempo⌧rat a deﬁned posi-tion. After correction for the ratio between the excerpt’stempo estimate⌧eand the reference tempo⌧r, we use lin-ear interpolation to warpR\u0000\u0000onto this axis, yielding ourtempo-normalized LLACFA. Despite using a log-tempoaxis, we stick to the term log-lag ACF since the inverserelation`=60r·⌧retains the logarithmic spacing, just in op-posite direction.In the bottom row of Figure 1, we show the LLACFs cor-responding to the prototypical RC patterns. Variation ofsrgives an intuition how the salience of different periodici-ties in the RC pattern is represented by the LLACF. Since⌧ris constant, all three LLACFs have clear peaks at thebeat periodicity (240BPM) and its integer subdivisions.Forsr=1in Figure 1(a), there is a strong peak at480BPM (corresponding to the straight eighth notes). With in-creasing swing ratio, this peak diverges into two lobes thatmove to other periodicities. In Figure 1(c), the ﬁrst peakresides at960BPM (offbeat equals a sixteenth note) andthe second peak is at320BPM (backbeat equals a dottedeight note).3.4LLACF-Based Swing Ratio EstimationIn order to estimate a swing ratio from the shape ofA, weconstruct a setAsr,sr2Rwith1sr4of prototype\n  \n9604802401206011.522.533.5\n  \n960480240120601234\n0.20.40.6\n0.20.40.60.8Swing ratioݏ୰\nTempo (BPM)(a)\n(b)\nFigure 3. Evolution of the LLACF computed from RCpatterns with increasing swing ratio.(a):LLACFs de-rived from novelty functions of idealized prototype RCpatterns at a reference tempo⌧rof240BPM.(b):LLACFsextracted from our test corpus that have been warped tomatch⌧r.LLACFs. They are extracted from novelty functionsof idealized RC patterns with ﬁxed reference tempo⌧rand varying swing ratiosr(cf. the time-domain plots inFigure 1). In Figure 3(a) we show the complete set ofprototype LLACFs with the log-tempo axis in BPM andthe swing ratio increasing from bottom to top. Darkershade of gray corresponds to higher periodicity salience.One can clearly see how the offbeat-related peaks changetheir periodicity with the swing ratio while the peaksrelated to the beat (and subdivisions thereof) reside at thesame periodicity.Now, our approach to swing ratio estimation is to comparethe extractedAto each of these prototype LLACFs andto select the swing ratio corresponding to the best match.For the comparison, we employ Pearson’s correlationcoefﬁcient. We have to take into account that the tempoestimate⌧eused for warping the LLACF to the referencelog-tempo axis underlyingAsrmay be slightly inaccurate.As a consequence, the resultingAmight exhibit a constantoffset with respect to the prototypeAsr. Thus, we shift theAagainst the log-tempo axis of eachAsrin a restrictedinterval[\u0000q·log2(↵):+q·log2(↵)]to ﬁnd the bestalignment. Finally, thesrcorresponding the maximumcorrelation coefﬁcient over all entries inAsris selected.4EvaluationIn this section, we describe the setup, metrics, and resultsof the experiments we conducted in order to compare man-ual, onset-based, and LLACF-based swing ratio estima-tion. In addition, some trends visible in the data are dis-cussed.274 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20155101520253035401234\n  Ground truth swing ratiosOnset-based estimatesLLACF-based estimates\nGround truth excerpts Swing ratio 𝑠r Figure 4. Comparison of the swing ratios estimated fromground truth RC onsets, automatically detected RC onsetsand LLACF analysis.4.1The Weimar Jazz DatabaseThe Weimar Jazz Database1consists of299(as in July2015) transcriptions of instrumental solos in jazz record-ings performed by a wide range of renowned jazz musi-cians. The solos have been manually annotated by musi-cology and jazz students at Liszt School of Music Weimaras part of the Jazzomat Research Project.2Several mu-sic properties are annotated, most notably the pitch, onsetand offset of all tones played by the soloists, as well asa manually tapped beat grid, chords, form parts, phraseboundaries, and articulation. For our work, we only usethe beat grid. From the complete Weimar Jazz Database,we automatically selected a subset of921excerpts that hadbeen labeled with swing feel. Because we will compare theswing ratios of drummers and soloists in our future work,the excerpts had to contain at least5consecutive eighthnotes played by the soloists. The total playtime of the se-lected excerpts amounts to roughly50minutes (out of8hours), their average duration is3.3seconds.4.2Evaluation SettingA subset of42excerpts have been manually annotated forRC onsets in order to create a ground truth for swing ra-tio estimation. The reference onsets were transcribed bytwo experienced student assistants of the Jazzomat Re-search Project using the software Sonic Visualiser [5]. Theground truth subset was split in two, approximately equalparts and each part was given to one of the annotators. Intotal,834RC onsets were manually annotated. In our eval-uation (cf. Sections 4.3, 4.4, and 4.5), we used the well-known metrics recall, precision and F-measure for quan-titative evaluation. In order to count an onset candidateas true positive, we allowed a maximum deviation of±30ms to the ground truth onset time. Furthermore, we usedPearson’s correlation coefﬁcient as a means to quantify theagreement between reference swing ratios and automati-cally estimated swing ratios. We ﬁxed the following ex-traction parameters for the automatic estimation of swing1http://jazzomat.hfm-weimar.de/dbformat/dboverview.html2http://jazzomat.hfm-weimar.de/ratios: The STFT blocksizewwas appr.46ms and the hop-sizerwas appr.5.8ms. The compression-constant\u0000was1000, the lower cutoffk0was set to equal appr.12.9kHz,the reference tempo⌧rwas240BPM, the LLACF octave-resolutionqwas36. The tolerance↵for tempo deviationswas1.2.4.3Cross-ValidationAt ﬁrst, we are interested in the agreement between ourhuman annotators, since we suspect that there may be am-biguous cases where it is not clear where an RC onset isexactly located in time or if there is an onset at all. Thus,we selected a small subset of11excerpts for which the an-notators created a cross-validation transcription. Runningthese against the larger set, we receive an F-measure ofappr.0.96. The average absolute time difference betweenmatched onsets in the reference and the cross-validation setamounts to7.8ms.4.4Onset-Based EvaluationNext, we used the previously validated ground truth an-notations as reference to assess the performance of ourautomated RC onset detection described in Section 3.2.In this scenario, we received an F-measure of appr.0.93and an average onset deviation of2.5ms. Since these re-sults seem surprisingly good, we wanted to quantify howmuch potential onset detection errors would propagate intothe swing ratio estimation. Using the procedure describedin Section 3.2, we determined ground truth swing ratiosfor all manually annotated excerpts. When we comparedthese to the swing ratios estimated from automatically de-tected RC onsets, we yielded a correlation coefﬁcient ofappr.0.66(see Figure 4). With regard to the compara-bly high F-measure obtained for the onset detection, thisunsatisfactory result may seem surprising at ﬁrst, but canbe explained using the example in Figure 2. There, wesee that only12out of18RC onsets are considered forswing ratio estimation. Intuitively, small deviations in thedetected onset times can lead to under- or overestimationof the swing ratio, especially for fast tempi, where subtletiming differences may get lost due to the coarse samplingof the analysis frames. Even worse errors may be causedby spurious onsets that fulﬁll the threshold criterion but areactually not RC patterns. This is the case for the sixth ex-cerpt in Figure 4, where some sort of RC swell is mistakenfor an onset triple, leading to a overestimation ofsr.4.5LLACF-Based EvaluationSince we found the correlation between ground truth swingratios and onset-based swing ratios to be unsatisfactory, werepeated the comparison with respect to swing ratios esti-mated from the LLACF as described in Section 3.3. Thistime, we received a correlation coefﬁcient of appr.0.9. InFigure 4, one can see that both methods behave similarProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 275501001502002503003500.511.522.533.54\n  \n501001502002503003506080100120140160180\n  Art TaylorConnie KayBilly HigginsElvin JonesTony WilliamsCarl AllenJeffrey WattsJack DeJohnetteMax RoachDennis ChambersTempo (BPM) Swing ratio 𝑠r \nOffbeat duration (ms) \nTempo (BPM) (a) (b) Figure 5. Scatter plots showing the relationship of tempo vs.(a):swing ratio and(b):offbeat duration. Each markercorresponds to one jazz excerpt. We only show the10most frequently represented drummers.but the onset-based swing ratios exhibit some pronouncedoutliers. Moreover, Figure 3 shows that the prototypicalLLACFs inAsrcorrespond quite well to the LLACFs ex-tracted from our test corpus. Both plots depict the LLACFsordered by the corresponding swing ratio. The typicalstructure of periodicity peaks is clearly visible, althoughthe LLACFs extracted from the jazz excerpts are muchmore noisy than the idealized LLACFs. This leads us tothe conclusion that the LLACF-based swing ratio estima-tion is a reliable method that should be preferred over theonset-based swing ratio estimation.4.6Comparison to Friberg and Sundstr¨omIn Section 1, we already indicated our aim to re-examinethe ﬁndings of Friberg and Sundstr¨om [9] on a larger scale.As can be seen in Figure 5(a), our automatically estimatedswing ratios show similar trends as the manually annotateddata used in the original paper. However, while Fribergand Sundstr¨om only had around40excerpts from variouspieces of four drummers, we are able to study several hun-dreds of RC patterns played by a wide range of drummersdue to our automated method (three among them—TonyWilliams, Jack DeJohnette, and Jeffrey Watts—were ex-amined by Friberg and Sundstr¨om, too).In Figure 5, we show the results obtained for the10drum-mers represented with the most excerpts. Each point in thescatterplots is placed according to (a)srvs.⌧eand (b)\u0000ovs.⌧e. In general, the negative correlation of swing ratioand tempo is clearly discernable—for the whole data setas well as for certain drummers like Elvin Jones or BillyHiggins, who vary their swing ratio from appr.2.5around150BPM to appr. 1.5 at250BPM, and in the case of Joneseven to around1.0at300BPM. However, there are alsodrummers who seems to keep almost the same swing ratioat different tempi, e.g., Art Taylor or Carl Allen.Additionally, Friberg and Sundstr¨om report the durationbetween the offbeat impulse and the next beat to be roughlyconstant at100ms for all tempi faster than150BPM(cf. [9, p. 337]). In general, this ﬁnding is supported byour data (see Figure 5(b)), but the offbeat durations have awider range from110ms to80ms and even70ms.5Conclusions and Future WorkIn this paper, we presented a microtiming study conductedon a subset of the publicly available Weimar Jazz Database.Future work will be directed towards extending our methodto more drummers and other recordings as well as to thecomparison between RC patterns and soloists. Exact onsettimes of all tones of the soloists, and thus their microtim-ing and swing ratio, are at hand within the Weimar JazzDatabase. A comparison between drummers’ and soloists’microtiming will allow for a larger scale re-examinationof one of the central ﬁndings in [9]: The swing ratio ofsoloists is in general lower then the swing ratio of the ac-companying drummer since soloists deliberately play be-hind the beat while synchronizing the offbeat with thedrummer. They do so, because, as Friberg and Sund-str¨om claim, “delayed downbeats and synchronized off-beats may create both the impression of the laid-backsoloist, which is often strived for in jazz, and at the sametime an impression of good synchronization” [9, p. 345].Therefore, using microtiming data from the Weimar JazzDatabase as well as automatically estimated swing ratiosof RC patterns may lead to new insights in the interactiveart of improvising together in a professional jazz ensemble.6AcknowledgmentsThe Jazzomat Research Project is supported by theGerman Research Foundation (Melodisch-rhythmischeGestaltung von Jazzimprovisationen. RechnerbasierteMusikanalyse einstimmiger Jazzsoli, DFG-PF 669/7-1).The authors would like to thank all student assistants par-ticipating in the transcription and annotation process. TheInternational Audio Laboratories Erlangen (AudioLabs) isa joint institution of the Friedrich-Alexander-Universit¨atErlangen-N¨urnberg (FAU) and Fraunhofer IIS.276 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157References[1]Fernando Benadon. Slicing the beat: Jazz eighth-notesas expressive microrhythm.Ethnomusicology, pages73–98, 2006.[2]Paul F. Berliner.Thinking in Jazz. The Inﬁnite Art ofImprovisation. University of Chicago Press, 1994.[3]Walter Gerard Busse. Toward objective measurementand evaluation of jazz piano performance via midi-based groove quantize templates.Music Perception,19(3):443–461, 2002.[4]Matthew W. Butterﬁeld. Why do jazz musicians swingtheir eighth notes?Music Theory Spectrum, 33(1):3–26, 2011.[5]Chris Cannam, Christian Landone, and Mark B. San-dler. Sonic visualiser: An open source application forviewing, analysing, and annotating music audio ﬁles.InProc. of the International Conference on Multime-dia, pages 1467–1468, Florence, Italy, 2010.[6]Geoffrey L. Collier and James Lincoln Collier. A studyof timing in two louis armstrong solos.Music Percep-tion, 19(3):463–483, 2002.[7]Mark C. Ellis. An analysis of ’swing’ subdivision andasynchronization in three jazz saxophonists.Percep-tual and Motor Skills, 73(3):707–713, 1991.[8]Arndt Eppler, Andreas M¨annchen, Jakob Abeßer,Christof Weiß, and Klaus Frieler. Automatic styleclassiﬁcation of jazz records with respect to rhythm,tempo, and tonality. InProc. of the Conference on In-terdisciplinary Musicology (CIM), December 2014.[9]Anders Friberg and Andreas Sundstr¨om. Swing ra-tios and ensemble timing in jazz performance: Evi-dence for a common rhythmic pattern.Music Percep-tion, 19(3):333–349, 2002.[10]Peter Grosche and Meinard M¨uller. Extracting predom-inant local pulse information from music recordings.IEEE Transactions on Audio, Speech, and LanguageProcessing, 19(6):1688–1701, 2011.[11]Peter Grosche, Meinard M¨uller, and Frank Kurth.Cyclic tempogram – a mid-level tempo representa-tion for music signals. InProc. of IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP), pages 5522–5525, Dallas, Texas, USA,March 2010.[12]Matthias Gruhne and Christian Dittmar. ImprovingRhythmic Pattern Features Based on Logarithmic Pre-processing. InProc. of the Audio Engineering Soci-ety Convention (AES), Munich, Germany, May 2009.Preprint 7817.[13]Andr´e Holzapfel and Yannis Stylianou. A scale trans-form based method for rhythmic similarity of music. InProc. of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP), pages317–320, April 2009.[14]Andr´e Holzapfel and Yannis Stylianou. Scale trans-form in rhythmic similarity of music.IEEE Trans-actions on Audio, Speech & Language Processing,19(1):176–185, 2011.[15]Henkjan Honing and W. Bas de Haas. Swing oncemore: Relating timing and tempo in expert jazz drum-ming.Music Perception: An Interdisciplinary Journal,25(5):471–476, 2008.[16]Jesper Højvang Jensen, Mads Græsbøll Christensen,and Søren Holdt Jensen. A tempo-insensitive repre-sentation of rhythmic patterns. InProc. of the Euro-pean Signal Processing Conference (EUSIPCO), pages1509–1512, Glasgow, Scotland, August 2009.[17]Franz Kerschbaumer.Miles Davis: Stilkritische Un-tersuchungen zur musikalischen Entwicklung seinesPersonalstils. Studies in jazz research. AkademischeDruck und Verlagsanstalt, 1978.[18]Ugo Marchand and Geoffroy Peeters. The modulationscale spectrum and its application to rhythm-contentdescription. InProc. of the International Conferenceon Digital Audio Effects (DAFx), pages 167–172, Er-langen, Germany, September 2014.[19]Will Parsons and Ernest Cholakis. It dont mean a thingif it aint dang, dang-a dang!Downbeat, 52(8):61, 1995.[20]Geoffroy Peeters. Rhythm classiﬁcation using spectralrhythm patterns. InProc. of the International Confer-ence on Music Information Retrieval (ISMIR), pages644–647, London, UK, September 2005.[21]Martin Pﬂeiderer.Rhythmus: Psychologische, theo-retische und stilanalytische Aspekte popul¨arer Musik.Transcript, 2006.[22]Peter Reinholdsson. Approaching jazz performancesempirically. some reﬂections on methods and prob-lems.Action and perception in rhythm and music,55:105–125, 1987.[23]Richard Franklin Rose.An Analysis of Timing in JazzRhythm Section Performances. PhD thesis, Universityof Texas, 1989.[24]Thomas V¨olkel, Jakob Abeßer, Christian Dittmar, andHolger Großmann. Automatic genre classiﬁcation onlatin music using characteristic rhythmic patterns. InProc. of the Audio Mostly: A Conference on Interactionwith Sound, pages 16:1–16:7, Pite˚a, Sweden, Septem-ber 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 277"
    },
    {
        "title": "Let it Bee - Towards NMF-Inspired Audio Mosaicing.",
        "author": [
            "Jonathan Driedger",
            "Thomas Prätzlich",
            "Meinard Müller"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415698",
        "url": "https://doi.org/10.5281/zenodo.1415698",
        "ee": "https://zenodo.org/records/1415698/files/DriedgerPM15.pdf",
        "abstract": "A swarm of bees buzzing “Let it be” by the Beatles or the wind gently howling the romantic “Gute Nacht” by Schu- bert – these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automati- cally generating audio mosaics with the objective to pre- serve the source’s timbre in the mosaic. Inspired by algo- rithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target record- ing. However, when applying the original NMF proce- dure, the resulting mosaic does not adequately reflect the source’s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source’s timbral character- istics in the resulting mosaic.",
        "zenodo_id": 1415698,
        "dblp_key": "conf/ismir/DriedgerPM15",
        "keywords": [
            "audio mosaics",
            "convey musical aspects",
            "preserve sources timbre",
            "non-negative matrix factorization",
            "activation matrix",
            "spectrogram",
            "sparse diagonal structures",
            "sources timbral characteristics",
            "iterative learning procedure",
            "extended update rules"
        ],
        "content": "LET IT BEE – TOWARDS NMF-INSPIRED AUDIO MOSAICINGJonathan Driedger, Thomas Pr¨atzlich, Meinard M¨ullerInternational Audio Laboratories Erlangen{jonathan.driedger,thomas.praetzlich,meinard.mueller}@audiolabs-erlangen.deABSTRACTA swarm of bees buzzing “Let it be” by the Beatles or thewind gently howling the romantic “Gute Nacht” by Schu-bert – these are examples ofaudio mosaicsas we want tocreate them. Given atargetand asourcerecording, thegoal of audio mosaicing is to generate amosaicrecordingthat conveys musical aspects (like melody and rhythm) ofthe target, using sound components taken from the source.In this work, we propose a novel approach for automati-cally generating audio mosaics with the objective to pre-serve the source’s timbre in the mosaic. Inspired by algo-rithms fornon-negative matrix factorization(NMF), ouridea is to use update rules to learn an activation matrixthat, when multiplied with the spectrogram of the sourcerecording, resembles the spectrogram of the target record-ing. However, when applying the original NMF proce-dure, the resulting mosaic does not adequately reﬂect thesource’s timbre. As our main technical contribution, wepropose an extended set of update rules for the iterativelearning procedure that supports the development of sparsediagonal structures in the activation matrix. We show howthese structures better retain the source’s timbral character-istics in the resulting mosaic.1. INTRODUCTIONUsing the sounds in a recording of buzzing bees to recre-ate a recording of the song “Let it be” by the Beatles is atypical example of an audio mosaic. In this example, therecording of the bees serves assource, while the Beatlesrecording is called thetarget. Ultimately, one should beable to identify the target recording when listening to themosaic, but at the same time perceive the timbre of thesource sounds. Therefore, the audio mosaic of “Let it be”with the bee recording could give the impression of beesbeing musicians, buzzing the song’s tune.Audio mosaicing is an interesting audio effect whichhas found its way into both artistic work as well as aca-demic research. Artists like John Oswald used thousandsof manually selected source audio snippets to create newc\u0000Jonathan Driedger, Thomas Pr¨atzlich, Meinard M¨uller.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Jonathan Driedger, Thomas Pr¨atzlich,Meinard M¨uller. “Let it Bee – Towards NMF-inspired Audio Mosaicing”,16th International Society for Music Information Retrieval Conference,2015.\nTime target Frequency Target \nTime source Frequency Source Time source Time target ≈ . Learned activations \nTime target Frequency Mosaic = \nFigure 1. Schematic overview of our proposed audio mo-saicing method. The sparse diagonal structures in the acti-vation matrix are important in order to preserve the timbreof the source in the mosaic.musical compositions1and real-time audio mosaicing hasbeen used by musicians as an instrument in live perfor-mances [4,22]. Over the years, many different systems foraudio mosaicing were proposed [1,3,5,11,13,17,18]. Thecore idea of most automated systems is to split the sourceinto short audio segments, which are suitably concatenatedafterwards to match spectral and temporal characteristicsof the target [19].In this work, we propose a novel way to create audiomosaics. Our idea is to learn anactivation matrixthat,when multiplied with the spectrogram of the source record-ing, approximates the spectrogram of the target recording(see Figure 1). The source spectrogram hereby serves as atemplate matrixwhich is ﬁxed throughout the learning pro-cess. This way, as opposed to many previous automatedmosaicing approaches, a frame of the target can be re-synthesized as the superposition of several spectral framesof the source , thus allowing “polyphony” of the sourcesounds.1Especially on his albumPlexure[16].350As a ﬁrst contribution, we propose an audio mosaicingprocedure which is inspired by well-known algorithms fornon-negative matrix factorization(NMF) [14]. Keepingthe template matrix ﬁxed (the source’s magnitude spectro-gram), this basic procedure learns an activation matrix byiteratively applying a standard NMF update rule to a ran-domly initialized matrix. Experiments show that in casethe source recording offers an appropriate amount of dif-ferent sounds, this procedure can closely approximate thespectrogram of the target recording. However, the source’stimbre is often barely recognizable in the resulting mo-saics. The reason is that the procedure recreates every tar-get frame independently, thus destroying temporal charac-teristics of the source in the ﬁnal audio mosaic. Further-more, the method can superimpose an arbitrary number ofspectral frames from the source to construct a good nu-merical approximation of a single target frame. A super-position of a large number of source sounds may howeverresult in a timbre that is no longer similar to the actual tim-bre of the source. Therefore, an exact approximation of thetarget’s spectrogram cannot be our procedure’s sole goal.As our main technical contribution, we therefore pro-pose an extended set of update rules that supports the de-velopment of sparse diagonal structures in the activationmatrix during the learning process (see the activation ma-trix in Figure 1). Rather than single frames, diagonal struc-tures activate whole frame sequences in their original or-der. This preserves the source’s temporal characteristicsin the resulting mosaic. Furthermore, the extended set ofupdate rules also limits the number of simultaneous acti-vations, making the learned activation matrix sparse andreducing the problem of too many source sounds being au-dible simultaneously. This way, we trade some approxima-tion quality for a better preservation of the source’s timbre.The idea of activating sequences of frames is inspiredby methods likenon-negative matrix factor deconvolution(NMFD) and related formulations [20,21], where templatesequences of frames from a dictionary are activated by sin-gle activation values. However, our approach is conceptu-ally different. Instead of changing the NMF problem for-mulation, our approach stays in the standard NMF setting,supporting the activation of whole frame sequences di-rectly in the activation matrix with additional update rules.Besides being computationally very efﬁcient and easy toimplement, this also has the advantage that we do not needto choose a maximal length of the sequences as in NMFD.Similarly, the sparseness constraint imposed by our proce-dure is not enforced by penalty terms in the problem for-mulation (as for example in [8, 10, 12, 23]), but also byadditional update rules.The remainder of this paper is structured as follows. InSection 2 we introduce the basic concept of using NMF-inspired update rules for the task of audio mosaicing. InSection 3 we present the extended set of update rules thatsupports the development of sparse diagonal structures in alearned activation matrix. The effects of these update ruleson the audio mosaics are discussed and demonstrated inSection 4.\n0 2 6 10 Time target [sec] 4 8 0 2 6 10 Time target [sec] 4 8 35 40 50 60 Time source [sec] 45 55 2 6 10 0 Time target [sec] 4 8 500 1000 1500 Frequency [Hz] 0 2000 500 1000 1500 Frequency [Hz] 0 2000 \n500 1000 1500 Frequency [Hz] 0 2000 \n35 Time source [sec] 40 45 50 55 60 (a) (b) \n(c) (d) \nFigure 2. Basic NMF-inspired audio mosaicing.(a):Mag-nitude spectrogram of “Let it be”V(target).(b):Mag-nitude spectrogram of a recording of beesW(source).(c):Activation matrixH.(d):The productWH(mosaic).2. BASIC NMF-INSPIRED AUDIO MOSAICINGNon-negative matrix factorization (NMF) has been ap-plied very successfully in a large variety of music pro-cessing tasks and beyond. Given a non-negative matrixV2RN⇥M\u00000, the goal of NMF is to decompose this ma-trix into two factorsW2RN⇥K\u00000andH2RK⇥M\u00000, whereN,M,K2N. The distance between the productWHandthe matrixVis minimized with respect to some distancemeasure, for example the Kullback-Leibler divergence(V||WH)=XnmVnmlogVnm(WH)nm\u0000Vnm+(WH)nm.(1)In the context of music processing, the matrixVis usuallya magnitude spectrogram of a music recording, the matrixWis interpreted as a set of spectral templates, and the ma-trixHconstitutes an activation matrix. Non-zero valuesin a row ofHactivate the associated template inWat therespective time instance. The two factorsWandHareusually learned by iteratively applying multiplicative up-date rules to two suitably initialized matrices [14].Fixing the template matrixWto be the magnitude spec-trogram of the source recording, the basic idea of our pro-posed audio mosaicing approach is to learn only the acti-vation matrixH. More precisely, we proceed as follows.Given the target recordingxtarand the source record-ingxsrc, we ﬁrst compute the complex valued spectro-gramsXtarandXsrcby applying the short-time Fouriertransform (STFT) to both recordings. Afterwards, wesetV:=|Xtar|,W:=|Xsrc|, and randomly initializeH(1)2(0,1]K⇥M. Fixing a number of iterationsL, wethen iteratively updateHwithH(`+1)km=H(`)kmPnWnkVnm/(WH(`))nmPnWnk,(2)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 351fork2[1 :K],m2[1 :M], and the iteration index`2[1 :L\u00001]. Finally, we setH:=H(L). The learnedactivation matrixHis then multiplied with the complexvaluedXsrc, yielding the complex valued spectrogram ofthe audio mosaicXmos:=XsrcH. To compute the audiomosaicxmos, we apply an “inverse” STFT to the spectro-gramXmoswhich also adjusts the phases such that arti-facts from phase discontinuities are reduced [9].Figure 2 shows this basic procedure applied to our run-ning example. In Figure 2a we see an excerpt of the mag-nitude spectrogram of the song “Let it be”. Our goal isto create an audio mosaic of this song, using the record-ing of buzzing bees, which can be seen in Figure 2b. Toincrease the range of different pitches occurring in oursource, we used a pitch-shifting algorithm [6] to createdifferently pitched versions of the bee recording and con-catenated them. Figure 2c shows an excerpt of the activa-tion matrixH, derived by applying the basic procedure de-scribed above. A ﬁrst observation aboutHis the predom-inance of horizontal activation structures. These patternscorrespond to single spectral frames in the source whichare activated repeatedly to mimic the stable spectral struc-tures in the target. Although the resulting mosaic, shown inFigure 2d, closely resembles these spectral structures, onecan hear a “stuttering” effect when listening to the recon-structed audio recording. This stuttering originates fromthe same frame of the source being repeated over and overagain. In Section 3.1, we aim to prevent the learning pro-cess from activating the same frame in fast repetition withan additional update rule.A second observation is that the matrixHusually ac-tivates many source frames simultaneously. The learningprocess can thus closely approximate the spectral shapesof the target frames. However, in the context of audio mo-saicing, this has several drawbacks. SinceHis multipliedwith the complex spectrogramXsrc, phase cancellation ar-tifacts may arise when superimposing many complex spec-tral frames. This way, especially low pitched sounds tendto cancel each other out and are not audible in the ﬁnalaudio mosaic. Furthermore, since a sound’s timbre is alsoclosely related to the energy distribution in its frequencyspectrum, adapting the spectral shapes may change thetimbre of the source. An update rule which sets a limiton the maximal number of simultaneous activations is pre-sented in Section 3.2.A third problem connected with the activation matrixshown in Figure 2c is the loss of temporal characteristicsof the source. The typical “buzzing sound” of the bees,which results from pitch modulations (see Figure 2b), islost in the mosaic (see Figure 2d). This is the case since thespectral frames of the source are activated independentlyof their order in the source spectrogram. To preserve sometemporal characteristics, the update rule presented in Sec-tion 3.3 supports the development of diagonal structures inthe activation matrix.\nTime target Time source (a) \nTime target Time source (b) \nTime target Time source (c) \nTime target Time source (d) \nFigure 3.(a):Activation matrixH(`).(b):Repetitionrestricted activation matrixR(`). The horizontal neighbor-hood is indicated in red.(c):Polyphony restricted acti-vation matrixP(`). For each column, the highest value isindicated in red.(d):Continuity enhancing activation ma-trixC(`). The diagonal kernel is indicated in red.3. LEARNING SPARSE DIAGONALACTIVATIONSThe core idea to overcome the issues of the basic NMF-inspired audio mosaicing procedure is to impose speciﬁcconstraints on the learned activation matrices by adaptingthe iterative update process. As discussed in the previoussection, we identiﬁed three main problems of the mosaicsgenerated by the basic procedure, all related to proper-ties of the the derived activation matrices. First, horizon-tal activation patterns cause stuttering artifacts in the mo-saics. Second, too many simultaneous activations lead tophase cancellations and overﬁtting of the spectral shapes.Third, the source’s temporal characteristics are destroyedby activating source frames independently of each other.We therefore introduce additional update rules to approachthese issues, see also Figure 3.3.1 Avoiding repeated activationsTo avoid activating the same spectral frame of the sourcein subsequent time-instances, the idea is to only keep thehighest activations in a horizontal neighborhood of the ma-trixH, suppressing the remaining values. However, wedo not want to interfere too much with the actual learningprocess in the ﬁrst few update iterations. The amount ofsuppression applied to the smaller values is therefore de-pendent on the iteration index`. Given the activation ma-trixH(`), the size of a horizontal neighborhoodr, and thenumber of iterationsL, we compute arepetition restrictedactivation matrixR(`)byR(`)km=(H(`)kmifH(`)km=µr,(`)kmH(`)km(1\u0000(`+1)L)otherwise,(3)352 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015with`2[1 :L\u00001]andµr,(`)kmbeing the maximum valueofH(`)in a horizontal neighborhoodµr,(`)km= max(H(`)k(m\u0000r),...,H(`)k(m+r)).(4)Note that the suppression of smaller values becomes strictin the last update iteration for`=L\u00001. Intuitively, theparameterrdeﬁnes the minimal horizontal distance (andtherefore the minimal time interval) between two activa-tions of the same source frame. Figure 3b shows the rep-etition restricted activation matrixR(`)derived from thetoy example activation matrix shown in Figure 3a, usingr=2,`=8, andL= 10. As opposed toH(`), there areno two dominant values next to each other inR(`).3.2 Restricting the number of simultaneousactivationsNext, we address the problem of too many simultaneousactivations. Setting a limitp2Non the number of activa-tions in one column of the activation matrix, we computeapolyphony restrictedactivation matrixP(`)in a similarmanner asR(`)byP(`)km=(R(`)kmifk2⌦p,(`)mR(`)km(1\u0000(`+1)L)otherwise,(5)where⌦p,(`)mcontains the indices of thephighest valuesin themthcolumn ofR(`). The parameterpcan be di-rectly interpreted as the desired degree of polyphony in themosaic. For example, settingp=1results in a mosaicwhere the source sounds are not heavily superimposed butmainly concatenated to mimic the most dominant featuresof the target. In Figure 3c, we see the polyphony restrictedactivation matrixP(`)derived fromR(`), usingp=1. Onecan see that inP(`)there is (at most) one single dominantvalue left in every column.3.3 Supporting time-continuous activationsTo support the development of diagonal structures that ac-tivate successive frames of the source, we now computeacontinuity enhancingactivation matrixC(`). The ideahere is to convolve the matrixPwith a diagonal kernel.Choosingc2N, which deﬁnes the length of the kernel,we computeC(`)km=cXi=\u0000cP(`)(k+i)(m+i).(6)Intuitively, the length2c+1of the kernel deﬁnes the mini-mal number of source frames that we would like to succes-sively activate. Figure 3d shows the matrixC(`)for our toyexample, computed withc=2. Note that inC(`)the num-ber of simultaneous dominant activations may locally ex-ceed the limit which was imposed in the computation of thepolyphony restricted activation matrixP(`). In practice,this is however not a problem and even desirable since thisway, the diagonal structures can overlap with each otherto some degree. Therefore, the corresponding audio seg-ments of the source are overlapped in the ﬁnal mosaic aswell, leading to smooth transitions between them.\n16.5 17 17.5 18 18.5 58.5 59 59.5 60 \n16.5 16.5 17 17.5 18 18.5 58.5 59 59.5 60 \n16.5 58 16.5 17 17.5 18 18.5 58.5 59 59.5 60 \n16.5 58 \n16.5 17 17.5 18 18.5 58.5 59 59.5 60 \n16.5 58 16.5 17 17.5 18 18.5 58.5 59 59.5 60 \n16.5 58 Time target [sec] Time target [sec] Time source [sec] Time source [sec] (a) (b) \n(c) (d) 0 0.2 0.4 0.6 0.8 1 \n0 0.01 0.02 0.03 \n0 0.2 0.4 0.6 0.8 1 \n0 0.02 0.04 0.06 0.08 0.1 Time target [sec] Time target [sec] Time source [sec] Time source [sec] \nFigure 4. The activation matrixHfor the mosaic of “Let itbee” with a recording of bees in different states.(a):H(1).(b):H(3).(c):H(6).(d):H(10). The repetition restrictingneighborhood is indicated in red.3.4 Adapting the activations to ﬁt the targetFinally, we perform the standard NMF update step to letthe mosaic adapt to the target again. Similarly to Equa-tion (2), we compute the activation matrix for the next it-eration byH(`+1)km=C(`)kmPnWnkVnm/(WC(`))nmPnWnk.(7)In summary, a single update step of the activation matrixHis computed by applying Equations (3), (5), (6), and (7)sequentially.Note that in one update iteration, the three intermediateupdate rules (3), (5), and (6) are insensitive to the targetand therefore may increase the distance measure of Equa-tion (1). However, as already discussed in Section 1, we arenot interested in minimizing this measure, but trade someapproximation accuracy for a better preservation of thesource’s timbre. In practice, our procedure usually yieldsan activation matrix that, when multiplied with the sourcespectrogram, approximates the target spectrogram to a suf-ﬁcient degree, while preserving the source’s timbre in themosaic much better than the basic procedure described inSection 2.Figure 4 shows an excerpt of the activation matrixHof our running example “Let it be” for several iteration in-dices`.Here, we set the repetition restriction parameter tor=3, the limit of simultaneous activations top= 10, thekernel parameter toc=3(resulting in a diagonal kernel oflength7), and the number of update iterations toL= 10.Figure 4a shows the random initialization of the activationmatrixH(1). After two iterations, one can already noticediagonal patterns inH(3), see Figure 4b. Figure 4c showsthe activations after another three update iterations. Thediagonal patterns inH(6)are even more prominent andone can observe that separate diagonal structures start toProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 3532 2.5 3 3.5 Time target [sec] 0 200 400 600 Frequency [Hz] \n2 2.5 3 3.5 Time target [sec] \n2 2.5 3 3.5 Time target [sec] 2 2.5 3 3.5 Time target [sec] 53.5 54 54.5 55 Time source [sec] 0 200 400 600 Frequency [Hz] \n0 200 400 600 Frequency [Hz] 0 200 400 600 Frequency [Hz] 53.5 54 54.5 55 Time source [sec] \n53.5 54 54.5 55 Time source [sec] (a) (b) (c) (d) \n(e) (f) 2 2.5 3 3.5 Time target [sec] \nFigure 5. The effect of diagonal activation patterns.(a):Spectrogram of the target recording “Let it be”.(b):Spec-trogram of the source recording of buzzing bees.(c):Ac-tivation matrixHderived with the basic approach.(d):Activation matrixHderived with the extended set of up-date rules.(e):Spectrogram of the audio mosaic resultingfrom the basic approach.(f):Spectrogram of the audiomosaic resulting from the extended procedure.emerge, leaving regions of lower values inbetween them.In Figure 4d, the activation matrixH(10)is shown. In thisﬁnal activation matrix, four clear diagonal structures haveemerged. The remaining activations are outside the visiblerange. Looking at the two upper diagonals, one can seethat although they seem to be rather close together, theyobey the repetition restricting horizontal neighborhood in-dicated in red. Furthermore, it is noteworthy that the lengthof the diagonals greatly exceeds the length of the diago-nal kernel. For example, while we used a diagonal kernelof length7, the lowest diagonal has a length of25non-zero activations, corresponding to an audio segment in thesource of roughly one second. This means that the proce-dure uses a whole one-second patch of source audio mate-rial to recreate the target between second17and18.4. EXPERIMENTS AND EXAMPLESIn this section, we both visually and acoustically demon-strate the effectiveness of our proposed method. As dis-cussed in previous sections, the main drawbacks of thebasic audio mosaicing approach described in Section 2were both the loss of temporal characteristics and spectralshapes of the source sounds in the resulting audio mosaics.The idea was to approach these problems by supporting thedevelopment of sparse diagonal structures in the activationmatrix with an extended set of update rules. In the follow-\n0 400 800 1200 1600 2000 Frequency [Hz] Magnitude Magnitude Magnitude (a) (b) (c) Figure 6. Comparison of spectral shapes.(a):A singlespectral frame of the target recording (“Let it be”). Har-monics are indicated by red circles.(b):The spectralframe of the mosaic computed with the basic procedure atthe same temporal position. Harmonics which are presentin both the original frame as well as in the mosaic are indi-cated by red circles.(c):The spectral frame of the mosaiccomputed by using the extended set of update rules.ing, we exemplify how these structures can preserve thesource’s desired characteristics in the audio mosaic.4.1 Preserving temporal characteristics of the sourceIn Figure 5, we once again revert to our running example.Here, spectrogram excerpts of the target recording “Let itbe” as well as the source recording of buzzing bees areshown in Figures 5a and 5b, respectively. The spectro-gram of the target recording exhibits sounds with very sta-ble pitches, resulting from the solo piano at the beginningof the song. In contrast, the buzzing of the bees leads torather strong amplitude modulations that are characteristicfor the sound. Figure 5c shows an excerpt of the activationmatrixHas derived by the basic NMF-inspired audio mo-saicing procedure. In this excerpt ofH, only two differentspectral frames of the source are activated repeatedly bythe procedure to mimic the stable pitch of the piano sound.The resulting spectrogram of the audio mosaic, shown inFigure 5e, approximates the target’s spectrogram quite pre-cisely. However, the characteristic pitch modulations of thebuzzing bee sound are lost almost completely. Looking atFigure 5d, one can see the activation matrixHderived byour proposed procedure based on the extended set of up-date rules. The diagonal patterns shown activate segmentsof the source that have a duration of roughly half a sec-ond. As can be seen by comparing the regions marked inred in the source (Figure 5b) and the mosaic spectrogram(Figure 5f), the temporal structures of these segments arepreserved in the mosaic. While the mosaic computed withthe extended set of update rules exhibits a lot of pitch mod-ulations, which reﬂect the preserved timbre of the buzzingbee sound, the tonal content as well as rhythmic structuresof the target are still maintained. For example, the twostrong partials of the target recording at around270Hz and354 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Name of the target Description of the targetName of the source Description of the sourceLetItBe An excerpt of the song “Let it be” by the Beatles (piano & singing).Bees Recording of a buzzing swarm of bees.GuteNacht An excerpt of “Gute Nacht” by Franz Schubert which is part of theromanticWinterreisesong cycle, taken from [15].Wind Recording of howling wind.FunkJazz An excerpt from a jazz piece performed by the band “Music Delta”(saxophone, synthesizer, bass, and drums), taken from [2].Whales Recording of whale songs and whale sounds.Stepdad Excerpt from the song “My leather, my fur, my nails” by the popband Stepdad (synthesizers, drums, and singing).Chainsaw Recording of a chainsaw’s sawing and engine sounds.Freisch¨utz Excerpt from the opera “Der Freisch¨utz” by Carl Maria von Weber(full orchestra, applause at the end).AirRaid Recording of an air raid siren.Vermont An excerpt of the song “Vermont” by the band “The Districts”(singing, guitar, bass, and drums), taken from [2].RaceCars Recording of engine sounds of starting race cars.Table 1. List of target and source recordings used in our experiments.300Hz in Figure 5a are also visible in the audio mosaic inFigure 5f, only this time pitch modulated. Similarly, theonset in the target at second2.6is present in the mosaic aswell.4.2 Preserving spectral shapes of the sourceIn Figure 6, we investigate typical spectral shapes of thetarget as well as the mosaic for our running example. Fig-ure 6a shows the spectral frame of the target’s spectro-gram at second4.6as a frequency-magnitude plot. Onecan see the harmonic structure with several clear partialsin this frame, resulting from the piano sound in the tar-get. The corresponding spectral frame of the mosaic com-puted by the basic procedure shown in Figure 6b showsa very similar spectral structure. Most of the harmonicsvisible in the target are also present in this frame (indi-cated by the red circles) and even the relations betweenpeak heights are often preserved. In contrast, the spectralframe of the mosaic computed with the extended set of up-date rules only roughly corresponds to the spectral shapeof the target frame, see Figure 6c. However, some of thedominant peaks in the target frame are still present in themosaic, leading to a sound that captures only the dominanttonal characteristics of the target. The noisy timbre of thebuzzing bees, visible by the increased noise level in theframe, is therefore preserved.4.3 Audio examplesIn order to also give an auditory demonstration of ourmethod, we set up an accompanying website for this pa-per at [7]. On this website, one ﬁnds the target recordingsas well as source recordings listed in Table 1. To ensurethat each source recording offers an adequate pitch range,we computed several pitch-shifted versions of it (using apitch-shifting algorithm from [6]) and concatenated them.For each pair of target and source, we then generated anaudio mosaic using both the basic mosaicing procedure de-scribed in Section 2 as well as the procedure based on theextended set of update rules proposed in Section 3. Forthese experiments, we used music recordings sampled at22050Hz, an STFT frame length of2048samples and ahop size of1024samples to compute the spectrograms.In order to derive the activation matrices for both proce-dures, we performedL= 20iterations of the respectiveupdate steps. For the extended set of update rules, we setthe repetition restriction parameter tor=3, the limit ofsimultaneous activations top= 10, and the kernel param-eter toc=3. To reconstruct time-domain signals from thederived complex valued mosaic spectrograms, we ﬁnallyperformed20iterations of the STFT inversion procedureproposed in [9].5. CONCLUSION AND FUTURE WORKIn this work we presented a novel approach for automati-cally generating an audio mosaic of a target recording us-ing the sounds from a source recording. The core ideaof this NMF-inspired procedure was to learn an activa-tion matrix that, when multiplied with the spectrogram ofthe source recording, yields the spectrogram of the mo-saic recording. As our main technical contribution, weproposed an extended set of update rules that supportsthe development of sparse diagonal structures in the ac-tivation matrix during the learning process. Our experi-ments showed that these diagonal activation structures cor-respond to the activation of whole sequences of spectralframes and help to preserve timbral characteristics of thesource in the mosaic.In future work we want to investigate if our proposedprocedure can also be applied in scenarios beyond audiomosaicing. One possibility is to examine whether support-ing the development of diagonal structures in the activa-tion matrix can also be beneﬁcial when learning not onlythe activation matrix, but also the template matrix. Such anNMF procedure could be applied for learning and identify-ing repeating patterns in feature sequences, similar to [24]who used techniques based on NMFD for this task. In thiscontext, we hope that our approach may yield a simpler im-plementation as well as more ﬂexibility since the maximallength of sequences does not need to be ﬁxed.Acknowledgments:This work has been supported by the German Re-search Foundation (DFG MU 2686/6-1). The Interna-tional Audio Laboratories Erlangen are a joint institutionof the Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg(FAU) and Fraunhofer Institut f¨ur Integrierte Schaltungen.Furthermore, we would like to thank Colin Raffel and theother organizers of theHAMRHack Day at ISMIR 2014,where the core ideas of the presented work were born.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 3556. REFERENCES[1]G. Bernardes.Composing Music by Selection:Content-Based Algorithmic-Assisted Audio Composi-tion. PhD thesis, Faculty of Engineering, University ofPorto, 2014.[2]R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,C. Cannam, and J. P. Bello. Medleydb: A multitrackdataset for annotation-intensive MIR research. InProc.of the 15th International Society for Music InformationRetrieval Conference ISMIR, pages 155–160, Taipei,Taiwan, October 2014.[3]G. Coleman, E. Maestre, and J. Bonada. Augment-ing sound mosaicing with descriptor-driven transfor-mation. InProc. of the International Conference onDigital Audio Effects (DAFx), Graz, Austria, 2010.[4]J. M. Comajuncosas, A. Barrachina, J. O’Connell, andE. Guaus. Nuvolet: 3D gesture-driven collaborative au-dio mosaicing. InProc. of the International Conferenceon New Interfaces for Musical Expression, pages 252–255, Oslo, Norway, 2011.[5]E. Costello, V . Lazzarini, and J. Timoney. A stream-ing audio mosaicing vocoder implementation. InProc.of the 16th International Conference on Digital AudioEffects (DAFx), Maynooth, Ireland, September 2013.[6]J. Driedger and M. M¨uller. TSM Toolbox: MAT-LAB implementations of time-scale modiﬁcation al-gorithms. InProc. of the International Conference onDigital Audio Effects (DAFx), pages 249–256, Erlan-gen, Germany, 2014.[7]J. Driedger, T. Pr¨atzlich, and M. M¨uller. Ac-companying website: Let it bee – towardsNMF-inspired audio mosaicing.http://www.audiolabs-erlangen.de/resources/MIR/2015-ISMIR-LetItBee/.[8]J. Eggert and E. K¨orner. Sparse coding and NMF. InProc. of the IEEE International Joint Conference onNeural Networks, volume 4, pages 2529–2533, July2004.[9]D. W. Grifﬁn and J. S. Lim. Signal estimation frommodiﬁed short-time Fourier transform.IEEE Trans-actions on Acoustics, Speech and Signal Processing,32(2):236–243, 1984.[10]P. O. Hoyer. Non-negative matrix factorization withsparseness constraints.Journal of Machine LearningResearch, 5:1457–1469, 2004.[11]J. Janer and M. de Boer. Extending voice-driven syn-thesis to audio mosaicing. In5th Sound and MusicComputing Conference, Berlin, Germany, July 2008.[12]J. Kim and H. Park. Toward faster nonnegative matrixfactorization: A new algorithm and comparisons. InProc. of the IEEE International Conference on DataMining (ICDM), pages 353–362, Pisa, Italy, 2008.[13]R. Kobayashi. Sound clustering synthesis using spec-tral data. InProc. of the International Computer MusicConference (ICMC), Singapore, 2003.[14]D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. InProc. of the Neural In-formation Processing Systems (NIPS), pages 556–562,Denver, USA, 2000.[15]M. M¨uller, V . Konz, W. Bogler, and V . Ariﬁ-M¨uller.Saarland music data (SMD). InProc. of the Interna-tional Society for Music Information Retrieval Confer-ence (ISMIR): Late Breaking session, 2011.[16]J. Oswald. Plexure. CD, 1993.http://www.allmusic.com/album/plexure-mw0000621108.[17]N. Schnell, M. A. S. Cifuentes, and J.-P. Lambert.First steps in relaxed real-time typo-morphological au-dio analysis/synthesis. InSound and Music Computing,Barcelona, Spain, 2010.[18]D. Schwarz. A system for data-driven concatenativesound synthesis. InProc. of the International Confer-ence on Digital Audio Effects (DAFx), Verona, Italy,July 2000.[19]D. Schwarz. Concatenative sound synthesis: The earlyyears.Journal of New Music Reaserch, 35(1), March2006.[20]P. Smaragdis. Non-negative matrix factor deconvolu-tion; extraction of multiple sound sources from mono-phonic inputs. InIndependent Component Analysisand Blind Signal Separation, volume 3195 ofLectureNotes in Computer Science, pages 494–499. SpringerBerlin Heidelberg, 2004.[21]P. Smaragdis, B. Raj, and M. Shashanka. Sparse andshift-invariant feature extraction from non-negativedata. InProc. of the IEEE International Conferenceon Acoustics, Speech, and Signal Processing ICASSP,pages 2069–2072, Las Vegas, Nevada, USA, 2008.[22]P. A. Tremblay and D. Schwarz. Surﬁng the waves :Live audio mosaicing of an electric bass performanceas a corpus browsing interface. InProc. of the Interna-tional Conference on New Interfaces for Musical Ex-pression, pages 447–450, Sydney, Australia, Septem-ber 2010.[23]T. Virtanen. Monaural sound source separation by non-negative matrix factorization with temporal continuityand sparseness criteria.IEEE Transactions on Audio,Speech and Language Processing, 15(3):1066–1074,2007.[24]R. J. Weiss and J. P. Bello. Unsupervised discovery oftemporal structure in music.IEEE Journal of SelectedTopics in Signal Processing, 5:1240–1251, 2011.356 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Raga Verification in Carnatic Music Using Longest Common Segment Set.",
        "author": [
            "Shrey Dutta",
            "Krishnaraj Sekhar PV",
            "Hema A. Murthy"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417751",
        "url": "https://doi.org/10.5281/zenodo.1417751",
        "ee": "https://zenodo.org/records/1417751/files/DuttaPM15.pdf",
        "abstract": "There are at least 100 r¯agas that are regularly performed in Carnatic music concerts. The audience determines the identity of r¯agas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A r¯aga verification framework is therefore suggested. The r¯aga verification system assumes that a specific r¯aga is claimed based on similarity of movements and motivic pat- terns. The system then checks whether this claimed r¯aga is correct. For every r¯aga, a set of cohorts are chosen. A r¯aga and its cohorts are represented using pallavi lines of com- positions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a r¯aga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 r¯agas from Charsur Foundation 1 is used for analysis. An equal error rate (EER) of 12% is obtained. 1",
        "zenodo_id": 1417751,
        "dblp_key": "conf/ismir/DuttaPM15",
        "keywords": [
            "Carnatic music concerts",
            "audience determines r¯agas",
            "listener imitation",
            "r¯aga verification framework",
            "specific r¯aga claimed",
            "motivic patterns",
            "pallavi lines",
            "Longest Common Segment Set (LCSS)",
            "dataset of 30 r¯agas",
            "equal error rate (EER)"
        ],
        "content": "RAGA VERIFICATION IN CARNATIC MUSIC USING LONGESTCOMMON SEGMENT SETShrey DuttaDept. of Computer Sci. & Engg.Indian Institute of TechnologyMadrasshrey@cse.iitm.ac.inKrishnaraj Sekhar PVDept. of Computer Sci. & Engg.Indian Institute of TechnologyMadraspvkrajpv@gmail.comHema A. MurthyDept. of Computer Sci. & Engg.Indian Institute of TechnologyMadrashema@cse.iitm.ac.inABSTRACTThere are at least 100r¯agasthat are regularly performedin Carnatic music concerts. The audience determines theidentity ofr¯agaswithin a few seconds of listening to anitem. Most of the audience consists of people who are onlyavid listeners and not performers.In this paper, an attempt is made to mimic the listener.Ar¯agaveriﬁcation framework is therefore suggested. Ther¯agaveriﬁcation system assumes that a speciﬁcr¯agaisclaimed based on similarity of movements and motivic pat-terns. The system then checks whether this claimedr¯agaiscorrect. For everyr¯aga, a set of cohorts are chosen. Ar¯agaand its cohorts are represented using pallavi lines of com-positions. A novel approach for matching, called LongestCommon Segment Set (LCSS), is introduced. The LCSSscores for ar¯agaare then normalized with respect to itscohorts in two different ways. The resulting systems anda baseline system are compared for two partitionings of adataset. A dataset of 30r¯agasfrom Charsur Foundation1is used for analysis. An equal error rate (EER) of 12% isobtained.1IntroductionR¯agaidentiﬁcation by machine is a difﬁcult task in Car-natic music. This is primarily because ar¯agais not deﬁnedjust by the solfege but bysvaras(ornamented notes) [13].The melodic histograms obtained for the Carnatic musicare more or less continuous owing to thegamak¯a2ladensvaras of ther¯aga[23]. Although the svaras in Carnaticmusic are not quantiﬁable, for notational purposes an oc-tave is divided into 12 semitones: S, R1, R2(G1), R3(G2),G3, M1, M2, P, D1, D2(N1), D3(N2) and N3. Eachr¯agaischaracterised by atleast 5 svaras.¯Arohanaandavarohanacorrespond to an ordering ofsvarasin the ascent and de-1http://www.charsurartsfoundation.org2Gamak¯ais a meandering of asvaraencompassing other permissiblefrequencies around it.c\u0000Shrey Dutta, Krishnaraj Sekhar PV , Hema A. Murthy.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Shrey Dutta, Krishnaraj Sekhar PV ,Hema A. Murthy. “Raga Veriﬁcation in Carnatic Music Using LongestCommon Segment Set”, 16th International Society for Music Informa-tion Retrieval Conference, 2015.scent of ther¯aga, respectively. Ragas with linear orderingof svaras are referred to as linear ragas such asMohonamr¯aga(S R2 G3 P D2 S). Similarly, non linear ragas havenon linear ordering such asAnanda Bhairaviraga (S G2R2 G2 M1 P D2 P S). A further complication arises owingto the fact that although thesvarasin differentr¯agasmaybe identical, the ordering can be different. Even if the or-dering is the same, in oner¯agathe approach to thesvaracan be different, for example,todianddhanyasi.There is no parallel in Western classical music tor¯agaveriﬁcation. The closest that one can associate with, iscover song detection [6, 16, 22], where the objective is todetermine the same song rendered by different musicians.Whereas, two different renditions of the samer¯agamaynot contain identical renditions of the motifs.Several attempts have been made to identifyr¯agas[2–4,7,8,12,14,26]. Most of these efforts have used small reper-toires or have focused onr¯agasfor which ordering is notimportant. In [26], the audio is transcribed to a sequence ofnotes and string matching techniques are used to performr¯agaidentiﬁcation. In [2], pitch-class and pitch-dyads dis-tributions are used for identifyingr¯agas. Bigrams on pitchare obtained using a twelve semitone scale. In [18], the au-thors assume that an automatic note transcription systemfor the audio is available. The transcribed notes are thensubjected to HMM basedr¯agaanalysis. In [12,25], a tem-plate based on the¯arohanaandavarohanais used to deter-mine the identity of ther¯aga. The frequency of thesvarasin Carnatic music is seldom ﬁxed. Further, as indicatedin [27] and [28], the improvisations in extempore enuncia-tion ofr¯agascan vary across musicians and schools. Thisbehaviour is accounted for in [10, 11, 14] by decreasingthe binwidth for computing melodic histograms. In [14],steady note transcription along with n-gram models is usedto performr¯agaidentiﬁcation. In [3] chroma features areused in an HMM framework to perform scale indepen-dentr¯agaidentiﬁcation, while in [4] hierarchical randomforest classiﬁer is used to matchsvarahistograms. Thesvaras are obtained using the Western transcription sys-tem. These experiments are performed on 4/8 differentr¯agasof Hindustani music. In [7], an attempt is made toperformr¯agaidentiﬁcation using semi-continuous Gaus-sian mixtures models. This will work only for linearr¯agas.Recent research indicates that ar¯agais characterised bestby a time-frequency trajectory rather than a sequence of605V ocalInstrumentsTotalMaleFemaleViolinVeenaSaxophoneFluteNumber of Ragas2527832230 (distinct)Number of Artists53378313105Number of Recordings1349714423254Total Duration of Recordings30 h22 h3h31 m10 m58 m57 hNumber of Pallavi Lines655475692010151244Average Duration of Pallavi Lines11 s8s10 s6s6s8s8 s (avg.)Total Duration of Pallavi Lines2h1h11 m2m55 s2m3hTable 1. Details of the database used. Durations are given in approximate hours (h), minutes (m) or seconds (s).quantised pitches [5, 8, 9, 19, 20, 24]. In [19, 20], thesamaof the tala (emphasised by thebolof tabla) is used to seg-ment a piece. The repeating pattern in a bandish in Hin-dustani Khyal music is located using the sama informa-tion. In [8, 19], motif identiﬁcation is performed for Car-natic music. Motifs for a set of ﬁver¯agasare deﬁned andmarked carefully by a musician. Motif identiﬁcation is per-formed using hidden Markov models (HMMs) trained foreach motif. Similar to [20], motif spotting in an¯al¯apanain Carnatic music is performed in [9]. In [24], a number ofdifferent similarity measures for matching melodic motifsof Indian music was attempted. It was shown that the in-tra pattern melodic motif has higher variation for Carnaticmusic in comparison with that of Hindustani music. It wasalso shown that the similarity obtained is very sensitive tothe measure used. All these efforts are ultimately aimedat obtaining typical signatures ofr¯agas. It is shown in [9]that there can be many signatures for a givenr¯aga. To alle-viate this problem in [5], an attempt was made to obtain asmany signatures for ar¯agaby comparing lines of compo-sitions. Here again, it was observed that the typical motifdetection was very sensitive to the distance measure cho-sen. Using typical motifs/signatures forr¯agaidentiﬁcationis not scalable, when the number ofr¯agasunder consider-ation increases.In this paper, this problem is addressed in a differentway. The objective is to mimic a listener in a Carnatic mu-sic concert. There are at least 100r¯agasthat are activelyperformed today. Most listeners identifyr¯agasby refer-ring to the compositions with similar motivic patterns thatthey might have heard before. Inr¯agaveriﬁcation, ar¯aga’sname (claim) and an audio clip is supplied. The machinehas to primarily verify whether the clip belongs to a givenr¯agaor not.This task therefore requires the deﬁnition of cohorts forar¯aga. Cohorts of a givenr¯agaare the ragas which havesimilar movements while at the same time have subtle dif-ferences, for example,darbarandn¯ayaki. Indarbarraga,G2 is repeated twice inavarohana. The ﬁrst is more or lessﬂat and short, while the second repetition is inﬂected. TheG2 inn¯ayakiis characterised by a very typicalgamak¯a.In order to verify whether a given audio clip belongs to aclaimedr¯aga, the similarity is measured with respect to theclaimedr¯agaand compared with its cohorts using a novelalgorithm calledlongest common segment set(LCSS). LCSSscores are then normalized usingZandTnorms [1, 17].The rest of the paper is organised as follows. Section 2describes the dataset used in the study. Section 3 describesthe LCSS algorithm and its relevance forr¯agaveriﬁca-tion. As the task isr¯agaveriﬁcation, score normalisation iscrucial. Different score normalisation techniques are dis-cussed in Section 4. The experimental results are presentedin Section 5 and discussed in Section 6. The main conclu-sions drawn from the key results in this paper are discussedin Section 72Dataset usedTable 1 gives the details of the dataset used in this work.This dataset is obtained from the Charsur arts foundation3.The dataset consists of 254 vocal and instrument live record-ings spread across 30r¯agas, including both target ragasand their cohorts. For every newr¯agathat needs to be ver-iﬁed, templates for ther¯agaand its cohorts are required.2.1Extraction of pallavi linesA composition in Carnatic music is composed of threeparts, namely,pallavi,anupallaviandcaranam. It is be-lieved that the ﬁrst phrase of the ﬁrstpallaviline of a com-position contains the important movements in ar¯aga.Abasic sketch is initiated in thepallaviline, developed fur-ther in theanupallaviandcaranam[21] and therefore con-tains the gist of ther¯aga. The algorithm described in [21]is used for extractingpallavilines from compositions. De-tails of the extracted pallavi lines are given in Table 1. Ex-periments are performed on template and test recordings,selected from these pallavi lines, as discussed in greaterdetail in Section 5.2.2Selection of cohortsWherever possible 4-5r¯agasare chosen as cohorts ofeveryr¯aga. The cohorts of everyr¯agawere deﬁned by aprofessional musician. Professionals are very careful aboutthis as they need to ensure that during improvisation, theydo not accidentally sketch the cohort. Interestingly, as in-dicated by the musicians, cohorts need not be symmetric.Ar¯agaAcan be similar in movement to ar¯agaB, butr¯agaBneed not share the same commonality withr¯agaA. The identity ofr¯agaBmay depend on phrases similartor¯agaAwith some additional movement. For example,3http://www.charsurartsfoundation.org606 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015to identify ther¯agaIndolam, the phrase G2 M1 D1 N2 S isadequate, while Jayantashreer¯agarequires the phrase G2M1 D1 N2 S N2 D1 P M1 G2 S.3Longest common segment setInr¯agaveriﬁcation, matching needs to be performedbetween two audio clips. The number of similar portionscould be more than one and spread across the entire clip.Therefore, there is a need for a matching approach that canﬁnd these similar portions without issuing large penaltiesfor gaps in between them. In this section, a novel algorithmcalled Longest Common Segment Set is described whichattempts to do the same.LetX=hx1,···,xm;xi2R;i=1···mibe a se-quence ofmsymbols andY=hy1,···,xn;yj2R;j=1···nibe a sequence ofnsymbols wherexiandyjare thetonic normalized pitch values in cents [9]. The similaritybetween two pitch values,xiandyj, is deﬁned assim(xi,yj)=(1\u0000|xi\u0000yj|3(3st)3if|xi\u0000yj|<3st0otherwise(1)wherestrepresents a semitone in cents. Due to differentstyles of various musicians, an exact match between twopitch values contributing to the samesvaracannot be ex-pected. Hence, in this paper a leeway of 3 semitones isallowed between pitch values. Musically two pitch values,3 semitones apart, cannot be called similar but this issueis addressed by the cubic nature of the similarity function.The function reaches its half value when the difference intwo symbols is approximately half a semitone. Therefore,higher similarity scores are obtained when the correspond-ing pitch values are at most half a semitone apart.A common subsequenceZXYin sequencesXandYisdeﬁned asZXY=8>>>><>>>>:⌦(xi1,yj1),···,(xip,yjp)↵1i1<···<ipm1j1<···<jpnsimk=1,···,p(xik,yjk)\u0000⌧sim(2)where⌧simis a threshold which decides the membershipof the symbol pair(xik,yjk)in a subsequenceZXY. Thevalue of⌧simis decided empirically based on the domainof the problem as discussed in Section 5. An example com-mon subsequence is shown with red color in Figure 1.3.1Common segmentsContinuous symbol pairs in a common subsequence arereferred to as a segment. Two different types of segmentsare deﬁned, namely hard and soft segments.Hard segmentis a group of common subsequence sym-bols such that there are no gaps in between as shown ingreen color in Figure 1. Then a hard segment, starting with  \nSoft segment running score102030405060708090100110\nCommon subsequenceHard segmentsSoft segments450500550600650700750800Sequence 202004006008001,0001,200Sequence 1\nDDDD200300400200300400PitchPitchFigure 1. An example of a common segment set betweentwo sequences representing the real dataa symbol pair(xi,yj), must be of the formHlXiYj=8><>:h(xi,yj),(xi+1,yj+1),···,(xi+l,yj+l)i1i<i+1<···<i+lm1j<j+1<···<j+ln(3)wherel+1represents the length of the hard segment. Thescore of thekthhard segmentHlXikYjkis deﬁned ashc⇣HlXikYjk⌘=lXd=0sim(xik+d,yjk+d)(4)Soft segmentis a group of common subsequence sym-bols where gaps are permitted with a penalty. Therefore, asoft segment consists of one or more hard segments (shownwith blue color in Figure 1). The gaps between the hardsegments decides the penalty assigned. Thus, the score ofthekthsoft segmentSXikYjk, consisting ofrhard seg-ments, is deﬁned assc⇣SXikYjk⌘=rXs=1hc⇣HlXikYjk⌘\u0000\u0000⇢(5)where\u0000is the total number of gaps betweenrhard seg-ments and⇢is the penalty for each gap. The number ofhard segments to be included in a soft segment is decidedby the running score of the soft segment. The runningscore of the soft segment increases during the hard segmentand decreases during the gap due to penalties as shown ingray-scale in Figure 1. During a gap, if the running scoredecreases below a threshold⌧rc(or becomes almost whitein Figure 1) then that gap is ignored and all the hard seg-ments, encountered before it, are included into a soft seg-ment.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6073.2Common segment setAll segments together correspond to a segment set. Thescore of a segment set (ss) is deﬁned asscore(ssXY)=Ppk=1c⇣ZXikYjk⌘2min(m, n)2(6)wherepis the number of segments,crefers to the scorecomputed in either (4) or (5) andZrefers to a segment(hard or soft). This equation gives preference to longersegments. For example, in case 1, there are 10 segmentseach of length 2 and in case 2, there are 4 segments each oflength 5. In both the cases the total length of the segmentsis 20 but in (6), case 1 is scored as 0.1 and case 2 is scoredas 0.25 when the denominator is taken to be202. Longermatched segments could be considered as a phrase or anessential part of it. Whereas, shorter matched segmentscould generally mean noise. Therefore, there is a heavierpenalty for shorter segments.3.3Longest common segment setLongest common segment set (lcss) is a segment setwith maximum score value as deﬁned in (7).lcssXY=argmaxssXY(score(ssXY))(7)Therefore, lcss can be obtained by maximizing score in (6)using dynamic programming.3.4Dynamic Programming algorithm to ﬁnd longestcommon segment setThe algorithm for ﬁnding the optimum soft segment setis given in Algorithm 1. Optimum hard segment sets arefound similarly. In the algorithm, tablescandsare usedfor storing the running score and the score of the commonsegment sets, respectively. Tableais used for storing thepartial scores froms. Tabledis maintained for backtrack-ing the path of the LCSS. The arrows represent the subpathto take while backtracking (up, left or cross). Input se-quences to function LCSS are appended with symbols\u0000xand\u0000ysuch that their similarity with any symbol is 0. Thisis mainly required to compute the last row and column ofscore table. On similarity, line 8 updates the running scorewith a value based on the similarity, whereas line 9 updatesthe score using the previous diagonal entry. When symbolsare dissimilar a gap is found. Lines 12 and 19 are used topenalize the running score. If it is an end of the segmentthen line 14 and 21 updates score as per (6). Line 26 up-dates tableawith the score value of the current segmentset when the beginning of a new segment is encountered.When a gap is encountered line 28 updates it to\u00001.T oﬁnd the longest common segment set, backtracking is per-formed to obtain the path in tabledthat has the maximumscore as given by table s. The boundaries of soft segmentscan be found using the cost values while tracing the path.4Raga VeriﬁcationLet Tr¯aga=\u0000t1,t2,···,tNr¯aga represent a set of tem-plate recordings, where ‘r¯aga’ refers to the name of theAlgorithm 1Algorithm for Soft-Longest Common Seg-ment SetData:c- table of size(m+2)⇥(n+2)for storing running scores- table of size(m+ 2)⇥(n+ 2)for storing scored- table of size(m+ 2)⇥(n+ 2)for path trackinga- table of size(m+2)⇥(n+2)for storing partial scores.1:functionLCSS(hx1,···,xm,\u0000xi,hy1,···,yn,\u0000yi)2:Initialize1strow and column ofc,s,dandato03:p min(m, n)4:fori 1tom+1do5:forj 1ton+1do6:ifsim(xi,yj)>⌧simthen7:di, j “-”8:ci, j ci\u00001,j\u00001+⇣sim(xi,yj)\u0000⌧sim1\u0000⌧sim⌘9:si, j si\u00001,j\u0000110:else ifci\u00001,j<ci, j\u00001then11:di, j “\"”12:ci, j max(ci\u00001,j\u0000⇢,0)13:ifdi\u00001,j=“-”then14:si, j ai\u00001,j⇤p2+c2i\u00001,jp215:else16:si, j si\u00001,j17:else18:di, j “ ”19:ci, j max(ci, j\u00001\u0000⇢,0)20:ifdi, j\u00001=“-”then21:si, j ai, j\u00001⇤p2+c2i, j\u00001p222:else23:si, j si, j\u0000124:q max(ai\u00001,j\u00001,ai\u00001,j,ai, j\u00001)25:ifq=\u00001anddi, j=“-00then26:ai, j si\u00001,j\u0000127:else ifci, j<⌧rcthen28:ai, j \u0000129:else30:ai, j qr¯agaandNr¯agais the total number of templates for thatr¯aga. During testing, an input test recording, X, with aclaimis tested against all the template recordings of theclaimedr¯aga. The ﬁnal score is computed as given in (8).score(X,claim) = maxY2Tclaim(score(lcssXY))(8)The ﬁnal decision, of accepting or rejecting the claim, di-rectly based on this score could be erroneous. Score nor-malisation with cohorts is essential to make a decision, es-pecially when the difference between twor¯agasis subtle.4.1Score NormalizationLCSS scores corresponding to correct and incorrect claimsare referred as true and imposter scores, respectively. Ifthe imposter is a cohortr¯aga, then the imposter score isalso referred as cohort score. Various score normalizationtechniques are discussed in the literature for speech recog-608 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015nition, speaker/language veriﬁcation and spoken term de-tection [1, 17].Zero normalization(Z-norm) uses the mean and vari-ance estimate of cohort scores for scaling. The advantageofZ-norm is that the normalization parameters can be es-timated off-line. Template recordings of ar¯agaare testedagainst template recordings of its cohorts and the resultingscores are used to estimate ar¯agaspeciﬁc mean and vari-ance for the imposter distribution. The normalized scoresusingZ-norm can be calculated asscorenorm(X,claim)=score(X,claim)\u0000µclaimI\u0000claimI(9)whereµclaimIand\u0000claimIare the estimated imposter parame-ters for the claimedr¯aga.Test normalization(T-norm) is also based on a meanand variance estimation of cohort scores for scaling. Thenormalization parameters inT-norm are estimated onlineas compared to their ofﬂine estimation inZ-norm. Duringtesting, a test recording is tested against template record-ings of cohortr¯agasand the resulting scores are used toestimate mean and variance parameters. These parametersare then used to perform the normalization given by (9).The test recordings of ar¯agamay be scored differentlyagainst templates corresponding to the samer¯agaor im-posterr¯aga. This can cause overlap between the true andimposter score distributions.T-norm attempts to reducethis overlap. The templates that are stored and the audioclip that is used during test can be from different environ-ments.5Performance evaluationIn this section, we describe the results ofr¯agaveriﬁ-cation using LCSS algorithm in comparison with RoughLongest Common Subsequence (RLCS) algorithm [15] andDynamic Time Warping (DTW) algorithm using differentnormalizations.5.1Experimental conﬁgurationOnly 17r¯agasout of 30 were used forr¯agaveriﬁcationas only for 17r¯agassufﬁcient number of relevant cohortscould be obtained from the 30r¯agas. This is due to non-symmetric nature of the cohorts as discussed in Section 2.Forr¯agaveriﬁcation, 40% of the pallavi lines are used astemplates and remaining 60% are used for testing. Thispartitioning of dataset is done into two ways, referred asD1 and D2. In D1, the variations of a pallavi line mightfall into both templates and test though it is not necessary.Variations of a pallavi line are different from the pallaviline due to improvisations. In D2, these variations can ei-ther belong to template or they all belong to test but strictlynot present in both. The values of thresholds⌧simand⌧rcare empirically chosen as 0.45 and 0.5, respectively.Penalty,⇢, issued for gaps in segments is empirically cho-sen as 0.5.5.2ResultsTable 2 and Figure 2 show the comparison of LCSS withDTW and RLCS using different normalizations. Equal Er-AlgorithmDatasetNo NormZ-normT-NormDTWD127.7829.8817.45D240.8140.0335.96RLCSD124.4327.2214.87D241.7242.5841.20LCSS (hard)D129.0031.7515.65D240.2840.9934.11LCSS (soft)D121.8924.1112.01D237.2438.9634.57Table 2. EER(%) for different algorithms using differentnormalizations on different datasets.ror rate (EER) refers to a point where false alarm rate andmiss rate is equal. ForT-norm, the best 20 cohort scoreswere used for normalization. LCSS (soft) withT-normperforms best for D1 around the EER point, and for highmiss rates and low false alarms, whereas it performs poorerthan LCSS (hard) for low miss rates and high false alarms.This behavior appears to be reversed for D2. The mag-nitude around EER is much greater for D2. This is be-cause, none of the variations of the pallavi lines in test arepresent in the templates. It is also shown that RLCS per-forms poorer than any other algorithms for D2. The curvesalso show no improvements forZ-norm compared to base-line with no normalization. This can happen due to the waynormalization parameters are estimated forZ-norm. Forexample, some of the templates, which may not be similarto the test, can be similar to some of the cohorts’ templates,resulting in higher mean. This would not have happened inT-norm where the test itself is tested against the cohorts’templates.6DiscussionIn this section, we discuss how LCSS (hard) and LCSS(soft) can be combined to achieve better performance. Wealso verify thatT-norm reduces the overlap between trueand imposter scores.6.1Combining hard-LCSS and soft-LCSSInstead of selecting a threshold, we will assume that atrue claim is correctly veriﬁed when its score is greater thanall the cohort scores. Similarly, a false claim is correctlyveriﬁed when its score is lesser than atleast one of the co-hort scores. Table 3 shows the number of claims correctlyveriﬁed only by hard-LCSS, only by soft-LCSS, by bothand by neither of them. It is clear that there is an overlapbetween the correctly veriﬁed claims of hard-LCSS andsoft-LCSS. Nonetheless, the number of claims distinctlyveriﬁed by both is also signiﬁcant. Therefore, the com-bination of these two algorithms could result in a betterperformance.6.2Reduction of overlap in score distribution byT-normFigure 3 shows the effect ofT-norm on the distributionof hard-LCSS scores. It is clearly seen that the overlap, be-tween the true and imposter score distributions, is reducedProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6090.001 0.01  0.1  0.2  0.5    1    2    5   10   20   400.10.20.5 1  2  5  10 20 40 80\nFalse Alarm probability (in %)Miss probability (in %)\n  \nno−norm lcss (soft)z−norm lcss (soft)t−norm lcss (soft)no−norm lcss (hard)z−norm lcss (hard)t−norm lcss (hard)no−norm rlcsz−norm rlcst−norm rlcsno−norm dtwz−norm dtwt−norm dtw0.001 0.01  0.1  0.2  0.5    1    2    5   10   20   400.10.20.5 1  2  5  10 20 40 80\nFalse Alarm probability (in %)Miss probability (in %)\n  \nno−norm lcss (soft)z−norm lcss (soft)t−norm lcss (soft)no−norm lcss (hard)z−norm lcss (hard)t−norm lcss (hard)no−norm rlcsz−norm rlcst−norm rlcsno−norm dtwz−norm dtwt−norm dtwa) DET curves for dataset D1 b) DET curves for dataset D2Figure 2. DET curves comparing LCSS algorithm with different algorithms using different score normalizationsDatasetClaim-Hard-Soft-BothNeithertypeonlyonlyD1True235528977False4678174554D2True4723155220False99751585168Table 3. Number of claims correctly veriﬁed by hard-LCSS only, by soft-LCSS only, by both and by neither ofthem for D1 and D2 usingT-norm\n−5−4−3−2−101234500.10.20.30.40.50.60.70.80.9\nLCSS (hard) scores without normalizationDensity\n  True scoresImposter scores\n−5−4−3−2−101234500.10.20.30.40.50.60.70.80.9\nLCSS (hard) scores with t−normDensity\n  True scoresImposter scores\nFigure 3. Showing the effect ofT-norm on the score dis-tributionsigniﬁcantly. For visualization purposes, the true score dis-tributions are scaled to zero mean and unit variance andcorresponding imposter score distributions are scaled ap-propriately.6.3Scalability ofr¯agaveriﬁcationThe veriﬁcation of ar¯agadepends on the number of itscohortr¯agaswhich are usually 4 or 5. Since it does notdepend on all ther¯agasin the dataset, as inr¯agaidentiﬁ-cation, any number ofr¯agascan be added to the dataset.7Conclusion and future workIn this paper, we have proposed a different approach tor¯agaanalysis in Carnatic music. Instead ofr¯agaidenti-ﬁcation,r¯agaveriﬁcation is performed. A set of cohortsfor everyr¯agais deﬁned. The identity of an audio clip ispresented with a claim. The claimedr¯agais veriﬁed bycomparing with the templates of the claimedr¯agaand itscohorts by using a novel approach. A set of 17r¯agasandits cohorts constituting 30r¯agasis tested using appropri-ate score normalization techniques. An equal error rate ofabout 12% is achieved. This approach is scalable to anynumber ofr¯agasas the givenr¯agaand its cohorts need tobe added to the system.8AcknowledgmentsThis research was partly funded by the European Re-search Council under the European Unions Seventh Frame-work Program, as part of the CompMusic project (ERCgrant agreement 267583). We are grateful to Padmasun-dari for selecting the cohorts.9References[1]R. Auckentaler, M Carey, and H Lloyd-Thomas. Scorenormalisation for text-independent speaker veriﬁcationsystems.Digital Signal Processing, 10:42–54, 2000.[2]P Chordia and A Rae. Raag recognition using pitch-class and pitch-class dyad distributions.In Proceed-ings of International Society for Music Information Re-trieval Conference, pages 431–436, 2007.[3]Pranay Dighe, Parul Agarwal, Harish Karnick, Sid-dartha Thota, and Bhiksha Raj. Scale independent ragaidentiﬁcation using chromagram patterns and swarabased features. InProceedings of IEEE InternationalConference on Multimedia and Expo Workshops, pages1–4, 2013.[4]Pranay Dighe, Harish Karnick, and Bhiksha Raj. Swarahistogram based structural analysis and identiﬁcationof indian classical ragas. InProceedings of 14th Inter-national Society for Music Information Retrieval Con-ference, pages 35–40, 2013.610 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[5]Shrey Dutta and Hema A. Murthy. Discovering typicalmotifs of a raga from one-liners of songs in carnaticmusic. InProceedings of 15th International Society forMusic Information Retrieval Conference, pages 397–402, 2014.[6]D.P.W. Ellis and G.E. Poliner. Identifying ‘coversongs’ with chroma features and dynamic program-ming beat tracking. InProceedings of IEEE Interna-tional Conference on Acoustics, Speech and SignalProcessing, volume 4, pages IV–1429–IV–1432, 2007.[7]S Arthi H G Ranjani and T V Sreenivas. Shadja, swaraidentiﬁcation and raga veriﬁcation in alapana usingstochastic models.In IEEE Workshop on Applicationsof Signal Processing to Audio and Acoustics, pages 29–32, 2011.[8]Vignesh Ishwar, Ashwin Bellur, and Hema A Murthy.Motivic analysis and its relevance to raga identiﬁcationin carnatic music. In2nd CompMusic Workshop, 2012.[9]Vignesh Ishwar, Shrey Dutta, Ashwin Bellur, andHema A. Murthy. Motif spotting in an alapana in car-natic music. InProceedings of 14th International Soci-ety for Music Information Retrieval Conference, pages499–504, 2013.[10]Gopala Krishna Koduri, Sankalp Gulati, and PreetiRao. A survey of raaga recognition techniques and im-provements to the state-of-the-art.Sound and MusicComputing, 2011.[11]Gopala Krishna Koduri, Sankalp Gulati, Preeti Rao,and Xavier Serra. Raga recognition based on pitch dis-tribution methods.Journal of New Music Research,41(4):337–350, 2012.[12]A.S. Krishna, P.V . Rajkumar, K.P. Saishankar, andM. John. Identiﬁcation of carnatic raagas using hid-den markov models. InIEEE 9th International Sympo-sium on Applied Machine Intelligence and Informatics,pages 107 –110, 2011.[13]T M Krishna and Vignesh Ishwar. Carnatic music :Svara, gamaka, motif and raga identity. In2nd Comp-Music Workshop, 2012.[14]V . Kumar, H. Pandya, and C.V . Jawahar. Identifyingragas in indian music. InProceedings of 22nd Interna-tional Conference on Pattern Recognition, pages 767–772, 2014.[15]Hwei-Jen Lin, Hung-Hsuan Wu, and Chun-Wei Wang.Music matching based on rough longest common sub-sequence.Journal of Information Science and Engi-neering, pages 27, 95–110., 2011.[16]Meinard M¨uller, Frank Kurth, and Michael Clausen.Audio matching via chroma-based statistical features.InProceedings of International Conference on MusicInformation Retrieval, pages 288–295, 2005.[17]Jiri Navratil and David Klusacek. On linear dets. InProceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, pages 229–232, 2007.[18]Gurav Pandey, Chaitanya Mishra, and Paul Ipe. Tansen: A system for automatic raga identiﬁcation. InPro-ceedings of 1st Indian International Conference on Ar-tiﬁcial Intelligence, pages 1350–1363, 2003.[19]P. Rao, J. Ch. Ross, K. K. Ganguli, V . Pandit, V . Ishwar,A. Bellur, , and H. A. Murthy. Melodic motivic anal-ysis of indian music.Journal of New Music Research,43(1):115–131, 2014.[20]Joe Cheri Ross, Vinutha T. P., and Preeti Rao. Detect-ing melodic motifs from audio for hindustani classicalmusic. InProceedings of 13th International Society forMusic Information Retrieval Conference, pages 193–198, 2012.[21]Sridharan Sankaran, Krishnaraj P V , and Hema AMurthy. Automatic segmentation of composition incarnatic music using time-frequency cfcc templates. InProceedings of 11th International Symposium on Com-puter Music Multidisciplinary Research, 2015.[22]J. Serra, E. Gomez, P. Herrera, and X. Serra. Chromabinary similarity and local alignment applied to coversong identiﬁcation.IEEE Transactions on Audio,Speech, and Language Processing, 16(6):1138–1151,2008.[23]Joan Serr`a, Gopala K. Koduri, Marius Miron, andXavier Serra. Assessing the tuning of sung indian clas-sical music. InProceedings of 12th International Soci-ety for Music Information Retrieval Conference, pages157–162, 2011.[24]Sankalp Gulati Joan Serra and Xavier Serra. An evalu-ation of methodologies for melodic similarity in audiorecordings of indian art music. InProceedings of 40thIEEE International Conference on Acoustics, Speechand Signal Processing, 2015.[25]Surendra Shetty. Raga mining of indian music by ex-tracting arohana-avarohana pattern.International Jour-nal of Recent trends in Engineering, 1(1), 2009.[26]Rajeswari Sridhar and Tv Geetha. Raga identiﬁcationof carnatic music for music information retrieval.In-ternational Journal of Recent trends in Engineering,1(1):1–4, 2009.[27]M Subramanian. Carnatic ragam thodi pitch analysisof notes and gamakams.Journal of the Sangeet NatakAkademi, XLI(1):3–28, 2007.[28]D Swathi. Analysis of carnatic music: A signal pro-cessing perspective. MS Thesis, IIT Madras, India,2009.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 611"
    },
    {
        "title": "Musicology of Early Music with Europeana Tools and Services.",
        "author": [
            "Erik Duval",
            "Marnix van Berchum",
            "Anja Jentzsch",
            "Gonzalo Alberto Parra Chico",
            "Andreas Drakos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417509",
        "url": "https://doi.org/10.5281/zenodo.1417509",
        "ee": "https://zenodo.org/records/1417509/files/DuvalBJCD15.pdf",
        "abstract": "The Europeana repository hosts large collections of digit- ized music manuscripts and prints. This paper investi- gates how tools and services for this repository can ena- ble Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out re- search that is impossible to do without such tools or ser- vices. We report on the methodology, user-centered de- velopment of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their re- search. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Mu- sic Recognition and computer-supported analysis of mu- sic scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repos- itory.",
        "zenodo_id": 1417509,
        "dblp_key": "conf/ismir/DuvalBJCD15",
        "keywords": [
            "Early Music musicologists",
            "research tools",
            "data sharing",
            "workflow integration",
            "facets filtering",
            "music score analysis",
            "activity streams",
            "less linear organization",
            "repository consumption",
            "research results"
        ],
        "content": "MUSICOLOGY OF EARLY MUSIC WITH EUROPEANA TOOLS AND SERVICES  Erik Duval1, Marnix van Berchum2, Anja Jentzsch3, Gonzalo Alberto Parra Chico1, Andreas Drakos4 1 erik.duval@cs.kuleuven.be, Dept. of Computer Science, KU Leuven, B 2 marnix.van.berchum@dans.knaw.nl KNAW-DANS, Utrecht University, NL 3 anja.jentzsch@okfn.org Open Knowledge Foundation, D 4 AgroKnow, GR  ABSTRACT The Europeana repository hosts large collections of digit-ized music manuscripts and prints. This paper investi-gates how tools and services for this repository can ena-ble Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out re-search that is impossible to do without such tools or ser-vices. We report on the methodology, user-centered de-velopment of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their re-search. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Mu-sic Recognition and computer-supported analysis of mu-sic scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repos-itory. 1.!INTRODUCTION AND BACKGROUND The basic aim of the work presented in this paper is to develop services and tools that leverage content in the Europeana Cloud for researchers in digital humanities [4]. In a first year of experimentation, we focused on con-tent in the Wittgenstein archives at the University of Ber-gen and the Axiom philosophy group at the VU Universi-ty Amsterdam [5]. In this paper, we report on experimen-tation in a second year of the project, where we targeted a research community of musicologists that focus on Early Music.  It is important to note that the Europeana Cloud pro-ject has a much wider scope: it is concerned with migrat-ing the backend technology of Europeana to a cloud-based infrastructure. The focus of our work is to demon-strate that this technical development enables new tools and services that make it possible for researchers in digi-tal humanities (in the specific case of the work presented in this paper: researchers in Early Music) to either carry out their existing research in a more effective or efficient way, or to carry out research work that is impossible without such tools and services, at least in practical terms, for instance because it would involve too much manual tedious human labor. In the early phase of the project, as the cloud-based services are still under development, we investigate this issue of added value by loosely integrating existing tools and services accessing the original Europeana services and other suitable services, and by imitating the workflow of the Europeana research platform, which is still under development. 2.!RESEARCH GOAL AND METHODOLOGY 2.1!Research questions In this paper, we address the following research ques-tions: 1.!What are the main problems for digital musicol-ogists whose research focuses on Early Music? 2.!How can we address these problems and demon-strate the potential added value of cloud-based tools and services on top of large repositories of content like Europeana for Early Music musi-cologists? 2.2!Methodology Our basic methodology is User Centered Design [1]. The users of this iteration were musicologists working on Ear-ly Music (up to and including Monteverdi). A small group (5 persons) was selected from within the network of the authors. Besides their focus on Early Music, the musicologists in the group share an affinity with technol-ogy, and to a different degree are all involved in applying technology to their research practice.   As designers and developers, we had regular formative evaluation sessions over Skype or Google Hangout with the musicologists. (In fact, this worked surprisingly well and allowed for many more regular meetings than we could have organized in more traditional settings with such a diverse, busy and geographically distributed group of participants.) We also had a face-to-face meeting at the \n © Erik Duval, Marnix van Berchum, Anja Jentzsch,. Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0). Attribution: Erik Duval, Marnix van Berchum, Anja Jentzsch, Gonzalo Alberto Parra Chico and Andreas Drakos. “Mu-sicology of Early Music with Europeana tools and services”, 16th Inter-national Society for Music Information Retrieval Conference, 2015. 632   end of the yearly development cycle, for a more in-depth evaluation (see section 6). In initial meetings the musicologists discussed with us the workflow, computational tools, and content that they currently use. It is important to note that the evaluation sessions fo-cused on usefulness and usability-in-the-large, i.e. on whether or not the foreseen tools and associated research methodology would actually be of any substantial added value to the researchers involved. We wanted, more spe-cifically, to find out whether our approach could help them to actually change the way they work, whether such an approach would address problems that they may or may not be aware of in their current way of working, etc. Only to a much lesser extent were we interested in find-ing out whether the Early Music researchers can carry out their current way of working in a more efficient way with our tools and methodology. 3.!RELATED WORK In the past decades, the musicology community in general has been actively involved in the use and development of digital tools for enhancing musicological research. The scholarly study of Early Music is no exception, focusing on very specific problems from this period of music his-tory, while still making use of generic solutions. The de-velopment of encoded music formats has been very im-portant, opening up opportunities for musicologists to make use of and analyze machine-readable scores [18]. Seminal work on music encoding is carried out from the eighties onwards, culminating for now in more recent work on how full digital, critical editions of Early Music could be conceived. Further proof of the affinity of the Early Music community can be found in a special issue of the journal Early Music (i.e. Volume xlii (2014), No. 4). Whereas some research has focused on Optical Music Recognition (OMR) for automated metadata generation [11], we rely on metadata from repositories of musical sources (manuscripts, prints) in Europeana and apply OMR techniques in a later step in order to generate a ma-chine readable music encoding for analysis (see section 5.5). In that sense, the scope and goal of the work pre-sented here is more similar to [6], though we focus spe-cifically on Early Music and a User-Centered Design ap-proach for end user tool design and development (section 2.2). An outcome of this approach is that we provide geo-spatial and time based visualization of search results, ra-ther than a more conventional list of search results, as used in for instance [11]. In fact, we believe that visual approaches to music access remain underexplored, de-spite some work like [16] and [19]. Our work is a bit dif-ferent from this earlier work on visualization in that it fo-cuses more on visualizations based on geospatial and time based characteristics of music rather than on visual-izing clusters of related music. The User-Centered Design approach, which is also cen-tral to the work presented in this paper, found its way al-ready in the emerging field of ‘digital musicology’ [2][3] but our focus is on leveraging the content from large-scale repositories for musicology. 4.!MAIN PROBLEMS FOR MUSICOLOGISTS At the initial stage of our work, we identified the follow-ing four core problems for the musicologists in our dis-cussions with them: 1.!Difficulty of creating the data and metadata needed: the creation of encoded music scores of Early Music (i.e. ‘musical data’) is a laborious task, which is often carried out with proprietary software packages not suited for the particular types of music notation from this period. Like-wise, the metadata on these scores, their original sources, the composers etc. are locked into paper publications and not easily transformed into dig-ital format. 2.!Lack of digital corpora with music scores: there are some repositories with music scores for Ear-ly Music, like for example CMME (http://www.cmme.org), ECOLM (http://www.ecolm.org), the Josquin Research Project (http://josquin.stanford.edu) and SIMSSA (http://www.simssa.ca) [6], but they are fragmented and it is tedious and time-consuming to go through the different reposito-ries (each with their own query facilities) and do a systematic search for a particular composer or theme. 3.!Information exchange and linking of data when working with different tools: although there are specific tools to process music scores, they do not inter-operate and it is again quite tedious and time-consuming to apply different tools on the same content and then to integrate the results of the different tools. 4.!Retrieval and analysis of contextual information about the music scores, from bibliographical and historical databases, like the Oxford Music Online (http://www.oxfordmusiconline.com/) or RILM (http://www.rilm.org).  As will become clear in the remainder of this paper, we eventually succeeded in addressing the 1st, 2nd and 3rd problem listed above. 5.!TOOL SETUP 5.1!Introduction In order to investigate how technology can help the musi-cologists with these problems, we designed, created, inte-grated and evaluated a set of prototype tools that extends the toolset we prepared for the philosophers the year be-fore. The complete toolset consists of (see Figure 1):  •!Ariadne Finder (section 5.2): this tool, personal-ized for musicologists, helps researchers search and find content coming from Europeana and other sources in a simple and integrated way - the intent is that this tool addresses problem 2 mentioned above; Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 633   •!TimeMapper (section 5.3): this integrated tool visualizes the search results from the Ariadne finder on a timeline and an interactive map, in order to enable the musicologists to further fil-ter the content and get a better overview of the different resources found on Europeana (http://timemapper.okfnlabs.org);  •!Activity Stream (section 5.4): this service, inte-grated in all the tools, captures and presents the different actions carried out by the users in their interactions with the tools; •!Aruspix (section 5.5): this is an optical music recognition (OMR) tool which transforms prints of Early Music scores into MEI [13]; •!Music21 (section 5.6): this is a Python-based set of tools for analysing music encoded as XML (http://web.mit.edu/music21/) [6].    Figure 1: Schema of interconnected tools; 5.2!Ariadne Finder A series of meetings with the musicology researchers en-abled us to identify the content collections of interest. To the Europeana base collection, we added the resources from RISM (http://www.rism.info/) , and integrated them in the Finder. RISM is a well-known and extensively used inventory of musical sources. The abbreviations of library sigla used in RISM, have an authoritative charac-ter within musicology, and can be used as a controlled vocabulary in a digital environment.  After the first year experimentation, we simplified the user interface of the Finder by removing some predefined categories from the home screen. Instead, we made a list of four search facets (i.e. provider, media type, language, and year) available on the first screen with the search re-sults.  The integration of the RISM collection was a great challenge: the data covered by RISM (metadata on prima-ry musical sources) are heterogeneous and quite different from the ones provided by Europeana. To allow the inte-gration with the Finder backend and to enable the visuali-sation of search results in a uniform way, transformation of the metadata to an internal format was required. More-over, linking to the actual resource was not possible, since RISM provides metadata on the current (physical) holding of the sources, and does not provide links to the digitized versions of the sources.   The Finder is used as the ‘baseline’ tool for the inte-gration of the other tools, listed below. Both the Activity Stream and the TimeMapper are integrated in the Finder to see the past user activities (i.e. searches) and to visual-ise search results respectively. When viewing an individ-ual search result, the connection to Music21, through Aruspix, is also available.  In Figure 2, the listing of the search results is shown, with the facets on the left that can be used to further re-fine the search. Finally, Figure 3 shows how an individu-al search result is displayed to the user, with the links to the functionality of Aruspix and Music21.  \n Figure 2 : Search results in the Finder   \n Figure 3 : Individual search result in the Finder  The Ariadne Finder for the Musicologists group can be accessed at http://greenlearningnetwork.com/cmme-finder/.  5.3!TimeMapper Europeana provides a variety of metadata for its re-sources, including thumbnail images, geo-coordinates and time information. TimeMapper visualizes the temporal and geographical characteristics of resources. TimeMapper is a data visualization tool that allows for the creation of timelines and timemaps using Google spreadsheets (http://timemapper.okfnlabs.org). While the Finder provides the user with a faceted search for Euro-peana resources, it might still be difficult to navigate through large amounts of search results. We integrated TimeMapper in our tool chain to provide an interactive geo-spatial visualization of the search results. This ena-bles users to quickly navigate the metadata and to order resources on the basis of time and place of publication. In \n634 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   this way, they can more easily identify resources worth studying in more detail. Figure 4 shows the TimeMapper when drilling down into resources that match the keyword “Gardano”. TimeMapper is available under the MIT licence. The tool can be accessed via the Ariadne Finder button labelled “View in TimeMapper”.  \n Figure 4: TimeMapper showing resources published by Gardano 5.4!Activity Stream Based on our earlier work on community reading aware-ness Error! Reference source not found. and supporting the Science 2.0 idea of enhancing collaboration among researchers [17], we have designed, developed and de-ployed a web application called the “Activity Stream (AS)”, enabling researchers to share their work related activities within a community. More specifically in the context of the Early Music musicologists, the application aggregates “search” and “visualize” activities, and makes researchers aware of what their peers are currently work-ing on.  In the first prototype, the AS presented information about “searches” that were carried out with the Ariadne Finder and terms that were “visualized” using the Time-Mapper, as illustrated by Figure 5. The activities in the stream are structured as: Actor | verb | (Object). For ex-ample, \"User from GR\" | \"has searched\" | \"Bolzano\". For the musicologists, two new activities were added to the activity stream: interpretation and processing. These rep-resent the usage of the Aruspix and Music21 components (see below).  \n Figure 5: Main screen of the activity stream  The Activity Stream is implemented as a web applica-tion (using HTML and JavaScript) and deployed using the Google App Engine (GAE). Together with the terms used to perform a search or visualization, a link to the tool showing the outcome of that action is provided. Al-so, in order to provide users the flexibility to filter activi-ties, tool grouping was added to the application. For in-stance, by clicking on the tool’s name (e.g.: Finder or TimeMapper) the user can consult the stream of activities from that tool only.  The Activity Stream allows us to digest different events sent from different tools (via REST services) used by researchers, but also provides the possibility to embed these in other software components. For example, the ap-plication supports RSS syndication as a passive notifica-tion system. Figure 6 illustrates the current activity sources and outlets. \n Figure 6: Information sources and destinations of the Activity Stream  5.5!Aruspix Aruspix is an optical music recognition (OMR) tool that scans early music prints, transcribes them and encodes them into the MEI standard [9][10][15]. While there are other OMR tools available, mainly for music in common music notation, Aruspix is the only tool to our knowledge that can handle scores printed in the 16th and 17th centuries with movable typefaces. Such scores are often difficult to examine with existing super-imposition and optical recognition software, as they pre-sent a number of specific layout and format problems and are quite often in a deteriorated state because of their age [12][13][14]. The printing techniques of that time mean that differ-ences can exist between copies produced in the same print run, and comparison of these copies by superimpo-sition can enable more accurate critical editions to be prepared. Digitizing the scores through optical recogni-tion can enable us to collate different editions regardless of layout, and is also useful in for instance the preparation of digital music libraries. For Europeana Cloud, we use the command line ver-sion of Aruspix that automatically converts digital scans of scores to MEI files in a page-wise fashion. We then need to combine the pages into a single score again. \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 635   Moreover, the MEI version being used by Aruspix is a new and not yet standardized one[13]. Since Music21 (see next section) needs MEI files that use the 2012 or 2013 specification, we developed an XSLT program to transform the MEI files that Aruspix delivers into this newer format. The command line version sends requested score tran-scriptions to the Music21 service for further analysis (Section 5.6). Furthermore, it sends activity on tran-scribed scores to the Activity Stream (Section 5.4). 5.6!Music21 Music21 is a Python-based object-oriented toolkit for computer-aided musicology that allows music infor-mation, extraction and generation, together with music notation editing and scripting in symbolic (score-based) forms (http://web.mit.edu/music21/)[6]. The toolkit is able to import different formats, such as MusicXML and MEI.  We extended the Music21 web application module in order to provide parsing and processing requests to a |Music21 installation running on a server. In the work-flow, Music21 is used after the Aruspix service has creat-ed an MEI version of a score. With an MEI file, a specific set of actions becomes available to the musicologists in order to support them with the analysis of the music in-volved: calculation of ‘Parts and Measures’, calculation of the ‘Pitch ranges’ and requesting the ‘legal melodic intervals’ of a score. 6.!EVALUATION 6.1!General evaluation To start the discussion, the complete workflow of tools was presented to the musicologists. Afterwards, questions were asked regarding the usefulness of their current tool setup. In general, the participants agreed that the way in which the tools support the research process is helpful. The connection of existing tools (optical music recogni-tion and processing of encoded scores) and automating the process of data sharing between these tools is of great value for them, as it saves them time with their research tasks, compared with using the tools individually. Actual-ly, some of the musicologists had not been able to manu-ally feed the output of one tool as input to the next tool in the workflow. While the participants found the overall workflow use-ful, they were also interested in details about specific parts of it. Some of them suggested that, in some cases, just one or two tools are more relevant for their research (e.g. converting a score into a computer readable format or importing their own encoded scores for processing with Music21). This is mainly related to their very varied technical background and research goals. Some of the participants are computational musicologists that regular-ly use tools like Music21, while others are more tradi-tional musicologists that work with the original sources and have very limited digital research experience. The participants agreed with the added value of the loosely integrated workflow while doing research on a single item (score), but also observed that the workflow could be automated for use at a larger scale (e.g. a large dataset of scores of a specific period or region). Such au-tomation could be of great value in order to answer re-search questions about a complete collection or in order to generate new questions for such a collection. 6.2!Ariadne Finder, TimeMapper and Activity Stream After the musicologists discussed the overall workflow, the loose coupling and setup of tools, they were prompted to assess the tools on an individual level. From the set of tools adapted from the experimentation the year before with the philosophers [6], the Time-Mapper was considered the most interesting and relevant for musicology research. In its current form, the tool pro-vides a visualization of scores based on location and year of print. The participants suggested extending the func-tionality of the tool, for example with the use of more in-formation than just the data of publication of the prints (e.g. include the information gathered in the Music21 tools, like parallel fifths, valid melodies, or other species counterpoints of a score or measure) or the possibility to compare different timelines that represent results for dif-ferent search terms. This feedback basically confirms the relevance and usefulness of information visualization techniques in general for musicology research [16][19]. The Finder was mostly seen as a tool that provides ex-isting functionality, similar to what other search engines provide, though the musicologists acknowledged the val-ue of having facets to filter the result set. They suggested to personalize facets to terms that are closer to musicolo-gist research practice, for example, to use ‘printed books’, ‘manuscripts’, ‘single pieces’ instead of ‘image’ or ‘text’ classification. The musicologists were more critical about the useful-ness of the Activity Stream (AS) in their research activi-ties. They were not sure that the current actions are rele-vant for them or even which alternative kinds of activities might be useful to be displayed in the tool. They mostly perceived the AS as an interesting communication device or as a source of information that is comparable to what is common in a Social Network (like Facebook, or more specific for research, like https://www.academia.edu or https://www.researchgate.net/). The participants suggest-ed functionality to enhance the perceived usefulness of the stream, such as a search for specific activities, the possibility to aggregate activities in order to obtain statis-tics from them, and the possibility to store results for later use. Participants also suggested other interesting ways to connect the tools, instead of only having a linear ap-proach, as in the current setup. For example, they men-tioned that it would be interesting to be able to take the output of Music21 (e.g. parallel fifths of a score) and map 636 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   the results, based on their location, with the TimeMapper. This can provide an overview of specific score character-istics and relate them to a particular location. 6.3!Aruspix and Music21 While the Aruspix version included in our workflow does not have a visual frontend for the users, the musicologists acknowledge its importance in the workflow. As men-tioned, optical music recognition (OMR) is a crucial step for them [11][12][13][14]. Regarding the current output of this tool, the musicologists would appreciate to see the encoding result and the percentage of errors after the OMR process. While in other sciences, researchers are used to work with and accept a certain percentage of er-rors, these may not be well accepted in the musicology domain where there is much less of a tradition to work with data that include errors. Nevertheless, the musicolo-gists appreciate what is happening behind the scenes and how good the obtained encoding is, and believe that the results could build trust from the user in the system. Moreover, information about errors can be used as a feedback mechanism for Aruspix: study participants men-tioned that they wanted such a facility to be as simple as possible but at the same time complete enough to get the desired information. The Music21 web interface was one of the most inter-esting tools for the musicologists. Besides the textual rendition of the analytical results, the participants would also like access to plots or statistics (e.g. note distribu-tion), as these could be more helpful in order to identify characteristics of a score. Currently, the Music21 inter-face only supports a specific set of generic calculations and processes [6]. The participants would like to have the freedom to build their own analysis, via text or through a graphical user interface. 6.4!Other comments During the face-to-face evaluation session, the partici-pants provided suggestions about the tools and the work-flow, but also about the underlying concepts. For exam-ple, some users suggested being able to push the generat-ed encoded scores by Aruspix (MEI or MusicXML) back into the Europeana repository, so that we would use OMR technology to generate metadata, as in [12][14]. Likewise, results created with the Music21 toolkit could be considered as metadata of a particular composition, and as such could also be fed back into the Europeana Cloud repository. Such an approach would enable sharing intermediate research results with peers and a more  Science2.0 approach to research [17]. While it was not the direct scope of our work, the partici-pants made a number of suggestions for enhancing the specific usability of the tools and providing a nicer user interface overall. Finally, the participants suggested additional tools or functionality to be considered. These included: •!Possibility to run batch processes, in order to get a broader overview of music characteris-tics of a set of scores. •!Support for playback mechanisms in Music21 (or Aruspix), in order to be able to validate and confirm the automatic encoding by listen-ing to the result. •!Possibility to annotate directly into the digital version of a score. •!Possibility to create their own visualizations based on the data obtained from different tools, especially from the Music21 output. •!Inclusion of additional musicology resources, for example from http://www.diamm.ac.uk/.  7.!CONCLUSION AND FUTURE WORK Basically, the User-Centered Development process seems to work as intended: the target users positively evaluated the end result. An important issue for the next cycle is to connect the frontend tools for researchers with the actual backend infrastructure of Europeana Cloud, which has progressed into deployment while our work was taking place. This integration in the production system will ena-ble us to work with more comprehensive content collec-tions. It is clear from the results that we obtained that there is substantial potential to support novel research methods on large-scale collections of music sources, using tech-nologies like Optical Music Recognition, information visualization, loose coupling of tools, and flexible search. Our work illustrates how this can help researchers in Ear-ly Music to carry out existing research in more efficient and effective ways, and even address research questions that are hard or impossible to work on with more tradi-tional means. As such, the potential for a Science2.0 ap-proach to musicology is quite considerable.  8.!ACKNOWLEDGEMENT We gratefully acknowledge the support of the Europeana Cloud project, funded under the ICT Policy Support Pro-gramme (ICT PSP) as part of the Competitiveness and Innovation Framework Programme by the European Community (grant agreement no. 325091).  We are very grateful for the intensive discussions and detailed feedback from the following Early Music musi-cologists: Frans Wiering (Utrecht University), Reinier de Valk (City University London), Eliane Fankhauser (Utrecht University), Laurent Pugin (RISM Switzerland) and Peter van Kranenburg (Meertens Institute - KNAW)   Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 637   9.!REFERENCES [1]!C. Abras, D. Maloney-Krichmar, and J. Preece, ”User-centered design,” Encyclopedia of Human-Computer Interaction, Vol. 37, No. 4, pp. 445–56, 2004. [2]!A. Aljanaki, D. Bountouridis, J. A. Burgoyne, J. Van Balen, F. Wiering, H. Honing, and R. Veltkamp, “Designing games with a purpose for data collection in music research. Emotify and hooked: Two case studies,” Games and Learning Alliance, pp. 29–40, 2014. [3]!M. Barthet and S. Dixon. “Ethnographic observations of musicologists at the British Library: Implications for music information retrieval,” ISMIR11: Proc. of the 12th International Society for Music Information Retrieval Conference, pp. 353–358, 2011. [4]!A. Benardou, C. Dallas, and A. Dunning, “From Europeana Cloud to Europeana Research: The challenges of a community-driven platform exploiting Europeana content,” Digital Heritage. Progress in Cultural Heritage: Documentation, Preservation, and Protection, pp. 802–810, 2014. [5]!G. Parra, J. Klerkx and E. Duval, \"What Should I Read Next?: Awareness of Relevant Publications through a Community of Practice,\" CHI '13 Extended Abstracts on Human Factors in Computing Systems, pp. 2375-2376, 2013. [6]!H. van den Berg, G. Parra, A. Jentzsch, A. Drakos and E. Duval, \"Studying the History of Philosophical Ideas: Supporting Research Discovery, Navigation, and Awareness,\" Proc. of the 14th Inter. Conf. on Knowledge Technologies and Data-driven Business, 12:1-12:8, 2014. [7]!M. S. Cuthbert and C. Ariza. “Music21: A toolkit for computer-aided musicology and symbolic music data,” ISMIR10: Proc. Of the 11th International Conference on Music Information Retrieval, pp. 637–642, 2010.  [8]!I. Fujinaga, A. Hankinson, and J. E. Cumming, “Introduction to SIMSSA (Single Interface for Music Score Searching and Analysis)”, DLfM14: Proc. of the 1st International Workshop on Digital Libraries for Musicology, pp. 1–3, 2014. [9]!A. T. Geertinger, and L. Pugin: “MEI for bridging the gap between music cataloguing and digital critical editions”, Die Tonkunst. Magazin für klassische Musik und Musikwissenschaft, 5.3, pp. 289-294, 2011. [10]!A. Hankinson, P. Roland, and I. Fujinaga, “The Music Encoding Initiative as a document-encoding framework”. ISMIR11: Proc. of the 12th International Society for Music Information Retrieval Conference, pp. 293–298, 2011. [11]!A. Hankinson, J. A. Burgoyne, G. Vigliensoni, A. Porter, J. Thompson, W. Liu, R. Chiu, and I. Fuji- naga, “Digital document image retrieval using optical music recognition,” ISMIR12: Proc. of the 13th International Society for Music Information Retrieval Conference, pp. 577–582, 2012. [12]!L. Pugin, ‘Optical Music Recognition of Early Typographic Prints using Hidden Markov Models’, ISMIR06: Proc. Of the 7th International Conference on Music Information Retrieval, pp. 53-56, 2006. [13]!L. Pugin, J. Hockman, J.A. Burgoyne, and I. Fujinaga, “Gamera Versus Aruspix: Two Optical Music Recognition Approaches,” ISMIR08: Proc. of the 9th International Society for Music Information Retrieval Conference, pp. 419-424, 2008. [14]!L. Pugin and T. Crawford, “Evaluating OMR on the Early Music Online Collection,” ISMIR13: Proc. of the 14th International Society for Music Information Retrieval Conference, pp. 439–44. 2013. [15]!P. Roland, A. Hankinson, and L. Pugin, \"Early music and the Music Encoding Initiative,\" Early Music, Vol. xlii, No. 4, pp. 605-611, 2014. [16]!M. Schedl, P. Knees, K. Seyerlehner, and T. Pohle, “The CoMIRVA Toolkit for Visualizing Music-Related Data”. EUROVIS07: Proc of the 9th Eurographics/IEEE-VGTC Symp. on Visualization, pp. 147-154, 2007. [17]!B. Shneiderman, “Science 2.0,” Science, Vol. 319, No. 5868, pp. 1349-1350, 2008. [18]!J. Stinson and J. Stoessel. “Encoding medieval music notation for research,” Early Music, Vol. xlii No. 4, pp. 613–617, 2014. [19]!S. Stober and A. Nürnberger. “A multi-focus zoomable interface for multi-facet exploration of music collections,” 7th International Symposium on Computer Music Modeling and Retrieval, pp. 339–354, 2010. 638 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Searching Lyrical Phrases in A-Capella Turkish Makam Recordings.",
        "author": [
            "Georgi Dzhambazov",
            "Sertan Sentürk",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1287656",
        "url": "https://doi.org/10.5281/zenodo.1287656",
        "ee": "https://zenodo.org/records/1287656/files/makam_acapella-master_1.0.zip",
        "abstract": "Turkish makam acapella sections dataset issung by professional singers and isa collection of recordings of compositions from the vocal form şarkı.They are selected to be the same as the recordings in version two of http://compmusic.upf.edu/turkish-sarki\n\nThe main intention is to provide acapella counterpart to polyphonic recordings. \n\nTHE DATASET \n\nAudio music content\n\nThe collection has annotations with section, lyrics phrases and lyrics words. Each section, lyrics word and lyrical phrase is aligned to its corresponding segment in the audio.Annotations of secitons (aranağme, zemin etc.) are taken fromhttps://github.com/MTG/turkish_makam_section_dataset\n\nFORMAT: All annotations in TextGrid (used in Praat)\n\nturkish-makam-acapella-sections-dataset-2.0.zip is organised by artist and makam_acapella-master_1.0.zip is organised by musicbrainz ID.\n\nUsing this dataset\n\nPlease cite one of the following publications if you use the dataset in your work:\n\n\nDzhambazov, G., Serra X.(2015).Modeling of Phoneme Durations for Alignment between Polyphonic Audio and Lyrics.Sound and Music Computing Conference 2015.\n\n\nOr\n\n\nDzhambazov, G., Şentrk S.,  Serra X. (2015). Searching Lyrical Phrases in A-Capella Turkish Makam Recordings. 16th International Society for Music Information Retrieval (ISMIR) Conference\n\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nIf you have any questions or comments about the dataset, please feel free to write to us.\n\nGeorgi Dzhambazov\nMusic Technology Group,\nUniversitat Pompeu Fabra,\nBarcelona, Spain\ngeorgi dot dzhambazov at upf dotedu\n\n\n\nhttp://compmusic.upf.edu/turkish-makam-acapella-sections-dataset",
        "zenodo_id": 1287656,
        "dblp_key": "conf/ismir/DzhambazovSS15"
    },
    {
        "title": "I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.",
        "author": [
            "Hamid Eghbal-zadeh",
            "Bernhard Lehner",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416762",
        "url": "https://doi.org/10.5281/zenodo.1416762",
        "ee": "https://zenodo.org/records/1416762/files/Eghbal-zadehLSW15.pdf",
        "abstract": "In this paper, we present a novel approach to extract song- level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefficient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame- level features. The i-vectors provide a low-dimensional and fixed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance be- tween i-vectors in order to predict the genre of songs. Second, for a supervised artist classification task we re- port the performance measures using multiple classifiers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similar- ity (which uses extra information such as rhythm). In artist classification using timbre descriptors, our method outper- formed the state of the art.",
        "zenodo_id": 1416762,
        "dblp_key": "conf/ismir/Eghbal-zadehLSW15",
        "keywords": [
            "timbral features",
            "Mel-frequency cepstral coefficient (MFCC)",
            "identity vectors",
            "factor analysis procedure",
            "unsupervised music similarity estimation",
            "genre prediction",
            "supervised artist classification",
            "performance measures",
            "multiple classifiers",
            "state of the art"
        ],
        "content": "I-VECTORS FOR TIMBRE-BASED MUSIC SIMILARITY AND MUSICARTIST CLASSIFICATIONHamid Eghbal-zadeh Bernhard Lehner Markus Schedl Gerhard WidmerDepartment of Computational Perception, Johannes Kepler University of Linz, Austriahamid.eghbal-zadeh@jku.atABSTRACTIn this paper, we present a novel approach to extract song-level descriptors built from frame-level timbral featuressuch as Mel-frequency cepstral coefﬁcient (MFCC). Thesedescriptors are called identity vectors ori-vectorsand arethe results of a factor analysis procedure applied on frame-level features. The i-vectors provide a low-dimensionaland ﬁxed-length representation for each song and can beused in a supervised and unsupervised manner.First, we use the i-vectors for an unsupervised musicsimilarity estimation, where we calculate the distance be-tween i-vectors in order to predict the genre of songs.Second, for a supervised artist classiﬁcation task we re-port the performance measures using multiple classiﬁerstrained on the i-vectors.Standard datasets for each task are used to evaluateour method and the results are compared with the stateof the art. By only using timbral information, we alreadyachieved the state of the art performance in music similar-ity (which uses extra information such as rhythm). In artistclassiﬁcation using timbre descriptors, our method outper-formed the state of the art.1. INTRODUCTION AND RELATED WORKIn content-based music similarity and classiﬁcation, acous-tic features are extracted from audio and characteristics ofa song are projected into a new space called feature space.In this space, different attributes can be captured based onthe features used. For example, features such as Fluctua-tion Pattern (FP) [26], reﬂect the variability related to therhythm; and features such as MFCCs, demonstrate the tim-bral perspective of a song. However, the diversity of musicgenres, the presence of different musical instruments andsinging techniques make the capturing of these variabili-ties difﬁcult. Different modeling techniques and machinelearning approaches are used to ﬁnd the factors in the fea-ture space that best represent these variabilities.Multiple approaches have been followed in the litera-ture for extracting the features from songs in which 1) clas-\n© Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl,Gerhard Widmer. Licensed under a Creative Commons Attribution 4.0International License (CC BY 4.0).Attribution:Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl, Gerhard Widmer. “I-Vectorsfor Timbre-Based Music Similarity and Music Artist Classiﬁcation”, 16thInternational Society for Music Information Retrieval Conference, 2015.sical frame-level features, 2) block-level features and 3)song-level features are the most frequently used methodsin MIR.1.1 Frame-level featuresIn the frame-level approach, features are often extractedfrom short-time frames of a song. In this approach, framesare ﬁrst classiﬁed directly, and then the results are com-bined to make a decision for a song.1.2 Block-level featuresBlock-level features process the frames in terms of blocks,where each block consists of a ﬁxed number of frames.They are built in two steps: ﬁrst, the block processing stepand second, the generalization step. In the ﬁrst step, byselecting a collection of frames using a pattern, blocks arebuilt. Then in the second step, the feature values of allblocks are combined into a single representation for thewhole song. In [29], six different block-level features areintroduced and a method is proposed to fuse all the blockstogether. Block-level features [5, 24, 26, 29] have shownconsiderable performances in the MIREX1challenges.1.3 Song-level featuresSong-level features are found useful in artist recognitionas well as music similarity estimation. In [30], a compactsignature is generated for each song, and then is comparedto the other songs using a graph matching approach forartist recognition. In [21] multivariate kernels have beenused to model an artist. Recently, [5,29] proposed methodsto extract a ﬁxed-length vector from a song to be used inmusic similarity estimation and genre classiﬁcation.The advantage of methods based on song-level featuresis that different tools such as dimensionality reduction (e.g.Principal Components Analysis (PCA) [15]) and projec-tions can be applied to songs. For example, in [5], super-vectors extracted via a Gaussian Mixture Model (GMM)are found useful to represent songs and calculate the sim-ilarity using Euclidean distance. In [24] a method usingsong-level features is presented, which models frame-leveldescriptors such as MFCCs and FP with a single Gaussianand then the similarity between songs is calculated usingKullback Leibler divergence. In [26], rhythm descriptors1Annual Music Information Retrieval eXchange (MIREX). More in-formation is available at:http://www.music-ir.org554are introduced to improve the performance of music simi-larity measures in [24].1.3.1 GMM and GMM-supervectorsGMMs have been frequently used for acoustic modeling inmusic processing [4, 5, 12]. In [4, 5], a GMM is used asaUniversal Background Model(UBM) for content-basedmusic similarity estimation and genre classiﬁcation.Gaussian-based features used in [5, 24] are other exam-ples of song-level features which use a Gaussian model tocreate a statistical representation of a song from frame-level features. Similar to [4, 5], a GMM supervector iscomputed for each song. This representation is a ﬁxed-length vector, and is computed using a UBM (which is aGMM, trained on a database of songs) via a procedure de-scribed in [4, 5].The ﬁrst drawback of GMM-based methods is thatwhen the rank of the GMM space (number of Gaussiancomponents) increases, the dimensionality of GMM super-vectors rises which causes problems such as the curse of di-mensionality. One solution to this issue would be to use di-mensionality reduction methods such as PCA. In our previ-ous work [9], we showed that this is not effective. Anothersolution would be to decompose these high-dimensionalsupervectors into multiple terms with lower ranks whichwe will discuss in the following section.1.3.2 Session and Speaker variabilityAs described in [18], there exists a second drawback ofGMM-based methods. The performance of these frame-works suffer from their inability to capture the variabilityknown assession variabilityin the ﬁeld of speaker ver-iﬁcation. In contrast tospeaker variabilitywhich is thevariability that appears between different speakers, sessionvariability is deﬁned as the variability that appears for aspeaker from one recording to another [18]. This variabil-ity is calledsessionbecause it appears inside a recordingsession of a speaker.1.3.3 Song, Genre and Artist variabilityIn MIR, similar to session variability, we deﬁnesong vari-abilityas the variability that appears between songs. Also,similar to speaker variability, we deﬁnegenre variabil-ityfor genre classiﬁcation as the variability that appearsbetween different genres, andartist variabilityfor artistrecognition as the variability appears between differentartists.The second drawback of GMM-based methods is thatthey can not distinguish between song variability and genre(or artist) variability. If we can provide a decomposition ofGMM supervectors in a way that separates the desired fac-tors, such as genre variability from undesired ones, suchas song variability, and at the same time decreases thedimensionality of GMM supervectors, then as a result abetter representation of GMM supervectors with lower di-mensionality and better discrimination power will be ob-tained. Factor Analysis (FA) provides the means to pro-duce such representations where a GMM supervector de-composes into multiple factors. An advantage of the fea-tures obtained by FA compared to block-level features andGaussian-based features is that FA can be performed in away that after decomposition, each component can exhibita speciﬁc variability such as artist or genre. Thus, desiredfactors can be kept and undesired factors can be removedfrom the song’s GMM supervector. By applying such de-composition on top of the GMM space, another space withbases of desired factors (e.g. genre space, with genre fac-tors) can be created.Recently, in the ﬁeld of speaker veriﬁcation, Dehak etal. [7] introducedi-vectorswhich outperformed the stateof the art and provided a solution for the problem ofsession variability in the GMM-UBM frameworks. Thei-vector extraction is a feature-modeling technique thatbuilds utterance-level features, and it has been success-fully used in other areas such as emotion recognition [34],language recognition [8], accent recognition [1] and audioscene detection [10].The i-vector method applies a FA procedure to extractlow-dimensional features from GMM supervectors. ThisFA procedure estimates hidden variables in GMM super-vector space, which provides better discrimination abilityand lower dimensionality than GMM supervectors. Thesehidden variables are the i-vectors and even thoughthei-vector extraction procedure is totally unsupervised,they can be used for both supervised and unsupervisedtasks. The aim of this paper is to introduce the i-vectorsto the MIR community and show their performance on twoof the major tasks in content-based MIR.2. FACTOR ANALYSIS PROCEDUREIn this paper, examples are given from agenre classiﬁ-cationpoint of view. The deﬁnitions and the method areextendable to other tasks in MIR such as artist classiﬁca-tion.2.1 Overview of Factor Analysis MethodsA FA model can be viewed as a GMM supervector space,where genre and song factors are its hidden variables.Genre and song factors are deﬁned in a way that for a givengenre, the values of the genre factors are assumed to beidentical for all songs within that genre. The song factorsmay vary from one song to another.Let’s assume we have aCmixture components GMMand letFbe the dimension of the acoustic feature vectors.For each mixture componentc=1,...,C, letmcdenotethe corresponding genre-independent mean vector (UBMmean vector) and letmdenote theC·F⇥1supervectorobtained by concatenatingm1,...,mC.Maximum a posteriori (MAP) [14] is a method that isused to extract genre-dependent GMM supervectors. InMAP, it is assumed that each genregcan be modeled onlyby a single genre-dependent GMM supervectorM(g).This supervector is calculated from a genre-independentvectormwhich is then adapted to a couple of songs froma speciﬁc genre known as the genre-adaptation data.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 555Similar to speaker modeling in speaker veriﬁca-tion [19], the MAP approach to genre modeling assumesthat for each mixture componentcand genreg, there is anunobservable offset vectorOgsuch that:M(g)=m+Og(1)Ogis unknown and can not be learned during the MAPtraining procedure.Further,eigenvoice MAP[17] assumes the row vec-tors of the matrixOgare independent and identically dis-tributed. A rectangular matrixVof dimensionsC·F⇥Risassumed whereRis a parameter such thatR⌧C·F. TheVmatrix has a lower rank thanC·Fand can be learnedfrom the training data. The supervectorM(g)decomposesinto factorsy(g)which have lower ranks usingV. Forgenreg, the FA used in eigenvoice MAP is as follows:M(g)=m+Vy(g)(2)wherey(g)is a hiddenR⇥1vector which has a standardnormal distribution. Eigenvoice MAP trains faster thanMAP, yet trainingVproperly needs a very large amountof data, also song factors are not considered in the decom-position ofM(g).A solution for separation between song and genre fac-tors was ﬁrst suggested in [19], and later improved in [16]as Joint Factor Analysis (JFA). JFA decomposition modelcan be written as follows:M=m+Vy+Ux+Dz(3)whereMis a song GMM supervector,mis a genre- andsong-independent supervector which can be calculated us-ing a UBM,VandDdeﬁne a genre subspace (genre matrixand diagonal residual, respectively), andUdeﬁnes a songsubspace. The vectorsy,zare the genre-dependent factors,andxis the song-dependent factor in their respective sub-spaces. They are assumed to be a random variable with astandard normal distribution. Unlike eigenvoice MAP, JFAgives us a modeling with separated genre and song factorswith low ranks, where they can be used to better separatesongs from different genres by removing song variability.Even though JFA showed better performance than pre-vious FA methods, in terms of separation between songand genre factors, experimental results in [6] proved that ifwe extract song and genre factors using JFA, song factorsalso contain information about genres. Based on this ﬁnd-ing, another FA model is proposed in [7], which deﬁnes anew low-dimensional space called Total Variability Space(TVS). The vectors in this new space, are called i-vectors.In the TVS, both song and genre factors are considered, butmodeled together as a new factor namedtotal factor. To-tal factors have lower dimensionality than GMM supervec-tors and one can represent a song by extracting total factorsfrom its GMM supervector. Because i-vector FA showedthe best results in speaker veriﬁcation [7], in this paper weuse it for multiple tasks in MIR. The FA procedure used toobtain i-vectors is described in the next section.\nframesgmm supervectorTi-vectorF\u0000nF\u0000C(F\u0000C)\u00001(F\u0000C)\u0000RR\u00001(a)(b)(c)(d)(e)Figure 1: Graphical representation of different vectorsextracted during i-vector FA.Fis the dimensionality ofacoustic features,Cis the number of Gaussian compo-nents, andRis the rank of the TVS matrix. a) frame-levelfeatures of a song. b) and c) GMM supervector. d) TVSmatrixT. e) i-vector.2.2 Overview of I-vectorsTVS refers to total factors that contain both genre and songfactors. In the TVS, a given song is represented by a low-dimensional vector calledi-vector, which provides a goodgenre separability. This i-vector is known as point estimateof the hidden variables in a FA model similar to JFA. Thisdescribes these hidden variables and their characteristics.In Figure 1, a graphical representation of vectors used indifferent steps during i-vector FA is provided. From eachsong, ﬁrst frame-level features of dimensionalityFare ex-tracted as shown in Figure 1-a. Then, aCmixture compo-nents GMM trained on a large number of songs is used toextract GMM supervectors of dimensionF⇥C. This rect-angular vector ( Figure 1-b) then reshapes to a(F·C)⇥1vector (Figure 1-c). A matrix of(C·F)⇤Rknown as TVSmatrix (T) is learned from a set of songs.Tmatrix is usedto reduce the dimensionality of GMM supervectors toRwhereRis the rank ofT, as can be observed in Figure 1-d.The resulting vectors are i-vectors having a low rank ofR(Figure 1-e).\n(a)bluesmetalpop\n(b)bluesmetalpopFigure 2: 2D PCA projected vectors extracted from songsof 3 different genres in GTZAN dataset. a) i-vectors. b)GMM supervectors.A comparison between GMM representation and i-vector representation is provided in Figure 2. This visu-alization is prepared by projecting GMM supervectors andi-vectors using PCA into a 2 dimensional plane. Multiple556 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015songs of 3 different genres from the GTZAN dataset2areselected, then both their GMM supervectors and i-vectorsare extracted. In Figure 2-a, a scatter plot of the song’sprojected i-vectors are shown. Also, in Figure 2-b, GMMsupervectors projected using PCA are displayed. It can beobserved that i-vector extraction was successful at increasethe discrimination between songs of different genres. Inthe following paragraphs, the i-vector FA is described.ACmixture components GMM (c=1,...,C) calledUBM can be trained on a large amount of data from multi-ple genres, where for componentc,wc,mcand⌃cdenotemixture weight, mean vector and covariance matrix respec-tively. Given a song of genreg, a GMM supervectorM(g)can be calculated from a sequence ofX1,...,X⌧frames.The i-vector FA equation decomposes the vectorM(g)asfollows:Mc(g)=mc+Ty(4)whereMc(g)corresponds to a subvector ofM(g)forcomponentc,mcis the genre- and song-independent vec-tor, andy\u0000N(0,1)is the genre- and song-dependentvector, known as the i-vector. A rectangular matrixToflow rank known as TVS matrix is used to extract i-vectorsfrom the vectorMc(g).The i-vectoryis a hidden variable, but we can ﬁndit using the mean of its posterior distribution. This pos-terior distribution is Gaussian and is conditioned to theBaumWelch (BW) statistics for a given song [17]. Thezero-order and the ﬁrst-order BW statistics used to esti-matey, are calledNcandPcrespectively (see Equation 6).Similar to [20], the BW statistics are extracted using theUBM as follows.A closed form of an i-vectorylooks as follows:y=(I+Tt⌃\u00001N(s)T)\u00001·Tt⌃\u00001P(s)(5)where we deﬁneN(s)as a diagonal matrix of dimen-sionC·F⇥C·FwithNc⇥I(c=1,...,CandIhasF⇥Fdimensions) diagonal blocks.P(s)is a vector withC·F⇥1dimensions and is made by concatenating allﬁrst-order BW statisticsPcfor a given songs; also⌃isa diagonal covariance matrix of dimensionC·F⇥C·Festimated during the factor analysis procedure; it modelsthe residual variability not captured by the TVS matrixT.The BW statisticsNcandPcare deﬁned as follows.Suppose we have a sequence of framesX1,...,X⌧anda UBM withCmixture components deﬁned in a featurespace of dimensionF. The BW statistics needed to esti-mate the i-vector for a given song are obtained by:Nc=Xt\u0000t(c)Pc=Xt\u0000t(c)Xt(6)where, for timet,\u0000t(c)is the posterior probability ofXtgenerated by the mixture componentcof the UBM.2http://marsyas.info/downloads/datasets.htmlSince BW statistics are calculated using a GMM, they arecalledGMM supervectorsin i-vector modeling.TVS matrixTis estimated via a expectation maxi-mization procedure using BW statistics. More informationabout the training procedure ofTcan be found in [7, 22].3. I-VECTORS FOR UNSUPERVISED MUSICSIMILARITY ESTIMATIONIn this section, i-vectors are used for music similarity es-timation task. Genre and song variability are the factorsused in this task.3.1 DatasetThe 1517Artists3dataset is used for training UBM andTmatrix. This dataset consists of freely available songs andcontains 3180 tracks by 1517 different artists distributedover 19 genres. The GTZAN dataset is used for musicsimilarity estimation which contains 1000 song excerptsof 30 seconds, evenly distributed over 10 genres.3.2 Frame-level FeaturesWe use MFCCs as one of our timbral features. MFCCsare the most utilized timbre-related frame-level features inMIR. They are a compact, and perceptually motivated rep-resentation of the spectral envelope.For the extraction of the MFCCs, we use an observationwindow of 10 ms, with an overlap of 50%. We extract25 MFCCs with the rastamat toolbox [11]. The ﬁrst andsecond order derivatives (deltas and double-deltas) of theMFCCs are also added to the feature vector.Additionally, we use the ﬁrst order derivative of a cent-scaled spectrum, calculated in the same way as explainedin [29]. These features are called Spectrum Derivatives(SD).3.3 BaselinesFour different baselines are used to be compared to ourmethod. The ﬁrst baseline is fusing block-level similar-ity measure (BLS) [29], which uses 6 different block-level features containing spectral pattern, delta spectralpattern, variance delta spectral pattern, logarithmic ﬂuctu-ation pattern, correlation pattern and spectral contrast pat-tern. These features are used with a similarity functionand a distance normalization method to calculate a pair-wise distance matrix between songs. The second baselineis called Rhythm Timbre Bag of Features (RTBOF) [26].RTBOF has two components of rhythm and timbre whichare modeled over local spectral features. The third base-line is MARSYAS (Music Analysis, Retrieval and Synthe-sis for Audio Signals) which has an open source toolbox tocalculate various audio features.4A similarity function isused to calculate a distance matrix of features extracted asdescribed in [32]. The last baseline (CMB) is a combina-tion of BLS and RTBOF, which reported in [29] as the bestsimilarity method in case of genre classiﬁcation measures.3This dataset can be downloaded fromwww.seyerlehner.info.4http://marsyas.infoProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 5573.4 Experimental SetupA UBM with 1024 Gaussian components is trained on the1517Artists dataset using 2000 consecutive frames fromthe middle area of each song. No labels are used duringthe training procedure of UBM andTmatrix. The TVSmatrixTis trained using 400 total factors, and used duringthe i-vector extraction procedure. The number of factorsand Gaussian components was chosen after a parameteranalysis step on a small development dataset which differsfrom the datasets used in this paper.Two sets of different i-vectors are used to calculate twosimilarity matrices for the GTZAN dataset. First, MFCCfeatures are used to extract i-vectors, and cosine distanceis used to calculate a pair-wise distance matrix between allsongs, since in [7] cosine distance has been successfullyused with i-vectors. UBM andTmatrix are also trainedusing MFCC features of 1517Artists.Second, SD features are used to extract another set ofi-vectors to calculate our second distance matrix using co-sine distance. Similar to MFCC i-vectors, a new UBMandTmatrix is trained using SD features extracted from1517Artist dataset.Pair-wise distance matrices are normalized using a dis-tance space normalization (DSN) proposed in [25]. Thedistance matrices for baseline methods are downloadedfrom the website5of the author of [29].3.5 EvaluationWe evaluate the music similarity measures using genreclassiﬁcation via k-nearest neighbor (KNN) classiﬁcation.This method is also used in [5, 24, 26, 29]. We use dif-ferent values ofkthat vary from 1 to 20. Also, we use aleave-one-out scenario for genre classiﬁcation using pair-wise distance matrices.3.6 Results and DiscussionThe KNN genre classiﬁcation accuracy calculated usingour method is compared to the baseline methods, and theresults are shown in Figure 3. As can be seen, our methodusing MFCC features achieved the performance of the BLSbaseline and outperformed MARSYAS. By combining thedistance matrices calculated using MFCC and SD i-vectorswith equal weights after applying DSN, we could achievethe performance of RTBOF baseline.Since the authors of the BLS method in [29] reported acombination of BLS and RTBOF (named as CMB in [29])to perform best, we also combined our MFCC+SD i-vector distance matrix with RTBOF with equal weightsafter applying DSN and achieved the performance ofCMB. Furthermore, by combining MFCC+SD i-vector andCMB distance matrix (with equal weights after DSN), wecould achieve a better performance than the best combinedmethod reported in [29].5www.seyerlehner.info\nFigure 3: Evaluation results of KNN genre classiﬁcationon GTZAN dataset.3.7 ResourcesThe MSR Identity Toolbox [28] was used for i-vector ex-traction. We also used drtoolbox [33] to apply PCA forvisualization in Figure 2.4. I-VECTORS FOR SUPERVISED ARTISTCLASSIFICATIONIn this section, i-vectors are used for artist recognition task.Artist and song variability are the factors used in this task.More details about artist recognition using i-vectors can befound in our previous work [9].4.1 DatasetThe artist classiﬁcation experiments were conducted usingthe artist20 dataset [12]. It contains 1413 tracks, mostlyrock and pop songs, composed of six albums from each ofthe 20 different artists.4.2 Frame-level FeaturesInstead of extracting the MFCCs ourselves, we use theones provided as part of the dataset in [12]. Neither ﬁrstnor second order derivatives of the MFCCs are used. Sim-ilar to the approach already discussed in Section 3.2, wealso include the ﬁrst order derivative of a cent-scaled spec-trum (SD features).4.3 BaselinesMultiple baseline methods from the literature are selectedand their performance is compared to that achieved by ourmethod. Results are reported for a 20-class artist classiﬁ-cation task on the artist20 dataset [12]. The ﬁrst baseline(BLGMM) models artists with GMMs using MFCCs [12].The second baseline (BLsparse) uses a sparse featurelearning method [31] of ‘bag of features’ (BOF). Both themagnitude and phase parts of the spectrum are used in this558 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015method. The third baseline is (BLsign). It generates a com-pact signature for each song using MFCCs, and then com-pares these by a graph matching technique [30]. The fourthbaseline (BLmultiv) uses multivariate kernels [21] with thedirect uniform quantization of the MFCC features. The re-sults for the latter three are taken from their publications,while the results for theBLGMMbaseline are reproducedusing the implementation provided with the dataset. Theperformance of all baselines on the artist20 dataset are re-ported using the same songs, and the same fold splits in the6-fold cross-validation.4.4 Experimental SetupSimilar to the setup followed in Section 3.4, a UBM with1024 Gaussian components and aTmatrix with 400 fac-tors are used for i-vector extraction. Unlike the setup inmusic similarity estimation, no other dataset is used totrainTand the UBM. Instead, in each fold the trainingset is used to train UBM andTmatrix. Unlike the setupdescribed in Section 3.4, we apply a Linear DiscriminantAnalysis (LDA) [23] to the i-vectors to reduce the dimen-sionality from 400 to 19. The reason we didn’t use LDAfor music similarity estimation is that the whole procedureof i-vector extraction in Section 3 was unsupervised, andno labels were used during the i-vector extraction process.In each fold, the LDA is trained on the same data thatUBM andTmatrix are trained. I-vectors are centered byremoving the mean calculated from training i-vectors, thenlength-normalized [13] before applying LDA. After apply-ing LDA, once again i-vectors are length-normalized sinceiterative length-normalization was found useful in [2]. Thelength normalization provides a standard form of i-vectors.We fuse MFCC and SD i-vectors of a song simplyby concatenating the dimensionality-reduced i-vectors andsubsequently feed them into the classiﬁers investigated.First, a Probabilistic Linear Discriminant Analysis(PLDA) [27] is used to ﬁnd the artist for each song (iv-PLDA). PLDA is a generative model which models bothintra-class and inter-class variance as multidimensionalGaussian and showed signiﬁcant results with i-vectors [3].Second, a KNN classiﬁer withk=3(3NN) and a co-sine distance is considered (iv3NN). Third, a DiscriminantAnalysis (DA) classiﬁer is investigated with a linear dis-criminant function and a uniform prior (ivDA).4.5 EvaluationA 6-fold cross-validation proposed in [12] is used to eval-uate the artist classiﬁcation task. In each fold, ﬁve albumsfrom each artist are used for training and one for testing.We report mean class-speciﬁc accuracy, F1, precision andrecall, all averaged over folds.4.6 Results and DiscussionThe results of artist classiﬁcation are reported in Table 1.Using MFCC i-vectors, our proposed method outper-formed all the baselines with all three classiﬁers. Also byusing MFCC+SD i-vectors, the results of artist classiﬁca-tion from all 3 classiﬁers improved. The best artist classiﬁ-cation performance is achieved using MFCC+SD i-vectorsand a DA classiﬁer yielding 11 percentage point improve-ment in accuracy and 10 percentage point improvement inF1 compared to the best known results among all the base-lines.MethodFeat. Acc% F1% Pr% Rec%BLGMM 20mfcc 55.90 55.18 58.74 58.20BLsparse BOF 67.50 n/a n/a n/aBLsign 15mfcc 71.50 n/a n/a n/aBLmultiv 13mfcc 74.30 74.79 n/a n/aivPLDA 20mfcc 83.30 82.58 83.72 84.02iv3NN20mfcc 82.43 81.70 83.06 83.03ivDA20mfcc 83.36 82.67 84.07 83.78ivPLDA 20mfcc+sd 85.27 84.58 85.87 85.68iv3NN 20mfcc+sd 83.68 83.05 84.10 84.55ivDA20mfcc+sd 85.45 84.59 85.80 85.68Table 1: Artist classiﬁcation results fordifferent methodson theartist20dataset.4.7 ResourcesWe used the same resources as reported in Section 3.7. Inaddition, we used the PLDA implementation from MSRIdentity Toolbox [28] and LDA from drtoolbox [33].5. CONCLUSIONIn this paper, we propose an i-vector based factor analy-sis (FA) technique to extract song-level features for unsu-pervised music similarity estimation and supervised artistclassiﬁcation. In music similarity estimation, our methodachieved the performance of state-of-the-art methods byusing only timbral information. In artist classiﬁcation, ourmethod was evaluated on a variety of classiﬁers and provedto yield stable results. The proposed method outperformedall the baselines on the artist20 dataset and improved thebest known artist classiﬁcation measures among baselines.To the best of our knowledge, our results are the highestartist classiﬁcation results published so far for the artist20dataset.6. ACKNOWLEDGMENTWe would like to acknowledge the tremendous help byDan Ellis from Columbia University, who shared the de-tails of his work, which enabled us to reproduce his ex-periment results. Thanks to Pavel Kuksa from Universityof Pennsylvania for sharing the details of his work withus. Also thank to Jan Schl¨uter from OFAI for his helpwith music similarity baselines. And at the end, we ap-preciate helpful suggestions of Rainer Kelz and Filip Ko-rzeniowski from Johannes Kepler University of Linz to thiswork. This work was supported by the EU-FP7 projectno.601166 (PHENICX), and by the Austrian Science Fund(FWF) under grants TRP307-N23 and Z159.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 5597. REFERENCES[1]Mohamad Hasan Bahari, Rahim Saeidi, David Van Leeuwen,et al. Accent recognition using i-vector, gaussian mean su-pervector and gaussian posterior probability supervector forspontaneous telephone speech. InICASSP. IEEE, 2013.[2]Pierre-Michel Bousquet, Driss Matrouf, and Jean-Franc ¸oisBonastre. Intersession compensation and scoring methodsin the i-vectors space for speaker recognition. InINTER-SPEECH, 2011.[3]Lukas Burget, Oldrich Plchot, Sandro Cumani, Ondrej Glem-bek, Pavel Matejka, and Niko Brummer. Discriminativelytrained probabilistic lda for speaker veriﬁcation. InICASSP.IEEE, 2011.[4]Chuan Cao and Ming Li. Thinkits submissions for mirex2009audio music classiﬁcation and similarity tasks. InMIREX.Citeseer, 2009.[5]Christophe Charbuillet, Damien Tardieu, Geoffroy Peeters,et al. Gmm-supervector for content based music similarity. InDAFx, Paris, France, 2011.[6]Najim Dehak.Discriminative and generative approaches forlong-and short-term speaker characteristics modeling: ap-plication to speaker veriﬁcation. Ecole de Technologie Su-perieure, 2009.[7]Najim Dehak, Patrick Kenny, R´eda Dehak, Pierre Du-mouchel, and Pierre Ouellet. Front-end factor analysis forspeaker veriﬁcation.Audio, Speech, and Language Process-ing, IEEE Transactions on, 2011.[8]Najim Dehak, Pedro A Torres-Carrasquillo, Douglas AReynolds, and Reda Dehak. Language recognition via i-vectors and dimensionality reduction. InINTERSPEECH.Citeseer, 2011.[9]H Eghbal-zadeh, M Schedl, and G Widmer. Timbral model-ing for music artist recognition using i-vectors. InEUSIPCO,2015.[10]Benjamin Elizalde, Howard Lei, and Gerald Friedland. Ani-vector representation of acoustic environments for audio-based video event detection on user generated content. InISM. IEEE, 2013.[11]Daniel PW Ellis. PLP and RASTA (and MFCC, and inver-sion) in Matlab, 2005. online web resource.[12]Daniel PW Ellis. Classifying music audio with timbral andchroma features. InISMIR, 2007.[13]Daniel Garcia-Romero and Carol Y Espy-Wilson. Analysis ofi-vector length normalization in speaker recognition systems.InINTERSPEECH, 2011.[14]Jean-Luc Gauvain and Chin-Hui Lee. Maximum a posterioriestimation for multivariate gaussian mixture observations ofmarkov chains.Speech and audio processing, IEEE Transac-tions on, 1994.[15]Ian Jolliffe.Principal component analysis. Wiley Online Li-brary, 2002.[16]Patrick Kenny. Joint factor analysis of speaker and sessionvariability: Theory and algorithms.CRIM, Montreal,(Report)CRIM-06/08-13, 2005.[17]Patrick Kenny, Gilles Boulianne, and Pierre Dumouchel.Eigenvoice modeling with sparse training data.Speech andAudio Processing, IEEE Transactions on, 2005.[18]Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and PierreDumouchel. Speaker and session variability in gmm-basedspeaker veriﬁcation.Audio, Speech, and Language Process-ing, IEEE Transactions on, 2007.[19]Patrick Kenny, Mohamed Mihoubi, and Pierre Dumouchel.New map estimators for speaker recognition. InINTER-SPEECH, 2003.[20]Patrick Kenny, Pierre Ouellet, Najim Dehak, Vishwa Gupta,and Pierre Dumouchel. A study of interspeaker variability inspeaker veriﬁcation.Audio, Speech, and Language Process-ing, IEEE Transactions on, 2008.[21]Pavel P. Kuksa. Efﬁcient multivariate kernels for sequenceclassiﬁcation.CoRR, 2014.[22]Driss Matrouf, Nicolas Scheffer, Benoit GB Fauve, and Jean-Franc ¸ois Bonastre. A straightforward and efﬁcient implemen-tation of the factor analysis model for speaker veriﬁcation. InINTERSPEECH, 2007.[23]Sebastian Mika, Gunnar Ratsch, Jason Weston, BernhardScholkopf, and Klaus-Robert Muller. Fisher discriminantanalysis with kernels. InSignal Processing Society WorkshopNeural Networks for Signal Processing, 1999.[24]Elias Pampalk. Audio-based music similarity and retrieval:Combining a spectral similarity model with information ex-tracted from ﬂuctuation patterns. InISMIR, 2006.[25]Tim Pohle and Dominik Schnitzer. Striving for an improvedaudio similarity measure.Music information retrieval evalu-ation exchange, 2007.[26]Tim Pohle, Dominik Schnitzer, Markus Schedl, Peter Knees,and Gerhard Widmer. On rhythm and general music similar-ity. InISMIR, 2009.[27]Simon JD Prince and James H Elder. Probabilistic linear dis-criminant analysis for inferences about identity. InComputerVision, ICCV. IEEE, 2007.[28]Seyed Omid Sadjadi, Malcolm Slaney, and Larry Heck. Msridentity toolbox-a matlab toolbox for speaker recognition re-search.Microsoft CSRC, 2013.[29]Klaus Seyerlehner, Gerhard Widmer, and Tim Pohle. Fusingblock-level features for music similarity estimation. InDAFx,2010.[30]Sajad Shirali-Shahreza, Hassan Abolhassani, and M Shirali-Shahreza. Fast and scalable system for automatic artist identi-ﬁcation.Consumer Electronics, IEEE Transactions on, 2009.[31]Li Su and Yi-Hsuan Yang. Sparse modeling for artist identi-ﬁcation: Exploiting phase information and vocal separation.InISMIR, 2013.[32]George Tzanetakis and Perry Cook. Musical genre classiﬁ-cation of audio signals.Speech and Audio Processing, IEEEtransactions on, 2002.[33]LJP Van der Maaten, EO Postma, and HJ van den Herik. Mat-lab toolbox for dimensionality reduction.MICC, 2007.[34]Rui Xia and Yang Liu. Using i-vector space model for emo-tion recognition. InINTERSPEECH, 2012.560 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Quantifying Lexical Novelty in Song Lyrics.",
        "author": [
            "Robert J. Ellis",
            "Zhe Xing",
            "Jiakun Fang",
            "Ye Wang 0007"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417577",
        "url": "https://doi.org/10.5281/zenodo.1417577",
        "ee": "https://zenodo.org/records/1417577/files/EllisXFW15.pdf",
        "abstract": "Novelty is an important psychological construct that affects both perceptual and behavioral processes.  Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/).  A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words.  An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist.  Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists.  An affirmative and highly consistent answer was found in both cases.  These results highlight the potential utility of the LNS as a feature for MIR.",
        "zenodo_id": 1417577,
        "dblp_key": "conf/ismir/EllisXFW15",
        "keywords": [
            "Novelty",
            "psychological",
            "construct",
            "perceptual",
            "processes",
            "lexical",
            "novelty",
            "score",
            "lyrics",
            "corpus"
        ],
        "content": "QUANTIFYING LEXICAL NOVELTY IN SONG LYRICS Robert J Ellis, Zhe Xing, Jiakun Fang, and Ye Wang School of Computing, National University of Singapore {robjellis, xingzhe.cs}@gmail.com; {fangjiak, wangye}@comp.nus.edu.sg  ABSTRACT Novelty is an important psychological construct that affects both perceptual and behavioral processes.  Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/).  A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words.  An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist.  Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists.  An affirmative and highly consistent answer was found in both cases.  These results highlight the potential utility of the LNS as a feature for MIR.  1. INTRODUCTION From 2004 through 2013, both U.S. and worldwide Google searches for “lyrics” outnumbered searches for “games”, “news”, and “weather”, as computed by Google Trends1. The importance listeners place on song lyrics has motivated several explorations for translating a song’s lyric into queryable features: for example, by topic [1], genre [2], or mood [3–6]. All these cited examples have incorporated word frequency information: as a key statistic in the computational process.  The inverse document frequency (IDF) statistic, for example, is used to identify “diagnostic” terms within a lyric that can be further related to a particular topic, genre, or mood.   .                                            In the present paper, we propose using IDF information to derive a quantifiable and queryable feature of song lyrics: a lexical novelty score (LNS).  “Lexical” refers to properties of individual words, as distinct from their grammatical function or syntactical arrangement.  Our LNS is based, in part, on the trimean of IDFs associated with the set of unique words in a lyric.  The greater the number of statistically infrequent (i.e.,                                                            1 http://www.google.com/trends/      explore#q=lyrics,+games,+news,+weather&cmpt “novel”) words in a lyric, the higher its IDF trimean.     .      Why might such a quantification of lexical novelty be useful?  A number of answers emerge from the domains of psycholinguistics and psychology. The novelty or unfamiliarity of a stimulus has a direct bearing on basic cognitive processing.  For example, words that are statistically infrequent (i.e., have a high IDF) are more difficult to perceive, recognize, and recall than more commonly encountered words (e.g., [7–9]).  The affective response associated with perceiving novelty, however, is a more complex process.  Berlyne [10], for example, extended a classic inverted–U relationship first proposed by Wilhelm Wundt [11]: a peak level of perceived pleasantness or “hedonic value” for moderately complex or moderately novel stimuli, and decreased liking for very simple/familiar or very complex/novel stimuli. Such a relationship has been documented across numerous classes of stimuli, including music [12], and can be further  modified by an individual perceiver’s preferences for novelty—a construct that has informed influential models of human personality [13].            .                           Taken together, this evidence suggests that a method to quantify novelty/complexity within song lyrics might find application within the domain of personalized music recommendation.  First, generated playlists could be optimized with the “right” level of lyric complexity based upon the user’s activity state (e.g., exercising, commuting, or intense studying) [14–15].  Second, by computing the level of lexical novelty in a user’s favorite artist, novel artists with a similar level of lexical novelty could be recommended. Third, songs with lyrics that are “not-too-simple” or “not-too-complex” could be used in paradigms supporting native or second language learning [16–17] or language recovery after brain injury [18].    2. RELATED WORK Methods for translating a text into a single summary statistic or “grade” have been employed in a number of domains. Mid-twentieth century development of readability metrics—designed to quantify the ease with which a written text could be comprehended—emerged from the human factors literature (for a review and some context, see [19]), and have come to be widely applied in a variety of natural language settings [20–21]. Readability metrics are simple mathematical transformations of a text’s orthographic features: letter  © Robert J Ellis, Zhe Xing, Jiakun Fang, and Ye Wang.  Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Robert J Ellis, Zhe Xing, Jiakun Fang, and Ye Wang. “Quantifying lexical novelty in song lyrics”, 16th International Society for Music Information Retrieval Conference, 2015. \n694count, syllable count, word count, and sentence count. 2 Word frequency information is only rarely incorporated into readability calculations; for example, tallying the number of “difficult” [22] or “unfamiliar” [23] words (as defined by a set of 3000 words), or the “average grade level” of words (from a set of 100,000 words) [24].                         By contrast, word frequency information is fun-damental to vector space model approaches for text retrieval [25]. The process by which candidate documents are matched to a particular query often involves the use of term frequency–inverse document frequency (tf*idf) calculations [26–27].  A useful summary statistic across a set of query terms is their average IDF [28–29].  It should be noted that our proposed idea of a lexical novelty score is distinct from prior uses of tf*idf for novelty detection [30], which attempts to detect new information in a “stream” of documents. It is also distinct from acoustic novelty audio segmentation methods based on changes in temporal self-similarity [31]. \n 3. DATASETS AND PREPROCESSING STEPS 3.1 Word frequency tables The two key data sources for the proposed IDF-based lyric LNS are a lyrics corpus and a look-up table of document frequencies (DFs).  Word frequencies could be estimated from the lyrics corpus itself.  However, such an operation could create a dependency between IDFs and resultant LNSs—or at least necessitate retabulating word frequencies and IDFs as more lyrics were added to the corpus.  Word frequency values derived from an independent corpus were thus desirable.           .                    Numerous tables of word frequencies have been published (reviewed in [32]): for example, the Brown corpus (1 million words), British national corpus (100M words), Corpus of Contemporary American English (450M words), and Google Books corpus (155 billion words of American English).  In the present work, we selected the use word frequency tables derived from the SUBTLEXUS corpus [9]; a corpus of subtitle transcripts of 8388 American films and television programs. A list of 74,286 non-stemmed words5 (46.7M word instances in total) has been compiled, with DFs (from 1 to 8388) and corpus frequencies (from 1 to 2,134,713) tabulated for each word.  In addition to being fully and freely available6, SUBTLEXUS word frequencies have the appealing property of being derived from spoken source material, which may provide a closer match to the usage patterns in sung speech.  The IDF of the ith word in the SUBTLEXUS table was computed as log10(8388/DFi).                                                              2 For an illustration, www.readability-score.com 5 The following items in the SUBTLEXUS table were excluded from this       tally: ’d, ’s, ’m, ’t, ’ll, ’re, don, gonna, wanna, couldn, didn, doesn.   6 http://expsy.ugent.be/subtlexus/ 3.2 Lyrics corpus Next, we discuss the issue of an appropriate lyrics corpus.  The Million Song Dataset [33] is associated with a smaller lyrics corpus (237,662 lyrics)7, obtained in partnership with musiXmatch8. The bag-of-words format used to store each lyric, however, only references the 5000 most frequent word stems (the part of a word common to all its inflectional and derivational variants; for example, “government”, “governor”, “governing”, and “governance” are all stemmed to “govern”) as computed by the Porter2 stemmer 9.  (In fact, the 5000-item stemmed word list contains more than 1000 non-English stems when cross-checked with a 266,447-item dictionary derived from existing dictionary lists10.) The manner in which word variants are used during communication, however, conveys rich information about the communicator’s language facility [35–37].  Furthermore, word variants can have very different IDFs; in SUBTLEXUS, the four variants of “govern” listed above have IDFs of .74, 1.32, 2.58, and 3.22, respectively. As a result, a LNS derived from word stems would ignore potentially “diagnostic” differences in lexical usage between lyrics..                           .            For this reason, a new lyrics corpus was obtained via special arrangement with LyricFind11, a leading provider of legal lyrics licensing and retrieval.  In addition to the lyrics corpus itself, metadata comprising performing artist, album, lyricist, and license territory information for each lyric was made available.  The full corpus contained 587,103 lyrics.  After restricting the corpus to lyrics with United States copyright, 389,029 lyrics remained. 3.3 Lyrics pre-processing A multi-step procedure converted each lyric from its original text format into a bag-of-words format.  Each lyric was first “cleaned” using a series of hand-crafted transformation rules (i.e., x o xc): (1) splitting of compounds (e.g., half-heartedohalf hearted) or removal of hyphenated prefixes (e.g., mis-heardo misheard); (2) elimination of contractions (e.g., you’ll’veoyou will have; gonnao going to); (3) restoration of dropped initial (e.g., ’tilountil), interior (e.g., ne’eronever), or final (tryin’otrying) letters; (4) abbreviation elimination (e.g., mr.omister); (5) adjustment of British English to American English spellings (e.g., colourocolor)12; and (6) correction of 4264 commonly misspelled words13.                  Each lyric was then cross-checked with the 266,447-item dictionary. Lyrics in which fewer than 80% of                                                            7 http://labrosa.ee.columbia.edu/millionsong/musixmatch 8  www.musixmatch.com 9  http://snowball.tartarus.org 10  http://wordlist.aspell.net 11 www.lyricfind.com 12 Using http://wordlist.aspell.net/varcon 13 Using http://en.wikipedia.org/wiki/Wikipedia:Lists_of_         common_misspellings/For_machines Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 695unique words could be matched to the dictionary were eliminated; 360,919 lyrics remained.  After removing duplicate lyrics, the final corpus contained 275,905 lyrics.                               A total of 67.6M word instances was present in this set of songs, with 66,975 unique words.  Of these items, 51,832 were an exact match with the 74,286-item SUBTLEXUS word list; this accounted for  99.7% of the 67.6M word instances in the lyrics corpus.  IDFs derived from the SUBTLEXUS corpus were generally in agreement with IDFs derived from the LyricFind corpus itself (Pearson’s r = .837). \n 4. LYRIC-LEVEL LEXICAL NOVELTY SCORE   4.1 First-pass LNS: IDFTM A first-pass LNS for a lyric was defined as the trimean of SUBTLEXUS-derived IDFs (IDFTM) associated with the set of w unique words in that lyric (wu):              ,                           (1) where Q1, Q2, and Q3 are the first quartile, second quartile (median), and third quartile, respectively. The trimean is an outlier-robust measure of central tendency [37]. For example, a low-frequency variant of a common word not “corrected” during the cleaning step would yield a spuriously high IDF; the trimean (but not the arithmetic mean) is robust to this kind of outlier.              .                             The higher a lyric’s IDFTM, the more low-frequency (i.e., novel) words it contains.  Figure 1 plots IDFTM as a function of wu for all 275,905 lyrics (using log10 scaling on the x-axis). Observed wu values range from 12 to 895.                                                    A few illustrative cases are highlighted on Figure 1.   The highest IDFTM (= 2.3212; LyricID 1142131; marked c) is “Yakko’s World” from the cartoon Animaniacs. (Example text: “There’s Syria, Lebanon, Israel, Jordan / Both Yemens, Kuwait, and Bahrain / The Netherlands, Luxembourg, Belgium, and Portugal / France, England, Denmark, and Spain”.) The lowest IDFTM (= 0.0016; LyricID 53540; marked d) is “You Don’t Know” by Killing Heidi. (“I can see you / And you don’t have a clue / Of what you’ve done / And there’s no reason / For what you’ve done to / Done to my ...”.)                     .                           Lyric e (LyricID 786811; “One More Bite of the Apple” by Neil Diamond) has the same wu as c (= 153), but a much lower IDFTM (= 0.0804), indicating lower lexical novelty: “Been away from you for much too long / Been away but now I’m back where I belong / Leave while I was gone away / But I do just fine”.  Lyric f (LyricID 78427; “Revelation” by Blood) has nearly the same wu as d (24 vs. 23) but a much higher IDFTM  (= 1.5454), indicating higher lexical novelty (“Writhe and shiver in agonies undreamable / Wriggling and gasping / Anticipating the tumescent / Revelation of the flesh”).      Finally, cases g (LyricID 335431; “The Tear Drop” by Armand van Helden) and h (LyricID 1452671; “Sunshine” by Bow Wow) both have wu = 195, but very different IDFTM values (1.8464 vs. 0.1378).  High lexical novelty is present in g (“A buttress breaching barrage blast / A tumultuous thunderbolt tirade / An annihilating eradicating avalanche of absolute absolution”); low lexical novelty is present h (“What you hear me talkin’ ’bout / You just ain’t gonna find out / Walkin’ around in somebody’s club / Now she’s sayin’ her house”). \n                                       Figure 1. Scatter plot of unique words (wu) versus IDFTM. \n       A clear relationship is visible between wu and IDFTM (Pearson’s r = .477): as wu increases, so does the minimum observed IDFTM.  This can be attributed to statistical patterns present in natural language.  Specifically, a small number of words account for a large percentage of total word instances; a phenomenon which follows Zipf’s law (e.g., [38]). In the SUBTLEXUS corpus, for example, 10 words (you, i, the, to, a, it, that, and, of, what) account for 24.3% of all 46.7M word instances. Because IDFTM is derived from the set of unique words in a lyric, as wu increases, so too must the number of lower-frequency (i.e., higher-IDF) words, causing the IDFTM to rise.  Such a pattern would manifest for any L-estimator (mean, median, midhinge, etc.).   .                 A more informative statistic could be obtained if the IDFTM of a lyric with w unique words were compared against a large distribution of simulated IDFTM values obtained from repeated random draws of w unique words from the set of lyrics that had more than w unique words.  This procedure is formalized next. 4.2 Scaling IDFTM: Monte Carlo simulations Consider two lyrics, one with IDFTM = 0.25 and wu = 50, and the other with IDFTM = 0.5 and wu = 200.  Two scaling distributions of simulated IDFTM values were created using a 10,000-iteration procedure.  To create the scaling distribution for wu = 50, on each iteration, a single lyric was randomly selected from the set of 239,225 lyrics with wu > 50.  The full set of words in that lyric (including repeated words) was randomly permuted, the first 50 unique words pulled, and the IDFTM of those words was taken.  To create the scaling distribution for  wu = 200, a similar procedure was performed, using the set of 15,124 lyrics with wu > 200.  Figure 2 presents an \n696 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015empirical cumulative distribution function (ECDF) of these two scaling distributions.  The “scaled IDFTM” is defined as the percentile P (i.e., the y-axis value on the ECDF, multiplied by 100) where x = IDFTM.  In the above example, when IDFTM = 0.25 and wu = 50, P = 85.8.  By contrast, when IDFTM = 0.25 and wu = 200, P = 10.3.  This can be interpreted as follows: with a longer lyric (wu = 200 vs. wu = 50), the likelihood of obtaining an IDFTM > 0.5 by chance (i.e., 100 – P) is much higher (89.7% vs. 14.2%); that is, it is a less novel occurrence.                    .  \n  Figure 2. ECDFs of simulated IDFTM values for two representative values of wu.   To scale the full set of IDFTM values, the above simulation was modified in the following manner.  First, the range of target wu values was capped at 275, thus reserving 5228 lyrics with wu > 275 to create the scaling distribution for wu = 275. Second, the set of target  P-values was defined as .01 to 99.99 in increments of .01. Third, to accurately estimate the “tails” of P (i.e., values near 0 and 100), many more Monte Carlo iterations at each wu are needed; thus, the number of iterations was increased from 10,000 to 1 million.                            .                                    Figure 3 highlights the results of this simulation.  A representative set of “iso-probability curves” resulting from the Monte Carlo simulation are superimposed on the scatter plot first shown in Figure 1.  A given curve plots the Pth percentile (where P = {.01, 10, 50, 90, 99, 99.9, 99.99}) of simulated IDFTM values across the set of wu values.  IDFP ≈ 0 indicates very low lexical novelty, IDFP ≈ 50 indicates moderate lexical novelty, and IDFP ≈ 100 indicates very high lexical novelty.   As expected, the iso-probability curves for low P-values mirror the pattern in the real data: higher IDFTM values as wu increases.     .                   \n   Figure 3. Representative iso-probability curves.  4.3 Second-pass LNS: Percentiles Each IDFTM was mapped to its corresponding IDFP using nearest neighbor interpolation.  IDFTM values below  P = .01 (n = 80) or above P = 99.99 (n = 52) were set to IDFP = 0 or IDFP = 100, respectively.  Figure 4 plots IDFP as a function of wu for the final set of 270,677 unique lyrics. The relationship between wu and IDFP  (r = –.106) is much weaker than between wu and IDFTM  (r = .477).  IDFP values were roughly uniform (mean = 44.29; standard deviation = 29.70; skewness = 0.255).                               \nFigure 4. Percentile-transformed novelty scores (IDFP) as a function of wu.                                         .  Figure 5 presents an ECDF of both IDFTM and IDFP, highlighting the six lyrics discussed earlier. Compared to IDFTM, IDFP better differentiates lyrics with high lexical novelty (cases c, f, and g) versus low novelty (cases d, e, and h).                                 .  \n                                   Figure 5. ECDFs for IDFTM  (upper) and IDFP  (lower). 5. ARTIST-LEVEL LEXICAL NOVELTY Having defined IDFP as the lyric-level LNS, we next sought to characterize lexical novelty at the artist level. Artist information was obtained via LyricFind ArtistIDs, which are distinct for different combinations of individual artists.  To increase the specificity of an artist-level score, lyrics recorded by multiple artists (e.g., holiday songs,  jazz standards) were excluded.  Artists associated with fewer than 10 unique lyrics (λu) were deemed to have an insufficient catalog, and were ignored.  A final set of 5884 artists (a total of 216,072 lyrics) remained.  The trimean of each artist’s λu IDFP values was then taken as a simple and intuitive artist-level LNS.                           .                      Figure 6 plots artist-level LNS as a function of λu; no correlation was present between them (r = –.009.)  The distribution of values (mean = 43.49; standard deviation = 21.20) was roughly symmetrical (skewness = .459).   \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 697  Figure 6.  Artist-level LNS as a function of λu.          .  6. BILLBOARD MAGAZINE “TOP” LISTS Having derived both a lyric-level and an artist-level point estimate of lexical novelty, any number of subsequent analyses may be performed.  As an illustrative example, we turn to Billboard Magazine’s 2013 ranking of the “All-Time Top 100 Songs”14 and “All-Time Top 100 Artists”15. Rankings were calculated based on overall success on the magazine’s “Hot 100” chart, a weekly ranking of the top 100 popular music singles in the United States, published since August 1958 [40–41].   .                                                        The Top Songs list was determined by Billboard using an inverse point system, with time spent in the #1 position of each weekly chart weighted highest, and time spent in the #100 position weighted lowest.  Of the 100 songs on the list, 95 were present in the LyricFind corpus. Lyrics for the remaining five were queried from metrolyrics.com and processed as described in Section 4.                                                                 The Top Artists list was determined by Billboard by aggregating all the songs which charted over the course of each artist’s career.  Of the 100 artists, 98 were among the set of 5884 artists with a valid artist-level LNS; the other two artists had λu < 10.   7.  EXPERIMENTAL HYPOTHESES Two hypotheses were examined, both driven by the assumption that high lexical novelty is less likely to be “chart-worthy”.  Specifically, we predicted that both lyric-level and artist-level LNSs would be lower in the set of Top Songs and Top Artists relative to “non-top” songs and artists in the LyricFind corpus.              .                                 Statistical significance was assessed using a nonparametric two-sample Mann–Whitney (MW) test.  A special sampling procedure was implemented to counter-act the bias towards smaller p-values when comparing large samples [41].  On each of 10,000 iterations, two samples were drawn. The first sample was always the n Top Song or Top Artist LNSs, and the second sample was a random draw (without replacement) of n LNSs                                                            14 billboard.com/articles/list/2155531/the-hot-100-all-time-top-songs 15 billboard.com/articles/columns/chart-beat/5557800/hot-100-55th-       anniversary-by-the-numbers-top-100-artists-most-no from the remaining set of songs or artists (where n is 100 for songs and 98 for artists).  The distribution of Z-values from the 10,000 MW tests indicates the strength of the difference between the samples: the more negative it falls, the greater our confidence that lexical novelty is systematically lower in the set of Billboard items.       .                          .     .                 .                                                     8. EXPERIMENTAL RESULTS 8.1 Billboard Top Songs analysis Figure 7a shows the ECDFs of lyric-level LNS for the set of 100 Top Songs and the remaining 270,582 songs.  They are markedly different: LNSs for the Top Songs are “pulled” towards zero, indicating reduced lexical novelty in this set.  Consistent with this, the distribution of Z-values (Figure 7b) is strongly negative: 98.4% of MW tests result were significant at p < .05, 89.9% at p < .01, and 61.1% at p < .001.  No correlation was present between Billboard’s song ranking and a song’s LNS  (r = –.148, p = .140).                                                  .                         . \n Figure 7. (a.) ECDFs of LNSs for the 100 Top Songs and the remaining 270,582 songs in the corpus. (b.) ECDF of Z-values from the 10,000 MW tests.  8.2 Billboard Top Artists analysis Figure 8a shows the ECDFs of artist-level LNS for the set of 98 Top Artists and the remaining 5786 artists.  As with the Top Songs, LNSs for the Top Artists are pulled towards zero, indicating reduced lexical novelty (i.e., lower IDFP trimean values) for the set of 98 Top Artists.  The Z-value distribution (Figure 8b) is more negative than in the Top Songs analysis: 99.3% of tests were significant at p < .001, 95.8% at p < .0001, and 85.5% at p < .00001.  As with the Top Songs, no correlation was present between Billboard’s artist ranking and artist-level LNS (r = –.059, p = .564).                          .                              .  \n Figure 8. (a.) ECDFs of artist-level LNSs for the 98 Top Artists and the remaining 5786 artists in the corpus.  (b.) ECDF of Z-values from the 10,000 MW tests. \n698 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20159. DISCUSSION 9.1 Summary Stimulus novelty has influence over perception, memory, and affective response.  Here, we define a lexical novelty score (LNS) for song lyrics.  The LNS is derived from the inverse document frequency of all unique words in a lyric, and is scaled with respect to the number of unique words.  Higher-order scores can be easily defined at the level of artists, albums, or genres, creating additional features for filtering operations or similarity assessments.       Although the construct validity of the LNS must be assessed by future user studies (see Section 9.2), a first-pass validation was performed by comparing LNSs associated with Billboard Magazine’s “official” lists of the 100 Top Songs and 100 Top Artists with LNSs from random sets of songs and artists.  Lexical novelty was significantly lower—in a highly consistent way—for items on the Billboard lists, supporting the broad hypothesis that moderate stimulus novelty is preferred over high stimulus novelty [10–12].                .      The absence of  any significant correlation between Billboard’s actual ranking of items on the Top Songs or Top Artists lists and our lexical novelty score should not be read as a “strike” against either Billboard’s methodology or our own.  Rather, we regarded these lists as a source of well-known independent data that enabled us to make a priori predictions concerning differences in lexical novelty at the set (rather than the item) level. 6.2 Future directions The present analyses of Billboard’s “Top 100” lists are but one of many analyses that could be performed.  Further work could explore differences in lexical novelty among genres, subgenres, or styles (using external sources of metadata, such as Echo Nest16, Rovi17 or 7digital18); changes in lexical novelty over time (e.g., using lyric copyright date information); or correlations between lexical novelty and other performance-related metrics, such as RIAA-tracked album sales19.      .                                                           A potential refinement of our LNS calculation would be to make it sensitive to parts of speech.  Numerous English words can serve as multiple parts of speech, often with very different word frequencies.  Capturing these usage patterns would, in principle, increase the sensitivity of the LNS. A revised SUBTLEXUS table of document frequencies is available that tallies parts-of-speech [42], as are widely used parts-of-speech taggers20,21, making this modification tractable.                                 .    .                                                                                              16 http://developer.echonest.com/docs/v4 17 http://developer.rovicorp.com 18 http://developer.7digital.com/ 19 https://www.riaa.com/goldandplatinumdata.php 20 http://ucrel.lancs.ac.uk/claws/trial.html 21 http://nlp.stanford.edu/software/lex-parser.shtml      Finally, user studies must be performed to answer whether the proposed LNS itself has construct validity.  These studies should evaluate, for example, whether lyrics with a high LNS yield longer reaction times and increased effort during a sentence processing task (e.g., as in [43]); or whether lyrics with a moderate LNS receive higher ratings of pleasure or liking than lyrics with either a low or a high LNS.                                         .        Together, these future steps will enhance the utility of the LNS in the context of music retrieval and recommendation applications.                               .  10. DATA SET AVAILABILITY With gratitude to LyricFind, much of the data presented here—lyrics in bag-of-words format; lyric, artist, and album IDs; and lyric- and artist-level lexical novelty scores—is made publically available for the first time: www.smcnus.org/lyrics/.  11. ACKNOWLEDGEMENT Kind thanks to Roy Hennig, Director of Sales at LyricFind, for making this collaboration possible.  This project was funded by the National Research Foundation (NRF) and managed through the multi-agency Interactive & Digital Media Programme Office (IDMPO) hosted by the Media Development Authority of Singapore (MDA) under Centre of Social Media Innovations for Communities (COSMIC).  12. REFERECNES [1] F. Kleedorfer, P. Knees, and T. Pohle, “Oh Oh Oh Whoah! Towards Automatic Topic Detection In Song Lyrics.,” in Proc. Int. Symp. Music Inf. Retrieval, 2008, pp. 287–292. [2] R. Mayer, R. Neumayer, and A. Rauber, “Rhyme and Style Features for Musical Genre Classification by Song Lyrics.,” in Proc. Int. Symp. Music Inf. Retrieval, 2008, pp. 337–342. [3] C. Laurier, J. Grivolla, and P. Herrera, “Multimodal music mood classification using audio and lyrics,” in Proc. 7th Int. Conf. Mach. Learn. Appl., 2008, pp. 688–693. [4] X. Hu, J. S. Downie, and A. F. Ehmann, “Lyric text mining in music mood classification,” Am. Music, vol. 183, no. 5,049, pp. 2–209, 2009. [5] M. Van Zaanen and P. Kanters, “Automatic Mood Classification Using TF*IDF Based on Lyrics.,” in Proc. Int. Symp. Music Inf. Retrieval, 2010, pp. 75–80. [6] X. Wang, X. Chen, D. Yang, and Y. Wu, “Music Emotion Classification of Chinese Songs based on Lyrics Using TF*IDF and Rhyme.,” in Proc. Int. Symp. Music Inf. Retrieval, 2011, pp. 765–770.  Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 699[7] K. Rayner and S. A. Duffy, “Lexical complexity and fixation times in reading,” Mem. Cognit., vol. 14, no. 3, pp. 191–201, 1986. [8] F. Meunier and J. Segui, “Frequency effects in auditory word recognition,” J. Mem. Lang., vol. 41, no. 3, pp. 327–344, 1999. [9] M. Brysbaert and B. New, “Moving beyond Kučera and Francis,” Behav. Res. Methods, vol. 41, no. 4, pp. 977–990, 2009. [10] D. E. Berlyne, Aesthetics and Psychobiology. New York: Appleton-Century-Crofts, 1971. [11] W. Sluckin, A. M. Colman, and D. J. Hargreaves, “Liking words as a function of the experienced frequency of their occurrence,” Br. J. Psychol., vol. 71, no. 1, pp. 163–169, 1980. [12] A. C. North and D. J. Hargreaves, “Subjective complexity, familiarity, and liking for popular music,” Psychomusicology, vol. 14, no. 1, pp. 77–93, 1995. [13] M. Zuckerman, Behavioral expressions and biosocial bases of sensation seeking. Cambridge university press, 1994. [14] M. Kaminskas and F. Ricci, “Contextual music information retrieval and recommendation,” Comput. Sci. Rev., vol. 6, no. 2, pp. 89–119, 2012. [15] X. Wang, D. Rosenblum, and Y. Wang, “Context-aware mobile music recommendation for daily activities,” in Proc. 20th ACM Int. Conf. Multimedia, 2012, pp. 99–108. [16] C. F. Mora, “Foreign language acquisition and melody singing,” ELT J., vol. 54, no. 2, pp. 146–152, 2000. [17] K. R. Paquette and S. A. Rieg, “Using music to support the literacy development of young English language learners,” Early Child. Educ. J., vol. 36, no. 3, pp. 227–232, 2008. [18] C. Y. Wan and G. Schlaug, “Music making as a tool for promoting brain plasticity across the life span,” The Neuroscientist, vol. 16, no. 5, pp. 566–577, 2010. [19] G. R. Klare, “The measurement of readability,” ACM J. Comput. Doc. JCD, vol. 24, no. 3, pp. 107–121, 2000. [20] T. G. Gunning, “The role of readability in today’s classrooms,” Top. Lang. Disord., vol. 23, no. 3, pp. 175–189, 2003. [21] G. K. Berland, M. N. Elliott, L. S. Morales, J. I. Algazy, R. L. Kravitz, M. S. Broder, and others, “Health information on the Internet: accessibility, quality, and readability in English and Spanish,” J. Am. Med. Assoc., vol. 285, no. 20, pp. 2612–2621, 2001. [22] J. S. Chall and E. Dale, Readability revisited: The new Dale-Chall readability formula. Brookline Books, 1995. [23] G. Spache, “A new readability formula for primary-grade reading materials,” Elem. Sch. J., pp. 410–413, 1953. [24] M. Milone, “Development of the ATOS readability formula.” Renaissance Learning, 2014. [25] G. Salton, A. Wong, and C.-S. Yang, “A vector space model for automatic indexing,” Commun. ACM, vol. 18, no. 11, pp. 613–620, 1975. [26] A. Aizawa, “An information-theoretic perspective of tf–idf measures,” Inf. Process. Manag., vol. 39, no. 1, pp. 45–65, 2003. [27] J. Ramos, “Using tf-idf to determine word relevance in document queries,” in Proc. 1st Inst. Conf. Machine Learning, 2003. [28] S. Cronen-Townsend, Y. Zhou, and W. B. Croft, “Predicting query performance,” in Proc. 25th ACM SIGIR, 2002, pp. 299–306. [29] L. Zheng, S. Wang, Z. Liu, and Q. Tian, “Lp-norm idf for large scale image search,” in IEEE CVPR, 2013, pp. 1626–1633. [30] J. Allan, C. Wade, and Alvaro Bolivar, “Retrieval and novelty detection at the sentence level,” in Proc. 26th ACM SIGIR, 2003, pp. 314–321. [31] J. Foote, “Automatic audio segmentation using a measure of audio novelty,” in IEEE Int. Conf. Multimedia Expo., 2000, vol. 1, pp. 452–455. [32] T. McEnery and A. Hardie, Corpus linguistics: Method, theory and practice. Cambridge University Press, 2011. [33] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, “The million song dataset,” in Proc. 12th Int. Symp. Music Inf. Retrieval, 2011, pp. 591–596. [34] A. Caramazza, A. Laudanna, and C. Romani, “Lexical access and inflectional morphology,” Cognition, vol. 28, no. 3, pp. 297–332, 1988. [35] G. Yu, “Lexical diversity in writing and speaking task performances,” Appl. Linguist., vol. 31, no. 2, pp. 236–259, 2010. [36] A. Xanthos, S. Laaha, S. Gillis, U. Stephany, A. Aksu-Koç, A. Christofidou, and others, “On the role of morphological richness in the early development of noun and verb inflection,” First Lang., p. 0142723711409976, 2011. [37] J. W. Tukey, Exploratory data analysis. Reading, MA: Addison-Wesley, 1977. [38] M. E. Newman, “Power laws, Pareto distributions and Zipf’s law,” Contemp. Phys., vol. 46, no. 5, pp. 323–351, 2005. [39] E. T. Bradlow and P. S. Fader, “A Bayesian lifetime model for the ‘Hot 100’ Billboard songs,” J. Am. Stat. Assoc., vol. 96, no. 454, pp. 368–381, 2001. [40] D. E. Giles, “Survival of the hippest: life at the top of the hot 100,” Appl. Econ., vol. 39, no. 15, pp. 1877–1887, 2007. [41] R. M. Royall, “The effect of sample size on the meaning of significance tests,” Am. Stat., vol. 40, no. 4, pp. 313–315, 1986. [42] M. Brysbaert, B. New, and E. Keuleers, “Adding part-of-speech information to the SUBTLEX-US word frequencies,” Behav. Res. Methods, vol. 44, no. 4, pp. 991–997, 2012. [43] A. D. Friederici, “Towards a neural basis of auditory sentence processing,” Trends Cogn. Sci., vol. 6, no. 2, pp. 78–84, 2002.   700 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Improving Visualization of High-Dimensional Music Similarity Spaces.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416472",
        "url": "https://doi.org/10.5281/zenodo.1416472",
        "ee": "https://zenodo.org/records/1416472/files/Flexer15.pdf",
        "abstract": "Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that ap- pear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phe- nomenon impacts three popular approaches to compute two- dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces be- fore low dimensional projection.",
        "zenodo_id": 1416472,
        "dblp_key": "conf/ismir/Flexer15",
        "keywords": [
            "Visualizations",
            "Music databases",
            "Intuitive exploration",
            "Music similarity spaces",
            "Negative impact",
            "Hubs and anti-hubs",
            "Empirical study",
            "Three popular approaches",
            "Two-dimensional visualizations",
            "High-dimensional spaces"
        ],
        "content": "IMPROVING VISUALIZATION OF HIGH-DIMENSIONAL MUSICSIMILARITY SPACESArthur FlexerAustrian Research Institute for Artiﬁcial Intelligencearthur.flexer@ofai.atABSTRACTVisualizations of music databases are a popular form ofinterface allowing intuitive exploration of music catalogs.They are often based on lower dimensional projections ofhigh dimensional music similarity spaces. Such similarityspaces have already been shown to be negatively impactedby so-called hubs and anti-hubs. These are points that ap-pear very close or very far to many other data points dueto a problem of measuring distances in high-dimensionalspaces. We present an empirical study on how this phe-nomenon impacts three popular approaches to compute two-dimensional visualizations of music databases. We alsoshow how the negative impact of hubs and anti-hubs canbe reduced by re-scaling the high dimensional spaces be-fore low dimensional projection.1. INTRODUCTIONVisualization via low dimensional projections is one wayto produce interfaces that allow navigation and access tomusic data sets. A very popular and inﬂuential approach isthe islands-of-music metaphor [14], where representationsof similar music form islands on a two-dimensional dis-play. Numerous variations of this approach have been pub-lished within the music information retrieval (MIR) com-munity (see e.g. [5, 9, 16, 24]). A recent trend towardsmore holistic MIR approaches [18, 23] including humancomputer interaction aspects is likely to increase interestin visualization in the near future. State-of-the-art visu-alization algorithms are said to be able to visualize high-dimensional data [28]. Precisely for such high-dimensionaldata a new aspect of the curse of dimensionality, the socalled hubness, has been discovered and described withinthe MIR community [1, 8]. This paper investigates the im-pact of hubness on visualization of high-dimensional mu-sic similarity spaces. In an empirical evaluation of threemethods for dimensionality reduction the negative impactof hubness is explored and it is shown how re-scaling ofthe similarity spaces as a preprocessing step can greatlyimprove the visualizations.c\u0000Arthur Flexer.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Arthur Flexer. “Improving visual-ization of high-dimensional music similarity spaces”, 16th InternationalSociety for Music Information Retrieval Conference, 2015.2. RELATED WORKHubness is a general problem of learning in high-dimensionalspace which has been discovered in MIR [1], but then gainedattention in a general machine learning context where it hasbeen discussed as a new aspect of the curse of dimension-ality [15,20]. Hub objects appear very close to many otherdata objects and anti-hubs very far from most other dataobjects. The effect is related to the phenomenon of concen-tration of distances and has been shown to have a negativeimpact on many tasks including classiﬁcation [15], near-est neighbor based recommendation [3] and retrieval [21],outlier detection [15] and clustering [19, 26].Visualization of music similarity spaces via low dimen-sional projections has a long tradition within MIR. Startingfrom the inﬂuential islands-of-music approach [14,16], nu-merous extensions and variations have been developed (seee.g. [5, 9, 24]). Although different methods for dimension-ality reduction have been explored, the most popular ap-proach seems to be self-organizing maps [10]. Despite thepopularity of these interfaces based on lower dimensionalprojections, it has not yet been clariﬁed how hubness in-ﬂuences these visualizations. To the best of our knowl-edge, there is only a single publication concerned withthe impact of hubness on visualization [6]. In an anal-ysis of dimensionality reduction of three audio databasesto two dimensions using multidimensional scaling, the au-thors show that projected data tends to be concentrated ina single large cluster centered around the largest hub.It is important to note that simple dimensionality reduc-tion does not reduce hubness. On the contrary it has beenshown that only projections to very few dimensions, wellbelow the intrinsic dimensionality of a data set, are ableto reduce hubness, but at the cost of a loss of distance in-formation [15]. On the other hand, results on re-scalingmethods to reduce hubness [20] show that it is possibleto decrease hubness without changing the intrinsic dimen-sionality and therefore the information content of the data.Thus a good approach to visualization of high dimensionaldata might be to ﬁrst re-scale to reduce hubness withoutchanging the intrinsic dimensionality, and then to apply di-mensionality reduction to the re-scaled data.3. DATAFor our experiments we used two standard music databases:the “GTZAN” collection consisting ofN= 1000audiotracks (each 30 s length) evenly spread over ten music gen-547res [27]; the “ISMIR2004”1collection containingN=1458tracks of six genres, with full-length audio being avail-able and exhibiting a highly imbalanced genre distributionwith classical music comprising almost half of the tracks.We decided to compute timbre information from the au-dio, since this is an integral part of many MIR systems andat the same time has already been shown to be suscepti-ble to hubness [3]. Every track is divided into overlappingframes for which 20 MFCCs are being computed whichare modeled via a single Gaussian with full covariancematrix. To compute a distance value between two Gaus-sians the symmetrized Kullback-Leibler (SKL) divergenceis used [11]. This results inN⇥Ndistance matricesDIandDGfor the ISMIR and GTZAN data sets. Please notethat SKL is symmetric and non-negative, but does not ful-ﬁll the triangle inequality and therefore is not a full metric.4. METHODSIn what follows we present three methods for dimension-ality reduction (TSNE, SAMMON, SOM) and two meth-ods to re-scale distance matrices in order to reduce hub-ness (MP, SNN). In Section 5 we will use MP and SNNas a preprocessing step before dimensionality reduction.This gives nine different combination of methods to com-pare: TSNE, MP TSNE, SNN TSNE, SOM, MP SOM,SNN SOM, SAMMON, MP SAMMON, SNN SAMMON.But ﬁrst we present evaluation indices that will be usedto measure the performance achieved in original and re-scaled data spaces.4.1 Evaluation measuresHubness (Sn): To characterize the strength of the hubnessphenomenon in a data set we use the so called hubnessmeasure [15]. This is based on then-occurrences of pointsx, which is the number of timesxoccurs in then-nearestneighbor lists of all other objects in the collection. Hub-ness is then deﬁned as the skewness of the distribution ofn-occurrencesOn:Sn=E⇥(On\u0000µOn)3⇤\u00003On.(1)A data set having high hubness produces few hub ob-jects with very highn-occurrence and many anti-hubs withn-occurrence of zero. This makes the distribution ofn-occurrences skewed with positive skewness indicating highhubness. Previous results [22] show that values above1.4are problematic.Nearest neighbor overlap (Lk): To quantify the degree towhich neighborhood relations are preserved we computethe overlap between nearest neighbor lists in the high di-mensional input space (NN(x)) and the low-dimensionaloutput space (NN(ˆx)):Lk=1NXi=1...N|NN(x)\\NN(ˆx)|/k.(2)1http://ismir2004.ismir.net/genre_contest/index.htmNearest neighbor classiﬁcation accuracy (Ck): We re-port the k-nearest neighbor (kNN) classiﬁcation accuracyCkusing leave-one-out cross-validation, where classiﬁca-tion is performed via a majority vote among the k nearestneighbors, with the class of the nearest neighbor used forbreaking ties.4.2 Dimensionality reductionDimensionality reduction algorithms try to map high di-mensional input data to lower dimensional output spaceswhile preserving information of the topology of the inputspace, i.e. preserving similarities or similarity orderings.All three methods used in this study are based on optimiza-tion algorithms that are initiated randomly and thereforecan give different solutions for different initializations. Allresults reported in Section 5 are based on single runs sincerepeated runs have shown that all three methods give com-parable solutions even for different initializations. Pleasenote that the original and re-scaled distance matricesDIandDGare normalized to have a smallest value of 0 and alargest value of 1 and, if necessary, changed to similaritiesbefore dimensionality reduction.t-Stochastic Neighbor Embedding (TSNE): A particu-larly successful algorithm for dimensionality reduction ist-SNE [28]. It ﬁrst converts similarities of high dimen-sional pointsxiandxjinto conditional probabilitiespj|ithatxiandxjare neighbors given a Gaussian probabilitydensity estimate centered atxi. It computes a similar prob-abilityqj|ifor the low dimensional counterpartsyiandyjbased on a Student-t density estimate. The mapping to thelower dimension is then achieved by minimizing the sumof the Kullback-Leibler divergences over all data points us-ing gradient descent:C=XiKL(Pi||Qi)=XiXjpj|ilogpj|iqj|i(3)We used the implementation by Laurens van der Maaten2that accepts similarity matrices as input (function “tsnep”)using standard settings as provided by the software and1000 iterations for all experiments.Sammon mapping (SAMMON): Sammon mapping [17]does dimensionality reduction by minimizing the follow-ing via steepest descent:1PN\u00001i=0Pj<id(xi,xj)N\u00001Xi=0Xj<i(d(xi,xj)\u0000ˆd(ˆxi,ˆxj))2d(xi,xj)(4)whereˆd(ˆxi,ˆxj)is the distance in the output space thatcorresponds to the distanced(xi,xj)in the input space andNis the number of points to be mapped. We used the im-plementation from the SOM Toolbox3for all experimentswith standard settings and 100 iterations.Self Organizing Map (SOM): The SOM [10] is an unsu-pervised neural network that visualizes high dimensional2http://lvdmaaten.github.io/tsne/3http://www.cis.hut.fi/projects/somtoolbox/548 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015data by mapping it to a two dimensional map grid. Datapoints that are similar in the original high dimensional spaceare mapped onto locations close to each other on the grid.In essence the SOM consists of an ordered set of so calledmap unitsri, each of which is assigned a reference vector(or model vector)miin the high dimensional input space.In an iterative learning procedure the model vectorsmiareadapted to the input data, very much like cluster centersin a k-means clustering procedure. The main difference isthat model vectors corresponding to neighboring map unitsriare adapted together, based on a neighborhood weight-ing function. This yields a topological organization of themodel vectorsmiin the two dimensional output space.For all our experiments we use SOMs with40⇥40output maps, thereby ensuring that we always have moremodel vectors than input vectors which is advantageousfor using SOM for visualization (see [2] for more on theusage of SOMs for clustering and visualization). We usethe NETLAB [12] SOM implementation with standard set-tings for the learning parameters (initial neighborhood sizeof 8 shrunk to 1 during an ordering phase lasting 50 itera-tions, followed by 400 iterations of a convergence phase).Since SOMs need data vectors and not distance matricesas input data, we use the full rows of the distance matri-ces as inputs (see e.g. [9, 13] for more detail or [22] for acriticism of this rather crude but standard approach).4.3 Reducing hubnessWe introduce the two methods we will apply to reduce hub-ness by using each method on the whole distance matrixand computing re-scaled distances. Both methods aim atrepairing asymmetric nearest neighbor relations which area consequence of the presence of hubs. A hubyis the near-est neighbor ofx, but the nearest neighbor of the hubyisanother pointa(a6=x). This is because hubs are by deﬁ-nition nearest neighbors to very many data points but onlyone data point can be the nearest neighbor to a hub.Mutual Proximity (MP): MP reinterprets the original dis-tance space so that two objects sharing similar nearest neigh-bors are more closely tied to each other, while two ob-jects with dissimilar neighborhoods are repelled from eachother. This is done by transforming the distance of two ob-jects into a mutual proximity in terms of their distributionof distances. It was shown that by using this mutual reinter-pretation of distances hubness is decisively reduced, whilethe intrinsic dimensionality of the data stays the same [20].To compute MP, we assume that the distancesDx,i=1..Nfrom an objectxto all other objects in our data set followa certain probability distribution, thus any distanceDx,ycan be reinterpreted as the probability ofybeing the near-est neighbor ofx, given their distanceDx,yand the proba-bility distributionP(X). In this work we assume that thedistancesDx,i=1..Nfollow a Gaussian distribution. MPis deﬁned as the probability thatyis the nearest neighborofxgivenP(X)andxis the nearest neighbor ofygivenP(Y):MP(Dx,y)=P(X>Dx,y\\Y> Dy,x).(5)GAUSSIAN TSNEGAUSSIAN SNN TSNE\nFigure 1. Maps obtained for Gaussian artiﬁcial data viaTSNE (left) and SNN TSNE (right). Hubs are shown asgreen circles and anti-hubs as red diamonds.Shared Nearest Neighbors (SNN): SNN [7] uses the neigh-borhood information to help enforce pairwise stability. SNNis computed as a set intersection of thek-nearest neighborlistsNNof two objectsx,y:SNN(x, y)=|NN(x)\\NN(y)|/k.(6)This way SNN strictly strengthens symmetric nearestneighbor relations which in turn should also manifest it-self in a reduction of hubness. We use SNN withk= 50because this already yields hubness valuesS5(see Sec-tion 4.1 ) below 1 for both ISMIR and GTZAN.5. RESULTSBefore we present our results using the ISMIR and GTZANdata sets we give a ﬁrst illustration based on artiﬁcial data.We sampled 1000 data points from a 50-dimensional Gaus-sian distribution and used Euclidean distance to compute adistance matrix. The hubnessS5of this data set is2.95.Similar to other work [20], we deﬁned anti-hubs as pointswith aO5=0, i.e. points never appearing in any nearestneighbor list of size 5. Hubs are points withO5>25, i.e.points that appear more than ﬁve times as expected. Thisdeﬁnition of hubs and anti-hubs is used for all results inthis paper and hubs and anti-hubs are always computed inthe high-dimensional spaces. Figure 1 plots two dimen-sional results obtained using TSNE alone (left plot) andSNN plus TSNE (right plot). As can be seen, TSNE mapsall hubs (green circles) to the center of the points and mapsall anti-hubs (red diamonds) to the edges. The right plotshows that SNN TSNE is able to map hubs and anti-hubsmuch more evenly across the whole set of mapped points.The hubnessS5of the re-scaled distance space after appli-cation of SNN is0.81.Next we present the visualization results obtained forthe ISMIR data set using different combinations of TSNE,SOM, SAMMON and MP and SNN in Figure 2. ThehubnessS5of the ISMIR data set is3.94. Re-scaling re-duces this value to1.25for MP and0.89for SNN. Hubsare again shown as green circles and anti-hubs as red dia-monds. When using TSNE (top row), we again see that thehub points are mapped to the center of the visualization andanti-hubs appearing all over the plot but also at the edgesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 549TSNEMP TSNESNN TSNE\nSOMMP SOMSNN SOM\nSAMMONMP SAMMONSNN SAMMON\nFigure 2. Visualization of ISMIR data set using different combinations of TSNE, SOM, SAMMON and MP and SNN.Hubs are shown as green circles and anti-hubs as red diamonds.where no other data points are mapped to. When usingthe combination MP TSNE, this situation shows only littleimprovement with some anti-hubs still being mapped to ar-eas where no other points can be found. Hub points are stillmapped to the center of the plot. When using the combi-nation SNN TSNE the result seems to be much improved,the plot showing much more structure and the hubs andanti-hubs no longer conﬁned to the center or edges. Look-ing at the results obtained for SOM (middle row), we canagain see that the hub points are mapped to the center ofthe plot whereas the anti-hubs are conﬁned to the left andbottom edge areas. When using MP SOM or even betterSNN SOM, hubs and anti-hubs are much more scatteredacross the whole plots. When using SAMMON (bottomrow), we can see that the visualization is heavily distortedwith a few data points being mapped far away from therest of the data. When using MP SAMMON, this distor-tion is no longer visible but both hubs and anti-hubs aremapped to the more central parts of the plots. Only SNNSAMMON seems to be able to map anti-hubs more or lessevenly across the plot, with hubs being mapped closer tothe edges. Overall the combination SNN TSNE seems toyield the best visualization results. Results are similar forGTZAN, but are not depicted for lack of space.To quantify the success in visualization, we computethe nearest neighbor overlapLkbetween high- and low-dimensional spaces for TSNE, SOM and SAMMON whichis shown in Figure 3 for both ISMIR (top row) and GTZAN(bottom row) data sets. In all six plots solid lines showresults when using dimensionality reduction only (TSNE,SOM or SAMMON), dash-dotted lines give results whenMP is used for preprocessing, dashed lines when SNN isused for preprocessing. The overlapLkis computed fora range ofk=5...500plotted on the x-axis to quantifypreservation of local and more global neighborhoods. Wecan see that for all three dimensionality reduction meth-ods and over the full range ofk, preprocessing via MPand SNN is able to increase the overlapLk. The onlyexception is SAMMON when applied to ISMIR, whereSNN gives worse results than using no preprocessing fork>200. Preprocessing with SNN is superior to usingMP in combination with TSNE and SOM. In combinationwith SAMMON, MP works a little better than SNN. Over-all TSNE performs better than SOM, which is again betterthan SAMMON. Again the combination SNN TSNE givesthe best results of all.Next we present a more detailed analysis of the near-est neighbor overlap results by concentrating onLkwithk= 50since this is where the difference in performanceis largest. In Figure 4 we give separate results for “all”data points, “hub”, “anti-hub” and “normal” (i.e. not hubsor anti-hubs) data points as bar plots for TSNE, SOM andSAMMON. Every bar plot shows a black bar for dimen-sionality reduction only, a gray bar for results when MP isused for preprocessing, a white bar when SNN is used. Forall three dimensionality reduction algorithms,L50is high-550 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20150200400020406080100TSNE − ISMIR\n0200400020406080100SOM − ISMIR\n0200400020406080100SAMMON − ISMIR\n0200400020406080100TSNE − GTZAN\n0200400020406080100SOM − GTZAN\n0200400020406080100SAMMON − GTZANFigure 3. Overlap of nearest neighbors in high and lowdimensions in percent (y-axis) vs. number of neighbors (x-axis) for ISMIR (top) and GTZAN (bottom). Plots givenfor TSNE, SOM and SAMMON with solid lines for usingdimensionality reduction only, dash-dotted lines for usingMP for preprocessing, dashed lines when SNN is used.ISMIRGTZAN-mpsnn-mpsnnorig71.478.176.561.767.861.6tsne62.663.970.439.141.052.8som65.265.069.240.437.749.7sammon47.051.346.316.127.325.3Table 1. Genre classiﬁcation accuracy in percent using50-nearest neighbor classiﬁcation for ISMIR and GTZANdata sets different combinations of TSNE, SOM, SAM-MON and MP and SNN as well as for the original (orig)high dimensional data space.est for hub points and worst for anti-hub points, with nor-mal points somewhere in between. Applying either MP orSNN as preprocessing generally increasesL50for all dif-ferent kinds of points, but also makesL50for hubs, anti-hubs and normal points perform much more comparable.Anti-hub points now perform almost as well as all otherpoints. The only exception is again SAMMON, which gen-erally performs very poorly and where SNN is not able toimprove the overall situation.As a further analysis of our visualization results, wegive kNN genre classiﬁcation4accuracy resultsC50whenusing different combinations of TSNE, SOM, SAMMONand MP and SNN as well as for the original high dimen-sional data space in Table 1. As for the original inputspace (row “orig”), MP and SNN increaseC50for IS-MIR, but only MP for GTZAN. This is in line with previ-ous comparison of MP and SNN [4]. Classiﬁcation resultsfor low-dimensional spaces are of course lower than thoseachieved on the original input spaces since any dimension-ality reduction incurs some loss of information. But for4Please note that we are of course aware of the controversial role ofgenre classiﬁcation in MIR, especially in the context of GTZAN [25], butthat accuracy only serves as a further illustration of results in this context.allhubantinorm020406080TSNE − ISMIR\nallhubantinorm020406080SOM − ISMIR\nallhubantinorm020406080SAMMON − ISMIR\nallhubantinorm020406080TSNE − GTZAN\nallhubantinorm020406080SOM − GTZAN\nallhubantinorm020406080SAMMON − GTZANFigure 4. Analysis of overlap of 50 nearest neighbors inhigh and low dimensions in percent (y-axis) vs. type ofdata points (x-axis: all, hub, anti-hub, normal) for ISMIR(top) and GTZAN (bottom). Bar plots are given for TSNE,SOM and SAMMON with black bars showing results fordimensionality reduction only, gray bars for using MP forpreprocessing, white bars when SNN is used.both TSNE and SOM on both data sets, SNN is able to in-creaseC50, which is additional indication that SNN is thepreprocessing method to prefer. Results for SAMMON aregenerally very low and rather mixed.Finally we show visualization results of the ISMIR dataset when using TSNE as well as SNN TSNE, which is thebest performing combination, in Figure 5 with differentgenres given in different colors. The color coding is as fol-lows: classical - black, jazzblues - blue, rockpop - red,world - green, metalpunk - yellow, electronic - cyan. Al-though it is hard to verbalize the information contained ina visualization, it seems apparent that the result for SNNTSNE (right plot) shows much more structure than the re-sult for TSNE only. This enables a more detailed pic-ture of the overlap between “classical” (black) and “world”(green) music. Also the position of genre “jazzblues”(blue) is now clearer between “classical/world” and theremaining three genres. Also “electronic” (cyan) musicseems to be a little more apart from “rockpop” (red) and“metalpunk” (yellow). Results for GTZAN, which con-sists of music from ten genres, are similar in tendency butnot shown for space considerations.6. DISCUSSIONSumming up the results presented in the previous section,we like to state that all three visualization methods areaffected by the hubness problem. Looking at the visual-izations, checking the amount of overlap between nearestneighbors in high and low dimensions for hub, anti-huband normal points makes it clear that there is a problem fordimensionality reduction of data with high values of hub-ness. It is also evident that preprocessing with either MP orSNN can help in this situation. Especially the combinationProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 551TSNE − ISMIRSNN TSNE − ISMIR\nFigure 5. Visualization of ISMIR data set using TSNE (left) and SNN TSNE (right) with color coded genres (see Section 5).of SNN and TSNE yields very improved results. Althoughthis paper used only one particular approach to computemusic similarity, previous work [3] has made it clear thatmany different approaches are affected by hubness. Forthe dimensionality reduction algorithms, we basically usedstandard settings since attempts to adjust parameters didnot really improve results. But of course a more rigorousparameter tuning should be part of future research.One particularity of the music similarity spaces usedin this work is the fact they are based on Gaussian mod-els of timbre information and therefore only distances be-tween models are available but not vector representations.Therefore all dimensionality reduction methods need to beable to deal with distance/similarity information as input.Whereas this is natural for SAMMON, it already consti-tutes a problem for SOM. We resorted to the standard butsomewhat crude approach to use the full rows of the dis-tance matrices as input vectors (with length equal 1000 or1485 for GTZAN and ISMIR). But there already exists asuperior approach [22] of directly using Gaussian modelsas inputs to SOMs and it would be very interesting to re-search the impact of hubness on this version of SOM. ForTSNE, we were able to use a variant (“tsnep”) that is ableto deal directly with similarity matrices. But as stated bythe authors [28], this should only be done if “these simi-larities can be interpreted as conditional probabilities” asexplained in Section 4.2. A theoretic examination as towhat extent MP and SNN fulﬁll this requirement will bepart of future work. When TSNE is being used with in-put vectors instead of a similarity matrix, the width of theGaussian probability densities are adapted locally accord-ing to a so-called perplexity term. This is an important partof the algorithm which is missing in case it is used with asimilarity matrix directly. It is an interesting research ques-tion whether this local adaption in itself is able to countersome of the problems due to hubness. But this can only bestudied if vectors are available as input to TSNE.As has already been noted in Section 3, the music simi-larity spaces are based on symmetric Kullback-Leibler di-vergences which do not fulﬁll the triangle inequality andtherefore do not exhibit all aspects of a true metric. Thereexists an extension of t-SNE [29] which uses multiple mapsto visualize non-metric similarities. Even more interesting,this extension is motivated with the notion of data pointswhich show high centrality, i.e. points which are similar tovery many other data points. In contrast to the discussionof hub points, such central points are in this case not seenas problematic but as a special challenge for a visualiza-tion algorithm. It would therefore be very interesting tostudy and compare these central and hub points and to ap-ply the t-SNE algorithm for non-metric similarities to datasets with high hubness.7. CONCLUSIONWe presented the ﬁrst substantial empirical evaluation ofthe impact of hubness on visualization of high-dimensionalmusic similarity spaces. Analyzing three popular methodsfor dimensionality reduction applied to two standard musicdata sets, we were able to show that hubs and anti-hubs dis-tort the lower dimensional representations. Generally hubsare mapped to the central parts of plots and anti-hubs usu-ally to the edges. We were able to show that preprocessingwith methods that have been designed to reduce hubnesscan greatly improve this situation. This results in visual-ization where hubs and anti-hubs are no longer mappedto peculiar locations, which also gives improved preserva-tion of neighborhood information when mapping to lowdimensions. Particularly a combination of preprocessingvia “shared nearest neighbors” followed by dimensionalityreduction via “t-SNE” proved to be most successful. Thisapproach could therefore be used as the core technologyfor future visualization interfaces to music catalogs.Acknowledgements:This work was supported by the Aus-trian Science Fund (FWF, grant P27082).552 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Aucouturier, J.-J., Pachet F.: Improving Timbre Simi-larity: How high is the sky?,Journal of Negative Re-sults in Speech and Audio Sciences, 1(1), 2004.[2]Flexer A.: On the Use of Self-organizing Maps forClustering and Visualization,Intelligent Data Analy-sis, V ol. 5, Number 5, pp. 373-384, 2001.[3]Flexer A., Schnitzer D., Schl¨uter J.: A MIREX meta-analysis of hubness in audio music similarity,Proceed-ings of the 13th International Society for Music Infor-mation Retrieval Conference, 2012.[4]Flexer A., Schnitzer D.: Can Shared Nearest Neigh-bors Reduce Hubness in High-Dimensional Spaces?,Proceedings of 1st International Workshop on HighDimensional Data Mining (HDM), IEEE InternationalConference on Data Mining, 2013.[5]Gasser M., Flexer A.: FM4 Soundpark: Audio-basedMusic Recommendation in Everyday Use,Proc. of the6th Sound and Music Computing Conference, 2009.[6]Gasser M., Flexer A., Schnitzer D.: Hubs and Or-phans - an Explorative Approach,Proceedings of the7th Sound and Music Computing Conference, 2010.[7]Jarvis R., Patrick E.A.: Clustering using a similaritymeasure based on shared near neighbors,IEEE Trans-actions on Computers, vol. 22, pp. 10251034, 1973.[8]Karydis I., Radovanovi´c M., Nanopoulos A., Ivanovi´cM.: Looking through the ”glass ceiling”: A conceptualframework for the problems of spectral similarity,Pro-ceedings of the 11th International Society for MusicInformation Retrieval Conference, pp. 267-272, 2010.[9]Knees P., Schedl M., Pohle T., Widmer G.: Exploringmusic collections in virtual landscapes,IEEE multime-dia, 14(3):4654, 2007.[10]Kohonen T.:Self-Organizing Maps, Springer, 2001.[11]Mandel M., Ellis D.: Song-level features and supportvector machines for music classiﬁcation,Proc. of the6th Int. Conf. on Music Information Retrieval, 2005.[12]Nabney I.:NETLAB: Algorithms for Pattern Recogni-tion, Springer Science & Business Media, 2002.[13]Pampalk E.:Computational models of music similarityand their application in music information retrieval,Doctoral dissertation, Vienna University of Technol-ogy, Austria, 2006.[14]Pampalk E., Dixon S., Widmer G.: Exploring musiccollections by browsing different views,Computer Mu-sic Journal, V ol. 28,No. 2, pp. 49-62, 2004.[15]Radovanovi´c M., Nanopoulos A., Ivanovi´c M.:Hubs in space: Popular nearest neighbors in high-dimensional data,Journal of Machine Learning Re-search, 11:2487-2531, 2010.[16]Rauber A., Fr¨uhwirth M.: Automatically Analyz-ing and Organizing Music Archives,Proceedings ofthe European Conference on Research and AdvancedTechnology for Digital Libraries, pp. 4-8, 2001.[17]Sammon J.W.: A nonlinear mapping for data structureanalysis,IEEE Transactions on Computers, vol. C-18,no. 5, pp. 401-409, 1969.[18]Schedl M., Flexer A., Urbano J.: The Neglected Userin Music Information Retrieval Research,Journal ofIntelligent Information Systems, 41(3), 523-539, 2013.[19]Schnitzer D., Flexer A.: The Unbalancing Effect ofHubs on K-medoids Clustering in High-DimensionalSpaces,Proceedings of the International Joint Confer-ence on Neural Networks (IJCNN), 2015.[20]Schnitzer D., Flexer A., Schedl M., Widmer G.: Localand Global Scaling Reduce Hubs in Space,Journal ofMachine Learning Research, 13:2871-2902, 2012.[21]Schnitzer D., Flexer A., Tomaˇsev N.: A Case for Hub-ness Removal in High-Dimensional Multimedia Re-trieval,Proceedings of the 36th European Conferenceon Information Retrieval (ECIR), 2014.[22]Schnitzer D., Flexer A., Widmer G., Gasser M.: Islandsof Gaussians: The Self Organizing Map and GaussianMusic Similarity Features,Proceedings of the EleventhInternational Society for Music Information RetrievalConference (ISMIR’10), 2010.[23]Serra X., Magas M., Benetos E., Chudy M., Dixon S.,Flexer A., Gomez E., Gouyon F., Herrera P., Jorda S.,Paytuvi O., Peeters G., Schl¨uter J., Vinet H., WidmerG.,Roadmap for Music Information ReSearch, PeetersG. (editor), 2013.[24]Stober S., N¨urnberger A.: MusicGalaxy - an adaptiveuser-interface for exploratory music retrieval,Proceed-ings of the 11th International Conference on Music In-formation Retrieval, 2010.[25]Sturm B. L.: Classiﬁcation accuracy is not enough,Journal of Intelligent Information Systems, 41(3), 371-406, 2103.[26]Tomaˇsev N., Radovanovi´c M., Mladeni´c D., Ivanovi´cM.: The Role of Hubness in Clustering High-dimensional Data,IEEE Transactions on Knowledgeand Data Engineering, V olume 26, Issue 3, 2013.[27]Tzanetakis G., Cook P.: Musical genre classiﬁcation ofaudio signals,IEEE Transactions on Speech and AudioProcessing, V ol. 10, Issue 5, 293-302, 2002.[28]van der Maaten L.J.P., Hinton G.E.: Visualizing High-Dimensional Data Using t-SNE,Journal of MachineLearning Research, 9(Nov):2579-2605, 2008.[29]van der Maaten L.J.P., Hinton G.E.: Visualizing Non-Metric Similarities in Multiple Maps,Machine Learn-ing, 87(1):33-55, 2012.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 553"
    },
    {
        "title": "Correlating Extracted and Ground-Truth Harmonic Data in Music Retrieval Tasks.",
        "author": [
            "Dylan Freedman",
            "Eddie Kohler",
            "Hans Tutschku"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418111",
        "url": "https://doi.org/10.5281/zenodo.1418111",
        "ee": "https://zenodo.org/records/1418111/files/FreedmanKT15.pdf",
        "abstract": "We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground- truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use estab- lished chord identification algorithms to produce a cor- responding extracted chord label dataset. We imple- ment methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison param- eters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global opti- mization algorithm to stochastically search for the best pa- rameter set to maximize the rank correlation for two har- monic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and ex- tracted chord labels, and returns a rank correlation coeffi- cient. The second task queries the set of songs with fabri- cated chord progressions, ranks each query’s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing.",
        "zenodo_id": 1418111,
        "dblp_key": "conf/ismir/FreedmanKT15",
        "keywords": [
            "traditional music information retrieval",
            "well-chosen parameters",
            "computationally extracted chord annotations",
            "ground-truth annotations",
            "established chord identification algorithms",
            "corresponding extracted chord label dataset",
            "local alignment scores",
            "harmonic retrieval tasks",
            "chord progression comparison parameters",
            "black-box global optimization algorithm"
        ],
        "content": "CORRELATING EXTRACTED AND GROUND-TRUTH HARMONIC DATAIN MUSIC RETRIEVAL TASKSDylan FreedmanHarvard Universityfreedmand@post.harvard.eduEddie KohlerHarvard Universitykohler@seas.harvard.eduHans TutschkuHarvard Universitytutschku@fas.harvard.eduABSTRACTWe show that traditional music information retrievaltasks with well-chosen parameters perform similarly usingcomputationally extracted chord annotations and ground-truth annotations. Using a collection of Billboard songswith provided ground-truth chord labels, we use estab-lished chord identiﬁcation algorithms to produce a cor-responding extracted chord label dataset. We imple-ment methods to compare chord progressions between twosongs on the basis of their optimal local alignment scores.We create a set of chord progression comparison param-eters deﬁned by chord distance metrics, gap costs, andnormalization measures and run a black-box global opti-mization algorithm to stochastically search for the best pa-rameter set to maximize the rank correlation for two har-monic retrieval tasks across the ground-truth and extractedchord Billboard datasets. The ﬁrst task evaluates chordprogression similarity between all pairwise combinationsof songs, separately ranks results for ground-truth and ex-tracted chord labels, and returns a rank correlation coefﬁ-cient. The second task queries the set of songs with fabri-cated chord progressions, ranks each query’s results acrossground-truth and extracted chord labels, and returns rankcorrelations. The end results suggest that practical retrievalsystems can be constructed to work effectively without theguide of human ground-truthing.1. INTRODUCTIONComputational algorithms to approximate harmonic con-tent in a song typically output sequences of chord sym-bols which can be evaluated in terms of accuracy usingtheir recall compared to human-annotated chord progres-sions. Leading algorithms to extract chord progressionsfrom audio ﬁles have an accuracy of around 80% usingpopular Western music [12, 15]. Though these algorithmscan effectively match human chord-labeling intuitions, itis largely unexplored how these approximated chord anno-tations perform in typical music retrieval tasks relative tohuman annotations. In this paper, we propose a methodc\u0000Dylan Freedman, Eddie Kohler, Hans Tutschku.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Dylan Freedman, Eddie Kohler, HansTutschku. “Correlating Extracted and Ground-Truth Harmonic Data inMusic Retrieval Tasks”, 16th International Society for Music InformationRetrieval Conference, 2015.for evaluating the correlation of music retrieval task resultsacross extracted and ground-truth datasets correspondingto the same collection of songs. We limit the scope of ourexploration to chord labels and a few established similaritymethods, but the resulting procedure can be generalized toother musical features such as melody, rhythm, and mid-level representations.1.1 ContributionThis paper explores an alternative way to evaluate the efﬁ-cacy of algorithms to extract musical features from songs.Rather than simply calculate accuracy of computationallyextracted information relative to a reference, or ground-truth, dataset, we propose the use ofcorrelational metrics.Given a set of common music informatics retrieval tasks ona set of songs, correlational metrics quantify to what extentthe output results differ between two input sets: computa-tionally extracted and ground-truth features for the sameset of songs. Testing this system on a chord labeling algo-rithm, we design an alignment-based system to calculateharmonic similarity, devise two simple tasks—evaluatingsimilarity between pairs of songs and querying by chordprogression—and use a global optimization algorithm overthe system’s parameters to maximize the resulting correla-tional metric. The input datasets used and the design of thesystem are described in the following sections.2. CHORD PROGRESSION DATASETSThe selection of songs we consider in this paper is moti-vated by availability. In order to compare ground-truth andcomputationally extracted chord datasets, it is necessary tohave a set of song ﬁles, their corresponding ground-truthchord progression data, and a computer algorithm to ex-tract chords from the audio ﬁles and create an extractedchord dataset. The number of reliable research-backed hu-man ground-truth chord progression datasets is scarce, thusto maintain a separation of algorithm from data, it is use-ful to use a chord extraction algorithm that predates theground-truth dataset such that it could not have been trainedagainst any of its data.2.1 Chord ExtractionChordino1is an open-source chord extraction softwareprogram written by Matthias Mauch based on his winning1http://isophonics.net/nnls-chroma5612009 and 2010 MIREX chord estimation algorithm sub-missions [4, 15]. Chordino achieves an 80% chord symbolrecall and is still considered state-of-the-art [16]. Thoughan algorithm by Khadkevich [12] currently has the high-est chord symbol recall in the 2014 MIREX audio chordestimation task, there is no publicly released source codefor his work, whereas Chordino is available as aVAMP2plugin. The ground-truth dataset we use, as detailed in thefollowing subsection, was compiled in 2011. Unlike Khad-kevich’s chord identiﬁcation algorithm released in 2014,there is no possibility that Chordino could have been inﬂu-enced by or tested against this dataset, maintaining a purityof separation between data and system. Chordino is theonly chord extraction algorithm considered in this paperand is used with default settings.2.2 Ground-Truth DatasetTheMcGill Billboardannotations collected in [3] andfreely available online3are a state-of-the-art human-annotated chord dataset. The dataset is comprised ofover 1,000 songs sampled from different decades from the1950s to the early 1990s across different Billboard chartsfrom the United States “Hot 100”.4The researchers hiredmusic experts and professional jazz musicians to annotatethe songs randomly sampled from the Billboard charts.Each song was annotated twice to maintain a standard ofaccuracy. The resulting dataset is the most comprehensivecurrent ground-truth set of chord annotations and is usedin recent MIREX chord annotation competitions. Impor-tantly, the dataset postdates the Chordino chord extractionalgorithm, obviating the possibility of training bias.We were able to locate source audio for 529 of theMcGill songs. The corresponding ground-truth annota-tions for these 529 form theground-truth McGill dataset,orMcGillg. We extracted chord annotations for each ofthese 529 songs using Chordino with default settings, lead-ing to the creation of theextracted McGill dataset, orMcGille. To maintain a consistent chord alphabet, we sim-plify the harmonies used within the ground-truth dataset tomatch the closest chord within the alphabet of chord quali-ties used by Chordino. We preserve the root and bass notesof each chord and evaluate the closest simpliﬁed chord us-ing theHartemetric as described in Subsection 3.2.1.3. A HARMONIC SIMILARITY SYSTEM3.1 Smith-Waterman Local Alignment AlgorithmThe Smith-Waterman algorithm [17] is a dynamic program-ming algorithm that searches through two sequences ex-haustively, looking for the pair of subsequences with opti-mal similarity based on the cost of transforming one subse-quence into the other using three operators. The sequencesare composed of symbols within an alphabet⌃. The ﬁrstoperator,substitution, deﬁnes the cost of transforming any2http://www.vamp-plugins.org/3http://ddmal.music.mcgill.ca/billboard4http://www.billboard.com/charts/hot-100one symbol into any other and can be represented as a two-dimensional cost matrixS, where|S|=|⌃|⇥|⌃|. Thesecond and third operators,insertionanddeletion, quan-tify the cost of removing or adding a number of elementsat a certain position in one of the subsequences, resultingin gaps in the ﬁnal alignment. These two operators can berepresented concisely using a gap functionWthat assignscosts to gaps of speciﬁed lengths. Given a substitution ma-trixSwith a negative expected value but positive valuesfor similar input symbols, the Smith-Waterman algorithmeffectively isolates the strongest local regions of similaritycorresponding to the highest score.Smith-Waterman is useful in the context of comparingchord progressions as it has mechanisms to deal well withinexact data, using different gap costs and chord substitu-tion functions that compensate for small errors. To accountfor songs in different keys, the score that is returned can bethe maximum Smith-Waterman score of all twelve trans-positions of one sequence relative to the other. Assuminga ﬁxed substitution and gap function, letsw(s1,s2)returnthe Smith-Waterman score for two sequencess1ands2.Ift(s, i)is a transpose function that returns a transposedsequence given an input sequencesand a number of semi-tonesi, we can express our ﬁnal score as a similarity func-tionSW:SW(s1,s2)=11maxt=0sw(s1,t(s2,i))(1)Due to its advantages and research that supports its efﬁ-cacy [6, 10], the Smith-Waterman algorithm will be usedto compare chord progressions in this paper and quantifyharmonic similarity. There are downsides to the Smith-Waterman algorithm. In its current form, the score returnedreﬂects only the optimal local alignment and does not con-sider other strong subregions of similarity. Allali et al. [1]describe a process for constructing a 3-dimensional Smith-Waterman algorithm that can account for modulations to anew key signature mid-song. These adaptations leave roomfor future experimentation. This paper focuses on only re-turning one optimal local alignment score in the highestscoring transposition.3.2 ParametersWe chose a number of parameters to alter the nature ofthe Smith-Waterman algorithm used. These parameters areused with global optimization techniques to ﬁnd good set-tings such that ground-truth and extracted chord annota-tions perform similarly.3.2.1 Chord Distance FunctionsWe consider two chord distance metrics. Like Haas etal. [6], we use Lerdahl’s Tonal Pitch Space (TPS) [14] asa chord distance function to populate the substitution ma-trixS.TPSquantiﬁes the distance between two chordsrelative to the key signature of a song based on psycholog-ical qualities of human chord perception. We utilize thekey ﬁnding approach in [6] to establish the tonic and modeof each song we are considering and assume no transpo-sitions occur midsong. We additionally consider a metric562 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015proposed in Harte’s PhD thesis (Harte) [11] which quanti-ﬁes the fraction of similar pitch classes between two chordsover their cumulative set of pitch classes. IfPc(c)returnsthe set of pitch classes for a given chordcthis can be ex-pressed as:Harte(c1,c2)=|Pc(c1)\\Pc(c2)||Pc(c1)[Pc(c2)|(2)We denote a variableCdto correspond to which distancefunction is used,TPSorHarte.We additionally devise two parameters to scale and sub-tract the functionCdsuch that a cost matrixSpopulatedbyCdhas a negative expected value. We ﬁrst normal-ize the chord distance function to a value in [0,1], where0 indicates no similarity and 1 perfect similarity. InTPSthis requires a division by 13.mxrepresents the amountby which this normalized value is multiplied andmstheamount it is subtracted. We round this number to the near-est integer out of consideration for the Smith-Watermanimplementation we used. We arbitrarily only consideredintegers from 1 through 30 inclusive for bothmxandmsas values in this range seemed to achieve a good resolutionof scaled chord distance values. Finally,Scan be pop-ulated based on the ﬁnal scaled and subtracted value andchoice ofCdby iterating over all possible pairs of chordsin⌃.3.2.2 Gap CostsWe only consider one class of gap functions,afﬁne gapfunctions[9], which can be deﬁned by the following equa-tion for gaps of sizei\u00001:W(i)=\u0000gapopen\u0000gapextension·(i\u00001)(3)The two constantsgapopenandgapextensionare parametersthat can be changed to alter the penalty of the initial gapand following gaps in the sequence alignment, a potentiallyuseful feature to model an initial alignment gap being moreor less costly than subsequent gaps. In our implementation,we considered integer values ranging from 0 through 127inclusive forgapopenandgapextension.3.2.3 NormalizationA difﬁculty with comparing Smith-Waterman scores is thatthey tend to have a positive correlation with increased se-quence length. There are approaches to combat this effectusing statistical learning techniques [2]. We tested a sim-pler normalization metric that returns values in [0,1]:SWnorm(s1,s2)=SW(s1,s2)max{SW(s1,s1),SW(s2,s2)}(4)We devise a parameterCPdto represent the chord pro-gression similarity function used,SWorSWnorm.4. EXPERIMENTAL DESIGNThis paper tests how similarly common harmonic musicretrieval tasks perform using extracted chord data versus\nCollection ofMusical SongsGround-TruthChord AnnotationsExtractedChord Annotations Evaluate Fully ConnectedChordal ComparisonsQuery Harmonicallyby Random N-GramsHarmonic Music Information Retrieval TaskGround-TruthResult RankingsExtracted ChordResult RankingsRank CorrelationCoefficientOutput ResultBlack-boxOptimizationInitial Setof Parameters\nAcceptable Result?YesDecrease TemperatureAdjust ParametersNoMore Iterations?YesReturn OptimalSet of ParametersNoRun Retrieval TaskFigure 1. Flowchart of experimental design. This exper-iment requires a collection of songs with correspondingground-truth and computationally extracted chord annota-tions. These different chord datasets describing the samecollection of songs are fed into a harmonic retrieval task inisolated experiments, each producing a different result list.These result lists are ranked and correlated to return a cor-relational metric. Global optimization techniques searchfor maximum correlational metric scores by running manyiterations of the retrieval task with changing parametersbased on the performance of the correlational result rela-tive to previous iterations. The returned set of parametersrepresents an approximate optimal conﬁguration for min-imizing algorithmic differences between human and com-putationally extracted chord inputs.human-produced data. We primarily test two tasks acrossMcGillgandMcGilledatasets, rank both sets of results,and calculate a correlational metricP. We then run a black-box optimization strategy to approximate a maximum forthis correlational metric across the different parameters de-tailed in Section 3.2. This process is outlined in a ﬂowchartin Figure 1.4.1 Retrieval TasksThis subsection describes the two high-level tasks that formthe substance of the experiments. Inputted with the param-eters described in the previous section, these algorithmsperform chord progression comparisons over a collectionof songs using the harmonic similarity system previouslyoutlined to accomplish a common music retrieval objec-tive. The result is a collection of harmonic similarity scoresthat can be enumerated in an ordered fashion.4.1.1 Fully Connected Pairwise Harmonic ComparisonThis method (FCC), given parameters and a collection ofchord annotations, returns the harmonic similarity scoresfor every pairwise combination of songs. The algorithmproceeds in a well-ordered manner such that no pair ofsongs is iterated twice and results are consistently posi-tioned across two sets of chord annotations correspondingto the same collection of songs (e.g.McGillgandMcGille).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 5634.1.2 Query by N-gramThis retrieval task (QBN), given parameters and a collec-tion of chord annotations, involves comparing the collec-tion of annotations with random chord sequence queries tosimulate a basic search algorithm. Each query sequenceis compared with every song in the database, and a two-dimensional table of harmonic similarity scores is returned.We initially fabricate 100 query sequences, generatedrandomly within the alphabet of chord qualities⌃but usedconsistently across experiments and song collections. Outof the 100 query sequences, four groups of 25 query se-quences are generated with lengths of 4, 8, 16, and 32,respectively. Each query sequence is padded in length byrepeating itself such that the length is at least that of thelongest song in the collection so that the Smith-Watermanscores are not restricted by length of query sequence. Wechose this repetition of query sequences to imitate the repet-itive structure of musical songs and emphasize the cyclicnature of chord progression perception. For each of the 100query sequences,QBNcollects harmonic similarity scoresby comparing the query sequence against each of the songsin the given collection. The result is a two-dimensional ta-ble of harmonic similarity scores of size 100 by the lengthof the input collection of songs.4.2 Correlational Metrics4.2.1 Ranking and the Spearman Correlation CoefﬁcientThe ranking of a sequence is a mapping of every element ofthe sequence to its position in the sequence. This ranking isdone such that elements with the same value are assignedthe average index of their positions. Two equally sizedlists of rankingss1ands2can be assigned a correlationcoefﬁcient based on the Spearman correlation coefﬁcient(⇢) [7]. Ifnis the length of one of the ranked lists,⇢canbe calculated:⇢(s1,s2)=1\u00006Pni(s1i\u0000s2i)2n(n2\u00001)(5)⇢returns a number in [-1,1], with 1 indicating a perfectpositive correlation, -1 a perfect negative correlation, and0 no correlation.4.2.2 Calculating the Correlational MetricFor each of the two experimental tasks, we compare the re-sulting harmonic similarity scores across ground-truth andextracted chord annotations corresponding to the same setof songs,McGillgandMcGille.The resulting correlational metricPis calculated byranking the result lists forMcGillgandMcGilleseparatelyand returning a rank correlation between both resultinglists. ForFCC,Pis calculated by simply ranking eachresult list and returning the Spearman correlation coefﬁ-cient⇢between the two ranked lists. ForQBN, each resultlist fromMcGillgandMcGillefor each of the 100 queriesgenerated is ranked independently. The correlation coefﬁ-cients⇢for each of the 100 pairs of ranked lists is averagedand returned asP.Variable Notation ValuesSimilarity FunctionCPd{SW,SWnorm}Gap Open Costgapopen[0,128]Gap Extension Costgapextension[0,128]Chord DistanceCd{Harte,TPS}Distance Multipliermx[1,30]Distance Subtractorms[1,30]Table 1. Summary of experimental parameters.4.3 Global Optimization with Simulated AnnealingTo derive optimal parameters to maximize the correlationalmetricPacross tasks, we use a basic implementation of thesimulated annealing algorithm [5,13]. Letfrefer to one ofthe retrieval tasks that takes as input a set of parametersstand runs overMcGillgandMcGilleto return a correlationalmetricP.We try to stochastically search for parameters insttomaximizef(st). Simulated annealing takes a function,move(st), which returns a new states0tthat is slightlychanged fromstin a random manner.fis recalculatedwiths0tto see if the move was beneﬁcial. A temperaturevariableTstores acceptable deltas between old and newstates. If|f(s0t)\u0000f(st)|>T, the move is rejected andstis left unchanged; otherwise,sttakes on the new statevalue,s0t.Simulated annealing runs with a ﬁxed number of iter-ationsit. In each iteration, we performmove(st), andfollowing each iteration,Texponentially decreases. Thisgives the optimization process more exploratory freedomin initial stages whenTis higher. Afterititerations, theresultingstis an approximate maximum off. This algo-rithm is useful in search spaces that are sufﬁciently com-plex or large, such that exact optimization algorithms areinfeasible.4.3.1 ImplementationLetstcontain our parameters (see Table 1):{CPd,gapopen,gapextension,Cd,mx,ms}. Themovefunction represents atransition to a nearby state—as each variable instis aninteger, the jump must be discrete. Ourmoveimplementa-tion takes a random step following a normal distribution foreach variable in the state, rounding the result to the nearestinteger and ensuring the value falls within the bounds ofthe variable. The standard deviation of this random stepfor each variable is chosen to be13of that variable’s range.CPdandCd, taking two possible function values each, canbe treated as integer variables with values in{0,1}. If amove results in a combination of parameters such that theexpected value ofSis not negative or there are no positivevalues, the scaling and subtraction factorsmxandmsarerandomized again from their last values following the samenormal distribution jump process. This process repeats un-tilShas a negative expected value and some positive val-ues so the Smith-Waterman algorithm can effectively iso-late localized chord comparison results.564 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015020040060080010000.450.500.550.600.650.700.75\n�������\nFigure 2. Simulated annealing performance inFCC. Eachdot represents an iteration of the algorithm and correla-tional metricP. The jagged line, an exponential movingaverage, demonstrates the relatively constant increase inperformance as iterations progress.For each task,FCCandQBN, we run 1,000 iterationsof simulated annealing to optimize the correlational met-ricPwith a temperatureTthat starts at 1 and decreasesexponentially to 0.005 at the ﬁnal iteration.5. RESULTSIn this section, we detail the results of the optimizationprocedures across the two retrieval tasks (FCCandQBN)as detailed in Subsection 4.1.5.1 Optimizing Fully Connected ComparisonAcross 1,000 iterations of simulated annealing for theFCCtask, the correlational metricPat each iterationgenerally increased (see Figure 2). The maximalPre-turned by the simulated annealing was 0.7619, indicat-ing a strong correlation. The parametersstresulting inthis correlation ﬁrst occurred at iteration 472 with values{CPd:SW,gapopen:0,gapextension: 28,Cd:TPS,mx:5,ms:9}. The correlation between the ranked result listsfor ground-truth and extracted chord data with these pa-rameters can be visualized with a 3-dimensional histogramin Figure 3.A common measure for accuracy in music retrieval isthe Average Dynamic Recall (ADR) [18], which has beenused to evaluate similarity assessments in MIREX com-petitions since 2005. In the context of retrieval results,ADR assesses at all given position how many songs haveoccurred up to that position that should have occurred rela-tive to ground-truth rankings, returning an average in [0,1],with 1 indicating perfect similarity. We calculated the ADRof theFCCresults list of extracted chord data relative tothe generated ground-truth results list, deriving a result of0.7664. As a warning, this measure is not particularly ap-plicable to our work as the output ground-truth results listdoes not demonstrate an actual ground-truth similarity as-sessment, but its use here nonetheless illustrates the corre-lation of this parameter set in the context of music retrieval.\n050 000100 000050 000100 0000.000.010.020.030.04Ground-Truth RankingExtracted Chord RankingOccurrence FrequencyFigure 3. 3-dimensional histogram of the optimal fullyconnected comparison (FCC) rankings. The correlation(⇢=0.76) is visible through the elevated diagonal band. Thedensity of points along this band is greatest at the cornersas evidenced by bin heights—this means salient stronglyand weakly ranked chord progression results are most pre-served by the parameters that led to this result.5.2 Optimizing Query by N-gramsLikeFCC, the correlational metricPalso generally in-creased across iterations inQBN(see Figure 4). The maxi-malPreturned by simulated annealing was 0.7790, occur-ring singularly with the parametersst={CPd:SWnorm,gapopen:1,gapextension: 82,Cd:TPS,mx:1,ms: 10}.The average ADR across each of the 100 queries with theseparameters was 0.7900.TaskPADRFCC0.7619 0.7664QBN0.7790 0.7900Table 2. Summary of experimental results.5.3 Parameter OptimizationThe harmonic retrieval tasks presented in this paper,FCCandQBN, rely on a common set of parametersst. Thoughgeneralizations on effective values for the parameter setcannot be fully founded, it can still be useful to future ex-perimentation to detail average correlational metric valuesassociated with ranges of parameter values from the simu-lated annealing experiments.CPdandCdare the variables that perhaps change thenature of the Smith-Waterman function the most funda-mentally. Average output correlational metric values forinputted choices ofCPdandCdare as follows:FCC QBNHarte TPSHarte TPSSW0.59 0.680.40 0.49SWnorm0.56 0.620.35 0.53where maximum values are underlined. According to theseobservational results,TPSoutperforms theHartechord dis-tance metric in both experiments in terms of maximizingcorrelation.gapopenandgapextensiontake a wider range of values,thus it is more useful to look at variable ranges and theirProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 565020040060080010000.00.20.40.60.8\n�������\nFigure 4. Simulated annealing performance inQBN.average outputs. Following are correlational metrics cor-responding to ranges of gap variable values:FCC QBNRangegapopengapextensiongapopengapextension00.71 0.640.69 0.491-80.67 0.620.46 0.49>80.61 0.640.34 0.47These results suggest that gap opening penalties of 0inﬂuence higher correlational harmonic metric score.Finally, we chart which scaling and subtraction factors,mxandms, produced the highest average correlation met-ric scores:FCC QBNmxmx1-9 10-19 20+1-9 10-19 20+1-90.67 0.64 0.690.62 0.51 0.65ms10-190.68 0.65 0.650.51 0.43 0.49>200.58 0.58 0.570.39 0.38 0.30These results are both consistent in assigning highercorrelational metric scores to large multiplication factorsand small subtraction factors. A possible explanation forthis behavior and favoritism towards gap penalties of 0is that these factor choices result in the highest Smith-Waterman expected values and result scores. Though thisexpected value is ensured to be negative, a value close to0 will more frequently match chords positively by chanceand result in longer local alignment scores that resembleglobal alignment scores. It is possible that global sequencealignment techniques used inFCCandQBNhave strongcorrelational harmonic metric scores. Further research inglobal sequence alignment could present promising corre-lational metric results.6. DISCUSSIONThis paper suggests a new class of similarity assessmentsin music information retrieval (MIR),correlational met-rics, and outlines an experimental procedure for assessingthese metrics. Correlational metrics capture the degree towhich ground-truth and extracted features perform simi-larly through retrieval tasks. It is possible that similar re-sults in a retrieval task do not necessarily imply correct orgood results. The experimental choices made in this paper,such as using local alignments and the chord distance met-rics, are demonstrated in MIR research as strong choicesfor matching human intuitions of similarity [8, 11]; how-ever, these experimental choices in this paper reﬂect onepossible use case. In the context of chord progressions,there does not exist any reliable ground-truth similarity as-sessments, which motivated this work.Further experimentation is necessary with differentchord extraction algorithms and settings. The chord extrac-tion algorithm used in this paper is highly accurate, whichmay imply stronger correlational metric scores. Testinga variety of chord extraction algorithms would render acomparison of correlational metric scores associated witha gradient of extraction algorithm accuracies, giving sta-tistical signiﬁcance to the resulting scores and potentiallyuncovering other salient observations. Once there existresearch-backed ground-truth similarity assessments forchord progressions, this work can be enriched with directcomparisons to human intuitions. In its current form, thispaper is limited to Western harmonies, and more specif-ically, pop songs from the 1950s onwards. Many otherfeatures could be investigated within our experimental de-sign, from those directly supplemental to harmony, suchas chord duration and melody, to external factors, such assong popularity or artist. Incorporating and testing morechord distance metrics and different parameters and rangeswould additionally beneﬁt this class of research. Modify-ing the retrieval tasks and implementing additional taskscould extend this work, as well. For instance, random-ized query sequences in theQBNtask could be generatedaccording to probabilistic n-gram models to match morelikely search inputs and limit bias in the resulting corre-lational metric score as a result of purely random queriesbeing unnatural and distant to the input datasets.Assuming the parameter choices that resulted in the op-timal correlational metrics in this paper resulted in a har-monic similarity metric that matches human intuitions ofsimilarity, this paper suggests that effective MIR systemscan be constructed without the need for ground-truth chordannotations and provides a framework for conducting suchexperiments. As there are few research-backed ground-truth chord datasets, this could massively expand the pos-sible realm of chord datasets to reliably harmonically com-pare. Correlational metrics can also be used in future re-search across other musical features. The potential impli-cations of this paper suggest that with proper algorithmsand parameters that currently exist in the literature, practi-cal MIR systems can be constructed and optimized to workwithout the guide of human ground-truthing in similarityassessments.7. REFERENCES[1]Julien Allali, Pascal Ferraro, Pierre Hanna, and CostasIliopoulos. Local transpositions in alignment of poly-566 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015phonic musical sequences. InString Processing andInformation Retrieval, pages 26–38. Springer, 2007.[2]Eric Breimer and Mark Goldberg. Learning signiﬁcantalignments: An alternative to normalized local align-ment. InFoundations of Intelligent Systems, pages 37–45. Springer, 2002.[3]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-jinaga. An Expert Ground-Truth Set for Audio ChordRecognition and Music Analysis. InProceedings of the12th International Society for Music Information Re-trieval Conference, pages 633–638, 2011.[4]Chris Cannam, Matthias Mauch, Matthew EP Davies,Simon Dixon, Christian Landone, Katy Noland, MarkLevy, Massimiliano Zanoni, Dan Stowell, and Luıs AFigueira. Mirex 2013 entry: Vamp plugins from thecentre for digital music, 2013.[5]Vladim´ırˇCern`y. Thermodynamical approach to thetraveling salesman problem: An efﬁcient simulation al-gorithm.Journal of Optimization Theory and Applica-tions, 45(1):41–51, 1985.[6]W Bas De Haas, Matthias Robine, Pierre Hanna,Remco C Veltkamp, and Frans Wiering. Compar-ing approaches to the similarity of musical chord se-quences. InExploring Music Contents, pages 242–258.Springer, 2011.[7]Persi Diaconis and Ronald L Graham. Spearman’sfootrule as a measure of disarray.Journal of the RoyalStatistical Society. Series B (Methodological), pages262–268, 1977.[8]Pascal Ferraro and Pierre Hanna. Optimizations of lo-cal edition for evaluating similarity between mono-phonic musical sequences. InLarge Scale SemanticAccess to Content (Text, Image, Video, and Sound),pages 64–69. Le Centre de Hautes Etudes Interna-tionales d’Informatique Documentaire, 2007.[9]Osamu Gotoh. An improved algorithm for matchingbiological sequences.Journal of Molecular Biology,162(3):705 – 708, 1982.[10]Pierre Hanna, Matthias Robine, and Thomas Rocher.An alignment based system for chord sequence re-trieval. InProceedings of the 9th ACM/IEEE-CS JointConference on Digital libraries, pages 101–104. ACM,2009.[11]Christopher Harte.Towards automatic extraction ofharmony information from music signals. PhD thesis,Department of Electronic Engineering, Queen Mary,University of London, 2010.[12]Maksim Khadkevich and Maurizio Omologo. Time-frequency reassigned features for automatic chordrecognition. InAcoustics, Speech and Signal Process-ing (ICASSP), 2011 IEEE International Conference,pages 181–184. IEEE, 2011.[13]Scott Kirkpatrick et al. Optimization by simulated an-nealing.Science, 220(4598):671–680, 1983.[14]Fred Lerdahl. Tonal pitch space.Music Perception,pages 315–349, 1988.[15]Matthias Mauch and Simon Dixon. Approximate notetranscription for the improved identiﬁcation of difﬁcultchords. InProceedings of the 11th International Soci-ety for Music Information Retrieval Conference, pages135–140, 2010.[16]Matt McVicar, Ra´ul Santos-Rodr´ıguez, Yizhao Ni, andTijl De Bie. Automatic chord estimation from au-dio: A review of the state of the art.Audio, Speech,and Language Processing, IEEE/ACM Transactionson, 22(2):556–575, 2014.[17]Temple F Smith and Michael S Waterman. Identiﬁca-tion of common molecular subsequences.Journal ofMolecular Biology, 147(1):195–197, 1981.[18]Rainer Typke, Remco C Veltkamp, and Frans Wiering.A measure for evaluating retrieval techniques basedon partially ordered ground truth lists. InMultimediaand Expo, 2006 IEEE International Conference, pages1793–1796. IEEE, 2006.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 567"
    },
    {
        "title": "A Statistical View on the Expressive Timing of Piano Rolled Chords.",
        "author": [
            "Mutian Fu",
            "Guangyu Xia",
            "Roger B. Dannenberg",
            "Larry A. Wasserman"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417034",
        "url": "https://doi.org/10.5281/zenodo.1417034",
        "ee": "https://zenodo.org/records/1417034/files/FuXDW15.pdf",
        "abstract": "Rolled or arpeggiated chords are notated chords per- formed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano per- formance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the on- sets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equiva- lent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music perfor- mance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians.",
        "zenodo_id": 1417034,
        "dblp_key": "conf/ismir/FuXDW15",
        "keywords": [
            "Rolled or arpeggiated chords",
            "performed by playing the notes sequentially",
            "expressive timing",
            "piano performance",
            "equivalent onset",
            "onset span",
            "research questions",
            "local tempo estimation",
            "Analysis of Variance",
            "piano duet dataset"
        ],
        "content": "A STATISTICAL VIEW ON THE EXPRESSIVE TIMING OF PIANO ROLLED CHORDS Mutian Fu1 Guangyu Xia2 Roger Dannenberg2 Larry Wasserman2 1 School of Music, Carnegie Mellon University, USA 2 School of Computer Science, Carnegie Mellon University, USA {mutianf,gxia,rbd,larry}@andrew.cmu.edu ABSTRACT Rolled or arpeggiated chords are notated chords per-formed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano per-formance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the on-sets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equiva-lent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music perfor-mance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians. 1. INTRODUCTION Rolled (or arpeggiated) chords are notated chords per-formed by playing the notes sequentially, usually from lowest to highest in pitch. It is a common technique and an integral part of musical expression. Especially, pianists use rolled chords to convey their interpretations of ex-pressive timings. In a very broad sense, every piano chord is rolled since no two notes are played exactly at the same time. However, very few works have investigated piano rolled chords. As a consequence, when dealing with chords, most expressive performance studies stick to the melody or top note, in part due to a lack of theoretical foundations. For example, when analyzing the timing of a chord, researchers usually simply take the onset of a cer-tain note in a chord (e.g., the first note or the highest note) as the onset of a rolled chord [4][13] even though authors realize this is not the best solution. When synthe-sizing the timing of a chord, people either put the note onsets of a chord at exactly the same time or decode the onsets of each note individually [6][15]. This situation motivates us to investigate some fundamental properties of rolled chords in order to set a better basis for future expressive performance studies.  We investigate two expressive timing properties of pi-ano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can func-tionally replace the onsets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We compute equivalent onset time and relative location within a rolled chord via local tempo estimation, assuming that local tempo is steady within a few beats. To be more specific, we first estimate a linear mapping (a tempo map) between real performance time and score time for each chord. Then, we compute the intersection between the tempo map and the chord\u0002s onset span to compute a hidden equivalent onset. Finally, we compare the equivalent onset with the note onsets of the rolled chord to figure out its relative location. For onset span, we focus on a more fundamental statistical prob-lem: if onset spans are considered random variables, are they drawn from the same distribution, or affected by different chords or performances? We solve this problem by using Analysis of Variance (ANOVA). In our case, ANOVA provides a statistical test of whether the means of onset spans of different chords are equal.  The next section presents related work. Section 3 de-scribes a new data set we created for this study. Section 4 presents an important data preprocessing (polyphonic alignment) procedure. In Sections 5 and 6, we show the methodologies for equivalent onset and onset span, re-spectively. In Section 7, we present experimental results. 2. RELATED WORK We review two realms of related work: polyphonic alignment and piano rolled chords. The former is only related to our data preprocessing procedure while the latter is related to the main goal of our study. 2.1. Polyphonic Alignment Researchers have developed both online and offline poly-phonic alignment algorithms for both audio and symbolic data. Our study uses offline symbolic polyphonic align-ment based on the MIDI representation. For audio-based polyphonic alignment, researchers usually first analyze an audio spectrogram to extract pitch and timing features and then perform an alignment \n © Mutian Fu, Guangyu Xia, Roger Dannenberg, Larry Wasserman Licensed under a Creative Commons Attribution 4.0 Inter-national License (CC BY 4.0). Attribution: Mutian Fu, Guangyu Xia, Roger Dannenberg, Larry Wasserman. “A Statistical View on the Ex-pressive Timing of Piano Rolled Chords.”, 16th International Society for Music Information Retrieval Conference, 2015. 578  \n based on extracted features. Cont [2] uses non-negative matrix factorization for polyphonic pitch analysis and then uses a hierarchical hidden Markov model to achieve the alignment by sequential modeling. Raphael [11] in-troduces a graphical method to detect latent tempo and current position in score. Compared to audio-based approaches, symbolic alignment is relatively easy since the target files usually contain accurate pitch and timing information. Bloch and Dannenberg [1] introduce two online algorithms as a part of the first polyphonic computer accompaniment system. Their work uses pitch information and a rating function to find the best fit between performance and score. Hoshishiba et al. [8] propose an offline approach by us-ing dynamic programming and spline interpolation, in which dynamic programming is used to find the maxi-mum match between performance data and score and spline interpolation is used to post-process and improve the result. A more recent research is done by Chen et al. [3], in which two methods are introduced. The first method sorts notes in a MIDI file by their onset and then uses longest common subsequence to map the perfor-mance to the score. The second method sets some cor-rectly matched notes as the pivots, separates note se-quence by those pivots, and optimizes the result recur-sively by forward and backward scanning. 2.2. Piano Rolled Chords Study There are fewer studies related to piano rolled chords. From an analysis perspective, Repp [12] investigates some descriptive properties of arpeggiated chord onsets by using a single piece of music. To be more specific, this study considers the relative onset timing and inter-onset-interval within arpeggiated chords. It compares the results between the performances by students and experts and draws the conclusion that arpeggiating patterns are subject to large individual differences. From the synthe-sis perspective, Kim et al. [9] predict the onsets of a rolled chord by first estimating the onset of the highest note and then adding intervals for the onsets of succeed-ing notes. 3. DATASET  Besides investigating the equivalent onset time and onset span of piano rolled chords, we contribute a piano duet dataset for rolled chord analysis and other studies on ex-pressive music performance [15]. The advantage of duet performance is that we are able to access the expressive timing from both parts. The dataset currently contains three pieces of music: Danny Boy, Serenade (by Schu-bert), and Ashokan Farewell [7]. Each piece contains a monophonic melody part and a polyphonic accompani-ment part. For the polyphonic part, the three pieces con-tain 32, 56, and 245 chords, respectively. Each piece is performed 35 to 42 times by 5 to 6 different pairs of mu-sicians (each pair performed each piece of music 7 times). This dataset is now accessible online via www.cs.cmu.edu/~gxia/data.  4. DATA PREPROCESSING Before investigating the equivalent onset and onset span of any rolled chord, we have to align the polyphonic pi-ano performance to the score. This task is done in two steps: forward alignment and backward correction.  Forward alignment: We adopt the online approach used by Bloch and Dannenberg [1] for the forward alignment step. Generally speaking, the algorithm takes a perfor-mance as sequential inputs and matches performance notes one-by-one to a reference of sorted chords. At each step of the alignment, it maximizes the number of matched score notes minus the number of skipped score notes.  Backward correction: The forward alignment procedure works well for most music, but may cause a problem when adjacent chords share the same note. \n Figure 1. A piano roll illustration of forward alignment procedure. As shown in Figure 1, dotted arrows represent correct matches while the solid arrow represents the false match. In this case, the top note in the 1st chord is skipped in the performance and the next chord\u0002s 1st performed note hap-pens to share the same pitch with the skipped note. As a consequence, the 1st chord \u0003borrows\u0004 the missing note from the 2nd chord. In the worst case, if all the chords share the same note, this mismatch behavior could hap-pen recursively. To address this issue, the backward cor-rection algorithm starts from the last chord and recursive-ly recovers the borrowed notes, if any. 5. EQUIVALENT ONSET If we replace all the note onsets of a rolled chord by a single onset, where should we place this single onset to let it sound most like the original chord? It is reasonable to assume that this equivalent onset is hidden within the range of the rolled chord\u0002s onset span and has some par-ticular relationship with the onsets. In this section, we first find out the location of the hidden equivalent onset by local tempo estimation. Then we propose two func-tional approximations to reveal relative onset location within each rolled chord. In the following sections, we \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 579  \n use n to denote the total number of chords of a piece of music and m to denote the total number of performances of a piece of music. 5.1. Absolute Location of Equivalent Onset If local tempo around rolled chords is stable, equivalent onsets can be linearly interpolated from neighboring on-sets. We consider the melody notes within 2 beats of rolled chords and transfer the equivalent onset estimation problem into a beat estimation problem.  Formally, if the current chord index is !, we denote its score onset and equivalent performance onset by !\"\"#$!! and !\"\"#$!!, respectively. We do equivalent onset estimation based on the melody notes whose onsets are within the range of !\"\"#$!!−2,!\"\"#$!!+2. To be more specific, we first estimate a linear mapping be-tween performance onsets and score onsets of the melody notes within this range. Then, if we denote the slope and the intercept of this linear mapping as ! and!!, respec-tively, we can find the equivalent onset by:  !\"\"#$!!=!!∙!\"\"#$!!+!! (1) \n Figure 2. An illustration of equivalent onset esti-mation by local linear mapping. This process is illustrated by Figure 2, in which the \u0001+\u0002 symbols represent the melody notes and the circle sym-bols represent accompaniment rolled chord. The line rep-resents the tempo map computed by linear mapping and the star point, on the line at score time 9, represents the equivalent onset computed by equation (1). 5.2. Relative Location of Equivalent Onset  Once the absolute location of equivalent onset is estimat-ed, we present two methods to model its relative location within rolled chords: the ratio model and the constant offset model. For both models, we consider the !\"\"#$!! computed in the last section as the ground truth and find the models\u0002 parameters by minimizing the difference be-tween the models\u0002 predictions and the ground truth. 5.2.1 Ratio Model The ratio model assumes that equivalent onset is decided by the first and last onset of a rolled chord as in the fol-lowing equation: !\"\"#$!!!(!)=(1−!)∙!\"\"#$!!!+!∙!\"\"#$!!!!!!!(2) In equation (2), !\"!\"#!!! and !\"!\"#!!! refer to the first and last note onsets in a rolled chord respectively. ! is the parameter that characterizes the relative location of equivalent onset. According to the value of !, the equiva-lent onset can be located as follows: !<0:  equivalent onset is before the first onset of the rolled chord. 0≤!≤1: equivalent onset is between first onset and the last onset of the rolled chord. !>1:  equivalent onset is after the last onset of the rolled chord. For each piece of music, total number of chords is ! and total number of performances is !, we find the opti-mal r value by equation (3): !=!\"#!\"#!!\"\"#$!!−!\"\"#$!!!(!)!!!!!!!!!!!!!!!!!!!(3) 5.2.2 Constant Offset Model The constant offset model assumes that the equivalent onset is decided by the first onset plus some constant off-set !. Formally,  !\"\"#$!!!!=!\"\"#$!!!+! (4) Similar to ratio model, we find the optimal ! value by !=!\"#!\"#!!\"\"#$!!−!\"\"#$!!!(!)!!!!!!!!!!!!!!!!!(5) 6. ONSET SPAN For onset span, we focus on a more fundamental statisti-cal problem: Do pianists make different interpretations for different chords or performances? As random varia-bles, are all onset spans drawn from the same distribution, or are there different distributions for different chords or performances? In this section, we answer this question by using Analysis of Variance (ANOVA). We begin by in-troducing the basic idea of ANOVA and then link it with our problem step by step. 6.1. One-way ANOVA for Chord Effect One-way ANOVA can provide a statistical test of wheth-er the means of several groups of data are identical [14]. Formally, if there are n groups indexed by ! and !!! de-notes the mean of group !, the null hypothesis and the alternative hypothesis are:    !!:!!=!!=!⋯=!! (6)  !!:!∃!,!′:!!!≠!!! (7) Generally speaking, one-way ANOVA computes an F-test statistic, which is the ratio of variance between groups to the variance within groups. If different group means are close to each other, this F-test statistics will have a relatively low value and hence retain the null hy-Performance time (sec)4567Score time (beat)67891011\ntempo slopechord onsetequivalent onsetmelody onset2accompiaccom1pi580 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015  \n pothesis. On the other hand, if this F-test statistics is greater than a certain threshold, the null hypothesis will be rejected. Now let us link this setting to our problem. When checking whether the onset spans of different chords are drawn from the same distribution, each \u0003group\u0004 corre-sponds to a chord and the group members correspond to the onset spans of a particular chord in different perfor-mances. In Figure 3, we can see the distributions of the onset span for each chord in Danny Boy. The goal is to test whether or not the means of the bars in the boxplot are equal to each other. \n Figure 3. A boxplot of the onset spans of the chords in Danny Boy. Remember each piece of music has n chords and m performances. Therefore, each piece has !=!∙! total samples. Referring to the notations in Section 5, the onset span of a rolled chord can be expressed via:  !!=!\"\"#$!!!−!\"\"#$!!! (8) We use !!\" to denote its value in the !!! performance. Therefore, the group mean in equation (8) can be com-puted by   !!!=!!=!!!\"!!!!! (9) The implementation of one-way ANOVA can be de-scribed in the following steps. First, compute the variation between the groups and record its degree of freedom.  !!!\"#$=!!!−!!!\"!!!!! (10) where !!=!!!\"!!!!!! , !!\"=!!!\"!!!!!!!!! . The degree of freedom of !!!\"#$, !!!\"#$= !−1. Second, compute the variation within individual sam-ples and record its degree of freedom,  !!!\"#!!\"=!!!\"!−!!!\"!!!!!!!!!!!!!!!!!! (11) The degree of freedom of !!!\"#!!\", !!!\"#!!\"=!!−!. Third, compute the F-test statistics by\u0001   !!!\"#$=!!!\"#$!!!\"#$ (12)  !!!\"#!!\"=!!!\"#!!\"!!!\"#!!\" (13)   !=!!!\"#$!!!\"#!!\" (14) Finally, compare this F-test statistic against a certain threshold to decide whether or not reject the null hypoth-esis. 6.2. Repeated-measurement One-way ANOVA for Chord Effect The previous section considered whether different chords have different onset spans. However, an important as-sumption when using one-way ANOVA is that samples from different groups are independent. In our case, each piece of music is performed by 5 or 6 different pairs of students. Chords played by the same person are clearly correlated. To eliminate the dependent factors produced by same performers, we use repeated-measurement ANOVA to adjust our results.  The general logic of repeated-measurements ANOVA is similar to independent one-way ANOVA. The differ-ence between those two methods is that repeated-measurements ANOVA removes variability due to the individual differences from the within group variance. This process can be understood as removing between-sample variability, and only keeping the variability of how the sample reacts to different conditions (chords). We point readers to Ellen and Girden\u0002s book [5] for more detailed descriptions. 6.3. ANOVA for Performance Effect Section 6.1 and 6.2 presented the method to inspect whether pianists make different interpretations on onset span for different chords. Following a very similar proce-dure, if we just exchange the index of ! and ! in 6.1 and keep everything else the same, we can inspect whether onset spans are interpreted differently for different per-formances.  7. EXPERIMENTAL RESULTS 7.1. Equivalent Onset 7.1.1 Ratio Model Figure 4 shows the results of the ratio model. In the fig-ure, the x-axis represents the ratio parameter r and the y-axis represents the relative difference (residual) between model estimated equivalent onset and the ground truth computed via local tempo estimation. Therefore, small numbers indicate better results. Each line corresponds to a piece of music. We see that the optimal r values are all within the range from 0 to 1, indicating that the equiva-lent onset consistently lies within the range of note on-Chord index5 10 15 20 25 30Onset interval (sec)00.050.10.150.20.25Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 581  \n sets. The optimal values are 0.42 for Danny Boy, 0.13 for Ashokan Farewell, and 0.78 for Serenade. \n Figure 4. Result of the ratio model. 7.1.2 Constant Offset Model Similar to Figure 4, Figure 5 shows the results of the con-stant offset model. The only difference is that the x-axis now represents the constant offset parameter s. We see that the optimal s values are all within the range from 0 to 20 milliseconds. The optimal values are 16 milliseconds for Danny Boy, 1 millisecond for Ashokan Farewell, and 17 milliseconds for Serenade. Compared to the ratio model, the optimal value for constant offset model is more consistent.  \n Figure 5. Result of the constant onset model. 7.1.3 Comparison with Highest Note Model In most expressive performance studies, people use the highest note onset as the equivalent onset, which we refer to as the \u0003highest note model.\u0004 In this section, we com-pare the results of the ratio model and constant offset model with the highest note model.  Figure 6 shows this comparison between different models, in which each sub-graph represents a piece of music. Again, smaller number means better prediction. Here, we also map the x-axis value of the ratio model to seconds by multiplying the ratios by the average onset spans. We see that for all the pieces, the ratio model gives better predictions than the highest note model. The con-stant offset model also does a good job on Danny Boy and Ashokan Farewell but does not outperform the highest note model for Serenade.   (a) Model comparison: Danny Boy. \n (b) Model comparison: Ashokan Farewell. \n (c) Model comparison: Serenade. Figure 6. Model comparison of three songs. 7.2. Onset Span For onset span experiments, we just show the one-way ANOVA table since the repeated-measurement adjust-ments call for extra notations but give us the same con-clusions. Table 1 shows the result of the one-way ANO-VA on different chords of Danny Boy. Similar to the re-sult of Danny Boy, Ashokan Farewell and Serenade all have the F-test statistics much larger than the thresholds. This indicates that differences between group means are significant. Therefore, we see that not all chords are drawn from the same distribution. In other words, musi-cians make different interpretations for onset spans of different chords.   Ratio-1 -0.5 0 0.5 1 1.5 2ResidualDanny BoyFarewellSerenade\nConstant Offset (sec)-0.2 -0.1 0 0.1 0.2ResidualDanny BoyFarewellSerenadeTime (sec)-0.01 0 0.01 0.02 0.03 0.04 0.05ResidualHighest NoteRatioConstant Offset\nTime (sec)-0.01 0 0.01 0.02ResidualHighest NoteRatioConstant Offset\nTime (sec)0.01 0.015 0.02 0.025ResidualHighest NoteRatioConstant Offset582 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015  \n Variable !! !\" ! ! Between 1.6762 31 7.98 4.29×10!!\" Within 8.8879 1312   Table 1. ANOVA for chord effect. Table 2 shows the result of the one-way ANOVA on different performances of Danny Boy. Again, we get sim-ilar results for Ashokan Farewell and Serenade, which all have a F-test statistic not big enough to reject the null hypothesis. This indicates that the differences between group means are not significant. Therefore, we see that the interpretations for the same chord’s onset span across different performances are relatively consistent.  Variable !!!!\"!!!!!Between 0.2752 41 0.85 0.7383 Within 10.289 1302   Table 2. ANOVA for performance effect. 8. CONCLUSION AND FUTURE WORK In conclusion, we create a database to investigate two expressive timing properties of rolled chords in order to set a theoretical basis for future expressive performance studies. We examined three models to characterize the relative location of equivalent onset within rolled chords. The ratio model outperforms the other models for all pieces of music including the highest pitch model used in most research. We also studied onset span. We see that differences are not merely random; musicians use differ-ent interpretations for different chords and the interpreta-tion for the same chord across different performances are relatively consistent. This suggests that in future expressive performance studies, in order to synthesize a rolled chord properly, we can use the equivalent onset as the anchor point (instead of the onset of the highest pitch) and consider the onset span as an important parameter. Although our ratio model improves upon the highest pitch model, the best ratio is different for different pieces and the absolute location of equivalent onset is still based on estimation. This sug-gests that in future work we should either look for a way to predict the ratio for a given piece of music, or more likely, that we should look for an even better model by combining objective and subjective evaluations. 9. REFERENCES [1] J. Bloch and R. Dannenberg, “Real-time Computer Accompaniment of Keyboard Performances,” Proceedings of the International Computer Music Conference\u0001pp. 279-290, 1985. [2] A. Cont, “Realtime Audio to Score Alignment for Polyphonic Music Instruments, Using Sparse Non-negative Constraints and Hierarchical HMMs,” Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 245-248, 2006. [3] C. Chen, J. Jang and W. Liou, “Improved Score-Performance Alignment Algorithms on Polyphonic Music,” Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 1365-1369, 2014. [4] P. Desain and H. Honing, “Does Expressive Timing in Music Performance Scale Proportionally with Tempo?” Psychological Research, pp. 285-292, 1994. [5] R. Ellen and E. Girden, ANOVA: Repeated Measures, Sage Publications, 1992. [6] S. Flossmann, M. Grachten and G. Widmer, “Expressive Performance Rendering With Probabilistic Models,” Guide to Computing for Expressive Music Performance, pp. 75-98, 2012. [7] J. Galway and P. Coulter, Lengends, Hal Leonard, 1997. [8] T. Hoshishiba, S. Horiguchi and I. Fujinaga, “Study of Expression and Individuality in Music Performance Using Normative Data Derived from MIDI Recordings of Piano Music,” Proceedings of the International Conference on Music Perception and Cognition, pp. 465-470, 1996. [9] T. Kim, F. Satoru, N. Takuya, and S. Shigeki, “Polyhymnia: An Automatic Piano Performance System With Statistical Modeling of Polyphonic Expression and Musical Symbol Interpretation,” Proceedings of the International Conference on New Interfaces for Musical Expression, pp. 96-99, 2011. [10] A. Kirke and E. Miranda, Guide to Computing for Expressive Music Performance, Springer Science & Business Media, 2012.  [11]  C. Raphael, “A Hybrid Graphical Model for Aligning Polyphonic Audio with Musical Scores,” Proceedings of the International Conference on Music Information Retrieval, pp. 387-394, 2004. [12] B. Repp, “Some Observations on Pianists' Timing of Arpeggiated Chords,” Psychology of Music, pp. 133-148, 1997. [13] B. Repp, “Relational Invariance of Expressive Microstructure across Global Tempo Changes in Music Performance: An Exploratory Study,” Psychological Research, pp. 269-284, 1994. [14] B. Tabachnick and L. Fidell, Using Multivariate Statistics, Haper and Row, 2001. [15] G. Xia and R. Dannenberg, “Duet Interaction: Learning Musicianship for Automatic Accompaniment,” Proceedings of the International Conference on New Interface for Musical Expression, 2015. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 583"
    },
    {
        "title": "Efficient Melodic Query Based Audio Search for Hindustani Vocal Compositions.",
        "author": [
            "Kaustuv Kanti Ganguli",
            "Abhinav Rastogi",
            "Vedhas Pandit",
            "Prithvi Kantan",
            "Preeti Rao"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417203",
        "url": "https://doi.org/10.5281/zenodo.1417203",
        "ee": "https://zenodo.org/records/1417203/files/GanguliRPKR15.pdf",
        "abstract": "Time-series pattern matching methods that incorporate time warping have recently been used with varying de- grees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal mu- sic. While these methods perform effectively due to the minimal assumptions they place on the nature of the sam- pled pitch temporal trajectories, their practical applicabil- ity to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational com- plexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algo- rithms, the compressed representation of the pitch time series itself is not well understood given the pervasive- ness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by do- main knowledge to design the representation and to opti- mize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal composi- tions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at or- ders of magnitude reduction in complexity.",
        "zenodo_id": 1417203,
        "dblp_key": "conf/ismir/GanguliRPKR15",
        "keywords": [
            "time warping",
            "melodic phrases",
            "audio search",
            "pitch inflections",
            "dimensionality reduction",
            "string matching",
            "domain knowledge",
            "representation design",
            "parameter optimization",
            "audio query"
        ],
        "content": "EFFICIENT MELODIC QUERY BASED AUDIO SEARCH FORHINDUSTANI VOCAL COMPOSITIONSKaustuv Kanti Ganguli1Abhinav Rastogi2Vedhas Pandit1Prithvi Kantan1Preeti Rao11Department of Electrical Engineering, Indian Institute of Technology Bombay2Electrical Engineering, Stanford Universitykaustuvkanti@ee.iitb.ac.inABSTRACTTime-series pattern matching methods that incorporatetime warping have recently been used with varying de-grees of success on tasks of search and discovery ofmelodic phrases from audio for Indian classical vocal mu-sic. While these methods perform effectively due to theminimal assumptions they place on the nature of the sam-pled pitch temporal trajectories, their practical applicabil-ity to retrieval tasks on real-world databases is seriouslylimited by their prohibitively large computational com-plexity. While dimensionality reduction of the time-seriesto discrete symbol strings is a standard approach that canexploit computational gains from the data compression aswell as the availability of efﬁcient string matching algo-rithms, the compressed representation of the pitch timeseries itself is not well understood given the pervasive-ness of pitch inﬂections in the melodic shape of the ragaphrases. We propose methods that are informed by do-main knowledge to design the representation and to opti-mize parameter settings for the subsequent string matchingalgorithm. The methods are evaluated in the context of anaudio query based search for Hindustani vocal composi-tions in audio recordings via the mukhda (refrain of thesong). We present results that demonstrate performanceclose to that achieved by time-series matching but at or-ders of magnitude reduction in complexity.1. INTRODUCTIONA bandish, or composition in the North Indian classicalvocal genre of khayal, is characterised by its mukhda,its almost cyclically repeated refrain. The singer elabo-rates within the raga framework in each rhythmic cyclebefore returning to the main phrase of the bandish (i.e.its mukhda). The automatic detection of this repetitivephrase, or motif, from the audio signal would contributec\u0000Kaustuv Kanti Ganguli, Abhinav Rastogi, Vedhas Pan-dit, Prithvi Kantan, Preeti Rao. Licensed under a Creative Commons At-tribution 4.0 International License (CC BY 4.0).Attribution:KaustuvKanti Ganguli, Abhinav Rastogi, Vedhas Pandit, Prithvi Kantan, PreetiRao. “EFFICIENT MELODIC QUERY BASED AUDIO SEARCH FORHINDUSTANI VOCAL COMPOSITIONS”, 16th International Societyfor Music Information Retrieval Conference, 2015.to important metadata concerning the identity of the ban-dish. The mukhda is recognised by the lyrics, location inthe cycle and its melodic shape. While these are in orderof decreasing ease in terms of manual segmentation of themukhda, the melodic shape characterized by a pitch con-tour segment is most amenable to pattern matching meth-ods. The challenge here arises from the improvisatory na-ture of the genre where the raga grammar allows for con-siderable variation in the melodic shape of any prescribedphrase. Previous work has shown that the variability inthe mukhda across the concert, similar to that of otherraga-characteristic phrases in a performance, can be char-acterized as globally constrained non-linear time-warpingwhere the constraint appears to depend on certain charac-teristics of the underlying melodic shape [16, 17, 21]. Adynamic time-warping (DTW) distance measure was usedon the time-series segments to model melodic similarityunder local and global constraints that were learned from araga-speciﬁc corpus [17]. More recent work has also vali-dated the DTW based similarity measure in the context ofmelodic motif discovery but the high computational costsassociated with time-series search limited its applicabil-ity [3, 9, 14]. Given that DTW based local matching, withrelatively minimal assumptions, on the pitch time-seriesderived from the audio is largely successful in modelingthe relevant melodic variations, we focus on targeting sim-ilar performance with greatly reduced complexity. Com-putationally efﬁcient methods to search and localize occur-rences of the mukhda in a concert, given an isolated audioquery phrase, have the following potential real-world ap-plications: (i) automatic segmentation of all occurrencesof the mukhda provided one manually identiﬁed instance,with a goal to reduce manual effort in the rich transcriptionof concert audio recordings, and (ii) retrieving a speciﬁcbandish from a database of concert recordings by queryingby its mukhda provided either by an audio fragment or byuser singing.The acoustic correlate of the melodic shape of a phraseis its pitch contour represented computationally by the de-tected pitch of the singing voice at close uniformly spacedintervals. Considering the concert recording context wherean instrumental ensemble accompanies the vocalist, thepitch detection is achieved by a singing voice detectionalgorithm coupled with predominant F0 extraction at uni-form closely spaced intervals throughout the concert. The591pitch contour can be treated as a one-dimensional time-series which can be searched for the occurrence of a spe-ciﬁc pattern as deﬁned by the query (another time-seriessegment). We note that the dimensionality of the time-series is typically very high due to the required dense sam-pling of the pitch contour across the concert duration. Ithas been observed that a sampling interval on the order of20 ms is necessary in order to preserve important pitch nu-ances as determined by the curve of rapidly decreasing cor-relation between melodically similar pitch contours withincreasing sampling interval [9].As mentioned earlier, DTW can be used in an exhaus-tive search across the concert of this sampled pitch timeseries to ﬁnd the optimal cost alignment between the queryand target pitch contours at every candidate location. Wesee therefore that any signiﬁcant computational complex-ity reduction can only come from the reduction of dimen-sionality of the search space. An obvious choice is a rep-resentation of the melodic contour that uses compact mu-sical abstractions such as a sequence of discrete pitch scaleintervals (essentially, the note sequence corresponding tothe melody if there was one). String-matching algorithmscan then be applied that ﬁnd the approximate longest com-mon subsequence between the query and target segmentsof discrete symbols. Krannenburg [11] used this approachon audio recordings of folk songs to establish similarityin tunes across songs. Each detected pitch value was re-placed by its MIDI symbol and the Smith-Waterman localsequence alignment algorithm was used on the resultingstrings. Note however that there was no reduction in thesize of the pitch time-series. If the pitch time-series issegmented into discrete notes, a far more compact stringrepresentation can be obtained by using each symbol torepresent a tuple corresponding to a note value and dura-tion. In this case, a number of melodic similarity methodsbased on the alignment of symbolic scores become avail-able [1, 6, 11, 12, 27]. The effectiveness of this approach,of course, depends heavily on the correspondence betweenthe salient features of the pitch contour and the symbolsequence. A speciﬁc challenge in the case of Hindustanivocal music is that it is characterized just as much by theprecisely intoned raga notes as it is by the continuous pitchtransitions and ornaments that contribute signiﬁcantly tothe raga identity, motivating a more careful considerationof the high-level abstraction [15,18].The main contributions of this work are (i) a study ofthe suitability of two distinct high-level abstractions for se-quence representation in the context of our melodic phraseretrieval task, and (ii) using domain knowledge for the set-ting of various representation and search parameters of thesystems. In the next section, we describe our test dataset ofconcerts with a review of musical and acoustic characteris-tics that are relevant to our task. This is followed by a pre-sentation of our melodic phrase retrieval methods includ-ing approaches to the compact representation of the pitchtime-series and discussion of the achievable reduction incomputational complexity with respect to the baseline sys-tem. A description of the experiments follows. Finally theresults are discussed with a view to providing insights onthe suitability of particular approaches to speciﬁc charac-teristics of the test data.2. TEST DATABASE DESCRIPTIONThe dataset comprises 50 commercial CD-quality concertaudio recordings by 18 eminent Hindustani vocal artists.The accompaniment consists of tanpura (drone) and tabla,along with harmonium or sarangi. The concerts have beenchosen from a large corpus [23] in a deliberate mannerso as to achieve considerable diversity in artists, ragasand tempo. We restrict our analysis to the vilambit (slowtempo) and madhyalaya (medium tempo) sections of theseconcerts for the current task. Drut (fast tempo) sections areexcluded because their mukhda phrases contain a consid-erable amount of context-dependent variation and hencemelodic similarity is not as strongly preserved. Table 1summarises our dataset where 39 concerts are of vilambitlaya and the remaining 11 are madhyalaya. The averageduration of a vilambit bandish is 17 minutes and containsan average of 20-25 mukhda instances that occur once eachin a rhythmic cycle.#SongDur(hrs)#GTDur(hrs)Ratio# UniqueRaga Artist50 13:13 1075 1:44 13% 34 18Table 1. Description of the test dataset.Manual annotation of the mukhda segments with startand end boundaries was carried out by a musician and val-idated by a second very experienced musician. Mukhdasare most easily identiﬁed by listening for the lyrical phrasethat occurs about the ﬁrst beat (sam) of the rhythmic cy-cle as evidenced by the accompanying tabla strokes. Themukhda is labeled together with its boundaries as detectedfrom the onsets of the lyric syllables. These annotationsserve as the ground truth (GT) for the evaluation of the dif-ferent systems under test which exploit only the similarityof melodic shape to that of the audio query. The query thuscould be an instance extracted from the audio track, or itcould be a sung or hummed likeness of the melodic phrasegenerated by the user.\nFigure 1. Pitch contour segments of distinct mukhdas.Sam of the corresponding rhythmic cycle is marked in red.592 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Both the cues easily available to listeners, the phonesof the lyrics (as uttered by the singer) and the sam tablastrokes cannot be extracted reliably from the polyphonicaudio signal. The predominant F0 extractor on the otherhand is more robust and achieves the tracking of the vocal-ist’s pitch based on dominance and continuity constraintswithout any explicit source separation. Our approach tomukhda detection is currently based on the computation ofmelodic similarity which, ideally, should encapsulate thenotion of musically perceived similarity. The low-levelacoustic correlate of the melody is the pitch contour, theimplementation of which is presented in the next section.\nFigure 2. Normalized DTW distance between the ﬁrstmukhda of the concert and subsequent mukhdas.Figure 1 shows pitch contour segments of threemukhdas manually extracted from the beginning, middleand towards the end of the madhyalaya bandish of a con-cert. Also marked is the location of the sam with respectto the mukhda pitch trajectory. We note the variability inthe melodic shape. Typically the tempo of the concert in-creases gradually over time (linked to the reduction in therhythmic cycle duration) leading to a decrease in mukhdaduration (from 13 sec to 7 sec in Figure 1). Rather than alinear compression, the melodic shape is modiﬁed by non-linear time warping [5]. Figure 2 shows a plot of DTWdistance between the ﬁrst mukhda of the concert and eachlater mukhda versus the temporal location (the correspond-ing sam) of the later mukhda. The distances are normalizedwith respect to that of the ﬁrst false detection. We observea trend of decreasing similarity with increasing time, aswell as the fact that the intervals between mukhdas are notidentical due to rhythmic cycle duration variability. Also,not every rhythmic cycle is marked by a mukhda. Finally,we note that the DTW distance measure is largely insensi-tive to the irrelevant differences, as seen from the distancevalues normalised with respect to the distance between theﬁrst mukhda and the nearest false detection.3. MELODIC PHRASE RETRIEVAL SYSTEMSIn this section, we consider various approaches towardsour end goal which involves searching the entire vocalpitch track extracted from the audio recording to identifypitch contour sub-segments that match the melodic shapeof the query. We present the audio pre-processing requiredto generate the pitch time-series followed by a discussionof the different systems in terms of algorithm design andcomplexity.3.1 Time series extraction from audioThe desired time-series representation is expected to cap-ture the melody line, and hence requires accurate pitch de-tection of the main voice in polyphonic audio. The singingvoice usually dominates over other instruments in a vocalconcert performance in terms of its level and continuityover relatively large temporal extents although the accom-paniment of tabla and other pitched instruments such as thedrone and harmonium are present. Predominant-F0 detec-tion is implemented by the salience based combination oftwo algorithms [20] which exploit the spectral propertiesof the voice with temporal smoothness constraints on thepitch. The pitch is detected at 20 ms intervals throughoutthe audio with zero pitch assigned to the detected purelyinstrumental regions. Next, the pitch values in Hz are con-verted to the cents scale by normalizing with respect theconcert tonic determined by automatic tonic detection [8].This normalization helps match a query across concerts bydifferent artists. The ﬁnal pre-processing step is to interpo-late short silence regions below a threshold (80 ms whichis empirically tuned in previous studies [16,17]) indicatingmusically irrelevant breath pauses or unvoiced consonantsby cubic spline interpolation so as to preserve the integrityof the melodic shape.3.2 Baseline systemOur baseline method is the “subsequence DTW”, an adap-tation of standard DTW to allow searching for the occur-rence and alignment of a given query segment within along sequence [13,26]. Given a queryQof lengthNsym-bols and a much longer sequenceSof lengthM(i.e. thesong or concert sequence in our context) to be searched, adynamic programming optimization minimizes the DTWdistance toQover all possible subsequences ofS. Theallowed step-size conditions are chosen to constrain thewarping path to within an overall compression / expansionfactor of 2. No further global constraint is applied. Thecandidate subsequences of the song are listed in order ofincreasing DTW distance to which a suitable threshold canbe applied to select and localize the corresponding regionsin the original audio. The time complexity of subsequenceDTW isO(MN)whereN(M)is the number of pitchsamples corresponding to the query (song) duration (i.e. 50pitch samples per second of the time series duration, giventhat the pitch is extracted at 20 ms intervals) [2, 13, 28].We see that the time-series dimensions contribute directlyto the complexity of the search. Our goal is to ﬁnd com-putationally simple alternatives to DTW by moving to lowdimensional string search paradigms. This requires prin-cipled approaches to converting the pitch time-series to adiscrete symbol sequence, two of which are presented next.3.3 Behavior based systemWith a goal to preserve the characteristic shape of themukhda including the pitch transitions in the mapping tothe symbol sequence, we consider the approach of Tanaka[25] who proposed “behavioral symbols” to capture dis-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 593Figure 3. Construction from a pitch time series of the BSsequence (BSS) and the modiﬁed BSS.tinct types of local temporal variation in a human motioncapture system. A melodic phrase can be viewed as a se-quence of musical gestures by the performer, with a behav-ioral symbol then potentially corresponding to a single (ar-bitrary movement) in pitch space. A sequence of symbolswould serve as a sketch of the melodic motif. In Tanaka’ssystem, the symbols are purely data-dependent and evolvefrom the analysis itself [24, 25]. We bring in musical con-text constraints as presented in the algorithm descriptionnext.The pitch time-series is segmented into ﬁxed durationwindows centered at uniformly spaced intervals so that thewindows are highly overlapping as illustrated in Figure 3.The pitch contour within each window is replaced by apiecewise ﬂat contour where each piece represents a ﬁxedfraction of the window. While Tanaka recommends nor-malization of the pitch values within the window to [0,1]range in order to eliminate vertical shifts and scaling be-tween otherwise similar shapes, we omit this step giventhat we are not looking for transposition or scaling invari-ance in the mukhda detection task. The piece-wise ﬂat sub-segments are obtained by the median of the pitch values inthe corresponding subsegment. We choose median as op-posed to mean [24] as it is less sensitive to the occasionaloutliers in the pitch contour. We bring in further domainconstraints by using the discrete scale intervals for thequantization of the piecewise sub-segments that describe aspeciﬁc behavioral symbol (BS). We obtain a sequence ofBS, one for each window position. Due to the high over-lap between windows, repetitions are likely in consecutivesymbols. These are replaced by a single BS which stepbrings in the needed time elasticity. Figure 3 illustratesthe steps of construction of the BS sequence (BSS) andits repetition removed version (the modiﬁed BSS) from asimulated pitch time-series.The database is pre-processed and the symbol se-quence representation of each complete concert recordingis stored. When a query is presented, it is converted toits symbol sequence (which currently depends on the songto be searched) and an exact sub-sequence search is im-plemented on the song string. The choice of the ﬁxed pa-rameters: window duration, hop duration and number ofsubsegments within a window turn out to heavily inﬂuencethe representation. The window duration should dependon the time scale of the salient features (movements in\nFigure 4. The two proposed systems of quantization,namely: behavior based and pseudo-note systems.pitch space). The subsegments must be small enough toretain the melodic shape within the window. The hop ofthe sliding window compensates for alignment differencesof the different occurrences of the template in the pitchtime-series of the song. We present “parameter settings”for two conﬁgurations.Version A: Fixed parameter setting (window = 126 sam-ples, hop = 5 samples, # subsegments per window = 3)Version B: Query dependent setting (window = (0.5 *N)samples, hop = 5 samples, # subsegments per window = 4)We present next an alternate approach to symbolic rep-resentation of the pitch contour.3.4 Pseudo-note systemAn approximation to staff notation can be achieved by con-verting the continuous time-series to a sequence of piece-wise ﬂat segments if the section pitches are chosen fromthe set of discrete scale intervals of the music. If theachieved representation indeed corresponds to some un-derlying skeleton of the melodic shape of the phrase, wecould anticipate obtaining better matches across variationsof the melodic phrase. We address the question of how wecan bring domain knowledge into this transformation. Aswe see from Figure 4, the continuous pitch contours cor-responding to the phrases are not directly suggestive of aspeciﬁc sequence of raga notes given that raga notes areembellished considerably when realized by the vocalist.In Indian music traditions, written notation has a purelyprescriptive role and achieving the transcription of a per-formed phrase to written notation requires raga knowledgeand much experience [19]. All the same there is a similar-ity across the mukhda repetitions that we wish to capturein our representation.We consider a simple representation of the melodicshape that features only the relatively stable regions of thecontinuous pitch contours that lie within a musically validinterval of a scale (raga) notes. The scale notes are detectedfrom the prominent peaks of the long-term pitch histogramacross the concert and the musically valid interval is cho-sen to be within 35 cents [17]. This step leaves fragmentsof the time-series that coincide with the scale notes whileomitting the remaining pitch transition regions. Next, alower threshold duration of 80 ms is applied to the frag-ments to discard fragments that are considered too short tobe perceptually meaningful as held notes [16]. This leavesa string of fragments each labeled by a svara (raga noteas shown in Figure 4 (right)). Fragments with the samenote value that are separated by gaps less than 80 ms are594 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015merged. The resulting symbol sequence thus comprisesthe scale notes occurring in the correct temporal order butwithout explicit durational information. The database ispre-processed and the symbol sequence representation ofeach complete concert recording is stored. When a queryis presented, it is converted to its symbol sequence and anapproximate sub-sequence search is implemented on theconcert string based on an efﬁcient string matching algo-rithm with parameter settings that are informed by domainknowledge as described next.The similarity measurement of the query sequence withcandidate subsequences of the song is based on the Smith-Waterman algorithm, widely used in bioinformatics butalso applied recently to melodic note sequences [11, 22].It performs the local alignment of two sequences to ﬁndoptimal alignments using two devices. A symbol of onesequence can be aligned to a symbol of the other sequenceor it can be aligned to a gap. Each of these operations hasa cost that is designed as follows.Substitution score: In its standard form, the Smith-Waterman algorithm uses a ﬁxed positive cost for an ex-act match and a ﬁxed negative score for symbol mismatch.In the context of musical pitch intervals, we would ratherpenalize small differences less than large differences. Wepresent alternate substitution score functions that incorpo-rate this.Gap Function: This function deducts a penalty fromthe similarity score in the event of insertion or deletion ofsymbols during the alignment procedure. The default gappenalty is linear, meaning that the penalty is linearly pro-portional to the number of symbols that comprise the gap.Another possibility, that is more meaningful for the melodycontext, is the afﬁne gap function where the gap openingcost is high compared to the cost incurred by adding eachsuccessive symbol to the gap [7]. This is achieved by aform given bymx+cwherexis the length of the gapandm,care constants. Intuitively, increasingcwill penal-ize gap openings to a greater extent, while increasingmwill have a similar effect with regard to gap extension. Wepresent different designs for the relative costs motivated bythe musical context.With variations in each of the above two controls of theSmith-Waterman algorithm, we obtain the following threedistinct versions of the pseudo-note system.Version A: This setting is similar to the default Smith-Waterman setting, with a distance-independent similarityfunction that assesses a score of +3 for symbol match and-1 for a substitution. Gap function is linear, with penaltyequal to symbol length of gap.Version B: Substitution score that takes pitch differenceinto account, i.e. Score of +3 for a match, 0 for symbolsdiffering by upto 2 semitones, -1 for substitution, and anafﬁne gap penalty with parametersm= 0.8,c= 1.Version C: Query dependent settings where we use the set-tings of B as default with the following changes for particu-larly fast varying and slowly varying query melodic shapesas determined by a heuristic measure of ratio of squarednumber of symbols to query duration. We have the fol-lowing parameter settings. (i) fast varying: Substitutionscore of +1 to symbols differing by upto 2 semitones. Gappenalty is afﬁne with parametersm= 1,c= 0.5, and (ii)slowly varying: Similarity score of -0.5 to symbols differ-ing by upto 3 semitones. Gap penalty is afﬁne with param-etersm= 0.5,c= 1.5.Finally, the Smith-Waterman algorithm has a time com-plexity given byO(MN2)whereNis the query lengthin symbols andMis the song length [22]. By constrain-ing the allowed gap length to be no longer than that ofthe query itself(N), justiﬁed by the musical context, weachieve a complexity reduction toO(MN).4. EXPERIMENTS AND EVALUATIONWe present experiments that allow us to compare the per-formance of the different systems on the task at hand,namely correctly detecting occurrences of the mukhda inthe audio concert given an audio query corresponding tothe melodic shape of the mukhda phrase. The queries aredrawn from a set of 5 mukhdas extracted from the earlypart (ﬁrst few cycles) of the bandish. The early mukhdarepetitions tend to be of the canonical form and hence cor-respond well with an isolated query that a musician mightgenerate to describe the bandish. For the investigation ofa given method, we process the database to convert eachconcert audio to the pitch time series and then to the cor-responding string representation. Next, the query is con-verted to the string representation and the search is exe-cuted. The detections with time-stamps are listed in orderof decreasing similarity with the query as determined bythe corresponding search distance measure. A detection isconsidered a true positive if the time series of the detec-tion spans at least 50% of that of one of the ground-truthlabeled mukhdas in the song. An ROC (precision vs recall)is obtained for each query by sweeping a threshold acrossthe obtained distances. The ROC for a song is derived bythe vertical averaging (i.e. recall ﬁxed and precision aver-aged) of the ROCs of the 5 distinct queries [4]. The perfor-mance for each song is summarized by the following twomeasures: precision at 50% recall and the equal error rate(EER) (point on the ROC at which false acceptance ratematches false rejection rate). We further present perfor-mance of the best performing pseudo-note system on songretrieval in terms of the mean reciprocal rank (MRR) [10]on the dataset of 50 concerts as follows. We use the set ofthe ﬁrst occurring labeled mukhda of each song to form atest set of 50 queries. Next for each test query, every songis searched to obtain a rank-ordered list of songs whoseﬁrst 5 detections yield the lowest averaged distance mea-sure to the query.5. RESULTS AND DISCUSSIONTable 2 compares the performances of the various systemson the task of mukhda detection in terms of the averageEER and average precision at a selected recall across the 50songs where each song is queried using each of the ﬁrst ﬁvemukhdas. We also report the computational complexityProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 595Figure 5. Histogram of the measure ‘Precision at 50%Recall’ across the baseline and proposed methods.reduction factor over that of the baseline method (given bythe square of the dimension reduction factor). To obtainmore insight into song dependence, if any, we show thedistribution of the precision values for the 50 songs set inthe bar graphs of Figure 5, one system for each category,represented by the best performing one.Method (version)Mean Prc at 50% RecReduc.EER Mean Std.Subseq DTW — 0.33 0.73 0.18 1Behavior basedsystem(A) 0.47 0.56 0.26100(B) 0.41 0.61 0.25Pseudo-notesystem(A) 0.47 0.61 0.192500 (B) 0.42 0.64 0.19(C) 0.41 0.65 0.18Table 2. Comparison of the two performance measuresand computational complexity reduction factor across thebaseline and proposed methods.From Table 2, we observe that the baseline system rep-resented by subsequence DTW on the pitch time-seriesperforms the best while the pseudo-note methods achievethe best computation time via a reduction proportional tothe square of the reported dimension reduction factor (i.e.50). We will ﬁrst comment on the relative strengths ofthese two systems, and later discuss the behavior basedsystem. We observe an improvement in performance ofthe pseudo-note system with the introduction of domainknowledge and query dependent parameter settings for thesubsequence search algorithm. From Figure 5, we see thatthe subsequence DTW has a right-skewed distribution in-dicating a high retrieval accuracy for a large number ofsongs. However we note the presence of low perform-ing songs too which actually do better with the pseudo-note system. Closer examination of these songs revealedthat these belonged to ragas characterized by heavily or-namented phrases. In the course of improvisation, themukhda was prefaced by rapidly oscillating pitch due tothe preceding context. This led to increased DTW distancebetween the query and mukhda instances. The oscillatingprelude was absent in the pseudo-note representation alto-gether leading to a better match.The behavior based system was targeted towards captur-ing salient features of the melodic shape of the phrase in asymbolic representation. The salient features should ide-ally include steady regions as well as speciﬁc movementsin pitch space that contribute to the overall melodic shape.As such, it was expected to perform better than the pseudo-note method which retains relatively sparse information asseen from a comparison of the two representations for anexample phrase in Figure 4. However, the selection of theduration parameters required for the time-series conver-sion turned out to be crucial to the accuracy of the system.Shortening the window hop interval contributed to reducedsensitivity to time alignment differences but at the cost ofreduced compression and therefore much higher time com-plexity. Further, the data dependence of symbol assign-ment requires the query to be re-encoded for every song tobe searched, and further if query dependent window lengthis chosen, the song must be re-encoded according to thequery. Future work should target obtaining a ﬁxed dictio-nary of symbols to pitch movement mappings by learningon a large representative database of concerts.Top ‘M’ hits Correct songs Accuracy1 41 / 50 0.822 45 / 50 0.903 48 / 50 0.96Table 3. Results of the song retrieval experiment.Finally, we note the song retrieval performance of thepseudo-note version C in Table 3. The mean reciprocalrank (MRR) is 0.89. The top-3 ranks return 48 of the 50songs correctly. The badly ranked songs were found to benarrowly superseded by other songs from the same ragathat happened to have phrases similar to the mukhda ofthe true song. This suggests the potential of the method inthe retrieval of “similar” songs where the commonality ofraga is known to be an important factor.In summary, the melodic phrase is a central componentfor audio based search for Hindustani music. Given theimprovisational nature of the genre as well as the lack ofstandard symbolic “notation”, time-series based matchingof pitch contours provides a reasonable performance at thecost of complexity. The conversion to a relatively sparserepresentation by retaining only ﬂat regions of the pitchcontour and introducing domain driven cost functions inthe string search is shown to lead to a slight reduction inretrieval accuracy while reducing complexity signiﬁcantly.The inclusion of further cues such as the lyrics and rhyth-mic cycle markers to mukhda detection is expected to im-prove precision and is the subject of future research.6. ACKNOWLEDGEMENTThis work received partial funding from the EuropeanResearch Council under the European Union’s Sev-enth Framework Programme (FP7/2007-2013)/ERC grantagreement 267583 (CompMusic).596 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]N. Adams, M. Bartsch, J. Shifrin, and G. Wake-ﬁeld. Time-series alignment for Music Information Re-trieval. InProc. of Int. Soc. for Music Information Re-trieval (ISMIR), pages 303–310, 2004.[2]A. Chan. An analysis of pairwise sequence align-ment algorithm complexities. Technical report, Stan-ford University, 2004.[3]R. B. Dannenberg and N. Hu. Pattern discovery tech-niques for music audio.Journal of New Music Re-search (JNMR), 32(2), 2002.[4]T. Fawcett. An introduction to ROC analysis.PatternRecognition Letters, 2006.[5]K. K. Ganguli and P. Rao. Tempo dependence ofmelodic shapes in Hindustani classical music. InProc.of Frontiers of Research on Speech and Music (FRSM),pages 91–95, March 2014.[6]C. Gomez, S. Abad-Mota, and E. Ruckhaus. An analy-sis of the Mongeau-Sankoff algorithm for Music Infor-mation Retrieval. InProc. of Int. Soc. for Music Infor-mation Retrieval (ISMIR), pages 109–110, 2007.[7]O. Gotoh. An improved algorithm for matching bi-ological sequences.Journal of Molecular Biology,162:705–708, 1982.[8]S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V. Ish-war, H. A. Murthy, and X. Serra. Automatic tonic iden-tiﬁcation in Indian art music: Approaches and Evalu-ation.Journal of New Music Research, 43(1):53–71,2014.[9]S. Gulati, J. Serra, V. Ishwar, and X. Serra. Miningmelodic patterns in large audio collections of Indianart music. InProc. of Int. Conf. on Signal Image Tech-nology & Internet Based Systems (SITIS), 2014.[10]Z. Guo, Q. Wang, G. Liu, J. Guo, and Y. Lu. A mu-sic retrieval system using melody and lyric. InProc. ofIEEE Int. Conf. on Multimedia & Expo, 2012.[11]P. V. Kranenburg.A Computational Approach toContent-Based Retrieval of Folk Song Melodies. PhDthesis, October 2010.[12]M. Mongeau and D. Sankoff. Comparison of musicalsequences.Computers and the Humanities, 1990.[13]M. Muller.Information Retrieval for Music and Mo-tion, Chapter 4: Dynamic Time Warping, pages 69–84.[14]M. Muller, N. Jiang, and P. Grosche. A robust ﬁtnessmeasure for capturing repetitions in music recordingswith applications to audio thumbnailing.IEEE Trans.on Audio, Speech & Language Processing, 21(3):531–543, 2013.[15]D. Raja.Hindustani Music: A Tradition in Transition.D. K. Printworld, 2005.[16]P. Rao, J. C. Ross, and K. K. Ganguli. Distinguishingraga-speciﬁc intonation of phrases with audio analysis.Ninaad, 26-27(1):59–68, December 2013.[17]P. Rao, J. C. Ross, K. K. Ganguli, V. Pandit, V. Ishwar,A. Bellur, and H. A. Murthy. Classiﬁcation of melodicmotifs in raga music with time-series matching.Jour-nal of New Music Research (JNMR), 43(1):115–131,April 2014.[18]S. Rao, J. Bor, W. van der Meer, and J. Harvey.TheRaga Guide: A Survey of 74 Hindustani Ragas. Nim-bus Records with Rotterdam Conservatory of Music,1999.[19]S. Rao and P. Rao. An overview of Hindustani musicin the context of Computational Musicology.Journalof New Music Research (JNMR), 43(1), April 2014.[20]V. Rao and P. Rao. Vocal melody extraction in the pres-ence of pitched accompaniment in polyphonic music.IEEE Trans. on Audio, Speech & Language Process-ing, 18(8), 2010.[21]J. C. Ross, T. P. Vinutha, and P. Rao. Detecting melodicmotifs from audio for Hindustani classical music. InProc. of Int. Soc. for Music Information Retrieval (IS-MIR), October 2012.[22]T. F. Smith and M. S. Waterman. Identiﬁcation of com-mon molecular subsequences.Journal of MolecularBiology, 147:195–197, 1981.[23]A. Srinivasamurthy, G. K. Koduri, S. Gulati, V. Ishwar,and X. Serra. Corpora for music information researchin Indian art music. InProc. of Int. Computer MusicConf. / Sound and Music Computing Conf., September2014.[24]Y. Tanaka, K. Iwamoto, and K.Uehara. Discovery oftime-series motif from multi-dimensional data basedon MDL principle.Machine Learning, 58:269–300,2005.[25]Y. Tanaka and K. Uehara. Discover motifs in multi-dimensional time-series using the Principal Compo-nent Analysis and the MDL principle. InProc. of Int.Conf. on Machine Learning & Data Mining in PatternRecognition, pages 252–265, 2003.[26]P. Tormene, T. Giorgino, S. Quaglini, and M. Ste-fanelli. Matching incomplete time-series with Dy-namic Time Warping: An algorithm and an applica-tion to post-stroke rehabilitation.Artiﬁcial Intelligencein Medicine, 45(1):11–34, 2008.[27]A. Uitdenbogerd and J. Zobel. Melodic matching tech-niques for large music databases. InProc. of ACM Int.Conf. on Multimedia, pages 57–66, 1999.[28]A. Vahdatpour, N. Amini, and M. Sarrafzadeh. To-wards unsupervised activity discovery using multi-dimensional motif detection in time-series. InProc. OfInt. Joint Conf. on Artiﬁcial Intelligence (IJCAI), 2009.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 597"
    },
    {
        "title": "Classical Music on the Web - User Interfaces and Data Representations.",
        "author": [
            "Martin Gasser",
            "Andreas Arzt",
            "Thassilo Gadermaier",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417717",
        "url": "https://doi.org/10.5281/zenodo.1417717",
        "ee": "https://zenodo.org/records/1417717/files/GasserAGGW15.pdf",
        "abstract": "We present a set of web-based user interfaces for explo- rative analysis and visualization of classical orchestral mu- sic and a web API that serves as a backend to those ap- plications; we describe use cases that motivated our devel- opments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval re- search groups and a world-renowned symphony orchestra. Furthermore, we describe two real-world applications that involve the work presented here. Firstly, our web ap- plications are used in the editorial stage of a periodically released subscription-based mobile app by the Royal Con- certgebouw Orchestra (RCO) 1 , which serves as a content- distribution channel for multi-modally enhanced record- ings of classical concerts. Secondly, our web API and user interfaces have been successfully used to provide real-time information (such as the score, and explanatory comments from musicologists) to the audience during a live concert of the RCO.",
        "zenodo_id": 1417717,
        "dblp_key": "conf/ismir/GasserAGGW15",
        "keywords": [
            "classical orchestral music",
            "exploratory analysis",
            "visualization",
            "web-based user interfaces",
            "backend API",
            "PHENICX project",
            "Royal Concertgebouw Orchestra",
            "subscription-based mobile app",
            "real-time information",
            "live concert"
        ],
        "content": "CLASSICAL MUSIC ON THE WEB – USER INTERFACES AND DATAREPRESENTATIONSMartin Gasser1, Andreas Arzt1,2, Thassilo Gadermaier1,Maarten Grachten1, Gerhard Widmer1,2martin.gasser@ofai.at,andreas.arzt@jku.at,thassilo.gadermaier@ofai.at,maarten.grachten@ofai.at,gerhard.widmer@jku.at1Austrian Research Institute forArtiﬁcial Intelligence (OFAI), Vienna, Austria2Dept. of Computational PerceptionJohannes Kepler Universit¨at, Linz, AustriaABSTRACTWe present a set of web-based user interfaces for explo-rative analysis and visualization of classical orchestral mu-sic and a web API that serves as a backend to those ap-plications; we describe use cases that motivated our devel-opments within the PHENICX project, which promotes avital interaction between Music Information Retrieval re-search groups and a world-renowned symphony orchestra.Furthermore, we describe two real-world applicationsthat involve the work presented here. Firstly, our web ap-plications are used in the editorial stage of a periodicallyreleased subscription-based mobile app by the Royal Con-certgebouw Orchestra (RCO)1, which serves as a content-distribution channel for multi-modally enhanced record-ings of classical concerts. Secondly, our web API and userinterfaces have been successfully used to provide real-timeinformation (such as the score, and explanatory commentsfrom musicologists) to the audience during a live concertof the RCO.1. INTRODUCTIONThe ways we enjoy music have changed signiﬁcantly overthe past decades, not least as a result of the increased use ofinternet and technology to deliver multimedia content. Ser-vices such as iTunes, Spotify, and YouTube offer easy ac-cess to vast collections of music, at any time and any place,through tablets and mobile telephones. Such services typ-ically rely on APIs (Application Programming Interface,a set of HTTP-callable URL’s orAPI endpointsprovidingcertain data or functionality) to index and stream multime-dia content.These API’s are often exposed (e.g. last.fm, Sound-cloud) to third parties for embedding functionalities intonew applications. Services and APIs such as the ones men-tioned above are generally geared towards a broad audi-1http://www.concertgebouworkest.nl/en/rco-editions/c\u0000Martin Gasser, Andreas Arzt, Thassilo Gadermaier, Maarten Grachten, Ger-hard Widmer.Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).Attribu-tion:Martin Gasser, Andreas Arzt, Thassilo Gadermaier, Maarten Grachten, Gerhard Widmer.“CLASSICAL MUSIC ON THE WEB – USER INTERFACES AND DATA REPRESENTA-TIONS”, 16th International Society for Music Information Retrieval Conference, 2015.ence, and offer functionality peripheral to music listening,like searching for music, and creating playlists.As far as the music listening process itself is concerned,average listeners of popular music access music in a lin-ear fashion, i.e., a piece is consumed from the beginning tothe end. However, in the world of classical music, we ob-served very different requirements - there are many casesthat beneﬁt from a more content-oriented infrastructure fordelivering music.We argue there are two important characteristics of clas-sical music that call for a more elaborate treatment of themusical content. First of all, classical pieces tend to belonger and typically have a more elaborate and complexstructure than pop songs. Consequently, part of the ap-praisal of classical music tends to lie in the awareness, andinterpretation of that structure, both by musicologists andby listeners. Secondly, as opposed to pop music, in classi-cal music the roles of composing and performing the mu-sic are usually clearly separated. This distinction leads to astronger notion ofpieceon the one hand, andperformanceon the other.The desire to gain insight into structural aspects of thepiece and its performance can be formulated as a use casefor a general interested audience, which we will callover-seeing the music. A second use case,comparing perfor-mances, is centered around the question how different per-formances may embody different interpretations of thesame piece. This use case may be more pertinent to mu-sicologists, or musicians who wish to prepare their perfor-mance of a piece. In thevirtual concert guideuse case,audience members are provided with multi-modal infor-mation about the music during a concert. As all efforts to-wards providing a digital concert experience require con-siderableeditorial supportby experts behind the scenes,we explicitely consider this use case as well. See [8] fora more detailed description and some initial user feedbackjustifying those use cases.It is clear that serving these use cases leads to require-ments on the service infrastructure that go beyond merestreaming of the data. Most importantly, dealing transpar-ently with synchronized multi-modal information sourcesincluding video, audio, musical scores, and structural an-notations and visualizations, requires these sources to be571aligned to a common timeline, the musical time. In thispaper, we present an API for dealing with multi-modal(video, audio, score) data that is geared towards these re-quirements. Rather than describing the API in detail, wechoose to give a brief overview of the entities involved andpresent various prototype applications that illustrate howthis API allows for an in-depth content-oriented presenta-tion of music. In addition to these prototypes, we discusstwo real-world applications that rely on this API.2. RELATED WORKThe general idea of providing multi-modal and content-based access to music has been expressed in a variety offorms in prior work. In [10], M¨uller et al. present an au-dio player for multi-modal access to music data. The goalof theFreisch¨utz Digital(cf. [11], [12]) project is the de-velopment of a multi-modal repository that comprises dig-itized versions of the libretto, various editions of the musi-cal score, and a large number of audio/video recordings ofperformances. Dixon et al. demonstrate seamless switch-ing between different versions of the same piece duringplayback in their MATCH application [3]. Raimond etal. [13] present an extensive framework for publishing andlinking music-related data on the web.As for the symbolic representation of musical scores,MusicXML2is thede factostandard format for exchangeof digital sheet music, and as such it has largely replacedMIDI3, which is frequently considered an inadequate rep-resentation, especially in the ﬁeld of classical music.Another approach towards a comprehensive representationof western music notation is the Music Encoding Initia-tive [5]. While we are aware of the advantages of generaland ﬂexible frameworks such as Music Ontology [13] andMEI [5], we have settled on a more stripped-down, usecase-centric approach that allowed us to reach our goalsquickly. We understand that this might mean a redesign ofsystem components at a certain stage, but we believe thatan agile approach is beneﬁcial in our case.In order to be able to process graphic score sheets, weuse a custom bar line ﬁnding algorithm, since we currentlyhave no need for a complete transcription of the graphicalscore. See Viglienson et al. [18] for a description of theproblem and the various problems that might occur. Fora general discussion of Optical Music Recognition errorsand their impact on aligning score to audio, the reader mayrefer to [16].3. WEB APIAs already mentioned in the introduction, we did not haveone single use case in mind. In order to stay as ﬂexible aspossible, we decided on implementing a Service OrientedArchitecture (SOA)4. By explicitly representing the datain the form of HTTP-accessible JSON ﬁles, we are able toserve many different applications, either web-based ones2http://www.musicxml.com/3http://www.midi.org/4http://www.opengroup.org/soa/source-book/soa/soa.htmor implemented in the form of native desktop or mobileapplications (see section 3 for a brief outline of the func-tionality currently offered by the web API).Because we are working with copyrighted material, wehad to protect our API (and consequently, also the user in-terfaces) with an authentication/authorization system thatprovides different access levels to different users; further-more, all communication between the front end HTML5application and the web service API is encrypted.3.1 Authentication and authorizationAccess to all API endpoints is secured via an API key. Aspecial API endpoint is provided that returns an API keyas response to submission of username/password creden-tials. An API key is associated internally with a certainaccess level that gives the user access to a pre-deﬁned setof resources within the service.3.2 API resourcesThe main resources represented in our web service are:3.2.1 PersonA person can either be a natural person (such as a com-poser or a conductor) or another acting entity (such as anorchestra).3.2.2 PieceA piece is the most general form of a musical composition.A piece references a composer (a person), a set of scoresand a set of performances.3.2.3 ScoreA score represents the notated form of a composition. Wemade the distinction between pieces and scores in order tobe able to represent different versions/editions or differentorchestrations of the same piece. A score references thecorresponding piece and a set of score images.The score resource also hosts several sub-resourcessuch as score images, a mapping from abstract score posi-tion in musical beats to the corresponding graphical posi-tion in the score image and information about the positionof bar lines and time signature changes in the score. Wehave also deﬁned avariantsub-resource, which representsa derivative version of the score with a certain repetitionstructure. This is motivated by the fact that the recordingof a piece may very well not include all repetitions writtenin its underlying score, and this is reﬂected in the actualscore variantof the recorded performance.3.2.4 PerformanceA performance represents a musical piece performed by amusician or an orchestra. Apart from the actual audio ﬁle,the performance resource also contains the alignment in-formation. A score-to-audio alignment provides links be-tween time instants in a symbolic representation of music(such as the beginnings of bars in a score) and correspond-ing time instants (e.g., the actual note onsets) in a recording572 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015of the performance. Alignments have been created auto-matically in the ﬁrst place by the approach described in [4],but they may have been reviewed and corrected by humanannotators, in order to increase the accuracy of musicalevent positions in the audio ﬁle.From the alignment information, it is quite straightfor-ward to compute the musical tempo for each of theseevents, thus yielding a tempo-curve of the performance. Asan alignment can be deﬁned on different granularity levels,such as for each bar or each beat of a bar, an API requestcan include a parameter that speciﬁes a certain granularityat which the tempo information is to be calculated (e.g.,one tempo value per bar or beat). Thistempoinformationis also exposed as a sub–resource ofperformancevia theAPI.Finally, the API provides functionality to calculate per-ceived loudness values from a given performance record-ing. In order to calculate loudness information from digi-tal audio signals, we decided on using the LUFS (LoudnessUnits relative to Full Scale) measure that was introduced inthe EBU R128 recommendation [17]. Like the tempo in-formation, loudness values are available at different gran-ularity levels speciﬁed in musical time.4. EXPLORATIVE USER INTERFACESIn this Section, we sketch ﬁve different interactive visual-izations based on the API described above. We start bydiscussing some general aspects and design choices of thevisualizations.In our user interface, we strictly follow the concept ofdeep linking[1]. The idea is that if a user wants to discussan interesting musical passage in a written conversation,she wants to be able to simply send an URL to another user,who can subsequently click on the link, whereupon the re-ceiver sees the user interface in the same state as the sender.Consider the URL/score/?score=315&variant=40&performance=1328&position=1823.10,which opens our score viewer interface with a conﬁgura-tion of a score, a score variant, and a performance audioﬁle, and it also jumps directly to beat position 1823.10.Because of the highly dynamic nature of the user inter-faces, we have decided to develop a single page applica-tion5that talks directly to our API. In order to simplifydevelopment, improve testability of the code, and to en-force modular and reusable development, we use the pop-ular AngularJS6web development framework.The user interface prototypes are largely inspired bythe use cases mentioned in section 1. While theoversee-ing the musicuse case has been the main motivation be-hind the hierarchical navigation element (see section 4.1)and the score viewer (see section 4.2), thecomparing per-formancesuse case has led to the development of userinterfaces visualizing performance-related parameters oftwo performances side by side. Thevirtual concert guideuse case motivates the integration of a score following and5http://itsnat.sourceforge.net/php/spim/spi_manifesto_en.php6http://angularjs.org\n(a) Overview\n(b) Zoom\n(c) PlaybackFigure 1: Hierarchical multi–scale navigation inBeethoven’sEroicareal-time score display component into a mobile appllica-tion (see section 5.2), but many components that coveredthe ﬁrst two use cases were reused for this use case. Also,in theeditorial supportuse case (which is largely coveredby the application described in section 5.1), the user inter-face prototypes hvae proven useful, as will be discussedbelow.4.1 Navigation elementRather than providing a ﬂat timeline, we propose a navi-gation element based on a multi–level segmentation of apiece. In his seminal paper [15], Shneiderman laid downsome basic principles of how user interfaces for interactivevisualization/navigation should be designed: Firstoverview, then zoom and ﬁlter, then details on demand (theso-calledVisual Information Seeing Mantra). Fig. 1 and 2show how we reﬂected those principles in our user inter-face. Fig. 1a shows a three-level visualization: On the toplevel, we see the name of the musical piece (in this case,the ﬁrst movement of BeethovensEroica). The mediumlevel shows the rough structure (Exposition, Development,Recapitulation, Code), and at the lowest level, the posi-tion of musical themes is shown. Fig. 1b corresponds tothezoomlevel - the user can use the mouse wheel to lit-erally zoom into the musical structure and study the formof the piece. By dragging the playback cursor to a cer-tain position or clicking on a structural element on anylevel, the score viewer (see ﬁg. 2) shows the detailed musi-cal notation corresponding to this position. The individualstructural elements are also color-coded (this feature canbe used to encode repetitions of the same section/theme byusing the same color for the visual elements) and the tex-tual annotations appear only on a certain level of detail, inorder to prevent text clutter.See the subsequent sections for a brief description of thedetailviews.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 573Figure 2: The score viewer with the interactive structurenavigation element on top.\nFigure 3: Dynagram visualization of two performances(differences of dynamic levels are shown, where red meansincreasing, blue means decreasing loudness).\nFigure 4: Performance worm visualization of two perfor-mances with structure navigation element below.\nFigure 5: Direct visualization of an alignment4.2 Score viewerThe score viewer element highlights the bar enclosing thecurrent position (given in musical beats) on thecorresponding score sheet. We are currently using scannedand annotated score sheets, but in the future, other scoresheet representations (e.g., rendered on demand from Mu-sicXML data) are imaginable and should be evaluated.4.3 Dynagrams and tempogramsDynagrams and tempograms show the evolution of a singleparameter of the performance (loudness in the case of dy-nagrams, musical tempo in the case of tempograms) overtime, and the parameter is shown on multiple temporal lev-els. This is achieved by smoothing the parameter-curveover increasing amounts of time and horizontally stack-ing the results, where the parameter strength is mapped tocolor. Fig. 3 shows dynagram visualizations of two perfor-mances of the same piece that are linked via the navigationelement described in 4.1. It integrates short term variationsof the respective feature with long term variations into onepicture. Therefore, it allows to grasp short term events aswell as long term evolution revealing more of the overallstructure of the performed piece. This type of visualizationbuilds on earlier work by Langner et al. [7], Sapp [14], andMartorell et al. [9].We provide two different ﬂavors of these visualizations,(1) where the parameter itself is used as input, showingthe absolute values evolving over time and (2) where thederivative of the parameter is used, such that onlychanges (e.g. crescendo, decrescendo in case of loudness)become visible, as is shown in ﬁg. 3.4.4 Performance wormThe performance worm is a visualization metaphor inte-grating the evolution of tempo and dynamics of a musi-cal performance over time in a two-dimensional tempo-loudness space [6]. Its purpose is to uncover hidden char-acteristics of shaping a performance and the relations be-tween tempo and loudness that are characteristic of a cer-tain style of interpretation. For a given temporal level, the 2dimensional tempo-loudness-trajectory is displayed where574 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015older values fade into the background as newer data-pointsare added on top. Fig. 4 shows performance worm visual-izations of two performances that are linked via the struc-tural navigation element below.4.5 Alignment viewerThe alignment visualization shows the waveform displaysof the audio signals of two performances of the same piece,and it connects the respective bar line positions (the down-beats) in the two performances. The resulting line patternreﬂects the tempo structures of both pieces, and also howthey interrelate.Fig. 5 shows two performances of the fourth movementof Beethoven’s Eroica (Wilhelm Furtw¨angler conductingthe Berlin Philharmonic Orchestra vs. John Eliot Gardinerconducting the Orchestre R´evolutionnaire et Romantique).We can conclude from the visualization the intrinsic tempostructure of the performances; while Furtw¨angler plays theﬁrst part slower and becomes faster in the second part, Gar-diner chooses to play faster in the ﬁrst part and becomeslower afterwards; in both performances, we can observe astrong ritardando in the middle of the piece (i.e., the temposlows down dramatically), Furtw¨angler’s ritardando beingstronger than Gardiner’s.5. APPLICATIONSWe employed our API and our visual interfaces in two con-crete applications:5.1 Application 1: Editorial review for a multi-modalmusic-publishing appOur PHENICX project partners develop a mobile app forApple’s iOS devices, and this app ist used as a distributionchannel for multi-modally enriched recordings of musicplayed by the RCO. One of their selling points is an an-imated view of the musical score while the audio/video isplayed back. Fig. 6 sketches the workﬂow during the pro-duction of an edition of the app, and it demonstrates howour API and the score viewer user interface are involved inthis process.The task at hand is to align a recording of orchestralmusic to a score representation. This problem is twofold:As the graphical score representation is often not availablein machine-readable form7, the ﬁrst problem is to ﬁnd thegraphical positions corresponding to musical events (e.g.,notes, barlines, or staves). Methods based on Optical Mu-sic Recognition turned out to be usable if high–qualitygraphical scores are available, but we often deal withscanned images from aged printed or even hand-writtenscores, where standard OMR results are unsatisfactory –therefore, manual intervention is sometimes necessary inthis stage. Secondly, the alignment of a recorded perfor-mance to a synthesized performance of a score (we usedthe implementation from [4]) needs to be checked, andmanually corrected where necessary.7MusicXML does encode some score layout information, but not allAlignmentAPIStore alignment\nSend correction requestRealignCheckNotiﬁcationStore alignmentGet alignmentClientAlign score/performanceNotiﬁcationCorrection\nFigure 6: Production/publishing workﬂow\nConcert Hall\nAudienceScore Follower\nDevice\nDevice\nDeviceAPIDistribution server\nFigure 7: Live score following applicationThe initial score performance alignment is done by syn-thesizing the piece from a symbolic representation suchas MIDI or MusicXML. After an internal reviewing andcorrection phase, our client has the opportunity to reviewthe alignment in the score viewer interface and object incase anything is wrong – if this is the case, the alignmentgoes back into the internal correction phase again. Thedeep linking functionality – as described in section 4 – hasproven to be useful in this iterative process, as it enablesthe client to pinpoint problematic spots very easily in writ-ten conversations. Once the client is satisﬁed with the qual-ity of the alignment, it is fetched in the form of a JSON8ﬁle from our web service API, and further used in the appdevelopment process.5.2 Application 2: Interactive program notes withintegrated live score followingThe idea is to provide the audience during a concert withadditional information about the piece currently played viamobile devices. We have decided to give audience mem-bers the possibility to choose between three options, basedon personal preference or expertise: (a) an interactive mu-sical score display with the current musical position high-lighted, (b) text comments by a musical expert and (c) anartistic video visualization of the music.8http://www.json.orgProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 575This application also implies an editorial stage: Allthree options (a), (b), and (c) rely on sequenced series ofevents (be it the display of bar positions on a score sheet,timed text messages, or video clips that are played back atcertain time instants) that have to be prepared beforehand.In this stage our API and user interfaces are already of use,as editors usually rely on a score representation to pinpointcertain annotations to the music in advance.During the live event in the concert venue, the clientapplications, usually running on tablets or smartphones inthe audience, access the data (score image sheets, mappingof musical positions to graphical positions) stored in ourAPI. The score follower constantly analyzes the incomingaudio stream, and sends the estimated score position to adistribution server. The distribution server subsequentlyforwards this information to the mobile devices that fetchscore images through our API (timed text messages andvideos are provided by an additional data source) and usethis information to realize an enriched experience for theaudience member. Fig. 7 roughly sketches the data ﬂow inthis application. We have documented the practicability ofour approach in [2].6. CONCLUSIONS AND FUTURE WORKWe have presented (1) a web service API providing accessto structure- and performance-related music data includingmultimedia elements like score images and audio ﬁles and(2) a set of web-based explorative user interface prototypesthat act as a frontend to this API. We have also presentedtwo real–life examples of where our API and the user in-terface prototypes proved to be useful.We are currently investigating various extensions to ourcurrent infrastructure; the workﬂow described insection 5.1 provides much potential for improvement – weare considering building user interfaces that allow appli-cation clients to directly mark alignment errors or evencorrect alignments directly in the user interface. By fullymigrating the alignment service to be based on a com-plete score representation such as MusicXML instead ofscanned score sheets and a MIDI as the correspondingmachine-readable score representation, the alignment pro-cess would be greatly simpliﬁed and the quality of auto-matic alignments could be improved – in this case, wecould reliably identify graphical positions of musicalevents, and it would be even possible to generate the graph-ical score directly from the symbolic representation. Inaddition, more detailed knowledge about instrumentationand performance parameters could also improve the qual-ity of synthesized performances and therefore the qualityof resulting automatic alignments (i.e., if a high–qualitysample library such as the Vienna Symphonic Library9,that allows for precise control of performance parameters,is used).9https://vsl.co.at/7. ACKNOWLEDGMENTSThis research is supported by the European Union Sev-enth Framework Programme FP7 / 2007-2013, through thePHENICX project (grant agreement no. 601166).8. REFERENCES[1]”Deep Linking” in the World Wide Web.http://www.w3.org/2001/tag/doc/deeplinking.html. Accessed: 2015-05-01.[2]Andreas Arzt, Harald Frostel, Thassilo Gadermaier,Martin Gasser, Maarten Grachten, and Gerhard Wid-mer. Artiﬁcial Intelligence in the Concertgebouw.pages 165–176, 2015.[3]Simon Dixon and Gerhard Widmer. MATCH: A MusicAlignment Tool Chest. InProceedings of the 6th Inter-national Conference for Music Information Retrieval,number Ismir, pages 492–497, 2005.[4]Maarten Grachten, Martin Gasser, Andreas Arzt, andGerhard Widmer. Automatic Alignment of Music Per-formances With Structural Differences. InProceedingsof the 14th International Society for Music InformationRetrieval Conference, 2013.[5]Andrew Hankinson, Perry Roland, and Ichiro Fuji-naga. The Music Encoding Initiative as a Document-Encoding Framework. InProceedings of the 12th Inter-national Society for Music Information Retrieval Con-ference, pages 293–298, Miami (Florida), USA, Octo-ber 24-28 2011.[6]J¨org Langner and Werner Goebl. Visualizing Expres-sive Performance in Tempo–Loudness Space.Com-puter Music Journal, 27(4):69–83, 2003.[7]J¨org Langner, Reinhard Kopiez, Christian Stoffel, andMartin Wilz. Realtime analysis of dynamic shaping.InProceedings of the 6th International Conference onMusic Perception and Cognition. Keele, UK: KeeleUniversity, Department of Psychology, pages 452–455,2000.[8]Cynthia C.S. Liem, Ron van der Sterren, Marcel vanTilburg,´Alvaro Saras´ua, Juan J. Bosch, Jordi Janer,Mark Melenhorst, Emilia G´omez, and Alan Han-jalic. Innovating the Classical Music Experience inthe PHENICX Project: Use Cases and Initial UserFeedback. In1st International Workshop on Interac-tive Content Consumption (WSICC) at EuroITV 2013,Como, Italy, 06/2013 2013.[9]Agust´ın Martorell and Emilia G´omez. Hierarchicalmulti-scale set-class analysis.Journal of Mathematicsand Music, 9(1):95–108, 2015.[10]Meinard M¨uller, Frank Kurth, David Damm, andChristian Fremerey. Lyrics-based Audio Retrieval and576 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Multimodal Navigation in Music Collections. InEuro-pean Conference on Research and Advanced Technol-ogy for Digital Libraries, volume 554975, pages 112–123, 2007.[11]Meinard M¨uller, Thomas Pratzlich, Benjamin Bohl,and Joachim Veit. Freischutz digital: A multi-modal scenario for informed music processing. InImage Analysis for Multimedia Interactive Services(WIAMIS), 2013 14th International Workshop on,pages 1–4, July 2013.[12]Thomas Praetzlich and Meinard Mueller. Freischuetzdigital: a case study for reference-based audio segmen-tation for operas. InProceedings of the 14th Interna-tional Society for Music Information Retrieval Confer-ence, November 4-8 2013.[13]Yves Raimond, Samer Abdallah, Mark Sandler, andFrederick Giasson. The Music Ontology.ISMIR 2007:8th International Conference on Music InformationRetrieval, 8:417–422, 2007.[14]Craig Sapp.Computational Methods for the Analysisof Musical Structure. PhD thesis, Stanford University,Department of Music, 2011.[15]Ben Shneiderman. The eyes have it: a task by data typetaxonomy for information visualizations.Proceedings1996 IEEE Symposium on Visual Languages, 1996.[16]Verena Thomas, Christian Fremerey, Meinard M¨uller,and Michael Clausen. Linking sheet music and audio-Challenges and new approaches.Dagstuhl Follow-Ups,3:1–22, 2012.[17]European Broadcasting Union. Loudness Normalisa-tion and Permitted Maximum Level of Audio Sig-nals.https://tech.ebu.ch/docs/r/r128.pdf. Accessed: 2015-05-01.[18]Gabriel Vigliensoni, Gregory Burlet, and Ichiro Fu-jinaga. Optical measure recognition in common mu-sic notation. InProceedings of the 14th InternationalSociety for Music Information Retrieval Conference,November 4-8 2013.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 577"
    },
    {
        "title": "PAD and SAD: Two Awareness-Weighted Rhythmic Similarity Distances.",
        "author": [
            "Daniel Gómez-Marín",
            "Sergi Jordà",
            "Perfecto Herrera"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416286",
        "url": "https://doi.org/10.5281/zenodo.1416286",
        "ee": "https://zenodo.org/records/1416286/files/Gomez-MarinJH15.pdf",
        "abstract": "Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we ob- served that measures of similarity may differ given the presence or absence of a pulse inducing sound and the im- portance of those measures is not constant along the pat- tern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the percep- tual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the tem- poral positions of the beat along the bar. We show that with these improvements, the correlation between the pre- viously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops.",
        "zenodo_id": 1416286,
        "dblp_key": "conf/ismir/Gomez-MarinJH15",
        "keywords": [
            "rhythm similarity",
            "perception of rhythms",
            "discriminating regions",
            "existing similarity metrics",
            "pulse inducing sound",
            "improvements",
            "homogeneous contribution",
            "temporal positions",
            "predictions based on metrics",
            "audio loops"
        ],
        "content": "PAD AND SAD: TWO AWARENESS-WEIGHTED RHYTHMICSIMILARITY DISTANCESDaniel G´omez-Mar´ınUniversitat Pompeu Fabradaniel.gomez@upf.eduSergi Jord`aUniversitat Pompeu Fabrasergi.jorda@upf.eduPerfecto HerreraUniversitat Pompeu Fabraperfecto.herrera@upf.eduABSTRACTMeasuring rhythm similarity is relevant for the analysisand generation of music. Existing similarity metrics tendto consider our perception of rhythms as being in timewithout discriminating the importance of some regionsover others. In a previously reported experiment we ob-served that measures of similarity may differ given thepresence or absence of a pulse inducing sound and the im-portance of those measures is not constant along the pat-tern. These results are now reinterpreted by reﬁning thepreviously proposed metrics. We consider that the percep-tual contribution of each beat to the measured similarityis non-homogeneous but might indeed depend on the tem-poral positions of the beat along the bar. We show thatwith these improvements, the correlation between the pre-viously evaluated experimental similarity and predictionsbased on our metrics increases substantially. We concludeby discussing a possible new methodology for evaluatingrhythmic similarity between audio loops.1. INTRODUCTIONRhythm similarity is an important problem for both musiccognition and music retrieval. Determining which aspectsof the musical ﬂow are used by musical brains to decideif two musical excerpts share similarities with respect torhythm, would make it possible to build algorithms thatapproximate human ratings about such relatedness. Theapplications of such algorithms in MIR contexts shouldbe obvious and some have already been addressed [33][13] [6] [20]. Unfortunately, there is a gap between theknowledge provided by cognitive models and engineeringmodels with respect to similarity in general, and rhythmsimilarity in particular. Rhythm similarity metrics used inMIR are frequently based on superﬁcial information suchas inter-onset intervals, overall tempo or beat rate, onsetdensity, and they usually consider full-length songs to de-rive a single similarity value. Contrastingly, rhythm sim-ilarity models developed by cognitive scientists insist onthe importance of syncopation, beat salience, periodici-c\u0000Daniel G´omez-Mar´ın, Sergi Jord`a, Perfecto Herrera.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Daniel G´omez-Mar´ın, Sergi Jord`a,Perfecto Herrera. “PAD and SAD: Two Awareness-Weighted RhythmicSimilarity Distances”, 16th International Society for Music InformationRetrieval Conference, 2015.ties and shorter time-scales to determine similarity. In thispaper we address the above-mentioned gap and proposetwo rhythm similarity distances that reﬁne those currentlyavailable (and probably rougher than desirable). The pro-posed distances have been derived from music cognitionknowledge and have been tuned using experiments involv-ing human listeners. We additionally show that they canbe adapted to work (at least) in a music-loop collectionorganization context, where music creators want to orga-nize their building blocks in rhythm-contrasting or rhythmﬂowing ways where similarity would provide the criterionfor such concatenation of elements.Previous work has used rhythmic descriptors, computedfrom audio signals, to analyze song databases. A commoncollection used for testing genre classiﬁcation methodolo-gies, The Ballroom Dataset, has been sorted automaticallyusing different rhythmic descriptors and methodologies [4][29] [9] [24]. Out of the ballroom dataset very few authorshave addressed rhythm in electronic music with rhythmicdescriptors [10] [23] [2]. The logic behind most of theseresearch is the assumption that if a corpus is classiﬁed ac-cording to annotated labels, the features used for that clus-tering are somehow related to the phenomena that generatethe clustering. In other words, a correct classiﬁcation im-plies that the features used are useful despite their percep-tual relevance.Using symbolic representations of music, other authorspropose metrics to evaluate rhythmic similarity that haveshown to be useful in melody classiﬁcation [33] or haveproven correlation with cognitive judgements in rhythmicsimilarity experiments [12] [25] [1].However, neither the audio-based methodologies or thesymbolic metrics for rhythm similarity ( [23] being an ex-ception) have been designed for exploring short audio seg-ments such as loops. Moreover, methodologies to evaluaterhythmic similarity between two audio loops and retrievea value that can be analogous to a human rating are notyet available. Therefore we want to develop perceptuallygrounded rhythm similarity metrics to be used with shortaudio loops.This paper is aimed to present two new rhythmic simi-larity metrics derived from revisiting the results of our cog-nitive experiments on rhythm similarity perception [8]. Af-ter revisiting our previous experiments, two metrics ariseas useful in similarity prediction tasks. Based on thosemetrics we then introduce a new methodology to explorerhythmic similarity between audio loops.666The metrics proposed are based on the requirement thatrhythmic similarity must be rooted in current knowledgeof rhythm perception, where the notions of beat entrain-ment, reinforcement and syncopation are fundamental. Wehypothesize that a proper rhythmic similarity measure canbe built upon those perceptual considerations, emphasiz-ing the idea that our attention when judging the similaritybetween two rhythms is not evenly distributed in time. Wespeciﬁcally propose that we are more aware of certain re-gions of a rhythm than others, affecting the way in whichwe measure their similarity. To test our hypothesis weuse the results of our previous perceptual experiments andcompare them with predictions computed with our metricsfor the same rhythmic patterns in order to determine theircorrelation.High correlation values between the similarity ratingsof our previous experiment and the metrics presented hereare found, suggesting that blending awareness and synco-pation is important for accurately predicting rhythmic sim-ilarity. Finally we want to explore if the measures we pro-pose, besides providing good ﬁts and predictions of humanjudgements, can be used to organize loop collections. Theuse of our metrics in audio analysis will be discussed in thelast sections of the paper, where we propose a methodologyand evaluate it using audio loops of drum break patterns.Our results for this pilot validation present signiﬁcant cor-relations between the similarity judgements of the subjectsand the predicted distances proposed here.2. STATE OF THE ART2.1 Beat InductionThe fact that us humans induce a pulse sensation when lis-tening to music is by no means trivial and it seems to bean innate and involuntary process [34]. It is known thatthe mechanisms that favour our acquisition of a beat whenlistening to music can also be triggered by any sequenceof onsets [26]. This emergent beat entrainment is a cog-nitive process that can be divided two stages: ﬁrst, we tryto infer a metrical structure either by computing distancesfrom intervals of the musical surface, where at least 5 to10 notes are needed [3], or just try to match the incom-ing sound to an internal repertoire of known rhythms. Fi-nally, once a meter has been hypothesized, it is maintainedin the form of expectancies that interact with the new in-coming sounds [17]. During this interaction, the expectedpulse can be reinforced or disconﬁrmed. When challenged,brain rejection signals have been measured by means ofEEG [15]. The occurrence of a disconﬁrmation is oftenreferred to as syncopation, indicating notes that were ex-pected on the beat but were presented on a previous metri-cal position [18].In order to represent the variability of expectancies alonga rhythmic pattern, researchers use proﬁles that indicatethe metrical weight of a note depending on its position.Different proﬁles that highlight the importance of a beatreinforcement or a syncopated event, depending on its oc-currence within a full metrical period, have been proposed.\nFigure 1. Lerdhal and Jackendorf0s [16] metrical weightproﬁle (left) and the experimentally revised version ofPalmer and Krumhansl [22] measured for musicians(right).A main theoretical proﬁle [16] and an updated version ex-perimentally revised with musicians [22] are presented inFigure 1. These proﬁles stress the existence of a percep-tual hierarchy of sound events depending on their occur-rence, suggesting that some reinforcements or syncopa-tions are perceptually more relevant than others. Theseideas have led to algorithms that measure the syncopa-tion of a monotimbral unaccented phrase [30]. Moreover,these algorithms have been used to correlate syncopationwith the difﬁculty to tap along rhythms [5], musical com-plexity [31] [7] [27] and musical pleasure and desire todance [35], stressing the idea that syncopation has a pow-erful effect on our perception of music.2.2 Rhythmic SimilarityOnce we can extract a numerical value from a pattern ofonsets such as its syncopation value, comparing patternsand establishing distances between them is mathematicallypossible. One main approach, proposed by Johnson-Laird[14], is to analyze the onsets present on every beat of arhythmic pattern and assign the beat to a category depend-ing if it reinforces the beat, challenges the beat or doesnothing to the beat. This approach has been modiﬁed [28]and successfully tested with humans under experimentalconditions [1]. These ideas will be further expanded through-out this paper.As most proposed similarity metrics are measured onmonotimbral, monotonal and unaccented symbolic repre-sentations of rhythm, there are others who have exploredthe use of string similarity techniques as the swap distanceor the edit distance [19] [21] to measure similarity betweenpatterns. The edit distance has proven to be a useful pre-dictor of human similarity judgements [32] [11] [25]. Butstill, the obtained ﬁt between the edit distance and subjec-tive similarity judgments has a big room for improvement.Here we use similarity metrics based on syncopation,speciﬁcally a variation of the theory of Johnson-Laird inwhich we expand the possible groups that a beat can besubscribed to (syncopation, reinforcement or nothing). Inthe following sections we present, test and discuss an im-provement over a previously published metric and explorethe possibility of using these symbolic metrics in rhythmicanalysis of audio signals.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6673. METHODIn this section we present different concepts that are thebuilding blocks of our rhythmic similarity algorithms. Wehave to make some simplifying assumptions, consideringone bar, monotimbral, monotonal, percussive patterns with4/4 time signature and a minimum resolution of a sixteenthnote. The symbolic representation of such patterns is bi-nary, where a 1 indicates an onset and 0 indicates a silence.Therefore the patterns used throughout this work are 16digit sequences of zeroes and ones.3.1 Beats to syncopation groupsRhythms are split in beats, in our case each beat has foursteps (four digits). Each beat of a rhythm is classiﬁed intoa group according to its relation with the pulse, either a re-inforcement or a challenge (See table 1). This method isa variation of Johnson-Laird0s method [14], in which beatsare clustered in three broad categories: syncopation, rein-forcement or nothing depending if the elements of the beatare a reinforcement, a challenge or have no interaction withthe pulse. We have expanded Johnson-Laird0s method bysplitting syncopation into three possible groups (groups 5to 7, Table 1), reinforcement is split in three groups (groups1 to 3, Table 1) and adding a new category where a syn-copation and a reinforcement are both present (group 8,Table 1). Expanding the groups in which a beat can beclassiﬁed offers more detail on the role of each segmentand helps differentiate between different syncopations ordifferent reinforcements.The procedure to classify each beat is to compute itssyncopation value using the beat proﬁle 2 0 1 0. This pro-ﬁle is derived from Lerdahl and Jackendorf0s [16] in whichweights are proportional to the duration of the note eachaccent represents: an accent of a whole note has a higherweight than an accent on a half note, which is higher thanan accent on a quarter note, and so forth. In our beat pro-ﬁle the ﬁrst onset, that is coincident with the pulse, has ahigher weight than the third onset which is coincident withan eighth note.It is important to note that an onset on the fourth stepof a beat generates a syncopation only if the ﬁrst step ofthe next beat is a silence. Therefore to calculate the ap-propriate syncopation values for every beat, the ﬁrst stepof the following beat has to be considered. The syncopa-tion value for each beat is the sum of each onset0s metricalweights.Each beat can then be assigned to one out of eight syn-copation categories, but we have considered the case of areinforcement on the ﬁrst step and a syncopation on thefourth step 1001(total syncopation value = 0) as specialcases belonging to syncopation group #8.3.2 CoincidenceWe propose here two metrics, one that explores if two pat-terns have the same onsets and silences on a speciﬁc beat,which we call pattern coincidence distance (PD) and theother one, named syncopation coincidence distance (SD)GroupvaluePatterns1310101010x2210001000x 1001x 1011x3100100010x 01100110x 11101110x4000000000x 1111x 0011x 0001x 0111x5-101000100x 11001100x 0101x 1101x6-200010011011111117-3010111018010011011Table 1. Relation between syncopation group, syncopa-tion value and beat patterns. The symbol ‘’ indicates asilence at the beginning of the next beat and the symbol ‘x’indicates an onset at the beginning of the nest beat.which explores if a speciﬁc beat of two patterns belong tothe same syncopation group (see Table 1).Here we give an illustrative example to understand PDand SD. The two ﬁrst beats of a given pattern A have thefollowing onset/silence conﬁguration 1001 0110 and an-other pattern B has 1100 0010. Their respective syncopa-tion groups are #8 #3 and #5 #3. The pattern coincidence(PD) is computed by looking at the percentage of coin-cident onsets and silences on the same beat of each pat-tern. Their coincidence values would be (2+3)/8 = 0.625because for the ﬁrst beat there are 2 out of four notes co-incident between 1001and 1100; and for the second beat,there are 3 coincidences between 0110 and 0010. In to-tal there are 2+3 coincidences out of 8 possible. On theother hand, to measure the syncopation coincidence (SD),for the ﬁrst beat of patterns A and B, we get that 1001be-longs to family #8 and 1100 belongs to family #5. Clearly8 is different from 5. But if we look at the second beat,0110 and 0010 belong to the same group #3, thus groupcoincidence is 0+1=1. With these metrics we obtain twomethods for measuring a numerical value of the coinci-dence between two coincident beats of different patterns.If the coincidence between all the beats of two patterns iscomputed, this value can be used as a measure of similar-ity between the two patterns. However, we might considerthat, as different onsets have different metrical weights de-pending on their position within a pattern (see Figure 1),beats can also have different perceptual relevance depend-ing on their position within the pattern. In this paper wehave conceptualized this factor as awareness.3.3 Awareness as an effect of metrical hierarchyOur previously published results [8] suggest a difference inthe relevance of each beat when measuring similarity be-tween two patterns based on coincidence. This awarenesshas proven important when exploring correlations betweenour experimental results of similarity and the rhythmic pat-terns compared. Thus we propose each beat to have dif-ferent relevance when evaluating similarity between twopatterns in the presence of a pre deﬁned metrical context.Awareness is conceived as weight factors applied to eachbeat0s coincidence metric (either PD or SD). These weights668 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015emphasize or moderate each beat0s importance on the ﬁnaldistance value. This concept will be addressed in the fol-lowing section and is decisive for explaining our results.3.4 Rhythmic Similarity MetricsOur metrics are straightforward and are based on comput-ing any of the two types of coincidence (either beat orsyncopation group), and using them directly or with anawareness-based weighting. We ﬁnally have four metrics,two of them non-weighted. Pattern coincidence Distance(PD) and Syncopation group coincidence Distance (SD),Pattern coincidence and Awareness Distance (PAD) andSyncopation group coincidence and Awareness Distance(SAD). The weights of the PAD and SAD metrics will beexplored in the following sections.PD=pc1+pc2+pc3+pc4(1)SD=sc1+sc2+sc3+sc4(2)PAD=pc1w1+pc2w2+pc3w3+pc4w4(3)SAD=sc1w1+sc2w2+sc3w3+sc4w4(4)Where pc(n) is pattern coincidence, sc(n) is syncopationgroup coincidence, w(n) is the weighting of each beat, n isthe order of the beat within a full metric cycle.4. EXPERIMENTIn previously published paper [8] we performed two rhyth-mic similarity experiments, one inducing the beat and an-other without inducing the beat. In this paper we are revis-iting the beat-induced experiment to test our new metricswith the similarity ratings obtained in the previous one.In one of the experiments, twenty one subjects (recruitedamong the MTG staff and UPF pool of students, all of themwith musical experience of more than 5 years as amateurperformers) rated different rhythm pairs in the presence ofa beat-inducing kick drum. The rhythm pairs were con-structed by making variations of a main pattern as shownin Table 2. A region of the base pattern was progressivelyshifted, generating new patterns. Nine different main pat-terns were designed and the length and origin of the re-gion was varied systematically. Thirty six rhythm pairsplus a control pair were tested by all the subjects who ratedsimilarity using a Likert scale of seven steps. To promoterhythm entrainment, a kick drum, coincident with the startof every beat, was presented before and simultaneouslywith the tested rhythms.5. RESULTSThe mode of the similarity ratings for each pair of patternswas used as the value capturing their similarity. All thepairs of patterns presented to the subjects are analyzed withthe metrics described in section 3, exploring the correla-tions with the similarity ratings reported for each pair.In our previously reported experiment, we computed thePD distance for every tested pair and observed a Spear-man Rank correlation with the subject0s similarity ratingsBase Patternvariation1010 1110 1000 10101101 0110 1000 10101010 1110 1000 10101010 10111000 10101010 1110 1000 10101001 0101 1000 10101010 1110 1000 10101010 1010 1100 1010Table 2. Example of four stimuli pairs used in the experi-ment. The left column has the base pattern and the derivedvariations are on the right column. The similarity measuresof the subjects are between the base pattern and each varia-tion. The underlined portion of the base pattern is repeatedin the variations.\nFigure 2. PD, PAD, SD and SAD predictions correlatedwith similarity ratings. X axis: similarity ratings, y axisPD, PAD, SD and SAD predictions from left to right.of 0.54 (p-value<0.005). We also computed the SD dis-tance which has a Spearman rank correlation value of 0.46with the similarity ratings (p-value<0.01).Here we calculate our newly introduced metrics PADand SAD (see Figure 2). To calculate PAD, a linear regres-sion between the coincidence result of each beat and thesimilarity ratings is computed. The normalized weightsobtained for beats 1 to 4 are 1, 0.27, 0.22 and 0.16 re-spectively. We take the weights of the linear regression asindications of the awareness for each beat. Using thoseweights we get the PAD distance with a Spearman Rankcorrelation value of 0.76 (p-value<0.001).To calculateSAD a linear regression between each beats coincidenceand similarity ratings generated the following normalizedweights for beat 1 to 4: 1, 0.075, 0.14 and 0.12 respec-tively. Again, we take the weights of the linear regressionand use them as indications of the awareness for each beat.Applying those weights we get the SAD distance whichhas a Spearman Rank correlation value of 0.81 (p-value<0.001).The resulting awareness proﬁles of both PAD and SADmetrics have a similar behaviour (see Figure 3). In bothcases the importance of the ﬁrst beat is almost 5 timeslarger than the other beats. Our experimental hypothesisis that this phenomena evidences a hierarchical organiza-tion of rhythmic elements in time where the ﬁrst elementof a rhythmic sequence is of greater importance than therest.The correlation values that have been obtained suggestthat the PAD and SAD metrics are better than previouslyexisting candidates to predict rhythmic similarity betweentwo patterns of onsets in the presence of a beat, the wayin which most of the music is experienced. The PAD andSAD metrics surpass the results found and reported in ourprevious experiment, which makes them suitable to be usedProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 669Figure 3. Awareness proﬁles of the PAD and SAD dis-tances that generated best correlations with rhythm simi-larity ratings.in real life scenarios.6. DISCUSSIONIt can be seen that the SAD metric has the highest corre-lation values with human similarity, rating slightly abovethe PAD metric, while the non-weighted metrics PD andSD are signiﬁcantly lower. This suggests that the conceptsof syncopation groups and beat awareness are perceptuallyrelevant.The drop in correlation values when there is no aware-ness weighting validates the idea of each beat having a dif-ferent importance when beat induced subjects try to makesense of them. It seems that the ﬁrst beat is the most im-portant followed by the third, the fourth and the second.The SAD metric is based on comparing if syncopationgroups are coincident between different patterns (see sec-tion 3.2). This means that a change from one family to anyother family is penalized by our algorithm despite if thechange is between syncopation to syncopation (groups 5to 7 in Table 1) or reinforcement to reinforcement (groups1 to 3 in Table 1) or if it is a change from a syncopationto a reinforcement group or to the nothing group (or viceversa). Since the SAD metric has a positive correlationwith similarity ratings, this suggests that any change be-tween groups decreases our perception of similarity. Onthe other hand, perception of rhythmic similarity is highlyinﬂuenced with the coincidence between syncopation groupsor patterns and the position of those coincidences withinthe pattern.7. PILOT VALIDATIONA straightforward application of PAD and SAD for explor-ing rhythm-similarity-based loop exploration can be dis-cussed. The simplest approach would be to use an onsetdetector to the loop signal and extract a general onset pat-tern. This would lead to a single-level pattern deprived ofany instrumental information where all musical interplay,the main information, would be lost. On the other hand,a robust source separation system would be ideal, wherean audio loop could be completely split into its differentinstrumental components and then converted to a symbolicrepresentation. But the technologies to perform such a taskare not yet reliable. An alternative would be to extract on-set patterns from meaningful frequency bands that could\nFigure 4. Predicted similarity vs similarity ratings of tenaudio loops using our methodology with PAD and SADmetrics.preserve spectral information present on the audio loop.We propose a methodology where a sound loop, of knownmetric length, is segmented every sixteenth note value andﬁltered in 23 Bark bands. This is a typical spectral repre-sentation which approximates frequency resolution of thehuman hearing. The energy peaks in each band are con-sidered as onsets and the rest as silences. In this way weconvert an audio loop into a binary matrix of onset and si-lences of 23 bands times the number of analysis windows.An audio loop is then decomposed in 23 parallel rhyth-mic patterns that can be compared with the 23 patterns ofanother audio loop measuring PAD and SAD distances be-tween bands. The sum of the band to band distances is theoverall PAD or SAD distance between two audio loops.Note that this methodology is tempo independent if theloops compared have the same known metrical length.As a pilot validation for our methodology, an experi-ment was carried out using nine different drum break loopsin audio format (downloaded from http://rhythm-lab.com).All loops were post processed to have a metrical length oftwo bars. Fifteen musically trained subjects were invited torate the rhythmic similarity between one audio loop and therest using a Likert scale divided in 5 steps, from ”very sim-ilar” to ”very different”. The mode of the results for eachpair was used as the representative similarity value and thecorrelations with PAD and SAD distances were measured.The awareness proﬁle used for both PAD and SAD was1 0.075 0.14 0.12 extracted from the results presented insection 5 (see Figure 3, right).The obtained results present (p-value<0.001) a signif-icant correlation between the similarity reported by the ﬁf-teen subjects and the PAD and SAD distances (Figure 4).The PAD distance has a 0.80 Spearman rank correlationvalue (p-value<0.01). The SAD distance has a Spear-mann correlation value of 0.75 (p-value<0.05).It is quite interesting that PAD and SAD distances pro-vide reliable similarity predictions, given the subjectivityof the task and the fact that the breaks come form very dif-ferent recordings with an obvious difference in timbre anddynamics. For this pilot validation The PAD has a highercorrelation value with the similarity ratings than the SADmetric.670 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. CONCLUSION AND FUTURE WORKBased on these results, we propose that measuring the PADand SAD distance between two rhythms with an inducedbeat as metrical context provider, is an effective way topredict human rhythmic similarity ratings. Perceptuallymotivated rhythm similarity measures that are applied toMIR problems should take into account both the synco-pation groups and a beat-awareness measure, in order tomatch subjective appreciations of rhythm similarity.The rhythms used in the foundational experiments ofour metrics are only limited to a 4/4 time signature, a 16step length, sixteenth note resolution and binary dynam-ics. Expanding the signature to other common signatures,smaller note resolutions and subtler dynamics is importantin order to broaden the validity and usefulness of our met-rics and methodology.Even though our methodology for measuring similarityamong loops yielded signiﬁcant high correlation values,both with PAD and SAD, it is important to consider thescale of the pilot validation is limited. New experimentswith a higher amount of loops should be carried out in or-der to explore the real advantages and limitations of ourmethodology.9. ACKNOWLEDGEMENTSWe would like to thank Juli´an Burbano for his help inthe analysis of the data. This research has been partiallysupported by the EU funded GiantSteps project (FP7-ICT-2013-10 Grant agreement nr 610591).10. REFERENCES[1]Erica Cao, Max Lotstein, and Philip N. Johnson-Laird.similarity and families of musical rhythms.Music Per-ception: An Interdisciplinary Journal, 31(5):444–469,2014.[2]Nick Collins. Inﬂuence in early electronic dance mu-sic: an audio content analysis investigation. InPro-ceedings of the 13th International Society for MusicInformation Retrieval Conference (ISMIR 2012), num-ber Ismir, pages 1–6, 2012.[3]Peter Desain and Henkjan Honing. Computationalmodels of beat induction: The rule-based approach.Journal of New Music Research, 28(1):29–42, 1999.[4]Simon Dixon, Fabien Gouyon, and Gerhard Widmer.Towards characterisation of music via rhythmic pat-terns. InProceedings of the 5th International Confer-ence on Music Information Retrieval (ISMIR 2004),volume 5, pages 509–516, 2004.[5]W. Tecumseh Fitch and Andrew J. Rosenfeld. Percep-tion and Production of Syncopated Rhythms.MusicPerception, 25(1):43–58, 2007.[6]Jamie Forth.Cognitively-motivated geometric methodsof pattern discovery and models of similarity in music.PhD thesis, Goldsmiths, University of London, 2012.[7]Francisco G´omez, Eric Thul, and Godfried T. Tous-saint. An experimental comparison of formal measuresof rhythmic syncopation.Proceedings of the Interna-tional Computer Music Conference, pages 101–104,2007.[8]Daniel G´omez-Mar´ın, Sergi Jord`a, and Perfecto Her-era. Strictly Rhythm: Exploring the effects of identicalregions and meter induction in rhythmic similarity per-ception. In11th International Symposium on ComputerMusic Multidisciplinary Research (CMMR), Plymouth,2015.[9]Fabien Gouyon, Simon Dixon, Elias Pampalk, andGerhard Widmer. Evaluating rhythmic descriptors formusical genre classiﬁcation. InAES 25th InternationalConference, pages 1–9, LONDON, 2004.[10]Matthias Gruhne, Christian Dittmar, and Daniel Gaert-ner. Improving Rhythmic Similarity Computation byBeat Histogram Transformations.10th InternationalSociety for Music Information Retrieval Conference(ISMIR 2009), (Ismir):177–182, 2009.[11]Catherine Guastavino, Francisco G´omez, Godfried T.Toussaint, Fabrice Marandola, and Emilia G´omez.Measuring Similarity between Flamenco RhythmicPatterns, 2009.[12]Ludger Hofmann-Engl. Rhythmic similarity: A theo-retical and empirical approach.7th International Con-ference on Music Perception and Cognition, (c):564–567, 2002.[13]Henkjan Honing. Lure(d) into listening: The potentialof cognition-based music information retrieval.Emper-ical Musicology Review, 5(4):121–126, 2010.[14]Philip N. Johnson-Laird. Rhythm and meter: A the-ory at the computational level.Psychomusicology: AJournal of Research in Music Cognition, 10(2):88–106, 1991.[15]Olivia Ladinig, Henkjan Honing, G´abor H´aden, andIstv´an Winkler. Probing Attentive and PreattentiveEmergent Meter in Adult Listeners without ExtensiveMusic Training, 2009.[16]Fred Lerdahl and Ray Jackendoff.A generative theoryof tonal music. MIT Press, 1985.[17]Justin London.Hearing in Time. Oxford UniversityPress, 2012.[18]H. Christopher Longuet-Higgins and Christopher S.Lee. The Rhythmic Interpretation of Monophonic Mu-sic.Music Perception: An Interdisciplinary Journal,1(4):424–441, 1984.[19]Marcel Mongeau and David Sankoff. Comparison ofmusical sequences.Computers and the Humanities,24(3):161–175, 1990.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 671[20]Alberto Novello, Martin M.F. McKinney, and ArminKohlrausch. Perceptual Evaluation of Inter-song Simi-larity in Western Popular Music.Journal of New MusicResearch, 40(1):1–26, 2011.[21]Keith S. Orpen and David Huron. Measurement ofsimilarity in music: A quantitative approach for non-parametric representations.Computers in music re-search, 4:1–44, 1992.[22]Caroline Palmer and Carol L. Krumhansl. Mental rep-resentations for musical meter.Journal of experimen-tal psychology. Human perception and performance,16(4):728–741, 1990.[23]Maria Panteli, Bruno Rocha, Niels Bogaards, andAline Honingh. Development of a Rhythm Similar-ity Model for Electronic Dance Music. Number Ismir,pages 537–542, 2014.[24]Geoffroy Peeters. Spectral and Temporal PeriodicityRepresentations of Rhythm for the Automatic Classi-ﬁcation of Music Audio Signal.IEEE Transactions onAudio, Speech, and Language Processing, 19(5):1242–1252, 2011.[25]Olaf Post and Godfried T. Toussaint. The Edit Distanceas a Measure of Perceived Rhythmic Similarity.Empir-ical Musicology Review, 6(3):164–179, 2011.[26]Dirk-jan Povel and Peter Essens. Perception of Tempo-ral Patterns.Music Perception1, 2(4):411–440, 1985.[27]Jeffrey Pressing. Cognitive complexity and the struc-ture of musical patterns.Noetica, 3(8):1–8, 1998.[28]Jasba Simpson and David Huron. The Perception ofRhythmic Similarity: A Test of a Modiﬁed Ver-sion of Johnson-Lairds Theory.Canadian Acoustics,21(3):89–94, 1993.[29]Leigh M. Smith. Rhythmic similarity using metricalproﬁle matching. InInternational Computer MusicConference, 2010.[30]Leigh M. Smith and Henkjan Honing. Evaluating andextending computational models of rhythmic syncopa-tion in music. InProceedings of the International Com-puter Music Conference, pages 688–691, 2006.[31]Eric Thul and Godfried T. Toussaint. Rhythm complex-ity measures: a comparison of mathematical modelsof human perception and performance. InProceedingsof the International Society for Music Information Re-trieval (ISMIR) Conference, Section 5c - Rhythm andMeter, number 8, pages 663–668, 2008.[32]Godfried T. Toussaint, Malcolm Campbell, and NaorBrown. Computational Models of Symbolic RhythmSimilarity: Correlation with Human Judgments.Ana-lytical Approaches to World Music, 1(2), 2011.[33]Anja V olk, J¨org Garbers, Peter Van Kranenburg, FransWiering, Remco C Veltkamp, and Louis P Grijp. Ap-plying rhythmic similarity based on inner metric anal-ysis to folksong research. InEighth International Con-ference on Music Information Retrieval. Austrian Com-puter Society, 2007.[34]Istv´an Winkler, G´abor P H´aden, Olivia Ladinig, Istv´anSziller, and Henkjan Honing. Newborn infants de-tect the beat in music.Proceedings of the NationalAcademy of Sciences of the United States of America,106(7):2468–2471, 2009.[35]Maria A G Witek, Eric F. Clarke, Morten L. Kringel-bach, and Peter Vuust. Effects of Polyphonic Context,Instrumentation, and Metrical Location on Syncopa-tion in Music.Music Perception: An InterdisciplinaryJournal, 32(2):201 – 217, 2014.672 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations.",
        "author": [
            "Thomas Grill",
            "Jan Schlüter"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417461",
        "url": "https://doi.org/10.5281/zenodo.1417461",
        "ee": "https://zenodo.org/records/1417461/files/GrillS15.pdf",
        "abstract": "The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and hu- man annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectro- grams with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hier- archical nature of structural organization, we explore dif- ferent strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data.",
        "zenodo_id": 1417461,
        "dblp_key": "conf/ismir/GrillS15",
        "keywords": [
            "Convolutional Neural Networks",
            "spectrogram features",
            "self-similarity lag matrices",
            "structural boundaries",
            "audio features",
            "structural organization",
            "two-level annotations",
            "boundary recognition performance",
            "significant improvement",
            "algorithm prediction"
        ],
        "content": "MUSIC BOUNDARY DETECTION USING NEURAL NETWORKSON COMBINED FEATURES AND TWO-LEVEL ANNOTATIONSThomas Grill and Jan Schl¨uterAustrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austriathomas.grill@ofai.at,jan.schlueter@ofai.atABSTRACTThe determination of structural boundaries is a key taskfor understanding the structure of a musical piece, but itis also highly ambiguous. Recently, Convolutional NeuralNetworks (CNN) trained on spectrogram features and hu-man annotations have been successfully used to tackle theproblem, but still fall clearly behind human performance.We expand on the CNN approach by combining spectro-grams with self-similarity lag matrices as audio features,thereby capturing more facets of the underlying structuralinformation. Furthermore, in order to consider the hier-archical nature of structural organization, we explore dif-ferent strategies to learn from the two-level annotations ofmain and secondary boundaries available in the SALAMIstructural annotation dataset. We show that both measuresimprove boundary recognition performance, resulting in asigniﬁcant improvement over the previous state of the art.As a side-effect, our algorithm can predict boundaries ontwo different structural levels, equivalent to the trainingdata.1. INTRODUCTIONThe decomposition of a piece of music into parts knownas movements, phrases, chorus and verse, etc., also com-monly referred to asmusical form, is an important task anda major challenge in music analysis. However, the identiﬁ-cation and exact placement of transition points, or,bound-ariesbetween such structural elements is often indistinct,even for trained human annotators. Figure 1 represents anexcerpt of the piece “The Wet Spot” by “Southern CultureOn The Skids” (index 1358 in the SALAMI collection,see Section 4.1). Two different sets of human-annotatedboundaries (ground truth) are depicted by vertical marksat the top and bottom of the plots. They clearly illustratethe ambiguity of annotating boundaries at a certain level ofdetail. The annotators agreed well on the positions of theboundaries, but for some of these they disagreed whetherthey should be considered strong (or ‘coarse’, delimitingc\u0000Thomas Grill and Jan Schl¨uter.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Thomas Grill and Jan Schl¨uter. “Mu-sic boundary detection using neural networks on combined features andtwo-level annotations”, 16th International Society for Music InformationRetrieval Conference, 2015.‘large scale’, resp., ‘functional’ sections)1or weak (‘ﬁne’,delimiting ‘small scale’ sections). This poses a problem asthe common methodology used for the evaluation of struc-tural annotation ignores the hierarchical nature and consid-ers only one level of detail, usually the coarse boundaries.The currently by far best-performing methods forboundary detection use Convolutional Neural Networks(CNNs), trained on large corpora of human-annotatedstructural annotations. The algorithms are based on mel-scaled log-magnitude spectrograms (MLSs), taking intoaccount a relatively short context of a few seconds, depend-ing on the desired precision. As shown in Figure 1a, theCNN based solely on an MLS or a variation such as MLS-HPSS (Harmonic-Percussive Source Separation, see [1]),has difﬁculties of identifying certain boundaries, indicatedby low probabilities in the prediction curve (Figure 1b).We have investigated in [3] thatself-similarity lag matrices(SSLMs, see Figures 1c and 1d) can be used as additionalalternative structural information to signiﬁcantly improveboundary detection.In this contribution, we expand on our approach bycombining more input features, and put particular fo-cus on the integration of multiple and two-level annota-tion ground-truth, as available in the SALAMI dataset.The structure of the paper is as follows: After giving anoverview over related work in Section 2, we propose ourmethod in Section 3. In Section 4, we describe the ex-perimental setup and our evaluation strategy. Section 5presents our main results. We wrap up in Section 6 witha discussion and outlook.2. RELATED WORKFollowing the overview paper by Paulus et al. [12], threefundamental approaches to music structure analysis canbe distinguished: Novelty-based, detecting transitions be-tween contrasting parts, homogeneity-based, identifyingsections that are consistent with respect to their musicalproperties, and repetition-based, building on the determi-nation of recurring patterns. Novelty is typically computedfrom self-similarity matrices (SSMs) or self-distance ma-trices (SDMs) by sliding a checkerboard kernel along thediagonal [2], building on audio descriptors like MFCCs,pitch class proﬁles, or rhythmic features [10]. Turnbull1See [16] and SALAMI Annotator’s Guide,http://www.music.mcgill.ca/˜jordan/salami/SALAMI-Annotator-Guide.pdf, accessed 2015-05-04531(a) Mel-scaled log-magnitude spectrogram\n(b) CNN predictions for coarse boundaries on HPSS-decomposedmel spectrogram\n(c) Self-similarity lag matrix, 88 seconds context (SSLM far)\n(d) Self-similarity lag matrix, 14 seconds context (SSLM near)\n(e) CNN predictions for coarse boundaries on SSLMs near andfar combined\n(f) CNN predictions for coarse boundaries on MLS-HPSS andSSLMs near and far combined\n(g) CNN predictions for ﬁne boundaries on MLS-HPSS andSSLMs near and far combinedFigure 1: Boundary recognition using CNNs on dif-ferent underlying audio features, illustrated on thepiece “The Wet Spot” by “Southern Culture On TheSkids”. Two sets of human annotation ground-truthare shown in red on top and bottom of each plot.Coarse boundaries are thick, ﬁne boundaries are thin.Visithttp://www.ofai.at/research/impml/projects/audiostreams/ismir2015for aversion with audio.et al. [17] compute difference features on more com-plex audio feature sets and use trained Boosted DecisionStumps for boundary detection. In order to capitalize onrepeated patterns, SSMs or SDMs are used with variousheuristic rules and optimization schemes for structure for-mation [4, 9, 11]. McFee and Ellis employ spectral clus-tering [6], or add a supervised learning scheme using or-dinal linear discriminant analysis and constrained cluster-ing [5]. When using end-to-end neural network techniquessuch as Ullrich et al.’s CNNs [18], the separation betweenthe fundamental approaches becomes blurred as the CNNinfers the relationships between audio features and groundtruth from the provided training data. In a similarly integralfashion, Serr`a et al. [15] propose an unsupervised methodexplicitly combining all three domains.3. PROPOSED METHODOur approach is derived from the work by Ullrich et al.[18]. In the following, we will mainly describe our exten-sions to this method.3.1 Feature extractionFor each audio ﬁle under analysis, we ﬁrst compute a STFTmagnitude spectrogram with a window size of 46 ms (2048samples at 44.1 kHz sample rate) and 50% overlap, and ap-ply a mel-scaled ﬁlterbank ofn= 80triangular ﬁlters from80 Hz to 16 kHz and scale magnitudes logarithmically.From this MLS we compute a HPSS decompositionwith a kernel size of21⇥21bins. Preliminary experimentsshowed that the actual size is a rather insensitive parame-ter. We either use MLS only or MLS-HPSS (two parallelchannels) as one part of the network input.Our method of generating the SSLMs, which representsimilarities of the MLS at one point in time in relation topoints in the past, up to a certainlag time, is derived fromwork by Serr`a et al. [15] and described in detail in [3].We use the MLS time seriesxi=1...Nfrom above, down-sample it by max-pooling of a factorp=2, and apply aDCT-II transformation on each frame with the static com-ponent omitted. Several of these frames are concatenatedwithin a local time context ofLbins, equivalent to 0.1 sec-onds, resulting in the time seriesˆxi. A cosine distancefunction\u0000cos(x,y)=1\u0000Dxkxk,ykykEis used to build thebNpc⇥bLpcrecurrence matrixDi,l=\u0000cos(ˆxi,ˆxi\u0000l),l=1...bLpc.(1)To reveal relationships between distances across this ma-trix, adaptive thresholding is performed with a smooth sig-moid transfer function\u0000(x)=1/(1 +e\u0000x), yieldingRi,l=\u0000✓1\u0000Di,l\"i,l◆.(2)The adaptive threshold, or, in this context, equalization fac-tor\"i,lis set to a quantileQwith=0.1of the distances\u0000cos(ˆxi,ˆxi\u0000j)and\u0000cos(ˆxi\u0000l,ˆxi\u0000l\u0000j)forj=1...bLpc,532 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015or\"i,l=Q⇣Di,1,...,Di,bLpc,Di\u0000l,1,...,Di\u0000l,bLpc⌘.(3)All indicesi<1are wrapped around toi0=i+bNpc,resulting in a time-circular SSLM.3.2 Feature preprocessingLike [18], for the MLS features, we pad the spectrogramwith pink noise of\u000070dB FS as needed to process thebeginning and end of a piece. For the MLS-HPSS vari-ant, the harmonic and percussive components are sepa-rated at this point. After subsampling the MLS by takingthe maximum over 6 adjacent time frames without overlap(max-pooling), we normalize to zero mean and unit vari-ance for each frequency band. For the SSLM features, weuse circular padding and pooling factors examined in [3]:A factor of 3 for a time context of 14 seconds (feature‘SSLM-near’), and a factor of 19 for a context of 88 sec-onds (feature ‘SSLM-far’). We then also normalize eachlag band to zero mean and unit variance.3.3 Convolutional neural networkCNNs are feed-forward networks that includeconvolu-tional layerscomputing a convolution of their input withsmall learned ﬁlter kernels of a given size. This allowsprocessing large inputs with few trainable parameters, andretains the input’s spatial layout. When used for binaryclassiﬁcation, the network usually ends in one or moredense layers integrating information over the full input atonce, discarding the spatial layout. Our architecture forthis work is based on the one used by Ullrich et al. [18] onMLS features for their MIREX submission [14]. It has aconvolutional layer of 328⇥6kernels (8 time frames and6 frequency bands), a max-pooling layer of3⇥6, anotherconvolution of 646⇥3kernels, a dense layer of 128 unitsand a dense output layer of 1 unit.We employ a variant of this architecture to support mul-tiple input features instead of one. A comparison of differ-ent architectural variations has been shown in [3], wherea late ‘time-synchronous fusion’ of the input features, per-formed in the last convolutional layer, yielded the best re-sults: since the input features cover the same temporal con-text at the same resolution, their feature maps can be syn-chronously convolved over time. Figure 2 shows the un-derlying CNN architecture used for all experiments in ourstudy. The inputs (bottom) are varied, e.g., MLS only isused instead of MLS-HPSS, or one of the input legs is leftout. For the outputs (top), either only the coarse unit isused, or both coarse and ﬁne.Training is done by mini-batch gradient descent, us-ing the same hyper-parameters and tweaks as Ullrich et al.[18]. Likewise, we follow the peak-picking strategy de-scribed therein to retrieve likely boundary locations fromthe network output.\nconvpool\nconvpooldense\nMLS-HPSSSSL M-nearconvconvconv\nconvpool\nSSLM-farconv\ncoarse\nﬁnedenseFigure 2: The CNN architecture in use for all the models.The full model is shown here, inputs or outputs were variedfor the different experiments.4. EXPERIMENTS4.1 Data setWe base our experiments on the data set described by Ull-rich et al. [18] which is a subset of the Structural Analysisof Large Amounts of Music Information (SALAMI) [16]version 1.2 database. A part of this SALAMI 1.2 data setwas also used in the “Audio Structural Segmentation” taskof the annual MIREX evaluation campaign in the years2012 through 2014.2Lately, the data set has been up-dated to version 2.03with a large number of issues ﬁxed.The entire data set contains over 1600 musical recordingsof different genres and origins. In SALAMI version 2.0,a total of 1164 recordings (with 763 double-annotated) arepublicly available. Identically to [18], we used 633 musi-cal pieces for training, 100 for validation and 487 piecesas a test set for ﬁnal evaluation of our models against thepublished results of the various MIREX submissions.4.2 EvaluationFor the MIREX campaign’s boundary retrieval task, threedifferent evaluation measures are used:Hit ratefor timetolerances±0.5and±3seconds, andMedian deviation.The latter computes the median time distance betweeneach annotated boundary and its closest predicted bound-ary, and vice versa. The former checks which predictedboundaries fall close enough to an unmatched annotatedboundary (true positives), records remaining unmatchedpredictions and annotations as false positives and nega-tives, respectively, and computes the precision, recall andF1scores. The Hit rate F1score is the measure most fre-quently used in the literature.2Music Information Retrieval Evaluation eXchange,http://www.music-ir.org/mirex, accessed 2015-04-303https://github.com/DDMAL/salami-data-public/releases/tag/2.0, accessed 2015-04-30Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 533As explicated in [18], baseline scores can be estimatedusing variations of regularly or randomly spaced grids assynthetic boundary estimates. For an evaluation toleranceof±0.5seconds, the baseline within our test data set isF1⇡0.15. Upper bounds, on the other hand, can be de-rived from the differences between two independent anno-tations of the same musical pieces. By analyzing the itemswithin our test data set that have been annotated twice (439pieces), we calculatedF1⇡0.74.In the existing literature, both tolerances of±0.5and±3seconds are commonly used. For this contribution,due to space constraints, we only evaluate for±0.5sec-onds, where the explorable space, that is, the distance be-tween the lower baseline and the upper bound exhibitedin human ground-truth annotations is much greater thanfor±3seconds (with lower and upper bounds at 0.33 and0.80, respectively). Our evaluation code is equivalent tothe boundary detection implemented inmireval[13],omitting the borders at the beginning and end of soundﬁles.Nieto et al. [8] have identiﬁed the F0.58measure to bemore perceptually informative than the typically used F1measure. As this is a relatively new ﬁnding and it is not aswell established as the F1measure (which is, e.g., used inMIREX), we base threshold optimization and model selec-tion on the latter.4.3 Combination of featuresBuilding on [3], we combine mel-scaled log-magnitudespectrograms (MLS) and self-similarity lag matrices(SSLM) as input features to the CNN. A decompositionof MLS into harmonic and percussive components (fea-ture ‘MLS-HPSS’) and the combination of two SSLMs,one a high-resolution, low lag matrix, the other one a low-resolution, high lag matrix, provides even more structuralinformation to the network. We mainly compare two mod-els: ‘MLS + SSLM-near’ (the model developed in [3]),and the more complex and computationally more expen-sive model ‘MLS-HPSS + combined SSLM’, integratingall available input features.The different input features are fused at a relativelylate stage in the network (see Figure 2), using a convo-lutional layer which spans all the vertical (frequency orlag time) components, but only a very short time context.This is motivated by the assumption that the input featuresare strongly correlated in time. Figure 3 shows boundaryrecognition scores for the ‘MLS + SSLM-near’ model andthree different context widths (1, 3 and 5 bins), evaluatedon the validation set. As can be seen, a temporal contextfor the fusion layer of more than a single bin does not im-prove the results.4.4 Consideration of multiple annotationsUp to now, CNN-based boundary recognition algorithmshave been trained on data sets with just one annotationversion per music piece. SALAMI data contains dou-ble annotations for the majority of training examples. ItFigure 3: Comparison of boundary recognition F1scoresfor different widths of the CNN fusion layer.\nFigure 4: Comparison of boundary recognition F1scoresfor different models trained with single and multiple anno-tations.is worth inspecting whether multiple, potentially contra-dicting annotations help or confuse the CNN training pro-cess. Figure 4 shows the results for three different modelstrained with single and multiple annotations, respectively,evaluated on the validation set. Employing multiple anno-tations by duplicating audio features and applying the alter-native target annotations, the number of training examplesincrease from1198707(with70317⇥3positive examples)to1670944(98913⇥3positive examples) data points, cor-responding to+39%. A positive effect can be observed formodels with more versatile structural information availablefor the network. In these cases, the increase of theF1scoreis in the range of 1–2%.4.5 Integration of ﬁne annotationTraditionally, boundary detection in MIR has been per-formed on only one structural level. As motivated in Sec-tion 1, we would like to deal with the ambiguity of anno-tating boundaries at a certain level of detail by capitalizingon the two-level annotations present in the training dataset. This way, the neural network should be able to reﬁneits distinction between main and secondary boundaries.We explored three different modes for the combinationof coarse and ﬁne boundaries: Firstly, by using only onetarget output vector by assigning full training weights tocoarse labels and reduced training weights (e.g., factors of0.3or0.5) to ﬁne labels. Secondly, by using two targetoutputs with equal weights, one for the coarse labels andone for the ﬁne labels (‘concat’ mode). And ﬁnally, usingtwo target outputs, with coarse labels and full weights as-signed to the ﬁrst output vector. Fine labels are assigned tothe second output vector, but only where they are distinctfrom a coarse label (‘contrast’ mode). This should create amore pronounced contrast between coarse and ﬁne labels534 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 5: Comparison of boundary recognition F1scoresfor different integration modes of the second-level ‘ﬁne’annotation, evaluated on our validation set.with the potential danger of some contradiction.Not for all of our training data two-level annotationswere available. We tried two variations: For the ﬁrst one,we put coarse boundaries where ﬁne ones were not avail-able (‘ﬁne + coarse’), and for the second one, we used onlythose annotations with two levels available (‘ﬁne only’),effectively reducing the number of training examples in-cluding multiple annotations to1224891(with74400⇥3positive examples).Figure 5 shows the results for the three combinationmodes (with different weighting parameters) and two dataset variations, computed on MLS input features and eval-uated on the validation set. The combination modes forcoarse and ﬁne data with two output vectors perform bet-ter than the ones with only one output. The ‘contrast’ modeexhibits instabilities for the results, most probably due tothe relatively small validation data set. We selected thebest-performing and reliable ‘concat’ mode with two out-put units as our working model. The distinction between‘ﬁne + coarse’ and ‘ﬁne only’ variations is more or less in-conclusive, with very little advantage for the latter. How-ever, as the spreading ofF1scores is less for ‘ﬁne only’,we settled for this variation of the ‘concat’ mode.5. RESULTSFigure 6 shows boundary recognition scores (on the pri-mary ‘coarse’ boundaries) of several of our models, withpeak-picking thresholds optimized on the validation set,and results evaluated on the test set. Each model variationhas been trained and evaluated ﬁve times. The individual,mean and ‘bagged‘ results are shown in the graph. ‘Bag-ging’ means that the outputs of all ﬁve models are aver-aged and peak-picking is performed on the result, therebyreducing statistical variations. Using a MLS-HPSS de-composition does not score signiﬁcantly higher than MLSonly. Likewise, using a combination of SSLM ‘near’ (14seconds lag, high resolution) and ‘far’ (88 seconds lag,low resolution) does not score higher than SSLM ‘near’only. However, in combination, it can be seen that all‘MLS-HPSS + combined SSLM’ results are higher thantheir respective equivalents of ‘MLS + SSLM-near’. Forboth combined models, using multiple annotations raisesthe scores relative to single annotations. Additional ﬁneAlgorithmF1F.58Rec. Prec.Upper bound (est.).74 .74All features, multi+ﬁne ann..508.529 .502 .572MLS+SSLM-near, multi+ﬁne.496 .506 .509 .536MLS+SSLM-near, single ann..469 .466 .504 .475SUG1 (2014).422 .442 .422 .490MP2 (2013).294 .280 .362 .271MP1 (2013).276 .270 .311 .269NB1 (2014).270 .246 .374 .229KSP2 (2012).263 .231 .422 .209Baseline (est.).15 .21Table 1: Boundary recognition scores for coarse bound-aries at a tolerance of±0.5seconds, evaluated on ourSALAMI 2.0 test dataset. Comparison of our models(in italics) with the ﬁve best-performing algorithms of theMIREX campaigns 2012 through 2014.AlgorithmF1F.58Rec. Prec.Upper bound (est.).75 .76All features, multi+ﬁne ann..485.523 .443 .587MLS+SSLM-near, multi+ﬁne.478 .515 .439 .576Baseline (est.).23 .17Table 2: Boundary recognition scores of two of our mod-els for ‘ﬁne’, second-level boundaries at a tolerance of±0.5seconds, evaluated on our SALAMI 2.0 test dataset.annotations for CNN training further increase the scores.On the right-hand-side of Figure 6 different feature com-binations using multiple ﬁne annotations are shown. Themore perspectives on the audio provided as input, thehigher the scores.See Table 1 for a listing of our results in comparison tothe best-performing algorithms of the MIREX campaigns2012 through 2014. All results have been evaluated onSALAMI 2.0 data. Note that the scores are generally lowerthan for SALAMI 1.2 annotations (cf. [18]). The reasonis that in the new data set version many formerly ‘trivialboundaries’ (sitting at the beginning or end of sound ﬁles)have been corrected. These boundaries have moved awayfrom the borders and are now headed, or trailed, respec-tively, by silence or crowd noise, and are therefore moredifﬁcult to predict. The ‘MLS+SSLM-near’ model trainedwith single annotations is equivalent to the model usedin [3], with an additional dense layer in the present work.‘All features’ denotes the ‘MLS-HPSS + combined SSLM’model, yielding the best boundary prediction results.Table 2 lists boundary recognition results of the ‘ﬁne’output unit of our network, trained and evaluated on the‘small-scale’, second-level annotations of the SALAMI 2.0data set. To our knowledge, only McFee and Ellis [6] haveso far evaluated their algorithms (as well as SMGA [15])on the secondary boundaries. They reportF1scores up to0.292±0.15on the SALAMI 1.2 dataset.Table 3 presents boundary recognition results on theBeatles-ISO dataset,4comprised of all 12 Beatles al-bums with 180 songs in total. We used the best-scoringmodel from above, using all input features, trained on4http://isophonics.net/content/reference-annotations-beatles, accessed 2015-04-30Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 535Figure 6: Comparison of boundary recognition F1scores on SALAMI 2.0 data for different models under examination.Threshold optimization performed on validation set, evaluation done on test set.AlgorithmF1F.58Rec. Prec.All features, multi+ﬁne ann..558.590 .522 .640MLS+SSLM-near, multi+ﬁne.526 .553 .500 .597SUG1.424 .457 .385 .510MP2-beatles.334 .321 .376 .311MP2-salami.322 .313 .355 .309NB1.286 .274 .332 .266MP1.278 .280 .285 .285NB2.266 .255 .302 .247NB3.227 .211 .287 .200Baseline (est.).15 .22Table 3: Boundary recognition scores at a tolerance of±0.5seconds, evaluated on the Beatles-ISO dataset (180songs). Two of our models are compared to several pub-lished state-of-the-art algorithms.SALAMI 2.0 with multiple coarse and ﬁne annotations.We were able to compare the predictions of our CNN tothe best-performing algorithms of last years’ MIREX sub-missions by Schl¨uter et al. (SUG1, personal communica-tion), McFee and Ellis [5] (MP1 and MP2, the latter opti-mized either for SALAMI and Beatles data),5and Nietoand Bello [7] (NB1, NB2, NB3),6respectively. Note thatthe scores of our models are above those of other state-of-the-art algorithms by a large margin, although we have nottrained or tuned our models in any way speciﬁcally on thekind of music realized by the Beatles.6. DISCUSSION AND OUTLOOKIn this contribution, we have dealt with the prediction ofmusically relevant structural boundaries, focused primarilyon the stylistically mixed SALAMI data set in its latestversion 2.0, with additional evaluation on the Beatles-ISOdata set.We have re-used the CNN architecture developed in [3]with some modiﬁcations. On the one hand, we have fed it a5https://github.com/bmcfee/olda, accessed 2015-05-016https://github.com/urinieto/SegmenterMIREX2014, accessed 2015-05-01combination of different input features and have been ableto show that the CNN is able to produce highest-scoringresults with HPSS-decomposed mel-scaled spectrograms(MLS) in combination with self-similarity lag matrices(SSLMs) on two different time-scales, covering both struc-tural detail and longer time context. On the other hand, wehave taken advantage of the fact that the SALAMI data setis annotated on two structural levels, and, for the most part,by two independent annotators. The integration of this sup-plementary data helps the CNN to take better informed de-cisions between primary and secondary boundaries. Eval-uated on SALAMI 2.0 data, we have been able to raisethe state of the art from the best MIREX submission [14]atF1=0.422, and our previous point of reference [3] atF1=0.469to the score ofF1=0.508for the best model,integrating all available input features, as well as multipleand two-level annotations. As the CNN model trained ontwo-level annotation possesses two output units, its subse-quent application also yields two independent predictionsfor ‘coarse’ and ‘ﬁne’ boundaries.Although we have not touched (nor listened to) musicby the Beatles while developing our models, evaluation onthis data set reveals that our models are quite robust, yield-ing a boundary recognition score ofF1=0.558, which issigniﬁcantly higher than the previously published state ofthe art.We are still actively exploring the possibilities of CNNsapplied to music structure discovery. That said, we haveneither exhaustively researched the space of possible inputfeatures, nor all meaningful variations of model architec-ture and learning parameters. There is plenty of remainingheadroom to the ‘upper bound’ inter-annotatorF1scores.7. ACKNOWLEDGMENTSThis research is funded by the Federal Ministry for Trans-port, Innovation & Technology (BMVIT) and the Aus-trian Science Fund (FWF) through project TRP 307-N23and the Vienna Science and Technology Fund (WWTF)through project MA14-018.536 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Derry Fitzgerald. Harmonic/percussive separation us-ing median ﬁltering. InProceedings of the 13th Inter-national Conference on Digital Audio Effects (DAFx-10), Graz, Austria, 2010.[2]Jonathan Foote. Automatic audio segmentation usinga measure of audio novelty. InProceedings of IEEEInternational Conference on Multimedia and Expo(ICME’00), volume 1, pages 452–455, New York,USA, 2000.[3]Thomas Grill and Jan Schl¨uter. Music Boundary De-tection Using Neural Networks on Spectrograms andSelf-Similarity Lag Matrices. InProceedings of the23rd European Signal Processing Conference (EU-SPICO 2015), Nice, France, 2015.[4]Lie Lu, Muyuan Wang, and Hong-Jiang Zhang. Re-peating pattern discovery and structure analysis fromacoustic music data. InMIR ’04: Proceedings of the6th ACM SIGMM international workshop on Multime-dia information retrieval, pages 275–282, New York,USA, 2004.[5]Brian McFee and Daniel P. W. Ellis. Learning to seg-ment songs with ordinal linear discriminant analysis.InInternational conference on acoustics, speech andsignal processing, ICASSP, 2014.[6]Brian McFee and Daniel PW Ellis. Analyzing songstructure with spectral clustering. InProceedings ofthe 15th International Society for Music InformationRetrieval Conference (ISMIR 2014), pages 405–410,Taipei, Taiwan, 2014.[7]Oriol Nieto and Juan Pablo Bello. Music SegmentSimilarity Using 2D-Fourier Magnitude Coefﬁcients.InProceedings of the 39th IEEE International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP), pages 664–668, Florence, Italy, 2014.[8]Oriol Nieto, Morwaread M Farbood, Tristan Jehan, andJuan Pablo Bello. Perceptual analysis of the f-measurefor evaluating section boundaries in music. InProceed-ings of the 15th International Society for Music In-formation Retrieval Conference (ISMIR 2014), pages265–270, Taipei, Taiwan, 2014.[9]Jouni Paulus and Anssi Klapuri. Music structure anal-ysis by ﬁnding repeated parts. InAMCMM ’06: Pro-ceedings of the 1st ACM workshop on Audio and musiccomputing multimedia, pages 59–68, New York, USA,2006.[10]Jouni Paulus and Anssi Klapuri. Acoustic features formusic piece structure analysis. InProceedings of the11th International Conference on Digital Audio Effects(DAFx-08), Espoo, Finland, 2008.[11]Jouni Paulus and Anssi Klapuri. Music structureanalysis using a probabilistic ﬁtness measure and agreedy search algorithm.IEEE Transactions on Audio,Speech, and Language Processing, 17(6):1159–1170,2009.[12]Jouni Paulus, Meinard M¨uller, and Anssi Klapuri.Audio-based music structure analysis. InProceedingsof the 11th International Conference on Music Infor-mation Retrieval (ISMIR 2010), pages 625–636, 2010.[13]Colin Raffel, Brian McFee, Eric J Humphrey, JustinSalamon, Oriol Nieto, Dawen Liang, and Daniel PWEllis. mireval: A transparent implementation of com-mon MIR metrics. InProceedings of the 15th Interna-tional Society for Music Information Retrieval Confer-ence (ISMIR 2014), Taipei, Taiwan, 2014.[14]Jan Schl¨uter, Karen Ullrich, and Thomas Grill. Struc-tural segmentation with convolutional neural networksmirex submission. InTenth running of the Music Infor-mation Retrieval Evaluation eXchange (MIREX 2014),2014.[15]Joan Serr`a, Meinard M¨uller, Peter Grosche, andJosep Ll. Arcos. Unsupervised music structure an-notation by time series structure features and seg-ment similarity. InIEEE Transactions on Multimedia,16(5):1229–1240, 2014.[16]Jordan Bennett Louis Smith, John Ashley Burgoyne,Ichiro Fujinaga, David De Roure, and J StephenDownie. Design and creation of a large-scale databaseof structural annotations. InProceedings of the 12thInternational Society for Music Information RetrievalConference (ISMIR 2011), pages 555–560, 2011.[17]Douglas Turnbull, Gert Lanckriet, Elias Pampalk, andMasataka Goto. A supervised approach for detect-ing boundaries in music using difference features andboosting. InProceedings of the 8th International Con-ference on Music Information Retrieval (ISMIR 2007),pages 51–54, 2007.[18]Karen Ullrich, Jan Schl¨uter, and Thomas Grill. Bound-ary Detection in Music Structure Analysis using Con-volutional Neural Networks. InProceedings of the 15thInternational Society for Music Information RetrievalConference (ISMIR 2014), Taipei, Taiwan, 2014.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 537"
    },
    {
        "title": "Comparing Voice and Stream Segmentation Algorithms.",
        "author": [
            "Nicolas Guiomard-Kagan",
            "Mathieu Giraud",
            "Richard Groult",
            "Florence Levé"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414916",
        "url": "https://doi.org/10.5281/zenodo.1414916",
        "ee": "https://zenodo.org/records/1414916/files/Guiomard-KaganG15.pdf",
        "abstract": "Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a bet- ter understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with poly- phonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Tem- perley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the al- gorithms on a corpus containing the 48 fugues of Well- Tempered Clavier by J. S. Bach as well as 97 files of pop- ular music containing actual polyphonic information. We discuss how to compare together voice and stream segmen- tation algorithms, and discuss their strengths and weak- nesses.",
        "zenodo_id": 1414916,
        "dblp_key": "conf/ismir/Guiomard-KaganG15",
        "keywords": [
            "polyphonic data",
            "musical score",
            "voice segmentation",
            "stream segmentation",
            "contiguous notes",
            "close pitches",
            "four algorithms",
            "corpus of fugues",
            "pop music",
            "evaluation metrics"
        ],
        "content": "COMPARING VOICE AND STREAM SEGMENTATION ALGORITHMSNicolas Guiomard-KaganMIS, U. Picardie Jules VerneAmiens, FranceMathieu GiraudCRIStAL (CNRS, U. Lille)Lille, FranceRichard Groult Florence LevéMIS, U. Picardie Jules Verne (UPJV)Amiens, France{nicolas, mathieu, richard, florence}@algomus.frABSTRACTV oice and stream segmentation algorithms group notesfrom polyphonic data into relevant units, providing a bet-ter understanding of a musical score. V oice segmentationalgorithms usually extract voices from the beginning to theend of the piece, whereas stream segmentation algorithmsidentify smaller segments. In both cases, the goal can beto obtain mostly monophonic units, but streams with poly-phonic data are also relevant. These algorithms usuallycluster contiguous notes with close pitches. We propose anindependent evaluation of four of these algorithms (Tem-perley, Chew and Wu, Ishigakiet al., and Rafailidiset al.)using several evaluation metrics. We benchmark the al-gorithms on a corpus containing the 48 fugues ofWell-Tempered Clavierby J. S. Bach as well as 97 ﬁles of pop-ular music containing actual polyphonic information. Wediscuss how to compare together voice and stream segmen-tation algorithms, and discuss their strengths and weak-nesses.1. INTRODUCTIONPolyphony, as opposed to monophony, is a music createdby simultaneous notes (see Figure 1) coming from severalinstruments or even from a single polyphonic instrument,such as the piano or the guitar. Polyphony usually implieschords and harmony, and sometimes counterpoint whenthe melody lines are independent.V oice and stream segmentation algorithms group notesfrom polyphonic symbolic data into layers, providing abetter understanding of a musical score. These algorithmsmake inference and matching for relevant patterns easier.They are often based on perceptive rules as studied byHuron [7] or Deutsch [6]. Chew and Wu gathered theserules into four principles [2]:•(p1)V oices are monophonic;c\u0000Nicolas Guiomard-Kagan, Mathieu Giraud, RichardGroult, Florence Levé.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Nicolas Guiomard-Kagan, MathieuGiraud, Richard Groult, Florence Levé. “Comparing V oice and StreamSegmentation Algorithms”, 16th International Society for Music Infor-mation Retrieval Conference, 2015.\nFigure 1: In this piano-roll representation, each segmentdescribes a note. The horizontal axis represents time andthe vertical axis represents the pitch.•(p2)At least once, all voices must be played simul-taneously;•(p3)Intervals are minimized between successive notesin the same stream or voice (pitch proximity);•(p4)V oices tend not to cross.Voice segmentationalgorithms extract voices from thebeginning to the end of the piece. Usually, the resultingvoices are monophonic (p1) and, at some point, all thevoices do appear (p2). The algorithms described by Chewand Wu [2] and Ishigakiet al.[9] ﬁrst identifycontigsofnotes, then link these contigs. These algorithms will be dis-cussed later. De Valket al.[5] proposed a machine learningmodel with a neural network to separate voices in lute tab-latures. The study of Kirlin and Utgoff [13] uses anothermachine learning model to separate voices, taking in con-sideration both actual polyphony andimplicitpolyphony,such as the one obtained with arpeggios.Stream segmentationalgorithms identify segments gen-erally smaller than complete voices. Astreamis a groupof coherent notes, usually respecting principles such asp3andp4. Temperley’s algorithm [17] extracts streamswith respect to several constraints. Rafailidiset al.’s al-gorithm [16], based on an earlier work by [11], uses ak-nearest neighbors clustering technique on individual notes.Both algorithms will be discussed in Sections 3.1 and 3.2.The study by Madsen and Widmer [15], inspired by Tem-perley [17], allows crossing voices. The method of Kilianand Hoos [12] starts by cutting the input score into sec-tions calledslicessuch that all the notes of a slice overlap;Then, an optimization method involving several evaluation493functions is applied to divide and combine the slices intovoices; The output voices can contains chords.Depending on the algorithms, the predicted streams canthus be small or large. However, such algorithms do pre-dict groups of notes, especially contiguous relevant notes,and may thus be compared against full voice segmentationalgorithms. De Nooijeret al.[4] made a comparison byhumans of several voice and stream separation algorithmsfor melody ﬁnding.In this paper, we independently evaluate some of thesealgorithms, benchmarking in the same framework voiceand stream segmentation algorithms. We compare somesimple and efﬁcient algorithms that were described in thelitterature [2, 9, 16] and added the algorithm in [17] forwhich an implementation was freely available. Our corpusincludes Bach’s fugues (on which many algorithms wereevaluated) but also pop music containing polyphonic ma-terial made of several monophonic tracks. The next twosections detail these algorithms. Section 4 presents theevaluation corpus, code, and methods. Section 5 detailsthe results and discusses them.2. VOICE SEPARATION ALGORITHMS2.1 BaselineTo compare the different algorithms, we use a very simplereference algorithm, based on the knowledge of the totalnumber of voices (p2). Thebaseline algorithmassigns areference pitch for each voice to be predicted, then assignseach note to the voice which has the closest reference pitch(Figure 2).\nFigure 2: The baseline algorithm assigns each note to thevoice having the closest reference pitch. This referencepitch is computed by averaging pitches on segments havingthe highest number of simultaneous notes. Here the middlevoice, V oice 1, has a reference pitch that is the average ofthe pitches of notes 7, 9 and 11.2.2 CWThe CW algorithm separates voices by using the four prin-ciples (p1, p2, p3, p4) [2].Contigs.The ﬁrst step splits the input data into blockssuch that the number of notes played at the same time dur-ing one block does not change. Moreover, when a notecrosses the border of two blocks and stops or starts to soundinside a block, the block is split in two at this time. Theobtained blocks are calledcontigs(Figure 3). By construc-\nFigure 3: Four contigs: Contig 3 contains three fragments,{6},{7,9,11}and{8,10}.tion, the number of played notes inside a contig is constant.Notes are grouped from the lowest to the highest pitch invoice fragments(Figure 3).\nFigure 4: Connection Policy: All fragments are connectedwith respect top3.Connection Policy.The second step links together frag-ments from distinct contigs (see Figure 4). The contigscontaining the maximal number of voices are calledmax-imal voice contigs(p2). The connection starts from thesemaximal contigs: Since the voices tend not to cross, theorder of the voices attributed to fragments of these contigshas a strong probability to be the good one (p2andp4).Given two fragments in contiguous contigs, CW deﬁnesaconnection weight, depending onn1, the last note of theleft fragment, and onn2, the ﬁrst note of the right frag-ment. Ifn1andn2are two parts of the same note, thisweight is\u0000K, whereKis a large integer, otherwise theweight is the absolute difference between the pitches ofthe two notes (p3). The fragments connected between twocontigs are the ones which minimize the total connectionweight (Figure 5).2.3 CW-PrioritizedIshigakiet al.[9] proposed a modiﬁcation of CW algo-rithm in the merging step between the contigs. Their keyobservation is that theentry of a voiceis often non am-biguous, contrary to the exit of a voice that can be a “fadeout” which is difﬁcult to precisely locate. Instead of start-ing from maximal voice contigs, they thus choose to start494 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 5: Connection between contigs: The selected linksare the ones minimizing the total weight (3 + 4 = 7).only from adjacent contigs with anincreasingnumber ofvoices. For example in Figure 3, the procedure starts bymerging Contig 1 with Contig 2. The choice of mergedfragments is identical to the method described in CW al-gorithm. After the merge of all fragments of two adjacentcontigsc1andc2, we get a new contig containing the samenumber of voices than inc2(see Figure 6).\nFigure 6: Contig combining: Contigs 1, 2, then 3 are com-bined, resulting in a Contig 1+2+3 with 3 voices.The procedure described above is reiterated as long astwo adjacent contigs have an increasing number of voices.If at the end of this procedure, there is more than one con-tig, they are merged by the original CW connection policy.3. STREAM SEGMENTATION ALGORITHMSWe also studystream segmentationalgorithms, which donot segment a score into voices but into streams that mayinclude overlapping notes. Streams can be melodic frag-ments, but also can cluster related notes, such as chords. Avoice can be thus split into several streams, and a streamcan cluster notes from different voices.3.1 StreamerThe algorithm proposed by Temperley extracts streams whilerespecting several constraints [17]. The ﬁrst constraint ispitch proximity: two contiguous notes with close pitchesare placed in the same stream (p3). The second constraintis temporal: when there is a long rest between two notes,the second note is put into a new stream (Figure 7). Thelast principle allows the duplication of a note in two voices(provided that the streams do not cross,p4).\nFigure 7: Due to the rest afternote 2, Streamer assigns notes 1and 2 to a stream that does notinclude any other notes.\nFigure 8: Stream Segment as-signs notes 12, 13, 14, and 15 ina same stream. The notes 13-15can be seen as a transposition ofnotes 12-14, forming a succes-sion of chords.3.2 Stream SegmentThe algorithm by Rafailidiset al.[16] clusters notes basedon ak-nearest-neighbors clustering. The algorithm ﬁrstcomputes a distance matrix, which indicates for each pos-sible pair of notes whether they are likely to belong to thesame stream. The distance between two notes is computedaccording to their synchronicity (Figure 8), pitch and onsetproximity (among others criteria); then for each note, thelist of itsk-nearest neighbors is established.3.3 CW-ContigsWe ﬁnally note that the ﬁrst step of the CW algorithm (con-tig creation) can be considered as a stream segmentation al-gorithm. We call this ﬁrst step CW-Contigs. For example,on the Figure 3, this method creates 8 streams correspond-ing to the 8 voice fragments of the four contigs.4. EVALUATION CORPUS AND METRICS4.1 Evaluation corpusUsually these algorithms are evaluated on classical music,in particular on counterpoint music such asfugues, wherethe superposition of melodic lines gives a beautiful har-mony. As a fugue is made up several voices, this naturallyconstitutes a good benchmark to evaluate voice separationalgorithms [2, 5, 9–11, 15–17]. We thus evaluated the al-gorithms on the 48 fugues of the two books of theWell-Tempered Clavierby J.-S. Bach1.We also wanted to evaluate other forms of polyphonicwriting. The problem is to have a ground truth for this task.From a set of 2290 MIDI ﬁles of popular music, we formeda corpus suitable for the evaluation of these algorithms. Wefocused on MIDI tracks (and not on MIDI channels). Wekept only “monophonic” tracks (where at most one note isplayed at any time) of sufﬁcient length (at least 20 % ofthe length of the longest track). We deleted the tracks cor-responding to the drums. We considered each remaining1.krnﬁles downloaded fromkern.ccarh.org[8]Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 495corpuswtc-iwtc-iipopﬁles242497voices3.53.43.0notes10411071874Table 1: Files, and aver-age number of voices andnotes for each corpus.track as an independant voice. Finally, we kept 97 MIDIﬁles with at least 2 voices, composed on average of 3.0voices (see Table 1).4.2 Evaluation codeWe implemented the algorithms CW-Contigs, CW, CW-Prioritized and Stream Segment, using a Python frame-work based on music21 [3]. The Streamer algorithm2wasrun with default parameters. As it quantizes input ﬁles, theoffset and duration of notes in the output are slightly dif-ferent from the ones in our original ﬁles: We thus had toassociate notes to the correct ones.4.3 Evaluation metrics4.3.1 Note-based evaluation.A ﬁrst evaluation is to ask whether the voices are correctlypredicted. Thenote precision (NPR)is the ratio betweenthe number of notes correctly predicted (in the good voice)over the total number of notes. On one voice, this mea-sure is the same than theaverage voice consistency (AVC)deﬁned by [2]. However on a piece or on a corpus, wecompute the ratio on the total number of notes, instead ofaveraging ratios as in [2]. Especially in the pop corpus, thedistribution of notes is not equal in all pieces and all voices,and this measure better reﬂects the ability of the algorithmto assign the good voice to each note.Computing NPR requires to assertwhich voice in theprediction corresponds to a given voice of the ground truth.In a fugue, there may be a formal way to exactly deﬁne thevoices and number them, from the lowest one to the high-est one. But, in the general case, this correspondance is notalways obvious. By construction, the two voice segmen-tation algorithms studied here predict a number of voicesequal to the maximal number of voices, whereas the streamsegmentation algorithms have no limit for the number ofstreams. In the general case, one solution is to compareeach voice predicted by the algorithm withthe most simi-lar voice of the ground truth, for example taking the voiceof the ground truth sharing the highest number of noteswith the predicted voice.Note-based evaluation tends to deeply penalize some er-rors in the middle of the scores: When a voice is split intwo, half of the notes will be counted as false even if thealgorithm did “only one” mistake. Moreover, this is not2downloaded fromwww.link.cs.cmu.edu/melismaa fair way to evaluate stream segmentation algorithms, asthey may predict (many) more streams than the number ofvoices. We thus use two other metrics, that better measurethe ability of the algorithms to gather notes into voices,even when a voice of the ground truth is mapped to severalpredicted voices. These metrics do not require to make thecorrespondence between predicted voices and voices of thetruth.4.3.2 Transition-based evaluation.The result of voice or stream segmentation methods canbe seen as a set oftransitions, that are pairs of succes-sive notes in a same predicted voice or stream. We com-pare these transitions against the transitions deﬁned by theground truth, and compute usual precision and recall ratios.Thetransition precision (TR-prec)(calledsoundnessby[13]) is the ratio of correctly assigned transitions over thenumber of transitions in the predicted voices. This is re-lated tofragment consistencydeﬁned in [2] – but the frag-ment consistency takes only into account the links betweenthe contigs, and not all the transitions. Thetransition recall(TR-rec)(calledcompletenessby [13]) is the ratio of cor-rectly assigned transitions over the number of transitions inthe truth. This is again related tovoice consistencyof [2].For each piece, we compute these ratio on all the voices– taking the number of correct transitions insideallthevoices, and computing the ratio over the number of tran-sitions inside eitherallthe predicted voices orallthe truth.When the number of voices in the ground truth and in theprediction are equal, the TR-prec and TR-rec ratios arethus equal: we simply call this measureTR. Figure 12, atthe end of the paper, details an example of NPR and TRvalues for the six algorithms.4.3.3 Information-based evaluation.Finally, we propose to adapt techniques proposed to eval-uate music segmentation, seen as an assignation of a labelto every audio (or symbolic) frame [1, 14]. Lukashevichdeﬁnes two scores,SoandSu, based on normalized en-tropy, reporting how an algorithm may over-segment (So)or under-segment (Su) a piece compared to the groundtruth. The scores reﬂect how much information there is inthe output of the algorithm compared to the ground truth(So) or conversely (Su). Here, we use the same metricsfor voice or stream segmentation: both the ground truthand the output of any algorithm can be considered as anassignation of label to every note. On the probability dis-tribution of these labels, we then compute the entropiesH(predicted|truth)andH(truth|predicted), that becomeSoandSuafter normalization [14]. As these scores are basedon notes rather than on transitions, they enable to measurewhether the clusters are coherent, even in situations whentwo simultaneous voices are merged in a same stream (giv-ing thus bad TR ratios).496 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015wtc-iwtc-iipopavg.NPRTRSoSuavg.NPRTRSoSuavg.NPRTRSoSuBaseline3.5 71.4% 63.7% 0.48 0.483.4 71.9% 62.6% 0.45 0.45389.5%87.1%0.770.75CW3.583%95.9%0.730.733.487.8%95.6% 0.73 0.733 84.6% 88.7% 0.760.76CW-Prioritized3.5 82.5%97.4%0.72 0.723.4 86.5%97.1% 0.740.743 64.8%89.4%0.51 0.5avg.TR-precTR-recSoSuavg.TR-precTR-recSoSuStreamer16 75.6% 68.3% 0.46 0.6215.4 75.6% 65.2% 0.42 0.57Stream Segment191.1 76.5% 62.1% 0.23 0.79214 77.4% 61.9% 0.21 0.79CW-Contigs226.299.4% 86.7%0.340.98282.399.4% 86.8%0.340.98Table 2: Results on the fugues and on the pop corpora. “avg.” is the average number of voices or streams predicted.5. RESULTS AND DISCUSSIONWe evaluated the six algorithms on the 48 fugues ofWell-Tempered Clavierby J. S. Bach, and moreover the voiceseparation algorithms were evaluated on the 97 pop ﬁles.Table 2 details the results.5.1 ResultsNote and transition-based evaluation.Between 80 % and90 % of the notes are assigned correctly to the right voiceby at least one of the voice separation algorithms. The re-sults conﬁrm that these NPR metric is not very meaningful.The baseline has good NPRs, and on the pop corpus, thebaseline NPR is even better than CW and CW-Prioritized.Compared to the baseline algorithm, all algorithms outputlonger fragments (see Figure 9). As expected, the transi-tion ratio (TR) metrics are better to benchmark the abilityof the algorithms to gather relevant notes in the same voice:all the algorithms have better TR metrics than the baseline.The three stream segmentation algorithms predict morestreams that the number of voices in the ground truth, hencelow TR-rec ratios. The TR-prec ratios are higher, betterthan the baseline, and the CW-Contigs has an excellentTR-prec ratio.Information-based evaluation.An extreme case is perfectprediction, with NPR = TR =100%,So=1andSu=1(like in Bach’s Fugue in E minor BWV 855 for both CWand CW-Prioritized). In a pop song (allsaints-bootiecall)where two voices play mostly same notes, the baseline al-gorithm merges all notes in the same voice, so NPR andTR are close to50%, butSois close to1andSuclose to0.\nFigure 9: Notes attributed to the wrong voice withthe baseline (left) and CW (right) algorithms on Bach’sFugue #2 – book II (in C minor, BWV 871). WhenCW makes errors, the voices are kept in a same predictedvoice.In the general case,Suis correlated with TR-prec, andSowith TR-rec. As expected, in stream segments algo-rithms,Suis better thanSo. Note that the Stream Segmenthas not the best TR-prec ratio (sometimes, it merges notesthat are in separate voices), but it has a quite goodSuscoreamong all the algorithms (when it merges notes from sep-arate voices, it tends to put in the same stream all notesthat are in related voices). The bestSuscores are obtainedby the CW-Contigs, conﬁrming the fact that the contig cre-ation is a very good method that makes almost no error.\nFigure 10: A note spanning twocontigs is split inAandA0.CW and CW-Prioritized link thefragments(A+A0),(B+C),keepingAin the same voice.The original implementation ofIshigakiet al.links the frag-ments(A+D),(B,A0), dupli-cating the whole noteA+A0.\nFigure 11: FragmentsAandBare in different contigs dueto the overlap of previous notes.Both CW-Prioritized and theoriginal implementation of Ishi-gakiet al.link the fragments(A+B+D)and(C), whereasCW links the fragments(A+C)and(B+D).5.2 Partitioned notes and link weightsWith CW algorithm, when a note is cut between two con-tigs and the voices assigned to those two fragments are dif-ferent, the predicted voices contain more notes than in theinput data. This case was not detailed in the study [2]. Toavoid split notes in the output of the algorithm, we chooseto allow voice crossing exactly at these points (Figure 10).Our results for CW-Prioritized differ from the ones ob-tained in [9]: Their A VC was better compared to CW. Inour implementation, the NPR ratio is lower for CW-Prio-ritized compared to CW. In our implementation (as in theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 497original algorithm of CW), there is a\u0000Kweight to the linkbetween two parts of the same note. In the Ishigakiet al.implementation, this weight is\u00001, and thus the algorithmkeeps partitioned notes in the output (see Figure 10). De-spite this difference, our CW-Prioritized implementationgives good results by considering TR both on the fuguesand on the pop corpus. even if it merges incorrectly somecontigs (see Figure 11).5.3 A challenging exposition passage in a fugueFigure 12 shows the results of the algorithms on a extractof Bach’s Fugue #16 – book I. This passage is quite chal-lenging for voice separation: all the four voices enter insuccession, and there is a sixth interval in the head of thesubject that often put voices very close. In the last measureof the fragment, there is even a crossing of voices when thesoprano is playing this sixth interval.The algorithms behave differently on this passage, butnone of them perfectly analyze it. Only CW-Prioritizedpredicts correctly the ﬁrst three measures, especially thestart of the alto voice at the ﬁrst two beats of measure 12.CW selects a bad link at the third beat of measure 14, re-sulting in a bad prediction in measures 12/13/14 (but a highTR ratio overall). Except on the places where almost all thealgorithms fail, Streamer has a correct result. Stream Seg-ment creates many more streams, and, as expected, assignsnotes that overlap in the same stream, as on the ﬁrst beatof measure 12.Finally, none of the algorithms successfully handles thevoice crossing, measure 15. CW-Contigs made here itsonly clustering error (otherwise it has an excellent TR-prec), linking the D of the soprano with the following G ofthe alto. As expected, this error is found again in CW andCW-Prioritized, and Streamer also splits apart the noteswith the highest pitch from the notes with the lowest pitch.At this point, Stream Segment creates streams containingboth voices. Handling correctly this passage would requireto have a knowledge of the patterns (including here thehead of the subject with the sixth leap) and to favor to keepthese patterns in a same voice, allowing voice crossing.6. CONCLUSIONSBoth voice and stream segmentation methods cluster notesfrom polyphonic scores into relevant units. One difﬁcultywhen benchmarking such algorithms is to deﬁne a groundtruth. Beside the usual fugues corpus, we proposed someideas to establish a pop corpus with polyphonic data suit-able for evaluation.Even stream segmentation algorithms give good resultsin separating voices, as seen by the TR ratios and theSuscore. The Streamer algorithm is very close to a full voiceseparation, predicting monophonic streams. The Stream  /noteheads.s2 : soprano/noteheads.d2triangle : alto/noteheads.s2cross : tenor/noteheads.s0harmonic : bass Baseline (4 voices)NPR: 45/82, TR: 60/78/accidentals.natural/noteheads.s0harmonic/noteheads.s2/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle/noteheads.s2cross/noteheads.s2/noteheads.u2triangle/noteheads.s0harmonic15/flags.d3/dots.dot/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/accidentals.natural/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/rests.3/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s0harmonic/noteheads.u2triangle/noteheads.d2triangle/noteheads.s0harmonic/rests.0/rests.0/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.u2triangle/noteheads.d2triangle/noteheads.d2triangle/noteheads.s2cross/accidentals.flat/noteheads.s2/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/rests.3/accidentals.natural/dots.dot/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/rests.3/noteheads.s2cross/rests.113/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/timesig.C44/noteheads.d2triangle/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.s2cross/noteheads.s2cross/rests.3/noteheads.s2cross/rests.1/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/noteheads.s2cross/noteheads.d2triangle/noteheads.s0harmonic/accidentals.natural/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic14/dots.dot/noteheads.s2cross/noteheads.s1cross/rests.3/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/accidentals.natural/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonicCW (4 voices)NPR: 36/82, TR: 69/78/accidentals.natural/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle/noteheads.u2triangle/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic15/flags.d3/dots.dot/noteheads.s2/noteheads.u2triangle/noteheads.u2triangle/accidentals.natural/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/rests.3/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s2/noteheads.s0harmonic/rests.0/rests.0/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s2cross/accidentals.flat/noteheads.s2/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/noteheads.s2/noteheads.s2cross/rests.3/accidentals.natural/dots.dot/noteheads.u2triangle/noteheads.u2triangle/noteheads.s0harmonic/rests.3/noteheads.u2triangle/rests.113/noteheads.s2/noteheads.u2triangle/noteheads.s2cross/noteheads.u2triangle/noteheads.u2triangle/noteheads.s2/noteheads.u2triangle/noteheads.s2/timesig.C44/noteheads.s2/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.u2triangle/noteheads.s2/rests.3/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s0harmonic/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0harmonic/accidentals.natural/noteheads.u2triangle/noteheads.s2/noteheads.u2triangle/noteheads.s0harmonic14/dots.dot/noteheads.s2/noteheads.u1triangle/rests.3/noteheads.s2/noteheads.s2/noteheads.u2triangle/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s0harmonicCW-Prioritized (4 voices)NPR: 73/82, TR: 76/78/accidentals.natural/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic15/flags.d3/dots.dot/noteheads.d2triangle/noteheads.s2cross/noteheads.s2cross/accidentals.natural/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/rests.3/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s2/noteheads.s0harmonic/rests.0/rests.0/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s2cross/accidentals.flat/noteheads.s2/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/noteheads.s2/noteheads.s2cross/rests.3/accidentals.natural/dots.dot/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonic/rests.3/noteheads.s2cross/rests.113/noteheads.d2triangle/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.d2triangle/timesig.C44/noteheads.d2triangle/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.s2cross/noteheads.d2triangle/rests.3/noteheads.d2triangle/rests.1/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonic/noteheads.d2triangle/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonic/accidentals.natural/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic14/dots.dot/noteheads.d2triangle/noteheads.s1cross/rests.3/noteheads.d2triangle/noteheads.d2triangle/noteheads.s2cross/accidentals.natural/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonicCW-Contigs (47 streams)TR-prec: 34/35, TR-rec: 34/78/accidentals.natural/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.s2cross/noteheads.u2triangle/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s2cross/noteheads.u2triangle/noteheads.s0harmonic15/flags.d3/dots.dot/noteheads.s0harmonic/noteheads.u2triangle/noteheads.u2triangle/accidentals.natural/noteheads.u2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.s2/noteheads.s2cross/noteheads.s2cross/noteheads.s2/rests.3/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/rests.0/rests.0/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s2cross/noteheads.s2cross/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flat/noteheads.s2/noteheads.u2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2/rests.3/accidentals.natural/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2cross/rests.3/noteheads.u2triangle/rests.113/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2cross/noteheads.s2/noteheads.s2cross/timesig.C44/noteheads.s0harmonic/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.u2triangle/noteheads.s2cross/rests.3/noteheads.s0harmonic/rests.1/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/noteheads.s0harmonic/accidentals.natural/noteheads.u2triangle/noteheads.s2cross/noteheads.s2/noteheads.s0harmonic14/dots.dot/noteheads.s0harmonic/noteheads.u1triangle/rests.3/noteheads.s2cross/noteheads.s2cross/noteheads.s2/accidentals.natural/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonicStreamer (5 streams)TR-prec: 72/77, TR-rec: 72/78/accidentals.natural/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.d2triangle/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic15/flags.d3/dots.dot/noteheads.d2triangle/noteheads.s2cross/noteheads.s2cross/accidentals.natural/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/rests.3/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.u2triangle/noteheads.s2/noteheads.s0harmonic/rests.0/rests.0/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s2cross/accidentals.flat/noteheads.s2/noteheads.s2cross/noteheads.s0harmonic/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/noteheads.s2/noteheads.s2cross/rests.3/accidentals.natural/dots.dot/noteheads.s2cross/noteheads.s2cross/noteheads.s2cross/rests.3/noteheads.s2cross/rests.113/noteheads.d2triangle/noteheads.s2cross/noteheads.u2triangle/noteheads.s2cross/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.d2triangle/timesig.C44/noteheads.s2/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.s2cross/noteheads.d2triangle/rests.3/noteheads.d2triangle/rests.1/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonic/noteheads.d2triangle/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonic/accidentals.natural/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic14/dots.dot/noteheads.d2triangle/noteheads.s1cross/rests.3/noteheads.d2triangle/noteheads.d2triangle/noteheads.s2cross/accidentals.natural/noteheads.d2triangle/noteheads.d2triangle/noteheads.s0harmonicStream Segment (19 streams)TR-prec: 55/63, TR-rec: 55/78/accidentals.natural/noteheads.s2/noteheads.s2cross/noteheads.u2triangle/noteheads.s2/noteheads.s2cross/noteheads.s2cross/noteheads.u2triangle/noteheads.s2cross/noteheads.u2triangle/noteheads.s215/flags.d3/dots.dot/noteheads.s2/noteheads.u2triangle/noteheads.u2triangle/accidentals.natural/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s2/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.d2triangle/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2/noteheads.u2triangle/rests.3/noteheads.s0harmonic/noteheads.s2cross/noteheads.d2triangle/noteheads.s2cross/noteheads.u2triangle/noteheads.d2triangle/noteheads.s2cross/rests.0/rests.0/noteheads.s0harmonic/noteheads.s2cross/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.u2triangle/accidentals.flat/noteheads.s2cross/noteheads.u2triangle/noteheads.s2/noteheads.s2cross/noteheads.d2triangle/noteheads.s2/noteheads.s2/noteheads.u2triangle/rests.3/accidentals.natural/dots.dot/noteheads.u2triangle/noteheads.u2triangle/noteheads.s2cross/rests.3/noteheads.u2triangle/rests.113/noteheads.d2triangle/noteheads.u2triangle/noteheads.s2/noteheads.u2triangle/noteheads.s0harmonic/noteheads.s2cross/noteheads.s0harmonic/noteheads.s2cross/timesig.C44/noteheads.s2/clefs.F/rests.2/accidentals.flat/accidentals.flat/rests.2/timesig.C44/rests.2/brackettips.up/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat/rests.1/rests.3/noteheads.u2triangle/noteheads.s2cross/rests.3/noteheads.d2triangle/rests.1/noteheads.d2triangle/noteheads.s2/noteheads.s0harmonic/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0harmonic/accidentals.natural/noteheads.u2triangle/noteheads.s2/noteheads.s2/noteheads.s0harmonic14/dots.dot/noteheads.s2/noteheads.s1/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s0harmonicFigure 12: Output of the ﬁve algorithms on the measures12 to 15 of Bach’s Fugue #16 – book I (in G minor, BWV861). After the initial chord with almost all the voices, thevoices enter in succession (alto and tenor: m12, bass: m13,soprano: m15).Segment algorithm further enables to output some poly-phonic streams that may be relevant for the analysis of thescore.Focusing on voice separation problem, the contig ap-proach, as initially proposed by [2], seems to be an excel-lent approach – very few transition errors are made insidecontigs, as shown by the raw results of the CW-Contigs al-gorithm. The challenge is thus to do the right connectionsbetween the contigs. The ideas proposed by [9] are inter-esting. In our experiments, we saw a small improvementin our CW-Prioritized implementation compared to CW,but details on how partitioned notes are processed shouldbe handled carefully. Further research should be done toimprove again the contig connection.498 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]Samer Abdallah, Katy Noland, Mark Sandler,Michael A Casey, Christophe Rhodes, et al. Theoryand evaluation of a bayesian music structure extractor.InInternational Conference on Music Information Re-trieval (ISMIR 2005), pages 420–425, 2005.[2]Elaine Chew and Xiaodan Wu. Separating voices inpolyphonic music: A contig mapping approach. InIn-ternational Symposium on Computer Music Modelingand Retrieval (CMMR 2005), pages 1–20. Springer,2005.[3]Michael Scott Cuthbert and Christopher Ariza. mu-sic21: A toolkit for computer-aided musicology andsymbolic music data. InInternational Society for Mu-sic Information Retrieval Conference (ISMIR 2010),2010.[4]Justin de Nooijer, Frans Wiering, Anja V olk, andHermi JM Tabachneck-Schijf. An experimental com-parison of human and automatic music segmentation.InInternational Computer Music Conference (ICMC2008), pages 399–407, 2008.[5]Reinier de Valk, Tillman Weyde, and EmmanouilBenetos. A machine learning approach to voice separa-tion in lute tablature. InInternational Society for MusicInformation Retrieval Conference (ISMIR 2013), pages555–560, 2013.[6]Diana Deutsch. Grouping mechanisms in music.Thepsychology of music, 2:299–348, 1999.[7]David Huron. Tone and voice: A derivation of the rulesof voice-leading from perceptual principles.Music Per-ception, 19(1):1–64, 2001.[8]David Huron. Music information processing using theHumdrum toolkit: Concepts, examples, and lessons.Computer Music Journal, 26(2):11–26, 2002.[9]Asako Ishigaki, Masaki Matsubara, and Hiroaki Saito.Prioritized contig combining to segragate voices inpolyphonic music. InSound and Music ComputingConference (SMC 2011), volume 119, 2011.[10]Anna Jordanous. V oice separation in polyphonic mu-sic: A data-driven approach. InInternational Com-puter Music Conference (ICMC 2008), 2008.[11]Ioannis Karydis, Alexandros Nanopoulos, ApostolosPapadopoulos, Emilios Cambouropoulos, and Yan-nis Manolopoulos. Horizontal and vertical integra-tion/segregation in auditory streaming: a voice sepa-ration algorithm for symbolic musical data. InSoundand Music Computing Conference (SMC 2007), 2007.[12]Jürgen Kilian and Holger H Hoos. V oice separation-a local optimization approach. InInternational Con-ference on Music Information Retrieval (ISMIR 2002),2002.[13]Phillip B Kirlin and Paul E Utgoff. VOISE: learningto segregate voices in explicit and implicit polyphony.InInternational Conference on Music Information Re-trieval (ISMIR 2005), pages 552–557, 2005.[14]Hanna M Lukashevich. Towards quantitative measuresof evaluating song segmentation. InInternational Con-ference on Music Information Retrieval (ISMIR 2008),pages 375–380, 2008.[15]Søren Tjagvad Madsen and Gerhard Widmer. Separat-ing voices in midi. InInternational Conference on Mu-sic Information Retrieval (ISMIR 2006), pages 57–60,2006.[16]Dimitris Rafailidis, Alexandros Nanopoulos, EmiliosCambouropoulos, and Yannis Manolopoulos. Detec-tion of stream segments in symbolic musical data. InInternational Conference on Music Information Re-trieval (ISMIR 2008), 2008.[17]David Temperley.The Cognition of Basic MusicalStructures. Number 0-262-20134-8. Cambridge, MA:MIT Press, 2001.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 499"
    },
    {
        "title": "Improving Melodic Similarity in Indian Art Music Using Culture-Specific Melodic Characteristics.",
        "author": [
            "Sankalp Gulati",
            "Joan Serrà",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418261",
        "url": "https://doi.org/10.5281/zenodo.1418261",
        "ee": "https://zenodo.org/records/1418261/files/GulatiSS15.pdf",
        "abstract": "Detecting the occurrences of r¯ags’ characteristic melodic phrases from polyphonic audio recordings is a fundamen- tal task for the analysis and retrieval of Indian art mu- sic. We propose an abstraction process and a complex- ity weighting scheme which improve melodic similarity by exploiting specific melodic characteristics in this music. In addition, we propose a tetrachord normalization to han- dle transposed phrase occurrences. The melodic abstrac- tion is based on the partial transcription of the steady re- gions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the dif- ferences in the melodic complexities of the phrases, a cru- cial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data compris- ing 625 annotated melodic phrases belonging to 10 differ- ent phrase categories. Results show that the proposed mel- odic abstraction and complexity weighting schemes sig- nificantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for deal- ing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the appli- cability of the proposed approach to other melody domi- nant music traditions such as Flamenco, Beijing opera and Turkish Makam music.",
        "zenodo_id": 1418261,
        "dblp_key": "conf/ismir/GulatiSS15",
        "keywords": [
            "fundamental task",
            "Indian art music",
            "melodic similarity",
            "Carnatic music",
            "melodic abstraction",
            "complexity weighting",
            "partial transcription",
            "duration truncation",
            "phrase detection accuracy",
            "tetrachord normalization"
        ],
        "content": "IMPROVING MELODIC SIMILARITY IN INDIAN ART MUSIC USINGCULTURE-SPECIFIC MELODIC CHARACTERISTICSSankalp Gulati†, Joan Serr`a?and Xavier Serra††Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain?Telefonica Research, Barcelona, Spainsankalp.gulati@upf.edu, joan.serra@telefonica.com, xavier.serra@upf.eduABSTRACTDetecting the occurrences of r¯ags’ characteristic melodicphrases from polyphonic audio recordings is a fundamen-tal task for the analysis and retrieval of Indian art mu-sic. We propose an abstraction process and a complex-ity weighting scheme which improve melodic similarityby exploiting speciﬁc melodic characteristics in this music.In addition, we propose a tetrachord normalization to han-dle transposed phrase occurrences. The melodic abstrac-tion is based on the partial transcription of the steady re-gions in the melody, followed by a duration truncation step.The proposed complexity weighting accounts for the dif-ferences in the melodic complexities of the phrases, a cru-cial aspect known to distinguish phrases in Carnatic music.For evaluation we use over 5 hours of audio data compris-ing 625 annotated melodic phrases belonging to 10 differ-ent phrase categories. Results show that the proposed mel-odic abstraction and complexity weighting schemes sig-niﬁcantly improve the phrase detection accuracy, and thattetrachord normalization is a successful strategy for deal-ing with transposed phrase occurrences in Carnatic music.In the future, it would be worthwhile to explore the appli-cability of the proposed approach to other melody domi-nant music traditions such as Flamenco, Beijing opera andTurkish Makam music.1. INTRODUCTIONThe automatic assessment of melodic similarity is one ofthe most researched topics in music information research(MIR) [3,14,30]. Melodic similarity models may vary con-siderably depending on the type of music material (sheetmusic or polyphonic audio recordings) [4, 8, 22] and themusic tradition [5, 18].Results until now indicatethat theimportant characteristics of several melody-dominant mu-sic traditions of the world such as Flamenco and Indian artmusic (IAM) need dedicated research efforts to devise spe-ciﬁc approaches for computing melodic similarity [23,24].These music traditions have large audio music repertoiresbut comparatively very fewer number of descriptive scoresc\u0000Sankalp Gulati†, Joan Serr`a?and Xavier Serra†.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Sankalp Gulati†, Joan Serr`a?andXavier Serra†. “Improving Melodic Similarity in Indian Art Music UsingCulture-speciﬁc Melodic Characteristics”, 16th International Society forMusic Information Retrieval Conference, 2015.(they follow an oral transmission), the automatic detectionof the occurrences of a melodic phrase in audio recordingsis therefore a task of primary importance. In this article,we focus on this task for IAM.Hindustani music (also referred to as north Indianartmusic) and Carnatic music (also referred to as south Indianartmusic) are the two art music traditions of India [6, 31].Both are heterophonic in nature, with melody as the dom-inant aspect of the music. A typical piece has a mainmelody being sung or played by the lead artist and a mel-odic accompaniment with the tonic pitch as the base refer-ence frequency [9].R¯agis the melodic framework andt¯alis the rhythm framework in both music traditions. R¯ags arecharacterized by their constituentsvars(roughly speaking,notes), by the¯aroh-avroh(the ascending and descendingmelodic progression) and, most importantly, by a set ofcharacteristic melodic or ‘catch’ phrases. These phrasesare the prominent cues for r¯ag identiﬁcation used by theperformer, to establish the identity of a r¯ag, and also thelistener, to recognize the r¯ag.The characteristic melodic phrases of a r¯ag act as thebasis for the artists to improvise, providing them with amedium to express creativity during a r¯ag rendition. Hence,the surface representation of these melodic phrases canvary a lot across their occurrences. This high degree ofvariability in terms of the duration of a phrase, non-lineartime warpings and the added melodic ornaments togetherpose a big challenge for melodic similarity computationin IAM. In Figure 1 we illustrate this variability by show-ing the pitch contours of the different occurrences of threecharacteristic melodic phrases of the r¯ag Alaiya Bilawal.We can clearly see that the duration of a phrase across itsoccurrences varies a lot and the steady melodic regions arehighly varied in terms of the duration and the presence ofmelodic ornaments. Because of these and other factors,detecting the occurrences of characteristic melodic phrasesbecomes a challenging task. Ideally, the melodic similar-ity measure should be robust to a high degree of variationand, at the same time, it should be able to discriminatebetween different phrase categories and irrelevant melodicfragments (noise candidates).For melodic similarity computation, the string matching-based and the set point-based approaches are extensivelyused for both musical scores and audio recordings [30].However, compared to the former, the set point-based ap-proaches are yet to be fully exploited for polyphonic audiomusic because of the challenges involved in melody extrac-680Time (s) Frequency (Cent) 1 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 200 400 600 800 1000 1200 1400 \nFigure 1. Pitch contours of occurrences of three different characteristic melodic phrases in Hindustani music. Contours arefrequency transposed and time shifted for a better visualization.tion and transcription [4]. A reliable melody transcriptionalgorithm is argued to be the key to bridge the gap betweenaudio and symbolic music, leading to the full exploitationof the potential of the set point-based approaches for audiomusic. However, for several music traditions such as Hin-dustani and Carnatic music, automatic melody transcrip-tion is a challenging and a rather ill-deﬁned task [25].In recent years, several methods for retrieving differ-ent types of melodic phrases have been proposed for IAM,following both supervised and unsupervised strategies [7,12, 13, 16, 17, 24, 26, 27]. Ross et al. [27] detect the oc-currences of the title phrases of a composition within aconcert recording of Hindustani music. The authors ex-plored a SAX-based representation [20] along with severalpitch quantizations of the melody and showed that a dis-similarity measure based on dynamic time warping (DTW)is preferred over the Euclidean distance. Noticeably, inthat work, the underlying rhythm structure was exploited toreduce the search space for detecting pattern occurrences.An extension of that approach [26] pruned the search spaceby employing a melodic landmark called ny¯as svar [11].Rao et al. [24] address the challenge of a large within-class variability in the occurrences of the characteristic phr-ases. They propose to use exemplar-based matching aftervector quantization-based training to obtain multiple tem-plates for a given phrase category. In addition, the authorspropose to learn the optimal DTW constraints in a previ-ous step for each phrase category in order to exploit thepossible patterns in the duration variability. For Carnaticmusic, Ishwar et al. [17] propose a two-stage approach forspotting the characteristic melodic phrases. The authorsexploit speciﬁc melodic characteristics (saddle points) toreduce the target search space and use a distance measurebased on rough longest common subsequence [19].On the other hand, there are studies that follow an un-supervised approach for discovering melodic patterns inCarnatic music [7, 12]. Since the evaluation of melodicsimilarity measures is a much more challenging task in anunsupervised framework, results obtained from an exhaus-tive grid search of optimal distance measures and parame-ter values within a supervised framework are valuable [13].In this study, we present two approaches that utilize spe-ciﬁc melodic characteristics in IAM to improve melodicsimilarity. We propose a melodic abstraction process basedon the partial transcription of the melodies to handle largetiming variations in the occurrences of a melodic phrase.For Carnatic music we also propose a complexity weight-ing scheme that accounts for the differences in the melodiccomplexities of the phrases, a crucial aspect for melodicsimilarity in this music tradition. In addition, we comeup with a tetrachord normalization strategy to handle thetransposed occurrences of the phrases. The dataset usedfor the evaluation is a superset of the dataset used in a re-cent study [13] and contains nearly 30% more number ofannotated phrases.2. METHODBefore we present our approach we ﬁrst discuss the mo-tivation and rationale behind it. A close examination ofthe occurrences of the characteristic melodic phrases inour dataset reveals that there is a pattern in the non-lineartiming variations [24]. In Figure 1 we show a few occur-rences of three such melodic phrases. In particular, we seethat the transient regions of a melodic phrase tend to spannearly the same time duration across different occurrences,whereas the stationary regions vary a lot in terms of theduration. In Figure 2 we further illustrate this by show-ing two occurrences of a melodic phrase (P1aandP2a).The stationary svar regions are highlighted. We clearlysee that the duration variation is prominent in the high-lighted regions. To handle such large non-linear timingvariations typically a non-constrained DTW distance mea-sure is employed [13]. However, such a DTW variant isprone to noisy matches. Moreover, the absence of a bandconstraint renders it inefﬁcient for computationally com-plex tasks such as motif discovery [12].We put forward an approach that abstracts the melodicrepresentation and reduces the extent of duration and pitchvariations across the occurrences of a melodic phrase. Ourapproach is based on the partial transcription of the melo-dies. As mentioned earlier, melodic transcription in IAMis a challenging task. The main challenges arise due to thepresence of non-discrete pitch movements such as smoothglides andgamakas1. However, since the duration vari-ation exists mainly during the steady svar regions, tran-scribing only the stable melodic regions might be sufﬁ-1Rapid oscillatory melodic movement around a svarProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 681Figure 2. Original pitch contours (P1a,P2a) and durationtruncated pitch contours (P1b,P2b) of two occurrences of acharacteristic phrase of r¯ag Alhaiya Bilawal. The contoursare transposed for a good visualization.\nFigure 3. Pitch contours of three melodic phrases (p1,p2,p3).p1andp2are the occurrences of the same characteris-tic phrase and both are musically dissimilar top3.cient. Once transcribed, we can then truncate the dura-tion of these steady melodic regions and hence effectivelyreduce the amount of timing variations across the occur-rences of a melodic phrase. Additionally, since the dura-tion truncation also reduces the overall length of a pattern,the computational time for melodic similarity computationis also reduced substantially.The rapid oscillatory pitch movements (gamakas) in Car-natic music bring up another set of challenges for the melo-dic similarity computation. Very often, two musically dis-similar melodic phrases obtain a high similarity score ow-ing to a similar pitch contour at a macro level. However,they differ signiﬁcantly at a micro level. In Figure 3 weillustrate such a case where we show the pitch contoursof three melodic phrasesP1,P2andP3, whereP1andP2are the occurrences of the same melodic phrase andboth are musically dissimilar toP3. Using the best per-forming variant of the similarity measure in [13] we obtaina higher similarity score between the pairs (P1,P3) and(P2,P3) compared to the score between the pair (P1,P2).This tendency of a high complexity time-series (higher de-gree of micro level variations) obtaining a high similarityscore with another low complexity time-series is discussedin [1]. We follow their approach and apply a complex-ity weighting to account for the differences in the melodiccomplexities between phrases in the computation of melo-dic similarity.In the subsequent sections we present our proposed ap-proach. As a baseline in this study we consider the methodthat was reported as the best performing method in a recentstudy for the same task on a subset of the dataset [13]. Wedenote this baseline method byMB.2.1 Melody Estimation and post-processingWe represent melody of an audio signal by the pitch of thepredominant melodic source. For predominant pitch esti-mation in Carnatic music, we use the method proposed bySalamon and G´omez [29]. This method performed favo-urably in MIREX 2011 (an international MIR evaluationcampaign) on a variety of music genres, including IAM,and has been used in several other studies for a similartask [7,12,13]. An implementation of this algorithm avail-able in Essentia [2] is used in this study. Essentia is anopen-source C++ library for audio analysis and content-based MIR. We use the default values of the parametersfor pitch estimation except the frame size and the hop size,which are set to 46 ms and 2.9 ms, respectively. For Hin-dustani music, we use the pitch tracks corresponding to thepredominant melody that are used in several other stud-ies on a similar topic [24, 27] and are made available tous by the authors. These pitch tracks are obtained usinga semi-automatic system for predominant melody estima-tion. This allows us to compare results across studies andavoid the effects of pitch errors on the computation of mel-odic similarity. After estimating the predominant pitch weconvert it from Hertz to Cent scale for the melody repre-sentation to be musically relevant.We proceed to post-process the pitch contours and re-move the spurious pitch jumps lasting over a few framesas well as smooth the pitch contours. We ﬁrst apply a me-dian ﬁlter over a window size of 50 ms, followed by a low-pass ﬁlter using a Gaussian window. The window size andthe standard deviation of the Gaussian window is set to50 ms and 10 ms, respectively. The pitch contours are ﬁ-nally down-sampled to 100 Hz, which was found to be anoptimal sampling rate in [13].2.2 Transposition InvarianceThe base frequency chosen for a melody in IAM is thetonic pitch of the lead artist [10]. Therefore, for a meaning-ful comparison of the melodic phrases across the record-ings of different artists, a melody representation should benormalized by the tonic pitch of the lead artist. We per-form this tonic normalization (Ntonic) by considering thetonic of the lead artist as the reference frequency duringthe Hertz to Cent conversion. The tonic pitch is automat-ically identiﬁed using a multi-pitch approach proposed byGulati et al. [10]. This approach was shown to obtain morethan 90% tonic identiﬁcation accuracy and has been usedin several studies in the past.Tonic normalization does not account for the pitch ofthe octave transposed occurrences of a melodic phrase with-in a recording. In addition, estimated tonic pitch some-times might be incorrect and a typical error is an offsetof ﬁfth scale degree. To handle such cases, we propose anovel tetrachord normalization (Ntetra). For this we anal-yse the difference (\u0000) in the mean frequency values of thetwo tonic normalized melodic phrases (p1,p2). We offsetthe pitch values of the phrasep1by the frequency in the set{- 1200, - 700, - 500, 0, 500, 700, 1200, 1700, 1900}that682 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015is closest to\u0000within a vicinity of 100 Cents.In additionto tetrachord normalization, we also experiment with meannormalization (Nmean), which was reported to improve theperformance in the case of Carnatic music [13].2.3 Partial TranscriptionWe perform a partial melody transcription to automaticallysegment and identify the steady svar regions in the melody.Note that even the partial transcription of the melodies is anon-trivial task, since we desire a segmentation that is ro-bust to different melodic ornaments added to a svar wherethe pitch deviation from the mean svar frequency can beup to 200 Cents. In Figure 2 we show such an example ofa steady svar region (P1afrom 3-6 s) where the pitch de-viation from the mean svar frequency is high due to addedmelodic ornaments. Ideally, the melodic region between 1and 6 s should be detected as a single svar segment.We segment the steady svar regions using a method de-scribed in [11], which addresses the aforementioned chal-lenges. A segmented svar region is then assigned a fre-quency value corresponding to the peak in an aggregatedpitch histogram closest to the mean svar frequency. Thepitch histogram is constructed for the entire recording andsmoothened using a Gaussian window with a variance of15 cents. As peaks of the normalized pitch histogram, weselect all the local maximas where at least one peak-to-valley ratio is greater than 0.01. For a detailed descriptionof this method we refer to [11].2.4 Svar Duration TruncationAfter segmenting the steady svar regions in the melody weproceed to truncate the duration of these regions. We hy-pothesize that, beyond a certain value\u0000, the duration ofthese steady svar regions do not change the identity of amelodic phrase (i.e. the phrase category). We experimentwith 7 different truncation durations\u0000={0.1 s, 0.3 s,0.5 s, 0.75 s, 1 s, 1.5 s, 2 s}and select the one that resultsin the best performance. In Figure 2 we show an exampleof the occurrences of a melodic phrase both before (P1a,P2a) and after (P1b,P2b) the svar duration truncation us-ing\u0000=0.1s. This example clearly illustrates that theoccurrences of a melodic phrase after duration truncationexhibit lower degree of non-linear timing variations. Wedenote this method byMDT.2.5 Similarity ComputationTo measure the similarity between two melodic fragmentswe consider a DTW-based approach. Since the phrase seg-mentation is known beforehand, we use a whole sequencematching DTW variant. We consider the best performingDTW variant and the related parameter values for each mu-sic tradition as reported in [13]. These variants were cho-sen based on an exhaustive grid search across all possiblecombinations and hence can be considered as optimal forthis dataset. For Carnatic music we use a DTW step sizecondition{(2,1),(1,1),(1,2)}and for Hindustani musica step size condition{(1,0),(1,1),(0,1)}. We use Sakoe-Chiba global band constraint [28] with the width of theDatasetRec. PC R¯ags Artists Duration (hr)CMD23 5 5 14 3.82HMD9 5 1 7 1.76Table 1. Details of the datasets in terms of the total num-ber of recordings (Rec.), number of annotated phrase cate-gories (PC), number of r¯ags, unique number of artists andtotal duration of the dataset.band as±10% of the phrase length. Note that before com-puting the DTW distance we uniformly time-scale the twomelodic fragments to the same length, which is the maxi-mum of the lengths of the phrases.2.6 Complexity WeightingThe complexity weighting that we apply here to overcomethe shortcoming of the distance measure in distinguish-ing two time series with different complexities is discussedin [1]. We apply a complexity weighting (↵) to the DTW-based distance (DDTW) in order to compute the ﬁnal sim-ilarity scoreDf=↵DDTW. We compute↵as:↵=max(Ci,Cj)min(Ci,Cj);Ci=2vuutN\u00001Xi=1(pi\u0000pi+1)2(1)where,Ciis the complexity estimate of a melodic phraseof lengthNsamples andpiis the pitch value of theithsample. We explore two variants of this complexity esti-mate. One of these variants is already proposed in [1] andis described in equation 1. We denote this method variantbyMCW1. We propose another variant that utilizes mel-odic characteristics of Carnatic music. This variant takesthe number of saddle points in the melodic phrase as thecomplexity estimate [17]. This method variant is denotedbyMCW2. As saddle points we consider all the local mini-mas and the local maximas in the pitch contour which haveat least one minima to maxima distance of half a semi-tone. Since such melodic characteristics are predominantlypresent in Carnatic music, the complexity weighting is notapplicable for computing melodic similarity in Hindustanimusic.3. EVALUATION3.1 Dataset and AnnotationsFor a better comparison of the results, for our evaluationswe use a music collection that has been used in severalother studies for a similar task [13, 24, 27]. However, wehave extended the dataset by adding 30% more number ofannotations of the melodic phrases, which we make avail-able at http://compmusic.upf.edu/node/269. The music col-lection comprises vocal recordings of renowned artists inboth Hindustani and Carnatic music. We use two separatedatasets for the evaluation, Carnatic music dataset (CMD)and Hindustani music dataset (HMD) as done in [13]. Themelodic phrases are annotated by two professional musi-cians who have received over 15 years of formal musictraining. All the annotated phrases are the characteristicProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 683CMDHMDPC #OccLmeanLstdPC #OccLmeanLstdC139 1.38 0.25H162 1.93 0.98C246 1.25 0.21H2154 1.40 0.79C338 1.23 0.24H347 1.30 0.78C431 1.11 0.17H476 2.38 1.33C545 0.76 0.08H587 1.17 0.36Total 199 1.13 0.29426 1.59 0.99Table 2. Details of the 625 annotated melodic phrases.PC: pattern category, #Occ: number of annotated occur-rences, andLmean,Lstdare the mean, standard deviationof the lengths of the patterns of a PC in seconds.phrases of a r¯ag. In Table 1 we summarize the relevantdataset details. Table 2 summarizes the details of the an-notated phrases in terms of their number of instances andbasic statistics of the length of the phrases.3.2 Setup, Measures and Statistical SigniﬁcanceWe consider each annotated melodic phrase as a query andperform a search across all the annotated phrases in thedataset (referred to as target search space). In additionto the annotated phrases, we add randomly sampled melo-dic segments (referred to as noise candidates) in the targetspace to simulate a real world scenario. We generate thestarting time stamps of the noise candidates by randomlysampling a uniform distribution. The length of the noisecandidates are generated by sampling the distribution ofthe duration values of the annotated phrases. The numberof noise candidates added are 100 times the number of totalannotations in the entire music collection. For every querywe consider the top 1000 nearest neighbours in the searchresults ordered by the similarity value. A retrieved melodicphrase is considered as a true hit only if it belongs to thesame phrase category as the query.To assess the performance of the proposed approachand the baseline method we use mean average precision(MAP), a common measure in information retrieval [21].To assess if the difference in the performance of any twomethods is statistically signiﬁcant we use the Wilcoxonsigned rank-test [32] withp<0.01. To compensate formultiple comparisons, we apply the Holm-Bonferroni met-hod [15].4. RESULTS AND DISCUSSIONIn Table 3 we summarize the MAP scores and the stan-dard deviation of the average precision values obtained us-ing the baseline method (MB), the method that uses dura-tion truncation (MDT) and the ones using the complexityweighting (MCW1,MCW2), for both the CMD and theHMD. Note thatMCW1andMCW2are only applicable tothe CMD (Sec. 2).We ﬁrst analyse the results for the HMD. From Table 3(upper half), we see that the proposed method variant thatapplies a duration truncation performs better than the base-line method for all the normalization techniques. More-HMDNormMBMDTMCW1MCW2Ntonic0.45 (0.25)0.52 (0.24)--Nmean0.25 (0.20)0.31 (0.23)--Ntetra0.40 (0.23)0.47 (0.23)--CMDNormMBMDTMCW1MCW2Ntonic0.39 (0.29)0.42 (0.29)0.41 (0.28)0.41 (0.29)Nmean0.39 (0.26)0.45 (0.28)0.43 (0.27)0.45 (0.27)Ntetra0.45 (0.26)0.50 (0.27)0.49 (0.28)0.51 (0.27)Table 3. MAP scores for the two datasets HMD andCMD for the four method variantsMB,MDT,MCW1andMCW2and for different normalization techniques.Standard deviation of average precision is reported withinround brackets.\nFigure 4. MAP scores for different duration truncationvalues (\u0000) for the HMD and the CMD.over, this difference is found to be statistically signiﬁcantin each case. The results for the HMD in this table corre-spond to\u0000=500 ms, for which we obtain the highest accu-racy compared to the other\u0000values as shown in Figure 4.Furthermore, we see thatNtonicresults in the best accu-racy for the HMD for all the method variants and the dif-ference is found to be statistically signiﬁcant in each case.In Figure 5 we show a boxplot of average precision valuesfor each phrase category and for bothMBandMDTto geta better understanding of the results. We observe that withan exception of the phrase categoryH2,MDTconsistentlyperforms better thanMBfor all the other phrase categories.A close examination of this exception reveals that the erroroften is in the segmentation of the steady svar regions ofthe melodic phrases corresponding toH2. This can be at-tributed to a speciﬁc subtle melodic movement inH2thatis confused by the segmentation method as a melodic orna-ment instead of a svar transition, leading to a segmentationerror.We now analyse the results for the CMD. From Ta-ble 3 (lower half), we see that using the method variantsMDT,MCW1andMCW2we obtain reasonably higherMAP scores compared to the baseline methodMBand thedifference is found to be statistically signiﬁcant for eachmethod variant across all normalization techniques. ThisMAP score forMDTcorresponds to\u0000=1 s, which is con-siderably higher than the MAP scores for other\u0000valuesas shown in Figure 4. We also see thatMCW2performsslightly better thanMCW1and the difference is found to684 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015H1 H2 H3 H4 H5 Figure 5. Boxplot of average precision values obtainedusingMBandMDTfor each melodic phrase category forthe HMD. These values correspond toNtonic.\nC1 C2 C3 C4 C5 Figure 6. Boxplot of average precision values obtainedusing methodsMB,MDTandMCWfor each melodicphrase category for the CMD. These values correspond toNtetra.be statistically signiﬁcant only in the case ofNtetra. We donot ﬁnd any statistically signiﬁcant difference in the perfor-mance of methodsMDTandMCW2. Unlike the HMD, forthe CMDNtetraresults in the best performance with a sta-tistically signiﬁcant difference compared to the other nor-malization techniques across all method variants. We nowanalyse the average precision values for every phrase cat-egory forMB,MDTandMCW2. SinceMCW2performsslightly better thanMCW1we only considerMCW2forthis analysis. In Figure 6 we see thatMDTperforms betterthanMBfor all phrase categories. We also observe thatMCW2consistently performs better thanMBwith the soleexception ofC2. This exception occurs becauseMCW2presumes a consistency in terms of the number of saddlepoints across the occurrences of a melodic phrase, whichdoes not hold true forC2. This is because phrases corre-sponding toC2are rendered very fast and the subtle pitchmovements are not the characteristic aspect of such mel-odic phrases. Hence, the artists often take the liberty ofchanging the number of saddle points.Overall we see that duration truncation of steady mel-odic regions improves the performance in both the HMDand the CMD. This reinforces our hypothesis that elon-gation of steady svar regions in the melodies of IAM inthe context of the characteristic melodic phrase does notchange the musical identity of the phrase. This correlateswith the concept of ny¯as svar (ny¯as literally means home),where the artist has the ﬂexibility to stay and elongate asingle svar. A similar observation was reported in [24],where the authors proposed to learn the optimal globalDTW constraints a priori for each pattern category. How-ever, their proposed solution could not improve the perfor-mance. Further comparing the results for the HMD and theCMD we notice thatNtonicresults in the best performancefor the HMD andNtetrafor the CMD. This can be at-tributed to the fact that the number of the pitch-transposedoccurrences of a melodic phrase is signiﬁcantly higher inthe CMD compared to the HMD [13]. Also, since thenon-linear timing variability in the HMD is very high, anynormalization (NmeanorNtetra) that involves a decisionbased on the mean frequency of the phrase is more likelyto fail.5. CONCLUSIONSIn this paper we brieﬂy presented an overview of the ap-proaches for detecting the occurrences of the characteris-tic melodic phrases in audio recordings of Indian art mu-sic. We highlighted the major challenges involved in thistask and focused on two speciﬁc issues that arise due tolarge non-linear timing variations and rapid melodic move-ments. We proposed simple and easy to implement solu-tions based on partial transcription and complexity weight-ing to address these challenges. We also put forward anew dataset by appending 30% more number of melodicphrase annotations to those used in previous studies. Weshowed that duration truncation of the steady svar regionsin the melodic phrases results in a statistically signiﬁcantimprovement in the computation of melodic similarity. Thisconﬁrms our hypothesis that the elongation of steady svarregions beyond a certain duration does not affect the per-ception of the melodic similarity in the context of the char-acteristic melodic phrases. Furthermore, we showed thatcomplexity weighting signiﬁcantly improves the melodicsimilarity in Carnatic music. This suggests that the extentand the number of saddle points is an important character-istic of a melodic phrase and is crucial to melodic similar-ity in Carnatic music.In the future, we plan to improve the method used forsegmenting the steady svar regions so that it can differen-tiate melodic ornaments from subtle svar transitions. Inaddition, we see a vast scope in further reﬁning the com-plexity estimate of a melodic phrase to improve the com-plexity weighting. It would also be worthwhile to explorethe applicability of this approach to music traditions suchas Flamenco, Beijing opera and Turkish Makam music.6. ACKNOWLEDGMENTSThis work is partly supported by the European ResearchCouncil under the European Unions Seventh FrameworkProgram, as part of the CompMusic project (ERC grantagreement 267583). We thank Kaustuv K. Ganguli and Vi-gnesh Ishwar for the annotations and valuable discussionsand, Ajay Srinivasamurthy for the proof-reading.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 6857. REFERENCES[1]G. E. Batista, X. Wang, and E. J Keogh. A complexity-invariant distance measure for time series. InSDM, vol-ume 11, pages 699–710, 2011.[2]D. Bogdanov, N. Wack, E. G´omez, S. Gulati, P. Her-rera, O. Mayor, G. Roma, J. Salamon, J. Zapata, andX. Serra. Essentia: an audio analysis library for musicinformation retrieval. InProc. of Int. Society for MusicInformation Retrieval Conf. (ISMIR), pages 493–498,2013.[3]M. A. Casey, R. Veltkamp, M. Goto, M. Leman,C. Rhodes, and M. Slaney. Content-based music in-formation retrieval: Current directions and future chal-lenges.Proc. of the IEEE, 96(4):668–696, 2008.[4]T. Collins, S. B¨ock, F. Krebs, and G. Widmer. Bridg-ing the audio-symbolic gap: The discovery of repeatednote content directly from polyphonic music audio. InAudio Engineering Society’s 53rd Int. Conf. on Seman-tic Audio, 2014.[5]D. Conklin and C. Anagnostopoulou. Comparative Pat-tern Analysis of Cretan Folk Songs.Journal of NewMusic Research, 40(2):119–125, 2010.[6]A. Danielou.The ragas of Northern Indian music.Munshiram Manoharlal Publishers, New Delhi, 2010.[7]S. Dutta and H. A. Murthy. Discovering typical motifsof a raga from one-liners of songs in Carnatic music.InInt. Society for Music Information Retrieval, pages397–402, 2014.[8]A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith.Query by humming: musical information retrieval inan audio database. InProc. of the third ACM Int. Conf.on Multimedia, pages 231–236. ACM, 1995.[9]S. Gulati. A tonic identiﬁcation approach for Indian artmusic. Master’s thesis, Music Technology Group, Uni-versitat Pompeu Fabra, Barcelona, Spain, 2012.[10]S. Gulati, A. Bellur, J. Salamon, H. G. Ranjani, V . Ish-war, H. A. Murthy, and X. Serra. Automatic tonic iden-tiﬁcation in Indian art music: approaches and evalu-ation.Journal of New Music Research, 43(1):55–73,2014.[11]S. Gulati, J. Serr`a, K. K. Ganguli, and X. Serra. Land-mark detection in hindustani music melodies. InInt.Computer Music Conf., Sound and Music ComputingConf., pages 1062–1068, 2014.[12]S. Gulati, J. Serr`a, V . Ishwar, and X. Serra. Miningmelodic patterns in large audio collections of indianart music. InInt. Conf. on Signal Image Technology &Internet Based Systems (SITIS-MIRA), pages 264–271,2014.[13]S. Gulati, J. Serr`a, and X. Serra. An evaluation ofmethodologies for melodic similarity in audio record-ings of indian art music. InIEEE Int. Conf. on Acous-tics, Speech and Signal Processing (ICASSP), pages678–682, 2015.[14]W. B. Hewlett and E. Selfridge-Field.Melodic simi-larity: Concepts, procedures, and applications, vol-ume 11. The MIT Press, 1998.[15]S. Holm. A simple sequentially rejective multiple testprocedure.Scandinavian journal of statistics, 6(2):65–70, 1979.[16]V . Ishwar, A. Bellur, and H. A. Murthy. Motivic anal-ysis and its relevance to raga identiﬁcation in carnaticmusic. InProceedings of the 2nd CompMusic Work-shop, pages 153–157, 2012.[17]V . Ishwar, S. Dutta, A. Bellur, and H. Murthy. Mo-tif spotting in an Alapana in Carnatic music. InProc.of Int. Conf. on Music Information Retrieval (ISMIR),pages 499–504, 2013.[18]Z. Juh´asz. Motive identiﬁcation in 22 folksong corporausing dynamic time warping and self organizing maps.InInt. Society for Music Information Retrieval Conf.,pages 171–176, 2009.[19]H. J Lin, H. H. Wu, and C. W. Wang. Music matchingbased on rough longest common subsequence.J. Inf.Sci. Eng., 27(1):95–110, 2011.[20]Jessica Lin, Eamonn Keogh, Stefano Lonardi, and BillChiu. A symbolic representation of time series, withimplications for streaming algorithms. InProc. of the8th ACM SIGMOD workshop on Research issues indata mining and knowledge discovery, pages 2–11,2003.[21]C. D. Manning, P. Raghavan, and H. Sch¨utze.Intro-duction to information retrieval, volume 1. Cambridgeuniversity press Cambridge, 2008.[22]A. Marsden. Interrogating Melodic Similarity: ADeﬁnitive Phenomenon or the Product of Interpreta-tion?Journal of New Music Research, 41(4):323–335,2012.[23]A. Pikrakis, J. Mora, F. Escobar, and S. Oramas. Track-ing melodic patterns in Flamenco singing by analyzingpolyphonic music recordings. InProc. of Int. Societyfor Music Information Retrieval Conf. (ISMIR), pages421–426, 2012.[24]P. Rao, J. C. Ross, K. K. Ganguli, V . Pandit, V . Ishwar,A. Bellur, and H. A. Murthy. Classiﬁcation of melodicmotifs in raga music with time-series matching.Jour-nal of New Music Research, 43(1):115–131, 2014.[25]S. Rao. Culture Speciﬁc Music Information Processing: A Perspective From Hindustani Music. In2nd Comp-Music Workshop, pages 5–11, 2012.[26]J. C. Ross and P. Rao. Detection of raga-characteristicphrases from Hindustani classical music audio. InProc. of 2nd CompMusic Workshop, pages 133–138,2012.[27]J. C. Ross, T. P. Vinutha, and P. Rao. Detecting melo-dic motifs from audio for Hindustani classical music.InProc. of Int. Conf. on Music Information Retrieval(ISMIR), pages 193–198, 2012.[28]H. Sakoe and S. Chiba. Dynamic programming algo-rithm optimization for spoken word recognition.IEEETrans. on Acoustics, Speech, and Language Process-ing, 26(1):43–50, 1978.[29]J. Salamon and E. G´omez. Melody extraction frompolyphonic music signals using pitch contour charac-teristics.IEEE Transactions on Audio, Speech, andLanguage Processing, 20(6):1759–1770, 2012.[30]Rainer Typke.Music retrieval based on melodic simi-larity. 2007.[31]T. Viswanathan and M. H. Allen.Music in South India.Oxford University Press, 2004.[32]F. Wilcoxon. Individual comparisons by ranking meth-ods.Biometrics bulletin, pages 80–83, 1945.686 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings.",
        "author": [
            "Swapnil Gupta",
            "Ajay Srinivasamurthy",
            "Manoj Kumar 0007",
            "Hema A. Murthy",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1267024",
        "url": "https://doi.org/10.5281/zenodo.1267024",
        "ee": "https://archives.ismir.net/ismir2015/paper/000086.pdf",
        "abstract": "The Tabla Solo dataset is a parallel corpus comprising time-aligned syllabic scores and audio-recordings of 38 solo tabla compositions. The audio and scores for these recordings is from the instructional video DVD titled Shades of Tabla by Pt. Arvind Mulgaonkar.\n\nA companion page to the paper is here:http://compmusic.upf.edu/ismir-2015-tabla\n\nIntroduction\n\nIn Hindustani music, tabla is the main rhythm accompaniment (Examples of individual strokes of tabla can be obtained from here). To showcase the nuances of the tāl (the rhythmic framework of Hindustani music) as well as the skill of the percussionist with the tabla, Hindustani music concerts feature a tabla solo. A tabla solo is intricate and elaborate, with a variety of pre-composed forms used for developing further elaborations. There are specific principles that govern these elaborations. Musical forms of tabla such as the thēkā, kāyadā, palatā, ̣ rēlā, pēśkār and gat ̣are a part of the solo performance and have different functional and aesthetic roles in a solo performance. Harmonium or sarangi usually plays the role of a time-keeper in tabla solo performances.\n\n\n\nPercussion in Hindustani music is organized and orally transmitted with the use of onomatopoeic mnemonic syllables (called the bōl) representative of the different strokes of tabla. Further, tabla has different stylistic schools called gharānās. The repertoires of major gharānās or schools of tabla differ in aspects such as the use of specific bōls, the dynamics of strokes, ornamentation and rhythmic phrases. But there are also many similarities due to the fact that same forms and same standard phrases reappear across these repertoires.\n\nThe Dataset\n\nThe syllabic representation for tabla solos provide a meaningful representation for analysis. This dataset uses a such a representation. The dataset comprises audio recordings, scores and time aligned syllabic transcriptions for 38 tabla solo compositions of different forms in tīntāl (a metrical cycle of 16 time units). The compositions are from the instructional video DVD Shades Of Tabla by Pandit Arvind Mulgaonkar, who is among the most renowned contemporary tabla maestros. Out of the 120 compositions in the DVD, we chose 38 representative compositions spanning all the gharānās of tabla (Ajrada, Benaras, Dilli, Lucknow, Punjab, Farukhabad). The dataset contains about 17 minutes of audio with over 8200 syllables.\n\nAudio\n\nThe audio is extracted from the DVD video and segmented at the level of compositions from the full audio recording. The audio files are mono wav files, sampled at 44.1 kHz with a bit depth of 16 bits. All audios have a soft harmonium accompaniment.\n\nAnnotations\n\nThe booklet accompanying the DVD provides a syllabic transcription for each composition. We used Tesseract, an open source Optical Character Recognizer (OCR) engine to convert printed scores to a machine readable format. The scores obtained from OCR were manually verified and corrected for errors, adding the the vibhāgs (sections) of the tāl to the syllabic transcription. A time aligned syllabic transcription for each score and audio file pair was obtained using a spectral flux based onset detector followed by manual correction. The score for each composition has additional metadata describing gharānā, composer and its musical form.\n\nThe scores in the booklet consists of 41 different mnemonic syllables that are reduced and mapped to 18 syllables based on the timbral similarity between the syllables. The list of syllables along with their mapping can be found here: Syllable Mappings\n\nDataset Organization\n\nThe dataset consists of set of four files for each composition:\n\n\n\t\n\tWAV audio file (*.wav)\n\t\n\t\n\tThe syllable scores as retrieved from the booklet with the metadata (*.txt)\n\t\n\t\n\tTime-aligned non-mapped syllabic score with stroke onset times (*.csv)\n\t\n\t\n\tTime-aligned mapped syllabic score with stroke onset times (*.csv)\n\t\n\n\nPossible Uses of the Dataset\n\nThe dataset can be used for variety of of MIR tasks such as onset detection, percussion transcription, rhythm and percussion pattern analysis, and tabla stroke modeling.\n\nUsing this dataset\n\nIf you use the dataset in your work, please cite the following publication:\n\n\nS. Gupta, A. Srinivasamurthy, M. Kumar, H. A. Murthy, X. Serra. Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings. In Proc. of the 16th International Society for Music Information Retrieval Conference (ISMIR), 2015.\n\n\nhttp://hdl.handle.net/10230/25697\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nAjay Srinivasamurthy\nPhD Student, Music Technology Group\nUniversitat Pompeu Fabra,\nBarcelona, Spain\najays.murthy@upf.edu\n\nSwapnil Gupta\nMasters Student, Sound and Music Computing\nUniversitat Pompeu Fabra,\nBarcelona, Spain\nsuapnilgupta.iiith@gmail.com\n\nXavier Serra\nHead, Music Technology Group\nUniversitat Pompeu Fabra,\nBarcelona, Spain\nxavier.serra@upf.edu\n\n\n\nhttp://compmusic.upf.edu/tabla-solo-dataset",
        "zenodo_id": 1267024,
        "dblp_key": "conf/ismir/GuptaSKMS15",
        "keywords": [
            "Tabla Solo dataset",
            "parallel corpus",
            "time-aligned syllabic scores",
            "audio-recordings",
            "38 solo tabla compositions",
            "instructional video DVD",
            "Shades of Tabla",
            "Hindustani music concerts",
            "tabla solo",
            "intricate and elaborate"
        ],
        "content": "DISCOVERYOF SYLLABIC PERCUSSION PATTERNS IN TABLA SOLORECORDINGSSwapnil Gupta⇤br\u001cTMBHX;mTi\u001cyR!2bim/B\u001cMiXmT7X2/mAjaySrinivasamurthy⇤\u001cD\u001cvbXKm`i?v!mT7X2/mManojKumar†K\u001cMQDT\u001cKF!;K\u001cBHX+QKHemaA. Murthy†?2K\u001c!+b2XBBiKX\u001c+XBMXavierSerra⇤t\u001cpB2`Xb2``\u001c!mT7X2/m⇤Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain†DONlab, Indian Institute of Technology Madras, Chennai, IndiaABSTRACTWe address the unexplored problem of percussion patterndiscoveryinIndianartmusic. PercussioninIndianartmu-sicusesonomatopoeicoralmnemonicsyllablesforthetrans-missionofrepertoireandtechnique. Thisisutilizedforthetaskofpercussionpatterndiscoveryfromaudiorecordings.From a parallel corpus of audio and expert curated scoresfor 38 tabla solo recordings, we use the scores to build aset of most frequent syllabic patterns of different lengths.Fromthisset,wemanuallyselectasubsetofmusicallyrep-resentativequerypatterns. Todiscoverthesequerypatternsinanaudiorecording,weusesyllable-levelhiddenMarkovmodels (HMM) to automatically transcribe the recordinginto a syllable sequence, in which we search for the querypattern instances using a Rough Longest Common Subse-quence (RLCS) approach. We show that the use of RLCSmakestheapproachrobusttoerrorsinautomatictranscrip-tion, significantly improving the pattern recall rate and F-measure. Wefurtherproposepossibleenhancementstoim-prove the results.1. INTRODUCTIONIn many music cultures, music is sometimes transmittedpartly through speech, using what are variously called vo-cables, oral mnemonics, solfège, etc [12]. In the case ofseveralpercussiontraditions,thechoiceofvowelsandcon-sonants is such that the syllables closely represent the un-derlying acoustic phenomenon they represent. The termacoustic-iconic mnemonicsystems coined by Hughes [12]explains this mnemonic based syllable systems where thecore aspect is the similarity of the phonetic features of thesyllables with the acoustic properties of the sounds theyrepresent. A well studied example of such a system is thetabla,wheretherepertoireandtechniqueistransmittedwiththe help of a system based on onomatopoeic oral sylla-bles[18]. Inthispaper,weexploretheuseofthemnemonicsyllablesystemoftablaforthediscoveryofpercussionpat-terns. Theuseofthesemnemonicsallowsustoworkwitha© Swapnil Gupta, Ajay Srinivasamurthy, Manoj Kumar,Hema A. Murthy, Xavier Serra.LicensedunderaCreativeCommonsAttribution4.0InternationalLicense(CC BY 4.0).Attribution:Swapnil Gupta, Ajay Srinivasamurthy,Manoj Kumar, Hema A. Murthy, Xavier Serra. “Discovery of SyllabicPercussion Patterns in Tabla Solo Recordings”, 16th International Soci-ety for Music Information Retrieval Conference, 2015.musicallyrelevantrepresentationthattrulyreflectstheun-derlying timbre, articulation and dynamics of the patternsplayed.Automatic discovery of patterns is a relevant Music In-formation Retrieval (MIR) task. It has applications in en-riched and informed music listening, enhanced apprecia-tion for listeners, in music training, and in aiding musicol-ogists working on such music cultures. We use the ono-matopoeicoralmnemonicsyllablestorepresent,transcribeand search for patterns in audio recordings of tabla solos.We first build a set of query patterns from the corpus ofscores in our dataset. Given an audio recording, we auto-maticallytranscribeitintoasequenceofsyllables. Wethenpropose a method for searching the query patterns in theautomatically transcribed score using approximate stringsearch. Wealsoproposeseveralextensionstoimprovethesearch performance. We first provide a brief introductionto tabla.1.1 Tabla and its solo performancesTablaisthemainrhythmaccompanyinginstrumentinHin-dustani music, the art music tradition from North India. Itconsists of two drums: a left hand bass drum called thebāyānordiggāandarighthanddrumcalledthedāyānthatcanproduceavarietyofpitchedsounds[15]. Toshowcasethenuancesofthetāl(therhythmicframeworkofHindus-tani music) as well as the skill of the percussionist withthe tabla, Hindustani music performances feature tabla so-los. A tabla solo is intricate and elaborate, with a varietyof pre-composed forms used for developing further elab-orations. There are specific principles that govern theseelaborations [10, p. 42]. Musical forms of tabla such astheṭhēkā, kāyadā, palaṭā, rēlā, pēśkārandgaṭare a partof the solo performance and have different functional andaesthetic roles in a solo performance.Playing a tabla is taught and learned through the useof onomatopoeic oral mnemonic syllables called thebōl,which are vocal syllables corresponding to different tim-bres that can be produced on the tabla. However, severalbōlscorrespondtothesamestrokeplayedonthetabla,cre-ating a many bōl to same timbre mapping, which can beexploitedtodiscoveracousticallysimilarpatterns. Thoughthe primary function of the bōls is to provide a representa-tion system, a rhythmic vocal recitation of the bōls, whichrequires high skills, is inserted into solo performances formusic appreciation.Tablahasdifferentstylisticschoolscalledgharānās. The385Sym.bōlsSym.bōlsDAD,DA, DAANAN,NA, TAA, TUKIKA, KAT, KE, KI,KIIDINDI, DIN, DING,KAR,GHENGEGA,GHE,GE,GHI,GIKDAKDA, KRA, KRI,KRUTATA,TI, RATITCHAP,TITTable 1: The bōls used in tabla, their grouping, and thesymbol we use for the syllable group in this paper. ThesymbolsDHA,DHE,DHET,DHI,DHIN,RE,TE,TII,TIN,TRAhaveaonetoonemappingwithasyllableofthesamename and hence not shown in the table.repertoiresofmajorgharānāsoftabladifferinaspectssuchas the use of specific bōls, the dynamics of strokes, orna-mentation and rhythmical phrases [4, p. 60]. But there arealso many similarities due to the fact that the same formsandstandardphrasesreappearacrosstheserepertoires[10,p. 52]. This enables in creation of a library of standardphrasesorpatternsacrosscompositionsofdifferentgharānās.1.2 Previous WorkEarly research related to tabla focused mainly on stroketranscription,asseenintheworkofGillet[9]. Chordia[6]extended the work adding additional features and classi-fiers, using a larger and more diverse dataset. The use oftabla syllables in a predictive model for tabla stroke se-quencewasalsodemonstratedrecentlybyChordiaetal.[7].RecentworkintranscriptionhasbeenreportedforMridan-gam, the percussion accompaniment used in South IndianCarnatic music, by Kuriakose et al. [13] and Anantapad-manabhan et al. [1]. The transcription task has a definiteanalogy to speech recognition and we can apply severaltoolsandknowledgefromthiswellexploredresearchareawith many state of the art algorithms and systems [11].There is significant literature on pattern search and re-trieval from percussion solos. Nakano et al. [16] addresstheproblemofdrumpatternretrievalusinganHMMbasedapproachusingonomatopoeiaastherepresentationfordrumpatterns, retrieving known fixed sequences from a libraryof drum patterns with snare and bass drums. We use asimilar approach, the main difference being that we use amusicallywellgroundedsyllabicrepresentation. Recently,Srinivasamurthyetal.[20]demonstratedtheuseofsyllablelevelHMMfollowedbyastringeditdistancetotranscribeand classify percussion patterns in Beijing Opera. Tsunooetal.[21]alsodemonstratedamusicclassificationtaskus-ingK-meansclusteringofbar-longpercussivepatternsandbasslinesextractedusingone-passdynamicprogramming.Whilethelasttwomentionedapproachesaimatclassifica-tion of patterns, we address the general task of retrievingpatterns from recordings of full length solo compositions.Transcription is often inaccurate with many errors, andanypatternsearchontranscribeddataneedstouseapproxi-matestringsearchalgorithms. Thereareseveralattemptstodealwithsearchinsymbolicsequences[22]. Wellexploredtechniquessuchaslongestcommonsubsequence(LCS)donotconsiderthelocalcorrelationwhilesearchingforasub-sequence [14]. To overcome this limitation, Lin et al. [14]proposed a novel Rough Longest Common Subsequence(RLCS) method for music matching. Dutta et al. [8] usedamodifiedversionofRLCSformotifspottinginālāpanasof Carnatic music. We propose to use a similar approachwithminormodificationstosuitthesymbolicdomainspe-cific to our use case. To the best of our knowledge, this isthe first work to explore syllabic pattern discovery as ap-plied to tabla solos in Hindustani music.2. PROBLEM FORMULATIONWe formulate the problem of discovery of percussion pat-ternsintablasolorecordings. Wepresentageneralframe-work for the task, while outlining some of the challenges.The approach we explore in this paper is to use syllablesto define, transcribe, and eventually search for percussionpatterns. We build a fixed set of syllabic query patterns.Given an audio recording, we obtain a time-aligned syl-labictranscriptionusingsyllableleveltimbralmodels. Foreachofthequerypatternsintheset,wethenperformanap-proximate search on the output transcription to obtain thelocations of the patterns in the audio recording. We de-scribe each of the steps in detail.We first compile a comprehensive set of syllables intabla. Although, the syllables vary marginally within andacrossgharānās,severalbōlscanrepresentthesamestrokeon the tabla. To address this issue, we grouped the full setof 41 syllables into timbrally similar groups resulting intoa reduced set of 18 syllable groups as shown in Table 1.Though each syllable on its own has a functional role, thistimbral grouping is presumed to be sufficient for discov-ery of percussion patterns. For the remainder of the paper,welimitourselvestothereducedsetofsyllablegroupsandusethemtorepresentpatterns. Forconvenience,whenitisclear from the context, we call the syllable groups as justsyllables and denote them by the symbols in Table 1. Fur-ther,weusebōlsandsyllablesinterchangeably. Letthesetof syllables be denoted asS={S1,S2,···SM},M=18.Apercussionpatternisnotwelldefinedandvarieddef-initions can exist. Here, we use a simplistic definition of apattern, as a sequence of syllables. A pattern is defined asPk=[s1,s2,···,sLk]wheresk2SandLkis the lengthofPk. Though,fordefiningpatterns,itisimportanttocon-sider the relative and absolute durations of the constituentsyllables, as well as the metrical position of the pattern inthetāl,weuseasimpledefinitionandleaveamorecompre-hensive definition for future work. In this paper, we takea data driven approach to build a set ofKquery patterns,P={P1,P2,···PK}.Givenanaudiorecordingx[n],itisfirsttranscribedintoasequenceoftime-alignedsyllables,Tx=[ (t1,s1),(t2,s2),···,(tLx,sLx)], wheretiis the onset time of syllablesi.The task of syllabic transcription has a significant analogyto connected word speech recognition using word models.Syllables are analogous to words and a percussion patterntoasentence-asequenceofwords. Finally,givenaquerypatternPkoflengthLk,wesearchforthepatternintheout-put syllabic transcriptionTx, to retrieve the subsequencesp(n)kinTx(n=1,···,Nk) that match the query, where386 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015FeatureextractionAudio recordings\nTranscriptionsEmbedded modelre-estimationViterbidecodingTestingTrainingFeatureextractionRLCSTest Audio\n0.153 0.484 DHA0.453 0.598 KA0.598 0.738 TA.... .... ...36.865 50.125 KI0.186 0.523 TIN0.453 0.598 RA0.598 0.738 TA.... .... ...42.965 50.125 KA0.188 0.387 KI0.453 0.598 TA0.598 0.738 TA.... .... ...36.865 50.125 KI0.058 0.239 DHA0.239 0.465 GE0.465 0.538 TA.... .... ...49.911 50.125 NAIsolatedstroke models\nPattern library,KI TA TA KATA TA KI TA... ... ...DHE RE DHE RETraining Data\nTranscriptionFigure 1: The block diagram of the approachNkisthenumberofretrievedmatchesforPk. Weusep(n)kand the corresponding onset times fromTxto extract au-dio segments corresponding to the retrieved syllabic pat-terns. Syllabic transcription is often not exact and it canhave common transcription errors such as insertions, sub-stitutions and deletions, to handle which we need an ap-proximate search algorithm.3. DATASETTo evaluate our approach to percussion pattern discovery,weneedaparallelcorpuswithtime-alignedscoresandau-dio recordings. These are useful both for building isolatedstroke timbre models and for a comprehensive evaluationoftheapproach. Webuiltadatasetcomprisingaudioreco-rdings,scoresandtimealignedsyllabictranscriptionsof38tabla solo compositions of different forms intīntāl(a met-rical cycle of 16 time units). The compositions were ob-tained from the instructional video DVDShades Of Tablaby Pandit Arvind Mulgaonkar1. Out of the 120 compo-sitions in the DVD, we chose 38 representative composi-tions spanning all the gharānās of tabla (Ajrada, Benaras,Dilli,Lucknow,Punjab,Farukhabad). Thebookletaccom-panyingtheDVDprovidesasyllabictranscriptionforeachcomposition. WeusedTesseract[19],anopensourceOpti-calCharacterRecognizer(OCR)enginetoconvertprintedscores to a machine readable format. The scores obtainedfromOCRweremanuallyverifiedandcorrectedforerrors,adding the thevibhāgs(sections) of the tāl to the syllabictranscription. The score for each composition has addi-tional metadata describing the gharānā, composer and itsmusical form.WeextractedaudiofromtheDVDvideoandsegmentedthe audio for each composition from the full audio record-ing. The audio recordings are stereo, sampled at 44.1 kHzandhaveasoftharmoniumaccompaniment. Atimealignedsyllabictranscriptionforeachscoreandaudiofilepairwasobtained using a spectral flux based onset detector [3] fol-1?iiT,ffKmbB+#`\u001cBMxXQ`;f`2H2\u001cb2fkky+827+@kj8y@9j//@N8+e@93dy/+e38R78IDPatternLCount1DHE, RE, DHE, RE, KI, TA, TA,KI, NA, TA, TA, KI, TA, TA, KI,NA16472TA, TA, KI, TA, TA, KI, TA, TA,KI, TA, TA, KI, TA, TA, KI, TA16103TA, KI, TA, TA, KI, TA, TA, KI8614TA, TA, KI, TA, TA, KI62145TA, TA, KI, TA43796KI, TA, TA, KI44507TA, TA, KI, NA41678DHA, GE, TA, TA497Table 2: Query Patterns, their ID (k), length (L) and thenumber of instances in the dataset (Total instances: 1425)lowed by manual correction by the authors. The datasetcontains about 17 minutes of audio with over 8200 sylla-bles. The dataset is freely available for research purposesthrough a central online repository2.4. APPROACHThe block diagram in Figure 1 shows us the overall ap-proach. It comprises three major steps:building a set ofquerypatterns,transcription,andsearch. Inthefollowingsections, we describe each of these in detail.4.1 Building a set of query patternsAdatadrivenapproachistakentocreateasetofquerypat-terns of lengthL=4,6,8,16. These lengths were chosenbased on the structure of tīntāl for differentlayas(tempoclasses)[4,p.126]. Usingthesimpledefinitionofapatternasasequenceofsyllables,weusethescoresofthecompo-sitionstogeneratealltheLlengthpatternsthatoccurinthescorecollection. Wesortthembytheirfrequencyofoccur-rencetogetanorderedsetofpatternsforeachstatedlength.Wethenmanuallychoosemusicallyrepresentativepatternsfromthisorderedsetofmostcommonlyoccurringpatternsto form a set of query patterns. Table 2 shows the chosenpatterns,theirlengthandtheircountinthedataset,leadingto a total of 1425 instances. We want a diverse collectionof patterns to test if the algorithms generalize. Hence wechoosepatternsthathaveavariedsetofsyllablesthathavedifferenttimbralcharacteristics,likesyllablesthatarehar-monic (DHA), syllables played with a flam (DHE,RE) andsyllables having bass (GE).4.2 TranscriptionSome bōls of tabla may be pronounced with a differentvowel or consonant depending on the context, without al-tering the drum stroke [5]. Furthermore, the bōls and thestrokes vary across different gharānās, making the task oftranscriptionoftablasoloschallenging. Tomodelthetim-braldynamicsofsyllables,webuildanHMMforeachsyl-lable (analogous to a word-HMM). We use these HMMsalong with a language model to transcribe an input audiosolo recording into a sequence of syllables.2?iiT,ff+QKTKmbB+XmT7X2/mfi\u001c#H\u001c@bQHQ@/\u001ci\u001cb2iProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 387The stereo audio is converted to mono, since there isno additional information in stereo channels. We use theMFCC features to model the timbre of the syllables. Tocapturethetemporaldynamicsofsyllables,weaddtheve-locity and the acceleration coefficients of the MFCC. The13 dimensional MFCC features (including the0thcoeffi-cient) are computed from the audio with a frame size of23.2 ms and a shift of 5.8 ms. We also explore the use ofenergy(asmeasured bythe0thMFCCcoefficient)intran-scriptionperformance. Hencewehavetwosetsoffeatures,MFCC_0_D_A, the 39 dimensional feature including the0th, delta and double-delta coefficients, and MFCC_D_A,the 36 dimensional vector without the0thcoefficient.Usingthefeaturesextractedfromtrainingaudiorecord-ings,wemodeleachsyllableSuusinga7-stateleft-to-rightHMM{\u0000u},1uU(= 18), including an entry andan exit non-emitting states. The emission density of eachemittingstateismodeledwithathreecomponentGaussianMixtureModel(GMM)tocapturethetimbralvariabilityinsyllables. Weexperimentedwithhighernumberofcompo-nents in the GMMs, but with little performance improve-ment. We use the time aligned syllabic transcriptions andtheaudiorecordingsintheparallelcorpustodoanisolatedHMMtrainingforeachsyllable. WethenusetheseHMMsfurther in an embedded model Baum-Welch re-estimationto get the final syllable HMMs.Tabla solos are built hierarchically using short phrases,and hence some bōls tend to follow a bōl more often thanothers. In such a scenario, a language model can improvetranscription. In addition to a flat language model withuniform unigram and transition probabilities, i.e.p(s1=Su) = 1/Uandp(si+1=Sv/si=Su) = 1/U, with1u, vUandibeing the sequence index, we explorethe use of a bigram language model learned from data.Fortesting,wetreatthefeaturesequenceextractedfromtest audio file to have been generated from a first ordertime-homogeneousdiscreteMarkovchain,whichcancon-sist of any finite length sequence of syllables. From theextracted feature sequence, we use the HMMs{\u0000u}and asyllable network constructed from the language model todoaViterbi(forced)alignment,whichaimstoprovidethebestsequenceofsyllablesandtheironsetsTx. Allthetran-scription experiments were done using the HMM Toolkit(HTK) [23].4.3 Pattern SearchTheautomaticallytranscribedoutputsyllablesequenceTxis used to search for the query patterns. Transcription isofteninaccurateinboththesequenceofsyllablesandintheexact onset times of the transcribed syllables. We need tohandleboththeseerrorsinapatternsearchtaskfromaudio.We primarily focus on the errors in syllabic transcriptioninthispaper. WeusethesyllableboundariesoutputbytheViterbi algorithm, without any additional post processing.We can improve the output syllable boundaries using anonset detector [3], but we leave this task to future work.Therearethreemainkindsoferrorsintheautomaticallytranscribedsyllablesequence: Insertions(I),Deletions(D),and Substitutions (B). Further, the query pattern is to besearchedinthewholetranscribedcomposition,wheresev-eralinstancesofthequerycanoccur. RoughLongestCom-mon Subsequence (RLCS) method is a suitable choice forsuch a case. RLCS is a subsequence search method thatsearchesforroughlymatchedsubsequenceswhileretainingthelocalsimilarity[14]. WemakefurtherenhancementstoRLCS to handle the I, D and B errors in transcription.We use a modified version of the RLCS approach asproposedbyLinetal.[14]withchangesproposedbyDuttaet al. [8] to handle substitution errors. We propose a fur-ther enhancement to handle insertions and deletions, andexplore its use in the current task. We first present a gen-eral form of RLCS and then discuss different variants ofthe algorithm.Given a query patternPkof lengthLkand a referencesequence (transcribed syllable sequence)Txof lengthLx,RLCS uses a dynamic programming approach to computea score matrix (of sizeLx⇥Lk) between the referenceand the query with a rough length of match. We can usea threshold on the score matrix to obtain the instances ofthe query occurring in the reference. We can then use thesyllableboundariesintheoutputtranscriptionandretrievethe audio segment corresponding to the match.Fortheeaseofnotation,weindexthetranscribedsylla-blesequenceTxwithiandthequerysyllablesequencePkwithj. Wecomputetheroughandactuallengthofthesub-sequencematchessimilartothewaycomputedbyDuttaetal. [8]. At every position(i, j), a syllable is included intothe matched subsequence ifd(si,sj)<\u0000, whered(si,sj)is the timbral distance between the syllables at positionsiandjin the transcription and query, respectively.\u0000is thethreshold distance below which the two syllables are saidto be equivalent. The matrices of rough length of match(C) and the actual length of match (Ca)are updated as,C(i, j)=C(i\u00001,j\u00001) + (1\u0000d(si,sj)).\nd(1)Ca(i, j)=Ca(i\u00001,j\u00001) +\nd(2)where,\ndisanindicatorfunctionthattakesavalueof1ifd(si,sj)<\u0000,else0. ThematrixCthuscontainsthelengthof rough matches ending at all combinations of the sylla-ble positions in reference and the query. The rough lengthand an appropriate distance measure handles the substitu-tion errors during transcription. To penalize insertion anddeletionerrors,wecomputea“density”ofmatchusingtwomeasures called the Width Across Reference (WAR) andWidth Across Query (WAQ), respectively. The WAR (R)and WAQ (Q) matrices are initialized toRi,j=Qi,j=0wheni.j=0,and propagated as,Ri,j=8><>:Ri\u00001,j\u00001+1d(si,sj)<\u0000Ri\u00001,j+1d(si,sj)\u0000\u0000,Ci\u00001,j\u0000Ci,j\u00001Ri,j\u00001d(si,sj)\u0000\u0000,Ci\u00001,j<Ci,j\u00001(3)Qi,j=8><>:Qi\u00001,j\u00001+1d(si,sj)<\u0000Qi\u00001,jd(si,sj)\u0000\u0000,Ci\u00001,j\u0000Ci,j\u00001Qi,j\u00001+1d(si,sj)\u0000\u0000,Ci\u00001,j<Ci,j\u00001(4)Here,Ri,jis the length of substring containing the subse-quencematchendingattheithandthejthpositionoftheref-erence and the query, respectively.Qi,jrepresents a simi-388 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015larmeasureinthequery. Whenincremented,Ri,jandQi,jareincrementedby1similartothewayformulatedbyLinet al. [14]. At the same time, the increment is done basedon the conditions formulated by Dutta et al. [8].Using the rough length of match (C), actual length ofmatch(Ca),andwidthmeasures(RandQ),wecomputeascorematrix\u0000thatincorporatespenaltiesforsubstitutions,insertions, deletions, and additionally, the fraction of thequery matched.\u0000i,j=\u0000\u0000\u0000·f\u0000Ci,jRi,j\u0000+(1\u0000\u0000)·f\u0000Ci,jQi,j\u0000\u0000·C(i,j)LkifCa(i,j)Lk\u0000\u00000otherwise(5)where\u0000i,jis the score for the match ending at theithandthejthpositionofthereferenceandthequery,respectively.fis a warping function for the rough match length densi-tiesCi,jRi,jin the reference andCi,jQi,jin the query. The pa-rameter\u0000controls their weights in the convex combina-tionforscorecomputation. ThetermCai,jLkisthefractionofthe query length matched and is used for thresholding theminimum fraction of the query to be matched.Startingwithallcombinationsofiandjastheendpointsof the match in the reference and the query, respectively,we perform a traceback to get the starting points of thematch. RLCS algorithm outputs a match when the scoreis more than a score threshold . However, with a simplescore thresholding, we get multiple overlapping matches,from which we select the match with the highest score. Ifthe scores of multiple overlapping matches are equal, weselect the ones that have the lowest width (WAR). Thisway, we obtain a match that has the highest score density.Weusethesenon-overlappingmatchesandthecorrespond-ing syllable boundaries to retrieve the audio patterns.4.3.1 Variants of RLCSThe generalized RLCS provides a framework for subse-quence search. The parameters⇢,\u0000, and\u0000can be tunedto make the algorithm more sensitive to different kinds oftranscriptionerrors. Thevariantsweconsiderhereusedif-ferentdistancemeasuresd(si,sj)inEqn (1)tohandlesub-stitutions and different functionsf(.)in Eqn (5) to handleinsertions and deletions. We explore these variants for thecurrent task and evaluate their performance.InadefaultRLCSconfiguration(RLCS0),weonlycon-siderexactsyllablematches. Weset\u0000=1anduseabinarydistancemetricbasedonthesyllablelabel,i.e.d(si,sj)=0ifsi=sj, and 1 otherwise. Further, an identity warpingfunction, f(y) = y is used.Theroughlengthmatchdensitiescanbetransformedus-ing a non-linear warping function to penalize low densityvaluesmorethanthehigherones,leadingtoanothervariantofRLCS(RLCS). Inthispaper,weonlyexplorewarpingfunctions of the form,f(y)=ey\u00001e\u00001(6)where>0is a parameter to control warping, larger val-ues oflead to more deviation from an identity transfor-mation. RLCS0isa limiting case of RLCSwhen!0.Wehypothesizethatthesubstitutionerrorsintranscrip-tionareduetotheconfusionbetweentimbrallysimilarsyl-lables. Atimbralsimilarity(distance)measurebetweenthesyllables can thus be used to make an RLCS algorithm ro-busttospecifickindsofsubstitutionerrors. Inessence,wewant to disregard and give a greater allowance for substi-tutions between timbrally similar syllables during RLCSmatching. Computing timbral similarity is a wide area ofresearch and has many different proposed methods [17],but we restrict ourselves to a basic timbral distance mea-sure: the Mahalanobis distance between the cluster cen-ters obtained using a K-means clustering of MFCC fea-tures(with3clusters)fromisolatedaudioexamplesofeachsyllable [2]. We call this variant of RLCS as RLCS\u0000andexperiment with different thresholds\u0000. For better repro-ducibility of the work in this paper, an implementation ofthe different variants of RLCS described is available3.5. EXPERIMENTS AND RESULTSWeexperimentwithdifferentsetsoffeaturesandlanguagemodels for transcription. With the best performing tran-scriptionconfiguration,weexperimentwithdifferentRLCSvariantsandreporttheirperformance. Wefirstdescribetheevaluation measures used in this paper.5.1 Evaluation measuresWe use the ground truth time aligned syllabic transcrip-tions to evaluate both the transcription and pattern searchalgorithms. We evaluate transcription performance usingthe measures often used in speech recognition, Correct-ness(Corr.) andAccuracy(Accu.). GiventhegroundtruthtranscriptionT⇤xoflengthN,thetranscribedsequenceTx,and the number of insertions, deletions and substitutionsasNI,ND, andNB, respectively, we compute Corr.=(N−ND−NB)/NandAccu.=(N−ND−NB−NI)/N. TheCorrectnessmeasurepenalizesdeletionsandsubstitutions,while Accuracy measure additionally penalizes insertions.For pattern retrieval, we don’t evaluate the accuracy ofboundary segmentation. However, we call a retrieved pat-tern from RLCS ascorrectly retrievedif it has at least a70% overlap with the pattern instance in ground truth. Toevaluate pattern search performance, we use the standardinformationretrievalmeasuresprecision(theratiobetweenthenumberofcorrectlyretrievedpatternsandallretrievedpatterns) and recall (the ratio between number of correctlyretrievedpatternsandthepatternsinthegroundtruth). Theharmonicmeanofprecisionandrecall,calledtheF-measureis also reported.5.2 Results and DiscussionThe transcription results shown in Table 3 are the meanvaluesinaleave-one-outcrossvalidationoverthedataset.We experimented with the two different MFCC features(MFCC_D_AandMFCC_0_D_A)andtwolanguagemod-els (a flat model and a bigram learnt from data). Overall,we see a best Accuracy of 53.13%, which justifies the useofarobustapproximatestringsearchalgorithmforpatternretrieval. Theuseofabigramlanguagemodellearnedfromdata improves the transcription performance. We see that3?iiT,ff+QKTKmbB+XmT7X2/mfBbKB`@kyR8@i\u001c#H\u001cProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 389FeatureCorr.Accu.FlatlanguagemodelMFCC_D_A64.0745.01MFCC_0_D_A64.2649.27Bigram languagemodelMFCC_D_A65.5349.97MFCC_0_D_A66.2353.13Table 3: Transcription results showing the Correctness(Corr.) andAccuracy(Accu.) measures(inpercentage)fordifferent features and language models. In each column,thevaluesinboldarestatisticallyequivalenttothebestre-sult (in a paired-sample t-test at 5% significance levels).the Accuracy measure is lower than the Correctness mea-sure, which shows that there are a significant number ofinsertion errors in transcription. We use the output tran-scriptionsfromthebestperformingcombination(MFCC_-0_D_Aandabigramlanguagemodel)toreporttheperfor-mance of the RLCS variants.To form a baseline for string search performance withthe output transcriptions, we used an exact string searchalgorithm and report its performance in Table 4 (shown asBaseline). We see that the baseline has a precision that issimilartotranscriptionperformance,butaverypoorrecallleading to a poor F-measure.ToestablishtheoptimumparametersettingsforRLCS,we performed a grid search over the values of\u0000,⇢and with RLCS0.\u0000and are varied in the range0to1. Toensure that the minimum length of the pattern matched isat least 2, we varied⇢between1.1/min(Lk)and 1.\u0000istheconvexsumparameterforthecontributionoftherough match length density of the reference and the querytowards the final score. With increasing\u0000, we give moreweight to the reference length ratio, allowing more inser-tions. We observed a poor true positive rate with larger\u0000,and hence we validate the observation that insertion errorscontribute to a majority of transcription errors.The best average F-measure over all the query patternsin an experiment using RLCS0is reported in Table 4. Wesee that RLCS0improves the recall, but with a lower pre-cision and an improved F-measure, showing that the flex-ibility in approximate matching provided by RLCS comesat the cost of additional false positives. The values of⇢,\u0000and that give the best F-measure are then fixed for allsubsequentexperimentstocomparetheperformanceoftheproposed RLCS variants.Itisobservedthatthepatternscomposedofsmallerrepet-itivepatterns(andhencehavingambiguousboundaries)re-sult in a poor precision (e.g.P2andP3in Table 2 with aprecisionof 0.108and 0.239,respectively).P1inTable 2,on the contrary, has non-ambiguous boundaries leading toa good precision of 0.692. The effect of the length of apattern on precision is also evident. Small patterns (withL=4) that have non-ambiguous boundaries (e.g.P8inTable 2withaprecisionof0.384)haveapoorprecisionascompared to longer patterns with non-ambiguous bound-aries (e.g.P1in Table 2). The reason for this is that thesmaller patterns are more prone to errors as the search al-gorithm has to match a lower number of syllables.TheresultswithothervariantsofRLCSarealsoreportedin Table 4. The results from RLCS\u0000show that the use ofVariant Parameter Precision Recall F-measureBaseline -0.479 0.254 0.332RLCS0\u0000=1 0.384 0.395 0.389RLCS\u0000\u0000=0.3 0.139 0.466 0.214RLCS\u0000\u0000=0.6 0.0837 0.558 0.145RLCS=1 0.412 0.350 0.378RLCS=4 0.473 0.268 0.342RLCS=7 0.482 0.259 0.336RLCS=9 0.481 0.258 0.335Table4: PerformanceofdifferentRLCSvariantsusingthebestperformingparametersettingsforRLCS0(⇢=0.875,\u0000=0.76and =0.6).a timbral syllable distance measure with higher threshold\u0000furtherimprovestherecall,butwithamuchlowerpreci-sion and F-measure. Although we find matches that havesubstitution errors using the distance measure, we retrieveadditionalmatchesthatdonothavesubstitutionerrorscon-tributing to additional false positives. On the contrary, us-inganon-linearwarpingfunctionf(.)inRLCSimprovesthe precision with a higher value of. The penalties onmatches with higher number of insertions and deletions ishigh and they are left out, leading to good precision at thecost of recall. We observe that both the above mentionedvariantsimproveeitherprecisionorrecallatthecostoftheother measure. They need further exploration with bettertimbral similarity measures to be combined in an effectiveway to improve the search performance.6. SUMMARYWeaddressedtheunexploredproblemofadiscoveringsyl-labicpercussionpatternsinTablasolorecordings. Thepre-sented formulation used a parallel corpus of audio record-ingsandsyllabicscorestocreateasetofquerypatterns,thatwere searched in an automatically transcribed (into sylla-bles) piece of audio. We used a simplistic definition of apatternandexploredRLCSbasedsubsequencesearchalgo-rithm,usinganHMMbasedautomatictranscription. Com-paredtoabaseline,weshowedthattheuseofapproximatestring search algorithms improved the recall at the cost ofprecision. Additionally,proposedvariantsimprovedeithertheprecisionorrecall,butdonotprovideasignificantim-provement in the F-measure over the basic RLCS.Forfuturework,weaimtoimprovesyllableboundariesoutput by transcription using onset detection. Inclusion ofthe rhythmic information can be an interesting aspect indefininganddiscoveringpercussionpatterns,andwillhelpin comprehensively evaluating the task of pattern discov-ery. The next steps would be to incorporate better timbralsimilarity measures and inclusion of segment boundariesintotheRLCSalgorithmthateffectivelycombinesthepro-posed variants.AcknowledgmentsThis work is partly supported by the European ResearchCouncil under the European Union’s Seventh FrameworkProgram, as a part of the CompMusic project (ERC grantagreement267583). TheauthorsthankPanditArvindMul-gaonkar for sharing the DVD of Tabla solo recordings.390 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]A. Anantapadmanabhan, A. Bellur, and H. A. Murthy.Modal analysis and transcription of strokes of the mri-dangam using non-negative matrix factorization. InProc. of the 38th IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP),pages181–185, Vancouver, Canada, May 2013.[2]J. J. Aucouturier and F. Pachet. Music similarity mea-sures: What’s the use? InProc. of 3rd InternationalConference on Music Information Retrieval, pages157–163,Paris, France, 2002.[3]J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,M.Davies,andMarkB.Sandler.Atutorialononsetde-tectioninmusicsignals.IEEETransactionsonSpeechandAudio Processing, 13(5):1035–1047, Sept 2005.[4]S. Beronja.The Art of the Indian tabla. Rupa and Co.NewDelhi, 2008.[5]A. Chandola.Music as Speech: An Ethnomusicolin-guisticStudy of India. Navrang, 1988.[6]P. Chordia. Segmentation and recognition of tablastrokes.InProc.ofthe6thInternationalConferenceonMusic Information Retrieval (ISMIR), pages 107–114,London,UK, September 2005.[7]P. Chordia, A. Sastry, and S. Şentürk. Predictivetabla modelling using variable-length markov and hid-den markov models.Journal of New Music Research,40(2):105–118,2011.[8]S. Dutta and H. A. Murthy. A modified rough longestcommon subsequence algorithm for motif spotting inan alapana of carnatic music. InProc. of the 20th Na-tional Conference on Communications (NCC), pages1–6,Kanpur, India, February 2014.[9]O. Gillet and G. Richard. Automatic labelling of tablasignals.InProc.ofthe4thInternationalConferenceonMusicInformationRetrieval(ISMIR),Baltimore,USA,October2003.[10]R. S. Gottlieb.Solo Tabla Drumming of North In-dia: ItsRepertoire,Styles,andPerformancePractices.MotilalBanarsidass Publishers, 1993.[11]X. Huang and L. Deng. An overview of modernspeechrecognition.InN.IndurkhyaandF.J.Damerau,editors,Handbook of Natural Language Processing,Chapman & Hall/CRC Machine Learning & PatternRecognition,pages339–366.ChapmanandHall/CRC,2ndedition, February 2010.[12]D. Hughes. No nonsense: the logic and power ofacoustic-iconic mnemonic systems.British Journal ofEthnomusicology, 9(2):93–120, 2000.[13]J.Kuriakose,J.C.Kumar,P.Sarala,H.A.Murthy,andU.K.Sivaraman.Aksharatranscriptionofmrudangamstrokesincarnaticmusic. InProc.ofthe21stNationalConferenceonCommunication(NCC),Mumbai,India,February2015.[14]H.Lin,H.Wu,andC.Wang.Musicmatchingbasedonroughlongestcommonsubsequence.JournalInforma-tion Science and Engineering,27(1):95–110, 2011.[15]M. Miron. Automatic Detection of Hindustani Talas.Master’s thesis, Music Technology Group, UniversitatPompeu Fabra, 2011.[16]T. Nakano, J. Ogata, M. Goto, and Y. Hiraga. A drumpattern retrieval method by voice percussion. InProc.ofthe5thInternationalConferenceonMusicInforma-tion Retrieval (ISMIR), pages 550–553, October 2004.[17]F.PachetandJ.J.Aucouturier.Improvingtimbresimi-larity: Howhighisthesky.Journalofnegativeresultsin speech and audio sciences,1(1):1–13, 2004.[18]A. D. Patel and J. R. Iversen. Acoustic and perceptualcomparisonofspeechanddrumsoundsinthenorthin-dian tabla tradition: An empirical study of sound sym-bolism. InProc. of the 15th International Congress ofPhoneticSciences(ICPhS),pages925–928,Barcelona,Spain, 2003.[19]R. Smith. An Overview of the Tesseract OCR Engine.InProc.oftheNinthInternationalConferenceonDoc-ument Analysis and Recognition (ICDAR), volume 2,pages 629–633, Washington, DC, USA, 2007.[20]A. Srinivasamurthy, R. Caro, H. Sundar, and X. Serra.Transcriptionandrecognitionofsyllablebasedpercus-sion patterns: The case of Beijing Opera. InProc. ofthe 15th International Society for Music InformationRetrievalConference(ISMIR),pages431–436,Taipei,Taiwan, October 2014.[21]E. Tsunoo, G. Tzanetakis, N. Ono, and S. Sagayama.Beyond timbral statistics: Improving music classifi-cation using percussive patterns and bass lines.IEEETransactions on Audio, Speech, and Language Pro-cessing, 19(4):1003–1014, 2011.[22]R. Typke, F. Wiering, and R. C Veltkamp. A surveyof music information retrieval systems. InProc. of the6thInternationalConferenceonMusicInformationRe-trieval(ISMIR),pages153–160,London,UK,Septem-ber 2005.[23]S. J. Young, G. Evermann, M. J. F. Gales, T. Hain,D.Kershaw,G.Moore,J.Odell,D.Ollason,D.Povey,V.Valtchev,andP.C.Woodland.TheHTKBook,ver-sion 3.4. Cambridge University Engineering Depart-ment, Cambridge, UK, 2006.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 391"
    },
    {
        "title": "Automatic Handwritten Mensural Notation Interpreter: From Manuscript to MIDI Performance.",
        "author": [
            "Yu-Hui Huang",
            "Xuanli Chen",
            "Serafina Beck",
            "David Burn",
            "Luc Van Gool"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418267",
        "url": "https://doi.org/10.5281/zenodo.1418267",
        "ee": "https://zenodo.org/records/1418267/files/HuangCBBG15.pdf",
        "abstract": "This paper presents a novel automatic recognition frame- work for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promis- ing performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of- interest detection, ii) music symbol detection and classifi- cation, and iii) transcription to modern music. In addition to the output in modern notation, our system can gener- ate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources avail- able to non-specialists as well, as they can now access such ancient music in a better understandable form.",
        "zenodo_id": 1418267,
        "dblp_key": "conf/ismir/HuangCBBG15",
        "keywords": [
            "hand-written mensural music",
            "novel automatic recognition framework",
            "scanned manuscript",
            "modern music scores",
            "promising performance",
            "complete pipeline",
            "region-of-interest detection",
            "music symbol detection and classification",
            "transcription to modern music",
            "MIDI file generation"
        ],
        "content": "AUTOMATIC HANDWRITTEN MENSURAL NOTATION INTERPRETER:FROM MANUSCRIPT TO MIDI PERFORMANCEYu-Hui Huang⇧?, Xuanli Chen⇧?, Seraﬁna Beck†, David Burn†, and Luc Van Gool⇧⌥⇧ESAT-PSI, iMinds, KU Leuven†Department of Musicology, KU Leuven⌥D-ITET, ETH Z¨urich{yu-hui.huang, xuanli.chen, luc.vangool}@esat.kuleuven.be,{serafina.beck, david.burn}@art.kuleuven.be?equal contributionABSTRACTThis paper presents a novel automatic recognition frame-work for hand-written mensural music. It takes a scannedmanuscript as input and yields as output modern musicscores. Compared to the previous mensural Optical MusicRecognition (OMR) systems, ours shows not only promis-ing performance in music recognition, but also works asa complete pipeline which integrates both recognition andtranscription.There are three main parts in this pipeline: i) region-of-interest detection, ii) music symbol detection and classiﬁ-cation, and iii) transcription to modern music. In additionto the output in modern notation, our system can gener-ate a MIDI ﬁle as well. It provides an easy platform forthe musicologists to analyze old manuscripts. Moreover,it renders these valuable cultural heritage resources avail-able to non-specialists as well, as they can now access suchancient music in a better understandable form.1. INTRODUCTIONCultural heritage has become an important issue nowadays.In the recent decades, old manuscripts and books have beendigitalized around the world. As more and more librariesare carrying out digitalization projects, the number of manu-scripts increases exponentially every day. The texts in thesemanuscripts can be further processed using Optical Char-acter Recognition (OCR) techniques while the music notescan be processed by Optical Music Recognition (OMR)techniques. However, due to the nature of the manuscript,the challenges of OMR and OCR have to be addresseddifferently. For example, OMR has to deal with differ-ent types of notations from different time periods, suchas Chant notation used throughout the medieval and theRenaissance periods while white mensural notation usedduring the Renaissance. Even within the same period, mu-sic symbols vary in different geographical areas [13]. Inc\u0000Yu-Hui Huang⇧?, Xuanli Chen⇧?, Seraﬁna Beck†,David Burn†, and Luc Van Gool⇧⌥.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Yu-Hui Huang⇧?, Xuanli Chen⇧?,Seraﬁna Beck†, David Burn†, and Luc Van Gool⇧⌥. “Automatic Hand-written Mensural Notation Interpreter: from Manuscript to MIDI Per-formance”, 16th International Society for Music Information RetrievalConference, 2015.\nFigure 1: The overview of our framework. (a) Originalimage after ROI selection. (b) After preprocessing. (c)Symbol segmentation. (d) Transcription results.addition to the semantic characteristic, OMR has the addi-tional problem as OCR of having to cope with the physicalcondition of historical documents [15].While several OMR systems exist for ancient musicscores in white mensural notation, most of them targetat printed scores. To name a few, Aruspix [3] is anopen source OMR software targeting those ancient printedscores; Pugin et al. utilized the Hidden Markov Models torecognize the music symbols and to incorporate the pitchinformation simultaneously. A comparative study made byPugin et al. [13] shows that Aruspix has better performanceon selected printed books than Gamut [11], which is an-other OMR software based on the Gamera [9] open-sourcedocument analysis framework. Gamut ﬁrst segments thesymbols based on the result after staff lines removal, andclassiﬁes it using kNN classiﬁer.Calvo-Zaragoza et al. [5] proposed an OMR systemwithout removing the staff lines. They utilized histogramanalysis to segment the staves as well as different mu-sic symbols, and classiﬁed by cross-correlating templates.Their method achieves averagely an extraction rate of 96%79Figure 2: A regular case happens at the end of each voice:due to a lack of space, the writer extends the staff linesa little bit (red dashed box) and squeezes the remainingsymbols on.on the Archivo de la Catedral de Malaga collection whichhas a certain printing style.In addition to the physical condition of the manuscripts,the substantial difference in style between writers ren-ders OMR challenge. One and the same symbol can ap-pear quite differently, depending on the writer. More-over, the symbols sometimes are written too close to eachother which increases the difﬁculty of symbol segmenta-tion. This usually happens at the end of each voice as thewriter wants to ﬁnish on the same line instead of addinga new one. In such cases, they usually elongate the stafflines manually in order to add more symbols, see e.g. Fig-ure 2. Such cases increase the difﬁculty to apply OMR onthese handwritten manuscripts in a systematic and consis-tent manner.Similar to Gamut, we remove staff lines to detect thesymbols, but differently, we employ the Fisher Vector [12]representation to describe images and Support Vector Ma-chines (SVM) to classify them. With relatively less train-ing data compared to others, our OMR system is able torecognize the symbols from different writers with high ac-curacy.In contrast to the modern music (the music from theso-called Common practice period), the music notation upto the Renaissance is much different in appearance. There-fore, transcription from an expert is required to further pro-cess the data. Our goal therefore was to design and im-plement a system that automatically transcribes such mu-sic for users who lack the expert knowledge about theseearly manuscripts. In particular, our system is able to au-tomatically transcribe most of contents in mensural mu-sic pieces as shown in Figure 1. We propose a new OMRsystem which not only recognizes the handwritten musicscores but also transcribes it from white mensural notationto the modern notation. The modern notation is then en-coded into MIDI ﬁles. The overall pipeline is describedin Figure 3. In addition to provide a user friendly plat-form for the musicologists to analyze the music from oldmanuscripts, our system renders these valuable culturalheritage resources to non-specialists as well. Compared tomost OMR system, the playable MIDI ﬁles in our systemhelp people without any music knowledge access those an-cient music.The remaining of this paper is structured as the follow-ings. Section 2 describes the image preprocessing steps. InSection 3, we introduce the core part of the OMR system,music symbol recognition. The transcription to modernnotation is explained in Section 4. Experimental settings\nFigure 3: The overall scheme of our framework.and results are shown in Section 5. Section 6 concludesthe paper.2. PREPROCESSINGFollowing typical OMR pipelines, we start from a prepro-cessing step. In consists of two parts, namely binarizationand stave detection.2.1 BinarizationIn some collections of manuscripts, each scanned im-age comes up with a color check and a ruler aside themain manuscript. In order to achieve a good quality, thenon-music parts need to be removed during the binariza-tion. Given a high resolution scanned image of a musicmanuscript, the boundaries of the page are ﬁrst detectedby histogram analysis of pixel intensity in gray-scaled im-age. Thresholds are set to the horizontal and vertical his-tograms to segment the x- or y-axis into two parts: thepage part containing the staves and the background partstogether with the color check and the ruler. Because theRegion of Interest (ROI) refers to the page part here, whichcontains much higher intensity of grey values compared tothe black background. Based on this fact, with properlychosen threshold, ROI could be well selected. The result isshown in Figure 1a.For those manuscripts containing colored initials ordecorations, we apply K-means clustering under the Labcolor space in order to ﬁlter out some colored non-musicelements after cropping out the color check and the ruler.In the experiments we put K to the value 2, and success-fully cluster the manuscript into two groups, the elements80 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015with red color and the others containing the stave. We se-lect the red group to build a mask to remove those non-music regions from the manuscript. After that, we thenapply Otsu threshold to do the binarization. For simplicity,we will focus on a speciﬁc style, generating to other stylesof manuscripts will be considered in the future work. Fig-ure 1b shows an example result after these preprocessingsteps applied.2.2 Stave detection and staff lines removalThe stave in mensural notation are mostly composed of ﬁvelines. Based on this assumption, we use the stave detectionprogram from [16]. Timofte et al. utilized dynamic pro-gramming to retrieve the patterns of ﬁve lines in order todetect the stave. While detecting the staff lines, the param-eters of staff line thickness and space between two stafflines are optimized at the same time. Figure 1b shows theresult after staff removal.3. SEGMENTATION AND CLASSIFICATIONWith the preprocessing steps of the previous section hav-ing been completed, we obtain binarized images withoutstaff lines. In this section, we ﬁrst describe how the sym-bols are segmented and then how the classiﬁcation of theindividual, segmented symbols works.3.1 SegmentationGiven a binarized image without staff lines (Figure 4a), weemploy the connected component analysis to separate dif-ferent symbols. However, the symbols touching the staffline in the original manuscript may become separate afterstaff removal. As the Figure 4b shows, a semibreve or aminim may be separated into two parts.To solve this prob-lem, we set up several heuristic rules to combine the partsof such broken symbols. For example, we observe thatsome overlapping or close neighbouring boxes detectedwith similar width could be merged into one individualsymbol. Therefore we merge neighbouring boxes in thiscase. Yet, this procedure might be risky in that two closeparts coming from different symbols may get erroneouslymerged as well. To tackle that, we set up a width thresholdfor merging boxes, i.e. if the box width is more than twotimes of the space between two staff lines, the two boxeswill not be merged. The ﬁnal result is shown in Figure 4c.Moreover, in order to distinguish the lyrics from the mu-sic symbols, we use the stave region detected from the pre-vious section as a mask to ﬁlter out those non-music sym-bols.3.2 ClassiﬁcationIn order to train the classiﬁer, we manually annotate theimage of each music symbol by drawing the bounding boxaround it using the image annotation tool [4] from the orig-inal manuscript. Because the bounding boxes may differfrom each other in size, for each cropped symbolI, we\n(a)\n(b)\n(c)Figure 4: (a) After staff removal, some symbols becomeseparated because some strokes touching the staff line areremoved as well. (b) The result of applying connectedcomponent analysis on Figure 4a. (c) The result after ap-plying heuristic rules to combine the broken symbols fromFigure 4b.ﬁrst normalize the image to the same height, which is de-termined by ensuring enough SIFT [10] features could beextracted to form the Fisher Vector representation [12].For training, we use a Gaussian Mixture Model (GMM)withK= 128components as the generative model forthe patch descriptors. To estimate the parameters of theGMM, we obtain 10000 sample descriptors by applyingdense SIFT in all the images from the training data, andreduce them fromD= 128-d toD= 64-d using PCA.Then, the mean, variance and weight of each Gaussiancomponent are estimated using the Expectation Maximiza-tion algorithm. The ﬁnal ﬁsher vector for an imageIhasdimension of2KD= 16384. This vector is then signed-squared-rooted andl2normalized. We follow the proce-dures in [7] to obtain the improved ﬁsher vector, that isafter pooling all the vectors of the training data, we applythe square rooting again to normalize. With bunch of vec-tors from training data, we train the classiﬁer using linearSVM. We train the multi-class classiﬁer with one versusone strategy and select the class with the highest probabil-ity.During testing, we already obtained the bounding boxinformation of each music symbol following the previoussteps. To avoid the effect caused by binarization and staffremoval, we extract the symbol again directly from theoriginal colored image using the same coordinates givenby the bounding box. Hereby, we would like to remindthat the preprocessing steps are for symbol segmentation,while both training and testing patches are extracted fromthe original manuscript. Each segmented symbol is de-scribed in Fisher Vector representation in a similar way aswe did for the training data. Then we use the trained multi-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 81class classiﬁer to predict its class.In our implementation, we used the VLFeat library [17]for the Fisher Vector and SIFT implementations, and lib-svm [6] with linear kernel and default settings for the Sup-port Vector Machine.3.3 Pitch detection and channel separationThe pitch information is essential for transcription, and thepitch level is determined by the relative position of the noteand the clef to the stave. After the music symbol is ex-tracted and classiﬁed, we follow the post-processing stepsdescribed in [14] to retrieve the pitch level.We divide the group of notes into two groups accord-ing to their stems. For the group of notes with stems, weperform histogram analysis to extract the y position of thestem, so that this can be used to localize the center of thenote head. The detail of the histogram analysis is as fol-lowing: we ﬁrst project all the horizontal pixels onto they-axis and then set up a threshold to separate the stem andthe note head, by employing the fact that note head part hashigher intensity of pixels than the stem. For the group ofnone-stem notes, we simply compute the middle point, de-parting from the highest and the lowest points of the sym-bol.For clefs, the point of relevance is much easier to locate,since they can only be situated on staff lines. We simplydetermine the middle point of two squares from clef c andof the two dots or blobs from the right part of clef f, whilewe locate the center of the blob for clef g. For key sig-natures, we only encounter the case of ﬂat, as the sharp israre in the dataset we use. We adopt a similar strategy tothat of the notes to locate the center of the blob for the ﬂatsymbol. With the relative position of the extracted sym-bol calculated, we connect this information to the staff lineposition in order to determine the pitch level of the corre-sponding symbol.In the case of choirbooks, there are always severalvoices within one page of a manuscript in our dataset.Thus, in order to transcribe the music correctly, we needto recognize these different voices. As each voice endswith barlines, we use this as a criterion to separate differ-ent voices. After a barline is detected, we switch the notesdetected afterwards to another channel.4. TRANSCRIPTIONWe aim at transcribing mensural music scores into mod-ern notations. This will render the music accessible to afar larger group of people, also because much of this mu-sic has not even been published. The tool is also valuablefor musicologists, because it takes over the time consum-ing manual transcription work. Instead, they can spendtheir time on the actual music analysis. With the vast dig-ital manuscript collections of libraries that are being madeavailable daily, the transcription tool makes it a lot eas-ier to establish concordances. Also, printed and often notpublished transcriptions are sometimes hard to get by, sothis tool means generally a big improvement of accessi-bility of transcriptions. Moreover, with all the availablesoftware libraries nowadays, such asmusic21[8] which isalso used in our work, MIDI ﬁles could be generated di-rectly from modern notation scores. Therefore the mensu-ral script could be more easily accessed by general public.4.1 Transcription rulesThere are several difﬁculties in transcribing mensural mu-sic. Apart from notational challenges like ligatures andcoloration, which are not supported yet, the main chal-lenge of mensural music transcription is how to translatethe mensuration, or time signature. In contrast to modernmusic, the time signature deﬁnes not only how long onemeasure is, but also deﬁnes how to divide a certain note.There are four kinds of notes that can be divided in dif-ferent ways. The division of maxima into longa is calledmodus maximarumormodus maior. From longa intobreves, it is calledmodus. From breves into semibreves,it is calledtempus. And from semibreves into minims, itis calledprolatio. For all of these four divisions, depend-ing on whether they areperfector not, either aternaryorbinarydivision is possible. If a note is divided in aper-fect, i.e.ternaryway, it will be divided into three sub-class notes. If one note however is divided in animperfect,i.e.binaryway, it will be divided into two sub-class notes.For example, in a case ofperfectum, a longa will be di-vided into three breves, while in a case ofimperfectum, themodus speciﬁes that one longa has to consist of two breves.This rule also applies to the other three transcription pairs.The temporal length of one breve in mensural musicdeﬁnes the length of one measure in modern music. Inthe normal case (i.e. without scaling of temporal length),the length of a semiminim equals that of a modern quarternote. Because a semiminim cannot be affected by the rulesofperfect / imperfectfor its division into its sub-class fusa(i.e. a quaver in modern notation), we are able to calcu-late the actual length of a breve, by treating semiminimsas a unit. For instance, iftempusandprolatio(which referto the semibreve-minim division and breve-semibreve, re-spectively), are bothperfect, then one breve will be dividedinto three semibreves, and each semibreve will in turn bedivided into three minims. As a result, one breve is dividedinto nine semiminims. If we treat one semiminim as a beat,then the corresponding time signature would be 9/4.In addition, there is a variant version of mensurationsymbols, these are the time signatures with a vertical linethrough the original symbol, usually calledcut-signs. Theyimply a reduction of all the temporal values, of notes andrests, by a factor of two. In other words, withcut-sign, theplaying speed of the music will be twice as faster. Notethat most mensural music is rather slow compared to con-temporary music. Beside thecut-signs, we also provide aparameter to artiﬁcially scale the speed of playing. In orderto achieve that, we only need to change the mapping rela-tionship between the mensural notes and the modern ones.For instance, in no-scaling cases, a semiminim is mappedto a quarter. If we speed up the music by two times, wejust need to map semiminim to a quaver, which is half the82 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015length of a quarter. In this case, one should adapt the timesignature accordingly.4.2 Implementation detailsGiven the aforementioned observations,the analysed men-sural music can be encoded into modern music. In ourpipeline, we ﬁrst check the mensuration of the musicpiece. Taking into consideration that the mensurationmight change at any time during the piece, this step shouldbe repeated any time during the process. If there havebeen any changes, we apply the reduction ratio to the mu-sic afterwards. After this, we can determine the map-ping relation between the semiminim and modern musicnotes and calculate the modern time signature accordingto the duration of one breve in the transcription. With de-termined time signatures and basic mapping relationshipsestablished, we can transcribe each element into modernmusical notation, note by note and rest by rest. If the di-vision is only binary or imperfect, we can directly tran-scribe the mensural music to modern music. We are stillworking on the transcription techniques for the perfect di-visions, which include a lot more exceptions that can stillpresent challenges. Once musical symbol recognition areready, all we need to do is to carefully encode these sym-bols. One should be especially aware of the possibilitythat clefs and/or time signatures change in the middle ofa piece. For this step, we chose the framework offered bymusic21[8] to encode the music information, because itoffers an automatic parsing library and APIs towards vi-sualization and MIDI output. The different voices in theoriginal music sheet are encoded into different ’part ob-jects’ in this framework, while the whole piece is treatedas a ’stream’ object. Another thing that needs to be takencare of is thepunctus divisionissometimes appearing inaperfectdivision, which looks exactly like a normal dotwith the function of prolonging note values, but instead ofprolonging, thepunctus divisionisfunctions as a kind ofbarline. Whether or not we are dealing with this kind ofdot, should be established from the note durations directlypreceding and following it.5. EXPERIMENTAL RESULTS5.1 Dataset and evaluationWe evaluate our pipeline on the Alamire collection whichincludes manuscripts of various writers in several books.Depending on the sources, those manuscripts are in highresolution from 7200x5400 to 10500x7400 pixels. Fortraining, we randomly select the manuscripts from thefollowing books:Vienna,¨Osterreichische Nationalbiblio-thek (VienNB), MS Mus. 15495, 15497, 15941, 18746;Brussels, Koninklijke Bibliotheek (BrusBR) Ms. 228, andIV .922[1]. We use the image annotation tool made byKl¨aser [4] to manually draw the bounding box around eachsymbol and to annotate the corresponding information. Intotal we have about 2800 samples for training over 33classes. The classes include the notes, rests, key signatureBookMunBS F LonBLR MS 72AN839 1313 1636Rext85.73% 94.36% 90.25%Table 1: Symbol extraction result on three books.(ﬂat), most of the frequent time signatures and other sym-bols such as barlines and custos. The testing data comesfrom different books, without any overlap with the trainingdata:Munich, Bayerische Staatsbibliothek, Mus. MS. F(MunBS F) [2]; London, British Library MS Royal 8G.vii(LonBLR), and ’s-Hertogenbosch, Archief van de IllustreLieve Vrouwe Broederschap, MS. 72A (MS 72A)[1]. In to-tal, there are about 3700 samples for testing. In our evalua-tion, we report the result of classiﬁcation and segmentationseparately.5.2 Symbol segmentationWe follow the evaluation process in [5]. The extraction rateis deﬁned asRext=MeT, whereMeis the number of mu-sic symbols extracted andTis the total number of musicsymbols within the manuscript. Table 1 shows the symbolsegmentation results on three collections whereNis thetotal number of symbols per book. Most false negatives ofdetection come from custodies, as they are often over seg-mented into several parts after staff removal. Some of theother false negatives come from the symbols on the sixthstaff line, below or above the stave, causing the symbolsabove or below the stave not correctly extracted. More-over, the ornate capitals in front of the piece may distractthe detection especially on the MunBS F collection. Un-like the colored initials in LonBLR, the black initial makesthe separation of symbols more difﬁcult. These issues arebeing solved and will be addressed in the future work.5.3 Symbol classiﬁcationTo evaluate the classiﬁcation step, we ﬁrst correct the seg-mentation errors from the last step as Figure 1c shows, andthen use prediction accuracy to evaluate the classiﬁcation.Table 2 presents the classiﬁcation result on the same col-lections. The accuracy reaches 98 % on the LonBLR andthe MS 72A collections, and 95 % on the MunBS F col-lection. After analysis, we found the typical error for theMS 72A collection is the misclassiﬁcation of a breve restas a colored breve. In MunBS F, most of the classiﬁcationerrors are from the semibreve notes which are mistakenlyclassiﬁed as points. Some incidents are caused by similarsymbols, such as the note fusa recognized as semiminimand the note maxima classiﬁed as longa. The reason mightbe found in the imbalanced training samples in our train-ing set. As some symbols do not happen appear so oftensuch as the note maxima and time signatures, they are lesspresent in the set. It makes the training collection morechallenging if one wants to avoid this issue.With limited training data, the use of the Fisher Vec-tors and SVMs yields a promising classiﬁcation perfor-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 83BookMunBS F LonBLR MS 72AAccuracy95.52% 98.83% 98.94%Table 2: Classiﬁcation result on three books.mance on handwritten symbols from different writers. Asthe manually annotated training data is hard to obtain, ourmethod shows an obvious advantage compared to earlieralternatives.6. CONCLUSIONIn this paper, we presented a framework to automati-cally analyse and transcribe handwritten mensural musicmanuscripts. The inclusion of the transcription part notonly provides the musicologists with a simple platformto more efﬁciently study those manuscripts, but also as-sists music amateurs to explore and enjoy this ancient mu-sic. Moreover, the MIDI-output feature offers the publicat large easy and convenient access to these musical trea-sures.We have collected a dataset of handwritten mensural no-tation symbols from different books for evaluation. We be-lieve it is fair to claim that our symbol segmentation attainsgood performance. The classiﬁcation based on the FisherVector representation and SVMs achieves very high clas-siﬁcation rate on handwritten symbols. Furthermore, weimplemented an accurate transcription mechanism whichembeds musicological information.We plan to extend this work by enabling counterpointchecking so that mistakes in original music manuscriptscan be pointed out to the musicologists easily. In addition,we intend to implement scribe identiﬁcation in our system(an early module for that is ready) to assist authorship iden-tiﬁcation.7. ACKNOWLEDGMENTSWe are grateful to Alamire Foundation for their supportand we would like to thank Lieselotte Bijnens, LonneMaris, Karen Schets and Tim Van Thuyne for their helpon symbol annotations. The work is funded by the Flem-ish IWT/SBO project: New Perspectives on Polyphony,Alamire’s musical legacy through high-technology re-search tools.8. REFERENCES[1]IDEM. http://elise.arts.kuleuven.be/alamire/. Ac-cessed: 2015-04-23.[2]Munich, Bayerische Staatsbibliothek,Handschriften-Inkunabelsammlung, Musica MSF. http://www.digitale-sammlungen.de/. Accessed:2015-04-23.[3]Aruspix project. http://www.aruspix.net/, 2008. Ac-cessed: 2015-04-23.[4]Image annotation tool with bounding boxes.http://lear.inrialpes.fr/people/klaeser/software, 2010.Accessed: 2015-04-23.[5]J. Calvo-Zaragoza, I. Barbancho, L. J. Tardn, andA. M. Barbancho. Avoiding staff removal stage in opti-cal music recognition: application to scores written inwhite mensural notation.Pattern Analysis and Appli-cations, pages 1433–7541, 2014.[6]C.-C. Chang and C.-J. Lin. LIBSVM: A li-brary for support vector machines.ACM Trans-actions on Intelligent Systems and Technol-ogy, 2:27:1–27:27, 2011. Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.[7]K. Chatﬁeld, V . Lempitsky, A. Vedaldi, and A. Zisser-man. The devil is in the details: an evaluation of recentfeature encoding methods. InBritish Machine VisionConference, 2011.[8]M. S. Cuthbert and C. Ariza. music21: A toolkit forcomputer-aided musicology andsymbolic music data.InProceedings of the ISMIR 2010 Conference, 2010.[9]M. Droettboom, G. S. Chouhury, and T. Anderson. Us-ing the gamera framework for the recognition of cul-tural heritage materials. InJoint Conference on Digi-tal Libraries : Association for Computing Machinery,2002.[10]D. G. Lowe. Distinctive image features from scale-invariant keypoints.International Journal of ComputerVision, 2004.[11]K. MacMillan, M. Droettboom, and I. Fujinaga. Gam-era: Optical music recognition in a new shell. InPro-ceedings of the International Computer Music Confer-ence, 2002.[12]F. Perronnin, J. S´anchez, and T. Mensink. Improvingthe ﬁsher kernel for large-scale image classiﬁcation. InProceedings of the 11th European Conference on Com-puter Vision, 2010.[13]L. Pugin and T. Crawford. Evaluating omr on the earlymusic online collection. InProceedings of the ISMIR2013 Conference, 2013.[14]L. Pugin, J. Hockman, J. A. Burgoyne, and I. Fujinaga.Gamera versus aruspix – two optical music recognitionapproaches. InProceedings of the ISMIR 2008 Confer-ence, 2008.[15]C. Ramirez and J. Ohya. Symbol classﬁcation ap-proach for omr of square notation manuscripts. InPro-ceedings of the ISMIR 2010 Conference, 2010.[16]R. Timofte and L. Van Gool. Automatic stave discov-ery for musical facsimiles. InAsian Conference onComputer Vision, 2012.84 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[17]A. Vedaldi and B. Fulkerson. VLFeat: An openand portable library of computer vision algorithms.http://www.vlfeat.org/, 2008.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 85"
    },
    {
        "title": "Four Timely Insights on Automatic Chord Estimation.",
        "author": [
            "Eric J. Humphrey",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417549",
        "url": "https://doi.org/10.5281/zenodo.1417549",
        "ee": "https://zenodo.org/records/1417549/files/HumphreyB15.pdf",
        "abstract": "Automatic chord estimation (ACE) is a hallmark re- search topic in content-based music informatics, but like many other tasks, system performance appears to be con- verging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite ar- guably achieving some of the highest results to date, both approaches plateau well short of having solved the prob- lem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that inval- idate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, stan- dard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conven- tional approaches conflate the competing goals of recogni- tion and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjec- tive, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper of- fers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large.",
        "zenodo_id": 1417549,
        "dblp_key": "conf/ismir/HumphreyB15",
        "keywords": [
            "Automatic chord estimation",
            "glass ceiling",
            "machine perception domains",
            "large-vocabulary ACE",
            "performance plateau",
            "music recordings",
            "harmony and tonality",
            "standard lexicons",
            "natural relationships",
            "conventional approaches"
        ],
        "content": "FOUR TIMELY INSIGHTS ON AUTOMATIC CHORD ESTIMATIONEric J. Humphrey1,2and Juan P. Bello11Music and Audio Research Laboratory, New York University2MuseAmi, Inc.ABSTRACTAutomatic chord estimation (ACE) is a hallmark re-search topic in content-based music informatics, but likemany other tasks, system performance appears to be con-verging to yet another glass ceiling. Looking toward trendsin other machine perception domains, one might concludethat complex, data-driven methods have the potential tosigniﬁcantly advance the state of the art. Two recent effortsdid exactly this for large-vocabulary ACE, but despite ar-guably achieving some of the highest results to date, bothapproaches plateau well short of having solved the prob-lem. Therefore, this work explores the behavior of thesetwo high performing, systems as a means of understandingobstacles and limitations in chord estimation, arriving atfour critical observations: one, music recordings that inval-idate tacit assumptions about harmony and tonality resultin erroneous and even misleading performance; two, stan-dard lexicons and comparison methods struggle to reﬂectthe natural relationships between chords; three, conven-tional approaches conﬂate the competing goals of recogni-tion and transcription to some undeﬁned degree; and four,the perception of chords in real music can be highly subjec-tive, making the very notion of “ground truth” annotationstenuous. Synthesizing these observations, this paper of-fers possible remedies going forward, and concludes withsome perspectives on the future of both ACE research andthe ﬁeld at large.1. INTRODUCTIONAmong the various subtopics in content-based music in-formatics, automatic chord estimation (ACE) has maturedinto a classic MIR challenge, receiving healthy attentionfrom the research community for the better part of twodecades. Complementing our natural sense of academicintrigue, the general music learning public places a highdemand on chord-based representations of popular mu-sic, as evidenced by large online communities surround-ing websites like e-chords1or Ultimate Guitar2. Given⇤Please direct correspondence toeric@museami.com1http://www.e-chords.com2http://www.ultimate-guitar.comc\u0000Eric J. Humphrey, Juan P. Bello.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Eric J. Humphrey, Juan P. Bello. “FourTimely Insights on Automatic Chord Estimation”, 16th International So-ciety for Music Information Retrieval Conference, 2015.the prerequisite skill necessary to manually identify chordsfrom recorded audio, there is considerable motivation todevelop automated systems capable of reliably performingthis task.The goal of ACE research is —or, at least, has been—to develop systems that produce “good” time-aligned se-quence of chords from a given music signal. Supplementedby efforts in data curation [2], syntax standardization [8],and evaluation [13], the bulk of chord estimation researchhas concentrated on building better systems, mostly con-verging to a common architecture [4]: ﬁrst, harmonic fea-tures, referred to as pitch class proﬁles (PCP) orchroma,are extracted from short-time observations of the audio sig-nal [7]; these features may then be processed by any num-ber of means, referred to in the literature aspre-ﬁltering;next,pattern matchingis performed independently overobservations to measure the similarity between the signaland a set of pre-deﬁned chord classes, yielding a time-varying likelihood; and ﬁnally,post-ﬁlteringis applied tothis chord class posterior, resulting in a sequence of chordlabels over time.However, despite continued efforts to develop bet-ter features [11], more powerful classiﬁers [10], or ad-vanced post-ﬁltering methods [1], performance appears tobe tapering off, as evidenced by recent years’ results atMIReX3. Thus, while other areas of machine perception,such as computer vision and speech recognition, are ableto leverage modern advances in machine learning with re-markable success, two recent efforts in large vocabularyACE were only able to realize modest improvements bycomparison [3, 9]. Acknowledging this situation begs anobvious question: why is automatic chord estimation dif-ferent, and what might be done about it? Through an in-vestigation of system behaviour and detailed error analysis,the remainder of this paper is an effort to shed some lighton the problem.2. RESEARCH METHODOLOGY2.1 Automatic SystemsGiven its long history, there are ample potential automaticchord estimation systems that could be considered in thisinquiry. Here, though, we choose to focus our investiga-tion on two recent, data-driven, large vocabulary systemsfor which we are able to obtain software implementations,3http://www.music-ir.org/mirex/wiki/MIREX\\_HOME673providing control over training and choice of chord vocab-ulary. Additionally, these system architectures are quitedifferent and should, as a result, yield different machineperspectives, a strategy that has proven useful in the anal-ysis of beat tracking systems [15].2.1.1 K-stream GMM-HMM with Multiband ChromaThe ﬁrst system considered is a modern, high-performingGMM/HMM chord estimation system [3], referred to hereas “kHMM.” A multiband chroma representation is com-puted from beat-synchronous audio analysis, producingfour parallel feature representations. Each is modeled bya separate multivariate Gaussian Mixture Model (GMM),whereby all chroma vectors and chord labels are rotated toaCroot. During inference, four separate observation like-lihoods over all chord classes are obtained by circularlyrotating the feature vector the GMM, thereby making themodel transposition invariant. These four chord class pos-teriors are then decoded jointly using a k-stream HMM,resulting in a single beat-aligned chord sequence.2.1.2 Deep Convolutional Neural NetworkAcknowledging the recent widespread success of deeplearning methods, a deep convolutional network is alsoconsidered [9], referred to as “DNN.” Time-frequencypatches of local contrast normalized constant-Q spectra, onthe order of one second, are transformed by a four-layerconvolutional network. Finding inspiration in the root-invariance strategy of GMM training, explicit weight-tyingis achieved at the classiﬁer across roots such that all qual-ities develop the same internal representations, allowingthe model to generalize to chords unseen during training.Following the lead of deep network research in automaticspeech recognition, likelihood scaling is performed aftertraining to control class bias resulting from the severe im-balance in the distribution of chords. Finally, chord poste-riors are decoded via the Viterbi algorithm [5].2.2 EvaluationExpressed formally, the modern approach to scoring anACE system is a weighted measure of chord-symbol re-call,RW, between a reference,R, and estimated,E, chordsequence as a continuous integral over time, summed overa discrete collection ofNannotation pairs:RW=1SN\u00001Xn=0ZTnt=0C(Rn(t),En(t))dt(1)Here,Cis a chordcomparisonfunction, bounded on[0,1],tis time,nthe index of the track in a collection,Tnthe du-ration of thenthtrack. This total is normalized by thesup-port,S, corresponding to the cumulative amount of timeover which the comparison rule is deﬁned forR, given bythe indicator function in a similar integral:S=N\u00001Xn=0ZTnt=01Rn(t)dt(2)Deﬁning the normalization termSseparately is usefulwhen comparing chord names, as it relaxes the assumptionthat the comparison function is deﬁned everywhere. Fur-thermore, setting the comparison function as a free vari-able allows for ﬂexible evaluation of a system’s outputs,and thus the focus on vocabulary can largely focus on thechoice of comparison function,C. The work presentedhere leveragesmireval, an open source evaluation tool-box providing a set of seven chord comparison functions,characterizing different relationships between chords [14].2.3 Reference Annotations2.3.1 Ground Truth DataThe ﬁrst major effort to curate reference chord annotations,now part of the larger Isophonics4dataset, covers the en-tire 180-song discography ofThe Beatles, as well as 20songs fromQueen, 14 from Carole King, and 18 fromZweieck; due to content access, only the 200 songs fromThe BeatlesandQueenare used here. Two other largechord annotation datasets were publicly released in 2011,offering a more diverse musical palette. The McGillBill-boarddataset consists of over 1000 annotations, of whichmore than 700 have been made public. This project em-ployed a rigorous sampling and annotation process, se-lecting songs from Billboard magazine’s “Hot 100” chartsspanning more than three decades. The other, providedby the Music and Audio Research Lab (MARL) at NYU5,consists of 295 chord annotations performed by undergrad-uate music students; 195 tracks are drawn from the USPopdataset6, and 100 from the RWC-Pop collection7, in thehopes that leveraging common MIR datasets might facili-tate access within the community. In all three cases, chordannotations are provided as “ground truth,” on the premisethat the annotations represent the gold standard.2.3.2 The Rock CorpusImportantly, the reference chord annotations discussed pre-viously offer a singular perspective, either as the output ofone person or the result of a review process. TheRockCorpus, on the other hand, is a set of 200 popular rocktracks with time-aligned chord and melody transcriptionsperformed by two expert musicians [6]: one, a pianist, andthe other, a guitarist, referred to as DT and TdC, respec-tively. This collection of chord transcriptions has seen littleuse in the ACE literature, as its initial release lacked timingdata for the transcriptions. A subsequent release resolvedthis issue, however, and doubled the size of the collection.While previous efforts have sought to better understand therole of subjectivity in chord annotations [12], this datasetprovides an opportunity to explore the behavior of ACEsystems as a function of multiple reference transcriptionsat a larger scale.4http://isophonics.net/content/reference-annotations5https://github.com/tmc323/Chord-Annotations6http://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html7https://staff.aist.go.jp/m.goto/RWC-MDB/rwc-mdb-p.html674 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Ref–DNN Ref–kHMM kHMM–DNNroot0.789 0.808 0.840thirds0.757 0.775 0.815majmin0.759 0.776 0.798mirex0.769 0.783 0.806triads0.705 0.721 0.783sevenths0.620 0.645 0.691tetrads0.567 0.588 0.678v1570.649 0.659 0.678Table 1. Weighted recall across comparison rules betweenthe ground truth references and both models, respectively,as well as against each other.3. LARGE-VOCABULARY CHORD ESTIMATIONHere we investigate large-vocabulary chord estimation asa basis for experimentation. First and foremost, it presentsa particularly challenging problem, and therefore offersa good deal of potential for subsequent analysis. Largechord vocabularies also avoid the inherent noise intro-duced by approximately mapping chords into the clas-sic major-minor formulation, e.g.A:sus2!A:majorC:dim7!C:min. Additionally, the large amount ofavailable data should be sufﬁcient for learning a large num-ber of chord classes.Before proceeding, the ground truth collections aremerged for training and evaluation, totaling 1235 tracks. Atotal of 18 redundant songs are identiﬁed via the EchoN-est Analyze API8and removed to avoid potential datacontamination during cross validation. All but one isdropped for each collision, preferring content from Iso-phonics, Billboard, and MARL, respectively, resulting ina ﬁnal count of 1217 unique tracks.To ensure a fair comparison between algorithms, theground truth data is partitioned into ﬁve distinct splits.Training is repeated ﬁve times for both systems addressedin Section 2.1 for cross validation, such that each split isused as a holdout test set once. Both models adopt thesame chord vocabulary, comprised of the thirteen most fre-quent chord qualities in all twelve pitch classes, as well asa no-chord class, for a total of 157 chord classes, consistentwith previous efforts [3]. Chords outside this strict vocab-ulary are ignored during training, rather than mapped totheir nearest class approximation. The Rock Corpus datais not used for training, and saved exclusively for analysis.3.1 Experimental ResultsWeighted recall is averaged over the ﬁve test splits arefor all reference chord labels according to the sevenmirevalcomparison rules, shown in Table 1. At ﬁrstglance, the overall statistics seem to indicate that the twosystems are roughly equivalent, with “kHMM” outper-forming “DNN” by a small margin. The automatic systemsperform best at root-level recall, and performance drops asthe comparison rules encompass more chords. Notably, acomparison of algorithmic estimations, given in the thirdcolumn, shows that these two systems do indeed offer very8http://developer.echonest.com/docs/v4DT–TdC (DT|TdC)–DNN (DT|TdC)–kHMMroot0.932 0.792 0.835thirds0.903 0.750 0.785majmin0.905 0.723 0.766mirex0.902 0.737 0.776triads0.898 0.719 0.760sevenths0.842 0.542 0.595tetrads0.835 0.540 0.590v1570.838 0.539 0.590Table 2. Weighted recall across comparison rules for thetwo human annotators, and the better match of each againstthe two automatic systems.different perspectives. Therefore, it will be valuable to notonly investigate where the estimated chord sequences dif-fer from the reference, but also how these estimated se-quences differ from each other.Similarly, weighted recall is also given for both systemsover the Rock Corpus in Table 2. It is an open questionas to how an estimated annotation might best be comparedagainst more than one human reference. For the purposesof analysis, the best matching reference-estimation pair ischosen at the track level and used to compute the weightedaverage. Still, performance on the Rock Corpus is lowerfor both automatic algorithms. This is likely a result of amismatch in chord vocabulary, as space of chords used inthe Rock Corpus is a smaller subset than the 157 estimatedby automatic systems. Additionally, it is curious to observea non-negligible degree of disagreement between the twohuman perspectives, with more than a15%discrepancy inthe tetrads condition. That said, the human annotators doagree a deal more that is attained by either system, indicat-ing that there is likely room for improvement.3.2 Track-wise VisualizationsWhile weighted recall gives a good overall measure of sys-tem performance, we are particularly interested in devel-oping a more nuanced understanding of how these systemsbehave. To this end, system performance is now examinedat the track-level, as real music is often highly self-similarand the chords within a song with be strongly related.Errors and other kinds of noteworthy behavior should bewell-localized as a result, making it easier to draw conclu-sions from the data.Two track-wise scatter plots are given in Figure 1, forthe ground truth and Rock Corpus datasets. The for-mer compares the agreement between multipleestima-tions, alongx, with the better matching estimation for thegiven reference, alongy, where each quadrant character-izes a different behavior: (I), all annotations agree; (II),one estimation matches the reference better than the other;(III), all annotations disagree; and (IV), the estimationsagree more with each other than the reference. Impor-tantly, this track-wise comparison makes it easier to iden-tify datapoints that can help address our original researchquestions. Tracks for which only one algorithm performswell (II) likely indicate boundary chords. Alternatively, in-stances where both algorithms produce poor estimations,Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 675Figure 1. Trackwise recall for the “tetrads” in two condi-tions: (top) over the ground truth data, illustratingmodelagreement versus the better match between the referenceand estimated annotations; (bottom) over the Rock Cor-pus data, illustrating annotator agreement versus the bettermatch between the two reference and kHMM annotations.and yetneitheragree (III), are curious and warrant furtherinspection. Finally, tracks that result in similarly incorrectestimations (IV) highlight some kind of greater challengeto automatic systems.The second plot, conversely, compares the agreementbetween multiplereferences, alongx, with the bettermatching reference for the given estimation, alongy, andanalogous characterizations by quadrant: (I), all annota-tions agree; (II), one reference matches the estimation bet-ter than the other; (III), all annotations disagree; and (IV),the references agree more with each other than the esti-mation. Here, annotator disagreement in the presence of amatching estimation (II) is indicative of subjectivity, whiledisagreement between all annotations (III) is suspiciousand should be explored. Furthermore, tracks with an es-timated annotation that fails to match either human per-spective (III & IV) likely identify room for improvement.4. QUALITATIVE ANALYSIS, IN FOUR PARTSUsing this suite of analysis tools described previously, athorough exploration of the relationship between referenceand estimated annotations is conducted, resulting in foursigniﬁcant insights. In the spirit of both reproducibility andopen access, a companion IPython notebook is made avail-able online9, providing additional visualizations comple-mentary to the following discussion.4.1 Invalid Harmonic AssumptionsAn exploration of quadrant (IV) from Figure 1 reveals thata large source of error stems from musical content or ref-erence chord annotations that violate basic assumptionsabout how chords are used. One common form of this be-havior is due to issues of intonation, where a handful ofrecordings are not tuned to A440, with some varying bymore than a quarter-tone: for example, “Stand By Me” byJimmy Rufﬁn, “I’ll Tumble 4 Ya” byThe Culture Club,“Every Breath You Take” byThe Police, or “Nowhere toRun” by Martha Reeves andthe Vandellas. Understand-ably, as a result, the estimated annotations differ by a semi-tone from the reference, and perform poorly across allcomparison rules.The second observation ﬁnds that some tracks in thedataset do not truly make use of, and are thus not well de-scribed by, chords. While a few classic songs byThe Bea-tleshave been known to be of questionable relevance fortheir instrumentation and lack of standard chords, such as“Revolution 9,” “Love You To,” or “Within You, WithoutYou”, analysis here identiﬁes several other tracks, span-ning rap, hip hop, reggae, funk and disco, that behavesimilarly: for example, “Brass Monkey” byThe BeastieBoys, “I, Me, & Myself” byde la Soul, “Don’t Push” bySublime, “Get Up (I Feel Like Being a Sex Machine)” byJames Brown, or “I Wanna Take You Higher” by Tina andIke Turner. This realization encourages the conclusion thatchords may not be a valid way to describe all kinds of mu-sic, and that using such songs for evaluation may lead toerroneous or misleading results.4.2 Limitations of Chord ComparisonsThe second observation resulting from this analysis is thedifﬁculty faced in the comparison of related chords. Byand large, ACE systems are often forced to either mapchords to a ﬁnite dictionary, or develop embedding rulesfor equivalence testing [14]. In either case, this quantiza-tion process assigns all observations to a one-of-Krepre-sentation effectively making all errors equivalent. For thepurposes of stable evaluation, this can have signiﬁcantlynegative consequences.9https://github.com/ejhumphrey/ace-lessons/experiments.ipynb676 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015E:7/3A:min/b3E:9E:7/3A:min/b3\nFigure 2. Six perspectives on “I Saw Her Standing There”, byThe Beatles, according to Isophonics (Iso), Billboard (BB),David Temperley (DT), Trevor deClercq (TdC), the Deep Neural Network (DNN), and the k-stream HMM (kHMM).Chords are naturally related to each other hierarchically,and cannot always be treated as distinct classes. Flat clas-siﬁcation problems —i.e. those in which different classesare disjoint— are built on the assumption of mutually ex-clusive relationships. In other words, assignment to oneclass precludes the valid assignment to any other class con-sidered. In the space of chords,C:dim7andC:majareperhaps mutually exclusive classes, but it is difﬁcult to saythe same ofC:maj7andC:maj, as the formercontainsthe latter. This conﬂict is a common source of disagree-ment between annotators of the Rock Corpus tracks, whichare easily identiﬁed in or near the quadrant (II) of Figure 1-b: for example, “Dancing In The Street” by Martha Reeves& The Vandellas, “All Apologies” byNirvana, or “Papa’sGot a Brand New Bag” by James Brown. In each case,the human perspectives each report related tetrads and tri-ads, e.g.E:7andE:maj, causing low annotator agree-ment, while the machine estimation alternates between thetwo trying to represent both. These kinds of errors are not“confusions” in the classic sense, but a limitation of evalu-ation methods to reliably quantify this behavior, and of themodel to represent this naturally structured output.4.3 Conﬂicting Problem DeﬁnitionsOver the years, the automatic prediction of chord se-quences from music audio has taken several names: es-timation, recognition, identiﬁcation, or transcription. Theanalysis here motivates the notion that this is not merelya matter of semantics, but actually a subtle distinctionindicative of two slightly different problems being ad-dressed. Chordtranscriptionis an abstract task relatedto functional analysis, taking into consideration high-levelconcepts such as long term musical structure, repetition,segmentation or key. Chordrecognition, on the other hand,is quite literal, and is closely related to polyphonic pitchdetection. Both interpretations are easily found in the col-lection of reference annotations, however, conﬂating thesetwo tasks to some unknown degree.Furthermore, the goal in transcription is to assign chordlabels to regions, and is closer in principle to segmentationthan classic approaches to chord estimation. One illustra-tive instance, “All Apologies” byNirvana, is identiﬁed inquadrant (II) of Figure 1. Here, the human annotators havedisagreed on the harmonic spelling of the entire verse, withDT and TdC reportingC#:majandC#:7, respectively.On closer inspection, it would appear that both annotatorsare in some sense correct; the majority of the verse is ar-guablyC#:maj, but a cello sustains the ﬂat-7thof this keyintermittently. The regions in which this occurs are clearlycaptured in the estimated annotations, corresponding to itsC#:7predictions. This proves to be an interesting discrep-ancy, because one annotator (DT) is using long-term struc-tural information about the song to apply a single chord tothe entire verse.4.4 Ground Truthvs.SubjectivityWhile the role that subjectivity can play in chord esti-mation is becoming better understood [12], it is not han-dled gracefully in current ACE methodology, and thereare two examples worth analyzing here. The ﬁrst, “I SawHer Standing There” byThe Beatles, is given in Figure2, where the pitch class of the chord’s root is mapped tocolor hue, and the darkness is a function of chord quality,e.g., allE:*chords are a shade of blue. No-chords are al-ways black, and chords that do not ﬁt into one of the 157chord classes are shown in gray. Perhaps the most strik-ing observation is the degree of variance between all an-notations. Based on the tetrads comparison, no two refer-ence annotations correspond to greater than a 65% agree-ment, with the DNN and kHMM scoring 28% and 52%Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 677Ver.Chord Sequence Score Ratings ViewsBillboardD:maj A:sus4(b7) B:min7 G:maj9—— —MARLD:maj D:maj/5 D:maj6/6 D:maj(4)/4—— —DTD:maj A:maj B:min G:maj—— —TdCD:maj A:maj B:min G:maj—— —DNND:maj A:sus4 B:min7 G:maj7—— —kHMMD:maj A:sus4 B:min G:maj—— —1D:maj A:maj B:min G:maj4/5 193 1,985,8782D:5 A:sus4 B:min7 G:maj5/5 11 184,6113⇤D:maj A:maj B:min G:maj4/5 23 188,1524⇤D:maj A:maj B:min G:maj74/5 14 84,8255⇤D:maj A:maj B:min G:maj5/5 248 338,2226D:5 A:5 D:5/B G:55/5 5 16,208Table 3. Various interpretations of the verse from “With or Without You” byU2, comparing the reference annotationsand automatic estimations with six interpretations from a popular guitar tablature website; a raised asterisk indicates thetranscription is given relative to a capo, and transposed to the actual key here.against the ground truth Isophonics reference, shown at thetop. Despite this low score, the DNN and kHMM esti-mations agree with at least one of the four human annota-tions 89.1% and 92.3% of the song, respectively. The twoexceptions occur during the out-of-gamut chords,E:7/3andA:min/b3, which the DNN callsAb:hdim7andC:maj6, respectively. While both estimated chords sharethree pitches with the Isophonics reference, the other hu-man annotators mark theA:min/b3instead as a root po-sitionC:maj. Given how subjective it might be for humanexperts to agree on possible inversions, typical evaluationstrategies may place too much emphasis on the root of achord.A second example to consider in the larger discus-sion of subjectivity is the verse of “With or Without You”byU2. Musically, one ﬁnds reasonably ambiguous har-monic content, consisting of a vocal melody, a movingbass line, a guitar riff, and a string pad sustaining a high-pitchedD. Complementing the four expert perspectivesprovided here, an Internet search yields six additional user-generated chord transcriptions from the website UltimateGuitar10. All human perspectives and both machine in-terpretations are consolidated in Table 3, noting both theaverage and number of ratings, as well as the number ofviews the public chord annotation has received. Thoughview count is not directly indicative of a transcription’s ac-curacy, it does provide a weak signal indicating that usersdidnotrate it negatively.This particular example provides several valuable in-sights. Nearly all perspectives are equivalent at the major-minor level, with the exception of the MARL annotation,which differs only slightly. That said, the differences be-tween user-generated annotations do not noticeably impactthe average ratings. This is an important considerationwhen building user systems, whereby objective measuresare valuable insofar as they correlate with subjective ex-perience. Similarly, these annotations are indicative of, atleast for this song, a preference for root position chords.Thus, subjectivity plays a role in the collection of refer-ence annotations, as well as the end-user experience.10http://tabs.ultimate-guitar.com/u/u2/with_or_without_you_crd.htm, accessed 19 April 2015.5. CONCLUSIONS AND FUTURE PERSPECTIVESIn this work, qualitative analysis of system performanceled to the identiﬁcation of four key observations affectingcurrent chord estimation methodology: one, not all musiccontent is valid in the context of chord estimation; two,conventional comparison methods struggle to accuratelycharacterize the complex relationships between chords;three, conventional methodology has mixed the somewhatconﬂicting goals of chord transcription and recognition toan undeﬁned degree; and four, the subjective nature ofchord perception may render objective ground truth andevaluation untenable.Looking to the future of automatic chord estimation, afew opportunities stand out. First and foremost, subjectiv-ity in reference annotations should be embraced rather thanresolved. Chord estimation may be better understood as atime-aligned “tagging” problem, modeled as multinomialregression, or as structured prediction. Furthermore, syn-thesizing multiple human perspectives into a continuous-valued chord afﬁnity vector would allow for more stableevaluation by encoding the degree to which a chord labelapplies to an observation. From a system design perspec-tive, chord transcription, as a disctinct task, stands to bene-ﬁt greatly from recent advances in music structure analysis.To the point, however, it is also crucial to distinguish be-tween the different ﬂavors of harmonic analysis, and howa collection of reference data does —or does not— reﬂectthe speciﬁc problem being addressed.In a more general sense, this inquiry also has implica-tions for the larger ﬁeld of content-based MIR. Perhapsmost presssing, the most powerful model cannot compen-sate for methodological deﬁciencies, and domain knowl-edge can be crucial to help understand system behaviour.Similarly, qualitative evaluation should play a larger rolein the assessment of automatic systems intended for user-facing applications. If nothing else, users studies can helpidentify objective measures that align well with subjec-tive experience. Finally, on a more practical note, high-performing systems can and should be used to facilitate thecuration of reference annotations. These systems can beused to solicit human perspectives at a much larger scale,for both new and previously annotated content.678 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20156. REFERENCES[1]Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent. Audio chord recognition with recur-rent neural networks. InProceedings of the 14th Inter-national Society for Information Retrieval Conference(ISMIR), pages 335–340, 2013.[2]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fuji-naga. An expert ground truth set for audio chord recog-nition and music analysis. InProceedings of the 12thInternational Society for Information Retrieval Confer-ence (ISMIR), pages 633–638, 2011.[3]Taemin Cho.Improved techniques for automatic chordrecognition from music audio signals. PhD thesis, NewYork University, 2014.[4]Taemin Cho and Juan Pablo Bello. On the relativeimportance of individual components of chord recog-nition systems.Audio, Speech, and Language Pro-cessing, IEEE/ACM Transactions on, 22(2):477–492,2014.[5]Taemin Cho, Ron J. Weiss, and Juan Pablo Bello. Ex-ploring common variations in state of the art chordrecognition systems. InProceedings of the Sound andMusic Computing Conference, 2010.[6]Trevor De Clercq and David Temperley. A corpus anal-ysis of rock harmony.Popular Music, 30(01):47–70,2011.[7]Takuya Fujishima. Realtime chord recognition of mu-sical sound: a system using common lisp music. InProceedings of the International Computer Music Con-ference (ICMC), 1999.[8]Christopher Harte, Mark B Sandler, Samer A Abdal-lah, and Emilia G´omez. Symbolic representation ofmusical chords: A proposed syntax for text annota-tions. InProceedings of the 6th International Soci-ety for Information Retrieval Conference (ISMIR), vol-ume 5, pages 66–71, 2005.[9]Eric J. Humphrey.An Exploration of Deep Learning inMusic Informatics. PhD thesis, New York University,2015.[10]Eric J. Humphrey and Juan Pablo Bello. Rethinkingautomatic chord recognition with convolutional neuralnetworks. InProceedings of the International Confer-ence on Machine Learning and Applications, 2012.[11]Meinard M¨uller and Sebastian Ewert. Towards Timbre-Invariant Audio Features for Harmony-Based Music.IEEE Transactions on Audio, Speech and LanguageProcessing, 18(3):649–662, March 2010.[12]Yizhao Ni, Matt McVicar, Raul Santos-Rodriguez,and Tijl De Bie. Understanding effects of subjectiv-ity in measuring chord estimation accuracy.Transac-tions on Audio, Speech, and Language Processing,21(12):2607–2615, 2013.[13]Johan Pauwels and Geoffroy Peeters. Evaluating auto-matically estimated chord sequences. InInternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 749–753. IEEE, 2013.[14]Colin Raffel, Brian McFee, Eric J Humphrey, JustinSalamon, Oriol Nieto, Dawen Liang, and Daniel PWEllis. mireval: A transparent implementation of com-mon MIR metrics. InProceedings of the 15th Inter-national Society for Information Retrieval Conference,2014.[15]Jos´e R Zapata, Andr´e Holzapfel, Matthew EP Davies,Jo˜ao Lobato Oliveira, and Fabien Gouyon. Assigninga conﬁdence threshold on automatic beat annotation inlarge datasets. InProceedings of the 13th InternationalSociety for Information Retrieval Conference, pages157–162, 2012.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 679"
    },
    {
        "title": "In Their Own Words: Using Text Analysis to Identify Musicologists&apos; Attitudes towards Technology.",
        "author": [
            "Charles Inskip",
            "Frans Wiering"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416422",
        "url": "https://doi.org/10.5281/zenodo.1416422",
        "ee": "https://zenodo.org/records/1416422/files/InskipW15.pdf",
        "abstract": "A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and pro- vides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point ap- proach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that ef- forts should be made into supporting development of their digital skills and providing usable, useful and relia- ble software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level.",
        "zenodo_id": 1416422,
        "dblp_key": "conf/ismir/InskipW15",
        "keywords": [
            "online survey",
            "quantitative data",
            "qualitative data",
            "musicologists",
            "digital humanities",
            "technology use",
            "research practices",
            "resource format",
            "digital skills",
            "comments on rewards"
        ],
        "content": "IN THEIR OWN WORDS: USING TEXT ANALYSIS TO IDENTIFY MUSICOLOGISTS' ATTITUDES TOWARDS TECHNOLOGY  Charles Inskip Frans Wiering Department of Information Studies,  University College London c.inskip@ucl.ac.uk Department of Information and Computing Sciences,  Universiteit Utrecht f.wiering@uu.nl ABSTRACT A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and pro-vides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point ap-proach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that ef-forts should be made into supporting development of their digital skills and providing usable, useful and relia-ble software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level. 1. INTRODUCTION  In the last two decades, an astonishing amount of com-puter technologies have been created for the processing of digitized music and music metadata. Quite a few of these are targeted at musicological research. Very often, such software, standards, services or resources are t h e  outcome of interdisciplinary collaborations between computer scientists, audio engineers, musicologists and/or library scientists. An ever-present subtext in the discourse around these collaborations is the potential of technology to transform the discipline of musicology. Yet the uptake of these technologies in mainstream musicolo-gy is not widespread. As a first step in a timely systemat-ic exploration of the area, this paper presents the results of a questionnaire amongst musicologists worldwide, fo-cussing on the use or non-use of technology resources in their daily work processes. Gathering insights into t h e  aims and values of the researchers is an important step towards creating a ‘musicology-centred’ design practice that is founded on human-centred design methods [1]. The key characteristic of such methods is to focus on human work practices and bottlenecks, and then to select or develop the technologies that remove these bottlenecks while respecting the aims and values of the humans in-volved. Whereas human-centred approaches to systems design are increasingly used in digital humanities, t h e y  have been rarely applied to digital musicology. The use of modern technology in the digital humani-ties has been widely explored in the last ten years [2-9]. Existing research has identified domain-specific differ-ences between humanities and scientific researchers i n  their information behaviours. These appear to be predom-inantly influenced by the analogue or digitised surrogate nature of the research objects in humanities, and the prac-tices of humanities researchers, which are frequently around lone research. Research indicates that humanists welcome technology when it speeds up workflow [8-9], rely on informal peer networks, primarily access mono-graphs, libraries and private collections, search by brows-ing and citation chasing, and use exploratory search strat-egies [2]. The core issue underlying technology adoption is thus not so much technophobia as the acceptability  and relevance of technology as part of the research process. This work sets out to explore the adoption of software tools by musicologists in their digital scholarship practic-es (“the ability to participate in emerging academic, pro-fessional and research practices that depend on digital systems” [10]). These tools, which allow the interrogation of digital musical artefacts (including music notation, digital audio, or contextual texts such as metadata) have been widely reported on and refined through the annual ISMIR conference. However it appears that there is some disconnect between this research strand and musicologi-cal users’ needs and requirements [11-14]. Although some efforts are made to consider user information needs and behaviours [15-19], these are outweighed by a sys-tems-centred approach to the development of new tools [19]. This may reflect the findings that developers deter-mine the success of their efforts more by the performance of the tool than its uptake by users [5, 6]. However, in the words of Borgman [20]: “until analytical tools and ser-vices are more sophisticated, robust, transparent, and easy to use for the motivated humanities researcher, it will be difficult to attract a broad base of interest within the humanities community.”  Although, for example, the AHRC-funded Transform-ing Musicology project [21] attempts to encourage closer collaboration between musicologists, computer scientists and software developers, only a few MIR projects seem to be based on an understanding of the work processes and related technology needs of musicologists [22-24]. Building on recent studies into the adoption of tools and resources by humanists [3, 4, 25], this research presents a large-scale investigation of the digital scholarship prac-tices of musicologists. The results will hopefully contrib- © Charles Inskip, Frans Wiering. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Charles Inskip & Frans Wiering “In their own words: using text analysis to identify musicologists' attitudes towards technology”, 16th International Society for Music Information Retrieval Conference, 2015. \n455   ute to the development of usable systems which reflect work practices and attitudes of this community. 2. METHODOLOGY We created an online survey named ‘What Do Musicolo-gists Do All Day’ (WDMDAD). With this survey we wanted to gather data on the research musicologists d o ,  how they use (or don’t use) technology in their research, and how they assess positive and negative aspects of technology. Our main purpose was to collect rich and de-tailed stories in their responses, which we did by means of open-ended questions, contextualized within demo-graphic data. We were seeking to explore behaviours and attitudes by encouraging the participants to communicate their experiences more freely than in a multiple choice survey. Our emphasis on rewards, frustrations, risks, limitations and benefits was drawn from a desire to en -courage constructive responses of both a positive and negative nature, and enable us to build on previous work in digital humanities, particularly [4]. Though the q u e s -tions are in English, we encouraged the participants to use their own language if they felt more comfortable this way. The questions are shown in Table 1. The survey ru-bric and questions were carefully designed to encourage musicologists with a broad range of digital skills and ex-perience to contribute to the survey. Responses are anon-ymous. All participants gave informed consent in the use of the data they provided, following ethical guidelines of the researchers’ institutions. The survey was published online using the Opinio sys-tem. After the final question, participants were linked to a Google Form, where they were given the option to leave contact details if they wished to be informed about t h e  results or participate in follow-up research. These person-al data were not linked to the survey responses, maintain-ing the researchers’ commitment to anonymity of the par-ticipants. The link to the survey was posted on various musicological mailing lists (including AMS, IAML (c. 700 subscribers), ICTM, SMT, musicology-all and sever-al national lists). To stimulate wide international partici-pation a mailing was sent to all (c. 700) members of the International Musicological Society and the Society f o r  Interdisciplinary Music Studies (c. 70 members). Invita-tions to participate were circulated by national societies or lists in Australia, Austria, France, Germany, Nether-lands (c. 200 members) and other countries. WDMDAD was mentioned a few times on social media. It is not known whether all participants are ‘genuinely’ musicolo-gists, but from reviewing the responses it is clear t h a t  they self-identified as such. It is also possible that partici-pation was skewed once the survey link was released ‘in-to the wild’. Responses were collected from 4 December 2014 until 6 March 2015. Initially, there were some tech-nical issues in showing the link to the Google Form, mainly for IOS devices, resolved after a few days. As a consequence, some participants submitted responses mul-tiple times. Duplicate responses were removed, as were responses that didn’t get beyond the first page (Q1-4). There was only one fake response. Reponses in languages other than English were translated by native speakers in collaboration with the research team who were able to provide explanatory context. The cleaned dataset re-sponses were analysed identifying themes and patterns in the data, using a combination of Excel, SPSS and Nvi-vo10.  Question Response Q1: What is your gender? male / female / prefer not to say Q2: What is your age? choose one of 6 categories Q3: Please identify your location from this list pick country from list Q4: What is your level of education? bachelor / mas-ters / PhD / other (specify) Q5: How confident would you say you are using digital systems and materials to find, organise and analyse research materials, and create and disseminate your findings?  5-point Likert scale (low-high) Q6: Where do you do your musicology re-search? (you can choose more than one, if you like) select from 4 cat-egories, if ‘oth-er’, specify Q7: What is your speciality? (you can choose more than one, if you like) select from 11 categories, if ‘other’, specify Q8: What are you currently researching? Text Q9: Which is the information or music re-source you use most in your musicology research and writing? choose one of 10 categories, if ‘other’, specify Q10: Which [Q9] do you use, why? text Q11: If you think you may have a prefer-ence for using digital or physical resources in your work, why do you think this is? text Q12: Tell a story about a rewarding or a frustrating experience (or both, if you like) with technology in your music research. text Q13: What do you think are the risks and limitations of the use of technology in mu-sicology research? text Q14: What do you think are the benefits of using technology in musicology research? text Table 1. Survey questions The full texts were imported into NVivo10 for analy-sis. After automated removal of stop-words, the remain-ing terms were ranked by frequency. Recognising the im-portance of frequency in terms of identifying vocabular-ies and enabling comparisons between texts, recent work in applied linguistics [26] has found some value in apply-ing the Hirsch index (h-index) [27] citation measure ap-proach to text analysis. The percentage of appearance of key terms is generally around the 1-2% level, which i s  not unusual in this type of work. Most words only appear once. The h-point (where term rank = term frequency) provides a threshold whereby important thematic words (autosemantics) lying above this point are considered to be more significant than those below the h-point. Here, as stop words (synsemantics) had previously been removed from the texts, this approach enabled the identification of high-ranking autosemantics which were more likely to be related to the theme of the text [26] and was preferable to 456 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   arbitrarily choosing the ‘top 10/15/20’ terms as it also en-abled comparison between texts. Visualisations of the concordances of the terms in the pre-h dom ain were ex-amined to provide insights into their context. This p r o -cess was repeated for each autosemantic term in the pre-h domain for each text (rewards, benefits, risks, limitations, frustrations). There were between 7,300 and 13,000 words in each of these corpora, each containing between 1,500 and 2,400 unique terms. 3. FINDINGS 3.1 Demographics  The data presented here focus on those aspects that a r e  relevant to the analysis presented in this paper. The total number of usable responses was 621, coming from 46 different countries. A large majority of survey partici-pants were from two continents: Europe (306) and North America (248). Around two thirds of the respondents (385) were from English-speaking countries. Responses span all career phases, with the highest rep-resentation of the 30-39 age group (Figure 1). Females (314) and males (294) participated in almost equal num-bers (13 prefer not to say). The respondents’ level of edu-cation is high, with ‘PhD /Doctorate’ (449) and ‘Masters’ (129) as the largest categories. The two most important locations for doing research are ‘Academic institution’ (493) and ‘Library, archive or museum’ (197).  Digital skills Count 1 2 2 18 3 132 4 256 5 213 Table 2. S e l f - e v a l u a t e d  l e v e l  o f  d i g i t a l  s k i l l s  ( 1 = l o w ,  5=high; mean=4.06, n=621) Respondents assess their digital skills quite highly (Table 2) but there are considerable age differences (Fig-ure 1). Although anecdotally there is a tendency for digi-tal skills to decrease with age, more than half of the 70+ respondents rate their digital skills (DS) at 4 or 5. \n Figure 1. A g e  g r o u p  a n d  d i g i t a l  s k i l l s  o f  p a r t i c i p a n t s  (n=621)   3.2 Preferred type of resource Respondents were asked to choose one type of preferred resource from a list (Table 3). Although some were reluc-tant to make a choice, overall 319 respondents prefer dig-ital resources, 271 prefer physical resources. Musical re-sources, whether audio or notation, are preferred by only 43 respondents. However, the responses to Q10 show that a considerable part of the archival and manuscript collec-tions are actually researched for their musical content. Resource Count Digital books and journals 193 Physical books and journals 188 Digitised archives and manuscript collections 104 Physical archives and manuscript collections 62 Other resource 31 Music audio on computer, phone, mobile device 15 Music audio on tape, record, CD 12 Physical collection of music editions 9 Digital collection of music editions 4 Online music audio collection 3 Table 3. Preferred resource (n=621) It can be seen in Figure 2 that there appears to be a  correlation between the preferred format and the level of digital skills, participants with digital skills 3 (DS3) pre-ferring physical resources, while those with 5 (DS5) lean towards digital resources.  \n Figure 2. Preferred information resource by digital skills (n=571) The participants were given the option to choose more than one speciality subject. The majority selected histori-cal musicology. The representations in Figure 3 provide some insights into the self-evaluated digital skills across speciality. While computational and systematic musicol-ogy shows a higher coverage of DS4 and DS5, perfor-mance practice, historical and library / archive / museum research and other areas of study show a higher propor-tion of DS1-3. 3.3 Rewards For Q12, an h-point of 23 was identified. Terms from the pre-h d o m a i n  a r e  e m b o l d e n e d  h e r e a f t e r .  ( R e s p o n d e n t  code in parentheses.) Access, here, is used in relation not only to access to the researchers’ own materials “almost wherever I am” (091) but more widely  to digitized pri-mary and secondary sources s u c h  a s  “databases, online journals, digitised books, scores” (168), “newspaper ar-chives” (201), “quality recordings” (221) and “high-quality color images” (557). This access allows engage-ment of a high quality: “It really makes me feel I could be \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 457   \nin a library in Italy” (270) a n d it is  n ot un us u al to f in d this being evaluated favourably in terms of time-saved. Further deep analysis through close reading of the texts of the concordances around these key autosemantics high-lighted the importance to the participants of using t e c h -nology to save time and increase the speed of their work-flow: “…now I can see them all in one afternoon” (022) and minimize the need for travel to engage with a wide range of primary and secondary sources. Images of manuscripts, scores and digital books are considered to be particularly useful, while favourable mentions of li-brary c a t a l o g u e s ,  digital a r c h i v e s ,  s c h o l a r l y  d a t a b a s e s  and various types of software (Sonic Visualiser, Audaci-ty, image manipulation) also feature widely in these texts: “I cannot think what I would be able to do without this software!” (592).  3.3 Benefits The process was repeated, examining the texts describing the participants views on the benefits of technology (Q14). The pre-h d o m a i n  (h=35) vocabulary featured some similar terms to those in the ‘rewards’ texts, but in-cluded a richer, less concentrated use of terms, reflected by the higher h v a l u e .  T h i s  i n d i c a t e s  t h e r e  i s  a  w i d e r  range of issues than in the ‘rewards’ texts. Once again, access was considered to be an important term in the vo-cabulary. It creates the “potential to formulate projects or research questions hitherto unthinkable” (003), saving time and money, reducing the need for travel to visit ar-chives and improving efficiency, enabling researchers to engage with up-to-date resources o r  materials (in the physical form as manuscripts or other paper-based docu-ments, or as recordings) located globally which would otherwise be out of reach because of distance, cost, or the fragility of unique items. Downsides are recognized: “it can be really time consuming to separate the wheat from the chaff” (313) a n d  “excess of information, lack of a methodology for analyzing recorded sound” (203). It is not only materials that are accessible: “… people, music, documents, can be accessed around the world” (336). This accessibility enables the collection and analysis of data“in a way which would not be possible for a human being” (021) which may lead to “…more accurate find-ings, as many things can be really 'counted', not the gut feeling that musicologists in the past had” (039). Gather-ing, organising, pro-cessing, manipulating and analyzing data are key benefits for some members of this com-munity: “modern tech-nology provides new re-search opportunities, it helps to work in a time-saving way and it makes communication easier and faster.” (286). The ability to share research data, ideas and findings more easily is also highlighted (“whether it be in formal 'journal' form or informal such as facebook, email, o r  texting” (249).  3.4 Risks For the texts relating to ‘risks’ the h-point was 20. The recurring theme of access h e r e  ( Q 1 3 )  f o c u s e s  o n  h o w  “the vast majority of resources have not been digitized” (65) an d  t h e r i s k  o f  l os s  of  k n ow l e d g e  ( t h r o u g h  l a c k  of comprehensive digitized collections, or closed subscrip-tions) and loss of artefacts (through failure of or d e v e l -opments in technology). It is suggested that “immediacy of access to a wide range of material encourages a rapid-ity of response and decision” (015) w h i c h  m a y  l e a d  t o  more superficial research and there are fears that physical objects may even “be overlooked” (319) leading to “priv-ileging digital sources” (188). Some of the views on ac-cess link to those on availability. Excessive amounts of available resources may lead to “complacency and over-confidence” (052), “an incomplete and imbalanced pic-ture” (186) or “laziness” (numerous). There is evidence of strong feelings in these texts that the wide availability of digitized resources may mean that “musicology will be too superficial and lose authority as a serious contribu-tion to society” (604) and that by focusing on electronic journals rather than books this  m ay  lead to “apparently clever new historicist readings that are in fact shallow.” (424). Th is  lin k s  to a s tron g  v iew  th at techn olog ical de-terminism is a problem: “Technology … cannot replace using the grey stuff between the ears” (003). While con-cerns about the risks of losing or corrupting insufficiently preserved or stored data a p p e a r ,  t h e r e  i s  a  f e a r  t h a t  t h e  problem in concentrating on the interpretation of large datasets may be “that is not feasible to listen through and analyze. It disincentivizes selective recording” (312) and “need[s] to be done with extreme care” (100). The ten-sion between digital m a t e r i a l s  a n d  t h e  materiality o f  physical sources an d resources rein f orces  this  apparen t fear of superficiality and, particularly, incompleteness of research “[s]ome things cannot be gleaned from digitized copies only” (090). For some, digital materials are not to be trusted because of the “seduction” and “temptation” of their (inherent) “shallowness”. This is not the only view: “From my informatics-biased standpoint, the use of digi-tal technology in music research is a clear net-positive as a way to augment and enhance traditional musicological approaches” (410). Figure 3. Percentage digital skills per speciality (n=1395) 458 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   3.5 Limitations The ‘limitations’ texts h-point was 22. The fears around materiality a r e  e c h o e d  i n  t h e  c o m m e n t s  o n  l i m i t a t i o n s  (Q13), partly because increased access t o  t h e  digital manifestation of information objects can be seen to lead to decreased availability of the physical item, and t h o s e which have not yet been digitized are also considered to be unavailable. Costly subscriptions to academic journals (JSTOR is particularly popular) are a concern to unaffili-ated researchers and those within academia alike (as sub-scriptions may be limited to on-site access): “digitization thus increasingly creates a dichotomy of researchers” (337) and Open Access is not seen to successfully solve this issue. The requirement to have access to the Internet and competency in the use of technology is also seen as a limitation by some. The use of archives continues to re-flect the concerns around the materiality argument and develops on the theme of comprehensive research prac-tices: “Carl Ludwig's 'Repertorium Organorum' may be hellish to use, but it's still indispensable” (058). The op-portunities for “serendipity” through browsing the physi-cal library are particularly highlighted: “Browsing in the digital realm is a far less productive activity than brows-ing in library stacks” (068) and digital archives “do not always capture the creative process, or iterations, of ma-terials” (420). Search f o r  sources may be incomplete, “missing the surrounding context” (037) and particularly OCR is limited. It seems likely there is a role for librar-ies here in terms of developing the search skills of their users alongside the functionality of their search interfac-es: “I'm never certain that all bases have been covered in a search” (233). When speaking about primary research sources, again the materiality is paramount: “It is much easier to turn a page physically” (341) as well as authori-ty: “Digital materials can be posted by anyone” (492). Although the participants generally seem happy to either read books o n l i n e  o r  f r o m  t h e  s h e l f  ( w i t h  s o m e  s t r o n g  exceptions relating to materiality, eye-strain and the ten-dency to skim electronic materials), they are wary of the problems around e-books’ usability and long term access. 3.6 Frustration Notable in the ’frustrations’ texts (Q12) (h-point=26) was the appearance of software b r a n d s ,  p a r t i c u l a r l y  Finale, Sibelius, Office a n d  Word. These frustrations are im-portant issues when considering the self-assessed digital skills of the participants. Despite most participants de-scribing themselves as being 3 – 5 in digital skills, they are suffering from software (or programmes) being dif-ficult to integrate with the idiosyncrasies of musical re-search practices as well as being time-consuming to learn, unreliable and unnecessarily updated. Although users may be familiar with Linux, LaTeX and Sonic Visualiser, some participants are not working with modern software. More generally, documents here are generated by the researchers and may be unexpectedly reformatted in some way by software, while data can be ‘the bane of my ex-istence’ (198) i n  t e r m s  o f  e n t e r i n g ,  a n d  i s  e a s i l y  l o s t  o r  corrupted if it has not been backed up (an ‘annoyance’ (368)). Books ( e l e c t r o n i c  o r  p h y s i c a l )  a n d  recordings can be difficult to find because library catalogues are not always intuitive, and e-books are difficult to read because library e-reader interfaces are ‘unfriendly’ (081) and ‘dif-ficult to use’ (086). Hardware can create difficulties – computers c a n  b e  ‘very old’, ‘slow’, and can ‘crash’ –  intervention by intermediaries may be required, although these can be unreliable. Although there is an understanding that not all re-sources have been digitized, and that material artefacts are still extremely important as research objects in their own right, there is clearly frustration that online a c c e s s , facilitated by seamless search, is not comprehensive and universal. There are issues around varying levels of online access to digital journals and databases caused by “patchy institutional subscriptions” (212) o r  a s  a n  o u t -come of being in the field or unaffiliated researcher sta-tus. This is compounded by problems with search. With-in e-books or databases there is an expectation that full-text search is readily available (and fully functional) with high precision (“There are a lot of bogus references to items .. that show up in search engines” (301)) and recall (“the database search was not picking up articles/reviews that I knew should be there” (284)) anticipating user con-text: “If one searches for 'organum' in the database 'Aca-demic Search Premier' -- all sorts of medical journal ar-ticles pop up.” (233). Lack of time i s  a  b i g  p r o b l e m  f o r  t h e s e  p a r t i c i p a n t s ,  not to be wasted on “learning software that I don’t end up using” (363). I n f r e q u e n t  u s e  o f  c o m p l e x  s o f t w a r e  i n  research workflows leads to difficulties: “Every time I come back to it, it feels like I have to learn it all over again” (363).  \n Figure 4. Key terms ranked by percentage use 4. DISCUSSION On examination of the various pre h-point vocabulary analyses discussed above, it is clear that while the partic-ipants are enthusiastic about the rewards and benefits of the use of technology in their research, they have strong reservations around the risks and limitations of these technologies, which are often realized through frustra-tions when trying to achieve their research objectives. In particular the issues around access, books and sources, finding a n d  searching a n d  time a r e  c o n s i d e r e d  t o  b e  both positive and negative (Figure 4).  In Figure 5 the use of the key terms is broken up by digital skills of participants: the closer to the centre the line becomes, the less frequently the term is used. T h i s  data is incomplete (n=2 for DL1; n=18 for DL2) and un-\nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 459   likely to be representative (reinforced by close examina-tion of the terms in context) and is not included here. However it is interesting to observe that there appears to be more emphasis on technical terms (software, data) by DL5 while the least frequently used term by DL3 is soft-ware. Libraries are em phasized by  DL3, while sources are ranked lower by DL5 than by their counterparts.  \n Figure 5. Key terms and digital skills ranked by percent-age appearances in texts The general consistency in the ranking of these terms is notable, reinforcing the idea that there is likely to be an agreed vocabulary and common practices within this community. Also, a common set of disciplinary values seem to emerge from the responses, emphasising qualities such as completeness, depth of analysis, accuracy, relia-bility, serendipity and the materiality of resources. It was observed above that musical resources were preferred by only a minority (7%) of the respondents. One possible explanation is that researchers study known musical items and mainly gather information about the music. However, many researchers study the musical content of archives and manuscript collections, and editing music is often their core activity. This relates in an interesting w a y  t o  shortcomings that are observed in music printing software such as Finale and Sibelius, especially for creating schol-arly editions of early music. Also, no tool support is re-ported for managing editorial data. There is clearly a case to be made for the development of systems that support the entire editorial workflow.  In summary, the (self-defined) musicologists who kindly took this survey and provided us with their thoughts clearly have access to technology (they did the survey online) and have positive and negative views (of-ten held simultaneously) about its value in their research process. They may work unaffiliated and alone, or in an office with colleagues, and it is quite likely they are inter-ested in historical or cultural musicology, or popular mu-sic studies. They are really excited about the increased access afforded by digital technologies and resources but some are wary of how digitization may make research superficial, undermining the discipline. They are habitual readers and want context-dependent access to physical and digital artefacts. They use software when it contrib-utes to their workflow, and have a range of levels of digi-tal fluency. Respondents rated their digital skills q u i t e  highly. However, the problems they report with consumer technologies suggest that they often overrated them-selves. Also, there are many signs of insecurity in work-ing with digital resources. Digital methodologies are ap-parently not yet well integrated with mainstream research practice. 5. CONCLUSIONS AND FUTURE WORK It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities and that efforts should be made into supporting the d e -velopment of their digital skills and in providing reliable user-centred software. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level. In the above we have presented an initial analysis of the WDMDAD data, and while the size of the sample al-lows some generalization we recognize that there are likely to be differences amongst sub-disciplines within the population. Further work will examine the data at a more granular level, providing better understanding o f  work practices within sub-disciplines. Resources and software mentioned by participants also merit attention, for example for creating a collection of application sce-narios. Finally, a comparison of the vocabularies used by musicologists and MIR researchers to describe technolo-gy may help to identify areas where misunderstanding may arise or values may clash. After completing this analysis, we will make the data available in a form th at guarantees the anonymity of the participants. Although it is too early to know in detail what musi-cologists do all day, we will use the findings of the WDMDAD survey to guide the next steps in our re-search, which will include in-depth interviews, work with focus groups and co-design of prototype tools in the pur-suit of answering this rather big question. Ultimately, we hope to raise the awareness of the importance of musicol-ogy centred design, and to contribute to the systematic creation of usable software and resources that enhance (and may ultimately transform) musicological research. 6. ACKNOWLEDGEMENTS The authors would like to express their gratitude to the participants who freely gave their valuable contributions, the societies who helped by promoting the survey, and the colleagues who offered guidance on SPSS and trans-lations. Frans Wiering was supported by the FES project COMMIT/ and the AHRC project Transforming Musi-cology. 7. REFERENCES  [1] Benyon, D. (2014). Designing Interactive Systems: A Comprehensive Guide to HCI, UX and Interaction Design. Pearson. [2] Barrett, A. (2005). The information seeking habits of graduate student researchers in the humanities. The Journal of Academic Librarianship 31(4) : 324-331. [3] Bulger, M. & Meyer, E., De la Flor, G., Terras, M., Wyatt, S., Jirotka, M., Eccles, K., Madsen, C. (2011). Reinventing Research? Information Practices in the \n460 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   Humanities. A Research Information Network Re-port, April 2011. doi: 10.2139/ssrn.1859267  [4] Gibbs, F. & Owens, T. (2012). Building Better Digi-tal Humanities Tools: Toward broader audiences and user-centered designs. Digital Humanities Quarterly 6 (2) available online at http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html  [accessed 10 Oct 2014] [5] Schreibman, S. & Hanlon, A. (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. Digital Humanities Online 4  ( 2 )  a v a i l a b l e  o n l i n e  a t  http://digitalhumanities.org/dhq/vol/4/2/000083/000083.html [accessed 10 Oct 2014] [6] Warwick, C., Terras, M., Huntington, P., & Pappa, N. (2008). If you build it will they come? The LAIRAH study Quantifying the Use of Online Re-sources in the Arts and Humanities through Statisti-cal Analysis of User Log Data. Literary and Linguis-tic Computing 23 (1) : 85-102. [7] Warwick, C. (2012). Studying users in digital hu-manities. In: Warwick, C., Terras, M., Nyhan, J. Dig-ital Humanities in Practice. London: Facet Publish-ing, 2012. 1-21 [8] Wiberley, S. & Jones, W. (1989). Patterns of infor-mation seeking in the humanities. College & Re-search Libraries 50 (6) 638 - 645 [9] Lehman, S. & Renfro, P. (1991). Humanists and Electronic Information Services: Acceptance and Re-sistance. College & Research Libraries 52 (5) 409 - 413 [10] JISC (2011) Developing Digital Literacies, available online at http://www.jisc.ac.uk/media/documents/fu-nding/2011/04/Briefingpaper.pdf  [accessed 15 Oct 2014]  [11] Barthet, M. & Dixon, S. (2011). Ethnographic obser-vations of musicologists at the British Library, Proc. of the 12th International Society For Music Infor-mation Retrieval Conference [12] Bonardi, A. (2000). IR for Contemporary Music: What the Musicologists Needs, Proc. of the Interna-tional Symposium on Music Information Retrieval, 2000  [13] Cook, N. (2005). Towards the compleat musicolo-gist? Proc. of the International Symposium on Music Information Retrieval, 2005 [14] Neubarth, K., Bergeron, M. & Conklin, D. (2011). Associations between musicology and music infor-mation retrieval. Proc. of the 12th International So-ciety For Music Information Retrieval Conference [15] Lee, J. (2010). Analysis of user needs and infor-mation features in natural language queries seeking music information. Journal of the American Society for Information Science and Technology, 61(5), 1025-1045. [16] LaPlante, A. (2011). Social capital and music dis-covery: an examination of the ties through which late adolescents discover new music. Proc. of 12th Inter-national Society for Music Information Retrieval Conference. [17] Liew, C. & Ng, S. (2006). Beyond the Notes: A Qualitative Study of the Information-Seeking Behav-ior of Ethnomusicologists. Journal of Academic Li-brarianship, 32 (1) 66-68 [18] Inskip, C., MacFarlane, A., Rafferty, P. (2010). Crea-tive professional users musical relevance criteria. Journal of Information Science 36 (4), 517-529. [19] Weigl, D. & Guastavino, C. (2011). User studies in the music information retrieval literature. Proc. of the 12th International Society For Music Information Retrieval Conference [20] Borgman, C. (2009). The Digital Future is Now: A Call to Action for the Humanities. Digital Humani-ties Quarterly 3  ( 4 )  a v a i l a b l e  o n l i n e  a t  http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html  [accessed 10 Oct 2014] [21] Transforming Musicology (2014). Transforming mu-sicology. Available online at http://www.transforming-musicology.org/  [accessed 13 Oct 2014] [22] European Science Foundation (2012). Musicology (Re)-mapped. Standing Committee for the Humani-ties: Discussion Paper. Available online at http://www.esf.org/fileadmin/Public_documents/Publications/musicology.pdf  [accessed 10 Oct 2014] [23] Wiering, F. (2013). User needs and challenges in digital musicology. Available online at http://www.staff.science.uu.nl/~wieri103/presentations/WieringLondonDigitalMusicLabFinal.pdf  [ac-cessed 10 Oct 2014] [24] Volk, A. & Wiering, F. (2011). Musicology. In Proc. of the Twelfth International Society for Music Infor-mation Retrieval Conference (pp. 1 8). Un iv ers ity  of  Miami. Presentation available online at http://ismir2011.ismir.net/tutorials/ISMIR2011-Tutorial-Musicology.pdf  [accessed 17 Oct 2014] [25] Next History (n.d.). Enhancing Historical Research with Text Analysis Tools. Available online at  http://nexthistory.org/ [accessed 10 Oct 2014] [26] Popescu, I.-I. (2011). Text ranking by the weight of highly frequent words, in Grzybek, P. (Ed.) & Köh-ler, R. (Ed.) (2011). Exact Methods in the Study of Language and Text. B e r l i n ,  B o s t o n :  D e  G r u y t e r  Mouton. [27] Hirsch, J. E. (2005). An index to quantify an individ-ual's scientific research output. Proceedings of the National Academy of Sciences of the United States of America, 102(46), 16569-16572. doi:10.1073/pnas.0507655102 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 461"
    },
    {
        "title": "Automatic Transcription of Ornamented Irish Traditional Flute Music Using Hidden Markov Models.",
        "author": [
            "Peter Jancovic",
            "Münevver Köküer",
            "Wrena Baptiste"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415116",
        "url": "https://doi.org/10.5281/zenodo.1415116",
        "ee": "https://zenodo.org/records/1415116/files/JancovicKB15.pdf",
        "abstract": "This paper presents an automatic system for note tran- scription of Irish traditional flute music containing orna- mentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hid- den Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cep- stral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evalu- ations are performed on monophonic flute recordings from Grey Larsen’s CD. The performance of the system is eval- uated in terms of the transcription of notes as well as detec- tion of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset de- tection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency.",
        "zenodo_id": 1415116,
        "dblp_key": "conf/ismir/JancovicKB15",
        "keywords": [
            "automatic system",
            "note transcription",
            "Irish traditional flute music",
            "ornamentation",
            "hidden Markov models",
            "mel-frequency cepstral coefficients",
            "signal energy",
            "fundamental frequency",
            "F-measure",
            "onset detection"
        ],
        "content": "AUTOMATIC TRANSCRIPTION OF ORNAMENTED IRISHTRADITIONAL FLUTE MUSIC USING HIDDEN MARKOV MODELSPeter Janˇcoviˇc1M¨unevver K¨ok¨uer2,1Wrena Baptiste1,21School of Electronic, Electrical & Systems Engineering, University of Birmingham, UK2School of Digital Media Technology, Birmingham City University, UKp.jancovic@bham.ac.uk, m.kokuer@bham.ac.ukABSTRACTThis paper presents an automatic system for note tran-scription of Irish traditional ﬂute music containing orna-mentation. This is a challenging problem due to the softnature of onsets and short durations of ornaments. Theproposed automatic transcription system is based on hid-den Markov models, with separate models being built fornotes and for single-note ornaments. Mel-frequency cep-stral coefﬁcients are employed to represent the acousticsignal. Different setups of parameters in feature extractionand acoustic modelling are explored. Experimental evalu-ations are performed on monophonic ﬂute recordings fromGrey Larsen’s CD. The performance of the system is eval-uated in terms of the transcription of notes as well as detec-tion of onsets. It is demonstrated that the proposed systemcan achieve a very good note transcription and onset de-tection performance. Over 28% relative improvement interms of theF-measure is achieved for onset detection incomparison to conventional onset detection methods basedon signal energy and fundamental frequency.1. INTRODUCTIONAutomatic transcription of music is concerned with con-verting an acoustic signal into a symbolic representationthat provides the information on individual notes playedand possibly also other higher-level information about thestructure of music. Over the last decade, there has been aconsiderable research interest in this ﬁeld. Although mostof the current research is devoted to polyphonic music tran-scription, transcription of monophonic music is still of in-terest due to existing large amount of real-world mono-phonic music of speciﬁc properties. This paper deals withthe transcription of notes and detection of their onsetsin monophonic ﬂute recordings of Irish traditional musicthat contains ornamentation. Ornamentation is used exten-sively in Irish traditional music by players of all melodyinstruments. Ornaments are notes of a very short duration.c\u0000Peter Janˇcoviˇc, M¨unevver K¨ok¨uer, Wrena Baptiste. Li-censed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Peter Janˇcoviˇc, M¨unevver K¨ok¨uer,Wrena Baptiste. “Automatic transcription of ornamented Irish traditionalﬂute music using hidden Markov models”, 16th International Society forMusic Information Retrieval Conference, 2015.They are central to the style of the performer, adding to theliveliness and expression of the music.A wide range of different approaches for automatic mu-sic transcription have been proposed. A variety of al-gorithms for estimating the fundamental frequency (F0),e.g., [4, 10, 16], were employed in transcription of music,e.g., [1, 5, 10]. As theF0estimation may suffer from mak-ing octave errors, music transcription systems typicallyemploy some way of temporal ﬁltering or post-processing.The use of hidden Markov models (HMMs) for post-processing was presented in several works. In [2, 15] thesequence of pitch salience and onset strength or energy dif-ference of adjacent signal frames were used as the inputfeatures to HMMs. In [6], the acoustic signal was ﬁrst seg-mented by applying an onset detection algorithm and thenHMM was used to track note candidates. Bayesian mod-eling that exploits knowledge of musical sound generationwas proposed in [3, 9] and applied for piano transcription.Recently, several methods based on learning of a model /classiﬁer of notes were presented, e.g., [7, 14]. In [14], asupport vector machine classiﬁer, trained on spectral fea-tures, was used to clasify frame-level note instances andthe classiﬁer output was then temporally smoothed using anote level HMM to perform transcription of piano record-ings. Modelling of a time-frequency representation of au-dio as a sum of basic elements representing the spectrumof a single note was presented in [7]. The transcription ofornamented Irish traditional ﬂute music was investigatedat the level of onset and ornament detection in [8, 11]. Inboth works, the presented ornament detection system wasbased on detecting onsets and using rules of musical or-namentation. An energy-based onset detection algorithmwas employed in [8], while a comparison of two energy-andF0-based onset detection algorithms was performedin [11].In this paper, we investigate an automatic transcriptionof ornamented Irish traditional ﬂute music by employinghidden Markov models (HMMs). The proposed system isbased on building an individual HMM for each note as wellas for each ornament. This enables to model the differ-ences in realisation of ornaments and notes and then detectornaments whose fundamental frequency is close to the or-namented note. Music signal is represented as a sequenceof Mel-frequency cepstral coefﬁcients. Different param-eter setups at various stages of the feature extraction andacoustic modelling are explored. Experimental evaluations756are performed using recordings of Irish traditional tunesplayed by ﬂute from Grey Larsen’s CD [13]. Evaluationsare presented for the task of onset detection and note tran-scription. Results are presented in terms of the precision,recall andF-measure. Onset detection evaluations are alsocompared to energy- andF0-based conventional onset de-tection algorithms. It is demonstrated that the proposedHMM-based transcription system achieves over 28% rela-tive improvement in terms of theF-measure in onset de-tection task over conventional onset detection algorithms.2. ORNAMENTED FLUTE MUSIC2.1 Ornamentation in Irish traditional ﬂute playingOrnaments are used as embellishments in Irish traditionalmusic [13]. They are notes of a very short duration, cre-ated through the use of special ﬁngered articulations. Theycan be split into single- and multi-note ornaments. Single-note ornaments, namely ‘cut’ and ‘strike’, are pitch artic-ulations. The ‘cut’ involves quickly lifting and replacinga ﬁnger from a tonehole, and corresponds to a higher notethan the ornamented note. The ‘strike’ is performed bymomentarily closing an open hole, and corresponds to alower note than the ornamented note. Multi-note orna-ments, namely ‘crann’, ‘roll’ and ‘shake’, are successiveuse of single-note ornaments. A schematic visualisation ofthe single- and multi-note ornaments in the time-frequencyplane is given in Figure 1.Frequency\nTime‘Cut’\n‘Strike’Frequency\nTime‘Shake’‘Roll’‘Crann’\n(a) (b)Figure 1. A schematic representation of single-note (a)and multi-note (b) ornaments in the time-frequency plane.2.2 Annotation of the audio dataThe audio signal was manually annotated by an experi-enced player of Irish traditional ﬂute. The annotation pro-vides segmentation of the audio signal, where each seg-ment is characterised by the following: the time of onset,time of offset, type of segment, note identity (if applica-ble), and note frequency (if applicable). The type of seg-ment may be one of the following: note, one of the types ofsingle-note or multi-note ornaments, and breath. The note00.511.522.53−1−0.8−0.6−0.4−0.200.20.40.60.81\nTime (s)\n020040060080010001200100020003000400050006000\nFrame indexFrequency (Hz)\ncutcutcutcut\nFigure 2. An extract from the tune ‘The Lonesome Jig’,depicting the waveform (top) and spectrogram (bottom).frequency is initially estimated automatically but checkedby the annotator, and, if needed, then corrected manually.Further information on the process of annotation of the au-dio recordings is presented in [12].2.3 Data statisticsThe ﬂute music we are dealing with contains notes in therange from D4 to B5, i.e., with the fundamental frequencyfrom 293 Hz to 987 Hz. Typically, only ﬁrst few harmon-ics of the notes are having sufﬁcient energy. An example ofwaveform and spectrogram is given in Figure 2. There arefour instances of the ‘cut’ ornament indicated in the spec-trogram at around frame indices 400, 540, 890 and 1130.Based on the manual annotation, we examined the dura-tion of the notes and single-note ornaments in our record-ings. The obtained histograms, depicted in Figure 3, indi-cate that the duration of ornaments is considerably lowerthan that of notes. The mean duration of single-note or-naments is 63 ms, while it is 209 ms for notes. In 95%of cases, the duration of single-note ornaments is between32 ms to 95 ms and of notes between 118 ms to 400 ms.\n02040608010012000.050.10.150.2\nDuration (ms)Proportion010020030040050060070000.050.10.150.20.250.30.350.4\nDuration (ms)Proportion(a) (b)Figure 3. The distribution of the duration of single-noteornaments (a) and notes (b) in our recordings.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7573. HMM-BASED NOTE TRANSCRIPTION ANDONSET DETECTIONThis section presents the proposed HMM-based system fortranscription of notes and detection of their onsets. Weﬁrst describe the representation of the audio signal and thenmodelling using HMMs.3.1 Feature representationThe acoustic signal is represented by a sequence of featurevectors, each vector capturing short-time spectral proper-ties of the signal. Since we are dealing with unaccom-panied music and in order to obtain lower-dimensionaland less-correlated features, the signal is represented us-ing Mel-frequency cepstral coefﬁcients (MFCCs). MFCCshave been widely used in speech and audio processing.The steps involved in converting audio signal into MFCCsis depicted in Figure 4.\nFigure 4. Processing steps used for converting the audiosignal into a sequence of Mel-frequency cepstral features.The signal is ﬁrst segmented into overlapping frames.The frame length determines the temporal and frequencyresolution. While longer frames allow for a ﬁner frequencyresolution, we are limited in the setting due to possiblyvery short duration of ornaments. Each signal frame ismultiplied by Hamming window. The windowed framesare then zero padded and the Fourier transform is appliedto provide the short-term Fourier spectrum. The short-time magnitude spectrum is passed through Mel-scale ﬁl-ter bank analysis and discrete cosine transform is appliedon the logarithm of the ﬁlter-bank energies to provideMFCCs. In order to include information on local dynam-ics, MFCCs were appended with their temporal deriva-tives, referred to as the delta and accelleration features,which were calculated as in [17]. The values of the pa-rameters within the processing steps need to reﬂect that weare dealing with ornamented ﬂute music. As such, in ourexperimental evaluations, we explored various parametersetups.3.2 ModellingThe model of each note is obtained by modelling the tem-poral evolution of feature vectors using a left-to-right (noskip allowed) hidden Markov model (HMM). We foundthat in addition to having an HMM for each note, it is es-sential to have also a separate HMM for each single-noteornament. This allows to deal with the fact that the real-isation of single-note ornaments may not fully reach thenotional frequency of a note but rather be somewhere be-tween two notes. In addition to this, we also create a modelfor breath and silence. These are used to model the tak-ing a breath by the player and the initial and ﬁnal silencesin recordings, respectively. Overall, we have 42 models,consisting of 14 models for notes, 14 models for cuts, 11models for strikes (strikes for some notes did not occur inour data) plus breath and silence. The state output prob-ability density function (pdf) at each HMM state is mod-elled using a mixture of Gaussians. This allows for a moreaccurate modelling of variations in playing notes than us-ing a single Gaussian distribution. Gaussian distributionswith a diagonal covariance matrix are used due to compu-tational reasons, as is typically done in speech and audiopattern processing. We explore the effect of using differ-ent number of HMM emitting states and Gaussian mixturecomponents in the experimental section. The transcriptionsystem was built using the HTK [17].3.2.1 TrainingThe initial values for the parameters of individual HMMswere estimated using isolated extracts from the audio sig-nal, by applying several iterations of the Viterbi style train-ing procedure. The isolated extracts were obtained basedon the manual time-stamp annotation, i.e., onset and offsettimes. Further training of the models was then performedusing several iterations of the Baum-Welch (aka forward-backward) algorithm. This uses continuous audio signal asinput and requires only the sequence of notes/ornamentslabels (i.e., no time-stamp). As such, this can eliminate theeffect of possible errors in time-stamp annotation at bor-ders of notes/ornaments on the trained models.3.2.2 RecognitionTo perform recognition, we need to construct a recogni-tion network. This deﬁnes the allowed sequences of mod-els (i.e., notes/ornaments). A network that closely reﬂectsthe knowledge of music could be employed. In this pa-per, we did not employ any such knowledge. We used aloop network that allows any note to follow any other note.We modiﬁed this slightly to reﬂect that an ornament modelneed to be followed by a note model. The network we usedis depicted in Figure 5. As this network allows the samenote to be subsequently repeated in the recognition output,we post-process the results such that the repetitions of thesame note are considered to be a single instance of the note,for example, the original recognition output B4 D4 D4 A5is considered to be B4 D4 A5. Note that a ﬁxed proba-bility value, aka word insertion penalty, can be associatedwith the transition from the end of one model to the startof the next model. This is useful in controlling the balancebetween the number of models being incorrectly insertedand deleted in the recognition result and we used it in ourexperimental evaluations.Given a sequence of feature vectors, the Viterbi algo-rithm is used to ﬁnd the optimal state sequence. This pro-vides the sequence of recognised models as well as the start(and the end) times of each recognised model, i.e., the on-set detection result.758 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 5. The recognition network used in the HMM-based note transcription system. The elipses denote indi-vidual HMMs. Models of ornaments are denoted by noteidentity appended with either ‘cut’ or ‘str’, representingmodels for ‘cut’ and ‘strike’ ornaments, respectively. ‘BR’denotes breath and ‘SIL’ silence models.4. EXPERIMENTAL EVALUATIONS4.1 Data descriptionEvaluations are performed using recordings of Irish tra-ditional tunes and training exercises played by ﬂute fromGrey Larsen’s CD which accompanied his book “EssentialGuide to Irish Flute and Tin Whistle” [13]. The tunes arebetween 16 sec and 1 min 22 sec long. All recordings aremonophonic and are sampled at 44.1 kHz sampling fre-quency.The collection consists of 19 tunes. The list of the tunes,with the number of notes and ornaments, is given in Ta-ble 1. In total, there are 3929 onsets, including notes andornaments. Out of these there are 804 single-note orna-ments (which includes also counts from parts of multi-noteornaments), consisting of 620 cuts and 184 strikes.First evaluations are performed to demonstrate the ef-fect of different parameter setups – due to computationalreasons, these experiments use all ﬁles for training of mod-els and also for testing. Final evaluations are performed us-ing the leave-one-out cross-validation procedure, in whichin turn 1 ﬁle is kept for testing and all the 18 remainingﬁles are used for training. The results were accummulatedover all ﬁles and then the evaluation measures calculated.4.2 Evaluation measuresPerformance of both the onset detection and note recog-nition is evaluated in terms of the precision (P), re-call (R) andF-measure. The deﬁnition of these measuresis the same as used in MIREX onset detection evaluations,speciﬁcally,P=NtpNtp+Nfp,R=NtpNtp+Nfn,F=2PRP+R.In the case of onset detection,Ntpis the number of cor-Tune Title Number of TimeNotes Ornaments (sec.)Cut StrikeStudy 5 55 16 0 20Study 6 56 24 0 20Study 11 76 20 0 26Study 17 48 19 0 16Study 22 127 0 28 47Maids of Ardagh 98 28 5 32Hardiman the .. 112 22 7 28The Whinny Hills .. 117 34 6 30The Frost is All .. 151 39 14 41The Humours of .. 289 113 19 82The Rose in the .. 152 33 13 39Scotsman over .. 153 33 9 38A Fig for a Kiss 105 27 9 28Roaring Mary 176 42 22 44The Mountain Road 105 20 6 25The Shaskeen 181 52 23 42Lady On The Island 118 21 1 21The Lonesome Jig 153 27 0 46The Drunken .. 185 50 22 43Total 2457 620 184 668Table 1. The list of tunes contained in the dataset, with thenumber of onsets and single-note ‘cut’ and ‘strike’ orna-ments and duration of each tune.rectly detected onsets andNfpandNfnis the number ofinserted and deleted onsets, respectively. The onset de-tection is considered as correct when it is within±50msaround the onset annotation.In the case of note recognition,Ntpis the number ofcorrectly recognised notes andNfpandNfnis the numberof inserted and deleted notes, respectively.4.3 Results for various parameter setups in featureextraction and modellingThis section explores the effect of different setups of pa-rameters in the feature extraction and HMM-based mod-elling on the task of onset detection. A comparison withthree conventional onset detection methods is also given.4.3.1 Conventional onset detection algorithmsThe conventional onset detection methods we employed toprovide a comparison are: two methods which exploit thechange of the signal amplitude over time, with process-ing performed in the temporal and spectral domain, anda method based on the fundamental frequency (F0). Thedescription of these methods, which we also used in ourprevious onset detection research, is provided in [11].We performed extensive evaluations with different pa-rameter values for each of the conventional onset detectionmethods. The best achieved performance for each of themethods is presented in Table 2. It can be seen that theF0-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 759based method performed better than each of the energy-based methods and achieved theF-measure of 91.2%.Algorithm for onset Evaluation performance (%)detection Precision RecallF-measuresig-energy (spectral) 94.8 85.6 89.9sig-energy (temporal) 90.8 88.4 89.6F089.3 93.2 91.2Table 2. Results of onset detection obtained using conven-tional onset detection methods.4.3.2 HMM-based onset detectionNow we explore the performance of the HMM-based sys-tem when using various parameter setup in the feature ex-traction and acoustic modelling.First, we compare results achieved by the HMM-basedsystem when using the estimatedF0with energy and theMFCCs (see the ﬁrst row in Table 4 for the parametersetup) as the input features. Results are presented in Ta-ble 3. It can be seen that when using the estimatedF0as input features to HMMs, theF-measure improved to93.8%, from 91.2% that was achieved using the conven-tionalF0-based method (as in Table 2). The performanceof the HMM-based system improved considerably furtherto 96.7% when using MFCC features as input, instead ofthe estimatedF0. As such, the use of HMMs driven withMFCC features provided over 60% error rate reductionover the best conventional method. The considerably betterperformance of the HMM-based system may be attributedto several factors. First, it is the statistical modelling of thetemporal evolution of features. Second, the features usedprovide information about the spectral content. This is un-like the energy-based andF0-based conventional methodswhich accummulate the information from the entire sig-nal bandwidth into a single detection function or into anF0estimate. Third, the use of HMM effectively incorpo-rates smoothing of the frame-based decisions and imposesa minimum duration of notes and ornaments.Features input to HMMF-measure (%)F0and energy, both with\u0000and\u0000293.8MFCC, both with\u0000and\u0000296.7Table 3. Results of the HMM-based onset detection whenusing an estimate ofF0and MFCCs as input features.Results obtained using diferent parameter setups in theMFCC feature extraction are presented in Table 4. The ﬁrstline in the table presents the best parameter setup valuesand this is: bandwidth of 4 kHz, frame length of 12.5 ms,frame-shift of 5 ms, Mel-scale ﬁlter-bank with 21 chan-nels, using 12 cepstral coefﬁcients, and appending deltaand acceleration coefﬁcients (which were extracted usingParameters in MFCC featureF-measureextraction (%)BW=4kHz, FrmL=12.5ms, FrmS=5ms, 96.7nFB=21, nCC=12,\u0000and\u00002Bandwidth 6 kHz 95.5(BW) 8 kHz 95.4Frame-length 10 ms 95.9(FrmL) 15 ms 96.320 ms 96.330 ms 95.7Frame-shift 3 ms 95.5(FrmS) 7 ms 95.4number of Mel ﬁlter-bank 17 96.1(nFB) 25 96.5Cepstral coefﬁcients 10 95.8(nCC) 14 96.6\u00002coefﬁcients no 95.8\u0000and\u00002coefﬁcients no 91.6Table 4. Results of the HMM-based onset detection interms of theF-measure obtained with different parametersetup in MFCC feature extraction.the window of 3 and 2 signal frames, respectively). Wenow analyse the effect of each parameter – in each exper-iment, only one parameter is modiﬁed at a time in refer-ence to the above best parameter setup. Let us start withvarying the frequency bandwidth of the signal. This wasperformed at the stage of designing the Mel ﬁlter-banks.For the bandwidth of 6 kHz and 8 kHz, the number of ﬁl-ters was adjusted such that the lower 4 kHz was in all casescovered by 21 ﬁlters. It can be seen that the performance issimilar when using the bandwidth of 6 kHz and 8 kHz butit improves considerably when the bandwidth is reduced to4 kHz. This reﬂects, as we have also noticed in our visualinspection of spectrograms, that there is little signal con-tent above 4 kHz in our ﬂute recordings and as such theinclusion of the higher frequency range acts detrimentallyto performance. This result may be used when analysingﬂute playing that contains accompaniments in higher fre-quencies or is recorded in live performances where otherunwanted sounds may be present in higher frequencies.Next, results using different length of frames show thatsimilar performance is achieved for lengths between 12 to20 ms. The performance starts to decrease considerablywhen frames of 30 ms are used. This is due to the pres-ence of ornaments, duration of which may be as short as20 ms. In the case of frame shift, it can be seen that set-ting this to 3 ms or 7 ms considerably degrades the per-formance in comparison to the use of 5 ms shift. Varyingthe number of ﬁlter-bank channels from 17 to 25 has onlyrelatively little effect, with performance being at the peakfor 21 channels. The use of 12 or 14 cepstral coefﬁcientsprovides very similar performance, while reducing this to10 has quite negative effect. Finally, experiments whenthe delta and accelleration features, denoted by\u0000and\u00002,760 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015respectively, are not included in feature representation arepresented. Results show a large decrease in performancewhen neither delta nor accelleration features are used. Thisdemonstrates the importance of incorporating informationon local dynamics of the acoustic signal.Next, we present the effect of different parameter setupsin acoustic modelling. We vary the number of states and ofmixture components of each state pdf for models of notesand ornaments. Results are presented in Table 5. The suit-able range of values for the number of states of note and or-nament models is determined based on the frame shift usedin the feature extraction and statistics of the duration of thenotes and ornaments. As such, we explored the range from6 to 12 for notes and from 2 to 6 for ornaments. It can beseen that there is not much performance variation when us-ing this range of values. In regard to the number of mixturecomponents, it can be seen that it is useful to have at least 4mixture components for note models, while even 2 mixturecomponents seem sufﬁcient for models of ornaments.Parameters in acoustic modellingF-measure (%)nStates for N / O / B: 8 / 4 / 8, 96.7nMix for N / O / B: 6 / 2 / 6nStates for notes 6 95.910 96.112 95.8nStates for ornaments 2 95.86 96.4nMix for notes 2 95.64 96.38 96.6nMix for ornaments 1 96.04 96.76 96.6Table 5. Results of HMM-based onset detection in termsof theF-measure obtained with different parameter setupin acoustic modelling. N, O, and B stand for note, orna-ment and breath, respectively.4.4 Results using the leave-one-out cross-validationThe ﬁnal experimental evaluations are performed using theleave-one-out cross-validation. The feature extraction andacoustic modelling parameter setup that achieved best per-formance in previous section is used. The achieved resultsof onset detection and note identity recognition are pre-sented in Table 6. It can be seen that very good perfor-mance is obtained for both tasks. The drop in onset de-tection performance in comparison to the results presentedin the previous section is expected as the testing ﬁles havenow not been seen during the training. Nevertheles, theperformance is improved by over 28% relative over theconventional onset detection algorithms whose parameterswere actually tuned based on both training and testing data.Evaluation performance (%)Precision RecallF-measureOnset detection 95.0 92.4 93.7Note recognition 96.4 95.2 95.8Table 6. Results of onset detection and note recognitionobtained by the HMM-based system using the leave-one-out cross-validation procedure.5. CONCLUSION AND FUTURE WORKIn this paper, we presented work on transcription of Irishtraditional ﬂute music containing ornamentation. The pre-sented system is based on modelling of individual notesand ornaments using hidden Markov models. Acoustic sig-nal is represented as a sequence of Mel frequency cepstralcoefﬁcients. A wide range of parameter setup values inboth the feature extraction and acoustic modelling wereexplored. Experimental evaluations were performed us-ing recordings of 19 Irish traditional ﬂute tunes, contain-ing in total 3929 onsets, out of which 804 corresponds toornaments. Using the leave-one-out cross-validation pro-cedure, the proposed HMM-based system achieved theF-measure of 93.7% in detecting onsets of ornaments andnotes. This represents over 28% error rate reduction com-pared to conventional onset detectors whose parameterswere even tuned to the testing data. TheF-measure in thetask of recognising the note identity was 95.8%.There are several possible extensions of this work weare currently considering. First, the presented evaluationswere performed using recordings from the same CD. Weplan to perform evaluations on a range of recordings fromseveral CDs in order to explore the capability of the systemin dealing with variability due to different recording condi-tions, makes of ﬂute instruments and performers. We willinvestigate techniques to compensate for such variabilityin order to improve robustness. Second, we plan to anal-yse the errors the automatic system makes in onset detec-tion and note identity recognition tasks and reﬂect this inmodiﬁcations to the system to further improve the perfor-mance. Then, the current HMM-based framework allowsdirectly and in a probabilistic manner to incorporate musi-cal knowledge on the sequences of notes used in ﬂute mu-sic. Such knowledge could be obtained from musicologistsand/or extracted automatically from annotations. Next, in-corporation of an explicit duration modelling of notes andornaments could help to reduce the number of falsely in-serted and deleted onsets. Finally, we plan to expand thesystem to deal with recordings, in which the ﬂute is accom-panied by other instruments and/or singing.6. ACKNOWLEDGEMENTSThis work was supported by the project ‘CharacterisingStylistic Interpretations through Automated Analysis ofOrnamentation in Irish Traditional Music Recordings’ un-der the ‘Transforming Musicology’ programme funded bythe Arts and Humanities Research Council (UK).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7617. REFERENCES[1]J. P. Bello, G. Monti, and M. Sandler. Techniques forautomatic music transcription. InInt. Symposium onMusic Information Retrieval (ISMIR), Plymouth, USA,2000.[2]E. Benetos and S. Dixon. Joint multi-pitch detectionusing harmonic envelope estimation for polyphonicmusic transcription.IEEE Journal of Selected Topicsin Signal Processing, 5(6):1111–1123, October 2011.[3]A.T. Cemgil, H.J. Kappen, and D. Barber. A genera-tive model for music transcription.IEEE Trans. onAu-dio, Speech, and Language Processing, 14(2):679–694, March 2006.[4]A. de Cheveigne and H. Kawahara. Yin, a fundamentalfrequency estimator for speech and music.Journal ofthe Acoustical Society of America, 111(4):1917–1930,April 2002.[5]K. Dressler. Extraction of the melody pitch contourfrom polyphonic audio. InInt. Conf. on Music Infor-mation Retrieval (ISMIR), London, UK, 2005.[6]V . Emiya, R. Badeau, and B. David. Automatic tran-scription of piano music based on HMM trackingof jointly-estimated pitches. In16th European SignalProcessing Conf. (EUSIPCO), Lausanne, Switzerland,2008.[7]B. Fuentes, R. Badeau, and G. Richard. Harmonicadaptive latent component analysis of audio and appli-cation to music transcription.IEEE Trans. on Audio,Speech, and Language Processing, 21(9):1854–1866,2013.[8]M. Gainza and E. Coyle. Automating ornamentationtranscription. InIEEE Int. Conf. on Acoustics, Speechand Signal Processing (ICASSP), Honolulu, Hawaii,2007.[9]S. Godsill and Manuel Davy. Bayesian harmonic mod-els for musical pitch estimation and analysis. InIEEEInt. Conference on Acoustics, Speech, and Signal Pro-cessing (ICASSP), volume 2, pages II–1769–II–1772,May 2002.[10]A.P. Klapuri. Multiple fundamental frequency estima-tion based on harmonicity and spectral smoothness.IEEE Transactions on Speech and Audio Processing,11(6):804–816, Nov 2003.[11]M. K¨ok¨uer, P. Janˇcoviˇc, I. Ali-MacLachlan, andC. Athwal. Automatic detection of single and multi-note ornaments in Irish traditional ﬂute playing. InProc. of the15th Int. Society for Music Information Re-trieval Conf. (ISMIR), pages 15–20, Taipei, Taiwan,Nov 2014.[12]M. K¨ok¨uer, D. Kearney, I. Ali-MacLachlan,P. Janˇcoviˇc, and C. Athwal. Towards the creationof digital library content to study aspects of style inIrish traditional music. InProc. of the Int. Workshopon Digital Libraries for Musicology (DLFM), London,2014.[13]G. Larsen.The Essential Guide to Irish Flute andTin Whistle. Mel Bay Publications, Paciﬁc, Missouri,USA, 2003.[14]G. E. Poliner and D. Ellis. A discriminative model forpolyphonic piano transcription.EURASIP Journal onAdvances in Signal Processing, 2007(1):048317, 2007.[15]M. P. Ryyn¨anen and A. P. Klapuri. Automatic tran-scription of melody, bass line, and chords in poly-phonic music.Computer Music Journal, 32(3):72–86,2008.[16]J. Salamon and E. G´omez. Melody extraction frompolyphonic music signals using pitch contour charac-teristics.IEEE Trans. on Audio, Speech, and LanguageProcessing, 20(6):1759–1770, 2012.[17]S. Young, D. Kershaw, J. Odell, D. Ollason,V . Valtchev, and P. Woodland.The HTK Book. V2.2.1999.762 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments.",
        "author": [
            "Berit Janssen",
            "Peter van Kranenburg",
            "Anja Volk"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418357",
        "url": "https://doi.org/10.5281/zenodo.1418357",
        "ee": "https://zenodo.org/records/1418357/files/JanssenKV15.pdf",
        "abstract": "To find occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to find occurrences of phrases in folk song melodies. We compare the similar- ity measures correlation distance, city-block distance, Eu- clidean distance and alignment, proposed for melody com- parison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic mel- odic similarity; moreover, wavelet transform and the ge- ometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the suc- cess of the different similarity measures through observing retrieval success in relation to human annotations. Our re- sults show that local alignment and SIAM perform on an almost equal level to human annotators.",
        "zenodo_id": 1418357,
        "dblp_key": "conf/ismir/JanssenKV15",
        "keywords": [
            "melodic segments",
            "themes",
            "phrases",
            "motifs",
            "human analysis",
            "large music corpora",
            "melodic similarity measures",
            "folk song melodies",
            "correlation distance",
            "city-block distance"
        ],
        "content": "A COMPARISON OF SYMBOLIC SIMILARITY MEASURES FORFINDING OCCURRENCES OF MELODIC SEGMENTSBerit JanssenMeertens Institute,Amsterdamberit.janssen@meertens.knaw.nlPeter van KranenburgMeertens Institute,Amsterdampeter.van.kranenburg@meertens.knaw.nlAnja VolkUtrecht University,the Netherlandsa.volk@uu.nlABSTRACTTo ﬁnd occurrences of melodic segments, such as themes,phrases and motifs, in musical works, a well-performingsimilarity measure is needed to support human analysis oflarge music corpora. We evaluate the performance of arange of melodic similarity measures to ﬁnd occurrencesof phrases in folk song melodies. We compare the similar-ity measures correlation distance, city-block distance, Eu-clidean distance and alignment, proposed for melody com-parison in computational ethnomusicology; furthermoreImplication-Realization structure alignment and B-splinealignment, forming successful approaches in symbolic mel-odic similarity; moreover, wavelet transform and the ge-ometric approach Structure Induction, having performedwell in musical pattern discovery. We evaluate the suc-cess of the different similarity measures through observingretrieval success in relation to human annotations. Our re-sults show that local alignment and SIAM perform on analmost equal level to human annotators.1. INTRODUCTIONIn many music analysis tasks, it is important to query alarge database of music pieces for the occurrence of a spe-ciﬁc melodic segment: which pieces by Rachmaninov quoteDies Irae? Which bebop jazz improvisers used a speciﬁcCharlie Parker lick in their solos? How many folk songsingers perform a melodic phrase in a speciﬁc way?In the present article, we compare a range of existingsimilarity measures with the goal of ﬁnding occurrencesof melodic segments in a corpus of folk song melodies.This is a novel research question, evaluated on annotationswhich have been made speciﬁcally for this purpose. Theinsights gained from our research on the folk song genrecan inform future research on occurrences in other genres.We evaluate similarity measures on a set of folk songs,in which human experts annotated phrase similarity. Wec\u0000Berit Janssen, Peter van Kranenburg, Anja V olk.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Berit Janssen, Peter van Kranenburg,Anja V olk. “A comparison of symbolic similarity measures for ﬁndingoccurrences of melodic segments”, 16th International Society for MusicInformation Retrieval Conference, 2015.use these annotations as evidence for occurrences of melo-dic segments in related songs. If we know that a similaritymeasure is successful in ﬁnding the annotated occurrencesin this set, we infer that the measures will be successful forﬁnding correct occurrences of melodic segments of phraselength in a larger dataset of folk songs as well. We describethe dataset in more detail in Section 2.In computational ethnomusicology various methods forcomparing folk song melodies have been suggested: assuch, correlation distance [12], city-block distance andEuclidean distance [14] have been considered promising.Research on melodic similarity in folk songs also showedthat alignment measures reproduce human judgements onagreement between melodies well [16].As this paper focusses on similarity of melodic seg-ments rather than whole melodies, recent research in mu-sical pattern discovery is also of particular interest. Twowell-performing measures in the associated MIREX chal-lenge of 2014 [7, 17] have shown success when evaluatedon the Johannes Keppler University segments Test Database(JKUPDT).1We test whether the underlying similaritymeasures of the pattern discovery methods also performwell in ﬁnding occurrences of melodic segments.Additionally, we apply the most successful similaritymeasures from the MIREX symbolic melodic similaritytrack in our research. The best measure of MIREX 2005(Grachten et al. [4]), was evaluated on RISM incipits, whichare short melodies or melodic segments, therefore relevantfor our task. In recent MIREX editions the algorithm byUrbano et al. [15] has been shown to perform well on theEsAC folk song collection.2We present an overview of the compared similarity mea-sures in Table 1, listing the music representations to whichthese measures have been originally applied, and which wetherefore also use in our comparisons. Moreover, we in-clude information on the research ﬁelds from which themeasures are taken, the database on which they were eval-uated, if applicable, and a bibliographical reference to arelevant paper. We describe the measures in Section 3.We evaluate the different measures by comparison withhuman annotations of phrase occurrence, through quanti-1http://www.music-ir.org/mirex/wiki/2014:Discovery_of_Repeated_Themes_%26_Sections_Results2http://www.esac-data.org659Similarity measureMusic representationsResearch ﬁeldDatasetAuthorsCorrelation distance (CD)duration weighted pitch sequenceEthnomusicology-[12]City block distance (CBD)pitch sequenceEthnomusicology-[14]Euclidean distance (ED)pitch sequenceEthnomusicology-[14]Local alignment (LA)pitch sequenceEthnomusicologyMTC[16]Structure induction (SIAM)pitch / onsetMIRJKUPTD[7]Wavelet transform (WT)duration weighted pitch sequenceMIRJKUPTD[17]B-spline alignment (BSA)pitch sequenceMlREsAC[15]I-R structure alignment (IRSA)pitch, duration, metric weightMIRRISM[4]Table 1. An overview of the measures for music similarity compared in this research, with information on the authors andyear of the related publication, and which musical data the measures were tested on, if applicable.fying the retrieval measures precision, recall and F1-score,and the area under the receiver-operating characteristiccurve. The evaluation procedure is described in detail inSection 4.The remainder of this paper is organised as follows:ﬁrst, we describe our corpus of folk songs and the anno-tation procedure. Next, we give details on the comparedsimilarity measures, and the methods used to implementthe similarity measures. We describe our evaluation pro-cedure before presenting the results, ﬁnally discussing theimplications of our ﬁndings and concluding steps for futurework.2. MATERIALWe evaluate the similarity measures on a corpus of Dutchfolk songs, MTC-ANN 2.0, which is part of the MeertensTune Collections [5]. MTC-ANN 2.0 contains 360 orallytransmitted melodies, which have been transcribed fromrecordings and digitized in various formats. Various meta-data have been added by domain experts, such as the tunefamily membership of a given melody: the melodies werecategorized into groups of variants, or tune families. Thevariants belonging to a tune family are considered as beingdescended from the same ancestor melody [1]. We parsethe **kern ﬁles as provided by MTC-ANN 2.0 and trans-form the melodies and segments into the required musicrepresentations using music21 [2].Even though MTC-ANN 2.0 comprises very well docu-mented data, there are some difﬁculties to overcome whencomparing the digitized melodies computationally. Mostimportantly, the transcription choices between variants canbe different: where one melody is notated in 3/4, and witha melodic range from D4 to G4, another transcriber mayhave chosen a 6/8 meter, and a melodic range from D3to G3. This means that notes which are perceptually verysimilar might be hard to match based on the digitized in-formation. Musical similarity measures might be sensitiveto these differences, or they might be transposition or timedilation invariant, i.e. work equally well under differentpitch transpositions or meters.Of these 360 melodies categorized into 26 tune families,we asked three Dutch folk song experts to annotate similar-ity relationships between phrases within tune families. Theannotators judged the similarity of phrases of 213 melodiesbelonging to 16 tune families, amounting to 1084 phraseannotations in total. The phrases contain, on average, ninenotes, with a standard deviation of two notes. The datasetwith its numerous annotations is publicly available.3For each tune family, the annotators compared all thephrases within the tune family with each other, and gaveeach phrase a label consisting of a letter and a number.If two phrases were considered “almost identical”, theyreceived exactly the same label; if they were considered“related but varied”, they received the same letter, but dif-ferent numbers; and if two phrases were considered “dif-ferent”, they received different letters. See an annotationexample in Figure 1.The three domain experts worked independently on thesame data. To investigate the subjectivity of similarityjudgements, we measured the agreement between the threeannotators’ similarity judgements using Fleiss’ Kappa,which yielded=0.73, constituting substantial agree-ment.The annotation was organized in this way to guaran-tee that the task was feasible: judging the occurrences ofhundreds of phrases in dozens of melodies (14714 compar-isons) would have been much more time consuming thanassigning labels to the 1084 phrases, based on their sim-ilarity. Moreover, the three levels of annotation facilitateevaluation for two goals: ﬁnding only almost identical oc-currences, and ﬁnding also varied occurrences. These twogoals might require quite different approaches.We focus on ﬁnding almost identical occurrences: if fora given query phraseqin one melody, at least one phraserwith exactly the same label (letter and number) appears inanother melodysof the same tune family, we consider it anoccurrence of melodic segmentqins. Conversely, if thereis no phrase with exactly the same label asqin melodys,this constitutes a non-occurrence.For all phrases and all melodies, within their respectivetune families, we observe whether the annotators agree onoccurrence or non-occurrence of query phrasesqin melo-diess. The agreement for these judgements, 14714 in total,was analyzed with Fleiss’ Kappa, with the result=0.51denoting moderate agreement. This highlights the ambigu-3http://www.liederenbank.nl/mtc/660 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 Record 73120 - Strophe 1/dots.dot/noteheads.s1/flags.d3/noteheads.s2/accidentals.sharp/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/clefs.G/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/clefs.G/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2A0B0C0D0 Record 75635 - Strophe 1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/accidentals.sharp/clefs.G/noteheads.s286/dots.dot/noteheads.s2/noteheads.s2/dots.dot42/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G/dots.dot/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s283/dots.dot/noteheads.s2/noteheads.s286/noteheads.s2/flags.d3/noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.d3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/clefs.G783/noteheads.s2/noteheads.s2/dots.dotA1B0E3D6Figure 1. An example for two melodies from the same tune family with annotations.AnnotatorsPrecisionRecallF1-score1 and 20.7450.7630.7541 and 30.8030.750.7762 and 30.7880.7190.752Table 2. The retrieval scores between annotators. For in-stance, annotator 2 agrees to 75% with the occurrences de-tected by annotator 1. The scores are symmetric.ity involved in ﬁnding occurrences of melodic segments.To compare the annotators’ agreement with the perfor-mance of the similarity measures in the most meaningfulway, we also compute the precision, recall and F1-scoreof each annotator in reproducing the occurrences detectedby another annotator. Table 2 gives an overview of theseretrieval scores. A higher retrieval score for a given simi-larity measure would indicate overﬁtting to the judgementsof one individual annotator.3. COMPARED SIMILARITY MEASURESIn this section, we present the eight compared similaritymeasures. We describe the measures in three subgroups:ﬁrst, measures comparing ﬁxed-length note sequences; sec-ond, measures comparing variable-length note sequences;third, measures comparing more abstract representationsof the melody.For our corpus, as melodies are of similar length, wecan transpose all melodies to the same key using pitch his-togram intersection. For each melody, a pitch histogram iscomputed with MIDI note numbers as bins, with the countof each note number weighed by its total duration in a mel-ody. The pitch histogram intersection of two histogramshqandhr, with shift\u0000is deﬁned asPHI(hq,hr,\u0000)=lXk=1min(hq,k+\u0000,hr,k),(1)wherekdenotes the index of the bin, andlthe total numberof bins. We deﬁne a non-existing bin to have value zero.For each tune family, we randomly pick one melody andfor each other melody in the tune family we compute the\u0000that yields a maximum value for the histogram intersec-tion, and transpose that melody by\u0000semitones.Some similarity measures use note duration to increaseprecision of the comparisons, others discard the note du-ration, which is an easy way of dealing with time dilationdifferences. Therefore, we distinguish between music rep-resentation aspitch sequences, which discard the durationsof notes, andduration weighted pitch sequences, which re-peat a given pitch depending on the length of the notes.We represent a quarter note by 16 pitch values, an eighthnote by 8 pitch values, and so on. Onsets of small durationunits, especially triplets, may fall between these samplingpoints, which shifts their onset slightly in the representa-tion. Besides, a few similarity measures require music rep-resentation asonset, pitchpairs, or additional informationon metric weight.3.1 Similarity Measures Comparing Fixed-LengthNote SequencesTo formalize the following three measures, we refer to twomelodic segmentsqandrof lengthn, which have ele-mentsqiandri. The measures described in this sectionare distance measures, such that lower values ofdist(q,r)indicate higher similarity. Finding an occurrence of a mel-odic segment within a melody with a ﬁxed-length simi-larity measure is achieved through the comparison of thequery segment against all possible segments of the samelength in the melody. The candidate segment which is mostsimilar to the query segment is retained as a match. Theimplementation of the ﬁxed-length similarity measures inPython is available online.4It uses thespatial.distancelibrary of scipy [10].Scherrer and Scherrer [12] suggest correlation distanceto compare folk song melodies, represented as durationweighed pitch sequences. Correlation distance is indepen-dent of the transposition and melodic range of a melody,but in the current music representation, it is affected bytime dilation differences.dist(q,r)=1\u0000Pni=1(qi\u0000¯q)·Pni=1(ri\u0000¯r)pPni=1(qi\u0000¯q)·Pni=1(ri\u0000¯r)(2)Steinbeck [14] proposes two similarity metrics for theclassiﬁcation of folk song melodies: city-block distanceand Euclidean distance (p.251f.). He suggests to comparepitch sequences, next to various other features of melodiessuch as their range, or the number of notes in a melody.As we are interested in ﬁnding occurrences of segments4https://github.com/BeritJanssen/MelodicOccurrencesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 661rather than comparing whole melodies, we analyze pitchsequences.City-block distance and Euclidean distance are not trans-position invariant, but as they are applied to pitch sequences,they are time dilation invariant. All the ﬁxed-length mea-sures in this section will be inﬂuenced by small variationsaffecting the number of notes in a melodic segment, suchas ornamentation. Variable-length similarity measures, dis-cussed in the following section, can deal with such varia-tions more effectively.3.2 Similarity Measures Comparing Variable-LengthNote SequencesTo formalize the following three measures, we refer to amelodic segmentqof lengthnand a melodysof lengthm, with elementsqiandsj. The measures described in thissection are similarity measures, such that lower values ofsim(q,s)indicate higher similarity. The implementationof these methods in Python is available online.4Mongeau and Sankoff [8] suggest the use of alignmentmethods for measuring music similarity, and they have beenproven to work well for folk songs [16]. We apply localalignment [13], which returns the similarity of a segmentwithin a melody which matches the query best.To compute the optimal local alignment, a matrixA(i, j)is recursively ﬁlled according to equation 3. The matrix isinitialized asA(i,0) = 0,i2{0,...,n}, andA(0,j)=0,j2{0,...,m}.WinsertionandWdeletiondeﬁne theweights for inserting an element from melodysinto seg-mentq, and for deleting an element from segmentq, re-spectively.subs(qi,sj)is the substitution function, whichgives a weight depending on the similarity of the notesqiandsj.A(i, j) = max8>>><>>>:A(i\u00001,j\u00001) +subs(qi,sj)A(i, j\u00001) +WinsertionA(i\u00001,j)+Wdeletion0(3)We apply local alignment to pitch sequences. In thisrepresentation, local alignment is not transposition invari-ant, but it should be robust with respect to time dilation.For the insertion and deletion weights, we useWinsertion=Wdeletion=\u00000.5, and we deﬁne the substitution score assubs(qi,sj)=(1ifqi=sj\u00001otherwise.(4)The local alignment score is the maximum value in thealignment matrix, normalized by the number of notesninthe query segment.sim(q,s)=1nmaxi,j(A(i, j))(5)Structure Induction Algorithms [7] formalize a melodyas a set of points in a space deﬁned by note onset andpitch, and perform well for musical pattern discovery [6].They measure the difference between melodic segmentsthrough so-called translation vectors. The translation vec-torTbetween points in two melodic segments can be seenas the difference between the pointsqiandsjin onset,pitch space. As such, it is transposition invariant, but willbe inﬂuenced by time dilation differences.T=✓qi,onsetqi,pitch◆\u0000✓sj,onsetsj,pitch◆(6)The maximally translatable pattern (MTP) of a transla-tion vectorTfor two melodiesqandsis then deﬁned asthe set of melody pointsqiwhich can be transformed tomelody pointssjwith the translation vectorT.MTP(q,s,T)={qi|qi2q^qi+T2s}(7)We analyze the pattern matching method SIAM, deﬁn-ing the similarity of two melodies as the length of the longestmaximally translatable pattern, normalized by the lengthnof the query melody:sim(q,s)=1nmaxT|MTP(q,s,T)|(8)3.3 Similarity Measures Comparing AbstractRepresentationsThe following three methods transform the melodic con-tour into a more abstract representation prior to compari-son.Velarde et al. [18] use wavelet coefﬁcients to comparemelodies: melodic segments are transformed with the Haarwavelet. The wavelet coefﬁcients indicate whether thereis a contour change at a given moment in the melody, andsimilarity between two melodies is computed through city-block distance of their wavelet coefﬁcients. The methodachieved considerable success for pattern discovery [17].We use the authors’ Matlab implementation to computewavelet coefﬁcients of duration weighed pitch sequences,and compute city-block distance between the coefﬁcientsof query segment and match candidates.Through the choice of music representation and com-parison of the wavelet coefﬁcients, this is a ﬁxed-lengthsimilarity measure sensitive to time dilation; however, it istransposition invariant.Urbano et al. [15] transform note trigrams to a seriesof B-spline interpolations, which are curves ﬁtted to thecontours of the note trigrams. The resulting series of B-splines of two melodies are then compared through align-ment. Different B-spline alignment approaches have per-formed well in various editions of MIREX for symbolicmelodic similarity.5We apply the ULMS2-ShapeL algorithm,6using themost recent version, different from its original publica-tion [15]. This algorithm discards the durations of thenotes and returns the local alignment score of query seg-ments and melodies. The score is normalized by the length5http://www.music-ir.org/mirex/wiki/2012:Symbolic_Melodic_Similarity_Results6https://github.com/julian-urbano/MelodyShape662 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015nof the query segment. This similarity measure is of vari-able length, sensitive to time dilation, but transposition in-variant.Grachten’s method [4] relies on Implication-Realization(IR) structures, as introduced by Narmour [9] as basic unitsof melodic expectation. Grachten et al. transform melo-dies into IR structures using a specially developed parser.The similarity of melodies is then determined based on thealignment of the IR structures. This method was successfulin the MIREX challenge for symbolic melodic similarity of2005.7In preparation of IR-structure alignment, we useGrachten’s [4] IR-parser, which takes the onset, pitch, du-ration and metric weight of a melody and infers the corre-sponding IR structures. To this end, we exclude all melo-dies which do not have annotated meter (n= 65), neededfor the computation of metric weight, from the corpus. Wealign the IR-structures with the same insertion and deletionweights and the same substitution function as Grachten’spublication, but as we are interested in ﬁnding occurrences,we use local alignment rather than the original global align-ment approach. Through the transformation of the note se-quences to IR-structure sequences, this similarity measureis transposition invariant, but it is sensitive to time dila-tion and ornamentation, which might affect the detectedIR-structures.4. EVALUATIONWe evaluate the potential success of a similarity measurethrough comparing the retrieved occurrences to the anno-tators’ judgements, separately for each annotator. Differ-ent thresholds on the similarity measures determine whichmatches are accepted as occurrences, or rejected as non-occurrences. For the distance measures (CD, CBD, ED,WT), matches with similarity values below the threshold,for the other measures, matches with similarity values abovethe threshold are considered occurrences.The relationship between true positives and false pos-itives for each measure is summarized in a receiver-oper-ating characteristic (ROC) curve with the threshold as pa-rameter. The area under the ROC curve (AUC) determineswhether a similarity measure overall performs better thananother, for which we calculate conﬁdence intervals andstatistical signiﬁcance using DeLong’s method for pairedROC curves, based on U statistics [3,11]. Furthermore, wereport the maximally achievable retrieval measures preci-sion, recall and F1-score with relation to the ground truth.5. RESULTSWe have analyzed the results with respect to all annotators,resulting in the same ranking of the similarity measures.Due to space constraints, we report and discuss our resultsin relation to annotator 1. We show the ROC curves ofthe eight different measures in Figure 2, which display thetrue positive rate against the false positive rate at different7http://www.music-ir.org/mirex/wiki/2005:Symbolic_Melodic_Similarity_ResultsMeasureF1-scorePrecisionRecallAUCBaseline0.680.521n/aCD0.680.5110.549CBD0.680.5110.574ED0.680.5110.568LA0.730.70.780.790SIAM0.730.750.710.787WT0.690.570.870.732BSA0.720.650.810.776IRSA0.690.540.950.683Table 3. Results of the compared similarity measures fordifferent music representations: the maximal F1-score, theassociated precision and recall, and the area under the ROCcurve (AUC).\n0.000.250.500.751.00\n0.000.250.500.751.00False positive rateTrue positive rateMeasureCDCBDEDLASIAMWTBSAIRSAROC curves of similarity measures\nFigure 2. The ROC curves for the various similarity mea-sures, showing the increase of false positive rate againstthe increase of the true positive rate, as a parameter of thethreshold.thresholds. The more of the higher left area a ROC curvecovers in a graph, the better; this indicates that the twoclasses are better separable.From Figure 2 it can be seen that the similarity mea-sures suggested in computational ethnomusicology (CD,CBD, ED) perform only marginally above chance. IR-structure alignment and wavelet transform obtain better re-sults, and B-spline alignment, local alignment and SIAMperform best.We summarize the area under the ROC curve (AUC),the maximally achieved F1-score, as well as the associatedprecision and recall in Table 3. We include a baseline inthis table which assumes that every compared melody con-tains an occurrence of the query segment, which leads toperfect recall, but poor precision, as the chance for a seg-ment to occur in a given melody are only about 50%.We compare the AUC values of the different measuresin Figure 3, showing conﬁdence intervals and signiﬁcanceof the pairwise differences between adjoining measures,indicated by stars (*p<.5, **p<.01, ***p<.001).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 663n.s.***************Correlation distanceEuclidean distanceCity−block distanceI−R structure alignmentWavelet transformB−spline alignmentStructure inductionLocal alignment\n0.60.70.8AUCSimilarity measureArea Under ROC Curve\nFigure 3. The area under the ROC curve of all similar-ity measures, ordered by the most successful to the leastsuccessful methods. The error bars indicate the conﬁ-dence intervals, and signiﬁcant difference between adjoin-ing measures is indicated by stars (*p<.5, **p<.01,***p<.001).6. DISCUSSIONOur results indicate that the distance measures (CD, CBD,ED) do not work very well, which contradicts the intu-itions of the computational ethnomusicologists who pro-pose them. This suggests that variations on pitch heightand contour, which mostly affect these measures, are notthe most informative aspect for human judgements on mu-sical similarity. Embellishments of a note sequence throughextra notes, for instance to accommodate slightly variedlyrics, on the other hand, would cause considerable de-crease of measured similarity, while they will be perceivedas minor variation, if at all by human listeners.Measures from symbolic melodic similarity (BSA,IRSA) and pattern discovery (WT) perform better over-all. Among these, I-R structure alignment performs leastwell. This performance might be improved by optimisingthe alignment scores for our dataset; the alignment weightswere trained on RISM incipits and might therefore not ﬁtthe folk songs optimally.Wavelet transform seems to capture some essential no-tions of music similarity for ﬁnding correct occurrences,showing that essentially the same technique - ﬁxed-lengthcomparison with city-block distance - can be much moresuccessful if it is applied to a different abstraction levelthan pitch sequences. Possibly a variable-length compari-son step would yield even better results.As expected from its success in symbolic melodic sim-ilarity MIREX tracks, B-spline alignment successfully re-trieves a large portion of relevant occurrences annotatedby human experts. However, it does not perform as well assome of the other measures in our comparison.Conﬁrming earlier research on melodic similarity in folksongs, alignment performs well in our task. We show thatlocal alignment is very successful in correctly identifyingoccurrences, even with a very simple substitution score,which only rewards equal pitches. Even better results mightbe achieved with different weights and substitution scores.SIAM, to our knowledge, has not been evaluated for de-tecting phrase occurrences in folk song melodies yet, butperforms on the same level as local alignment. This im-plies that SIAM is a good candidate for ﬁnding occurrencesof melodic segments successfully, especially in corporawhere transposition differences cannot be resolved throughpitch histogram intersection, for instance in classical mu-sic and jazz, where key changes might make the estimationof transposition more difﬁcult.With maximal F1-scores of 0.73, the results of localalignment and SIAM come close to the between-annotatorF1-scores between 0.75 and 0.78. This shows that we can-not do much better for our problem on this dataset withoutoverﬁtting.7. CONCLUSIONWe conclude that both local alignment and SIAM seemadequate methods for ﬁnding occurrences of melodic seg-ments in folk songs. Based on the retrieval scores, they ﬁndalmost the same amount of relevant occurrences as humanannotators among each other.The measures investigated in this paper were appliedto speciﬁc music representations. A wider range of musicrepresentations will be compared in future work. More-over, the results will need to be analyzed in more detailwith special attention to the cases where the similarity mea-sures err, i.e. are false positives and false negatives morefrequent for a speciﬁc tune family? And if so, do the an-notators also disagree most on these same tune families?Besides, it is important to investigate the true positives aswell, and ascertain that they are found in the correct posi-tions in a melody.The similarity measures compared in this article canbe applied to other music corpora, which will give evendeeper insights into relationships between melodies basedon melodic segments that are shared between them. Wecan learn much about melodic identity and music similarityfrom both the conﬁrmation and refutation of our ﬁndingsin other music genres.8. ACKNOWLEDGEMENTSBerit Janssen and Peter van Kranenburg are supported bythe Computational Humanities Programme of the RoyalNetherlands Academy of Arts and Sciences, under the aus-pices of the Tunes&Tales project. For further information,seehttp://ehumanities.nl. Anja V olk is supportedby the Netherlands Organisation for Scientiﬁc Researchthrough an NWO-VIDI grant (276-35-001). We thank Gis-sel Velarde, Maarten Grachten and Juli´an Urbano for kindlyproviding their code and helpful comments, Sanneke vander Ouw, Jorn Janssen and Ellen van der Grijn for their an-notations, and the anonymous reviewers for their detailedsuggestions.664 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20159. REFERENCES[1]Samuel P. Bayard. Prolegomena to a Study of thePrincipal Melodic Families of British-American FolkSong.The Journal of American Folklore, 63(247):1–44, 1950.[2]Michael Scott Cuthbert and Christopher Ariza. mu-sic21 : A Toolkit for Computer-Aided Musicology andSymbolic Music Data. In11th International Societyfor Music Information Retrieval Conference (ISMIR2010), number Ismir, pages 637–642, 2010.[3]Elizabeth R. Delong, David M. Delong, and Daniel L.Clarke-Pearson. Comparing the Areas Under Twoor More Correlated Receiver Operating Characteris-tic Curves: A Nonparametric Approach.Biometrics,44(3):837–845, 1988.[4]Maarten Grachten, Josep Llu´ıs Arcos, and RamonL´opez de M´antaras. Melody Retrieval using the Im-plication / Realization Model. InMIREX-ISMIR 2005:6th International Conference on Music Information re-trieval, 2005.[5]Peter Van Kranenburg, Martine De Bruin, Louis PGrijp, and Frans Wiering. The Meertens Tune Collec-tions. Technical report, Meertens Online Reports, Am-sterdam, 2014.[6]David Meredith. COSIATEC and SIATECCompress:Pattern Discovery by Geometric Compression. InMu-sic Information Retrieval Evaluation eXchange, 2014.[7]David Meredith, Kjell Lemstr¨om, and Geraint A. Wig-gins. Algorithms for discovering repeated patterns inmultidimensional representations of polyphonic music.Journal of New Music Research, 31(4):321–345, 2002.[8]Marcel Mongeau and David Sankoff. Comparison ofMusical Sequences.Computers and the Humanities,24:161–175, 1990.[9]Eugene Narmour.The Analysis and Cognition of Ba-sic Melodic Structures. The Implication-RealizationModel.University of Chicago Press, Chicago, 1990.[10]Travis E. Oliphant. Python for Scientiﬁc Computing.Computing in Science and Engineering, 9(3):10–20,2007.[11]Xavier Robin, Natacha Turck, Alexandre Hainard,Natalia Tiberti, Fr´ed´erique Lisacek, Jean-charlesSanchez, and Markus M¨uller. pROC : an open-sourcepackage for R and S + to analyze and compare ROCcurves.BMC Bioinformatics, 12(1):77, 2011.[12]Deborah K. Scherrer and Philip H. Scherrer. An Exper-iment in the Computer Measurement of Melodic Vari-ation in Folksong.The Journal of American Folklore,84(332):230–241, 1971.[13]T.F. Smith and M.S. Waterman. Identiﬁcation of com-mon molecular subsequences.Journal of MolecularBiology, 147(1):195 – 197, 1981.[14]Wolfram Steinbeck.Struktur und¨Ahnlichkeit. Meth-oden automatisierter Melodienanalyse.B¨arenreiter,Kassel, 1982.[15]Juli´an Urbano, Juan Llor´ens, Jorge Morato, and SoniaS´anchez-Cuadrado. MIREX 2012 Symbolic MelodicSimilarity: Hybrid Sequence Alignment with Geomet-ric Representations. InMusic Information RetrievalEvaluation eXchange, pages 3–6, 2012.[16]Peter van Kranenburg, Anja V olk, and Frans Wiering.A Comparison between Global and Local Features forComputational Classiﬁcation of Folk Song Melodies.Journal of New Music Research, 42(1):1–18, 2012.[17]Gissel Velarde and David Meredith. A Wavelet-BasedApproach to the Discovery of Themes and Sections inMonophonic Melodies. InMusic Information RetrievalEvaluation eXchange, 2014.[18]Gissel Velarde, Tillman Weyde, and David Meredith.An approach to melodic segmentation and classiﬁca-tion based on ﬁltering with the Haar-wavelet.Jour-nal of New Music Research, 42(4):325–345, December2013.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 665"
    },
    {
        "title": "Instrument Identification in Optical Music Recognition.",
        "author": [
            "Yucong Jiang",
            "Christopher Raphael"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416116",
        "url": "https://doi.org/10.5281/zenodo.1416116",
        "ee": "https://zenodo.org/records/1416116/files/JiangR15.pdf",
        "abstract": "We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instru- ments in the system. Each instrument may apply to sev- eral staves, may be represented with several possible text strings, and may appear at several possible positions rela- tive to the staves. We find the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character recog- nition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text la- beling for each system, as well as the character template models, thus adapting to the font at hand. Our implemen- tation alternately optimizes over the text label identifica- tion and re-estimates the character templates. Experiments are presented on 10 different scores showing a significant improvement due to adaptation.",
        "zenodo_id": 1416116,
        "dblp_key": "conf/ismir/JiangR15",
        "keywords": [
            "score",
            "instrument",
            "orchestra",
            "recognition",
            "staves",
            "optical",
            "music",
            "template",
            "character",
            "optimization"
        ],
        "content": "INSTRUMENT IDENTIFICATION IN OPTICAL MUSIC RECOGNITIONYucong JiangSchool of Informatics and ComputingIndiana University, Bloomingtonyujiang@indiana.eduChristopher RaphaelSchool of Informatics and ComputingIndiana University, Bloomingtoncraphael@indiana.eduABSTRACTWe present a method for recognizing and interpreting thetext labels for the instruments in an orchestra score, therebyassociating staves with instruments. This task is one ofmany necessary in optical music recognition. Our approachtreats the score system as the basic unit of processing. Agraph structure describes the possible orderings of instru-ments in the system. Each instrument may apply to sev-eral staves, may be represented with several possible textstrings, and may appear at several possible positions rela-tive to the staves. We ﬁnd the optimal labeling of stavesusing a globally optimal dynamic programming approachthat embeds simple template-based optical character recog-nition within the overall recognition scheme. When givenan entire score, we simultaneously optimize on the text la-beling for each system, as well as the character templatemodels, thus adapting to the font at hand. Our implemen-tation alternately optimizes over the text label identiﬁca-tion and re-estimates the character templates. Experimentsare presented on 10 different scores showing a signiﬁcantimprovement due to adaptation.1. INTRODUCTIONIn some scores, particularly those for small ensemble, in-struments appear in the same position in all systems mak-ing it easy to associate instruments with staves. This schemewould be typical for a string quartet or sonata for solo in-strument and piano. Occasionally large-ensemble scoresfollow this convention as well, though it requires consider-ably more space as empty staves must be written out everytime any instrument is not used in a particular system, thuscreating longer scores and lowering the density of informa-tion. For these reasons many publishers avoid this layoutstyle, instead notating only the instruments that play in aparticular system. In this case text labels, usually appear-ing in the left margin of the system, identify the instru-ment(s) associated with the individual staves, as in Figure1. These are the scores we treat here, while our goal is thelabeling of each staff with its associated instrument. Suchlabeling is necessary for nearly any aspect of optical musicc\u0000Yucong Jiang, Christopher Raphael.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Yucong Jiang, Christopher Raphael.“Instrument Identiﬁcation in Optical Music Recognition”, 16th Interna-tional Society for Music Information Retrieval Conference, 2015.recognition (OMR) using “instrument-labeled scores”, asit allows one to link systems together in a meaningful way.The ﬁrst steps of our OMR system [9] are to identifythe staves in a page, and then to group these into systems.While these tasks present challenges due to the wide vari-ation in printed scores, they are among the easier OMRtasks, and are handled reasonably well by our system. Theresulting score systems, including the precise locations ofall the staves they contain, constitute the input to our stafflabeling process.In spite of the mature nature of optical character recog-nition (OCR), our staff labeling problem is highly chal-lenging when viewed purely in these terms. The text stringswe seek to recognize are usually a single word or abbrevi-ation, thus providing only a small portion of data for eachrecognition problem. Furthermore, even though the vocab-ulary is constrained to the instrument names used in thescore, OCR will encounter difﬁculties distinguishing sim-ilar strings, such as “Violin I” and “Violin II,” or “Vln.”,“Vla.”, and “Vcl.”. Finally, there often is other irrelevanttext in the area of the names we seek to recognize, furtherhindering the recognition. But even if OCR were enoughto recognize the instrument names, our goal includes morethan this. As it is common for text strings to apply to mul-tiple staves in a score, we need the name-to-staff mappingas well.Thus, unlike the bottom-up approach used in [12], ourtop-down approach uses a graphical model that generatesall legitimate possible partitions of the system into stavesand all legitimate possible labelings of these partitions withinstruments. The graphical representation enables a dy-namic programming approach that embeds rather genericOCR into the “innermost loop” for the recognition of indi-vidual text labels. The model may include strong assump-tions about the possible orderings of instruments, with themost obvious choice being that the instruments appear inthe order initially given on the ﬁrst score page, (though anysubset of instruments can be omitted).Perhaps the biggest challenge of our task is the fontvariation between scores. Even controlling for the heightof the staff, one still sees considerable variation in the sizeand shape of the characters between fonts. It might be pos-sible to develop anomnifontapproach [2,3], meaning a textmodel that is trained from a variety of fonts, and thus capa-ble of recognizing this same variety. However, as charactermodels are required to accept a wider range of presenta-tions for each given letter, they become less capable of dis-612Figure 1. One page with an 8 staves system and an 11staves system.tinguishing between different letters. For this reason om-nifont models generally perform worse than models tunedfor the speciﬁc task at hand.Borrowing a well-known idea from pattern recognition,[11, 13], we address this challenge bysimultaneouslyrec-ognizing the instrument labels on the entire collection ofsystems in a scoreandlearning the font model for the doc-ument at hand. Our algorithm iteratively recognizes thesystems, then retrains the font models using the optimallabeling and text alignments produced in the recognitionphase. One might expect that this approach is simply toogreedy to succeed, failing to explore the high-dimensionalworld of possible character models and system interpreta-tions, while almost guaranteed to get stuck in a mediocrelocal optimum. However, we present experiments that showa largemonotonicimprovement in recognition accuracy aswe iterate this process, culminating with excellent recog-nition results. The approach is feasible due to the highlyrestrictive assumption made by our graphical model, thusconstraining the admissible interpretations to a tiny frac-tion of those arising without this restriction. Our experi-ments demonstrate that this graph model is the differencebetween basically successful and unsuccessful results.2. LABELING STA VES WITH INSTRUMENTSThe usual notational convention for large-ensemble scoreslists all instruments on the ﬁrst page of a piece or move-ment, whether or not the instruments play in this page [10].\nFigure 2. A directed graph representing the possible or-derings for the instruments.In subsequent pages, instruments, perhaps in abbreviatedform, are written in the left score margin. Usually the in-strument labels are displayed immediately to the left of theassociated staff line, though the labels sometimes describecollectionsof staves, such as “Strings”, “Horns”, etc. Insuch a case the text label usually appears centered with re-spect to the group of staves, often emphasized by a brack-eting of the associated staves in the left margin of the score.Figure 1 shows a typical example.2.1 THE MODELOur staff labeling procedure requires input from the userexplaining the labeling scheme(s) used in the score at hand.Typically, the instruments appearing in a system are a sub-sequence of the order given on the ﬁrst page of the score.However, variations are possible such as substituting a col-lective name for the individual labels, e.g. using the sin-gle text label “Strings” instead of the the individual la-bels “Violin 1”, “Violin 2”, “Viola”, “Violoncello” and“Bass” [10]. We assume that the possible labelings can bedescribed by a directed graph,G, as in Figure 2, where thepossible paths through the graph give the legitimate labelsequences. We assume the graph is supplied by the usereither implicitly or explicitly. Each vertexg2Gis asso-ciated with an instrument,I(g), so recovering the correctpath will give the sequence of instruments employed in thesystem.As mentioned above there may be several staves as-sociated with a particular instrument or group of instru-ments, though we do not require such a convention to befollowed consistently. We rely on the user to list, for eachinstrument, the possible labeling variations encountered inthe score. We describe this information as a collectionofpatternsfor each graph node,P(g)={p1,...,pc},where we suppress the dependence of the list length,c,ongin our notation. Each pattern,phas three attributes:p=(pk,pl,pa)giving the number of staves used for theinstrument(s),pk, the location of the text label with respectto the group of staves,pl, and the speciﬁc character se-quence used for the text label,pa. For instancepl=0would mean that the label appears in the middle of thegroup of staves (next to the middle staff ifpkis odd andbetween the middle two staves ifpkis even). Whenpl6=0,plgives the integral number of inter-staff half spaces aboveor below the middle location where the text will be found.In Figure 1pl=0for all instruments.Every pair of adjacent vertices,g0,gare connected byProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 613|P(g)|directed arcs labeled with the various patterns,P(g),though these are not explicitly drawn in Figure 2. Thusthere may be several arcs that connectg0tog, each ac-counting for a possible number of staves for the instrument,I(g), location of the text label, and the actual text itself.There is a one-to-one correspondence between the allow-able labelings of each system and the legal paths throughthe graph. That is, ifs, g1,...,gMis a path beginning fromthe start vertex,s, with arc labelsp1,...,pMthat correctlyaccounts for the number of staves in the system,N,MXm=1pkm=N(1)then the labeling associates the ﬁrstpk1staves with instru-mentI(g1), the nextpk2staves with instrumentI(g2)andso on. The path may terminate anywhere in the graph otherthan at the start vertex,s, as long as Eqn. 1 is satisﬁed.2.2 RECOGNIZING THE TEXT LABELINGWe score every legal path throughGas a sum of arc scoresand compute the best scoring path through dynamic pro-gramming (DP). For this purpose we deﬁneE(n, g), forn=0,...,N,g2G, as the best scoring interpretationof the ﬁrstnstaves ending in stateg. We computeEbyinitializingE(0,s)=0, then visiting the staves in order:n=1,...,Ncomputing for eachg2G,E(n, g) = maxg0p!gE(n\u0000pk,g0)+L(n, p)(2)where the maximum is over all legal arcs going fromg0togwithpkn. In Eqn. 2L(n, p)is the arc score measuringthe plausibility that the text labelpapositioned at relativepositionplis used for stavesn\u0000pk+1,...,n. While wepresentLin more detail in Section 2.3, for now it sufﬁcesto say thatLmeasures the quality of the best match of thetext,pa, to the score image data in the area determinedbyn, pk,pl. It is worth noting thatL=0is a neutralresult, meaning that the optimal placement of the letters ofpaexplains the data as well as a background model. Incontrast, positive (negative) scores ofLindicate evidencefor (against) the labeling implied by the transition ofg0p!g. Thus our algorithm has no inherent bias for assigningmore or less text labels in the optimal interpretation.Having computedE(n, g)forn=1,...,Nandg2G,the score of the optimal path is given bymaxg2GE(N,g),while it is a simple matter to recover the optimal sequenceof vertices and transitions that produce the optimal score.We denote these byg⇤1,...,g⇤Mandp⇤1,...,p⇤M.2.3 CHARACTER RECOGNITIONOur approach to character recognition is standard template-based [7, 8], and will only be discussed brieﬂy and infor-mally here.L(n, p)evaluates the quality of the hypothesisn, p. The information contained innandpcollectively de-scribes a reasonably precise vertical location in the image.The task in computingLis to search the area around thisposition for the optimal locations of the characters ofpa,subject to reasonable constraints regarding their spacing.Suppose the characters ofpa:c1,...cLhave rectangu-lar templatesm1,...,mLwhich are hypothesized to beplaced at image locations(x1,y1),...,(xL,yL). Each tem-plate is a matrix of valuesml(i, j)2MwhereM={b, w, t, n}indexesblack,white,transitional, andnullgreylevel probability models denoted byPb,Pw,Pt,Pn.Pbmostly “expects” to see low grey levels,Pw“expects” tosee high grey levels,Ptis a mixture of these two models,andPnis anullorbackgroundmodel taken as the normal-ized grey level histogram of the entire image.Lx,y(n, p)isthen deﬁned to be the normalized data log likelihood givenbyLx,y(n, p)=LXl=1Xi,jlogPml(i,j)(J(xl+i, yl+j))Pn(J(xl+i, yl+j))(3)where(x,y)denotes the entire collection of template lo-cations,J(x, y)is the image grey level at pixel(x, y)andthe inner sum uses the range fori, jappropriate for thelthrectangular character template.In computing Eqn. 3 we consider a variety of possiblevertical positions for the text baseline, and all reasonablepositions for the characters along that baseline so thatL(n, p) = maxx,yLx,y(n, p).Thus the computation consists of a loop over baseline po-sitions with each iteration accomplished by a DP compu-tation that optimally locates the character templates.2.4 SIMULTANEOUS OPTIMIZATIONSection 2.2 gives our procedure for ﬁnding the optimal textlabeling for the staves of a system. Computing this label-ing requires at least reasonable character templates, thoughit would be preferable to have templates that represent thefont at hand. Unfortunately, fonts differ greatly from onemusic document to another, both in size and shape, so wehave no way of knowinga priorithe font used for instru-ment names in any given score. Our approach here is tosimultaneouslyestimate both the optimal text labeling andthe optimal character templates, thusadaptingto the font athand while we recognize. While simultaneous estimationof both interpretation and model parameters is infeasiblefor many recognition problems, we rely here on the stronggraph-based assumptions we have made on the family ofpossible labelings. In essence, our assumptions about in-strument order are powerful enough to get reasonable esti-mates of the instrument labels even with poorly speciﬁedcharacter templates. Thus we can use this labeling, and theprecise character template positions that come with it, tore-estimate our character templates. Our overall approachthen becomes an iteration between the (re)estimation ofinstrument labels and the (re)estimation of character tem-plates, similar with [5, 6]. In practice this approach con-verges after only a few iterations, and usually does so withsigniﬁcantly better recognition accuracy than with the orig-inal character templates, as discussed in Section 3.614 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015More precisely, we letzdenote a possible text labelingfor the entire collection of system staves. Thuszincludesa path of vertices and arcs throughGforeachsystem inthe score, as discussed in Section 2.2, as well as the char-acter template positions that result from theL(n, p)com-putations of Section 2.3. Let✓denote the complete col-lection of character templates employed in Section 2.3, forall letters and punctuation used in the text labels. Finallywe let¯E(z,✓)be the summed data log likelihood score ofEqn. 3 produced by evaluating the complete set of recog-nized characters inzat their precise positions using thecharacter templates of✓.Starting from our initial character templates✓0, the ba-sic two-stage iteration of our algorithm is thenzl+1= arg maxzE(z,✓l)(4)✓l+1= arg max✓E(zl+1,✓)(5)forl=0,1,.... The update forzin Eqn. 4 is simply thedynamic programming procedure from Section 2.2 appliedto each system in the score, which is guaranteed to producea global optimum forz. The✓update is accomplished bymaximum likelihood estimation, as follows. Suppose ouralphabet of characters and punctuation isc1,...,cQ. Alsosuppose thatcqappears at locations(xq1,yq1),...,(xqR,yqR)as deﬁned through the text labeling and implicit templatealignment of Section 2.2. We then letcq(i, j) = arg maxµ2MRXr=1logPµ(J(xqr+i, yqr+j))Pn(J(xqr+i, yqr+j))forq=1,...,Q, which is, by deﬁnition, Eqn. 5.The proposed algorithm is simply coordinate-wise opti-mization overzand✓, which guarantees that the sequenceE(z1,✓1),E(z2,✓2)...is non-decreasing. Furthermore,the sequence is guaranteed to converge due to our ﬁnite(but large) domain. In practice, this happens in only a fewiterations.It is worth noting that our strategy is different from theusual EM scheme [4] for performing maximum likelihoodestimation of model parameters. To implement EM wewould need a probabilistic model for the graph transitions,which would be easy to supply, but absent from our cur-rent formulation. However, EM attempts to increase themarginaldata likelihood rather than the likelihood of theoptimal path. Thus, if we were to replace our parameterestimation step by an iteration of EM, we still may endup decreasing our objective function. That said, EM isless greedy in its approach than our proposed algorithm,which, in principle, seems like a good attribute. In prac-tice, we doubt there would be any signiﬁcant difference inthe performance of these two approaches.3. EXPERIMENTS AND RESULTSTable 1 describes the collection of scores used in our evalu-ation, all obtained from the IMSLP website [1], consistingof 10 scores from 6 different publishers. In our evaluationwe used about 20 pages from each score.scoreIMSLPpagesPublisherindexID12483150-69New York: Charles Foley2036312-21Moscow: Muzgiz/Muzyka3654602-18,21,22,234005692-19Leipzig: Breitkopf & H¨artel5318752-216010862-207063072-21Leipzig: Ernst Eulenburg8001912-219085352-21Vienna: Universal Edition10073542-25Berlin: SchlesingerTable 1. Information about the scores.\nFigure 3. Two kinds of regions for possible text positions.Our OMR system begins by computing the locations ofthe staves and the partition of staves into systems for eachpage of the score. These systems constitute the input to oursystem.In all cases we use a graphical model based on the ﬁrstpage of the score, allowing for any subsequence of thenamed instruments, as in Figure 2. Several scores allow forthe collective labeling of the strings with a single text tag,rather than an enumeration of instruments. For eachg2G(i.e. each instrument) we supply the appropriate patterns,P(g)={p1,...,pc}, by hand. Some instruments havetwo patterns, though most have only one. For all of theinstruments and scores in our test set we only consider pat-terns wherepl=0meaning that the text must lie in themiddle of the collection of staves associated with an instru-ment. Referring to Figure 3, this means we search in thetext in region 1 whenpk=1, and region 2 whenpk=2,with obvious extensions to larger staff groupings. All ofour test scores place instrument names in the left margin,though our approach easily accommodates other possiblepositions. We also supply the text strings,pa, and the num-ber of staves for each pattern,pk. Even with instrumentshaving two patterns, both used the same text (pa) in ourmodels.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 615scorenumbernumber of errorsindexof staves✓0✓1✓2✓313991942123951303317211040433144053911181644333207273321083643409381407252453810436✓4✓5✓6✓725232221Table 2. Total number of staves and number of errors ineach iteration for each score.3.1 ORIGINAL TEMPLATESThe character template set includes all the (case sensitive)letters used in the instrument names of the 10 scores, inaddition to a comma, hyphen, period, and space, giving 34characters in all. Each score uses a subset of this collectionin labeling instrument names. We create our original setof templates,✓0, by, for each character, randomly choos-ing an example from one of the 10 scores and thresholdingthe grey levels to choose probability models for each pixel.This collection will be the initial conﬁguration for all 10scores before we begin the adaptation process. We sam-pled from the test scores as a way of ensuring we couldﬁnd examples of all the required templates, and note that,on average, only a tenth of these initial templates comefrom any individual score. A better scheme might use anomnifont model as our starting place.We simultaneously estimate the staff labelings and thetrained collection of character templates, iterating the ap-proach of Section 2.4 until the results converge. Table 2lists the total number of staves assigned incorrect instru-ment labels after each iteration of the algorithm, for eachof the 10 scores. As shown in the table, 7 scores correctlylabeled all the staves, 2 scores have only one labeling error,while the 10th score has 21 out of 436 staff labeling errors(4.82%), which is still low. The algorithm converged onall scores within 4 iterations, except for the the score con-taining the most errors, which used 8 iterations, as shownin the table.As the original set of templates,✓0, comes from a vari-ety of different scores with different fonts and sizes, theydon’t match any particular score font well. Our hope isthat, through our iterative training process, the templateswilladaptto the current score. Figure 4 gives an exam-ple from Score 6 with the original and learned charactersdrawn on top of the score image at their estimated loca-tions. Clearly the original templates matched the actualfont poorly, especially in size, as can be seen in the mid-dle panel of the ﬁgure. The right panel of the ﬁgure showsthe analogous result after two iterations of recognition and\nFigure 4. Comparing templates before (middle) and after(right) training. The instrument names in the actual score(left) are “Fl.”, “Ob.” and “Klar.” in order.retraining. After this process most of the character tem-plates match the font better, though not all of them. Dueto the greedy nature of our algorithm it is necessary thatour original templates, hence the original recognition andmatch, are close enough to pull the result into the correctlocal optimum. In this case the ‘O’ and ‘b’ in “Ob.” weremisspeciﬁed and consistently matched poorly in the ﬁrstiteration. As these characters don’t appear in other instru-ment names, there was no counteracting force helping toguide the models toward reasonable results, thus the out-come of the ﬁgure.Although some of the trained results don’t look partic-ularly good, Table 2 shows striking improvement in stafflabeling due to training,showing monotonic decrease inthe number of errors. Here is where the strength of thegraphical model comes into play. Even with the poorlyspeciﬁed character models for “Ob.”, this is the only in-strument name that can come between “Fl.” and “Klar.”for which we have good models. This leads to the correctlabeling in spite of the uneven training.There are three scores having errors in our experiments.The one error in Score 1 is caused by unrelated text appear-ing in the left margin which was recognized as an incorrectinstrument name. The one error in Score 5 mistakes “Vc.”as “Vla.” with “l” and “a” squeezed together. This hap-pens in a three-staff system, thus the constraints imposedby graph ordering are less potent.For Score 10, the errors are caused by badly trainedtemplates. 8 out of 19 templates used in this score con-verged into unrecognizable glyphs. We suppose this hap-pens because the font size of this score is obviously smallerthan other scores and thus harder to adapt to. But surpris-ingly, this score still has reasonably accurate instrumentlabeling, which is our objective.3.2 DROPPING THE GRAPHICAL CONSTRAINTFor comparison we ran a similar experiment without the or-dering constraint imposed by the graph, thus allowing anygroup of staves to be labeled with any instrument. In thiscase all instrument orders are possible, even allowing forrepetition of instruments. The results are shown in Table 3.After four iterations, the number of errors doesn’t seem to616 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015scorenumbernumber of errorsindexof staves✓0✓1✓2✓3539178716973644359494857836410291103103Table 3. Total number of staves and number of errors ineach iteration for 3 scores without the graphical orderingconstraint.decrease. This is because many of the trained templates be-come unrecognizable — some of them become pure whitespace! Without the strong ordering constraint there is lessgravitational pull toward the desired optimum, while weimagine the joint space of interpretations and models to beﬁlled with local optima.3.3 WORD MODELS VS. CHARACTER MODELSOut of curiosity, we modiﬁed our model to view the in-strument names as single rigid glyphs rather than characterbased models that allow for some ﬂexibility in the place-ment of individual characters. Our experiments (not pre-sented here) show that this approach works well when theactual document is consistent with the assumption we aremaking, but fails badly otherwise. Given the wide varietyof typographical conventions encountered in music scores,we don’t recommend this approach.4. CONCLUSIONSWe have presented a method of interpreting the instrumentname labels, which are a common way of labeling staves inlarge ensemble scores, showing nearly perfect recognitionin all but one of the test scores we examined. The unusualaspect of our approach is that we simultaneoulsy estimateboth the labels we seek, as well as the text font used forthe score. The experiments show convincing evidence thatthe strong assumption we make regarding possible label-ings is powerful in practice and largely responsible for thereliability of the approach. In future work we will considerinitial text models trained from a large variety of scores,as well as feature based, rather than template based, datamodels.5. REFERENCES[1]IMSLP website.http://imslp.org.[2]Henry S Baird and George Nagy. Self-correcting 100-font classiﬁer. InIS&T/SPIE 1994 International Sym-posium on Electronic Imaging: Science and Technol-ogy, pages 106–115. International Society for Opticsand Photonics, 1994.[3]Issam Bazzi, Richard Schwartz, and John Makhoul.An omnifont open-vocabulary OCR system for englishand arabic.Pattern Analysis and Machine Intelligence,IEEE Transactions on, 21(6):495–504, 1999.[4]Jeff A Bilmes et al. A gentle tutorial of the EM algo-rithm and its application to parameter estimation forgaussian mixture and hidden markov models.Interna-tional Computer Science Institute, 4(510):126, 1998.[5]Gary E Kopec and Mauricio Lomelin. Document im-age decoding approach to character template estima-tion. InImage Processing, 1996. Proceedings., Inter-national Conference on, volume 1, pages 213–216.IEEE, 1996.[6]Gary E Kopec and Mauricio Lomelin. Document-speciﬁc character template estimation. InElectronicImaging: Science & Technology, pages 14–26. Inter-national Society for Optics and Photonics, 1996.[7]Ayatullah Faruk Mollah, Nabamita Majumder, Sub-hadip Basu, and Mita Nasipuri. Design of an opticalcharacter recognition system for camera-based hand-held devices.CoRR, abs/1109.3317, 2011.[8]Shunji Mori, Ching Y Suen, and Kazuhiko Yamamoto.Historical review of OCR research and development.Proceedings of the IEEE, 80(7):1029–1058, 1992.[9]Christopher Raphael and Jingya Wang. New ap-proaches to optical music recognition. InISMIR, pages305–310, 2011.[10]G. Read.Music Notation: A Manual of Modern Prac-tice.[11]Zheng Song, Qiang Chen, Zhongyang Huang, YangHua, and Shuicheng Yan. Contextualizing object de-tection and classiﬁcation. InComputer Vision and Pat-tern Recognition (CVPR), 2011 IEEE Conference on,pages 1585–1592. IEEE, 2011.[12]Verena Thomas, Christian Wagner, and MichaelClausen. OCR based post processing of OMR for therecovery of transposing instruments in complex or-chestral scores. InProceedings of the 12th Interna-tional Society for Music Information Retrieval, pages411–416, 2011.[13]Frank Wessel and Hermann Ney. Unsupervised train-ing of acoustic models for large vocabulary continu-ous speech recognition.Speech and Audio Processing,IEEE Transactions on, 13(1):23–31, 2005.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 617"
    },
    {
        "title": "Graph-Based Rhythm Interpretation.",
        "author": [
            "Rong Jin 0004",
            "Christopher Raphael"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418287",
        "url": "https://doi.org/10.5281/zenodo.1418287",
        "ee": "https://zenodo.org/records/1418287/files/JinR15.pdf",
        "abstract": "We present a system that interprets the notated rhythm ob- tained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the con- nected graph. The graph identification problem is cast as an optimization where each potential edge is scored ac- cording to its plausibility. We seek the optimally scor- ing graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhyth- mic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets.",
        "zenodo_id": 1418287,
        "dblp_key": "conf/ismir/JinR15",
        "keywords": [
            "system",
            "interprets",
            "notated",
            "rhythm",
            "optical",
            "music",
            "recognition",
            "graph",
            "edges",
            "optimization"
        ],
        "content": "GRAPH-BASED RHYTHM INTERPRETATIONRong JinIndiana UniversitySchool of Informatics and Computingrongjin@indiana.eduChristopher RaphaelIndiana UniversitySchool of Informatics and Computingcraphael@indiana.eduABSTRACTWe present a system that interprets the notated rhythm ob-tained from optical music recognition (OMR). Our approachrepresents the notes and rests in a system measure as thevertices of a graph. We connect the graph by adding voiceedges and coincidence edges between pairs of vertices, whilethe rhythmic interpretation follows simply from the con-nected graph. The graph identiﬁcation problem is cast asan optimization where each potential edge is scored ac-cording to its plausibility. We seek the optimally scor-ing graph where the score is represented as a sum of edgescores. Experiments were performed on about 60 scorepages showing that our system can handle difﬁcult rhyth-mic situations including multiple voices, voices that mergeand split, voices spanning two staves, and missing tuplets.1. INTRODUCTIONPast decades have seen a number of efforts on the problemof Optical Music Recognition (OMR)with overviews of thehistory and current state of the art found at [2, 3, 8, 14].OMR can be divided into two subproblems: identifying themusic symbols on the page and interpreting these symbols,with most efforts devoted to the former problem [7,13,16].However, the interpretation problem is also important forgenerating meaningful symbolic representations. In thispaper, we focus on the rhythm interpretation of musicalsymbols, which appears to be the most challenging inter-pretation problem.Many OMR systems [11] perform some sort of rhythminterpretation in order to play back and verify the recog-nized music symbols. When there are not enough notesor too many notes to match the meter of the measure, theOMR system often “ﬂags” the measure to suggest that thereis something wrong, alerting the user to correct the mea-sure. In this way, rhythm interpretation is used as a check-ing tool for correcting recognized scores.There are a few research efforts that correct recogni-tion results automatically. Droettboom [6] proposed met-ric correction as part of an OMR system. Using the factc\u0000Rong Jin, Christopher Raphael.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Rong Jin, Christopher Raphael.“Graph-Based Rhythm Interpretation”, 16th International Society forMusic Information Retrieval Conference, 2015.\nFigure 1. Three system measures from Rachmaninoff Pi-ano Concerto No.2 showing some of the difﬁculties in in-terpreting rhythm. All three measures are in 4/4 time.\nFigure 2. Two system measures from Rachmaninoff Pi-ano Concerto No.2 showing some of the difﬁculties in in-terpreting rhythm. Both are in 4/4 time.343that rhythmically coincident notes are usually aligned ver-tically, this work applies different corrections on inconsis-tent notes. Church [5] proposed a rhythmic correction witha probabilistic model that converts the rhythm of a suspi-cious measure to the most similar measure in the piece.Byrd [4] proposed improving OMR with multiple recog-nizers and sequence alignment.The approaches mentioned above work for simpler sit-uations such as monophonic music or measures withoutcomplex tuplets. However, some music scores, especiallythose for piano, are ﬁlled with rhythmically challengingsituations such as missing tuplets or voices that come andgo within a measure. Simple approaches are likely to failon a signiﬁcant proportion of these measures.Our paper differs from other work we know by address-ing the most challenging examples usingcompleteinfor-mation (the system measure), instead of trying to correctthe misrecognized symbols. Our research questions are:given perfect symbol recognition is the system able to un-derstand rhythm as a human would? When there are multi-ple voices interwoven in one measure, can the system sep-arate the voices? When there are implicit symbols suchas omitted rests and missing tuplets, can the system stillinterpret correctly?Figures 1 and 2 show some challenging examples thatillustrate the problem we address. The left measure in Fig-ure 1 shows an example using multiple voices. When mul-tiple voices are present it is nearly impossible to interpretrhythm without identifying these voices as such. In an ef-fort to avoid overlapping symbols, some notes in the mea-sure that ideally should align vertically do not. The mid-dle measure in Figure 1 shows an example of missing tu-plets (tuplets are not labeled). What is most unusual, andwould likely go unnoticed by anyone other than an OMRresearcher, is that these beamed groups would normally bewritten with two beams rather than one, though the mean-ing is still clear. In addition, the 9-tuplet is not explicitlyindicated with a numeral — a common notational conven-tion.The right measure in Figure 1 shows another exampleof missing triplet for the 3 beamed eighth notes in the ﬁrststaff, as well as a quarter note plus an eighth note pair in thesecond staff. A further complication is that this measureis, in some sense, incomplete, as the voice in the secondstaff jumps onto the ﬁrst staff on the second quarter andthen jumps back on the third quarter. The left measurein Figure 2 demonstrates an example of special beamingof a sextuplet where the ﬁrst eighth note is separate fromﬁve beamed eighth notes. The right measure in Figure 2demonstrates an example where all four beamed groupsare triplets while the voice jumps back and forth betweenthe two beamed groups.The examples all seem innocent until one considers theassumptions on rhythm notation that must underlie an in-terpretation engine. One quickly comes to see that typicalin vivonotation contains a fair amount of “slang” that maybe readily understood by a person familiar with the idiom,but is much harder for the machine. [9] has more demon-strations of such ”slang” in music scores.In this paper we present an algorithm that is generallycapable of correctly interpreting notation such as the ex-amples we have discussed. In our presentation, Section 2introduces our rhythm graph and optimization on the graphscore. In Section 3, we present our experiments on threescores and discuss the results.2. METHODS2.1 InputWe ﬁrst perform optical music recognition with ourCeres[12] OMR system taking the score image as input. Theoutput is stored as a collection of labeled primitive sym-bols such as solid note head, stem, beam, ﬂag, and etc.,along with their locations. The user deletes or adds prim-itive symbols using an interactive interface. Editing sym-bols at the primitive level allows us to keep useful infor-mation such as stem direction and beaming as well as theexact primitive locations which are important for rhythminterpretation.After this correction phase, we assemble the primitivesymbols into meaningful composite symbols (chords andbeamed groups). This step is done in a simple rule-basedmethod. Each note or rest is assigned to the staff measureit belongs to.2.2 Rhythm GraphWe form a graph of the rhythmically relevant symbols foreach system measure. The set of vertices of the graph,which we denote asV, are the notes, rests, and bar linesbelonging to the system measure. All vertices are givena nominal duration represented as a rational number. Forexample, a dotted eighth would have nominal length 3/16,while we give the bar lines duration 0. Sometimes the ac-tual vertex duration can differ from the nominal length, aswith missing tuples. In these cases, we need to identifywhich symbols are tuplet symbols in order to interpret therhythm correctly.Vertices can be connected by either voice or coinci-dence edges, as shown in Figure 3. Voice edges, whichare directed, are used for symbols whose position is under-stood in relation to a “previous” symbol, as in virtually allmonophonic music. That is, the onset time of a symbol onthe “receiving” end of a voice edge is the “preceding” sym-bol’s onset time plus duration. Coincidence edges link ver-tices that share the same onset time, as indicated by theircommon horizontal location. Using these edges we can in-fer the onset time of any note or rest connected to a barline. We denote byEthe complete collection of all possi-ble edges.We formulate the rhythm interpretation problem as con-strained optimization. Given the set of vertices,V, and pos-sible edges,E, we seek the subset ofE,E⇤, and the label-ing ofVthat maximizesH=Xe2E⇤\u0000(e)+Xv2V'(l(v))(1)344 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015where function\u0000(e)represents how plausible each edgesis according to the music rules,llabels vertexvas tuplet ornon-tuplet, and function'(l)penalizes labeling vertices astuplet so as to favor simple interpretations whenever pos-sible. The subsetE⇤and labeling are constrained to con-struct a consistent and connected graph.2.3 Constructing edgesWe construct the graph beginning with the left bar line(which has an onset time of 0), by iteratively connectingnew vertices to the current graph with voice and coinci-dence edges until all vertices form a single connected com-ponent. More speciﬁcally, we connect the current vertexwith a voice edge to a previously visited vertex. This vertexhas to be either a bar line or a vertex in the same staff mea-sure. (Piano staves are treated as one staff because voicesoften move between left and right hand parts.) This newvoice edge deﬁnes a unique onset for the current vertex.Then we add coincidence edges between the current ver-tex and all past vertices so that both have nearly the samehorizontal position and have the same onset time. We mayalso add coincidence edges between the incoming vertexand a past vertex having adifferentonset time, leading to aconﬂict that must be resolved, as discussed in Section 2.4.Different combinations of edges give different onset timesto the vertices.As an edgeeis introduced to the graph we score it ac-cording to its plausibility\u0000(e). There are different kindsof musical knowledge [15] we hope to model in comput-ing these scores, as follows.1.The left bar line has an onset time of 0. The right barline has an onset time of the measure’s meter. Novertices can have onset times greater than the meter.2.The onset times must be non-decreasing in the hor-izontal positions of the symbols in the image. Thatis, if vertex A lies to the left of vertex B it cannothave an onset that is after that of vertex B.3.A vertex has a unique onset time. Thus, if multiplepaths connect a vertex to the graph they must givethe same onset time.4.Vertices connected by coincidence edges should havethe same approximate horizontal position in the im-age. Vertices with the same horizontal image posi-tions should should have the same onset time.5.Vertices in a beamed group note are usually con-nected by voice edges, while we penalize voices thatexit a beamed group before it is completed.6.Vertices connected by a voice edge usually have thesame stem direction and tend to appear at similarstaff height.The ﬁrst two rules above are hard constraints thatmustbe followed. When they are violated our algorithm sim-ply will not add the offending edge. The other rules canbe violated for different reasons. For example, symbolshaving the same onset time may not align in the image be-cause one is moved to avoid overlap with other symbols, orbecause the image is skewed or rotated through the scan-ning process. Such violations lead to penalties of the edgescores.2.4 Conﬂict Resolution by ReinterpretationIf we disregard the right bar line and construct a spanningtree from the remaining vertices we are guaranteed that ev-ery vertex can be reached through a unique path startingfrom the left bar line, thus giving each vertex a unique on-set time. While this approach has the compelling appealof simplicity, it would fail in any case where the nominalnote length is not the correct interpretation, as with miss-ing tuplets. Instead, we identify such cases by allowingmultiplepaths to a vertex, and thus multiple rhythmic in-terpretations. When the result of these multiple paths givesconﬂicting onset positions for a vertex we consider reinter-preting some notes in terms of missing tuplets to resolvethe conﬂict. In such a case we treat the earlier onset timeas the correct one, while reinterpreting the path leading tothe later onset time. This is because the nominal length ofa tuplet note is usually greater than the true length. Whilethere are exceptions to this rule, as with duplets in triplemeter, we do not treat such cases in the current work.As an example, consider the situation in Figure 3. Herethe ﬁrst coincidence edge considered (dotted line in the 1stgraph) does not create any conﬂict since both paths givethe onset position of 1/4. However, the coincidence edgefor the quarter note on the top staff (dotted line in the 2ndgraph) gives the onset time of 1/2 while the voice edgegives the onset time of 5/8, thereby generating a conﬂict.Thus we must reinterpret the path giving the later onsettime of 5/8 to be consistent with the onset time of 1/2. Inthis case the desired interpretation is that the three eighthnotes form an implicit triplet, and thus have note lengthsof 1/12 rather than 1/8 (bottom graph). Another exampleof a conﬂict arises when a voice edge links to the right barline and attributes an onset time for the bar line other thanits true position (which is the meter viewed as a rationalnumber). In this case we must reinterpret the path leadingto the right bar line.When reinterpreting we must consider the path that gen-erates the onset position in conﬂict — but how far back-ward should we go? The collection of reinterpretable ver-tices could spill over into multiple voices and staff mea-sures, thus generating an intractable collection of possibil-ities to consider. Here we make some simplifying assump-tions to keep the computation from becoming prohibitivelylarge. First of all, recall that we consider the staff mea-sures of a system one at a time. After a staff measure iscompletely analyzed and reduced to a single interpretation,we do not consider future reinterpretations of the measure.Thus reinterpretation is conﬁned to the current staff mea-sure (or two staves in the case of the piano). Furthermorewe do not allow the reinterpretation process to generate ad-ditional inconsistencies. This rules out the reinterpretationof any vertex connecting to a measure in a previously ana-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 345lyzed staff measure. Even with these restrictions the com-putation can still be signiﬁcant, as we must simultaneouslyconsider the possibility of a number of different tuplet hy-potheses, thus requiring an effort that is exponential in thenumber of hypotheses.One might contrast this approach with a purely top-down model-based strategy that considers every possiblerhythmic labeling. Such a strategy would be our prefer-ence if computationally feasible, and, in fact, was the ap-proach we explored in [10]. The problem is that there are,a priori, a large enough collection of possible labelingsso that, when coupled with unknown voicing, the compu-tation does not always prove tractable. This is why weuncover candidates for reinterpretation prompted by coin-cidence edges. Thus the modeling of our algorithm liessomewhere between top-down and bottom-up recognition.It is model-based, yet it relies on the data itself to promptcertain data interpretations. While not necessarily an argu-ment in favor of our approach, this appears to be a centralpart of the human strategy for rhythm understanding.We consider several cases of reinterpretation:1.A beamed group can be reinterpreted as a beamedtuplet note of simple duration (1/2, 1/4, etc.), as inthe left measure of Figure 2.2.Three consecutive vertices that add up to 3/8 couldbe reinterpreted as missing triplet of total length 1/4,as in the middle measure of Figure 2. This rulecan be generalized to include other kinds of triplets(quarter note or sixteenth note) and to include tupletsother than 3.3.We cangloballyreinterpret all vertices along the voicepath, as in the right measure in Figure 2, meaningthat all note lengths are rescaled to create the desiredcollective duration.The score function'(l(v))in Eqn (1) penalizes the com-plexity of a reinterpretation, thus favoring simple interpre-tations whenever possible.2.5 Dynamic Programming for OptimizationDuring graph construction, each time we add a new vertexinto the graph we consider adding voice edges between thenew vertex and all vertices already in the graph. Thus, onlyconsidering the voice edges, the number of possible graphswithnvertices would ben!. Since a common system mea-sure may have more than 50 vertices, it is computationallyinfeasible to search the whole graph space. This situationcan be improved by dynamic programming: after any newvertex has been added to the graph, if two different graphsgive identical onset times for each vertex we prune the onewith lower score.The order in which the vertices are considered is im-portant in producing a feasible search. One way wouldbe to visit all vertices in the system measure according totheir horizontal location on the image. The problem withthis approach is that the constraints imposed by the right\nFigure 3. Constructing the rhythm graph of an examplemeasure. Voice (red) and Coincidence (Purple) edges areautomatically constructed to identify the onset time of ver-tices (notes, rests and bar lines).346 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015bar line, which has a known onset position, do not comeinto play until the very end of the graph construction. Analternative ﬁrst considers the vertices in left-right mannerfrom a single staff measure, then continuing one-by-onewith the other staff measures. Each time a staff measureis completed we continue only with the best scoring singlegraph. In this way, we will greatly reduce the number ofpartial graphs we carry through the process.Among all measures in our experiments the maximumnumber of graph hypotheses we encounter during the DPcomputation is usually less than 100, even in the systemmeasures with 50 to 60 vertices. The measures posing thegreatest computational challenge are those having multi-ple voices, missing tuplets, and, at the same time, similarrhythm between the voices. The left example measure inFigure 4 shows such a case. It may seem easy for a per-son to recognize that there are two voices in the ﬁrst staff.Here four quarter notes form one voice, and four pairs oftriplets, consisting of an eighth rest and two eighth notes,form another voice. However, it’s not an easy task for acomputer. The second staff measure doesn’t provide muchinformation since it also has the similar missing tupletswhich are hard to distinguish from nominal rhythm untilone encounters the right bar line. Other measures in thesame system also don’t provide aligned symbols to anchorthe search. The number of graph hypotheses for this sys-tem measure grows up to 2600 at the end of the measure.This measure represents the maximum number of hypothe-ses attained throughout our experiments. This is still easilyfeasible computationally.3. EXPERIMENTSIn the experiments, we have chosen three different scoresof varying degrees of rhythmic complexity for evaluation,all taken from the IMSLP [1].3.1 Rachmaninoff Piano Concerto No.2The orchestra score of Rachmaninoff Piano Concerto No.2 is a highly challenging example for our rhythm inter-pretation algorithm. The score has 371 system measures,with each system measure containing up to 15 staff mea-sures. The piece covers different types of rhythmic dif-ﬁculties such as polyphonic voices, missing tuplets, andvoices moving between staff measures. In addition somepages of the score are rotated and skewed due to the scan-ning process, creating difﬁculty detecting coincidence be-tween notes.We get 355 out of 371 (95.7%) system measures cor-rectly. In the following paragraphs, we will discuss threerepresentative examples in which our system fails to ﬁndthe correct rhythm.Failure case 1In the left example in Figure 4 we failto interpret all the missing triplets. The result producedby our system did not recognize the ﬁrst and last triplet inthe ﬁrst staff, instead treating those beamed eighth notesas normal eighth notes. The system gives the left eighthnote in the beam the same onset time as the eighth noterest, explaining it as coincidence with the eighth note restsince they almost align vertically. In this case we foundthat the correct interpretation was actually generated byour system, but survives with a lower score. This type ofscenario, where the correct interpretation survives but doesnot win, occurs a number of times in our experiments. Inthis case, the reason is because we give a high penalty fortuplet reinterpretation, while a give comparatively lowerpenalty when allegedly coincident symbols are not per-fectly aligned. Therefore, the state that has fewer tupletsbut worse alignment gets a higher score.Failure case 2The right example in ﬁgure 4 is anotherexample where our system does not produce the correctrhythm. The difﬁculty in this measure is the voice thatmoves between the treble and bass staves of the piano.While we successfully recognized two missing sextupletsin the treble staff, we failed to recognize that the quarternote in treble staff and eighth note in the bass staff forma triplet. In our result, they are interpreted as a normalquarter note and a normal eighth note with the eighth notealigned to the 3rd sixteenth through a coincidence edge.This happens because we impose a penalty for interpretinga missing tuplet, while the eighth note aligns reasonablywell with the third 16th note, providing a plausible expla-nation. However, the isolated eighth note is the only notethat has the wrong onset time. This case also shows thatour algorithm is capable of recovering from local errors toproduce mostly correct results, even though not perfect.Failure case 3Our third incorrect case is shown in theleft of Figure 5. In the ﬁrst staff of this example, the dot-ted half note chord and ﬁrst eighth note in the ﬁrst beamgroup both begin at the start of the measure. However,we have a maximal horizontal distance between two notesthat have the same onset time, which serves the importantrole of pruning graphs graphs that exceed this threshold —usually this is the correct thing to do. In this particularcase these two notes exceeded the threshold, thus we losethe correct interpretation. For such a case, we can alwaysmake the threshold larger, but this weakens the power ofthe alignment rule elsewhere. Of course, there will alwaysbe special cases where our threshold is not large enough.In the right measure in Figure 5, the eighth rest and wholenote “high” c in the ﬁrst staff are very far away from thehalf note in staff three due to the long grace note ﬁgure.Presumably the grace note ﬁgure beginson the beat, so thecoincidence suggested by the score is correct, though thispeculiarity lies outside of the basic modeling assumptionswe have employed: here two notes at the same rhythmicposition are not intended to sound at the same time! Wehave a few other examples of this general type of failure,such as when we can’t compute horizontal distances ac-curately due to image skew. Given the reasons above, wedecide to keep the threshold as strict as it is, because itprovides a signiﬁcant help with keeping the computationtractable.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 347Figure 4. Examples for failure case 1 and failure case 2from Rachmaninoff Piano Concerto No. 2. Both measuresare in 4/4 time.\nFigure 5. Examples for failure case 3 from RachmaninoffPiano Concerto No.2. Both measures are in 4/4 time.\nFigure 6. Two examples from Debussy’s 1st Arabesque.Both measures are in 4/4 time.3.2 Borodin String Quartet No.2, 3rd MovementWe also tested on the 3rd movement (Notturno) from Borodin’s2nd String Quartet. This is a “medium” difﬁculty scoreconsisting of 4 staves for each system. The third movementhas 180 systems measures over 6 pages. 22 out of 180 sys-tem measures contain triplets, and, while all of these areexplicitly notated with numerals in the score, we deliber-ately didn’t include these in our rhythm interpretation pro-cess. The system gets 100 percent correct rhythm on all ofthese measures.3.3 Debussy 1st ArabesqueUsually the more staves in a system, the more coincidenceedges between different staves, thus providing anchors forreinterpretation when needed. Thus solo piano music canbe particularly challenging with only two staves. In mea-sures that are monophonic or homophonic we can’t iden-tify inconsistencies until we reach the end of the measureas both nominal and tuplet hypotheses are consistent withspacing. In order to demonstrate that our system is alsocapable of handling these challenges, we experimented onthe ﬁrst of the two Debussy Arabesques, containing 107measures.This piece has a variety of rhythmic difﬁculties. 73/107(68%) of the system measures have at least one, and up tosix missing tuplets, while 17/107 measures contain voicesmoving between the two staves. This latter category is par-ticularly difﬁcult because the measures are monophonic asin Figure 6, and thus do not provide coincidence clues.Therefore, our algorithm only sees conﬂicts at the end ofthe measure and must reinterpret the entire measure at once.However, our results show that we are generally capable ofhandling such situations. There’s only one measure thatwe don’t get exactly correct as shown in the right of Figure6. In this measure, there are four missing beamed grouptriplets. In our best scoring solution, we found the ﬁrst andlast triplets but are missing the middle two. The correct in-terpretation also survives into the ﬁnal list but with a lowerscore.4. CONCLUSIONWe have presented a graph-based rhythm interpretation sys-tem. Experiments show that given the perfect symbol recog-nition, our system is generally capable of interpreting difﬁ-cult notation involving separating multiple voices and iden-tifying implicit symbols such as missing tuplets. It alsoshows that it’s a difﬁcult and interesting problem and worthfurther exploration. One possibility will be using trainedpenalty parameters for a particular score. A rare notationor rhythmic pattern could appear repeatedly in one score,thus we hope an adaptive model would improve the result.Also, since there are always exceptions in all music-relatedquestions, human interactive methods are another interest-ing direction to explore.348 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20155. REFERENCES[1]International music score library project.http://imslp.org/.[2]D. Bainbridge and T. Bell. The challenge of optical mu-sic recognition.Computers and the Humanities, 2001.[3]D. Blostein, H. Dorothea, and H. Baird. A critical sur-vey of music image analysis.Structured Document Im-age Analysis. Springer Berlin Heidelberg, 1992.[4]D. Byrd. Prospects for improving OMR with multiplerecognizers. InProceedings of the International Sym-posium on Music Information Retrieval, 2006.[5]M. Church and M. Cuthbert. Improving rhythmic tran-scriptions via probability models applied post-OMR.InProceedings of the International Symposium on Mu-sic Information Retrieval, 2014.[6]M. Droettboom, I. Fujinaga, and K. Macmilan. Opti-cal music interpretation. InStructural, Syntactic, andStatistical Pattern Recognition, 2002.[7]I. Fujinaga. Adaptive optical music recognition ph.d.thesis, mcgill university,montreal. 1997.[8]I. Fujinaga. Optical music recogni-tion bibliography.http://www.music.mcgill.ca/ ich/research/omr/omrbib.html, 2000.[9]J. Hook. How to perform impossible rhythms.Societyfor Music Theory, 2011.[10]R. Jin and C. Raphael. Interpreting rhythm in opticalmusic recognition. InProceedings of the InternationalSymposium on Music Information Retrieval, 2012.[11]G. Jones, B. Ong, I. Bruno, and K. Ng. Optical mu-sic imaging: music document digitisation, recogni-tion, evaluation, and restoration.Interactive Multime-dia Music Technologies, pages 50–79, 2008.[12]C. Raphael and R. Jin. The Ceres system for opticalmusic recognition. InInternational Conference on Pat-tern Recognition Applications and Methods, 2014.[13]C. Raphael and J. Wang. New approaches to opticalmusic recognition. InProceedings of the InternationalSymposium on Music Information Retrieval, 2011.[14]A. Rebelo, G. Capela, and J. Cardoso. Optical mu-sic recognition: state-of-the-art and open issues.Inter-national Journal of Multimedia Information Retrieval,2012.[15]G. Reed. Music notation:a manual of modern practice.1979.[16]F. Rossant and I. Bloch. Robust and adaptive OMR sys-tem including fuzzy modeling,fushion of musical rules,and possible error detection. InEURASIP Journal onApplied Signal Processing, 2007.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 349"
    },
    {
        "title": "Evaluating the General Chord Type Representation in Tonal Music and Organising GCT Chord Labels in Functional Chord Categories.",
        "author": [
            "Maximos A. Kaliakatsos-Papakostas",
            "Asterios I. Zacharakis",
            "Costas Tsougras",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417379",
        "url": "https://doi.org/10.5281/zenodo.1417379",
        "ee": "https://zenodo.org/records/1417379/files/Kaliakatsos-Papakostas15.pdf",
        "abstract": "The General Chord Type (GCT) representation is ap- propriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descrip- tive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert an- notations (Kostka & Payne harmonic dataset). Addition- ally, novel methods for grouping and clustering chords, ac- cording to their GCT encoding and their functional role in chord sequences, are introduced. The results of both har- monic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for rep- resenting effectively harmony in computational systems.",
        "zenodo_id": 1417379,
        "dblp_key": "conf/ismir/Kaliakatsos-Papakostas15",
        "keywords": [
            "harmonic context",
            "tone simultaneities",
            "GCT representation",
            "harmonic rearrangement",
            "standard roman numeral chord type",
            "hierarchic harmonic systems",
            "tonal system",
            "post-tonal music",
            "traditional polyphonic systems",
            "GCT encoding"
        ],
        "content": "EVALUATING THEGENERAL CHORD TYPEREPRESENTATION INTONAL MUSIC AND ORGANISING GCT CHORD LABELS INFUNCTIONAL CHORD CATEGORIESMaximos Kaliakatsos-Papakostas, Asterios Zacharakis, Costas Tsougras, Emilios CambouropoulosSchool of Music Studies, Aristotle University of Thessaloniki, Greece{emilios,maxk,tsougras,aszachar}@mus.auth.grABSTRACTThe General Chord Type (GCT) representation is ap-propriate for encoding tone simultaneities in any harmoniccontext (such as tonal, modal, jazz, octatonic, atonal). TheGCT allows the re-arrangement of the notes of a harmonicsonority such that abstract idiom-speciﬁc types of chordsmay be derived. This encoding is inspired by the standardroman numeral chord type labelling and is, therefore, idealfor hierarchic harmonic systems such as the tonal systemand its many variations; at the same time, it adjusts to anyother harmonic system such as post-tonal, atonal music, ortraditional polyphonic systems. In this paper the descrip-tive potential of the GCT is assessed in the tonal idiom bycomparing GCT harmonic labels with human expert an-notations (Kostka & Payne harmonic dataset). Addition-ally, novel methods for grouping and clustering chords, ac-cording to their GCT encoding and their functional role inchord sequences, are introduced. The results of both har-monic labelling and functional clustering indicate that theGCT representation constitutes a suitable scheme for rep-resenting effectively harmony in computational systems.1. INTRODUCTIONComputational systems developed for harmonic analysisand/or harmonic generation (e.g. melodic harmonisation),rely on chord labelling schemes that are relevant and char-acteristic of particular idioms [7, 10, 20, 21, 26]. There ex-ist various typologies for encoding note simultaneities thatembody different levels of harmonic information/abstractionand cover different harmonic idioms. For instance, somecommonly used chord notations in tonal music are the fol-lowing: ﬁgured bass (pitch classes denoted above a bassnote – no concept of ‘chord’), popular music guitar stylenotation or jazz notation (absolute chord), roman numeralencoding (relative chord to a key) [18] - see, Harte’s [12]c\u0000Maximos Kaliakatsos-Papakostas, Asterios Zacharakis,Costas Tsougras, Emilios Cambouropoulos.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Maximos Kaliakatsos-Papakostas, As-terios Zacharakis, Costas Tsougras, Emilios Cambouropoulos. “EV AL-UATING THEGENERAL CHORD TYPEREPRESENTATION INTONAL MUSIC AND ORGANISING GCT CHORD LABELS INFUNCTIONAL CHORD CATEGORIES”, 16th International Society forMusic Information Retrieval Conference, 2015.formal tonal chord symbol representation. For atonal andother non-tonal systems, pitch-class set theoretic encod-ings [8] may be employed. There exists no single chordencoding scheme that can be applied to all harmonic sys-tems with sufﬁcient expressiveness.Preliminary studies on the General Chord Type (GCT)[3] representation (e.g. for probabilistic melodic harmoni-sation [15]) indicate that it can be used both as a means torepresent accurately harmonic chords and to describe mu-sically meaningful relations between different harmoniclabels in diverse music idioms. The GCT provides accu-rate harmonic representation in a sense that it encompassesall the pitch-class-related information about chords. At thesame time, for every pitch class simultaneity the GCT al-gorithm rearranges pitch classes so that it identiﬁes a rootpitch class and a chord base type and extension, leadingto chord representations that convey musical meaning fordiverse music idioms.It is true that the main strength of the GCT representa-tion is its application in non-tonal harmonic idioms; somesuch preliminary examples have been presented in [2, 3,14]. This paper, however, focuses on the tonal idiom, asthis provides a well-studied system with reliable groundtruth data against which a chord labelling and grouping al-gorithm can be tested. If the GCT representation can copewith such a sophisticated hierarchical harmonic system asthe tonal system, then it seems likely that it can deal withother non-tonal systems (even though other simpler repre-sentations may also be adequate). Applying and testing theGCT on other musics is part of ongoing research.The paper at hand addresses two issues regarding theGCT representation. First, an evaluation of the GCT’s abil-ity to label chords is performed by comparing the chordroots and types it produces with human expert annotations(roman-numeral analysis) on the Kostka & Payne dataset.This analysis provides clear indications about the inter-pretational efﬁciency of the GCT (around 92% agreementwith human annotations). Secondly, a grouping process isproposed, which allows the identiﬁcation of the functionalrole of chord groups in GCT form. An initial groupingstage, solely based on the GCT expression of the chords,allowsn in a second stage, the identiﬁcation of functionalsimilarities according to ﬁrst-order transitions of GCT chordgroups. The results of this analysis on a set of Bach Choralesindicate that the functional role of GCT chord groups isdetermined in a reliable manner, agreeing with theoretic427functional characteristics of chords in this idiom.2. THE GENERAL CHORD TYPEREPRESENTATIONHarmonic analysis is a rather complex musical task that in-volves not only ﬁnding roots and labelling chords within akey, but also segmentation (points of chord change), iden-tiﬁcation of non-chord notes, metric information and moregenerally musical context [27]. In this section, we focus onthe core problem of labelling chords within a given pitchhierarchy (e.g. key). We assume, for simplicity, that a fullharmonic reduction (main harmonic notes) is available asinput to the model along with key/modulation annotations.It is suggested that the GCT representation scheme can beused in the future so as to facilitate the harmonic reductionper se of an unreduced musical surface (e.g. by identifyingdissonant chord extensions in relation to a chord’s conso-nant base).The General Chord Type (GCT) representation, allowsthe re-arrangement of the notes of a harmonic simultane-ity such that a maxinal consonant part determines the baseof the chord, and the rest of the dissonant notes form thechord extension; the lowest note of the base is the root ofthe chord. The GCT representation has common charac-teristics with the stack-of-thirds and the virtual pitch rootﬁnding methods for tonal music, but has differences as well(see [3]). This encoding is inspired by the standard romannumeral chord type labelling, but is more general and ﬂex-ible. A brief description of merely the GCT core algorithmis presented below (due to space limitations); a more ex-tended discussion on the background concepts necessaryfor the GCT model as well as a more detailed descriptionof the GCT representation are presented in [3].2.1 Description of the GCT AlgorithmGiven a classiﬁcation of intervals into consonant/dissonant(binary values) and an appropriate scale background (i.e.scale with tonic), the GCT algorithm computes, for a givenmulti-tone simultaneity, the ‘optimal’ ordering of pitchessuch that a maximal subset of consonant intervals appearsat the ‘base’ of the ordering (left-hand side) in the mostcompact form; the rest of the notes that create dissonantintervals to one or more notes of the chord ‘base’ form thechord ‘extension’. Since a tonal centre (key) is given, theposition within the given scale is automatically calculated.Input to the algorithm is the following:•Consonance vector: a Boolean 12-dimensional vec-tor is employed indicating the consonance of pitch-class intervals (from 0 to 11). E.g., the vector[1,0,0,1,1,1,0,1,1,1,0,0]means that the unison, minorand major third, perfect fourth and ﬁfth, minor andmajor sixth intervals are consonant – dissonant in-tervals are the seconds, sevenths and the tritone; thisspeciﬁc vector is referred to in this text as the tonalconsonance vector.•Pitch Scale Hierarchy: is given in the form of scaletones and a tonic. E.g., aDmajor scale is given as:Table 1. GCT chord labelling exampleInput:Bbmajor scale:[10,[0,2,4,5,7,9,11]]Input: Consonance vector:[1,0,0,1,1,1,0,1,1,1,0,0]– input[53,63,69,72,75]– input converted to pc-set:[0,3,5,9]– maximal consonant subset:[0,5,9]– rewrite in narrowest range:[5,9,0]– Dissonant tone3goes to the end:[5,9,0,3]– Lowest tone is root, i.e.5(noteF)– Chord with root0:[0,4,7,10](i.e., dominant seventh)– Absolute chord:[5,[0,4,7,10]](i.e., F7)– Relative position: root is7semitones above the tonicBb– Chord in relative position:[7,[0,4,7,10]]– No other maximal subset exists.Output:[7,[0,4,7,10]](i.e. V7)2,[0,2,4,5,7,9,11], or anAminor pentatonic scaleas:9,[0,3,5,7,10]•Input chord: list of pitch classes (MIDI pitch num-bers modulo 12).Algorithm 1GCT algorithm (core) – computational pseu-docodeRequire:(i) the pitch scale (tonality), (ii) a vector of theintervals considered consonant, (iii) the pitch class set (pc–set) of a note simultaneityEnsure:The roots and types of the possible chords de-scribing the simultaneity1:ﬁnd all maximal subsets of pairwise consonant tones2:select maximal subsets of maximum length3:forall selected maximal subsetsdo4:order the pitch classes of each maximal subset in themost compact form (chord ‘base’)5:add the remaining pitch classes (chord ‘extensions’)above the highest of the chosen maximal subset’s(if necessary, add octave – pitches may exceed theoctave range)6:the lowest tone of the chord is the ‘root’7:transpose the tones of the chord so that the lowestbecomes 08:ﬁnd position of the ‘root’ in regards to the giventonal centre (pitch scale)9:end forSince the aim of this algorithm is not to perform so-phisticated harmonic analysis, but rather to ﬁnd a practicaland efﬁcient encoding for tone simultaneities (to be used,for instance, in statistical learning and automatic harmonicgeneration in the context of the project COINVENT [25]),we decided to extend the algorithm so as to reach a singlechord type for each simultaneity (no ambiguity) in everycase . These additional steps are described in [3] and takeinto account overlapping of maximal subsets and avoid-ance of non-scale notes in the base of chord types.An example taken from Beethoven’sAndante Favori(Figure 1) illustrates the application of the GCT algorithmfor different consonance vectors. For the tonal vector, GCTencodes classical harmony in a straightforward manner.428 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015All instances of the tonic chord are tagged as[0,[0,4,7]];the dominant seventh (inverted or not) is[7,[0,4,7,10]];the third to last chord is a minor seventh on the seconddegree encoded as[2,[0,3,7,10]]; the second and fourthchord is a Neapolitan sixth chord encoded as[1,[0,4,7]](which means major chord on lowered second degree) witha secondary dominant in between (the pedal G ﬂat note inthe third chord is not taken into account). This way wehave an encoding that is analogous to the standard romannumeral encoding (Figure 1, ‘tonal’). If the tonal contextis changed to a chromatic scale context and all intervalsare considered equally consonant, i.e. all entries in conso-nance vector are 1s, we get the second ‘atonal’ GCT anal-ysis (Figure 1, ‘atonal’) which amounts to normal orders(not prime forms) in standard pc-set analysis. In pitch classset theory normal orders do not have roots – however, theyhave transposition values (T0-T11) in relation to a refer-ence pc (normally pc 0); the normal orders with transposi-tion values of pc-set theory are equivalent to the GCT forthe atonal consonance vector. Obviously, for tonal music,this pc-set-like analysis is weak as it misses out or obscuresimportant tonal hierarchical relationships; however, it canencode efﬁciently non-tonal musics. More examples fromnon-tonal music in [2, 3, 14].2.2 Qualitative evaluation of the GCT in tonal musicWe tested the GCT algorithm on the Kostka-Payne datasetcreated by David Temperley. This dataset consists of the46 excerpts that are longer than 8 measures from the work-book accompanying Kostka and Payne’s theory textbookTonal Harmony, 3rd edition (McGraw-Hill, 1995)1. Giventhe local tonality (key), the GCT algorithm was applied toall the Kostka-Payne excerpts. Then, the resulting GCTswere compared to the Kostka-Payne ground truth (i.e. theroman numeral analysis included in the Instructor’s Man-ual, not taking into account chord inversions). From the919 chords of the dataset, GCT successfully encodes 847chords, and 72 chords are labelled differently. This meansthat the algorithm labels92.16% of all chords correctly.The identiﬁed mistakes can be categorised as follows:a) Twenty three (23) mislabeled chords were diminishedseventh chords[0,3,6,9]. As explained earlier, these sym-metric chords can have as their root any of the four con-stituent notes. In most cases these were viio7chords invarious inversions, referring either to the main key or toother keys as applied chords, but in some cases they wereembellishing (non functional) chords.b) Twenty two (22) half-diminished chords[0,3,6,10]were labelled as minor chords with added sixth[0,3,7,9];e.g.[B,D,F,A]was re-ordered as[D, F, A, B]. As a con-sequence, all iiø6/5chords in minor keys were identiﬁed asivadd6chords, and all viiø7-type chords in major keys wereidentiﬁed as iiadd6chords.c) Seventeen (17) cases had a salient note missing (e.g.diminished chord without root, dominant seventh withoutthird, half-diminished seventh without third, etc) and this1The dataset set is available in machine readable format at¡http://theory.esm.rochester.edu/temperley/kp-stats/index.html¿.resulted in ﬁnding a wrong root; e.g.[G], D, F], viio7inA minor without 3rd, was identiﬁed as[D, F, A[], i.e. asiv5[;[B,F,A], viiø7in C major without 3rd, appears as[F, A, B], i.e. IV5[;[C,E,B[,D[],V7/9in F minor, isidentiﬁed as [B[,D[,F[,C], i.e. as iv5[/9;[E[,G,D[,C],i.e. V7/13in A[major erroneously appeared as[C,E[,G,D[], i.e. iii9[, while[C, E, B[, A[], i.e. V7/13in F minorappears almost correctly as[C,E,G],B[], i.e. as V5]/7(the difference is that in the ﬁrst case the 13th interval wasmajor).d) Eight (8) chords were misspelled because they ap-peared over a pedal note (pedal notes were included inour GCT analysis, while they were omitted in Temperley’sanalysis); e.g.[D, A, C], G],aV7over a tonic pedal inD major, appeared as[A, D, G, C]], i.e. as V4/7/10, and[D,C],G,B], a viiø7over a tonic pedal, is described as[G, B, D, C]], i.e. as IV11].e) Two (2) sus4 chords[0,5,7]were identiﬁed incor-rectly as[0,5,10]; e.g.[C,F, G],Vsus4in F major con-tains the dissonant interval[F,G]and was erroneously re-ordered as[G, C, F], i.e. as ii4/7(quartal chord).On the other hand, the GCT algorithm correctly identi-ﬁed numerous functionally ambiguous chords, such as var-ious cases of augmented 6th chords (mainly German types,but also Italian and French types) formed over a varietyof scale degrees (6[,2[,4, etc.). It also correctly iden-tiﬁed most harmonic circles of ﬁfths, applied dominants,neapolitan chords, chords produced by modal mixture andcomplex triadic chords (with more than four members).Overall, in the context of tonal music and the for stan-dard tonal consonance vector, the GCT algorithm producesquite satisfactory results. However, it makes primarily thefollowing types of mistakes: ﬁrstly, it yields ambiguousresults regarding the root of symmetric chords such as thefull diminished seventh and augmented chords – to disam-biguate the root for symmetrical chords (mainly for dimin-ished seventh chords), harmonic context has to be takeninto account (e.g. the root of the following chord); sec-ondly, it assigns the wrong root to chords that have ‘dis-sonant’ intervals at their triadic base, such as diminishedﬁfths in half-diminished chords or major second in sus4chords; thirdly, tertian chords that have notes missing fromtheir base (e.g. missing third in seventh chords) are misin-terpreted as their upper denser part is taken as the chordsbase and the lower root as an extension; and, ﬁnally, pedalnotes, when taken into account for the identiﬁcation of theGCT type, produce complex and functionally incorrect re-sults.In order to correct such cases, a more sophisticated modelfor harmonic analysis is required, which extends the purelyrepresentational scope of the current proposal. Such a modelwould take into account voicing (e.g. the bass note), chordtransition probabilities (functions), and, even, higher-leveldomain-speciﬁc harmonic knowledge (e.g. speciﬁc typesof chords used in particular idioms).The GCT algorithm captures the common-practice roman-numeral harmonic analysis encoding scheme (for the ‘stan-dard’ consonance vector) reasonably well. Additionally, itProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 429Figure 1. Beethoven,Andante Favori, reduction of mm.189-198. Tonal and atonal GCT analysis (see text).adapts to non-tonal systems, such as atonal, octatonic ortraditional polyphonic music. The question is whether theGCT representation works well on such non-tonal systems.The GCT representation has been employed in the case oftraditional polyphonic music from Epirus [14], whereby,song transcriptions were initially converted to the GCT en-coding, followed by a learning HMM scheme. This schemewas then employed to learn chord transitions, which was ﬁ-nally used to create new harmonisations in the polyphonicstyle of Epirus. Ongoing research is currently studying theapplication of GCT on various harmonic idioms, from me-dieval music to 20th century music, and various pop andfolk traditions.3. GROUPING GCT CHORDSChord relationships, and more speciﬁcally chord similar-ity/distance in tonal and non-tonal music, have been stud-ied by various music theorists/researchers; some notableexamples are the work by Hindemith [13], the classiﬁ-cation scheme by Harris [11], pitch-class set (pcset) the-ory [8, 9], neo-riemannian theory [4, 5], tonal pitch spacetheory [19] and the work by Quinn [22]. Empirical studieshave attempted to evaluate aspects of such theories in anempirical manner - see, for instance, [1, 16, 17, 24]. Apartfrom sensory, cognitive and musicological factors that playa signiﬁcant role in such studies (and also in the ﬁrst chordgrouping algorithm below), the work herein makes addi-tional use of data-driven information derived from statisti-cal harmonic analysis in order to tackle similarity of dif-ferent chord groups based on their functionality (i.e. tran-sitions between chords) cf. related work by Quinn andMavromatis [23].A large number of unique note simultaneities may ap-pear in a certain musical style. These simultaneities, how-ever, are organised into fewer more cognitively manage-able chord families/categories. Things like octave equiva-lence, interval inversion equivalence, root, tonal centre andso on, enable a parsimonious ‘packing’ of the great varietyof actual note simulaneities into a relatively small numberof musically meaningful chord categories. This categori-cal organisation of chords is probably most apparent in thecase of tonal music; for instance, ‘major chord’ applies tomany vertical note conﬁgurations that may appear in differ-ent guises such as open/closed position, different registersand keys, with doubled or missing or, even, extra notes.The GCT algorithm re-organises note simultaneities interms of ‘root’, ‘base’, ‘extension’ and relative root to lo-cal key, giving the same label to pitch collections that haveidentical structure in relation to a tonal centre. However,missing or extra notes are not taken into account, result-ing in a larger number of chords than what is musicallyacceptable (at least for tonal music). For instance, theGCTs:[7,[0,4,7]],[7,[0,4]],[7,[0,4,10]],[7,[0,4,7,10]]are all independent chord labels whereas they could begrouped under one dominant chord label (these share thesame relative root and are all subsets of the[0,4,7,10]chord type). Additionally, the GCTs:[11,[0,3,6]]and[11,[0,3,6,9]]are diminished chords on the seventh scaledegree; these cannot be grouped with the previous GCTsbecause of the different relative root and chord type, eventhough we know that they also belong to the dominantchord functional category.In the next two subsections, ﬁrstly, a simple algorithm ispresented that groups raw GCTs into GCT chord categoriesbased on GCT properties, such as, relative root, type sim-ilarity and relationship to underlying scale/key; secondly,an algorithm is developed that further organises the aboveGCT categories into functional chord categories by exam-ining the function of chords, i.e., chords that tend to befollowed by the same chords (similar rows in a chord tran-sition matrix) are considered to have the same function.These two algorithms tidy up the initial raw GCTs intomeaningful chord categories, each represented by the mostfrequently occurring instance (exemplar) .3.1 Grouping chords based on their GCT propertiesFollowing the aforementioned example, the ‘exemplar[7,[0,4,7]]might be found in several ‘reduced’ (e.g.[7,[0,4]])or ‘expanded’ (e.g.[7,[0,4,7,11]]) forms, that actuallyrepresent the same chord label. According to the GCTrepresentation, further abstraction can be achieved throughgrouping GCT expressions of simultaneities that ‘evidently’concern the same chord.Grouping of GCTs has been studied under some ba-sic assumptions about the chord characteristics that are re-ﬂected by the root scale degree, the base and the scale notesunderlying a GCT expression. Speciﬁcally, GCT expres-sions are grouped into more general GCT categories thatpotentially contain several GCT members according to thecriteria described below: two chords belong to the samegroup if1.they have the same scale degree root,430 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20152.their GCT bases are subset-related and3.they both contain notes that either belong or not tothe given scale context.Regarding criterion 2, two basesB1andB2are consid-ered subset-related ifB1✓B2orB2✓B1, e.g.[0,4]✓[0,4,7]while[0,4]6⇢[0,3,7]. Criterion 3 is utilised toidentify and group together chords that belong to secondarytonalities within the primary tonality of the piece. For in-stance, in a diatonic major context, whilec1=[ 0,[0,4,7]]andc2=[ 0,[0,4,7,10]]fulﬁl criteria 1 and 2, according tocriterion 3 they are not grouped together since c2 includesvalue 10, which is mapped to the non-diatonic 10 pitchclass value. In a major context [0,[0,4,7,10]] is secondarydominant to the IV (V/IV) and is differentiated from the Imajor chord.Each GCT group includes the GCT types that satisfy theaforementioned three criteria. Furthermore, each group isrepresented by the ‘exemplar’ GCT type, which is the onethat is more often met in the datasets under study. Somecommon chord groups in the major scale Bach Choralesare illustrated in Table 2. This table also includes the func-tional naming of each group in order to assist the com-parison of the derived GCT types and the standard roman-numeral labelling. Testing this simple algorithm on setsof both major and minor Bach chorales gives a reasonableﬁrst classiﬁcation of the ‘raw’ GCTs.3.2 Functional similarity of chordsAccording to functional harmony each chord can be viewednot only in terms of its actual pitches, roots, chord type andso on, but also in terms of its ‘dynamic’ attributes accord-ing to its position in a chord sequence and to the chords thatusually follow [18]. For instance, in the tonal idiom, dom-inant chords are ‘expected’ to resolve to a (relative) tonicchord. Therefore, different chords can be similar accord-ing to the purpose they serve in terms of their functionalitywithin chord sequences.In this Section, a ﬁrst approach to derive the function-ality of the GCT chord groups is addressed by observingtheir succeeding chords in chord sequences extracted fromspeciﬁc idioms. In order to capture the functional relationsbetween GCT groups of speciﬁc music idioms, the ﬁrst-order Markov transition table is considered for all the GCTchord sequences that pertain to a certain idiom. The pro-posed approach below, tackles chord similarity by employ-ing the Euclidean distance metrics related to the probabil-ity distribution for each chord group to precede any other(i.e., euclidean distance between rows of the transition ma-trix).Figure 2 illustrates a colour-based graphic interpretationof the transition matrix obtained from a collection of BachChorales in major mode (darker areas indicate higher prob-abilities); transitions between chords that pertain to thesame GCT chord group are disregarded (this neutralises thediagonal). Furthermore, GCT chord groups that occurred4 times or less in the entire dataset were discarded, sincetheir functional role can hardly be determined by so fewobservations. The probability that a GCT chord group isfollowed by another (a row of the transition matrix in Fig-ure 2) is regarded as a vector that deﬁnes the position ofthis group into the ‘space of transitions’. Thereby, func-tional relations between GCT groups according to theirmost common successors can be deduced by employingdistance metrics between rows of the transition matrix.\nFigure 2. The ﬁrst-order Markov transition matrix of GCTgroups in the major Bach Chorales. The numbers after thecolon indicate the number of times a representative of aGCT group was found in the data.3.3 Functional similarity resultsThe Euclidean distance between transitions of GCT groups(rows in the transition matrix depicted in Figure 2) in a setof major Bach chorales has been utilised to produce thedendrogram of distances illustrated in Figure 3. For clarityof presentation, GCT groups with rare occurrences (lessthan 4) were not considered, although their placement inthe grouping results was explainable. The six annotatedclusters underpin interesting functional relations betweenthe chords involved (the comments are presented in dimin-ishing cluster coherence order):Cluster 1comprises the double dominant V/V[2,[0,4,7,10]]and its subset viio/V[6,[0,3,6]]. Both chords haveidentical harmonic function (pre-dominant) and they al-ways lead to the dominant V chord as applied dominants.Cluster 4contains the dominant V[7,[0,4,7]]and theleading-tone triad viio[11,[0,3,6]], which is a subset ofthe dominant 7th chord. Both chords have strong dominantfunction.Cluster 6contains the applied dominant of the sub-me-diant, i.e. V/vi, and the corresponding applied diminished7th chord, i.e. viio7/vi. The GCT algorithm erroneouslydescribes the second chord as its enharmonic equivalent[11,[0,3,6,9]], i.e. as viio7[B, D, F, A[], while it shouldbe[8,[0,3,6,9]], i.e. viio7/vi[G], B, D, F]. However, thestrong clustering relation could help to disambiguate theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 431functional nameexemplarGroup memberstonic[0,[0,4,7]][0,[0,4,7]] [0,[0,4]] [0,[0,4,7],[11]]dominant[7,[0,4,7]][7,[0,4,7]] [7,[0,4,7],[10]] [7,[0,4],[10]] [7,[0,4]]subdominant[5,[0,4,7]][5,[0,4,7]] [5,[0,4]] [5,[0,4,7],[11]]V / IV[0,[0,4,7],[10]][0,[0,4,7],[10]] [0,[0,4],[10]]Table 2. Four tonal chord groups and their exemplar GCTs. Notice how the group of[0,[0,4,7]]has been separated fromthe group of[0,[0,4,7],[10]], due to the non-diatonic pitch class 10 of the latter.root of the diminished 7th chord; this is future work forimproving the descriptiveness efﬁciency of the GCT repre-sentation.Cluster 5groups the applied dominant of the super-tonic, i.e. V/ii[9,[0,4,7]], and the corresponding applieddiminished triad, i.e. viio/ii[1,[0,3,6]]. Clusters 1, 4, 5, 6are of the same category, as they share the same dominantfunction.Cluster 2is different, as it groups three chords that have(or may have) tonic harmonic function, the tonic I[0,[0,4,7]], the submediant vi[9,[0,3,7]]and the mediant iii[4,[0,3,7]]. In functional harmony [6], these chords are la-beled as T, Tp and Tg accordingly and the last two chordshave a diatonic (with two common tones) third-relationwith the ﬁrst.Cluster 3is similar to cluster 2, as it groups two chordswith diatonic third-relation, however in this case the chordsshare subdominant harmonic function: the subdominantIV[5,[0,4,7]]and the supertonic ii[2,[0,3,7]]. In func-tional harmony, they are described as S and Sp accordingly.Overall, the proposed data-driven functional approachto chord grouping seems to be quite reliable. Further test-ing is necessary on larger and more varied corpora.4. CONCLUSIONSThe paper at hand examines two main topics: a) the abilityof the GCT algorithm to analyse chord sequences (in com-parison to roman numeral analysis) and b) the possibilty toorganise the ‘raw’ GCT labels in higher-level chord fam-ilies according to the internal GCT properties and to dy-namic functional properties in terms of chord successionsin harmonic corpora. The ﬁrst study was based on com-paring the annotations of chords produced by the GCT al-gorithm with the harmonic annotations of human experts(around 92% accuracy in the Kostka-Payne dataset). So,with its ability to identify roots and chord types, the GCTcan be used as an interpretation/analytic tool allowing itto be classiﬁed as a hybrid between neutral representations(e.g. Forte pc-set theory analysis) and interpretative ones(e.g. roman numeral analysis). For the second study, in-formation about transitions of the GCT chord groups wereutilised to identify similarities between these groups ac-cording to their successors, thus, reﬂecting functional rela-tions.The results are promising, since they illustrate the abil-ity of the GCT to accurately label chords, but also to re-veal chord groups according to (higher) functional mean-ing in the tonal system. It is maintained that if the GCTrepresentation can cope with such a sophisticated hierar-00.511.5[2,0,4,7,10]:28[6,0,3,6]:13[0,0,4,7]:410[9,0,3,7]:100[4,0,3,7]:29[5,0,4,7]:130[2,0,3,7]:82[0,0,4,7,10]:16[7,0,4,7]:343[11,0,3,6]:39[9,0,4,7]:13[1,0,3,6]:6[11,0,3,6,9]:5[4,0,4,7]:21\n5\n214365Figure 3. (a) The dendrogram derived from the Euclideandistances between rows of the transition matrix (Figure 2).chic harmonic system as the tonal system, then it seemslikely that it can deal with other non-tonal systems as well.Preliminary examples presented in [2, 3, 14] illustrate thepotential of the GCT to represent non-tonal harmonic id-ioms; further reseach is under way to unveil the potentialof the proposed representation in other musics.5. ACKNOWLEDGEMENTSThis work is founded by the COINVENT project. Theproject COINVENT acknowledges the ﬁnancial support ofthe Future and Emerging Technologies (FET) programmewithin the Seventh Framework Programme for Researchof the European Commission, under FET-Open grant num-ber: 611553.6. REFERENCES[1]Emmanuel Bigand, Richard Parncutt, and Fred Ler-dahl. Perception of musical tension in short chord se-432 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015quences: The inﬂuence of harmonic function, sensorydissonance, horizontal motion, and musical training.Perception & Psychophysics, 58(1):125–141, 1996.[2]E. Cambouropoulos. The Harmonic Musical Surfaceand Two Novel Chord Representation Schemes. InD. Meredith, editor,Computational Music Analysis,page (forthcoming). Springer, 2015.[3]Emilios Cambouropoulos, Maximos Kaliakatsos-Papakostas, and Costas Tsougras. An idiom-independent representation of chords for compu-tational music analysis and generation. InProceedingof ICMC–SMC 2014, 2014.[4]Richard Cohn. Neo-riemannian operations, parsimo-nious trichords, and their” tonnetz” representations.Journal of Music Theory, pages 1–66, 1997.[5]Richard Cohn. Introduction to neo-riemannian theory:a survey and a historical perspective.Journal of MusicTheory, pages 167–180, 1998.[6]Diether de la Motte.Harmonielehre. Kassel: Barenre-iter – Verlag, 1976.[7]Kemal Ebcioglu. An expert system for harmonizingfour-part chorales.Computer Music Journal, 12(3):43–51, 1988.[8]Allen Forte.The structure of atonal music. Yale Uni-versity Press, New Haven, 1973.[9]Allen Forte. Pitch-class set genera and the origin ofmodern harmonic species.Journal of Music Theory,pages 187–270, 1988.[10]Mark Thomas Granroth-Wilding.Harmonic analysisof music using combinatory categorial grammar. PhDthesis, Institute for Language, Cognition and Compu-tation School of Informatics University of Edinburgh,Edinburgh, Scotland, November 2013.[11]Simon John Minshaw Harris.A proposed classiﬁcationof chords in early twentieth-century music.PhD thesis,King’s College London (University of London), 1985.[12]Christopher Harte, Mark Sandler, Samer A. Abdallah,and Emilia G´omez. Symbolic representation of musi-cal chords: A proposed syntax for text annotations.InProceedings of the 4th International Conferenceon Music Information Retrieval (ISMIR), pages 66–71,London, UK, 2005.[13]Paul Hindemith.The craft of musical composition,vol.1: Theoritical part (Trans. A. Mendel). Associatedmusic, New York, 1937/1942.[14]M. Kaliakatsos-Papakostas, A. Katsiavalos,C. Tsougras, and E. Cambouropoulos. Harmonyin the polyphonic songs of epirus: Representation,statistical analysis and generation. In4th InternationalWorkshop on Folk Music Analysis (FMA) 2014, June2014.[15]Maximos Kaliakatsos-Papakostas and Emilios Cam-bouropoulos. Probabilistic harmonisation with ﬁxedintermediate chord constraints. InProceeding ofICMC–SMC 2014, 2014.[16]Carol L Krumhansl.Cognitive foundations of musicalpitch, volume 17. New York: Oxford University Press,1990.[17]Tuire Kuusi. Chord span and other chordal character-istics affecting connections between perceived close-ness and set-class similarity.Journal of New Music Re-search, 34(3):259–271, 2005.[18]Steven G Laitz.The complete musician: an integratedapproach to tonal theory, analysis, and listening. Ox-ford University Press, New York, 2012.[19]Fred Lerdahl.Tonal pitch space. Oxford UniversityPress, 2001.[20]Francois Pachet and Pierre Roy. Musical harmoniza-tion with constraints: A survey.Constraints, 6(1):7–19,January 2001.[21]Jean-Franc ¸ois Paiement, Douglas Eck, and Samy Ben-gio. Probabilistic melodic harmonization. InProceed-ings of the 19th International Conference on Advancesin Artiﬁcial Intelligence: Canadian Society for Compu-tational Studies of Intelligence, AI’06, pages 218–229,Berlin, Heidelberg, 2006. Springer-Verlag.[22]Ian Quinn. Listening to similarity relations.Perspec-tives of New Music, 39, 2001.[23]Ian Quinn and Panayotis Mavromatis. V oice-leadingprototypes and harmonic function in two chorale cor-pora. InMCM, pages 230–240. Springer, 2011.[24]Art G. Samplaski.A comparison of perceived chordsimilarity and predictions of selected twentieth-centurychord-classiﬁcation schemes, using multidimensionalscaling and cluster analysis.PhD thesis, Indiana Uni-versity, 2000.[25]M. Schorlemmer, A. Smaill, K.U. K¨uhnberger,O. Kutz, S. Colton, E. Cambouropoulos, and A. Pease.Coinvent: Towards a computational concept inventiontheory. In5th International Conference on Computa-tional Creativity (ICCC) 2014, June 2014.[26]Ian Simon, Dan Morris, and Sumit Basu. Mysong: Au-tomatic accompaniment generation for vocal melodies.InProceedings of the SIGCHI Conference on HumanFactors in Computing Systems, CHI ’08, pages 725–734, New York, NY , USA, 2008. ACM.[27]D. Temperley. Computational models of music cogni-tion. In D. Deutsch, editor,The psychology of music(2nd Edition). Academic Press, San Diego, 2012.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 433"
    },
    {
        "title": "Neuroimaging Methods for Music Information Retrieval: Current Findings and Future Prospects.",
        "author": [
            "Blair Kaneshiro",
            "Jacek P. Dmochowski"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416082",
        "url": "https://doi.org/10.5281/zenodo.1416082",
        "ee": "https://zenodo.org/records/1416082/files/KaneshiroD15.pdf",
        "abstract": "Over the past decade and a half, music information re- trieval (MIR) has grown into a robust, cross-disciplinary field spanning a variety of research domains. Collabo- rations between MIR and neuroscience researchers, how- ever, are still rare, and to date only a few studies using approaches from one domain have successfully reached an audience in the other. In this paper, we take an initial step toward bridging these two fields by reviewing studies from the music neuroscience literature, with an emphasis on imaging modalities and analysis techniques that might be of practical interest to the MIR community. We show that certain approaches currently used in a neuroscientific setting align with those used in MIR research, and discuss implications for potential areas of future research. We ad- ditionally consider the impact of disparate research objec- tives between the two fields, and how such a discrepancy may have hindered cross-discipline output thus far. It is hoped that a heightened awareness of this literature will foster interaction and collaboration between MIR and neu- roscience researchers, leading to advances in both fields that would not have been achieved independently.",
        "zenodo_id": 1416082,
        "dblp_key": "conf/ismir/KaneshiroD15",
        "keywords": [
            "music information retrieval (MIR)",
            "collaborations between MIR and neuroscience",
            "imaging modalities",
            "analysis techniques",
            "bridging music neuroscience and MIR",
            "review of studies",
            "neuroscientific approaches",
            "practical interest to MIR",
            "future research directions",
            "disparate research objectives"
        ],
        "content": "NEUROIMAGING METHODS FOR MUSIC INFORMATION RETRIEVAL:CURRENT FINDINGS AND FUTURE PROSPECTSBlair KaneshiroCenter for Computer Research in Music and AcousticsStanford University, Stanford, CA, USAblairbo@ccrma.stanford.eduJacek P. DmochowskiDepartment of PsychologyStanford University, Stanford, CA, USAdmochowski@gmail.comABSTRACTOver the past decade and a half, music information re-trieval (MIR) has grown into a robust, cross-disciplinaryﬁeld spanning a variety of research domains. Collabo-rations between MIR and neuroscience researchers, how-ever, are still rare, and to date only a few studies usingapproaches from one domain have successfully reachedan audience in the other. In this paper, we take an initialstep toward bridging these two ﬁelds by reviewing studiesfrom the music neuroscience literature, with an emphasison imaging modalities and analysis techniques that mightbe of practical interest to the MIR community. We showthat certain approaches currently used in a neuroscientiﬁcsetting align with those used in MIR research, and discussimplications for potential areas of future research. We ad-ditionally consider the impact of disparate research objec-tives between the two ﬁelds, and how such a discrepancymay have hindered cross-discipline output thus far. It ishoped that a heightened awareness of this literature willfoster interaction and collaboration between MIR and neu-roscience researchers, leading to advances in both ﬁeldsthat would not have been achieved independently.1. INTRODUCTIONSince its inception, music information retrieval (MIR) hasbeen characterized as an interdisciplinary and multifacetedﬁeld, drawing from such diverse domains as informationscience, music, computer science, and audio engineeringto explore topics ranging from indexing and retrieval tomusical analysis and user studies [22, 24]. The ﬁeld hasbecome increasingly collaborative over time, and cross-disciplinary output has grown [33].However, one ﬁeld that has yet to establish itself as adeﬁnitive sub-discipline of MIR is that of neuroscience.Recent papers by Aucouturier and Bigand [6,7] have high-lighted the challenges faced by MIR researchers attempt-ing to publish in cognitive science and neuroscience jour-nals, pointing out that MIR approaches have occupied atc\u0000Blair Kaneshiro, Jacek P. Dmochowski.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Blair Kaneshiro, Jacek P. Dmochowski.“Neuroimaging Methods for Music Information Retrieval: Current Find-ings and Future Prospects”, 16th International Society for Music Infor-mation Retrieval Conference, 2015.best a marginal or incidental role in that literature. The au-thors cite as a main obstacle a fundamental lack of interest,or understanding, from the cognitive science/neurosciencecommunity. At the same time, the few brain-based MIRstudies published to date [16, 40, 52] have emphasizedapplication over background, potentially leaving readerslacking sufﬁcient introduction to the imaging techniqueand brain response of interest. As things currently stand,the ﬁelds of MIR and neuroscience operate largely inde-pendently, despite sharing approaches and questions thatmight beneﬁt from cross-disciplinary investigation.In an effort to begin reconciling these two ﬁelds,the present authors—whose backgrounds collectively spanmusic, neuroscience, and engineering—present a reviewof studies drawn from the music neuroscience literatureand examine their relevance to MIR research. Whilesuch a review will not immediately resolve the signiﬁ-cant philosophical issues described above, it may perhapsopen a window between the two disciplines by highlight-ing shared approaches and potential collaborations whileacknowledging differences in aims and motivations. Envi-sioned outcomes are twofold: First, that MIR researchersmay ﬁnd, in brain responses, a new setting to apply analy-sis techniques already developed for other types of data;and second, and more importantly, that heightened aware-ness of this literature will increase collaborations betweenMIR and neuroscience researchers, advancing both ﬁeldsand leading to the formation of a robust cross-discipline.Since a review of the entire literature on music and neu-roscience would be beyond the scope of this paper, wenarrow the present focus to approaches that align closelywith MIR applications. For rigor, we include only peer-reviewed papers, though interested readers are encour-aged to visit other venues—including but not limited toICMPC, SMPC, and late-breaking ISMIR proceedings—for a wealth of additional ideas and ﬁndings. The primaryfocus here is on EEG, though behavioral and fMRI studieswill be touched upon as appropriate.The remainder of this paper is structured as follows.First, we evaluate the suitability of various neuroimagingmodalities for MIR research (§2). We then review threeneuroimaging approaches used in music research (§3) andconsider how these methods, and others, might be used forMIR research (§4). We conclude with a discussion of di-verging objectives between the two ﬁelds, and opportuni-ties for future cross-disciplinary research (§5).5382. NEUROIMAGING METHODS FOR MIRNeuroimaging is the use of magnetic, electrical, hemody-namic, optical, or chemical means to measure activity inthe central nervous system, most often the cerebral cortex(Table 1). The central idea behind bridging neuroimag-ing with MIR is that music is encoded by the brain, andthus can be “read out” or decoded using imaging tech-niques. In order to exploit this idea, it would be advan-tageous to track neural activity at the temporal resolutionof music (i.e., milliseconds), which necessitates the use oftechniques that provide direct electromagnetic measuresof neural activity. While techniques measuring hemody-namic responses, such as functional magnetic resonanceimaging (fMRI), provide superb spatial resolution that canindirectly probe neural activation on a millimeter scale andelucidate the functional brain networks recruited to processmusic, the sluggishness of these responses makes them lesslikely to play a role in MIR.EEG MEG ECoG fMRI DTITemporal Resolution high high high low NASpatial Resolution low low high high highInvasiveness low low high low lowMobility/Portability high low low low lowField of View large large small large largeExpense to Operate low high NA high highTable 1. Characteristics of neuroimaging techniques fre-quently used in music and auditory research. Adapted fromMehta and Parasuraman [39].On the other hand, electroencephalography (EEG) andmagnetoencephalography (MEG) provide millisecond tem-poral resolution that can in principle be used to infer prop-erties of the stimuli evoking encephalographic responses.EEG and MEG consist of sensors placed at or near thescalp surface that detect mass superpositions of activityin the cerebral cortex. The signal-to-noise ratio (SNR)of EEG/MEG is inherently low, typically on the order of-20 dB. However, as activity is usually collected over aspatial aperture consisting of tens or hundreds of sensors,multivariate approaches can be used to derive spatial ﬁltersthat will enhance the desired signal while suppressing thenoise. The limitation of EEG/MEG is low spatial resolu-tion that results from a spatial smoothing of the evoked sig-nal and renders it difﬁcult to localize the underlying source.In order to achieve ﬁne resolution in both space and time,electrodes can be placed directly on the cortical surface, aninvasive practice that is feasible only in the case of neuro-logical disease where it is known as electrocorticography(ECoG), which has been recently employed to study pro-cessing of music [47, 48, 55]. Note, however, that in thecontext of MIR, precise spatial localization is likely not afundamental requirement. All of the above techniques re-fer to imaging the function of the brain; methods that mea-sure the connections among brain areas, such as diffusiontensor imaging (DTI), have also been used in the contextof music research (e.g., [38]).In order to feasibly integrate neuroimaging with MIR,a form of imaging that is inexpensive, noninvasive, andﬁnely temporally resolved is required. For these reasons,our primary focus in the present paper is on EEG, whichrepresents the most promising modality for bridging neuralresponses with MIR. Moreover, EEG offers a whole-brainﬁeld of view that allows for studying the interaction of dis-tributed brain areas during musical processing.3. APPROACHES OF INTERESTIn this section we review three approaches that may proveuseful for MIR. The ﬁrst is an early-latency response gen-erated by the auditory brainstem, while the latter two in-volve longer latency cortical responses.3.1 The Frequency-Following ResponseThe frequency-following response (FFR) is an early-latency subcortical response generated by the auditorybrainstem less than 10 msec after an auditory stimulus oc-curs. It is a sustained, phase-locked response that oscil-lates at the same frequency as an auditory stimulus to suchan extent that the stimulus can be “played back” from anaverage of many trials of the brain response [25].The FFR is typically recorded from a single electrodeat the vertex of the head, plus reference and ground elec-trodes. The response is averaged over many stimulus pre-sentations, and is usually analyzed in the frequency ortime-frequency domain. The FFR has an especially lowSNR; therefore, FFR experiments require on the order ofhundreds or thousands of stimulus presentations. The fre-quency range of interest for this response is primarily un-der 1,000 Hz, and studies presented here generally usecomplex, synthesized stimuli with fundamental frequen-cies no greater than 300 Hz. An introduction to the re-sponse and technique can be found in the 2010 tutorial bySkoe and Kraus [51], and recent ﬁndings pertaining to mu-sic are summarized in a 2013 review by Bidelman [9].Despite being an early, low-level auditory response, theFFR has been found to show effects of learning-based neu-ral plasticity. Its involvement in the music literature grewout of speech studies that compared subcortical responsesof speakers of tone languages, such as Mandarin and Thai,to those of English speakers. These studies showed thatFFRs to certain pitch-varying phonemes and phoneme-like stimuli were more robust in the tone-language speak-ers than in the English speakers, pointing to experience-dependent processing enhancements [29–32, 56]. Trainedmusicians, who possess a complementary type of pitchexpertise, became a population of interest in generalizingthese ﬁndings. For example, a study by Wong et al. [62]showed that musicians exhibited more robust encoding ofMandarin phonemes than did nonmusicians, despite notbeing tone-language speakers.The ﬁrst study to investigate the FFR speciﬁcally in re-sponse to musical stimuli was a 2007 study by Musacchiaand colleagues [41]. Here, musicians’ enhanced subcor-tical encoding of speech and musical stimuli presented inaudio, visual, and audiovisual modalities could be identi-ﬁed in both the time and frequency domains of the brainProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 539response. Subsequent studies have investigated encodingof musical intervals by musicians and nonmusicians [34],as well as encoding of music by both musicians and Man-darin speakers [10, 11].Musical characteristics of the stimuli have also beenfound to modulate the strength of the FFR. A 2009 studyby Bidelman and Krishnan [12], revealed enhanced encod-ing for consonant versus dissonant musical intervals. Theauthors later found a similar effect in responses to pleasant(major/minor) versus unpleasant (augmented/diminished)triads [13]. It should be noted that these results cannot bemerely a reﬂection of the acoustical properties of the stim-uli, as the consonant and dissonant intervals are interleaved(e.g., the dissonant tritone lies between the consonant P4and P5), as are the constituent intervals (major and minorthirds) comprising the different types of musical triads.3.2 Single-Trial EEG ClassiﬁcationWe now move from the auditory brainstem to the cerebralcortex, where responses begin roughly 50 msec after stim-ulus onset and are typically recorded from between 32–256 electrodes arranged across the surface of the scalp atregular intervals, often by means of a cap or net. Corti-cal responses are generally analyzed in a lower frequencyrange than FFRs, usually below 50 or 60 Hz.Cortical EEG research has a long history of univari-ate analysis. Readers may be familiar with time-averagedevent-related potential (ERP) studies, which focus on am-plitudes and latencies of particular waveform peaks fromselected electrodes. Some recent studies have taken a dif-ferent approach to EEG analysis by classifying single tri-als of the brain response. The goal in this case is to cor-rectly predict, from the brain response, which stimulus theparticipant was experiencing (see Blankertz et al. [15] foran introduction and tutorial). This multivariate approachenables data from multiple electrodes and time points tobe analyzed at once. Classiﬁcation of neuroimaging datahas a longer history in fMRI (as multi-voxel pattern analy-sis [43]) than in EEG; however, the overarching methodol-ogy lends itself well to extracting stimulus- or task-relevantcomponents out of noisy, high-dimensional EEG data, as isdone with other types of data used in music research [50].The ﬁrst single-trial EEG classiﬁcation study focusingon musical stimuli was published in 2011 by Schaefer andcolleagues [49]. They found that brain responses to sevenshort excerpts of naturalistic music1from a variety of gen-res could be classiﬁed signiﬁcantly above chance. Morerecently, Stober et al. recorded EEG responses from EastAfrican listeners who heard twelve Western and twelveEast African rhythms, and used deep-learning techniquesto predict both the rhythm family of a stimulus (2-classproblem) as well as the individual rhythm (24-class prob-lem) from the EEG [52]. The prediction task of EEG clas-siﬁcation has also extended beyond characterizing the stim-uli to labeling listeners’ emotional states—for example, inresponse to music videos [28] and musical excerpts [16].1The term “naturalistic music” is used to refer to ecologically validmusical material as opposed to controlled, synthesized stimuli.A brain-computer interface (BCI) is often cited as ageneral application of single-trial EEG classiﬁcation [14].In a musical context, a successful BCI would enable a userto communicate mentally by selectively interacting with anongoing musical stimulus. Studies by Vlek and colleaguesshowed that subjective (mentally imposed) metrical ac-cents on a beat sequence could be detected in the EEGresponse [60], and that a classiﬁer trained upon responsesto perceived accents could be used to detect the imaginedaccents [61]. In a recent EEG study by Treder et al. [58],also working toward BCI application, listeners were playedpolyphonic musical stimuli wherein each stream producedintermittent “oddball” musical events, and attended to justone of the streams. The authors leveraged the fact thatthe brain responds differently to attended oddball auditorystimuli than to unattended oddballs, and classiﬁed brain re-sponses to just the oddball events in the music in order toidentify the attended stream.3.3 Tracking Temporal Dynamics of AcousticalFeaturesCertain music cognition studies have drawn explicitly fromMIR techniques, utilizing acoustical features developedspeciﬁcally for music analysis [59]. These studies useshort-term (e.g., spectral ﬂux, spectral centroid) and long-term (e.g., musical mode, pulse clarity) acoustical features,computationally extracted from musical stimuli, as a basisfor quantitatively comparing stimuli with responses.A 2010 behavioral study by Alluri and Toiviainen [1]set the foundation for this approach in the music cognitionliterature. The authors formulated perceptual scales suit-able for assessing timbre of naturalistic music, and thenlinked human ratings of short musical excerpts to the ex-cerpts’ constituent short-term acoustical features. Subse-quent fMRI studies used a reﬁned set of short-term fea-tures, as well as long-term features, to characterize theirmusical stimuli. Alluri and colleagues identiﬁed brain re-gions whose fMRI time series correlated with those of theacoustical features of a tango piece [2], and later predictedbrain activations from the features of a variety of musicalexcerpts [3]. A 2014 study by Toiviainen and colleaguestook the inverse approach, predicting acoustical featuresfrom fMRI-recorded responses to Beatles songs [57].Acoustical feature representation has also been studiedin ongoing EEG. In contrast to relatively short epochs usedin FFR and classiﬁcation analysis, ongoing-EEG epochscan span many minutes, and are thus well suited to theanalysis of responses to longer musical excerpts such assongs [17]. A 2013 study by Cong and colleagues usedthe same stimulus and long-term acoustical features as the2012 Alluri study [2] in an ongoing-EEG paradigm, de-composing the EEG response into temporally independentsources using Independent Component Analysis (ICA),and then identifying sources whose frequency contentcorresponded to the time courses of the acoustical fea-tures [17]. More recently, Lin and colleagues also usedEEG ICA sources to link ongoing-EEG responses to musi-cal mode and tempo in shorter musical excerpts [36].540 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20154. MIR APPLICATIONSIn the previous section, we reviewed three approaches usedto study brain responses to music: The FFR, which directlyencodes the pitch of an auditory stimulus, and two analy-sis techniques used for classifying and characterizing cor-tical responses. We will now discuss MIR applications ofneuroimaging data. We consider the relevance of each ap-proach to MIR research and assess the added value of an-alyzing the brain response—over analyzing, for example,the auditory stimulus directly.4.1 TranscriptionThe FFR is unique among the auditory responses presentedhere in that it directly reﬂects the stimulus. As describedabove, the FFR has been used primarily as a measure of en-coding. To date, its robustness has been the main attributeof interest, reﬂecting effects of expertise (tone-languagespeaker or musician) and stimulus properties (musical con-sonance or pleasantness) in the brain response.The FFR could prove to be a powerful transcriptiontool; to our knowledge, this application has not yet beenexplored. From an MIR perspective, there would be littleadded value in transcribing responses to the simple musi-cal stimuli used in the FFR studies described here (mostlymonophonic, sometimes intervals or triads—see§3.1), astranscription could be easily accomplished directly fromthe audio. However, selective attention has been foundto enhance FFR amplitudes for simultaneously presentedspeech stimuli [26, 35]; therefore, future research couldstudy this topic further using musical stimuli, for exam-ple to extract a melody from polyphonic music—an opentopic in audio MIR research, but something a human canaccomplish effortlessly. Though FFRs to imagined soundshave yet to be conﬁrmed, an FFR-based transcription sys-tem of this kind would certainly open another exciting andnovel avenue for future research.As described above, FFR studies typically involve up tothousands of stimulus repetitions due to low SNR. There-fore, signal-processing techniques that could efﬁciently ex-tract the FFR out of the EEG—perhaps by recording the re-sponse from a montage of multiple electrodes, analogousto the use of multiple microphones in a source-separationscenario—would provide a useful resource for more ﬂex-ible experiment design, and provide a critical step towardFFR-based transcription.4.2 Tagging and AnnotationCharacterizing musical attributes and listener responses isa recurring goal in MIR research, and has also been ex-plored in EEG research [28, 37, 40]. In their 2010 pa-per, Alluri and Toiviainen [1] draw explicit connectionsbetween their proposed approach and the use of acousti-cal features in computational systems for music categoriza-tion. Along these lines, the acoustical feature followingapproach used in neuroimaging studies could extend be-yond the prediction of the features from the brain response(as in [57]), toward a global prediction of musical genrefrom combinations of these features over time, as is donein audio-based genre classiﬁcation.Interestingly, a ﬁne-grained temporal representation ofacoustical musical features in the brain response has yet tobe explored using noninvasive imaging techniques. Whileshort-term acoustical features were used in the behavioraland fMRI studies discussed above (§3.3), they were aver-aged or downsampled to match the length of the behav-ioral stimuli (1.5 seconds) or the sampling rate of fMRI(0.45–0.5 Hz) [1–3, 57]. At the same time, the studies us-ing EEG—arguably the best modality for investigating rep-resentation of short-term acoustical features—consideredonly long-term acoustical features in their analysis [17,36].It may be the case, too, that neurally encoded features ofmusic do not correspond exactly to the hand-crafted acous-tical features discussed here; therefore, feature-learningapproaches could also prove useful for connecting tem-porally resolved stimulus features to the brain response,whether to study feature processing and representation, orto develop an annotation tool.Single-trial EEG classiﬁcation could also be applied tothis problem. Of the classiﬁcation studies discussed here,only one used naturalistic music as stimuli [49]; the oth-ers used rhythmic patterns [52, 60, 61] or short events seg-mented from an ongoing stimulus [58]. One possibilityfor future MIR application could be to classify responsesto a larger set of naturalistic musical excerpts to build,for example, a classiﬁcation model that surpasses excerpt-level speciﬁcity and instead predicts genre, mood, or otherglobal attributes from responses to new musical excerpts.4.3 Predicting Large-Scale Audience PreferencesBrain responses can also be used to model listener prefer-ences. This topic has been explored to some extent in themusic neuroscience literature (e.g., [4]). However, to ac-complish a widespread application of this goal—for exam-ple, in a neuromarketing setting [5]—would require that re-sponses of the experimental sample generalize beyond thatsample to a large-scale measure of success, such as sales ofa product or ratings collected from the general public [8].Recent studies have successfully used brain responsesfrom a small sample to predict large-scale audience prefer-ences. In a 2012 study, Berns and Moore collected fMRIresponses and subjective ratings from participants who lis-tened to a set of unknown songs. The authors then trackedthe sales of the songs over the next three years and founda brain region whose activity correlated signiﬁcantly witheventual song popularity [8]. Recent studies by Falk etal. [23] and Dmochowski et al. [21] showed that large-scale success of television commercials could be predictedfrom fMRI and EEG responses, respectively. In all threeof these studies, the brain responses of the experimentalsample correlated more strongly with large-scale measuresof popularity and success than they did with self-reportedpreferences of that same sample. These ﬁndings lend cre-dence to the theory that brain responses provide objectivemeasures of preference, and that generalizations may bedrawn from these responses with greater validity than sub-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 541Company Product Application Website FeaturesEmotiv EPOC commercialhttp://emotiv.com/ﬁxed montage, wireless, iOS, AndroidNeuroSky MindWave, MindSet commercialhttp://neurosky.com/ﬁxed montage, wireless, iOS, AndroidEGI Avatar researchhttp://avatareeg.com/ﬂexible montage and sensors, wireless, AndroidGrass Comet researchhttp://www.grasstechnologies.com/ﬂexible montage and sensorsNeuroelectrics StarStim researchhttp://www.neuroelectrics.com/ﬂexible montage and sensors, wireless, stimulationTable 2. Selected portable/mobile EEG systems.jective ratings from a small experimental sample—eventhe very sample providing the brain responses. Therefore,MIR researchers may ﬁnd brain-based measures of pref-erence or success to be a useful channel of information inpredicting or modeling large-scale music popularity.4.4 Portable/Mobile EEGWhile not an application per se, another area of growinginterest in neuroscience involves portable and mobile EEGsystems. It should be noted that nearly all of the studiesreviewed here were conducted in controlled laboratory set-tings; thus, the listening experiences of the experimentalparticipants likely did not reﬂect their experiences of musicin everyday life. However, a number of commercial- andresearch-grade systems have come to market over the pastdecade (Table 2), and have recently begun to gain tractionin the scientiﬁc literature as valid data-acquisition tools.In an MIR context, a 2013 study by Morita et al. usedthe NeuroSky MindSet to assess mental states in responseto music [40], and the 2014 study by Stober and col-leagues (§3.2) used a portable Grass system for data col-lection [52]. Other recent scientiﬁc publications reportreal-time 3D imaging implementations using wireless EEGwith a smartphone interface built using Emotiv equip-ment [44, 54], and a 2014 study by De Vos and colleaguesshowed that usable single-trial auditory responses could berecorded from a custom portable apparatus, also built off ofthe Emotiv system [18, 19]. The adoption of such method-ologies by the scientiﬁc community presents an opportu-nity for MIR researchers to study music consumption andmusic processing in real-world listening situations [36].5. DISCUSSIONIn this review, we have surveyed neuroimaging techniquesthat can be used in MIR research, and highlighted a numberof potential research topics spanning the two ﬁelds. Why,then, have collaborations not ﬂourished to date?One answer may emerge from a consideration of fun-damental motivational differences between the two ﬁelds.Neuroscience, by deﬁnition, is the study of the brain; there-fore, the thrust of much neuroscientiﬁc research is to gainan understanding of brain functioning underlying process-ing of various stimuli, including music. As a result, exper-iment design, data analysis, and interpretation of resultswill tend toward this goal, even when analysis involves de-coding or prediction of stimulus or response features. Auseful perspective on this topic is provided by Naselarisand colleagues [42], who characterize encoding versus de-coding approaches used in fMRI research: Encoding ap-proaches assess variations in neural space in response tovariations in stimulus space, or perhaps seek to predict thebrain response from the stimulus. Decoding, on the otherhand, seeks to predict information about the stimulus fromthe brain response. In a neuroscientiﬁc setting, both ap-proaches are used to map stimulus features to responses inorder to better understand brain processing.This objective is clearly evident in the studies reviewedabove (§3). The FFR, providing arguably the most decod-able brain signal, is used primarily to study neural encod-ing of auditory stimuli. One outcome of single-trial clas-siﬁcation is the identiﬁcation of temporal and spatial EEGcomponents that best discriminate or differentiate stimulior stimulus categories. The acoustical feature studies alsofocused upon identifying brain areas whose activity covar-ied with the stimuli, and not speciﬁcally on transcription.Of the approaches described above, perhaps only the BCI-focused EEG classiﬁcation studies are purely application-based, with system performance taking priority over an ex-ploration of the underlying neural processing—though anunderstanding of the latter is often a design considerationin the development of a high-performing BCI system.MIR, on the other hand, tends to be a more application-and goal-oriented ﬁeld [7]. For MIR researchers, then,brain data may serve more as a medium through which in-formation about music may be recovered, than as the fun-damental object of investigation. This disparity in what thebrain, and brain data, represent in the overall goal of the re-search may be partly responsible for the lack of connectionand collaboration between the two ﬁelds to date.Another likely hinderance to the incorporation of neuro-scientiﬁc techniques in MIR is access to data. Historically,researchers have had to acquire their own data, which re-quires access to equipment as well as domain-speciﬁc ex-pertise in experiment design and data collection. Follow-ing that, data preprocessing and analysis can require sig-niﬁcant signal-processing proﬁciency to extract stimulus-related information from noisy EEG recordings, especiallyfor the single-trial and ongoing-EEG approaches discussedabove. Luckily, the global scale of music neuroscience re-search now underway should provide many opportunitiesfor collaboration, whereby MIR researchers may bypasssome of the above steps if they wish. In addition, thecreation of publicly available repositories of neuroimag-ing data has become a recent area of focus in the fMRIcommunity [45, 46], and the EEG community is followingsuit (music-related EEG datasets include Koelstra et al.’sDEAP [27] and Stober et al.’s OpenMIIR [53]). Such pub-lic datasets, as well as open-source analysis packages suchas EEGLAB [20], can facilitate cross-disciplinary research542 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015even in the absence of formal collaborations.While the ﬁelds of MIR and neuroscience have yet toform a strong connection, there exist many opportunitesfor collaboration that could advance both ﬁelds. It is hopedthat the studies and ideas presented in this review will proveuseful to both MIR researchers and neuroscientists. It islikely that the two ﬁelds will take some time to grow closer;therefore, MIR output using neuroscientiﬁc data may notimmediately reach the neuroscientiﬁc audience (nor shouldit be intended to). Even so, we hope that a greater knowl-edge of neuroscientiﬁc approaches and ﬁndings will sparkthe interest of MIR researchers and lead to future intersec-tions between these two exciting ﬁelds.6. ACKNOWLEDGMENTSThis research is supported by the Wallenberg NetworkInitiative: Culture, Brain, Learning. The authors thankJonathan Berger, Anthony Norcia, Jorge Herrera, and fouranonymous ISMIR reviewers for their helpful suggestionsand feedback on this paper.7. REFERENCES[1]V. Alluri and P. Toiviainen. Exploring perceptual and acous-tical correlates of polyphonic timbre.Music Perception: AnInterdisciplinary Journal, 27(3):223–242, 2010.[2]V. Alluri, P. Toiviainen, I. P. J¨a¨askel¨ainen, E. Glerean,M. Sams, and E. Brattico. Large-scale brain networks emergefrom dynamic processing of musical timbre, key and rhythm.NeuroImage, 59(4):3677–3689, 2012.[3]V. Alluri, P. Toiviainen, T. E. Lund, M. Wallentin, P. Vuust,A. K. Nandi, T. Ristaniemi, and E. Brattico. From Vivaldito Beatles and back: Predicting lateralized brain responses tomusic.NeuroImage, 83(0):627–636, 2013.[4]E. Altenm¨uller, K. Sch¨urmann, V. K. Lim, and D. Parlitz.Hits to the left, ﬂops to the right: Different emotions duringlistening to music are reﬂected in cortical lateralisation pat-terns.Neuropsychologia, 40(13):2242–2256, 2002.[5]D. Ariely and G. S. Berns. Neuromarketing: The hope andhype of neuroimaging in business.Nature Reviews Neuro-science, 11(4):284–292, 2010.[6]J. J. Aucouturier and E. Bigand. Mel Cepstrum & Ann Ova:The difﬁcult dialog between MIR and music cognition. InIS-MIR, pages 397–402, 2012.[7]J. J. Aucouturier and E. Bigand. Seven problems thatkeep MIR from attracting the interest of cognition andneuroscience.Journal of Intelligent Information Systems,41(3):483–497, 2013.[8]G. S. Berns and S. E. Moore. A neural predictor of culturalpopularity.Journal of Consumer Psychology, 22(1):154–160,2012.[9]G. M. Bidelman. The role of the auditory brainstem in pro-cessing musically relevant pitch.Frontiers in psychology,4:264, 2013.[10]G. M. Bidelman, J. T. Gandour, and A. Krishnan. Cross-domain effects of music and language experience on the rep-resentation of pitch in the human auditory brainstem.Journalof cognitive neuroscience, 23(2):425–434, February 2011.[11]G. M. Bidelman, J. T. Gandour, and A. Krishnan. Musiciansand tone-language speakers share enhanced brainstem encod-ing but not perceptual beneﬁts for musical pitch.Brain andcognition, 77(1):1–10, October 2011.[12]G. M. Bidelman and A. Krishnan. Neural correlates of con-sonance, dissonance, and the hierarchy of musical pitchin the human brainstem.The Journal of Neuroscience,29(42):13165–13171, 2009.[13]G. M. Bidelman and A. Krishnan. Brainstem correlates of be-havioral and compositional preferences of musical harmony.Neuroreport, 22(5):212–216, March 2011.[14]B. Blankertz, G. Curio, and K. R. M¨uller. Classifying singletrial EEG: Towards brain computer interfacing. InAdvancesin Neural Information Processing Systems, pages 157–164,2002.[15]B. Blankertz, S. Lemm, M. Treder, S. Haufe, and K. R.M¨uller. Single-trial analysis and classiﬁcation of ERPcomponents—a tutorial.NeuroImage, 56(2):814–825, 2011.[16]R. Cabredo, R. S. Legaspi, P. S. Inventado, and M. Numao.An emotion model for music using brain waves. InISMIR,pages 265–270, 2012.[17]F. Cong, V. Alluri, A. K. Nandi, P. Toiviainen, R. Fa, B. Abu-Jamous, L. Gong, B. G. W. Craenen, H. Poikonen, M. Huoti-lainen, and T. Ristaniemi. Linking brain responses to natu-ralistic music through analysis of ongoing EEG and stimu-lus features.IEEE Transactions on Multimedia, 15(5):1060–1069, August 2013.[18]M. De Vos, K. Gandras, and S. Debener. Towards a truly mo-bile auditory braincomputer interface: Exploring the P300to take away.International Journal of Psychophysiology,91(1):46–53, 2014.[19]S. Debener, F. Minow, R. Emkes, K. Gandras, and M. De Vos.How about taking a low-cost, small, and wireless EEG for awalk?Psychophysiology, 49(11):1617–1621, 2012.[20]A. Delorme and S. Makeig. EEGLAB: An open source tool-box for analysis of single-trial EEG dynamics including inde-pendent component analysis.Journal of Neuroscience Meth-ods, 134(1):9–21, 2004.[21]J. P. Dmochowski, M. A. Bezdek, B. P. Abelson, J. S. John-son, E. H. Schumacher, and L. C. Parra. Audience preferencesare predicted by temporal reliability of neural processing.Na-ture communications, 5:4567, 2014.[22]J. S. Downie. Music information retrieval.Annual Review ofInformation Science and Technology, 37(1):295–340, 2003.[23]E. B. Falk, E. T. Berkman, and M. D. Lieberman. From neuralresponses to population behavior: Neural focus group pre-dicts population-level media effects.Psychological Science,23(5):439–445, 2012.[24]J. Futrelle and J. S. Downie. Interdisciplinary communitiesand research issues in music information retrieval. InISMIR,pages 215–221, 2002.[25]G. C. Galbraith, P. W. Arbagey, R. Branski, N. Comerci,and P. M. Rector. Intelligible speech encoded in the hu-man brain stem frequency-following response.Neuroreport,6(17):2363–2367, November 1995.[26]G. C. Galbraith, S. M. Bhuta, A. K. Choate, J. M. Kitahara,and T. A. Mullen. Brain stem frequency-following responseto dichotic vowels during attention.Neuroreport, 9(8):1889–1893, June 1998.[27]S. Koelstra, C. Muhl, M. Soleymani, J. S. Lee, A. Yaz-dani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras. DEAP:A database for emotion analysis using physiological signals.IEEE Transactions on Affective Computing, 3(1):18–31, Jan-uary 2012.[28]S. Koelstra, A. Yazdani, M. Soleymani, C. Mhl, J. S. Lee,A. Nijholt, T. Pun, T. Ebrahimi, and I. Patras. Single trialclassiﬁcation of EEG and peripheral physiological signals forrecognition of emotions induced by music videos. InBrainInformatics, volume 6334 ofLecture Notes in Computer Sci-ence, pages 89–100. Springer Berlin Heidelberg, 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 543[29]A. Krishnan, G. M. Bidelman, and J. T. Gandour. Neural rep-resentation of pitch salience in the human brainstem revealedby psychophysical and electrophysiological indices.HearingResearch, 268(12):60–66, 2010.[30]A. Krishnan, J. T. Gandour, and G. M. Bidelman. The effectsof tone language experience on pitch processing in the brain-stem.Journal of Neurolinguistics, 23(1):81–95, 2010.[31]A. Krishnan, J. T. Gandour, G. M. Bidelman, and J. Swami-nathan. Experience-dependent neural representation of dy-namic pitch in the brainstem.Neuroreport, 20(4):408–413,March 2009.[32]A. Krishnan, Y. Xu, J. Gandour, and P. Cariani. Encoding ofpitch in the human brainstem is sensitive to language experi-ence.Cognitive Brain Research, 25(1):161–168, 2005.[33]J. H. Lee, M. C. Jones, and J. S. Downie. An analysis ofISMIR proceedings: Patterns of authorship, topic, and cita-tion. InISMIR, pages 58–62, 2009.[34]K. M. Lee, E. Skoe, N. Kraus, and R. Ashley. Selective sub-cortical enhancement of musical intervals in musicians.TheJournal of Neuroscience, 29(18):5832–5840, 2009.[35]A. Lehmann and M. Sch¨onwiesner. Selective attention mod-ulates human auditory brainstem responses: Relative contri-butions of frequency and spatial cues.PloS one, 9(1):e85442,2014.[36]Y. P. Lin, J. R. Duann, W. Feng, J. H. Chen, and T. P. Jung.Revealing spatio-spectral electroencephalographic dynamicsof musical mode and tempo perception by independent com-ponent analysis.Journal of NeuroEngineering and Rehabili-tation, 11(1), 2014.[37]Y. P. Lin, C. H. Wang, T. P. Jung, T. L. Wu, S. K. Jeng, J. R.Duann, and J. H. Chen. EEG-based emotion recognition inmusic listening.IEEE Transactions on Biomedical Engineer-ing, 57(7):1798–1806, July 2010.[38]P. Loui, D. Alsop, and G. Schlaug. Tone deafness: A newdisconnection syndrome?The Journal of Neuroscience,29(33):10215–10220, 2009.[39]R. K. Mehta and R. Parasuraman. Neuroergonomics: A re-view of applications to physical and cognitive work.Frontiersin Human Neuroscience, 7(889), 2013.[40]Y. Morita, H. H. Huang, and K. Kawagoe. Towards music in-formation retrieval driven by EEG signals: Architecture andpreliminary experiments. InIEEE/ACIS 12th InternationalConference on Computer and Information Science (ICIS),pages 213–217, June 2013.[41]G. Musacchia, M. Sams, E. Skoe, and N. Kraus. Musi-cians have enhanced subcortical auditory and audiovisualprocessing of speech and music.Proceedings of the NationalAcademy of Sciences, 104(40):15894–15898, 2007.[42]T. Naselaris, K. N. Kay, S. Nishimoto, and J. L. Gallant. En-coding and decoding in fMRI.NeuroImage, 56(2):400–410,2011.[43]K. A. Norman, S. M. Polyn, G. J. Detre, and J. V. Haxby.Beyond mind-reading: Multi-voxel pattern analysis of fMRIdata.Trends in Cognitive Sciences, 10(9):424–430, 2006.[44]M. K. Petersen, C. Stahlhut, A. Stopczynski, J. E. Larsen, andL. K. Hansen. Smartphones get emotional: Mind reading im-ages and reconstructing the neural sources. InAffective Com-puting and Intelligent Interaction, volume 6975 ofLectureNotes in Computer Science, pages 578–587. Springer BerlinHeidelberg, 2011.[45]R. A. Poldrack, D. M. Barch, J. Mitchell, T. Wager, A. D.Wagner, J. T. Devlin, C. Cumba, O. Koyejo, and M. Mil-ham. Toward open sharing of task-based fMRI data: TheOpenfMRI project.Frontiers in Neuroinformatics, 7(12),2013.[46]R. A. Poldrack and K. J. Gorgolewski. Making big dataopen: Data sharing in neuroimaging.Nature neuroscience,17(11):1510–1517, November 2014.[47]C. Potes, P. Brunner, A. Gunduz, R. T. Knight, and G. Schalk.Spatial and temporal relationships of electrocorticographicalpha and gamma activity during auditory processing.Neu-roImage, 97:188–195, 2014.[48]C. Potes, A. Gunduz, P. Brunner, and G. Schalk. Dynamicsof electrocorticographic (ECoG) activity in human temporaland frontal cortical areas during music listening.NeuroIm-age, 61(4):841–848, 2012.[49]R. S. Schaefer, J. Farquhar, Y. Blokland, M. Sadakata, andP. Desain. Name that tune: Decoding music from the listeningbrain.NeuroImage, 56(2):843–849, 2011.[50]R. S. Schaefer, S. Furuya, L. M. Smith, B. B. Kaneshiro, andP. Toiviainen. Probing neural mechanisms of music percep-tion, cognition, and performance using multivariate decoding.Psychomusicology: Music, Mind, and Brain, 22(2):168–174,2012.[51]E. Skoe and N. Kraus. Auditory brain stem response to com-plex sounds: A tutorial.Ear and hearing, 31(3):302–324,June 2010.[52]S Stober, D. J. Cameron, and J. A. Grahn. Classifying EEGrecordings of rhythm perception. InISMIR, pages 649–654,2014.[53]S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn. To-wards music imagery information retrieval: Introducing theOpenMIIR dataset of EEG recordings from music perceptionand imagination. InISMIR, 2015.[54]A. Stopczynski, C. Stahlhut, J. E. Larsen, M. K. Petersen,and L. K. Hansen. The smartphone brain scanner: A portablereal-time neuroimaging system.PloS one, 9(2):e86733, 2014.[55]I. Sturm, B. Blankertz, C. Potes, G. Schalk, and G. Curio.ECoG high gamma activity reveals distinct cortical repre-sentations of lyrics passages, harmonic and timbre-relatedchanges in a rock song.Frontiers in Human Neuroscience,8(798), 2014.[56]J. Swaminathan, A. Krishnan, and J. T. Gandour. Pitch encod-ing in speech and nonspeech contexts in the human auditorybrainstem.Neuroreport, 19(11):1163–1167, July 2008.[57]P. Toiviainen, V. Alluri, E. Brattico, M. Wallentin, andP. Vuust. Capturing the musical brain with Lasso: Dynamicdecoding of musical features from fMRI data.NeuroImage,88(0):170–180, 2014.[58]M. S. Treder, H. Purwins, D. Miklody, I. Sturm, andB. Blankertz. Decoding auditory attention to instruments inpolyphonic music using single-trial EEG classiﬁcation.Jour-nal of neural engineering, 11(2):026009, April 2014.[59]G. Tzanetakis and P. Cook. Musical genre classiﬁcation ofaudio signals.IEEE Transactions on Speech and Audio Pro-cessing, 10(5):293–302, July 2002.[60]R. J. Vlek, R. S. Schaefer, C. C. A. M. Gielen, J. D. R.Farquhar, and P. Desain. Sequenced subjective accents forbrain-computer interfaces.Journal of neural engineering,8(3):036002, June 2011.[61]R. J. Vlek, R. S. Schaefer, C. C. A. M. Gielen, J. D. R.Farquhar, and P. Desain. Shared mechanisms in perceptionand imagery of auditory accents.Clinical Neurophysiology,122(8):1526–1532, 2011.[62]P. C. M. Wong, E. Skoe, N. M. Russo, T. Dees, and N. Kraus.Musical experience shapes human brainstem encoding of lin-guistic pitch patterns.Nature neuroscience, 10(4):420–422,April 2007.544 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "An Iterative Multi Range Non-Negative Matrix Factorization Algorithm for Polyphonic Music Transcription.",
        "author": [
            "Anis Khlif",
            "Vidhyasaharan Sethu"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416686",
        "url": "https://doi.org/10.5281/zenodo.1416686",
        "ee": "https://zenodo.org/records/1416686/files/KhlifS15.pdf",
        "abstract": "This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is partic- ularly well suited to the task of automatic music tran- scription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time- frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the pro- posed algorithm proceeds iteratively by initially decom- posing a part of the time-frequency representation into a combination of a small subset of all possible notes then re- investing this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The res- olution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy.",
        "zenodo_id": 1416686,
        "dblp_key": "conf/ismir/KhlifS15",
        "keywords": [
            "Non-negative Matrix Factorisation (NMF)",
            "Automatic music transcription (AMT)",
            "Iterative algorithm",
            "Time-frequency representation",
            "Notes",
            "Resolution of dimensionality problem",
            "Transcription accuracy",
            "Octave",
            "Subset",
            "Large subset"
        ],
        "content": "AN ITERATIVE MULTI RANGE NON-NEGATIVE MATRIXFACTORIZATION ALGORITHM FOR POLYPHONIC MUSICTRANSCRIPTIONAnis Khlif´Ecole des Mines ParisTech, Franceanis.khlif@mines-paristech.frVidhyasaharan SethuUniversity of New South Wales, Australiav.sethu@unsw.edu.auABSTRACTThis article presents a novel iterative algorithm based onNon-negative Matrix Factorisation (NMF) that is partic-ularly well suited to the task of automatic music tran-scription (AMT). Compared with previous NMF basedtechniques, this one does not aim at factorizing the time-frequency representation of the entire musical signal into acombination of the possible set of notes. Instead, the pro-posed algorithm proceeds iteratively by initially decom-posing a part of the time-frequency representation into acombination of a small subset of all possible notes then re-investing this information in the following step involving alarge subset of notes. Speciﬁcally, starting with the lowestoctave of notes that is of interest, each iteration increasesthe set of notes under consideration by an octave. The res-olution of a lower dimensionality problem used to properlyinitialize matrices for a more complex problem, results ina gain of some percent in the transcription accuracy.1. INTRODUCTIONThe term Automatic Music Transcription (AMT) refers tothe task of designing a system that automatically trans-poses an acoustic signal into a written format that can beread by a musician e.g. sheet music. In Western music, thebasic unit of this transposition is the note, which is partlydeﬁned by its duration and its pitch. When more than onenote can occur at the same time, the music is said to bepolyphonic. Further, each instrument has its own harmonicpattern that is time-dependent for each of its notes. Indeed,the spectral content during the onset part of a note is differ-ent from the one during the sustain or fading parts. AMTof polyphonic musics amounts to tracking the fundamentalfrequencies among a mixture of musical events with pos-sibly overlapping harmonics. Many approaches have beenproposed but the results are still unsatisfactory comparedto what can be achieved by a human expert [5]. Lately,techniques like NMF [17] [16] [7] and Probabilistic Latentc\u0000Anis Khlif, Vidhyasaharan Sethu.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Anis Khlif, Vidhyasaharan Sethu.“An Iterative Multi Range Non-Negative Matrix Factorization algorithmfor polyphonic music transcription”, 16th International Society for MusicInformation Retrieval Conference, 2015.Component Analysis (PLCA) [4] [18] have gained greatinterest since they have proved very efﬁcient in bringingforward the underlying structure of musical data. Both areconceptually linked and have been shown equivalent un-der certain formulations [8]. They provide a frameworkunder which the transcription can be formulated as a cost-function minimization problem, which are deeply studiedproblems and many algorithms exist to solve them. How-ever, these algorithms (such as gradient descent, expecta-tion maximization, alternating least-squares, etc...) sufferfrom major ﬂaws. They offer no guarantees of ﬁnding aglobal minimum (if any) in general, and can easily getstuck in local ones. On top of this, they are highly sensitiveto initial conditions and an improper initialization can leadto bad results [6] [1]. These issues are great liabilities forAMT because the intricate nature of harmonically relatedsounds results in the existence of many local minima whichin turn increases the chance of an incorrect transcription.In this paper we present an NMF-based algorithm tai-lored for the task of AMT, showing increased robustnesswith respect to the issues of ﬁnding proper initializationparameters and avoiding irrelevant local minima.2. THE NMF FRAMEWORK2.1 General overviewThe different steps of the algorithm are presented in Fig-ure 1. First, the time-frequency representation of the sig-nal (spectrogram) is computed by applying a Constant QTransform (CQT) on successive time windows. Then theproposed IMRNMF algorithm is applied to the spectro-gram to produce a matrix representing the activation ofeach note accross time. This matrix is then post-processedto extract chunks representing potential notes which arethen weighted before being truly acknowledged as a noteand transcribed.NMF aims at representing a non-negative signal as anadditive synthesis of events taken from a ﬁnite dictionary.The original signal is then represented by the activation ateach time of a subset of these events. If the signal lendsitself to such description, the decomposition will likely bemeaningful in the sense that it will bring out some of theunderlying structure. In the case of AMT, the decompo-sition of the music into events that can be assimilated tonotes, would be most desirable. A time-frequency repre-330Figure 1: Overall algorithm.sentation, like the spectrogram (which is a matrix contain-ing the amplitude spectrum for a sequence of time win-dows) is an example of additive data where the sourceswould be constituted by the amplitude spectra of the dif-ferent notes composing it. As mentioned, we will con-sider that the time-frequency representation is obtained viaa CQT, which allows all frequencies of interest for all notesto be contained in the same number of frequency bins (inthe time frequency representation) regardless of the octaveor the note unlike the standard Discrete Fourier Transform.More formally, given the spectrogramY2RN⇥T+, whereN is the number of frequency bins and T the number oftemporal frames, and givenKN,T; ﬁndW2RN⇥K+the spectral dictionary matrix, andH2RK⇥T+such thatY⇡WH(1)Where,Ydenotes the spectrogram obtained from theCQT, and which is decomposed into weighted sums of aﬁnite set of notes whose spectra constitute the columns ofW.This decomposition does not have an exact solution andconsequently the typical approach is to ﬁnd a solution thatminimises a suitable cost function,CY(W, H)with the con-strainsts that the elements of W and H are positive. Histor-ically, as introduced by Paatero [15] the canonical normof the matrices difference:kY\u0000WHkwas taken as acost function. Incidently, a factorization is inherently de-pendent on the cost function used to weight the reconsti-tution. As a result, the choice of a relevant cost functionto increase the accuracy of the decomposition has beenlargely studied and yielded signiﬁcant increases in the re-sults. In the next section we review some of the key prin-ciples driving current efforts to enhance the transcriptionthrough NMF related techniques.2.2 Achieving a good factorizationThe best factorization we could hope for, would expressYas the activation of spectral templates that correspond ex-actly to the ones of the notes present in the excerpt. Thatimplies especially, that no existing note be expressed as thesum of two or more elements (columns) of the dictionaryW, (no false detection), or that no combination of two ormore notes be expressed by a single element (no deletion).Such issues are referred to as cross-row talk. A commonresponse to cross-row talk is to try to increase the sparsityof the decomposition matrices, and especially the columnsofH. (A vector is said to be sparse when most of its ele-ments are zeros). The energy is concentrated in a few unitswhich are used to represent typical data vectors. Havinga control over sparsity provides more robustness in ”real-life” situations where the number of sources is not knownby advance and a higher rank than needed is ﬁxed for thedecomposition matrix.Controlling the sparsity is mainly achieved by choosinga suitable cost functionCYand estimation methods thatallow desirable properties to be enforced onWandH. Al-though, the task of ﬁnding a minimum forCYis not easysince the problem is often ill-posed, the reformulation ofthe factorization problem in terms of approaches such asConvex Quadratic Programming [7] [19] [11] provides el-egant frameworks to naturally introduce new cost functions(with regularization parameters), or enforce relevant con-straints onWandH.The control over sparsity can be explicit. In [12], Hoyerdevelops algorithm to enforce constant predeﬁned sparsi-tiessw,shoverWandH. Such conditions are not real-istic in real-life situations for audio data since the degreeof polyphony can evolve throughout the excerpt. In [11],Heiler and Schn¨orr, give a formulation of the factorizationas a second order cone programming problem, enablingthem to enforce only boundary conditions on the sparsi-ties. In [1], an adaptation of the ALS algorithm calledAlternating Hoyer-Constrained Least Squares is proposed.However, this way of enforcing sparsity is often too re-strictive in the case of musical data where the degree ofpolyphony is free to evolve during time, on top of the factthat we do not have prior knowledge on it. Consequently,we would prefer a softer, implicit control over sparsity. Insuch cases, it is often achieved through cost functions thatare expressed in a form where the variation of a parameterprovides an input to indirectly affect sparsity. In [7] thecost function is deﬁned byCy=12kY\u0000WHk22+\u00001kHk1+\u000022kHk22(2)The coefﬁcient\u00001weights the importance given to sparsevectors against a good reconstitution, and\u00002is a Tikhonovregularization parameter. Other successful approaches haveconsidered a class of divergences called\u0000-divergences ascost functions [10], which were successfully applied toAMT in [7].d\u0000(Y|W, H)is deﬁned by:d\u0000(Y|W, H)=8><>:Y⌦logYWH\u0000Y+WH \u0000=1YWH\u0000logYWH\u00001\u0000=01\u0000(\u0000\u00001)(Y\u0000+(\u0000\u00001)(WH)\u0000\u0000\u0000Y⌦(WH)\u0000\u00001)else(3)Where the divisions, the logarithm and the powers haveto be understood element-wise,⌦is the element-wise prod-uct, and1the matrix containing only ones. The choiceProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 331of\u0000provides an indirect control over sparsity. It can benoted that in the case of\u0000=2it reduces to the Euclideandistance, and in the case\u0000=1to the KL-div KL diver-gence, which has been found to promote sparsity [17]. Theminimization of both those cost functions can be achievedthrough multiplicative update rules given in [10] and [7].This is the cost function which has been adopted in theproposed method.Finally, all the algorithms mentioned are highly sensi-tive to initial conditions and perform poorly when dimen-sion and the density of local minima increase. In the caseof AMT, initializing the spectral dictionary matrixWsothat the elements (columns) are structurally relevant, im-proves the factorization a great deal. In [16], the columnsofWare initialized with one for each note at harmonicpositions and zeros elsewhere. It makesWrelevant forthe transcription and straightforward to associate to a note.While it is not too difﬁcult to see howWcan be intitial-ized, it is much less obvious forH.In the next section, we present a versatile algorithm toperform the factorization which can be used with any up-date rules, enhances the sparsity and gives element of an-swer as to how initializeHleading to increased robustness.3. THE PROPOSED FACTORIZATIONALGORITHM3.1 PrincipleThe proposed algorithm performs an iterative factorizationof the spectrogram by initially starting with a single oc-tave of notes prior to incrementing it by an octave in eachsubsequent iteration. The algorithms performs by startingfrom the lowest octave, and by including one higher oc-tave at each step until the whole range of note is covered.LetS={n0,. . . ,nK\u00001}2NKbe an interval of integerscontaining the midi notes considered. The i-th range is thesubset ofSdeﬁned byri={n0,. . . ,n12i\u00001}.\nFigure 2: Cutting of the midi scale in ranges.Only considering the notes lying in this range comesdown to focusing on subregions, in terms of frequency andnotes, of the decomposition matrices deﬁned as follows.Y(i)⇡W(i)H(i)(4)with:Y(i)=Y[lb,uib],•(5)W(i)=W[lb,uib],[ls,uis](6)The columns ofWand the rows ofH, indexed by thesources, are restricted to the subset{ls,. . . ,uis}wherelsdenotes the source of the lower note anduisthe sourceof the higher note of the i-th range. We have the follow-ing equalities:ls=n0anduis=n12i\u00001. The rows ofWandYrepresenting the frequency bins are restricted tothe subregion{lb,. . . ,uib}wherelbdesigns the lower fre-quency bin associated with the fundamental frequency ofthe lower note in the range, anduibthe upper bound for thefrequency bins associated with the fundamental frequencyof the higher note in the i-th range. As previously men-tioned, the spectrogram is computed with a CQT, thereforewe can note that the semitone resolution, b, i.e., the numberof bins associated with a single semitone is a constant. Thesuperscript (i) denotes the restriction of a matrix to the i-thrange. With this notation, we can express the boundariesas:lb=b(n0\u00001) + 1anduib=b(n12i\u00001). All the tempo-ral frames are considered at each step of the factorization,this is noted•.As it has been said, any multiplicative update rule canbe used with this approach. Speciﬁcally, in the work re-ported in this paper, the update rules (8) and (9) for theKL divergence are applied as follows toH(i)and the sub-matrixW(i).H(i) H(i)⌦tW(i)(Y(i)⌦(W(i)H(i)).\u0000\u00002)tW(i)(W(i)H(i)).\u0000\u00001(7)W(i) W(i)⌦(Y(i)⌦(W(i)H(i)).\u0000\u00002)tH(i)(W(i)H(i)).\u0000\u00001tH(i)(8)ThenH(i+1)is initialized as follows (see 3):8>><>>:H(i+1)[ls,uis],•=H(i)H(i+1)[uis,ui+1s],•=random positive matrix(9)\nFigure 3: Initialization ofH(i+1)fromH(i).Figures depicting the evolution of the activation matrixthroughout the different steps are shown in section 5.3.2 Motivation and advantagesThis method has been designed as a way to compensatefor some of the weaknesses of NMF applied to AMT, prin-cipally being having to use more potential sources than332 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015strictly necessary in the decomposition (which can causeconfusion in the factorization, hence the necessity of en-forcing sparsity), and the high spectral similarity betweencertain combination of harmonically related notes, whichadded to the high number of sources is likely to increasethe probability of falling into a local minimum. Startingfrom the lowest octave, helps secure a sound bass-line andavoid confusing notes with weak fundamental with theirupper octave counterpart (octave problem); as it is likely tohappen in the usual implementation since low notes oftenhave a weak fundamental. Incrementing the set of notes bya single octave is also a step in this direction, in order tolimit as much as possible the risks of mistaking a note forone of its harmonically related counterparts. Beside, lim-iting the number of sources reduces the dimension of theproblem and heuristically, the risks of falling into a localminimum. Re-investing knowledge in the next steps of thefactorization helps converge toward a better minimum byensuring convergence on growing subspaces, where confu-sion is less likely. The resulting activation matrix is muchsparser, and much easier to post-process because of themore distinct activation peaks.An additional advantage of the proposed method is thatit allows for different treatments on the parts of the spectro-gram that are factorized. For instance, it allows for the deﬁ-nition of octave-based tolerance thresholds in terms of am-plitude or spatial repartition (peaks with a maximum valueunder a threshold or ranging on less than a given numberof frames will be discarded). Various works in the ﬁeldsof psychoacoustics and acoustic signal processing showedthat such treatment is of the utmost importance in order toreliably weight and perform competitive selection betweenacoustic events distributed across a large frequency spanand with different amplitudes [13] [20] [14].4. BACK-END TRANSCRIPTIONThe back-end transcription limits itself to the mere detec-tion of activation events inH, since the initialization ofWmade straightforward the association between events andnotes.Hhaving previously been normalized we applieda threshold-based onset detection, allowing to debit acti-vation matrix rows into chunks that can further along beweighted and sieved before being labelled as note. Thosechunks are bits of the activation matrix deﬁned by: the midinote (the row number), the onset time and the offset time.The computation of the onsets is performed by applying anadaptive thresholding on the ﬁrst order differential vectorof each row ofHas suggested in [2]. The thresholdingvalue is based on the mean of the half-wave rectiﬁed ﬁrstorder differential signal on the 100 neighbouring frames.The onset is deﬁned as the ﬁrst frame for which the am-plitude is superior than 0.2 times the thresholding value (ithas experimentally been found as a good value).A score on the chunks was deﬁned in order to performa post-selection of the chunk and screen out the ones thatare very likely false positives. This cost function is basedon features of the chunks considered as indicators of theprobability of this chunk to represent a true positive. Thisfeatures are: the length of the chunkl, the maximum valueof the amplitude within this chunkm, the value of the ﬁrstorder differential of the signal at the onset time (represent-ing the steepness of the onset)d, and the energy of thesignalewithin the chunk against the cumulated energy ofthe signal of lower harmonics during the same time rangeel. The score of a chunk is deﬁned as:S=( 1\u0000exp(\u0000lc1))(1\u0000exp(\u0000mc2))(1\u0000exp(\u0000dc3))(1\u0000exp(\u0000ec1el))(10)wherec1,c2,c3, andc4are arbitrary constants. For thetests we used(c1,c2,c3,c4)=( 8,0.1,0.03,0.66)and onlychunks with a score higher than0.2were kept. Thesevalues were experimentally determined as reasonable andwere kept ﬁxed for the totality of our test. No music-speciﬁc ﬁne-tuning was performed.5. EXPERIMENTAL RESULTSTests were performed on the MAPS ENSTDkCldatabase [9] which is composed exclusively of pianorecordings with a wide variety of polyphony, genre,tempo, and rhythm. The set of notes taken into accountranges between the midi notes 21 and 108. The spectro-gram is computed by a CQT algorithm with sixty binsper octave to be robust to frequency shifts around thetheoretical peak position. beta divergence cost function,with\u0000=1(KL-divergence) was chosen for all matrixfactorisations. The matrixWis kept ﬁxed during the step-by-step factorization then, an additional standard NMF isperformed with initialization from previous results. OurIterative Multi Range Non-negative Matrix Factorization(IMRNMF) system is compared against an NMF-basedsystem without the range-by-range factorization but thesame back-end transcription algorithm; and the winningalgorithm of the MIREX 2013 competition in Multi-F0note tracking and Multi-F0 note estimation based onShift Invariant Probabilistic Latent Component Analysis(SIPLCA) [3]. The matrixWis initialized ofﬂine usingthe array provided with theSIPLCAsource code whichconsists of pre-extracted and pre-shifted spectral templatesfor various instruments. An onset-based metric is usedwith a 50 ms tolerance.The transcription is performed on the ﬁrst 30 secondsof each track in the database. The thresholding and weigh-ing constants used in the back-end transcription as well asin the IMRNMF are kept ﬁxed during the whole test in-dependently of the extract being processed, and even bet-ter results can be achieved with a case-by-case ﬁne tun-ing of these constants, based on parameters such as genreand tempo. Below are shown illustrative examples of theevolution of the activation matrix on the MAPSMUS-schu1433ENSTDkCl track.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 333MethodAccuracyF-measureNMF0.380.55SIPLCA0.370.53IMRNMF0.520.69Table 1: Comparative results on the MAPS ENSTDkCldatabase.\nFigure 4:H(2)after the ﬁrst 2 steps\nFigure 5:H(4)after the ﬁrst 4 steps\nFigure 6:H(6)after the ﬁrst 6 steps\nFigure 7: Final output of the factorization\nFigure 8:Hobtained withSIPLCA\nFigure 9: Backend transcription output6. CONCLUSION AND FUTURE WORKA novel Iterative Multi-Range Non-negative MatrixFactorisation (IMRNMF) based algorithm for automaticmusic transcription is presented in this paper. At thecost of increased computational requirements, thoughstill perfectly accessible, the proposed system leads toan increase in transcription accuracy compared to thetop-performing existing algorithms. This increase may be334 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015better explained by the increased sparsity of the activationmatrix. The improved sparsity is most likely due to theproposed algorithm ﬁnding better local minima to the costfunction when compared to the traditional NMF. Whilea number of parameters in the proposed systems are em-pirically determined at this stage (thresholding constants,weighting parameters, chunk-wise cost function in theﬁnal decision process...), a more data-driven approach toestimating them may lead to even better performance andwill be addressed in future work.Acknowledgements.Many thanks to EmmanouilBenetos for providing the sources of the Shift-InvariantProbabilistic Latent Component Analysis algorithm.7. REFERENCES[1]Russell Albright, James Cox, David Duling, Amy NLangville, and C Meyer. Algorithms, initializations,and convergence for the nonnegative matrix fac-torization. Technical report, Tech. rep. 919. NCSUTechnical Report Math 81706. http://meyer. math.ncsu. edu/Meyer/Abstracts/Publications. html, 2006.url: http://citeseerx. ist. psu. edu/viewdoc/download,2006.[2]Juan Pablo Bello, Laurent Daudet, Samer Abdallah,Chris Duxbury, Mike Davies, and Mark B Sandler. Atutorial on onset detection in music signals.Speech andAudio Processing, IEEE Transactions on, 13(5):1035–1047, 2005.[3]Emmanouil Benetos, Srikanth Cherla, and TillmanWeyde. An efﬁcient shift-invariant model for poly-phonic music transcription. In6th International Work-shop on Machine Learning and Music, 2013.[4]Emmanouil Benetos and Simon Dixon. A shift-invariant latent variable model for automatic musictranscription.Computer Music Journal, 36(4):81–94,2012.[5]Emmanouil Benetos, Simon Dixon, Dimitrios Gian-noulis, Holger Kirchhoff, and Anssi Klapuri. Auto-matic music transcription: Breaking the glass ceiling.InISMIR, pages 379–384. Citeseer, 2012.[6]Michael W Berry, Murray Browne, Amy N Langville,V Paul Pauca, and Robert J Plemmons. Algorithms andapplications for approximate nonnegative matrix fac-torization.Computational Statistics & Data Analysis,52(1):155–173, 2007.[7]Arnaud Dessein, Arshia Cont, Guillaume Lemaitre,et al. Real-time polyphonic music transcription withnon-negative matrix factorization and beta-divergence.InISMIR-11th International Society for Music Infor-mation Retrieval Conference, pages 489–494, 2010.[8]Chris Ding, Tao Li, and Wei Peng. On the equivalencebetween non-negative matrix factorization and proba-bilistic latent semantic indexing.Computational Statis-tics & Data Analysis, 52(8):3913–3927, 2008.[9]Valentin Emiya, Roland Badeau, and Bertrand David.Multipitch estimation of piano sounds using a newprobabilistic spectral smoothness principle.Audio,Speech, and Language Processing, IEEE Transactionson, 18(6):1643–1654, 2010.[10]C´edric F´evotte and J´erˆome Idier. Algorithms for non-negative matrix factorization with the\u0000-divergence.Neural Computation, 23(9):2421–2456, 2011.[11]Matthias Heiler and Christoph Schn¨orr. Learningsparse representations by non-negative matrix factor-ization and sequential cone programming.The Journalof Machine Learning Research, 7:1385–1407, 2006.[12]Patrik O Hoyer. Non-negative matrix factorization withsparseness constraints.The Journal of Machine Learn-ing Research, 5:1457–1469, 2004.[13]Anssi Klapuri. Sound onset detection by applying psy-choacoustic knowledge. InAcoustics, Speech, and Sig-nal Processing, 1999. Proceedings., 1999 IEEE Inter-national Conference on, volume 6, pages 3089–3092.IEEE, 1999.[14]Brian CJ Moore.An introduction to the psychology ofhearing. Brill, 2012.[15]Pentti Paatero and Unto Tapper. Positive matrix fac-torization: A non-negative factor model with optimalutilization of error estimates of data values.Environ-metrics, 5(2):111–126, 1994.[16]Stanisław A. Raczy´nski, Nobutaka Ono, and ShigekiSagayama. Multipitch analysis with harmonic nonneg-ative matrix approximation. Inin ISMIR 2007, 8thInternational Conference on Music Information Re-trieval, pages 381–386, 2007.[17]Paris Smaragdis and Judith C Brown. Non-negativematrix factorization for polyphonic music transcrip-tion. InApplications of Signal Processing to Audio andAcoustics, 2003 IEEE Workshop on., pages 177–180.IEEE, 2003.[18]Emmanuel Vincent, Nancy Bertin, and Roland Badeau.Harmonic and inharmonic nonnegative matrix factor-ization for polyphonic pitch transcription. InAcoustics,Speech and Signal Processing, 2008. ICASSP 2008.IEEE International Conference on, pages 109–112.IEEE, 2008.[19]Rafal Zdunek and Andrzej Cichocki. Nonnegative ma-trix factorization with quadratic programming.Neuro-computing, 71(10):2309–2320, 2008.[20]Ruohua Zhou.Feature extraction of musical contentfor automatic music transcription. PhD thesis, EPFL,2006.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 335"
    },
    {
        "title": "Extending a Model of Monophonic Hierarchical Music Analysis to Homophony.",
        "author": [
            "Phillip B. Kirlin",
            "David L. Thomas"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416296",
        "url": "https://doi.org/10.5281/zenodo.1416296",
        "ee": "https://zenodo.org/records/1416296/files/KirlinT15.pdf",
        "abstract": "Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learn- ing algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the mu- sic, as opposed to taking all of the musical voices into ac- count. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present de- tails of the new model, an algorithm for determining the most probable analysis of the music, and a number of ex- periments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the result- ing analyses produced.",
        "zenodo_id": 1416296,
        "dblp_key": "conf/ismir/KirlinT15",
        "keywords": [
            "computers",
            "data-driven",
            "Schenkerian analysis",
            "machine learning",
            "music theory",
            "probabilistic models",
            "monophonic music",
            "homophonic music",
            "analysis",
            "rests"
        ],
        "content": "EXTENDING A MODEL OF MONOPHONICHIERARCHICAL MUSIC ANALYSIS TO HOMOPHONYPhillip B. KirlinandDavid L. ThomasDepartment of Mathematics and Computer Science, Rhodes Collegekirlinp@rhodes.edu, thodl15@gmail.comABSTRACTComputers are now powerful enough and data setslarge enough to enable completely data-driven studies ofSchenkerian analysis, the most well-established variety ofhierarchical music analysis. In particular, we now haveprobabilistic models that can be trained via machine learn-ing algorithms to analyze music in a hierarchical fashionas a music theorist would. Most of these models, however,only analyze the monophonic melodic content of the mu-sic, as opposed to taking all of the musical voices into ac-count. In this paper, we explore the feasibility of extendinga probabilistic model developed for analyzing monophonicmusic to function with homophonic music. We present de-tails of the new model, an algorithm for determining themost probable analysis of the music, and a number of ex-periments evaluating the quality of the analyses predictedby the model. We also describe how varying the way themodel interprets rests in the input music affects the result-ing analyses produced.1. INTRODUCTIONMusic analysis is primarily concerned with studying thestructure of music compositions, both at the small- andlarge-scale levels. Hierarchical music analysis, best exem-pliﬁed bySchenkerian analysis, illustrates the structure ofa music composition by identifying hierarchical relation-ships among the notes of the music. These relationshipscollectively group the notes into a series of hierarchicallevels that demonstrate the function of each note in the mu-sic in relation to other notes at various levels of the hierar-chy.One of the complicating factors of Schenkerian analy-sis is that there is no single established algorithm for per-forming the analysis. Instead, textbooks present guidelinesand sample analyses from which students gradually learnthe techniques, often through trial and error. Historically,there have been a number of research endeavors that at-tempted to replicate the Schenkerian analysis procedure:purely algorithmic efforts run into problems because of thec\u0000Phillip B. Kirlin and David L. Thomas.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Phillip B. Kirlin and David L. Thomas.“Extending a Model of Monophonic Hierarchical Music Analysis to Ho-mophony”, 16th International Society for Music Information RetrievalConference, 2015.conﬂicting and ambiguous nature of the Schenkerian anal-ysis rule set [4, 6] and up until recently, machine learningapproaches often hit roadblocks due to the lack of a largestandardized corpus of Schenkerian analysis upon whichto train [5, 10, 11].However, more recent efforts to create such a corpusof Schenkerian analysis have led to a data-driven systemcapable of learning to analyze music in a hierarchical fash-ion [7, 9]. This system, however, is only capable of hier-archically analyzing the monophonic main melody of thecomposition, with any other voices or harmonic parts con-tributing only auxiliary information to the algorithms. Inthis work, we study the practicality of extending this mono-phonic model of music analysis to support homophonictextures with a soprano part and a supporting bass line. Wepresent evidence that there are homophonic patterns thatcan be harnessed by machine learning techniques, demon-strate the workings of a probabilistic model on homophonicinput, and evaluate the system both for accuracy and fordetermining where mistakes are made.Because Schenkerian analysis is one of the most com-prehensive forms of music analysis available today [3], theuses of this work extend beyond the obvious applicationof studying computationally-produced analyses of music.Algorithms for calculating music similarity or identifyingmusical styles or genres could be enhanced with the prob-abilistic model described here, as could systems for musicrecommendation or new music discovery. At a more fun-damental level, studying computational models of musicanalysis can lead us towards a better understanding of mu-sical perception and structure [1].2. MODELING MONOPHONY ANDHOMOPHONYSchenkerian analysis hypothesizes that music composi-tions are structured as a series of hierarchical levels de-ﬁned byprolongations: situations where a note, chord, ormelodic interval remains in control of a passage of mu-sic even though it may not be sounding constantly duringthat time. Consider the ﬁve-note descending phrase in Fig-ure 1, occurring over G major harmony. In this melody, amusic theorist would identify a prolongation over the ﬁrstthree notes D–C–B: the note C prolongs the passing mo-tion from the D to the B. A similar prolongation occursamong the notes B–A–G. This places the notes D, B, andG — the notes being prolonged — at a higher structural715level than the C or the A./noteheads.s2/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2DCBAGFigure 1. An arpeggiation of a G-major chord with passingtones. The slurs are a Schenkerian notation used to indicatethe locations of prolongations.However, there is another level of prolongation at workin this melody. The interval of a ﬁfth between the ﬁrst noteD and the last note G is prolonged by the motion to andfrom the B in the middle. This leads to a three-level hierar-chy of intervals as shown in the binary tree in Figure 2(a).An equivalent structure, illustrated in Figure 2(b), is knownas amaximal outerplanar graphorMOP: this structure isequivalent to a binary tree of melodic intervals but repre-sents the same information more succinctly [14]. We claimthat any hierarchical analysis can be represented as a MOP,and therefore, illustrated as a fully-triangulated polygon.(a)(b)D–GD–BB–GD–CC–BB–AA–GDGBCAFigure 2. The prolongational hierarchy of a G-major chordwith passing tones represented as (a) a tree of melodic in-tervals, and (b) a MOP.By combining a corpus of musical excerpts and corre-sponding MOP analyses with a supervised machine learn-ing algorithm, it is possible to learn a probabilistic modelover MOP structures. This model admits aO(n3)algo-rithm for determining the most probable MOP for a newpiece of music [9].In the original MOP model of prolongation, a single tri-angle describes the elaboration of a parent melodic intervalby two child intervals. This model, as ﬁrst conceived, canonly represent monophonic note sequences. As it would bedesirable to enable hierarchical music analysis of all thevoices within a composition, it is worth exploring exten-sions to represent multi-voice musical textures. One pos-sibility is using a separate MOP to represent the structureof each voice in the music: this would allow for indepen-dent analyses of each voice. This representation, however,would increase the computational complexity of the al-gorithm for determining the most probable MOP analysisfromO(n3)toO(n6)for a two-voice composition [13].Instead, we investigate MOPs that store multiple pitchesin a single vertex. In particular, we study MOPs that storeup to two pitches per vertex, with the pitches derived fromseparate soprano and bass voices. We call these new MOPsinterval MOPs, so named because the two pitches stored ina vertex form a harmonic interval between the soprano andbass parts. Where it is necessary to differentiate betweenthe two varieties of MOPs, we will call the original type ofMOP amonophonic MOP.2.1 The Interval MOP ModelConsider the ﬁve-note descending melodic pattern fromFigure 1, now augmented with a bass line, as in Figure3(a). The equivalent interval MOP is shown alongside, inFigure 3(b). Clearly, the triangles within an interval MOPhave the same prolongational interpretations as in mono-phonic MOPs. One will observe, however, that there canbe potential conﬂicts in the prolongational structure be-tween different voices. For instance, consider the melodicvoices in Figure 4(a), where the prolongational slurs implythat the MOP-like structure in Figure 4(b) is necessary torepresent the prolongations in the soprano and bass parts.Unfortunately, the deﬁnition of a MOP prohibits any edge-crossings of this variety: such a crossing breaks the stricthierarchy necessary to maintain the mathematical (and aswe will show later, computational) properties inherent in aMOP.(a)(b)D-GG-GB-BC-GA-D/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/brace178/clefs.G/accidentals.sharp/noteheads.s1/clefs.F/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2Figure 3. (a) A musical passage with independent sopranoand bass parts, and (b) the corresponding interval MOP.(a)(b)D-DC-AC-FB-G /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/clefs.G/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2??Figure 4. (a) A musical passage with conﬂicting sopranoand bass prolongations and (b) the only way of represent-ing both prolongations in a MOP-like structure, illustratingthe conﬂicting edges that would arise.Though note-against-note textures are easily representedin interval MOPs, some explanation is necessary for howto handle more complicated rhythms. When one voice hasa change in pitch while another voice has a sustained pitch(i.e., oblique motion), the sustained note may be duplicatedin the interval MOP. For example, consider the ﬁrst andsecond beats in Figure 3(a): the bass note G is held for bothof these beats and is duplicated in the ﬁrst two vertices ofthe interval MOP in Figure 3(b).Rests can be explicitly represented in an interval MOP.Suppose that the half note G in the bass part of of Figure3(a) were a quarter note followed by a quarter rest. Thiswould alter the interval MOP of Figure 3(b) to have thevertex C–G store the pair C–(rest) instead.Every interval MOP contains two additional vertices,representing the STARTand FINISHof a composition. TheSTARTvertex is always (temporally) the ﬁrst vertex in aMOP: it does not correspond to the ﬁrst note in the mu-sic, but rather should be thought of as occurring before thestart of the music. Similarly, the FINISHvertex is alwaysthe last vertex in a MOP and temporally occurs after theend of the music. These extra vertices are necessary to per-mit any pair of intervals in a MOP to represent the most716 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015abstract level of the musical hierarchy. Consider a MOPnot containing such extra vertices: because prolongationsare oriented temporally, with the left and right vertices ofa prolongation always higher in the structural hierarchythan the middle vertex, the most abstract edge of the MOPwould have to be between the ﬁrst harmonic interval of themusic and the last. Because the ﬁrst and last harmonic in-tervals are not always the most musically important pairof events, including STARTand FINISHallow for any pairof harmonic intervals in the music to represent the mostabstract level in the structural hierarchy [14].In the remainder of this paper, we study the feasibility ofusing interval MOPs to represent Schenkerian analyses fortwo-voice homophonic compositions. We do not considerpolyphonic textures with completely independent voicesdue to the likelihood of encountering conﬂicting prolon-gational structures, such as in Figure 4(a), and the inabilityof interval MOPs to represent such structures, as discussedearlier. We show that there are patterns that arise in the en-coding of music analyses in the interval MOP structure,we illustrate algorithms for harnessing these patterns andidentifying the probabilistically most likely interval MOPanalysis for new pieces of music, and we conclude withexperiments showing how (1) accurately these algorithmscan reproduce ground truth analyses and (2) what sorts oferrors the algorithms make.3. CONSTRUCTING INTERVAL MOPS FROMREAL-WORLD ANALYSESEarlier work in computational Schenkerian analysis hasveriﬁed that there are regularities in the prolongations thathumans identify during the analysis procedure. Speciﬁcally,if we recall that each triangle in a MOP corresponds to athree-note prolongation, then it has been shown that var-ious types of triangle occur more frequently than others[9]. However, in order to conﬁrm this ﬁnding for intervalMOPs, we ﬁrst require an algorithm to convert a pair ofmonophonic MOPs — one representing the soprano lineand one representing the bass line — into a single inter-val MOP. The strategy we use is to ﬁrst align the notesof the monophonic MOPs to create an initial completely-untriangulated interval MOP consisting of correspondingpairs of notes between the soprano and bass MOPs. A pairof notes is created any time there is an temporal overlapbetween a soprano note and a bass note, so an individualnote may appear multiple times in an interval MOP. Next,interior edges are added from the original soprano MOPin corresponding locations in the interval MOP; this hasthe same effect as copying every prolongation from the so-prano MOP to the interval MOP. Lastly, all edges are addedfrom the original bass MOP to the interval MOP that can beadded without creating conﬂicts (overlapping edges). Weprioritize the soprano prolongations because the sopranovoice is more easily heard in the overall music and usuallyis more melodically signiﬁcant.We ran an experiment to verify the appropriateness ofusing interval MOPs as a representation of a multi-voiceSchenkerian analysis. We used an updated version of theSCHENKER41 corpus: a data set containing 41 excerpts ofcommon practice period music and corresponding Schenke-rian analyses. All of the music in the corpus is for a solokeyboard instrument or for voice with keyboard accompa-niment, is in a major key, and does not modulate. All of theexcerpts are between two and sixteen measures in length,but most are either four or eight measures long. 39 of theSchenkerian analyses in the corpus are taken from text-books and two analyses were sourced from a local expertmusic analyst [7]. We translated all the musical excerptsfrom monophonic MOPs to interval MOPs using the algo-rithm described above. Because we are interested in con-ﬁrming that there are patterns in prolongational data as rep-resented by interval MOPs, we examined how often everytype of triangle appeared in the converted interval MOPs.Speciﬁcally, we calculated the frequencies of all trian-gle types in the corpus in order to test the statistical sig-niﬁcance given the null hypothesis that the corpus anal-yses represented as interval MOPs resemble randomly-constructed MOPs in their triangle frequencies. Determin-ing the expected frequency of a triangle in a MOP underthis null hypothesis is straightforward precisely because ofthe mathematical underpinnings of the MOP formulation.Assume we have a polygon withnvertices, numberedclockwise from 0 ton\u00001, and we are interested in the num-ber of times that the triangle between verticesx,y, andz(x<y<z)appears across all complete triangulationsof this polygon. We observe that any triangle drawn insidea polygon necessarily divides the interior of the polygoninto four regions: the triangle itself, plus the three regionsoutside the triangle but inside the polygon, as in Figure 5(though it is possible for some of these regions to be de-generate line segments). Any complete triangulation of thepolygon that contains4xyzmust necessarily completelytriangulate the three regions outside of the triangle, and wesimply multiply the number of ways of triangulating eachof those three regions to obtain the total number of com-plete triangulations that contain4xyz.xyzCn+x-z-1Cz-y-1Cy-x-1\nFigure 5. The number of times4xyzappears in all pos-sible triangulations of the octagon can be calculated fromthe sizes of the shaded regions.The number of ways of triangulating each of the threeregions is directly related to the size of each region, whichwe can calculate from the values of the verticesx,y, andz. The sizes (number of vertices in the polygons) of theseregions arey\u0000x+1,z\u0000y+1, andn+x\u0000z+1, re-spectively. Precisely because edges in MOPs cannot cross,the number of triangles that will appear in each of theseProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 717regions is solely a function of the size of each region: thenumber of ways to triangulate each region is the Catalannumber for the size of each region minus two, and there-fore the complete calculation for the expected frequency isCy\u0000x\u00001·Cz\u0000y\u00001·Cn+x\u0000z\u00001,whereCi=1i+1\u00002ii\u0000.We ran binomial tests for each type of triangle by com-paring the expected frequencies of the triangles with theobserved frequencies in the corpus. We found that therewere 48 different types of triangles that were possible inthe corpus of interval MOPs, where a triangle type was de-ﬁned by categorizing the three harmonic intervals betweenthe endpoints as eitherconsonant,dissonant,single(forsingle notes), ornot applicable(for MOP vertices contain-ing a STARTor FINISHvertex). We checked for trianglesthat were statistically signiﬁcant at the 5% level. By us-ing theˇSid´ak correction, we found that only triangles thathad ap-value of less than 0.001 would be considered sig-niﬁcant; there were six triangles that matched this criteria.These triangles are described in Figure 6.4. A PROBABILISTIC INTERPRETATION OFINTERVAL MOPSNow that we have veriﬁed that there are statistically sig-niﬁcant prolongational patterns in the corpus of intervalMOPs, we may continue towards our goal of developingan algorithm to harness the patterns in such a way as to beable to analyze new compositions. We proceed in a mannersimilar to that which was used in the original probabilisticmodel of monophonic MOPs [9].Given two monophonic sequences of notes, a sopranolineS=s1,s2,...,sn, and a bass lineB=b1,b2,...,bm,our goal is to calculate the most probable analysisAforthese notes, which means maximizingP(A|S, B). An in-terval MOP is deﬁned by the set of trianglesT1,T2,...,Tkwithin, and thus we deﬁneP(A|S, B)=P(T1,T2,...,Tk).This full joint probability distribution cannot be efﬁcientlyestimated using the amount of training data available tous, so we decompose it into a product of probabilities ofindividual triangles:P(T1,T2,...,Tk)⇡P(T1)·P(T2)···P(Tk).In other words, we assume that each triangle in an intervalMOP is independent of the other triangles. An earlier ex-periment [8] veriﬁes that this does not appreciably alter theprobabilistic rankings of the MOP analyses.We deﬁne the individual probability of a triangle withinan interval MOP analysis in terms of random variables rep-resenting the three endpoints of the triangle:P(Ti)=P(Ci|Li,Ri).The three random variables in this distribution each repre-sent either a harmonic interval or a single soprano or bassnote with a rest in the other voice. The endpoints are unam-biguously named because MOPs are oriented by the tem-poral dimension: later notes always appear to the right ofearlier notes.Our goal is to use the SCHENKER41 data set to esti-mateP(C|L, R), but this is impractical due to the high-dimensional nature of the random variables involved: wewould like to use melodic, harmonic, and rhythmic fea-tures of the triangle endpoints, and a data set of 41 analysesdoes not give us enough data to do this by directly countingtriangle frequencies and normalizing them into a probabil-ity distribution. Instead, we use random forests [2], a typeof ensemble classiﬁer, to estimate this probability. Specif-ically, we create a large collection of decision trees, witheach tree designed to predict a certain feature of the middlepointC, trained on a subset of the features of the left andright endpointsLandR. The predictions of all the treesfor a given feature ofCare then aggregated and normal-ized into a probability distribution [12].4.1 FeaturesWe use a set of twenty-seven features to represent a trian-gle. Speciﬁcally, we use eighteen features solely involvingthe left and right endpoints (LandR) to predict nine fea-tures for the center point (C). These features are:•The category of the interval involving the soprano andthe bass note, listed either asCons(Consonant) orDis(Dissonant) (three features, one each forL,C, andR).•For a given note in an interval, the scale degree (1-7)of the note (six features).•The harmony present in the music at the time of theinterval as a Roman numeral (six features). These har-monic labels, provided by experts, are included in theSCHENKER41 corpus.•The broader category of harmony present in the musicat the time of the interval, such astonicordominant(six features).•For a given note in an interval, whether the note wasa chord tone in the harmony present at the time (sixfeatures).In some situations, certain features are not applicable.In the case thatLorRis a STARTor FINISHvertex, thefeatures are marked with invalid values to denote their in-eligibility. Furthermore, in situations whereL,C, orRisnot an interval, but instead a single note, only half of theattributes per category listed above are applicable.5. EVALUATIONAs mentioned earlier, one reason for preferring intervalMOPs to a more complicated representation for multi-voiceprolongational hierarchies is the mathematical elegance ofthe structure, which makes it an efﬁcient choice from whichto infer probabilistic patterns. No less important is fact thatcomputing the optimal triangulation of a polygon can bedone inO(n3)time by using a standard dynamic program-ming algorithm. This is the basis of the existing PARSE-MOPalgorithms designed for monophonic MOPs; we adaptthe algorithms to work with interval MOPs.There are three variations of PARSEMOP; each varia-tion is given different amounts ofa prioriinformation re-garding the most abstract level of the hierarchical analysis718 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Interval LInterval MInterval R0255075100125230DisDisConsConsConsConsConsConsConsSingleDisConsDisConsDisConsConsCons< 1 x 10-6p-value7.39 x 10-64.64 x 10-41.66 x 10-52.77 x 10-42.94 x 10-4Triangle frequency (     observed,      expected) Figure 6. Types of triangles statistically signiﬁcant at the 5% level.being produced. Heinrich Schenker theorized that all tonalmusic compositions were derived from one of a small setof simple structures involving a short melodic progressionharmonized in a speciﬁc way. Thus, the Schenkerian anal-ysis process of ﬁnding prolongations theoretically alwaysreveals one of these structures, known as thefundamentalstructureorUrsatz, at the background level.All variants of the PARSEMOPalgorithm accept the mu-sical score as input, and are told which notes of the scoreconstitute the soprano and bass lines. PARSEMOP-A hasno conception of theUrsatzbuilt into the algorithm, andtherefore will not necessarily ﬁnd one of the fundamen-tal structures in the music when it runs. PARSEMOP-B, onthe other hand, in addition to the musical score, is alsoinformed as to which speciﬁc notes in the score shouldbe placed into the background fundamental structure. Thisversion of PARSEMOP, therefore, will always ﬁnd the cor-rect background structure. PARSEMOP-C is a compromisebetween the structurally-unaware PARSEMOP-A, and theoverly-aware PARSEMOP-B: this version is informed as towhich musicalpitchesconstitute the fundamental structureand in what order they should appear in the output, but thealgorithm is not told the exact locations of the correspond-ing notes in the score.We used leave-one-out cross-validation in conjunctionwith the SCHENKER41 corpus to evaluate how well thethree PARSEMOPalgorithms could reproduce the ground-truth analyses in the corpus. Speciﬁcally, for each of the41 excerpts in the corpus, we trained our probabilisticmodel on the interval MOPs derived from the other 40 ex-cerpts, and then used each PARSEMOPalgorithm to de-rive the most probable analysis for the original piece omit-ted. We compared the algorithmically-produced MOPs tothe ground-truth MOPs using an metric callededge accu-racy, which is the proportion of internal edges in an inter-val MOP that correspond to an edge in the ground-truthinterval MOP. We use this metric rather than proportion oftriangles that match between two analyses because thereare cases where two analyses can have edges in common,indicating some similarity, yet have no triangles in com-mon.Although occasionally music analysts may disagree onwhat the “correct” Schenkerian analysis should look likefor a piece of music, the limited amount of data allowed usonly one ground-truth analysis per musical excerpt.Figure 7 shows the aggregate edge accuracy levels forthe three PARSEMOPalgorithms. For the sake of compar-ison, we included the average edge accuracy as would beobtained by a baseline algorithm that analyzes music ran-domly: this hypothetical algorithm creates triangulationsuniformly at random from the space of all possible com-plete triangulations.\nFigure 7. Edge accuracies for the three PARSEMOPalgo-rithms and the baseline randomized algorithm.Interestingly, the accuracy levels obtained by analyzingall possible analyses for a given piece of music do not fol-low a uniform or normal distribution. In fact, the distri-bution of edge accuracies as would be obtained by select-ing a complete triangulation uniformly at random is quiteskewed, as can be seen in Figure 8. This means that eventhough the PARSEMOPalgorithms never break 80% accu-racy, when compared against the baseline algorithm, theyare doing quite well. In fact, we can use the distributionof edge accuracies from the baseline algorithm to judgeeach PARSEMOPalgorithm’s accuracy against the null hy-pothesis that the PARSEMOPalgorithm does no better thanrandom. This results inp-values of0.1022,<0.0001, and0.0061for the -A, -B, and -C varieties of PARSEMOP, re-spectively.\nFigure 8. Distribution of edge accuracies under the base-line random algorithm.We also analyzed where in the algorithmically-producedMOP analyses PARSEMOPwas making mistakes. Speciﬁ-cally, for each non-perimeter edge in a PARSEMOPanaly-sis that did not correspond to an edge in the ground MOP,we computed theedge depth: a number between 0 and 1 in-dicating how far down in the structural hierarchy the edgelies, with 0 being the most abstract level of the hierarchyProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 719Figure 9. Probability distributions of the locations of errors in the PARSEMOPanalyses.and 1 being the surface level of the music. We producedprobability distributions illustrating the hierarchical loca-tions where PARSEMOPis most likely to make an error;these are shown in Figure 9. These distributions are unsur-prising: PARSEMOP-A and -C make fewer errors at the ex-treme levels of the hierarchy due to the surface-level musicconstraining the low-level decisions at one end and fewerhigh-level decisions to be made at the other. Furthermore,PARSEMOP-B makes fewer mistakes at the most abstractlevel because theUrsatzhas been supplied ahead of time.ParseMop-AParseMop-BParseMop-CBaseline0.4700.4310.4460.7510.7920.7770.5610.5480.5470.2640.2740.269Figure 10. Edge accuracies after three varieties of rest ad-justment for the three PARSEMOPalgorithms and the base-line randomized algorithm.6. ACCOUNTING FOR RESTSThough the results from the previous section indicate thatthe PARSEMOPalgorithms using interval MOPs are doingrelatively well when compared against the baseline algo-rithm, there is still plenty of room for improvement. Onearea we hypothesized that could be adversely affecting ac-curacy is the presence of rests in the soprano and bass parts.Recall that each vertex in an interval MOP holds both a so-prano and bass note, but only for note pairs sounding atthe same time. Notes may be paired with rests if a rest is“sounding” at the same time as a note in the other voice.This may be the wrong musical interpretation, however,in situations where rests are stylistic indications for perfor-mance (e.g., a substitute for staccato markings), rather thanindications that a melodic line contains a true pause. Thus,we present a modiﬁcation to the interval MOP constructionalgorithm that within a voice, extends each note throughany intervening rests up to the start of the next note within avoice. In essence, all rests are eliminated from the sopranoand bass parts, and notes durations are increased to ﬁll thegaps. There are three different versions of the rest adjust-ment algorithm that control which voices are adjusted: justthe soprano, just the bass, or both voices adjusted.The rest adjustment algorithm, when applied to all ofthe soprano lines in the corpus, modiﬁes the durations of102 notes out of a total of 931. When applied to the bassline, the algorithm elongates 316 notes out of a total of 908.We re-ran the earlier cross-validation experiment witheach of the three versions of the rest adjustment algorithm;the updated edge accuracies are shown in Figure 10. In-terestingly, the only situation in which the rest adjustmentalgorithm had any large affect on the edge accuracy wasfor PARSEMOP-A, where it increased the edge accuracyfrom roughly 40% to between 44% and 47%. Effects onPARSEMOP-B and PARSEMOP-C were much smaller, andin some cases caused a slight decrease in accuracy. Theeffects on thep-values under the null hypothesis that thePARSEMOPanalyses resemble analyses chosen at randomwere also small; these newp-values are shown in Table 1.PARSEMOPvariant: A B CSop only0.0420<0.0001 0.0090Bass only0.0853<0.0001 0.0133Sop & Bass0.0601<0.0001 0.0110Table 1.p-values calculated under the null hypothesis thatPARSEMOPanalyses (with the rest adjustment algorithm)resemble analyses done randomly.7. DISCUSSIONOverall, the results from this study are encouraging. Theedge accuracies and their improvement over the randombaseline algorithm imply that interval MOPs can success-fully model a homophonic prolongational hierarchy. In-terval MOPs maintain all of the mathematical and com-putational advantages of monophonic MOPs, including astraightforward learning algorithm and a computationally-efﬁcient method for ﬁnding the most probable analysis fora new piece of music.However, it is clear that interval MOPs cannot representall of the prolongational situations that could arise in poly-phonic textures, namely conﬂicting prolongations betweenvoices. We plan on studying the feasibility of using inde-pendent MOPs for the soprano and bass; this will alleviatethe representational issue, but may require an approxima-tion algorithm for ﬁnding the most probable MOPs for newcompositions in order to remain computationally tractable.720 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Samer A. Abdallah and Nicolas E. Gold. Comparingmodels of symbolic music using probabilistic gram-mars and probabilistic programming. InProceedingsof the Joint 11th Sound and Music Computing Confer-ence and 40th International Computer Music Confer-ence, pages 1524–1531, 2014.[2]Leo Breiman. Random forests.Machine Learning,45(1):5–32, 2001.[3]Matthew Brown.Explaining Tonality. University ofRochester Press, 2005.[4]R. E. Frankel, S. J. Rosenschein, and S. W. Smo-liar. Schenker’s theory of tonal music—its explicationthrough computational processes.International Jour-nal of Man-Machine Studies, 10(2):121–138, 1978.[5]´Edouard Gilbert and Darrell Conklin. A probabilisticcontext-free grammar for melodic reduction. InPro-ceedings of the International Workshop on ArtiﬁcialIntelligence and Music, 20th International Joint Con-ference on Artiﬁcial Intelligence, pages 83–94, Hyder-abad, India, 2007.[6]Michael Kassler. Proving musical theorems I: The mid-dleground of Heinrich Schenker’s theory of tonality.Technical Report 103, Basser Department of ComputerScience, School of Physics, The University of Sydney,Sydney, Australia, August 1975.[7]Phillip B. Kirlin. A data set for computational stud-ies of Schenkerian analysis. InProceedings of the 15thInternational Society for Music Information RetrievalConference, pages 213–218, 2014.[8]Phillip B. Kirlin and David D. Jensen. Probabilisticmodeling of hierarchical music analysis. InProceed-ings of the 12th International Society for Music Infor-mation Retrieval Conference, pages 393–398, 2011.[9]Phillip B. Kirlin and David D. Jensen. Using super-vised learning to uncover deep musical structure. InProceedings of the 29th AAAI Conference on ArtiﬁcialIntelligence, pages 1770–1776, 2015.[10]Alan Marsden. Schenkerian analysis by computer: Aproof of concept.Journal of New Music Research,39(3):269–289, 2010.[11]Panayotis Mavromatis and Matthew Brown. Parsingcontext-free grammars for music: A computationalmodel of Schenkerian analysis. InProceedings of the8th International Conference on Music Perception &Cognition, pages 414–415, 2004.[12]Foster Provost and Pedro Domingos. Tree induc-tion for probability-based ranking.Machine Learning,52(3):199–215, September 2003.[13]Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami. On multiple context-free grammars.Theoretical Computer Science, 88(2):191–229, 1991.[14]Jason Yust.Formal Models of Prolongation. PhD the-sis, University of Washington, 2006.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 721"
    },
    {
        "title": "Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.",
        "author": [
            "Peter Knees",
            "Ángel Faraldo",
            "Perfecto Herrera",
            "Richard Vogl",
            "Sebastian Böck",
            "Florian Hörschläger",
            "Mickael Le Goff"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.4153506",
        "url": "https://doi.org/10.5281/zenodo.4153506",
        "ee": "https://zenodo.org/records/4153506/files/original_metadata.zip",
        "abstract": "The GiantSteps+ EDMKey Dataset includes 600 two-minute sound excerpts from various EDM subgenres, annotated with single-key labels. This dataset focus in problematic Beatport excerpts, so it is biased, but it is interesting to test the robustness of key recognition systems. These 600 tracks have beenanalysed by Daniel G. Camhi and &Aacute;ngel Faraldo, providingpitch-class set descriptions, key and modalchanges, comments and confidence levels for theindividual tracks.\n\nThis dataset is a revision of the originalGiantSteps Key Dataset, available in Github (https://github.com/GiantSteps/giantsteps-key-dataset) and initially described in:\n\nKnees, P., Faraldo, &Aacute;., Herrera, P., Vogl, R., Bck, S., Hrschlger, F., Le Goff, M. (2015). Two Datasets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections. In Proceedings of the 16th International Society for Music Information Retrieval Conference, 364370. Mlaga, Spain.\n\nThe original audio samples belong to online audio snippets from Beatport,an online music store for DJ&#39;s and Electronic Dance Music Producers(http:\\\\www.beatport.com). If this dataset were used in further research, we would appreciate the citation of thecurrent DOI(10.5281/zenodo.1101082) and thefollowing doctoral dissertation, where a detailed description of the properties of this datasetcan be found:\n\n&Aacute;ngel Faraldo (2017).Tonality Estimation in Electronic Dance Music:A Computational and Musically Informed Examination. PhD Thesis. Universitat Pompeu Fabra, Barcelona.",
        "zenodo_id": 4153506,
        "dblp_key": "conf/ismir/KneesFHVBHG15",
        "keywords": [
            "GiantSteps+",
            "EDM Key Dataset",
            "Electronic Dance Music",
            "Key Recognition",
            "Pitch-Class Set",
            "Key and Modal Changes",
            "Beatport",
            "Tempo Estimation",
            "Tonality Estimation",
            "Music Information Retrieval"
        ]
    },
    {
        "title": "Corpus-Based Rhythmic Pattern Analysis of Ragtime Syncopation.",
        "author": [
            "Hendrik Vincent Koops",
            "Anja Volk",
            "W. Bas de Haas"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417213",
        "url": "https://doi.org/10.5281/zenodo.1417213",
        "ee": "https://zenodo.org/records/1417213/files/KoopsVH15.pdf",
        "abstract": "This paper presents a corpus-based study on rhythmic pat- terns in the RAG-collection of approximately 11.000 sym- bolically encoded ragtime pieces. While characteristic mu- sical features that define ragtime as a genre have been de- bated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. There- fore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee’s model of syn- copation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short synco- pation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study con- firms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features.",
        "zenodo_id": 1417213,
        "dblp_key": "conf/ismir/KoopsVH15",
        "keywords": [
            "corpus-based study",
            "rhythmic patterns",
            "RAG-collection",
            "ragtime genre",
            "syncopation patterns",
            "Longuet-Higgins and Lees model",
            "genre-specific features",
            "onset density",
            "amount of syncopation",
            "musicological hypothesis"
        ],
        "content": "CORPUS-BASED RHYTHMIC PATTERN ANALYSIS OF RAGTIMESYNCOPATIONHendrik Vincent KoopsUtrecht Universityh.v.koops@uu.nlAnja VolkUtrecht Universitya.volk@uu.nlW. Bas de HaasUtrecht Universityw.b.dehaas@uu.nlABSTRACTThis paper presents a corpus-based study on rhythmic pat-terns in theRAG-collection of approximately 11.000 sym-bolically encoded ragtime pieces. While characteristic mu-sical features that deﬁne ragtime as a genre have been de-bated since its inception, musicologists argue that speciﬁcsyncopation patterns are most typical for this genre. There-fore, we investigate the use of syncopation patterns in theRAG-collection from its beginnings until the present timein this paper. Using computational methods, this paperprovides an overview on the use of rhythmical patterns ofthe ragtime genre, thereby offering valuable new insightsthat complement musicological hypotheses about this genre.Speciﬁcally, we measure the amount of syncopation foreach bar using Longuet-Higgins and Lee’s model of syn-copation, determine the most frequent rhythmic patterns,and discuss the role of a speciﬁc short-long-short synco-pation pattern that musicologists argue is characteristic forragtime. A comparison between the ragtime (pre-1920)and modern (post-1920) era shows that the two eras differin syncopation pattern use. Onset density and amount ofsyncopation increase after 1920. Moreover, our study con-ﬁrms the musicological hypothesis on the important role ofthe short-long-short syncopation pattern in ragtime. Theseﬁndings are pivotal in developing ragtime genre-speciﬁcfeatures.1. INTRODUCTIONThis paper presents a corpus-based study into rhythmicpatterns in a ragtime corpus (RAG-collection) of approx-imately 11000 pieces (rags), collected by an internationalgroup of ragtime lovers1. TheRAG-collection (RAG-C)was introduced in [16], together with an overview of openquestions and a computational conﬁrmation of musicolog-ical hypotheses of ragtime music.Esparza et al. [3] argue that in MIR, genre classiﬁcationhas often been used as a proxy for measuring the success1Ragtime Admirers Group, seehttp://ragtimecompendium.tripod.com/c\u0000Hendrik Vincent Koops, Anja Volk, W. Bas de Haas.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Hendrik Vincent Koops, Anja Volk,W. Bas de Haas. “Corpus-Based Rhythmic Pattern Analysis of RagtimeSyncopation”, 16th International Society for Music Information RetrievalConference, 2015.of rhythmic similarity measures, based on the assumptionthat“rhythmic content is more or less homogeneous withincertain musical styles”. Their research shows that even fordance music this is often a problematic assumption. There-fore, a better understanding of the relation between rhythmand genre is important. Musicologists and ragtime fanshave argued that rhythmic patterns and syncopation pro-vide the most distinct features of the genre [1]. EdwardBerlin argues that syncopation is“at the core of the con-temporary understanding of ragtime”[2]. However, mu-sicologists also argue that the use of rhythmic patterns hasnot been stable within the development of the genre overtime. Therefore, we investigate ragtime’ssyncopation, itstypical rhythmicalpatternsand their evolution over time.Huron et al. [7] have shown for related genres that syn-copation increases through history, something we hypoth-esize will be the case for ragtime as well. We reﬂect on therhythmicalpatternsof the genre: what are the most char-acteristic rhythmic patterns used in ragtime syncopation,and does their use change over time. Berlin [2] argued forthe importance of a speciﬁc short-long-short pattern in theragtime genre, of which Volk and De Haas [16] showedthat its use increased through history. We extend the re-search in [16] by investigatingallpatterns, to ﬁnd the rel-ative importance of this speciﬁc pattern. We hypothesizethat compared to other patterns appearing in ragtime syn-copation, this short-long-short pattern is one of the mostcharacteristic patterns for the ragtime genre.Our corpus-based study of syncopation complementsextensive research on syncopation in music cognition, inwhich predominantly short rhythmic patterns are studied.Syncopation is considered to create violations in listen-ers’ expectations [11], to contribute to rhythmic complex-ity [17] and to contribute to a sense of groove in music [12,13]. Studying syncopation for entire compositions insteadof short stimuli contributes to understanding how much vi-olation and complexity is used in real compositions of agenre that is considered to be “highly syncopated”.Contribution.The contribution of this paper is three-fold. We present a ﬁrst full, systematic analysis of allrhythmic patterns in melodies appearing in a large corpusof ragtime music. Through a statistical analysis of the fre-quency of patterns over time, this study shows which pat-terns are more important in different time periods. Sec-ond, by using a formal model of syncopation, this study isable to focus on the syncopated parts of rags, commonlythought to be the most characteristic element of ragtime483music. Through this model, it shows the increase of synco-pation use together with its most important rhythmical pat-terns over time. Third, a tactus ﬁnding algorithm is intro-duced that is capable of correctly identifying the number ofbeats in a bar of a ragtime piece. These three contributionsare pivotal in understanding the characteristic features ofragtime music.Synopsis.The remainder of this paper is structured asfollows: Section 2 provides an introduction to ragtime mu-sic and its use of syncopation. Section 3 details the mainmethodology for analysing patterns and syncopation in theRAG-C. Section 4 details the results of syncopation analy-sis and pattern ﬁnding. The paper closes with conclusionsand discussion in section 5.2. RAGTIMEMusicologists agree that ragtime’s most striking elementcan be found in its use of syncopated rhythmical patterns.Berlin [2] even argues that other musical features are ofhardly any importance: ragtime music has no unique musi-cal form, and its melodies do not bear any distinctive traits(except with regard to rhythm). Although rags with hardlyany syncopation exist, musicologists do agree that synco-pation is the dominant and distinctive element in the evo-lution of the ragtime genre. It is therefore that a study intoragtime will invariably involve the analysis of rhythmicalpatterns and syncopation.In this research, we divide the history of ragtime musicinto two eras: the pre-1920ragtime eraand the post-1920modern era. The two eras are distinguished by a remark-able increase in rhythmic experimentation and syncopationaround 1920 [8, page xix]. This change was in part inﬂu-enced by the French Impressionist music and piano per-formers mimicking the very complex rhythms of piano-rollmusic that were in style.2.1 SyncopationSyncopation is“the displacement of the normal musicalaccent from a strong beat to a weak one”, often used bycomposers to avoid regular rhythm by varying position ofthe stress on notes [14]. Musicologists have argued thatragtime’s main identifying trait is its “ragged”, orsynco-patedrhythm. A speciﬁc syncopated pattern is thought tobe of extra importance by Harer [6] and Berlin [2]: the‘short-long-short’121pattern. The 121 pattern appears as1.Untied (inUbar parts):/timesig.C44/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/rests.1|IOIOOOIO OOOOOOOO|/timesig.C44/noteheads.s2 /noteheads.s2/noteheads.s2/rests.1/flags.u3/flags.u3|OOOOOOOO IOIOOOIO|2.Tied (inTbar parts):/timesig.C44/rests.2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2|OOOOIOIO OOIOOOOO|/rests.2/noteheads.s2/timesig.C44/noteheads.s2/rests.2/noteheads.s2/rests.1/rests.1/noteheads.s2OOOOIOIO|OOIOOOOO3.Augmented:/timesig.C44/noteheads.s1/noteheads.s2/noteheads.s2|IOOOIOOO OOOOIOOO|Figure 1: 121 patterns in musical notation (left) and equivalentbinary onset pattern (right).ˇ“(ˇ“ˇ“(in44and asˇ“===ˇ“ˇ“===in24. The next sections detail threevariants of the 121 pattern: untied, tied and augmented.Examples of these three types of syncopation in24can alsobe found in Figure 1.Untied syncopation.Inuntiedsyncopation, a patternstarts on a strong metrical position and does not pass overa bar line. In24, the pattern either starts on the ﬁrst quar-ter note position or the second quarter note position. In44,the 121 pattern (ˇ“(ˇ“ˇ“() would start on either the ﬁrst quar-ter note position or the third quarter note position. This isvisualized in Figure 2 as theUbar parts. This way, the121 pattern always constitutes the ﬁrst or second half of abar. Musicologists have argued that this type of syncopa-tion is more characteristic of rags from the early pre-1920ragtime era, being more prominent at the turn of the cen-tury [10], [2, p. 84].Tied syncopation.Tiedsyncopation refers to a patternstarting on a weak metrical position. Just like untied syn-copation, the tied version appears in two variants: eithercreating a tied note over the center of the bar, or over thebarline to the next bar. This is visualized in Figure 2 as theTbar parts. In44this means the pattern starts at the secondor fourth quarter note position. In24this means the patternstarts at the ﬁrst or third eighth note position.Thetiedpattern was found to increase during the pre-1920 era by [16]. Musicologists have argued that com-posers increasingly relied on tied syncopation in the late1910s and 1920s as the ragtime style matured [10, p. 76].Augmented syncopation.A third version of syncopa-tion often found in ragtime music is calledaugmentedsyn-copation. This type of syncopation augments the 121 to thelength of a complete bar (3 of Figure 1). The augmentedpattern appears asˇ“˘“ˇ“in44, and asˇ“(ˇ“ˇ“(in24. This results ina weaker syncopated pattern, which is more characteristicof early ragtime era [2, page 83], but became relatively rareafter 1903.3. METHODOLOGYThis study investigates the use of syncopation and rhyth-mical patterns in theRAG-C, and how these change overtime. We hypothesize that syncopation is an important fea-ture of the ragtime genre, that increases over time. To testthis hypothesis, we ﬁrst extract rhythmical onset patternsrags in theRAG-C, as detailed in Section 3.1. Then, to beable to group the onsets inbarsfor analysis, the number ofbeats per bar need to be determined. To achieve this, a tac-tus ﬁnding algorithm (Section 3.2) that ﬁnds the number ofbeats in a bar of a ragtime piece is implemented.Differentiating between bars with and without syncopa-tion provides insight in the patterns that are most importantwithin ragtime syncopation. To measure the degree of syn-copation of a bar, a model by Longuett-Higgins and Lee isused, as detailed in Section 3.3. These syncopation mea-surements are then used in a pattern recognition step (Sec-tion 3.4), to ﬁnd the frequencies of all possible patternsand the relative 121 frequencies. The following sectionsdescribe each of these steps in detail.484 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20153.1 Onset ExtractionCharacteristic of ragtime music is a ragged or syncopatedmelody over a stable accompaniment that reinforces themeter. The importance of ﬁrst separating a piece into itsindividual rhythmic layers for syncopation measurementswas shown in [13]. Therefore, to be able to analyse synco-pation of the melody of a rag, we split it from its accom-paniment. The accompaniment is used in a tactus ﬁndingstep (detailed in Section 3.2), and the melody is used in apattern ﬁnding step (section 3.3).The melody and accompaniment are split using the sky-line with dip-detection method detailed in [15, 16], whichperforms a near-perfect splitting of a melody and its ac-companiment on a subset of theRAG-C. To be able toanalyse rhythmical patterns properly, both the melody andaccompaniment are quantized. We use the technique de-scribed by Volk & De Haas [16], with the exception ofusing four bins per quarter note, instead of twelve. Thisresults in quantisation to a sixteenth note grid, which wecan apply to the formal model of syncopation describedin Section 3.3. Because of differentnormalized averagequantisation deviations(the average deviation of notes di-vided by theMIDIquarter note length, see [16]) betweenﬁles in the dataset, we keep track of the quantisation er-ror, and disregard allMIDIﬁles with a normalized averagequantisation error above 2%.This results in two sequences of onsets per rag, one rep-resenting the rhythm of the melody, and one representingthe rhythm of the accompaniment. The onsets in the se-quences are represented withI’s as sounding events andO’s as non-sounding events. See the bottom two rows ofthe tree in Figure 2 for an example with its music notationequivalent.3.2 Tactus FindingThis study analyses the onset patterns that appear in syn-copatedbarsof rags. The method in Section 3.1 resultsin a sequence of onsets, therefore we need a way to seg-ment this sequence into bars. One way to achieve this isto use the annotatedMIDItime signature of the rags, butfrom a manual inspection this information was found to benot always reliable. Therefore, a tactus ﬁnding algorithmis created that is able to ﬁnd the number of beats in a bar.This information is used to group the right number of on-sets into bar representations: from a sequence of onsets tosegments representing bars.Two features of ragtime music facilitate time signaturedetection from onset patterns with greater ease, comparedto other genres. First, most rags are written in either24or44, other meters are rare. Secondly, a characteristic featureof ragtime is a stable metre pattern in the accompanimentunderneath a syncopated melody [9]. As a general rule, theaccompaniment“reinforces the meter with a regular alter-nation of low bass notes or octaves on the beat, alternatingwith mid-range chords between the beats”[1].In44(and22), this alternation appears asˇ“ˇ“ˇ“ˇ“. In24, thispattern appears asˇ“==ˇ“ˇ“==ˇ“. This pattern can be used to esti-mate the number of beats in a bar for duple time signatures...¯0.˘“0.ˇ“0.ˇ“(0I,ˇ“(.ˇ“(-3I,ˇ“(.ˇ“-2.ˇ“(-2O,?.ˇ“(-3I,ˇ“(.˘“-1.ˇ“-1.ˇ“(-1O,?.ˇ“(-3O,?.ˇ“-2.ˇ“(-2O,?.ˇ“(-3O?.\n44.?.?.....\nU1.\nU2.\nT1.\nT2Figure 2: Example of a hierarchical metric tree with values.UandTdenote the bar parts where we search for patterns. Theonset pattern (I’s andO’s) represents the 121-pattern (ˇ“(ˇ“ˇ“() in aUpart of a bar. TheLHLvalue of this bar is((\u00002)\u0000(\u00003)) +((\u00001)\u0000(\u00003)) = 1 + 2 = 3.The algorithm presented in this paper ﬁnds the number ofbeats of a rag by assuming that in24the onset density in theaccompaniment is higher than in44. The onset densitydfora sequence of onsets is calculated by dividing the numberof onsets on even positions by the number of onsets on oddpositions. Ifdis larger than a certain threshold, it is as-sumed that the onset density is low, and a time signature of44is assumed. If the fraction is lower than a threshold, theonset density is high and the time signature is assumed tobe24. Using this information, the onset sequence is eithersegmented in 16 onsets per bar in the case of44and 8 onsetsper bar in the case of24. These bar onset patterns are thenused in a next step in which the amount of syncopation ismeasured, as explained in Section 3.3.Evaluation.The tactus ﬁnding algorithm is evaluatedin an experiment using 200 randomly selected rags fromtheRAG-C. After quantization and selecting rags with anormalized average quantization error below 2%, 72 ragsremain. The rags are manually annotated with their cor-rect time signature by a music expert. Using the techniquedescribed in 3.2, the algorithm predicts the correct numberof beats in a bar in 92% (66) of the rags using a thresholdofd=0.8. Of the six songs that were incorrectly iden-tiﬁed, two are not in a24or44time signature (34and68) andfour lack the typical accompaniment pattern. These resultsshow that this method is highly useful as a preprocessingstep for segmenting onsets into bars.3.3 Longuet-Higgins & Lee SyncopationMeasurementFor pattern analysis, we differentiate between bars withand without syncopation and analyse the former, to ﬁndits most characteristic patterns. A formal model of synco-pation introduced by Longuet-Higgins & Lee (LHL) [11]provides a numerical representation of syncopation in abar by assuming that a rhythm in a meter is interpretedby a listener by minimizing the amount of syncopation.In an experimental comparison between different syncopa-tion measurements, Gom´ez et al. [5] found that theLHLagrees closely with the human judgement of syncopation.The notion of minimizing syncopation is expressed in theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 485algorithm, in which syncopation is deﬁned to occur whena note occurs on a weaker position than its succeeding rest(or tied note). This was also shown empirically by Fitch etal. [4], who showed that the recall of a rhythm decreasedwith higherLHLsyncopation.TheLHLmodel computes syncopation using a tree ofmetric hierarchy (see Figure 2 for an example). This treeis built to a minimal depth needed to represent the notes.For example, if a44bar only contains two half notes (˘“˘“), atree of depth 1 is used. In case a note appears on a deeperlevel, a deeper tree is used (e.g. depth 4 in˘“@ˇ“)ˇ“).The nodes of the tree are populated with valueskgivento the left children and\u0000dto right children, wherekis thevalue of the parent of a node and\u0000dis the negative valueof the depth of the tree at that node. The value of the rootof the tree is 0.In theLHLmodel, syncopation occurs where a note (I)with a lower value is followed by a rest (O) with a highervalue. The example in Figure 2 contains two of these (I,O)pairs, the second eighth note followed by a rest, and thethird eighth note followed by a rest. The amount of synco-pation for a pair is the difference in values:O-I,(\u00002)\u0000(\u00003) = 1for the ﬁrst example. The total syncopationvalue of an entire bar is the sum of syncopation pairs withinthat bar:nXi=1(⌫(Oi+1)\u0000⌫(Ii))if⌫(Ii)<⌫(Oi+1)(1)where the subscript denotes theithposition in the bar oflengthnand⌫(')denotes the metric tree value of'.3.4 Pattern ﬁndingTo ﬁnd the frequencies of onset patterns in theRAG-C,apattern ﬁnding algorithm is created. We are interested inthe bar parts where the tied, untied and augmented 121pattern can appear. Therefore, this algorithm ﬁnds the fre-quency of candidate patterns inUandTbar parts (seeFigure 2). With this quantitative measurement of patternfrequencies, we measure whether the 121 pattern is in-deed characteristic for ragtime music in these bar parts,and what other patterns are important. To be able to searchfor patterns inUbar parts, each bar from theRAG-Cisconcatenated with half of the bar that follows it.To ﬁnd the frequencies of patterns inUandT, all pos-sible combination ofI’s andO’s’ are generated for thelength of half a bar. For example, in the case of a bar in44quantized on sixteenth notes, a full bar contains 16 onsets.Therefore, all candidate patterns (\u0000) of length 8 are gener-ated: [O,O,O,O,O,O,O,O]. . . [I,I,I,I,I,I,I,I]. The frequen-cy of each candidate pattern⇢2\u0000is calculated by count-ing how often each one appears in one of theUandTparts,normalized over the total number of bars. Calculating thefrequency of all pattern results in distributions of patternsinUandTbar parts.4. RESULTSThis section describes statistics of syncopation and the re-sults of ﬁnding the most frequently used patterns inUandTparts of syncopated bars. First, results of ﬁnding themost frequent patterns in the entireRAG-Care presented(Section 4.1). Then, theRAG-Cis split into rags fromthe ragtime era (before 1920) and the modern era (after1920) to ﬁnd which patterns are characteristic for theseeras. These results are presented in Section 4.2. In the nextsections,¯xdenotes an average and\u0000denotes a variance.4.1 Syncopation in theRAG-CFrom theRAG-C, 356519 bars are extracted, of which 46%(163197) are syncopated (i.e.LHL>0). The averageLHLvalue of syncopated bars is¯x=2.02,\u0000=1.08. ThelargestLHLsyncopation is 15, corresponding with only 28bars in theRAG-C. Nevertheless, a little over half of thebars (54% = 193322) in theRAG-Cis devoid of any synco-pation (i.e.LHL=0).Finding the most frequenly used patterns inTandUbar parts of bars withLHL>0yields the results in Fig-ure 3. Note that patterns are part of a syncopatedbar,and not necessarily syncopated themselves. For example,a bar consisting of|IOIOOOIO OOOOOOOO|is synco-pated because of the 121 pattern in the ﬁrst half of the bar(IOIOOOIO), however, the second half (OOOOOOOO) isdevoid of any syncopation.The ﬁgure shows that the 121 pattern appears as themost frequent pattern inTbar parts, and as a third mostfrequent pattern inUbar parts. This afﬁrms the hypothesisthat when taking rags from all time periods in considera-tion, the 121 pattern is indeed one of the most character-istic ragtime patterns. The ﬁgures show that for the wholeRAG-C, the 121 is more characteristic inTthan inU.Ragtime and Modern era.We split theRAG-Cintopre-1920ragtime erabars and post-1920modern erabars,[10100010][10001000][10000000][10101010][10001010][10100000][00101000][00001000][10101000][00100000]T0.000.050.100.150.20Average frequencyper bar\n[10101010][10001000][10100010][00001000][00101010][00101000][10001010][00000000][10000000][10100000]UFigure 3: The 10 most frequent patterns inTandUparts of barswithLHL>0. The 121 pattern is visualized darker.\n0246810121416Longuet-Higgins and Lee syncopation value0.000.050.100.150.200.250.300.350.400.45Average frequency per barFigure 4: Percentage of bars withLHL>0 in ragtime era (dark)and modern era (light). Values>7are too small to be visible.486 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015to ﬁnd the change in syncopation degree over time. TheaverageLHLsyncopation of a ragtime era bar withLHL>0is¯x=1.9, and¯x=2.4in the modern era. In a Wilcoxontest for the null hypothesis that two related paired samplescome from the same distribution, we ﬁnd thatp⌧0.001,which shows that the modern era is signiﬁcantly strongersyncopated. Also taking into account the non-syncopatedbars shows a signiﬁcant difference, with¯x=0.83(ragtimeera) and¯x=1.26(modern era), again withp⌧0.001.Figure 4 shows distibution ofLHLsyncopation found insyncopated bars from these two eras. The ﬁgure shows thatbars withLHL=1 are more common in the ragtime era, andLHL=2 is almost equally common in the ragtime era as inthe modern era. Nevertheless, it also shows that bars withLHL>3 are more characteristic for bars from the modernera. Bars withLHL>5 occur twice as often in the modernera compared to the ragtime era.Syncopation per rag.To ﬁnd the distribution and de-gree of syncopated bars of complete rags in theRAG-C, wecomputed statistics on rags. The average syncopation perrag for the wholeRAG-Cis¯x=0.95,\u0000=0.6. AnLHLvalue of 1 roughly corresponds with a single syncopationinside one of theUparts, resulting in a bar ofˇ“==ˇ“><.For the ragtime era, the average syncopation per rag is¯x=0.85,\u0000=0.52. For the modern era this is¯x=1.28,\u0000=0.74. Therefore, in the modern era, syncopation moreoften appears on weaker metric positions that correspondwith lower values in theLHLtree, thereby increasing theLHLvalue of the bar. An example of this is?ˇ“>ˇ“?. For botheras, we ﬁnd that the number of syncopated bars per ragis around 50%, which means that not the number of syn-copated bars increases, but the use of syncopation insidebars does. We found that the difference in syncopation be-tween ragtime and modern era to be highly signiﬁcant withp⌧0.001. When only taking into account the syncopatedbars, we ﬁnd¯x=1.84,\u0000=0.54per rag for the ragtimeera, and¯x=2.29,\u0000=0.67per rag for the modern era,again withp⌧0.001.Both the statistics on rags and bars show that overall,stronger syncopation is more characteristic of modern erarags. In the modern era, syncopation occurred more of-ten on weaker metric positions, thereby increasing theLHLsyncopation. The next section details the difference in pat-terns found between these eras.4.2 Frequent Patterns in Ragtime and Modern EraTo ﬁnd a change in pattern use in syncopation over time,we look at the patterns found in syncopated bars from theragtime era and modern era. Figure 5 and Figure 6 showthe 10 most frequent patterns found inUandTbar partsIn the ﬁgures, the 121 pattern is visualized darker.Patterns appearing inUbar parts.The left side ofFigure 5 shows that the 121 pattern inUoccurs more fre-quently in the modern era compared to the ragtime era.Secondly, it shows that the 121 pattern inUalso becamemore important over time compared to other patterns. Al-though the 121 pattern in the modern era is the second mostfrequent pattern, the difference between the ﬁrst and third\n[10001000][00001000][10101010][10100010][00101010][10001010][00101000][10000000][00000000][10100000]Ragtime eraU0.000.050.100.150.200.25Average frequencyper bar\n[10101010][10100010][00101010][00101000][10001000][00001000][10001010][00000000][00100010][10100000]Modern eraUFigure 5: The 10 most frequentUpatterns found in bars withLHL>0 in ragtime and modern era. 121 pattern is visualizeddarker.[10001000][10100010][10000000][10001010][10101010][10100000][00101000][00001000][10101000][00100000]Ragtime eraT0.000.050.100.150.200.25Average frequencyper bar\n[10100010][10101010][10001010][10001000][00100010][10000000][10100000][00100000][00101010][00101000]Modern eraTFigure 6: The 10 most frequentTpatterns found in bars withLHL>0 in ragtime and modern era. 121 pattern is visualizeddarker.\nRagtimeModernU0.000.020.040.060.080.100.12RagtimeModernTAverage frequencyper barFigure 7: Frequency of augmented pattern inU(left) andT(right) bar parts of ragtime era and modern era bars.most frequent pattern (?ˇ“(ˇ“==ˇ“) is minimal, The ﬁgure af-ﬁrms the hypothesis that the 121 pattern is an importantpattern amongst other patterns, and its importance for theragtime genre inUincreased over time.Another difference between the ragtime and modern eracan be found in the onset density and metrical position ofonsets. In the ragtime era, the top most frequent patternshave a lower onset density and have more notes on strongmetrical position, indicating that the patterns used in theragtime era are less complex. In the modern era, the topmost frequent patterns are more dense and have more notesoccurring on weaker metrical positions.Patterns appearing inTbar parts.Figure 6 shows the10 most frequent patterns found inTparts of syncopatedbars. The ﬁgure shows that the 121 pattern is an impor-tant pattern inT, being the second most frequent patternin the ragtime era and by far the most frequent pattern inthe modern era. These results afﬁrm the hypothesis thatboth the importance and use of the 121 pattern inThasincreased over time. In a study by Huron et al. [7], a top10 of most frequently found syncopation patterns in soundrecordings of American popular music spanning the periodProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4871890 to 1939 is presented. The most frequently found syn-copation pattern appears here as well, as the ﬁfth most im-portant pattern in the modern era bars:?ˇ“ˇ“((OOIOOOIO).Furthermore, an increase in onsets on weak metrical po-sitions is observed. The ﬁrst couple of most frequent pat-terns in the ragtime era are simple rhythms on strong met-rical positions. Conversely, in the ragtime era we observedenser patterns. Both the increase of 121 use and use ofdenser patterns is in line with the argument of Jasen [8]that after around 1920, rags became more difﬁcult to play,because“[. . . ] writers were no longer writing for the at-home amateur pianist, [. . . ], but were writing for them-selves and for other professional performers”.UandTpatterns between eras.A comparison be-tween the leftmost ﬁgures of Figure 5 and Figure 6 showsthat for bothTanU, the most frequent ragtime era patternis the same regular sparse pattern (˘“˘“). Since this patternitself is not syncopated according to theLHLmodel, thisshows that in syncopated bars from the ragtime era mostoften only one half of a bar contains syncopation, indicat-ing a lower amount of overall syncopation compared to themodern era. The 121 pattern is an important pattern for theragtime era, being the fourth most important pattern inUbar parts and second most important pattern inTbar parts.The use of the 121 pattern increased over time, both inUas inT. In the modern era, the 121 pattern is by far themost important pattern inTbar parts, and the second inU.Overall, the most frequent patterns in the ragtime erashow more sparse onset patterns on strong metrical posi-tions, indicating simpler rhythms. The most frequently ob-served pattern in the ragtime era (ˇ“ˇ“) corresponds with apart of the third most common syncopation pattern foundby Huron et al. [7] in sampled music from 1890 to 1939.Nevertheless, the research by Huron et al. does not focusspeciﬁcally on ragtime music, so further cross-genre re-search is needed to ﬁnd if this pattern is speciﬁcally impor-tant for ragtime. Conversely, it is observed that the patternsin the modern era are more dense. Onsets appear more fre-quently on weaker metrical positions, increasing the com-plexity of patterns in terms of onset density over time. Thisagrees with the musicological hypotheses that earlier rag-time is simpler, and the exceptional renewed rhythmicalcreativity from around 1920 onwards [2, 8].Augmented syncopation.Figure 7 shows the frequen-cies of the augmented 121 pattern inUandTbar parts.InU, a difference of around 60% is observed, which re-ﬂects the argument by Berlin [2] that the pattern becomes“quite rare” at the end of the ragtime era. Although rareto begin with inT, the occurrence drops with 50% in themodern era compared with the ragtime era. Care should betaken with drawing conclusions from these results becauseof the low frequency. The observations on the augmentedpattern underline the overall trend of ragtime moving to-wards using onsets on weaker metrical positions and in-creased onset density of patterns, thereby becoming moresyncopated.5. DISCUSSION AND CONCLUSIONThrough this study, we were able to conﬁrm new and ex-isting hypotheses on increasing syncopation and rhythmicpattern use in the ragtime genre.Ragtime music is often described as ‘highly syncopated’.Through theRAG-C, we showed for the ﬁrst time that ina large corpus this translates into about half of the barsof rags being syncopated. Musicologists have argued thatsyncopation is important for the ragtime genre. Throughthe computational means in this study, we can afﬁrm thehypothesis syncopation is a characteristic feature of thegenre. We can also conﬁrm the hypothesis that the amountof syncopation is not stable over time, but increased after1920. More speciﬁcally, by exploring this notion of in-creased syncopation we discovered that the number of syn-copated bars is approximately equal in ragtime and modernera rags, but that theLHLvalues of bars increases.In an analysis of all patterns used in ragtime syncopa-tion, we showed the top 10 most frequently used patterns insyncopated bars. We found that over time, onset patternsbecame more dense with more notes on weaker metricalpositions. This ﬁnding is consistent with the increase ofLHL. We can afﬁrm the ﬁndings by Volk and De Haas [16]on the increase of 121 after the ragtime era. In addition, weshowed that the 121 pattern is a highly important rhythmi-cal pattern for the genre, being one of the most frequentlyused patterns compared to all other patterns.Our corpus-based study on syncopation complementsstudies in music cognition research, which have investi-gated syncopation’s role on violating listeners’ expecta-tions, thereby contributing to listening pleasure of the mu-sic [17]. These studies are predominantly carried out onshort rhythmic stimuli. Understanding the full power ofsyncopation requires its study within entire compositionsas realized within this paper. Violating listeners’ expecta-tion through the use of syncopation in this ragtime corpusis realized on average in half of the bars in the melody.Whether or not there are other genres that use even moreviolations, while still providing a clear sense of meter, willhave to be addressed in future research.To study what ‘highly syncopated’ means in the con-text of other genres, we plan on comparing the amount ofsyncopation in theRAG-Cto other genre datasets. Further-more, a study into the use of the 121 pattern in other genreswould shed light on the relative importance of the patternto ragtime and other genres.6. ACKNOWLEDGEMENTSWe thank R.C. Veltkamp and D. Bountouridis for provid-ing valuable comments on an earlier draft on this text. Theauthors would like to thank anonymous reviewers for theirvaluable comments and suggestions. H.V. Koops, A. Volkand W.B. de Haas are supported by the Netherlands Orga-nization for Scientiﬁc Research, through theNWO-VIDI-grant\u0000\u0000\u0000-\u0000\u0000-\u0000\u0000\u0000to A. Volk.488 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]Edward A. Berlin.Ragtime. Oxford Music Online.Grove Music Online. Oxford University Press. Ad-dressed: April 24, 2015.[2]Edward A. Berlin.Ragtime: A musical and culturalhistory. Univ of California Press, 1984.[3]Tlacael Miguel Esparza, Juan Pablo Bello, and Eric J.Humphrey. From genre classiﬁcation to rhythm sim-ilarity: Computational and musicological insights.Journal of New Music Research, 44(1):39–57, 2015.[4]W. Tecumseh Fitch and Andrew J. Rosenfeld. Percep-tion and production of syncopated rhythms.Music Per-ception: An Interdisciplinary Journal, 25(1):pp. 43–58, 2007.[5]Francisco G´omez, Eric Thul, and Godfried T Tous-saint. An experimental comparison of formal measuresof rhythmic syncopation. InProceedings of the Inter-national Computer Music Conference, pages 101–104,2007.[6]Ingeborg Harer.Ragtime: Versuch einer Typologie.Schneider, 1989.[7]David Huron and Ann Ommen. An empirical study ofsyncopation in american popular music, 1890–1939.Music Theory Spectrum, 28(2):211–231, 2006.[8]David A. Jasen.Ragtime: an encyclopedia, discogra-phy, and sheetography. Taylor & Francis, 2007.[9]Samuel A. Floyd Jr. and Marsha J. Reisser. The sourcesand resources of classic ragtime music.Black MusicResearch Journal, 4:pp. 22–59, 1984.[10]Stanley V. Kleppinger. On the inﬂuence of jazz rhythmin the music of Aaron Copland.American Music,21(1):pp. 74–111, 2003.[11]H. Christopher Longuet-Higgins and Christopher S.Lee. The perception of musical rhythms.Perception,11(2):115–128, 1982.[12]Guy Madison and George Sioros. What musicians doto induce the sensation of groove in simple and com-plex melodies, and how listeners perceive it.Frontiersin Psychology, 5(894), 2014.[13]George Sioros, Andr´e Holzapfel, and Carlos Guedes.On measuring syncopation to drive an interactive mu-sic system. InProceedings of the 13th InternationalSociety for Music Information Retrieval Conference,ISMIR 2012, Porto, Portugal, October 8-12, pages283–288, 2012.[14]Syncopation.The Oxford Dictionary of Music, 2nd ed.rev. Oxford University Press, 2006.[15]Alexandra L. Uitdenbogerd and Justin Zobel. Manipu-lation of music for melody matching. InProceedings ofthe sixth ACM international conference on Multimedia,pages 235–240. ACM, 1998.[16]Anja Volk and W. Bas de Haas. A corpus-based studyon ragtime syncopation. InProceedings of the 14thInternational Society for Music Information RetrievalConference, ISMIR 2013, Curitiba, Brazil, November4-8, 2013, pages 163–168, 2013.[17]Maria A.G. Witek, Eric F. Clarke, Mikkel Wallentin,Morten L. Kringelbach, and Peter Vuust. Syncopation,body-movement and pleasure in groove music.PloSone, 9(4):e94446, 2014.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 489"
    },
    {
        "title": "An Efficient State-Space Model for Joint Tempo and Meter Tracking.",
        "author": [
            "Florian Krebs",
            "Sebastian Böck",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414966",
        "url": "https://doi.org/10.5281/zenodo.1414966",
        "ee": "https://zenodo.org/records/1414966/files/KrebsBW15.pdf",
        "abstract": "Dynamic Bayesian networks (e.g., Hidden Markov Mod- els) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhyth- mic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic pa- rameters from a piece of music. While this allows the mutual dependencies between these parameters to be ex- ploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We in- corporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets.",
        "zenodo_id": 1414966,
        "dblp_key": "conf/ismir/KrebsBW15",
        "keywords": [
            "Dynamic Bayesian networks",
            "Meter tracking",
            "Rhythmic parameters",
            "Joint inference",
            "Bar pointer model",
            "State-space discretisation",
            "Tempo transition model",
            "Beat and downbeat tracking",
            "Accuracy improvement",
            "Reduced complexity"
        ],
        "content": "An Efﬁcient State-Space Model for Joint Tempo and Meter TrackingFlorian Krebs, Sebastian B¨ock, and Gerhard WidmerDepartment of Computational PerceptionJohannes Kepler University, Linz, Austriaflorian.krebs@jku.atABSTRACTDynamic Bayesian networks (e.g., Hidden Markov Mod-els) are popular frameworks for meter tracking in musicbecause they are able to incorporate prior knowledge aboutthe dynamics of rhythmic parameters (tempo, meter, rhyth-mic patterns, etc.). One popular example is thebar pointermodel, which enables joint inference of these rhythmic pa-rameters from a piece of music. While this allows themutual dependencies between these parameters to be ex-ploited, it also increases the computational complexity ofthe models. In this paper, we propose a new state-spacediscretisation and tempo transition model for this class ofmodels that can act as a drop-in replacement and not onlyincreases the beat and downbeat tracking accuracy, but alsoreduces time and memory complexity drastically. We in-corporate the new model into two state-of-the-art beat andmeter tracking systems, and demonstrate its superiority tothe original models on six datasets.1. INTRODUCTIONBuilding machines that mimic the human understandingof music is vital for a variety of tasks, such as organisingand managing today’s huge music collections. In this con-text, automatic inference of metrical structure from a mu-sical audio signal plays an important role. Generally, themetrical structure of music builds upon a hierarchy of ap-proximately regular pulses with different frequencies. Inthe centre of this hierarchy is thebeat, a pulse to whichhumans choose to tap their feet. These beats are againgrouped into bars, with thedownbeatdenoting the ﬁrst beatof each bar.Several approaches have been proposed for tackling theproblem of automatic inference of meter (or subcompo-nents such as beats and downbeats) from an audio sig-nal, with approaches based on machine learning currentlybeing the most successful [1, 5, 12, 13, 22]. All of theseapproaches incorporate probabilistic models, but with dif-ferent model structures: the systems introduced in [5, 13,22] decouple tempo detection from the detection of thec\u0000Florian Krebs, Sebastian B¨ock, and Gerhard Widmer.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Florian Krebs, Sebastian B¨ock, andGerhard Widmer. “An Efﬁcient State-Space Model for Joint Tempo andMeter Tracking”, 16th International Society for Music Information Re-trieval Conference, 2015.beat/downbeat phase, which has the advantage of reducingthe search space of the algorithms but can be problematicif the tempo detection is erroneous. Others [1, 12] modeltempo and beat/downbeat jointly, taking into account theirmutual dependency, which leads to increased model com-plexity.One popular model that jointly models tempo and barposition is thebar pointer model, ﬁrst proposed in [20].In addition to tempo and bar position, the model also inte-grates various rhythmic pattern states. It has been extendedby various authors: in [12,14] the beneﬁt of using rhythmicpattern states to analyse rhythmically diverse music wasdemonstrated, in [18] a simpliﬁcation for models with mul-tiple rhythmic pattern states was proposed, in [17] the la-bel of an acoustic event was additionally modelled in orderto enable a drum robot to distinguish different instruments,and in [6] it was applied to a drum transcription task. Thesealgorithms share the problem of a high space and timecomplexity because of the huge state-space in which theyperform inference. In order to make inference tractable,the state-space is usually divided into discrete cells, witheither ﬁxed [1, 6, 12, 14, 17, 20] or dynamic [15, 18, 21] lo-cations in the state-space. While the former approach canbe formulated as a hidden Markov model (HMM), whichperforms best but is prohibitively complex, the latter usesparticle ﬁltering (PF), which is fast but performs slightlyworse in sub-tasks such as downbeat tracking [15].In this paper, we propose a modiﬁed bar pointer modelwhich not only increases beat and downbeat tracking ac-curacy, but also reduces drastically time and memory com-plexity. In particular, we propose (a) a new (ﬁxed grid)discretisation of the joint tempo and beat/bar state-spaceand (b) a new tempo transition model. We incorporated thenew model into two state-of-the-art beat and meter trackingsystems, and demonstrate its superiority on six datasets.2. METHODIn this section, we describe how we tackle the problem ofmetrical structure analysis using a probabilistic state-spacemodel. In these models, a sequence ofhidden variables,which in our case represent the meter of an audio piece,is inferred from a sequence ofobserved variables, whichare extracted from the audio signal. For ease of presenta-tion, we now consider a state-space of two hidden vari-ables, the position within a bar and the tempo. Includ-ing additional hidden variables, e.g., a rhythmical pattern72state [12, 14,18, 20,21] or an acoustic event label [6, 17] isstraightforward. In the following, we describe the originalbar pointer model [20], its shortcomings, and the proposedimprovements.2.1 The original bar pointer modelThe bar pointer model [20] describes the dynamics of ahypothetical pointer which moves through the space of thehidden variables throughout a piece of music. At each timeframek, we refer to the (hidden) state of the bar pointer asxk=[ \u0000k,˙\u0000k], with\u0000k2{1,2,. . . ,M}denoting the po-sition within a bar, and˙\u0000k2{˙\u0000min,˙\u0000min+1,. . . ,˙\u0000max}the tempo in bar positions per time frame.Mis the totalnumber of discrete positions per bar,N=˙\u0000max\u0000˙\u0000min+1is the total number of distinct tempi,˙\u0000minand˙\u0000maxarerespectively the lowest and the highest tempo. See Fig. 1afor an illustration of such a state space. Finally, we denotethe observation features asyk.Overall, we want to compute the most likely hiddenstate sequencex⇤1:K={x⇤1,x⇤2,. . . ,x⇤K}given a sequenceof observations{y1,y2,. . . ,yK}for each audio piece asx⇤1:K= arg maxx1:KP(x1:K|y1:K).(1)withP(y1:K|x1:K)/P(x1)KYk=2P(xk|xk\u00001)P(yk|xk).(2)Here,P(x1)is theinitial state distribution,P(xk|xk\u00001)is thetransition model, andP(yk|xk)is theobservationmodel, which we further describe in the bottom of this sec-tion. Eq. 1 can be solved using the well-known Viterbialgorithm [16]. Finally, the set of downbeat framesDcanbe extracted from the sequence of bar positions asD={k:\u0000⇤k=1},(3)and the set of beat frames can be obtained analogously byselecting the time frames which correspond to a bar posi-tion that matches a beat position.2.1.1 Initial distributionHere, any prior knowledge (e.g., about tempo distributions)can be incorporated into the model. Like most systems, weuse a uniform distribution in this work.2.1.2 Transition modelThe transition modelP(xk|xk\u00001)can be further decom-posed into a distribution for each of the two hidden vari-ables\u0000k, and˙\u0000kby:P(xk|xk\u00001)=P(\u0000k|\u0000k\u00001,˙\u0000k\u00001)··P(˙\u0000k|˙\u0000k\u00001).(4)The ﬁrst factor isP(\u0000k|\u0000k\u00001,˙\u0000k\u00001)=1x,(5)where1xis an indicator function that equals one if\u0000k=(\u0000k\u00001+˙\u0000k\u00001\u00001)modM+1, and zero otherwise. Themodulo operator makes the bar position cyclic (the last,light grey column in Fig. 1a is identical to the ﬁrst column).The second factorP(˙\u0000k|˙\u0000k\u00001)is implemented byIf˙\u0000min˙\u0000k˙\u0000max,P(˙\u0000k|˙\u0000k\u00001)=8<:1\u0000p˙\u0000,˙\u0000k=˙\u0000k\u00001;p˙\u00002,˙\u0000k=˙\u0000k\u00001+ 1;p˙\u00002,˙\u0000k=˙\u0000k\u00001\u00001,(6)otherwiseP(˙\u0000k|˙\u0000k\u00001)=0.p˙\u0000is the probability of a tempo change. From Eq. 6 it canbe seen that the pointer can perform three tempo transitionsfrom each state (indicated by arrows in Fig. 1a).2.1.3 Observation modelIn this paper, we use two different observation models:The ﬁrst one usesrecurrent neural networksto derive aprobability of a frame being a beat or not [1]. The sec-ond one models the observation probabilities with Gaus-sian mixture models from a two-dimensional onset fea-ture [12, 14]. As the focus of this paper lies on the statediscretisation and the tempo transition model, the reader isreferred to [1, 12, 14] for further details.2.2 Shortcomings of the original modelPrevious implementations of the bar pointer model [1, 2,6, 12, 14, 17] followed [20] in dividing the tempo-positionstate space into equidistant points, with each point alignedto an integer-valued bar position and tempo (see Fig. 1a).This discretisation has a number of drawbacks, which arefurther explained in the following.2.2.1 Time resolutionAs shown in Fig. 1a, the number of position grid points perbar is constant across the tempi. This means that the gridof a bar played at a low tempo has a lower time resolu-tion than of a bar played at high tempo, because both aredivided into the same number of cells. In contrast, thereare more observations available for a bar at a low tempothan for a bar at a high tempo, since the observations areextracted at a constant frame rate. This causes a mismatchbetween the time resolution of the feature extraction andthe time resolution of the discretised bar position.2.2.2 Tempo resolutionAs shown in Fig. 1a, the distance between two adjacenttempo grid points is constant across the grid. This is in-consistent with tempo sensitivity experiments on humans,which have shown that the human ability to notice tempochanges is proportional to the tempo, with the JND (justnoticeable difference) being around 2-5% of the inter beatinterval [4]. Therefore, in order to get a sufﬁciently hightempo resolution at lower tempi, a huge number of tempostates has to be chosen.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 73(a) Original discretisation [20]\n12345678910111213141516123456\nBar positionTempo(b) Proposed discretisationFigure 1: Toy example withM= 16andN=6: Each dot corresponds to a (hidden) state in the tempo-bar-positionstate-space. The arrows indicate examples of possible state transitions.2.2.3 Tempo stabilityAs the tempo model (see Eq. 6) forms a ﬁrst-order Markovchain, the current tempo state is independent of all tempostates given the past tempo state. This means that the tempomodel is not able to reﬂect any long term dependencies be-tween tempo states, which may result in unstable tempotrajectories.2.3 Proposed modelThis section introduces a solution to the problems describedabove. To simplify notation we assume a bar has fourbeats. Extending to other time signatures [20] or modellingbeats instead of bars [1] is straightforward.2.3.1 Time resolutionWe propose making the number of discrete bar positionsMdependent on the tempo by using exactly one bar posi-tion state per audio frame (and thus per observation featurevalue). The number of observations per bar (four beats) ata tempoTin beats per minute (BPM) isM(T)=round(4⇥60T⇤\u0000)(7)with\u0000being the audio frame length. Using Eq. 7, wecompute the number of bar positions of the tempo limitsM(Tmin)andM(Tmax).2.3.2 Tempo resolutionWe can now either model allNmaxtempi that correspondto integer valued bar positions in the interval[M(Tmax),M(Tmin)], withNmax=M(Tmin)\u0000M(Tmax)+1,(8)or select only a subset ofNtempo states. In Section 3, weevaluate the performance of the transition model for vari-ous numbers of tempo states. ForN<Nmax, we choosethe tempo states by distributingNstates logarithmicallyacross the range of beat intervals, trying to mimic the JNDsof the human auditory system [4].2.3.3 Tempo stabilityTo increase the stability of the tempo trajectories we onlyallow transitions at beat positions within a bar. This is il-lustrated in Fig. 1b with the arrows showing examples ofpossible state transitions. In contrast to the original modelwhich allows three tempo transitions at every time step, weallow transitions to each tempo, but only at beat times. Thenew tempo transition model then becomes:If\u0000k2B,P(˙\u0000k|˙\u0000k\u00001)=f(˙\u0000k,˙\u0000k\u00001)elseP(˙\u0000k|˙\u0000k\u00001)=⇢1,˙\u0000k=˙\u0000k\u00001;0,otherwise(9)Bis the set of bar positions that corresponds to beats, andf(·)is a function that models the tempo change probabil-ities. We experimented with various functions (Gaussian,Log-Gaussian, Gaussian mixtures), but found this expo-nential distribution to be performing best:f(˙\u0000k,˙\u0000k\u00001)=exp(\u0000\u0000⇥|˙\u0000k˙\u0000k\u00001\u00001|)(10)where the rate parameter\u00002Z\u00000determines the steep-ness of the distribution. A value of\u0000=0means that tran-sitions to all tempi are equally probable. In practice, formusic with roughly constant tempo, we set\u00002[1,300].Fig. 2 shows the tempo transition probabilities for variousvalues of\u0000.2.4 Complexity of the inference algorithmIn this section, we investigate time and memory complex-ity of the bar pointer model, considering only the com-plexity of the (Viterbi) inference and ignoring the contribu-tion of computing the observation features and observationprobabilities.Both time and space complexity depend on the numberof states of the model. The number of states, in turn, de-pends on the number of bar positions, the tempo ranges,the audio frame length, and the tempo resolution that wechose to model. Let us assume that we have a model with74 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20150.40.60.811.21.41.600.20.40.60.81\ntempok / tempok−1f(tempok, tempok−1)\n  lambda=10lambda=25lambda=100\nFigure 2: Tempo change probability density (Eq. 10) forvarious values of\u0000.Shidden states,Tpossible state transitions per frame, andan audio excerpt withKframes. The memory requirementof the algorithm is then simplyS⇥K, as we have to storethe best predecessor state for each of theSstates for eachtime frame during Viterbi decoding. The time complexity,on the other hand, isT⇥K, as we have to computeTtran-sitions at each time step. In Table 1 we show the values ofSandTof the models used in this paper.3. EXPERIMENTAL SETUPIn this section, we evaluate the proposed model with real-world music data in two experiments1. In the ﬁrst exper-iment, we investigated the effect of the number of tempostatesNand the rate parameter\u0000of the tempo transi-tion function on the meter tracking performance on a train-ing set. We evaluated only the beat tracking performance,as this is the most fundamental task that we wanted tosolve. In the second experiment, we integrated the pro-posed model with the parameters determined in Experi-ment 1 into two state-of-the-art systems and compared themeter tracking performance in terms of accuracy and com-plexity with the original models. Below, we describe thedatasets, the evaluation metrics, and the meter trackingmodels.3.1 DatasetsIn this work, we used seven test datasets, one for Experi-ment 1 and the remaining six for Experiment 2. For moredetails about each datasets, see the corresponding refer-ences:Experimental dataset: This dataset is a subset of the1360-songsdataset [8] excluding the Hainsworth dataset,because it was used in Experiment 2. In total, it includes1139 excerpts (total length 662 minutes).Ballroom dataset[9]: A dataset of 698 30-second ex-cerpts of ballroom dance music (total length 364 minutes).1Additional information as well as the code to reproduce the resultsof this paper are available athttp://www.cp.jku.at/people/krebs/ismir2015/It was annotated with beat and downbeat times in [14].Hainsworth dataset[10]: A dataset with 222 pieces (to-tal length 199 minutes), covering a wide spectrum of gen-res.SMC dataset[11]: A dataset with 217 pieces which areconsidered difﬁcult for meter inference (total length 145minutes). This set is also part of the MIREX evaluation.Greek dataset[12]: 42 full songs of Cretan leapingdances in2/4meter (total length 140 minutes).Turkish dataset[12]: 82 one-minute excerpts of TurkishMakam music (total length 82 minutes).Indian dataset[19]: The same subset of 118 two-minutelong pieces (total length 235 minutes) as used in [12].3.2 Evaluation metricsTo assess the ability of an algorithm to infer metrical struc-ture, we used ﬁve evaluation metrics - four for beat track-ing and one for downbeat tracking.F-Measure(F): computed from the number of true pos-itives (correctly detected beats within a window of±70msaround an annotation), the false positives, and the falsenegatives.CMLt: quantiﬁes the percentage of correctly trackedbeats at the correct metrical level. In order to count a beatas correct, both previous and next beats have to match anannotation within a tolerance window of±17.5%of theannotated beat interval.AMLt: the same as CMLt, but the detected beats arealso considered to be correct if they occur on the off-beator at double or half of the ground-truth tempo.Cemgil(Cem): places a Gaussian function with stan-dard deviation of 40 ms around the annotations and com-putes the average likelihood of the corresponding beat clos-est to each annotation. In contrast to the other measureswith hard decision boundaries (due to rectangular tolerancewindows), this measure is also sensitive to small timingdifferences between annotated and detected beats.Information Gain(D): measures the deviation of thebeat error distribution from a uniform distribution by com-puting the Kullback-Leibler divergence.Downbeat F-Measure(DB-F): is the same F-measureas used for beats, but considers only downbeats.We implemented the evaluation metrics according to [3]with standard settings. To make them comparable withother work, we excluded the ﬁrst ﬁve seconds in Experi-ment 2 when comparing with the model from [12] but didnot exclude them when comparing with the results from [1].3.3 Meter tracking modelsTo compare the proposed to the original model, we testedits performance with two state-of-the-art meter trackingsystems:RNN-BeatTracker[1]: This model uses arecurrent neu-ral networkto compute the probability of a frame being abeat. This probability is used as an observation probabilityfor an HMM which jointly models tempo and the positionProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 75within a beat period. We used the same MultiModelBeat-Tracker model as described in [1]. The model uses a framelength of10ms. Only beats are detected with this model.GMM-BarTracker[12, 14]: Gaussian Mixture Models(GMMs) are used to compute the observation probabilitiesfor an HMM that jointly models tempo, position within abar and a set of rhythmic bar-patterns. For Experiment 1,the GMMs were trained on theBallroom, theBeatles[3],theHainsworthand theRWCPopular[7] datasets, usingthree rhythmic patterns that correspond to the time signa-tures2/4,3/4and4/4. Pieces with other time signatureswere excluded. For Experiment 2, we used an updated2version of the model described in [12]. The model uses aframe length of20ms and integrates eight rhythmic pat-tern states, one for each of the rhythmic classes. It outputsbeats and downbeats.Note that the difference betweenoriginalandproposedlies only in the deﬁnition of the hidden states and the tran-sition model; both use the same observation model, initialdistribution, and tempo ranges.4. RESULTS AND DISCUSSION4.1 Experiment 1In this experiment, we evaluated the inﬂuence of two pa-rameters of the proposed transition model on the metertracking performance. These parameters are the width ofthe tempo change distribution parametrised by the rate\u0000(Section. 2.3.3, Fig. 3) and the number of tempo statesN(Section 2.3.1, Fig. 4). We chose to display theCemgilaccuracy in Figs. 3 and 4, because it is the only measurethat makes a soft decision to count a beat as correct by us-ing a Gaussian window and thus also takes into accountsmall timing variations. Generally, the plots for the othermeasures were similar.Fig. 3 shows the effect of the parameter\u0000on theCemgilbeat tracking accuracy for both theRNN-BeatTrackerandtheGMM-BarTrackeron theexperimentaldataset, usingthe maximum number of tempo statesNmax. The max-imumCemgilvalues were obtained with\u0000= 125, and\u0000= 95respectively.Using these settings for\u0000, we investigated the effect ofthe number of tempo statesNon the beat tracking per-formance, which is shown in Fig. 4. As the two systemsuse a different audio frame rate, the maximum number oftempo statesNmaxis different too (see Section 2.3.1). Us-ing a tempo range of[55,215]BPM as in [1], theRNN-BeatTrackerhas at mostNmax= 82tempo states, whilefor theGMM-BarTrackerNmax= 41. As can be seenfrom Fig. 4, theCemgilaccuracy converges at⇡75tempostates for theRNN-BeatTrackerand at⇡40for theGMM-BarTracker. This ﬁnding suggests that theBarTrackermightalso beneﬁt from a higher audio frame rate and therefore ahigher number of tempo states. In addition, the numberof tempo states is a suitable parameter to select a trade-offbetween speed and accuracy.2http://www.cp.jku.at/people/krebs/ismir2014/01002003004005004550556065\nLambdaCemgil\n  \nRNN−BeatTrackerGMM−BarTrackerFigure 3: Effect of parameter\u0000on beat trackingCemgilmetric on theexperimentaldataset.\n2030405060708054565860626466\nNumber of tempo statesCemgil\n  \nRNN−BeatTrackerGMM−BarTrackerFigure 4: Effect of the number of tempo states on beattrackingCemgilmetric on theexperimentaldataset.4.2 Experiment 2In this experiment, we integrated the proposed model intotwo state-of-the-art meter tracking systems (Section 3.3)and compared them to the original models. The beat anddownbeat accuracy scores of the original [1, 12] and pro-posed models, together with the number of states and tran-sitions, are shown in Table 1. The proposed model usedthe parameters\u0000andNobtained in Experiment 1.As can be seen, the proposed transition model outper-forms the original model with respect to all performancemetrics on all datasets (exceptAMLt(-0.2%) on the Ball-room dataset), with the added advantage of drastically re-duced complexity. TheCMLtmetric in particular seems tobeneﬁt from the proposed model, with up to20%relativeimprovement on theGreekdataset. Apparently, the restric-tion to change tempo only at beat times results in higherstability and therefore better performance in measures thatare sensitive to continuity, such asCMLtandAMLt.A comparison of the state-space sizes of the original andproposed models shows that the latter uses far fewer statesand transitions. This is particularly apparent for theGMM-76 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015F Cem CMLt AMLt D DB-F States TransitionsRNN-BeatTrackerBallroomOriginal [1] (20 tempo states)0.910 0.845 0.830 0.924 3.469- 11 520 33 280Proposed (82 tempo states)0.919 0.880+ 0.854 0.922 3.552- 5 617 8 343Proposed (55 tempo states)0.917 0.878 0.848 0.921 3.536- 3 369 4 496HainsworthOriginal [1] (20 tempo states)0.840 0.707 0.803 0.881 2.268- 11 520 33 280Proposed (82 tempo states)0.851 0.730 0.805 0.885 2.337- 5 617 8 343Proposed (55 tempo states)0.851 0.729 0.791 0.886 2.332- 3 369 4 496SMCOriginal [1] (20 tempo states)0.529 0.415 0.428 0.567 1.460- 11 520 33 280Proposed (82 tempo states)0.540 0.430 0.460 0.613 1.579- 5 617 8 343Proposed (55 tempo states)0.543 0.431 0.458 0.613 1.578- 3 369 4 496GMM-BarTrackerGreekOriginal [12] (18 tempo states)0.916 0.810 0.778 0.952 2.4200.777 133 200 376 800Proposed (35 tempo states)0.956 0.850 0.935+ 0.965 2.6250.812 26 716 41 708IndianOriginal [12] (18 tempo states)0.799 0.684 0.613 0.845 1.9880.476 133 200 376 800Proposed (35 tempo states)0.850+ 0.737+ 0.703 0.942+ 2.415+0.515 26 716 41 708TurkishOriginal [12] (18 tempo states)0.861 0.679 0.694 0.840 1.4310.617 133 200 376 800Proposed (35 tempo states)0.877 0.689 0.732 0.877 1.5750.632 26 716 41 708Table 1: Performance of the original and proposed transition model on theBallroom,Hainsworth,SMC,Greek,Indian,andTurkishdataset. The+symbol denotes signiﬁcant (p<0.05) improvement over the result in the row above, using aone-way analysis of variance (ANOV A) test of signiﬁcance.BarTracker, which hasa prioria larger state space becauseit models (a) bars instead of beats and (b) eight rhythmicpatterns. With the originalGMM-BarTracker, processinga four-minute piece (12 000frames at50fps), required re-membering1.60⇥109state ids in the Viterbi algorithm,which needs6.39GB stored as 32-bit integers. In con-trast, using the proposed model, only0.32⇥109statesmust be stored - a demand that can be met using 16-bitintegers in only0.64GB of memory. With a MATLABimplementation on an Intel Core i5-2400 CPU with3.1GHz, we can therefore reduce the computation time fortheTurkishdataset from45.8minutes to4.2minutes, in-cluding the computation of the audio features (which takesonly18seconds). Additionally, as already shown in Exper-iment 1, we can further reduce the number of tempo statesfrom82(the maximum number of tempo states as com-puted in Section 2.3.1) to55with theRNN-BeatTracker,with only marginal performance decrease. Compared tothe original model, this implies a reduction of the num-bers of states and transitions by factors of three and seven,respectively. Since in the proposed model most positionstates are needed to model lower tempi, the lower tempolimits mainly determine the size of the state space.5. CONCLUSIONSIn this paper, we have proposed a new discretisation andtempo transition model that can be used as a drop-in re-placement for variants of thebar pointer model. We haveshown that our model outperformed the original one in 32of 33 test cases, while substantially reducing space andtime complexity. We believe that this is an important steptowards lightweight, real-time capable, high-performancemeter inference systems.As part of future work, we plan to investigate whetherchanging tempo only at beat positions also stabilises theparticle ﬁlter versions of thebar pointermodel [15, 18],which would further facilitate reducing computational com-plexity.6. ACKNOWLEDGMENTSThis work is supported by the European Union SeventhFramework Programme FP7 / 2007-2013 through theAustrian Science Fund (FWF) project Z159 and the Gi-antSteps project (grant agreement no. 610591). Thanks toIngrid Abfalter for proofreading.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 777. REFERENCES[1]S. B¨ock, F. Krebs, and G. Widmer. A multi-model ap-proach to beat tracking considering heterogeneous mu-sic styles. InProceedings of the 15th International So-ciety for Music Information Retrieval Conference (IS-MIR), Taipei, 2014.[2]T. Collins, S. B¨ock, F. Krebs, and G. Widmer. Bridg-ing the audio-symbolic gap: The discovery of repeatednote content directly from polyphonic music audio. InAES 53rd International Conference on Semantic Au-dio, London, 2014. Audio Engineering Society.[3]M. Davies, N. Degara, and M. Plumbley. Evaluationmethods for musical audio beat tracking algorithms.Queen Mary University of London, Tech. Rep. C4DM-09-06, 2009.[4]C. Drake and M. Botte. Tempo sensitivity in auditorysequences: Evidence for a multiple-look model.Per-ception & Psychophysics, 54(3):277–286, 1993.[5]S. Durand, J. Bello, D. Bertrand, and R. Gael. Down-beat tracking with multiple features and deep neuralnetworks. InProceedings of the International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), Brisbane, 2015.[6]G. Dzhambazov. Towards a drum transcription systemaware of bar position. InProceedings of the AES 53rdInternational Conference on Semantic Audio, London,2014.[7]M. Goto. AIST annotation for the RWC musicdatabase. InProceedings of the 7th International Con-ference on Music Information Retrieval (ISMIR), pages359–360, Victoria, 2006.[8]F. Gouyon.A computational approach to rhythmdescription-Audio features for the computation ofrhythm periodicity functions and their use in tempo in-duction and music content processing. PhD thesis, Uni-versitat Pompeu Fabra, 2005.[9]F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-takis, C. Uhle, and P. Cano. An experimental compari-son of audio tempo induction algorithms.IEEE Trans-actions on Audio, Speech, and Language Processing,14(5):1832–1844, 2006.[10]S. Hainsworth and M. Macleod. Particle ﬁltering ap-plied to musical tempo tracking.EURASIP Journal onApplied Signal Processing, 2004:2385–2395, 2004.[11]A. Holzapfel, M. Davies, J. Zapata, J. Oliveira, andF. Gouyon. Selective sampling for beat tracking evalu-ation.IEEE Transactions on Audio, Speech, and Lan-guage Processing, 20(9):2539–2548, 2012.[12]A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Track-ing the odd: Meter inference in a culturally diverse mu-sic corpus. InProceedings of the 15th International So-ciety for Music Information Retrieval Conference (IS-MIR), Taipei, 2014.[13]F. Korzeniowski, S. B¨ock, and G. Widmer. Probabilis-tic extraction of beat positions from a beat activationfunction. InProceedings of the 15th International So-ciety for Music Information Retrieval Conference (IS-MIR), Taipei, 2014.[14]F. Krebs, S. B¨ock, and G. Widmer. Rhythmic patternmodeling for beat and downbeat tracking in musicalaudio. InProceedings of the 14th International Societyfor Music Information Retrieval Conference (ISMIR),Curitiba, 2013.[15]F. Krebs, A. Holzapfel, A. T. Cemgil, and G. Widmer.Inferring metrical structure in music using particle ﬁl-ters.IEEE/ACM Transactions on Audio, Speech, andLanguage Processing, 23(5):817–827, 2015.[16]L. Rabiner. A tutorial on hidden markov models and se-lected applications in speech recognition.Proceedingsof the IEEE, 77(2):257–286, 1989.[17]U. S ¸ims ¸ekli, O. S¨onmez, B. Kurt, and A. Cemgil. Com-bined perception and control for timing in robotic mu-sic performances.EURASIP Journal on Audio, Speech,and Music Processing, 2012(1):1–20, 2012.[18]A. Srinivasamurthy, A. Holzapfel, A. Cemgil, andX. Serra. Particle ﬁlters for efﬁcient meter trackingwith Dynamic Bayesian networks. InProceedings ofthe 16th International Society for Music InformationRetrieval Conference (ISMIR), Malaga, 2015.[19]A. Srinivasamurthy and X. Serra. A supervised ap-proach to hierarchical metrical cycle tracking fromaudio music recordings. InProceedings of the Inter-national Conference on Acoustics, Speech and Sig-nal Processing (ICASSP), pages 5217–5221, Florence,2014.[20]N. Whiteley, A. Cemgil, and S. Godsill. Bayesian mod-elling of temporal structure in musical audio. InPro-ceedings of the 7th International Conference on MusicInformation Retrieval (ISMIR), Victoria, 2006.[21]N. Whiteley, A. Cemgil, and S. Godsill. Sequential in-ference of rhythmic structure in musical audio. InPro-ceedings of the International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages IV–1321, Honolulu, 2007.[22]J. Zapata, M. Davies, and E. G´omez. Multi-feature beattracking.IEEE/ACM Transactions on Audio, Speechand Language Processing, 22(4):816–825, 2014.78 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Training Phoneme Models for Singing with &quot;Songified&quot; Speech Data.",
        "author": [
            "Anna M. Kruspe"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416858",
        "url": "https://doi.org/10.5281/zenodo.1416858",
        "ee": "https://zenodo.org/records/1416858/files/Kruspe15.pdf",
        "abstract": "Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several charac- teristics that differentiate it from speech. Therefore, algo- rithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recogni- tion of phonemes in singing. We noticed that this reco- gnition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. Howe- ver, such data does exist for speech. We therefore propose to make phoneme recognition mo- dels more robust for singing by training them on speech data that has artificially been made more “song-like”. We test two main modifications on speech data: Time stret- ching and pitch shifting. Artificial vibrato is also tested. We then evaluate models trained on different combinations of these modified speech recordings. The utilized mode- ling algorithms are Neural Networks and Deep Belief Net- works.",
        "zenodo_id": 1416858,
        "dblp_key": "conf/ismir/Kruspe15",
        "keywords": [
            "speech recognition",
            "singing characteristics",
            "speech algorithms",
            "phoneme recognition",
            "phoneme models",
            "speech data",
            "artificially made",
            "vibrato",
            "speech recordings",
            "modifications"
        ],
        "content": "TRAINING PHONEME MODELS FOR SINGING WITH “SONGIFIED”SPEECH DATAAnna M. KruspeFraunhofer IDMT, Ilmenau, Germanykpe@idmt.fraunhofer.deABSTRACTSpeech recognition in singing is a task that has not beenwidely researched so far. Singing possesses several charac-teristics that differentiate it from speech. Therefore, algo-rithms and models that were developed for speech usuallyperform worse on singing.One of the bottlenecks in many algorithms is the recogni-tion of phonemes in singing. We noticed that this reco-gnition step can be improved when using singing data inmodel training, but to our knowledge, there are no largedatasets of singing data annotated with phonemes. Howe-ver, such data does exist for speech.We therefore propose to make phoneme recognition mo-dels more robust for singing by training them on speechdata that has artiﬁcially been made more “song-like”. Wetest two main modiﬁcations on speech data: Time stret-ching and pitch shifting. Artiﬁcial vibrato is also tested.We then evaluate models trained on different combinationsof these modiﬁed speech recordings. The utilized mode-ling algorithms are Neural Networks and Deep Belief Net-works.1. INTRODUCTIONAutomatic speech recognition has been a ﬁeld of researchfor more than 30 years now and encompasses a large va-riety of research topics. However, speech recognition al-gorithms have so far only rarely been adapted to singing.One of the reasons for this seems to be that most of thesetasks get harder when using singing because singing datahas different characteristics, which are also often more va-ried than in pure speech [13]. For example, the typicalfundamental frequency for women in speech is between165and200Hz, while in singing it can reach more than1000Hz. Other differences include harmonics, durations,pronunciation, and vibrato.Speech recognition in singing has many interesting prac-tical applications, such as automatic lyrics-to-music ali-gnment, keyword spotting in songs, language identiﬁcationof musical pieces or even full lyrics transcription.A ﬁrst step in many of these tasks is the recognition ofc\u0000Anna M. Kruspe.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Anna M. Kruspe. “Training phonememodels for singing with “songiﬁed” speech data”, 16th International So-ciety for Music Information Retrieval Conference, 2015.phonemes in the audio recording. We showed in [12]that phoneme recognition tends to act as a bottleneck intasks such as language identiﬁcation and keyword spot-ting in singing. Other publications also demonstrate thatphoneme recognition on singing is more difﬁcult than onspeech [15] [6] [13]. This is further compounded by themodels which have usually been trained on pure speechdata.As shown on a small scale in [6] and [12], recognition getsbetter when singing is used as part of the training data. Thebig problem with this is the lack of phoneme-annotated sin-ging data sets.When there is a scarcity of suitable training data, attemptsare often made to generate such data artiﬁcially. For exam-ple, this is often done when models for noisy speech arerequired [11] [7]. In this paper, we therefore propose tomake existing speech data sets more “song-like” and usethese modiﬁed datasets to train models for phoneme reco-gnition in singing. We test this procedure with the com-monly used TIMIT speech dataset [10] and train NeuralNetworks (NNs) and Deep Belief Networks (DBNs) onmodiﬁed versions of it. We then test the models’ perfor-mances on an unaccompanied singing dataset and on thetest section of TIMIT.This paper is structured as follows: We ﬁrst give an intro-duction to the state of the art in section 2 and describe thedatasets in section 3. We then present our new approachin section 4. Section 5 contains our experiments and theirresults. Finally, we give a conclusion in section 6 and sug-gest future work in section 7.2. STATE OF THE ARTAs described in [13] and in [12], there are signiﬁcant diffe-rences between speech and singing data, such as pitch andharmonics, vibrato, phoneme durations and pronunciation.This makes phoneme recognition on singing harder thanon speech.Several approaches to this task have been published. In [5],Gruhne et al. describe a classical approach that employsfeature extraction and various machine learning algorithmsto classify singing into 15 phoneme classes. It also inclu-des a step that removes non-harmonic components fromthe signal. The best result of58%correctly classiﬁed fra-mes is achieved with Support Vector Machine (SVM) clas-siﬁers. The approach is expanded upon in [17].Fujihara et al. describe an approach using ProbabilisticSpectral Templates to model phonemes in [4]. The pho-336neme models are gender-speciﬁc and only model ﬁve vo-wels, but also work for singing with instrumental accompa-niment. The best result is65%correctly classiﬁed frames.Mesaros presented a complex approach that is basedon Hidden Markov Models which are trained on Mel-Frequency Cepstral Coefﬁcients (MFCCs) and then adap-ted to singing using three phoneme classes separately [15][14]. The approach also employs language modeling andhas options for vocal separation and gender and voice ad-aptation. The achieved phoneme recognition rate (accu-racy) on unaccompanied singing is\u00006.4%without adap-tation and20%with singing adaptation using 40 phonemes(the negative value is equivalent to a Levenshtein distanceof1.064, which means that there were more insertion, de-letion, or substitution errors than phoneme instances). Theresults also improve when using gender-speciﬁc adaptation(to an average of18.75%) and even more when languagemodeling is included (to33.4%).Finally, Hansen presents a system in [6] which combi-nes the results of two Multilayer Perceptrons (MLPs), oneusing MFCC features and one using TRAP (Temporal Pat-tern) features. Training is done with a small amount ofsinging data. Viterbi decoding is then performed on theseposterior probabilities. On a set of 27 phonemes, this ap-proach achieves a recall of up to48%.It should be obvious from this overview that comparingthese approaches is not easily possible. Each one usesa different dataset, a different phoneme set, and differentevaluation measures.3. DATASETS3.1 Speech dataFor training our phoneme recognition models, we used thewell-known TIMIT speech dataset [10]. Its training sec-tion consists of 4620 phoneme-annotated English utteran-ces spoken by native speakers. Each utterance is a fewseconds long.The test section of TIMIT contains similar 1680 similarlyphoneme-annotated utterances. We used it to test the ge-neral performance of our models.3.2 Singing dataTo test the performance on singing data, we used the dataset previously presented in [6] and [12]. It consists of thevocal tracks of 19 commercial pop songs in studio quality.We use unaccompanied singing to avoid a possible sourceof interference. They do not contain background music,but have been post-processed (e.g. EQ, compression,reverb). Some of them contain choir singing. Of these19 songs, 12 were annotated with time-aligned phonemesand could therefore be used for our phoneme recogni-tion experiments. We split these 12 songs into 562 clips,each of which roughly represents a line of the songs’ lyrics.4. PROPOSED APPROACHAn overview of our approach is shown in ﬁgure 1. We ﬁrstgenerate ﬁve variants of the TIMIT speech dataset (trainingset). MFCC features are then extracted from these new da-tasets and used to train two models per dataset: A NeuralNetwork and a Deep Belief Network.Similarly, MFCCs are extracted from the TIMIT Test setand from the singing dataset. The ten previously trainedmodels are used to recognize phonemes on these test da-tasets. Viterbi decoding can then be used to generate pho-neme sequences. Finally, the results are evaluated.4.1 Training data modiﬁcationsIn order to make the training data more “song-like”, wedeveloped several variants of this dataset. Table 1 showsan overview over the ﬁve datasets generated from TIMITusing three modiﬁcations. DatasetNis the original TIMITtraining set. For datasetP, four of the eight blocks of TI-MIT were pitch-shifted. For datasetT, ﬁve blocks weretime-stretched and vibrato was applied to two of them. IndatasetTP, the same is done, except with additional pitch-shifting. Finally, datasetMcontains a mix of these modi-ﬁed blocks.In detail, the modiﬁcations were performed in the follo-wing way:Time stretchingFor time stretching, we used the phasevocoder from [3], which is an implementation ofthe Flanagan/Dolson phase vocoder [9] [2]. Thisalgorithm works by ﬁrst performing a Short-TimeFourier Transform (STFT) on the signal and thenresampling the frames to a different duration andperforming the inverse Fourier transform.As demonstrated in [12], time variations in singingare mainly performed on vowels and are often muchlonger than in speech. We therefore used the TI-MIT annotations to only pick out the vowel seg-ments from the utterances. They were modiﬁed ran-domly to a duration between5and100times theoriginal duration and then re-inserted into the ut-terance. This effectively leads to more vowel framesin the training data, but since there is already a largeamount of instances for each phoneme in the origi-nal training data, the effects of this imbalance shouldbe negligible.Pitch shiftingTo pitch-shift the signal, we used codefrom the freely available Matlab toolAutoTune Toy[1] which also implements a phase vocoder. In thiscase, the fundamental frequency is ﬁrst detected au-tomatically. The signal is then stretched or expandedto obtain the new pitch and interpolated to retain theoriginal duration.Using the TIMIT annotations, we split the utteranceup into individual words, then generate a pitch-shifted version of each word and concatenate the re-sults. Pitches are randomly selected from a rangebetween60%and120%of the original pitch.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 337Figure 1: Overview of our phoneme recognition systemNPTTPMDR1NNNNNDR2NNNNNDR3NNNNPDR4NNTTPTVDR5NPTTPTPVDR6NPTTPTVDR7NPTVTPVPDR8NPTVTPVTPVTable 1: The ﬁve TIMIT variants that were used for trai-ning (rows are TIMIT blocks, columns are the ﬁve data-sets). Symbols: N - Unmodiﬁed; P - Pitch-shifted; T -Time-stretched; V - VibratoVibratoThe code for vibrato generation was also takenfromAutoTune Toy. It functions by generating a sinecurve and using this as the trajectory for the pitchshifting algorithm mentioned above. We used a sineof amplitude0.2and frequency6Hz.In singing, vibrato is commonly done on long so-unds, which are usually vowels. Since spoken vo-wels are usually very short, vibrato cannot be per-ceived on them very well. We therefore only appliedvibrato when time stretching was also applied. Vi-brato was then added to the extracted and stretchedvowels.4.2 ModelsUsing the generated data, we trained models using twomachine learning algorithms: Classical Neural Networks(NNs) and Deep Belief Networks (DBNs). Both were im-plemented using the Theano framework for Python1. Inboth cases, we ﬁrst extracted Mel-Frequency Cepstral Co-efﬁcients (MFCCs) and retainen the ﬁrst 13 plus their del-tas and double-deltas as features. We also expanded thetraining data to use 9 context frames. The output layer re-presents the 39 phonemes of the CMU Sphinx phonemeset2. To make the training more exact, these phonemeswere split into triphones, making the dimension of the out-put layer 117.Our ﬁrst models are traditional Neural Networks with twolayers of 200 units each.In recent publications, DBNs have been used very success-fully for phoneme recognition (e.g. [16]). We thereforealso trained DBNs on the speech data. We chose an archi-tecture with three hidden layers and 300 units each. Theﬁrst hidden layer is a Gaussian RBM.Both models are used to generate posterior phoneme pro-babilities. The results for the triphone states of each pho-neme are summed up into one probability for the phoneme.We then run a simple Viterbi decoding on these posteriorsto generate phoneme sequences. In this decoding, all pho-nemes have equal transition probabilities, only the inser-tion penalty is variable (i.e., the transition probability toanother phone). No language models are employed. Wekeep this post-processing simple on purpose so that the re-sults of the various models are easily comparable.4.3 Evaluation measuresAs described in section 2, there is no single common eva-luation measure for phoneme recognition in singing. Wedecided to compare our results using three measures:1http://www.deeplearning.org, last checked 04/29/152http://cmusphinx.sourceforge.net/, last checked04/29/15338 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Percentage of correct framesThis measure describesthe percentage of correctly classiﬁed frames. Cor-rect in this case means that the exact phoneme waschosen for this frame during Viterbi decoding. A si-milar measure was used by Fujihara [4] and Gruhne[5].Phoneme error rateThis is the most commonly usedevaluation measure in phoneme recognition forspeech. It is equal to the Levenshtein distance nor-malized by the length of the ground truth phonemesequence:PER=D+I+SN(1)whereDare deletions,Iare insertions, andSaresubstitutions of phonemes andNis the length of thesequence.The accuracy measure used by Mesaros [15] [14] isthe same as1\u0000PER.Weighted phoneme error rateMesaros also uses a mea-sure calledcorrectwhich ignores insertions. Thismakes sense if we assume that the phoneme resultsare used afterwards by an algorithm that is tolerantto insertions. We decided to go one step further andassume that if algorithms are tolerant to insertions,they can also be somewhat tolerant to deletions. Forcases like this, Hunt suggested a weighted error ratethat punishes insertions and deletions less heavilythan substitutions [8]:PERHunt=0.5D+0.5I+SN(2)5. EXPERIMENTSWe performed our experiments by training a set of modelson all ﬁve TIMIT variants where all other parameters wereleft equal. We then classiﬁed two sets of data with thesemodels: The unmodiﬁed “Test” part of the TIMIT speechdataset (which was not used in training) and our singingdataset. On these phoneme posterior probabilities, we ranthe described simple Viterbi algorithm. The insertion pen-alty was optimized to generate phoneme strings who wereclosest in length to the ground truth phoneme strings. Thethree evaluation measures described in 4.3 were then cal-culated on the result of the Viterbi decoder.We tested two machine learning algorithms: Neural Net-works and Deep Belief Networks.5.1 Neural Network modelsFigure 2 shows the results of the Neural Network models.As ﬁgure 2a demonstrates, results for singing are generallyworse than for speech. The base result for singing is a per-centage of correct frames of14.9%(model trained on theoriginal TIMIT dataset which is denoted asNhere). Whencomparing the models trained on the various TIMIT mo-diﬁcations, a slight improvement is observed for theTandMvariants. For theTdataset, which includes randomlytime-stretched vowels, the result improves to15.4%. Thisis a very small improvement, but it is still interesting tonote. In contrast, none of the modiﬁcations improved theresult on the speech data at all. The base result here is30%.(It should be noted that much higher ﬁgures can be foundin literature, but we have not yet tested improvements likelanguage models or adaptations. Our focus for now was tocompare the different TIMIT modiﬁcations).When looking at the phoneme error rate in ﬁgure 2b ins-tead of the pure frame accuracy, the results become morevisible. The base phoneme error rate for singing is1.16,but falls to1.07for theTPandMmodiﬁcations. Forspeech, it rises from0.6to0.68(TP) and0.66(M) instead.The P andTmodiﬁcations form a middle ground here. ThePvariant (randomly pitch-shifted words) does not changethe results very much in either direction: It decreases theerror rate on singing by0.03and increases it on speechby less than0.01. TheTvariant (randomly time-stretchedvowels) decrease the error on singing by just0.02, but in-crease it on singing by0.07.If we weight insertions and deletions lower than modiﬁca-tions, the phoneme error rates decrease generally (see ﬁ-gure 2c). The described effects are still active when usingthis evaluation measure. The error rate falls from0.88to0.83on singing, and rises from0.48to0.54on speech.The tendency forPandTis similar here.5.2 Deep Belief Network modelsFigure 3 shows the same evaluation measures for the DeepBelief Networks. In general, the results are better and theeffect of the various training sets is similar, but more pro-nounced.The base percentage of correct frames is14%here and ri-ses to19%when training on the randomly timed datasetT.On speech data, the best result is38%for models trainedon the original TIMIT data and falls for all other variants.The phoneme error rate falls from1to0.91on the singingdata. Again, the results are best when the models are trai-ned on theTPorMdatasets, with the model trained onTperforming just slightly worse. The lowest error rate onspeech is0.41with theNmodel.The weighted phoneme error rate sinks from0.77to0.71on the singing data.5.3 Confusion of Deep Belief NetworksAfter evaluating the general performance of the Deep Be-lief Networks, we examined the results in detail. As anexample, table 2 shows the phoneme-wise results for thesinging data. The ﬁrst two columns lists the frame-wiseprecisions and recalls when using theNmodel, the twocolumns after that show the same values for theMmodel,and the last column lists the three phonemes with whichthe concerned phoneme is confused most frequently whenusing theMmodel (exceptsil). This leads to several in-teresting discoveries.It turns out that the precisions of long vowels such asaa,iy, oroyimprove when using theMmodel for recogni-tion, but some consonant accuracies become worse. Thismakes sense since theMmodiﬁcations place an emphasison vowels by randomly stretching them. The consonantProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 339(a) Percentage of correct frames\n(b) Phoneme error rate\n(c) Weighted phoneme error rateFigure 2: Evaluation measures for the results obtained with Neural Network models on singing data (Acap) and on speechdata (Timit). The models were trained with the ﬁve different Timit variants (different colors).\n(a) Percentage of correct frames\n(b) Phoneme error rate\n(c) Weighted phoneme error rateFigure 3: Evaluation measures for the results obtained with Deep Belief Network models on singing data (Acap) and onspeech data (Timit). The models were trained with the ﬁve different Timit variants (different colors).results may become worse because the training data alsocontains randomly pitch-shifted versions, which may notbe a natural modiﬁcation (this can be veriﬁed by lookingat the results of theTmodel). Consonants generally seemharder to recognize since they are shorter than vowels and,for the most part, are less static over their duration. Somevery bad consonant results can also be explained becausethey occur very rarely in the singing dataset (e.g.zh,oy).The most frequent confusion for most phonemes occurswith thesilstate, which serves as the general “non-pho-neme” state. This confusion is not displayed here.Consonants are often confused with similar consonants(e.g.m/n) or with softer consonants that can be extendedover a longer duration (e.g.s,f). This may be relatedto singing technique. However, they are also frequentlyconfused with some vowels, particularlyuhandiy. Thismight be caused by slight timing inaccuracies in the trai-ning annotations which become exaggerated by the time-stretching, or even by some merging of neighboring pho-nemes by the speaker.Longer vowels are almost exclusively confused with otherlong vowels. This poses a contrast to speech, where theyare usually confused with similar short vowels (e.g.aa!ah).This becomes more conclusive when considering the con-fusions of short vowels. In the singing data, short vowelsare very often confused with similar long vowels (e.g.eh!ae). When stretching out such short vowels in singing,singers will automatically change to such a longer vowel.Additionally, some vowels are confused with more “open”vowels (e.g.ey!ae). This is also caused by singingtechnique. These two very interesting effects could be ex-ploited to improve phoneme recognition on singing in thefuture.6. CONCLUSIONIn this paper, we evaluated phoneme models trained on va-rious artiﬁcially “songiﬁed” variants of the TIMIT speechdataset. The reason for this is the lack of phoneme-annotated singing datasets. We generated ﬁve such vari-ants by randomly time-stretching vowels, randomly pitch-shifting words, and by adding vibrato to long vowels.MFCC features were extracted from these datasets andthen used to train two models each: A Neural Network anda Deep Belief Network. We then used these models to re-cognize phonemes in singing data and in unrelated speechdata. No additional mechanics were used to improve theresults, such as language modeling or gender or speakeradaptation.In general, the results are not as good as the state of the artfor the speech data. For the singing data, it is very hardto compare the results to the state of the art because otherpublications use different datasets, phoneme sets, and eva-luation measures. However, this was not necessarily ourgoal - we were mainly interested in the comparative per-formance of the various models.As expected, recognizing phonemes in speech seems to bemuch easier than in singing. Deep Belief Models perfor-med better than their Neural Network counterparts in alltest cases. For speech, the models trained on the unmodi-ﬁed TIMIT dataset always performed best. The best resultis38%correctly classiﬁed frames, a phoneme error rate of0.41, and a weighted phoneme error rate of0.32.For singing, the models trained on the modiﬁed TIMIT da-340 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Ph.Prec.Rec.Prec.Rec.Conf.NNMMMaa0.180.080.350.09ao, ay, owiy0.330.270.430.25ey, uh, aech0.00.020.010.03s, sh, iyzh0.260.020.00.0iy, y, sheh0.060.110.130.15ae, ey, aaah0.00.350.030.23aa, er, aeao0.390.170.250.14aa, ow, lih0.010.080.050.11ey, iy, aeey0.360.170.260.2iy, ae, ayaw0.10.070.080.09aa, ae, ayay0.270.440.230.44aa, ae, iyae0.380.160.40.16aa, ay, awer0.220.10.190.08aa, ae, ehng0.070.160.060.11n, uh, iysh0.580.050.510.09s, jh, zth0.00.00.00.0dh, er, eyoy0.00.00.00.0ao, ay, aadh0.040.140.040.08er ,iy ,uhow0.070.230.160.25ae, ae, aahh0.140.090.090.15iy, uh, shjh0.070.080.120.07y, z, shb0.130.190.090.24ey, m, iyd0.020.190.020.1iy, n, erg0.040.360.030.32y, ow, nf0.020.130.020.5s, er, iyk0.020.370.050.31iy, y ,uhm0.260.240.130.22uh, n, erl0.10.140.110.15ao, er, own0.180.280.20.25uh, er, uwuh0.230.020.150.02er, ih, ehp0.010.150.010.1er, iy, ls0.410.410.440.46z, iy, err0.160.240.170.18er, aa, aot0.00.290.00.42s, sh, iyw0.240.260.190.2ao, l, uwv0.00.020.00.25er, aa, my0.310.080.160.12iy , y, uhz0.280.180.280.17s, iy, nuw0.130.350.120.33uh, er, iyTable 2: Results per phoneme (singing data): Precisionand recall with theNandMmodels, and most frequentconfusions withMmodel (exceptsil)taset produced better results. The best result is for singingis18%correctly classiﬁed frames, a phoneme error rateof0.91, and a weighted phoneme error rate of0.71. Theimprovement over the models trained on the unmodiﬁedTIMIT data is6%for the correctly classiﬁed frames,0.09for the phoneme error rate, and0.06for the weighted pho-neme error rate.The models trained on data that was only pitch-shifted onlyshowed a very slight difference when compared to the ori-ginal data. MFCCs are supposed to be pitch-invariant, andpitch-shifting therefore does not seem to make a big dif-ference. This modiﬁcation might be useful when usingother features, though. A bigger improvement on sin-ging data was achieved when training the models on time-stretched speech data. In fact, this dataset generated thehighest percentage of correctly classiﬁed frames. In thistime-stretched dataset, we also applied vibrato to the stret-ched vowels, which happens naturally in singing. Howe-ver, since the effect of pitch-shifting seemed to be small,we assume that vibrato did not have a big effect either.There were also two datasets where both modiﬁcations(time-stretching and pitch-shifting) were mixed. Both pro-duced the best phoneme error rates in singing.The results were also analyzed on a phoneme-wise basis. Itturned out that vowels were recognized more exactly withthe modiﬁed models, while consonants were recognizedsomewhat worse. This may be caused by the emphasis ofthe generated data on longer vowels.The most interesting effect seen in the confusion matricesis the confusion of short vowels with similar longer vowels.This has a foundation in singing technique and would beinteresting to further explore to improve phoneme recogni-tion in singing.In general, we showed that phoneme recognition in sin-ging can be improved when training models on artiﬁcialsinging data. This ﬁnding can now be used to improveother approaches. For example, it can be combined withthe techniques described in [15].7. FUTURE WORKAs described in section 5.3, many phoneme confusionsmay arise from inexact or unnatural time stretching on thespeech recordings. A more natural approach to this is re-quired and we need to make sure that stretched vowels donot “leak” into neighboring consonants. We also noticedthat short vowels in singing often shift towards their longversions. We will exploit this interesting effect in futurephoneme recognition approaches, e.g. by allowing theseconfusions or composing vowels of several states.In this paper, we tried to apply three characteristics of sin-ging to speech recordings, but there are more, such as diffe-rent pronunciations and different forming of sounds. Suchother characteristics could also be tested in a similar way.Conversely, we could also attempt to make our features andmodels more robust to these variations. In the past, this hasoften been done by adapting models trained on speech tosinging in some way (also see section 2). Adaptations togender or voice also proved helpful.We kept the approach fairly simple for now, but the resultscould be improved by employing language modeling in therecognition process. We will implement this in future ver-sions.A possible alternative would be creating a dataset from po-lyphonic music data by using the lyrics and force-aligningthem.Finally, it will be interesting to see how the results of thisphoneme recognition approach can be applied to practicaltasks, such as lyrics-to-music alignment, keyword spotting,and language identiﬁcation. For these purposes, the algo-rithm must also be tested on accompanied singing data.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 3418. REFERENCES[1]C. Arft. AutoTune Toy, 2010. Web resource, Lastchecked: 4/29/15.[2]M. Dolson. The phase vocoder: A tutorial.ComputerMusic Journal, 10(4):14–27, 1986.[3]D. P. W. Ellis. A phase vocoder in Matlab, 2002. Webresource, Last checked: 04/29/15.[4]H. Fujihara, M. Goto, and H. G. Okuno. A novel fra-mework for recognizing phonemes of singing voice inpolyphonic music. InWASPAA, pages 17–20. IEEE,2009.[5]M. Gruhne, K. Schmidt, and C. Dittmar. Phoneme re-cognition on popular music. InProceedings of the 8thInternational Conference on Music Information Retrie-val (ISMIR), Vienna, Austria, September 2007.[6]J. K. Hansen. Recognition of phonemes in a-cappellarecordings using temporal patterns and mel frequencycepstral coefﬁcients. In9th Sound and Music Compu-ting Conference (SMC), pages 494–499, Copenhagen,Denmark, 2012.[7]H.-G. Hirsch and D. Pearce. The Aurora experimentalframework for the performance evaluation of speechrecognition systems under noisy conditions. InISCAITRW ASR, pages 29–32, 2000.[8]M. J. Hunt. Figures of merit for assessing connected-word recognisers.Speech Communication, 9(4):329–336, 1990.[9]R. M. Golden J. L. Flanagan. Phase vocoder.Bell Sys-tem Technical Journal, pages 1493–1509, November1966.[10]J. S. Garofolo et al. TIMIT Acoustic-Phonetic Con-tinuous Speech Corpus. Technical report, LinguisticData Consortium, Philadelphia, 1993.[11]C. Jankowski, A. Kalyanswamy, S. Basson, andJ. Spitz. NTIMIT: A phonetically balanced, continuousspeech telephone bandwidth speech database.ICASSP,pages 109–112, 1990.[12]A. M. Kruspe. Keyword spotting in a-capella singing.In15th International Conference on Music InformationRetrieval (ISMIR), Taipei, Taiwan, 2014.[13]A. Loscos, P. Cano, and J. Bonada. Low-delay singingvoice alignment to text. InProceedings of the ICMC,1999.[14]A. Mesaros and T. Virtanen. Automatic recognition oflyrics in singing.EURASIP J. Audio, Speech and MusicProcessing, 2010, 2010.[15]A. Mesaros and T. Virtanen. Recognition of phonemesand words in singing. InICASSP, pages 2146–2149.IEEE, 2010.[16]A.-R. Mohamed, G. E. Dahl, and G. Hinton. Acousticmodeling using deep belief networks.IEEE Trans. Au-dio, Speech, Lang. Process, pages 14–22, 2012.[17]G. Szepannek, M. Gruhne, B. Bischl, S. Krey, T. Har-czos, F. Klefenz, C. Dittmar, , and C. Weihs.Classiﬁ-cation as a tool for research, chapter Perceptually Ba-sed Phoneme Recognition in Popular Music. Springer,Heidelberg, 2010.342 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Towards Support for Understanding Classical Music: Alignment of Content Descriptions on the Web.",
        "author": [
            "Taku Kuribayashi",
            "Yasuhito Asano",
            "Masatoshi Yoshikawa"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416138",
        "url": "https://doi.org/10.5281/zenodo.1416138",
        "ee": "https://zenodo.org/records/1416138/files/KuribayashiAY15.pdf",
        "abstract": "Supporting the understanding of classical music is an im- portant topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Re- cently, Kuribayashi et al. proposed a method for obtain- ing content descriptions from the web. However, the con- tent descriptions on a single page frequently explain a spe- cific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descrip- tions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create ap- plications systems to integrate descriptions with musical scores and performances.",
        "zenodo_id": 1416138,
        "dblp_key": "conf/ismir/KuribayashiAY15",
        "keywords": [
            "supporting",
            "classical",
            "music",
            "understanding",
            "research",
            "fields",
            "text",
            "analysis",
            "acoustics",
            "analysis"
        ],
        "content": "TOWARDS SUPPORT FOR UNDERSTANDING CLASSICAL MUSIC:ALIGNMENT OF CONTENT DESCRIPTIONS ON THE WEBTaku Kuribayashi*Yasuhito Asano Masatoshi YoshikawaGraduate School of Informatics, Kyoto University, Japanchoco.ms@gmail.com,{asano, yoshikawa}@i.kyoto-u.ac.jpABSTRACTSupporting the understanding of classical music is an im-portant topic that involves various research ﬁelds such astext analysis and acoustics analysis. Content descriptionsare explanations of classical music compositions that helpa person to understand technical aspects of the music. Re-cently, Kuribayashi et al. proposed a method for obtain-ing content descriptions from the web. However, the con-tent descriptions on a single page frequently explain a spe-ciﬁc part of a composition only. Therefore, a person whowants to fully understand the composition suffers from atime-consuming task, which seems almost impossible for anovice of classical music. To integrate the content descrip-tions obtained from multiple pages, we propose a methodfor aligning each pair of paragraphs of such descriptions.Using dynamic time warping-based method along with ournew ideas, (a) a distribution-based distance measure namedw2DD, and (b) the concept ofpassage expressions, it ispossible to align content descriptions of classical musicbetter than when using cutting-edge text analysis methods.Our method can be extended in future studies to create ap-plications systems to integrate descriptions with musicalscores and performances.1. INTRODUCTIONWhen listening to classical music, we can enhance our un-derstanding of the music by reading descriptions of thecontents of the music simultaneously, which is even truerfor those who are not experts of the ﬁeld of music, suchas amateur players in a college orchestra. Those peoplewould want to read content descriptions written by expertswhen they play or listen to a composition.A content description of classical music is deﬁned as anobjective description related to the structure of the compo-sition that explains speciﬁc parts of it, often using technicalterms and the names of instruments [19]. Reading thosepassages along with the music can help people to under-stand what the part we are listening to means technically,c\u0000Taku Kuribayashi, Yasuhito Asano, MasatoshiYoshikawa. Licensed under a Creative Commons Attribution 4.0 Interna-tional License (CC BY 4.0).Attribution:Taku Kuribayashi, YasuhitoAsano, Masatoshi Yoshikawa. “Towards Support for UnderstandingClassical Music: Alignment of Content Descriptions on the Web”, 16thInternational Society for Music Information Retrieval Conference, 2015.⇤Current Afﬁliation: Accenture Japan Ltd.which is difﬁcult to understand without preliminary knowl-edge. An example of a content description of Beethoven’sSymphony No. 9,1is the following. “The opening theme,played pianissimo over string tremolos, so much resem-bles the sound of an orchestra tuning, many commentatorshave suggested that it was Beethoven’s inspiration.” Thisexample of a content description explains what instruments(strings) are doing technically (pianissimo, tremolos) in aspeciﬁc part (the opening theme).Books and the web are two major sources of content de-scriptions of classical music. Any person having such aninterest can ﬁnd important musical knowledge by readingbooks such as the well-knownA History of Western Mu-sic[14], which includes not only historical knowledge ofthe development of western music but also abundant refer-ences to other important books. Some encyclopedias con-tain descriptions of orchestral compositions.Nevertheless, books have several important limitations.One is that they can hold only a few descriptions. An-other problem is that once books are published, they can-not be updated easily or consistently. Although classicalmusic compositions are not increasing to any great degree,the performances are increasing constantly. With the riseof the internet and international communication, there aremore descriptions of music and performances. Differentperspectives and ways of analysis continue to appear. Withthe form of printed publications, it is difﬁcult to update theincreasing amount of information continuously.The web is an alternative source of information, offer-ing resources such as “DW3 Classical Music Resources”[11] or Wikipedia. However, it is often difﬁcult to ﬁndsufﬁcient information to understand some compositions.Conventional search engines are unsuitable for the verticalsearch for content descriptions because their results ofteninclude commercial websites that do not describe the con-tents of the compositions. Kuribayashi et al. [19] proposeda method that we can use to collect descriptions from theweb. Content descriptions gathered from a number of webpages using their method can be classiﬁed into two cate-gories: ones that describe the overall contents of the music,and ones that describe speciﬁc parts of the composition.We call the latter onespartial content descriptions. Bothare essential for technical understanding of the music, al-though it is often difﬁcult to understand where in the com-position partial content descriptions explain. Furthermore,1http://en.wikipedia.org/wiki/SymphonyNo.9(Beethoven),viewed on Jan. 4, 2013371a single page seldom includes partial content descriptionsexplains every important part of the composition; a pagemight describe the introduction in detail, while anotherpage might explain the ﬁnal part mainly. Therefore, it canbe helpful to integrate pieces of information in partial con-tent descriptions from different sources, that is, to checkhow they complement each other.We propose a method for the alignment of every pairof paragraphs which are partial content descriptions in dif-ferent web pages. As a dataset, we manually extract para-graphs corresponding to partial content descriptions fromthe content descriptions collected from multiple web pagesby the method of Kuribayashi et al. [19]. Each alignmentclariﬁes which sentence in a paragraph matches a sentencein the other paragraph. We can understand the music moreeasily and efﬁciently by seeing the alignments which in-tegrate the pieces of information in them than merely byreading a single web page. Actually, showing the align-ment is beneﬁcial in many situations, as for (1) beginners,(2) experts who want to support those beginners, and (3)future applications.(1) For beginners, the alignment can help them integratepieces of information from different websites. If beginnershave difﬁculties understanding one website or feel the needfor more information related to a speciﬁc part of the com-position, they can look at the information that correspondsto the speciﬁed part of the description.(2) For those with a specialized knowledge of classicalmusic, there is always a demand that they want to supportbeginners as they come to understand music. Web servicessuch as YouTube have several videos that are designed tohelp beginners to understand classical music. However,preparing all the materials necessary for the explanationis a task that is both difﬁcult and time-consuming. Show-ing the alignment of sentences provides materials that cansupport greater understanding. Therefore, showing such analignment is an important aid to experts who try to supportbeginners.(3) In the future, we seek to develop a system that in-tegrates our methods and studies of the analysis of musicand music scores; the most important feature is to aligncontent descriptions with the music itself. Beginners willespecially beneﬁt from this system because the hardest taskfor beginners is to ascertain which part of the music thepartial content descriptions are referring to. The ﬁrst stepof this ultimate application is analysis of the sentences andtheir mutual alignment.The main contributions of this paper are as follows.•We proposed a novel method named w2DD+PE foraligning partial content descriptions based on dy-namic time warping using the following two ideas:(a) the distribution divergence of semantic vectors ofwords, and (b)passage expressions.•We presented a way to show the aggregated resultsof our methods for collecting and aligning partialcontent descriptions.2. RELATED WORKThis paper deals with various ﬁelds of study, includinganalysis of music, temporal information, multi-documentsummarization, and parallel corpus discovery; the sub-ject of this paper is the analysis of temporal informa-tion in music, and the methodology utilizes the ideas ofmulti-document summarization and parallel corpus dis-cover models. We will take a look at some of the previousworks related to each ﬁeld of study.2.1 Musical KnowledgeMusic has remained an important topic of research fromvarious aspects, including acoustics, music theory, andpsychology. We list a few related works that are closelyrelated to understanding the support and analysis of music.In the area of understanding support and collecting mu-sical knowledge, Fineman [11] reported a project called“DW3 Classical Music Resources.” The project was a col-lection of web links that gathered various forms of knowl-edge related to classical music for college students major-ing in music. The link quality was scrutinized by experts,making it easier for students to obtain information that can-not be found easily via conventional web searches. Unfor-tunately, the project was ceased in 2007.Other works that are related to the future applicationof this research include the following. Some studies havebeen made to analyze the structure of the music itself, suchas research by Sumi et al. [36], which created a system forinference of the chord from other data such as the basepitch. Maezawa et al. [24] proposed a system that linksthe performance and the interpretation of the composition.Using these studies with our research, it would be possiblein the future to analyze and extract the music structure andlink it to the content descriptions of the composition.2.2 Temporal InformationAs Alonso et al. state in [2], temporal information is animportant factor in information retrieval in general. Re-searches that deal with temporal information in natural lan-guage are being studied widely, as in [34], [27], [23], [3],[16]. In order to extract temporal expression, Schilder et al.[32] use ﬁnite state transducer (FST); Str¨otgen et al. [33]use regular expressions, and Mani et al. [26] use machinelearning. Chambers et al. [7] focus on the relationshipsbetween events, whereas Lapata et al. [20] concentrate onthe relationships between expressions in a single sentence.Kimura et al. [17] propose a system that shows chronolog-ically organized information obtained by web searching ona single person. Schilder et al. [32] extract temporal infor-mation from news articles.As we see from these examples, researches on tempo-ral information have various aspects, including many view-points on the subject and granularity. In our research, wedeal with temporal information in one composition, whichis generally an hour or two at the longest.372 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20152.3 Multi-document SummarizationTo gain knowledge from multiple sources, summarizationof information is an important technique. One type of sum-marization, the extractive method, chooses subsets of theoriginal document to convey the meaning of the whole text.In the task of multiple document summarization, numer-ous approaches have been taken. Mani et al. [25] take angraph-based approach, and many of recent studies followsimilar ideas [39] [8] [5] [13] [10]. Other approaches in-clude Bayesian models [9] [6], topic models [15], rhetoric-based models [4], and cluster-based models [30] [37].2.4 Parallel Corpus DiscoveryBecause we are interested in discovering potential align-ments from different documents, we will take a look atprevious works that utilize techniques for investigating par-allel corpora. Caroline et al. [21] apply dynamic timewarping to movie subtitles to construct parallel corporafor machine translation. Previous works on ﬁnding par-allel texts from bilingual, often non-parallel, corpora in-clude [12], [29], [38] and [35].3. ALIGNMENT OF DESCRIPTIONS3.1 Collection of Content DescriptionsWe adopted a method of Kuribayashi et al. [19] to col-lect content descriptions from the web. Their method uti-lizes labeled latent Dirichlet allocation (labeled LDA) [31]which is a supervised learning to classify documents prob-abilistically. They proposed eight classes of descriptions(one of them corresponds to content descriptions) con-tained in the pages obtained by inputting names of compo-sitions to a search engine, and trained labeled LDA withmanually-classiﬁed 1540 pages. Note that informationother than text, such as images or HTML tags, is removedfrom these pages using nwc-toolkit2. Applying the trainedlabeled LDA to paragraphs obtained by inputting the nameof a composition to a search engine, we can collect para-graphs corresponding to content descriptions.From our investigation of a number of content descrip-tions gathered by this method, we found out that par-tial content descriptions in a single paragraph are orderedchronologically for a composition. Therefore, the se-quence alignment of those sentences is more suitable forintegrating information of partial content descriptions thanother methods such as matching sentences.3.2 Bootstrapping Method for Acquiring PassageExpressionsIt is quite a difﬁcult task to identify what part of a musi-cal piece that a description corresponds, because most par-tial content descriptions do not contain measure numbers.Here we try to obtain as much information related to thecorrespondence of one expression to another. For instance,if we have two descriptions “The ﬁrst theme is played by2http://nwc-toolkit.googlecode.com/svn/trunk/docs/tools/text-extractor.htmlsolo ﬂute” and “The lyrical ﬁrst subject appears after theintroduction,” we can see the relationship between them;because the words “theme” and “subject” are semanticallysimilar in this context, we can align these two sentencesand understand that the theme is lyrical and is played bythe ﬂute. If we have another sentence talking about “soloﬂute,” we can also infer the relationship of that descrip-tion to the two sentences above. We have to identify whattypes of nouns point to the parts of music, which we callpassage expressions, in order to perform this inference. Ifwe are able to obtain those expressions, then we would beable to use them to align sentences that correspond to thesame part of the composition. In the future, we might alsobe able to employ them to mapping of the actual parts ofmusic by ﬁnding measure numbers or giving some infor-mation manually.To obtain the passage expressions, we focus on thegrammatical structure of content description sentences. Incontent descriptions, the most basic structure of sentencesis subject-verb-object, where the verb describes the rela-tionship between two passage expressions (subject and ob-ject). Therefore, we use a bootstrapping method as in [1],using the relation between the subject and the predicate toextract appropriate nouns. Because a simple bootstrappingmethod tends to produce noises in the results, we also pro-posed ﬁltering methods to reduce those noises.The corpus for the bootstrapping method is 2300 para-graphs which are the top 100 paragraphs obtained by ap-plying the method of Kuribayashi et al. [19] to each of 23compositions.First, we prepared an initial list of 14 nouns and 29verbs for the bootstrapping method. Then we expandedthat list when two of the triplet of the subject, the verb andthe object (or the object of the preposition) were already inthe list, by adding the third word. We did not add the thirdword when the subject was a personal pronoun (“I”, “we”,“you”, “he”, or “she”) because the word was inappropriatein almost all cases.Instead of adding all the words that appear in the triplet,we eliminate words that do not fulﬁll certain conditions toreduce noise words that are not relevant to content descrip-tions. The following ﬁltering methods incorporate the re-sults of the labeled LDA-based method [19] and word2vec[28]3which converts a word to a vector based on the co-occurence of words in a corpus; the similarity of words canbe calculated using the vectors corresponding the words.L-LDAWords that are stop words or that do not appearin the training data of labeled LDA in the methodsof [19] are not added.word2vecWords that are below the threshold (0.128 and0.3) of word2vec similarity. Word2vec using thesame corpus as the one used for our bootstrappingexplained above. The word2vec similarity used hereis deﬁned as the maximum of the similarities be-tween the word and the seed nouns of the bootstrap-ping method.3https://code.google.com/p/word2vec/Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 373L-LDA && word2vecOnly words that fulﬁll both of theabove two are added. The threshold of the word2vecscore is 0.128.L-LDAkword2vecWords that fulﬁll either one of thetwo above are added. The threshold of the word2vecscore is 0.3.3.3 Alignment Method using Dynamic Time WarpingWe propose a method called “word sets to Distribu-tion Distance-based alignment using Passage Expressions”(w2DD +PE) for ﬁnding an alignment of pairs of para-graphs of content descriptions. This method is based ondynamic time warping (DTW), a well-known technique forﬁnding an alignment of two sequences. Applying DTW toparagraphs requires a distance measure of two sentences.We propose a new distance measure employing (a) the dis-tribution of word vectors of each sentence, and (b) passageexpressions.3.3.1 w2DDA simple measure of distance between two sentences is totake the average of semantic vectors of words in the sen-tence calculated using word2vec and calculate the cosinedistance. However, adopting the average loses much infor-mation about how the vectors distribute. Therefore, it isrequired to propose a new method to capture the feature ofthe distribution corresponding to each sentence.The fundamental idea of our new method, namedw2DD, is to measure the distance between two sentencesby the distance between the distributions of their corre-sponding vectors. We ﬁrstly reduce each vector to a smallnumber of dimensions using principal component analysisbecause the 200 dimensions obtained by word2vec are toonumerous to handle. The number of dimensions is deter-mined empirically, and eleven dimensions were sufﬁcientfor the cumulative proportion of 70%. Secondly, we con-vert the 11-dimension vectors of each sentence into a his-togram, in order to apply a distance measure for a pair ofprobabilistic distributions. For the conversion, we divideeach dimension into halves (resulting in211subspaces)and count the number of vectors in each subspace; the se-quence of the numbers forms the obtained histogram. Wetried splitting each dimension into 2, 3, and 4, but the resultdid not change at all, so we chose 2. Then we calculated thedistance of the pair of histograms by the Jensen–Shannondivergence using the following formula:JSD(PkQ)=12(XxlogP(x)R(x)+XxlogQ(x)R(x))(1)wherePandQare the histograms corresponding to thetwo sets of vectors,xis each subspace,P(x)is the numberof vectors ofPinxdivided by the total number of vectorsofP, andR(x)=P(x)+Q(x)2.3.3.2 Passage ExpressionsFirst, to utilize the information of passage expressions insentences containing no such expressions, we merge suchsentences into the previous sentence having a passage ex-pression. Then, we calculate the distance of two sentencess1ands2as follows. Letsim(p1,p2)be the cosine simi-larity between the semantic vectors of passage expressionsp1ins1andp2ins2. The distanceDist(s1,s2)isDist(s1,s2)=8>><>>:↵JSD(s1,s2)+(1\u0000↵)(1\u0000maxp1,p2(sim(p1,p2)))(if maxp1,p2(sim(p1,p2))6= 0)JSD(s1,s2) (otherwise)(2)whereJSD(s1,s2)is the value calculated as in Section3.3.1. If either one of the paragraphs is without a passageexpression, thenmaxp1,p2(sim(p1,p2)) = 0. Thereforeonly the Jensen–Shannon divergence matters. Also,↵isthe coefﬁcient factor, which was set to 0.2, 0.4, 0.6, 0.8,and 1.0.4. EVALUATION4.1 ProcedureAn input of the alignment is a pair of paragraphs whichare partial content descriptions explaining a common sec-tion in a composition. The labeled LDA-based method ofKuribayashi et al. [19] is able to collect content descrip-tions, although it is not able to extract partial content de-scriptions from them. Consequently, our data set consistsof 32 paragraphs (135 sentences) manually extracted fromthe top 100 paragraphs for each of 10 classical music com-positions obtained by their method; the number of pairs are41. The extraction and assignment of each paragraph to asection is based on keywords corresponding to sections,such as “movement” (the most basic divisions of a musiccomposition), “exposition” and “development” (commonstructures within a movement). The keywords are selectedfor the sonata form, and a selection specialized for othertypes of classic music, “theme and variations” for exam-ple, is also possible. A method for automatic extractionand assignment is a candidate of future studies.To see how each of our ideas work, we used the fol-lowing variants of methods for calculating the distance be-tween two sentences.Baseline1the cosine distance of the averages of word2vecvectors.Baseline1+PEthe cosine distance of the following 400dimension vectors for the two sentencess1ands2.The ﬁrst 200 dimensions are the average of theword2vec vector of each sentence. The second 200dimensions are the word2vec vector of passage ex-pressionp1fors1orp2fors2, respectively;p1andp2are the pair of the closest expressions in terms ofword2vec cosine similarity.Baseline2the cosine distance calculated usingsentence2vec.4This is an implementation ofPara-graph Vectorproposed by Le and Mikolov [22],4https://github.com/klb3713/sentence2vec374 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1. Example of how to calculateF-measure.which is an advanced method of word2vec that in-corporates the order of words in a sentence to rep-resent its semantics. Their experiments showedthatParagraph Vectorperforms better than previousmethods for several tasks; word vector averaging,Naive Bayes, SVMs, and recursive neural networkfor a sentiment analysis task; vector averaging, bag-of-words, and bag-of-bigrams for an information re-trieval task.Baseline2+PEthe cosine distance calculated using sen-tence2vec incorporated with the passage expressionvector by the same way Section 3.3.2.w2DDthe method described in Section 3.3.1w2DD+PEthe method described in Section 3.3.2.For Baseline1+PE, Baseline2+PE, and w2DD+PE, weused the ﬁltered list of passage expressions described inSection 3.1.The ground truth for the alignment of each pair of para-graphs was created manually by one of the authors whois an enthusiast of classical music, with the help of vari-ous books and websites on the compositions. We evaluateeach method using precision, recall, andF-measure. Weexplain below how they are calculated employing Figure 1which illustrates an example of the result of alignment oftwo paragraphs. The red dots represent the manual align-ment result, and black dots indicate the output a method; inthe manual alignment, the ﬁrst sentence of paragraph 1 (x-axis) corresponds to the ﬁrst, second, and third sentencesof paragraph 2 (y-axis), the second sentence of paragraph1 corresponds to the third sentence of paragraph2, and soon. The precision is the number of matching red and blackdots over the number of black dots, 9/13 in this case. Therecall is the number of matching red and black dots overthe number of red dots, 9/15 in this case. TheF-measureis deﬁned by the following equation.F=2·precision·recallprecision+recall(3)4.2 ResultsTables 1, 2, and 3 present the experimental results. Com-paring the “No PE” row with the others in each table, wesee that employing PE improves the results in general.Comparing the three tables, we see that w2DD performsmuch better than baseline methods. Especially, w2DD+PEwith L-LDA (↵=0.2) and w2DD+PE with L-LDA &&word2vec (↵=0.2) are the methods that resulted in thebestF-measure (shown in bold in the Table 3).Table 1. Results of Baseline1 (No PE) and Baseline1+PE.MethodPrecision RecallF-measureNo PE0.595 0.534 0.563No Filtering0.608 0.633 0.620L-LDA0.618 0.647 0.632word2vec (0.128)0.582 0.618 0.600word2vec (0.3)0.562 0.607 0.584L-LDA && word2vec0.592 0.629 0.610L-LDAkword2vec0.591 0.615 0.602\nFigure 2. Visualization of Tchaikovsky’s Symphony No.5(Each block of sentences separated by dotted lines is froma single web page.)Because the baseline methods “compress” the wordvectors in a sentence into a single vector, they are con-sidered to lose much information of the words. On theother hand, w2DD keeps the information on how variedthe words are in the sentence.The numbers of passage expressions employed in L-LDA and L-LDA && word2vec were 30 and 26, respec-tively. Passge expressions were generally effective as men-tioned above, while higher alpha often made the perfor-mance worse. These results would indicate that the worddistribution employed in w2DD is more important thanpassage expressions.4.3 VisualizationTo present the results of our methods to users for under-standing support, we created a prototype of a system tovisualize them in a table form, whose examples can be ac-cessed online.5Each row corresponds to a part of mu-sic. Figure 2 shows a single row corresponding to the“4th movement” of the table for Tchaikovsky’s “Sym-phony No.5.” In this row, there are three blocks of sen-tences separated by dotted lines, each of which indicates aparagraph retrieved from one web page. As we hover thecursor over one of the sentences, the sentences of other de-scriptions that are aligned with that sentence by our methodis highlighted (shown as pink in the ﬁgure).The recapitulation part builds up the tension and ends5http://bit.ly/1vHMkgmProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 375Table 2. Results of Baseline2 (No PE) and Baseline2+PE.Method↵Precision RecallF-measureNo PE-0.610 0.550 0.578No Filtering0.20.621 0.562 0.5900.40.630 0.568 0.5970.60.603 0.544 0.5720.80.474 0.417 0.444L-LDA0.20.659 0.598 0.6270.40.670 0.604 0.6350.60.627 0.565 0.5940.80.474 0.417 0.444word2vec(0.128)0.20.619 0.562 0.5890.40.630 0.568 0.5970.60.603 0.544 0.5720.80.474 0.417 0.444word2vec(0.3)0.20.625 0.565 0.5930.40.627 0.565 0.5940.60.603 0.544 0.5720.80.474 0.417 0.444L-LDA &&word2vec0.20.659 0.598 0.6270.40.670 0.604 0.6350.60.627 0.565 0.5940.80.474 0.417 0.444L-LDAkword2vec0.20.616 0.559 0.5860.40.627 0.565 0.5940.60.603 0.544 0.5720.80.474 0.417 0.444up with a brief stop. From the three highlighted descrip-tions, it is readily apparent that common tone modulationis used cleverly in the recapitulation; the fate theme en-genders a suspenseful buildup; and a fermata rest followsthe majestic chords in B major. By reading the aligned de-scriptions that are retrieved from multiple pages, a moredetailed and thorough view of the part of the music can beobtained than by reading just one description.5. CONCLUDING REMARKSAs described in this paper, we proposed methods for sup-porting the understanding of classical music using mutualalignment of partial content descriptions. Our methodw2DD+PE uses word sets to Distribution Distance(w2DD) and the concept ofpassage expressions, which areexpressions that serve as the key to identiﬁcation of whichparts of the music the descriptions correspond to. Althoughthe concept of passage expressions is unique to the ﬁeld ofclassical music, w2DD can be applied to other domains oftext data. It is one of our future tasks to apply w2DD toother datasets.Future studies will be undertaken to create an applica-tion system that can help beginners to appreciate classicalmusic. By integrating our methods with studies of musi-cal analysis such as [36] and [24], or other applications ofmusic-related information retrieval such as [18], it is ex-pected to be possible to support beginners in their effortsto understand and enjoy music.Table 3. Results of w2DD (No PE) and w2DD+PE.Method↵Precision RecallF-measureNo PE-0.746 0.688 0.716No Filtering0.20.671 0.733 0.7010.40.671 0.733 0.7010.60.690 0.748 0.7180.80.667 0.718 0.691L-LDA0.20.680 0.7800.7270.40.678 0.774 0.7230.60.675 0.760 0.7150.80.669 0.745 0.705word2vec(0.128)0.20.639 0.721 0.6780.40.639 0.721 0.6780.60.651 0.718 0.6830.80.658 0.718 0.687word2vec(0.3)0.20.648 0.736 0.6890.40.648 0.736 0.6890.60.652 0.739 0.6930.80.659 0.745 0.699L-LDA &&word2vec0.20.680 0.7800.7270.40.678 0.774 0.7230.60.673 0.757 0.7120.80.669 0.745 0.705L-LDAkword2vec0.20.637 0.733 0.6810.40.637 0.733 0.6810.60.641 0.727 0.6810.80.651 0.736 0.6916. ACKNOWLEDGMENTSThis work was supported by JSPS KAKENHI Grant Num-ber 15K00423 and the Kayamori Foundation of Informa-tional Science Advancement.7. REFERENCES[1]E. Agichtein and L. Gravano. Snowball: Extracting re-lations from large plain-text collections. InProc. of the5th ACM conference on Digital Libraries, pages 85–94, 2000.[2]O. Alonso, M. Gertz, and R. Baeza-Yates. On the valueof temporal information in information retrieval. InACM SIGIR Forum, volume 41-2, pages 35–41, 2007.[3]O. Alonso, M. Gertz, and R. Baeza-Yates. Clusteringand exploring search results using timeline construc-tions. InProc. of the 18th CIKM, pages 97–106, 2009.[4]J. Atkinson and R. Munoz. Rhetorics-based multi-document summarization.Expert Systems with Appli-cations, 40(11):4346–4352, 2013.[5]E. Canhasi and I. Kononenko. Weighted archetypalanalysis of the multi-element graph for query-focusedmulti-document summarization.Expert Systems withApplications, 41(2):535–543, 2014.[6]A. Celikyilmaz and D. Hakkani-T¨ur. Discovery of top-ically coherent sentences for extractive summarization.InProc. of HLT ’11, pages 491–499, 2011.[7]N. Chambers, S. Wang, and D. Jurafsky. Classifyingtemporal relations between events. InProc. of the 45th376 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Annual Meeting of the ACL on Interactive Poster andDemonstration Sessions, pages 173–176, 2007.[8]J. Christensen, Mausam, S. Soderland, and O. Etzioni.Towards Coherent Multi-Document Summarization. InProc. of HLT-NAACL ’13, pages 1163–1173, 2013.[9]H. Daum´e III and D. Marcu. Bayesian query-focusedsummarization. InProc. of the 44th Annual Meeting ofthe ACL, pages 305–312, 2006.[10]R. Ferreira, L. de Souza Cabral, F. Freitas, R. D. Lins,G. de Franc ¸a Silva, S. J. Simske, and L. Favaro. Amulti-document summarization system based on statis-tics and linguistic treatment.Expert Systems with Ap-plications, 41(13):5780–5787, 2014.[11]Y . Fineman. DW3 Classical Music Resources: Man-aging Mozart on the Web.Libraries and the Academy,1(4):383–389, 2001.[12]P. Fung and P. Cheung. Mining verynon-parallel cor-pora: Parallel sentence and lexicon extraction via boot-strapping and em. InProc. of EMNLP, pages 57–63,2004.[13]G. Glavaˇs and J.ˇSnajder. Event graphs for informationretrieval and multi-document summarization.ExpertSystems with Applications, 41(15):6904–6916, 2014.[14]D. J. Grout, C. V . Palisca, et al.A History of WesternMusic. Number Ed. 5. WW Norton & Company, Inc.,1996.[15]A. Haghighi and L. Vanderwende. Exploring contentmodels for multi-document summarization. InProc. ofthe NAACL ’09, pages 362–370, 2009.[16]J. Hobbs and J. Pustejovsky. Annotating and reasoningabout time and events. InProc. of AAAI Spring Sympo-sium on Logical Formalizations of Commonsense Rea-soning, volume 3, pages 74–82, 2003.[17]R. Kimura, S. Oyama, H. Toda, and K. Tanaka. Cre-ating personal histories from the Web using namesakedisambiguation and event extraction. InWeb Engineer-ing, pages 400–414. 2007.[18]P. Knees, T. Pohle, M. Schedl, and G. Widmer. A musicsearch engine built upon audio-based and web-basedsimilarity measures. InProc. of the 30th SIGIR, pages447–454, 2007.[19]T. Kuribayashi, Y . Asano, and M. Yoshikawa. Rankingmethod specialized for content descriptions of classicalmusic. InPoster Proc. of the 22nd WWW, pages 141–142, 2013.[20]M. Lapata and A. Lascarides. Learning Sentence-internal Temporal Relations.Journal of Artiﬁcial In-telligence Research (JAIR), 27:85–117, 2006.[21]C. Lavecchia, K Sma¨ıli, and D. Langlois. Building par-allel corpora from movies. InProc. of the 4th NLPCS,2007.[22]Q. V . Le and T. Mikolov. Distributed representationsof sentences and documents.Proc. of the 31st ICML,pages 1188–1196, 2014.[23]X. Ling and D. S. Weld. Temporal Information Extrac-tion. InThe AAAI Conference on Artiﬁcial Intelligence,pages 1385–1390, 2010.[24]A. Maezawa, M. Goto, and H. G. Okuno. Query-By-Conducting: An interface to retrieve classical-musicinterpretations by real-time tempo input. InProc. of the11th ISMIR, pages 477–482, 2010.[25]I. Mani and E. Bloedorn. Multi-document summa-rization by graph search and matching. InProc. ofAAAI’97/IAAI’97, pages 622–628, 1997.[26]I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Puste-jovsky. Machine learning of temporal relations. InProc. of the 44th Annual Meeting of the ACL, pages753–760, 2006.[27]P. Mazur and R. Dale. Wikiwars: A new corpus for re-search on temporal expressions. InProc. of the EMNLP2010, pages 913–922, 2010.[28]T. Mikolov, K. Chen, G. Corrado, and J. Dean. Ef-ﬁcient estimation of word representations in vectorspace.Proc. of Workshop at ICLR, 2013.[29]D. S. Munteanu and D. Marcu. Extracting parallelsub-sentential fragments from non-parallel corpora. InProc. of the 44th Annual Meeting of the ACL, pages81–88, 2006.[30]D. R Radev, H. Jing, M. Sty´s, and D. Tam. Centroid-based summarization of multiple documents.Informa-tion Processing & Management, 40(6):919–938, 2004.[31]D. Ramage, D. Hall, R.Nallapati, and C.D. Manning.Labeled LDA: A supervised topic model for credit at-tribution in multi-labeled corpora. InProc. of EMNLP ,Volume 1, pages 248–256, 2009.[32]F. Schilder and C. Habel. From temporal expressions totemporal information: Semantic tagging of news mes-sages. InProc. of the Workshop on Temporal and Spa-tial Information Processing-Volume 13, page 9, 2001.[33]J. Str¨otgen and M. Gertz. HeidelTime: High qualityrule-based extraction and normalization of temporalexpressions. InProc. of the 5th International Workshopon Semantic Evaluation, pages 321–324, 2010.[34]J. Str¨otgen and M. Gertz. Multilingual and cross-domain temporal tagging.Language Resources andEvaluation, 47(2):269–298, 2013.[35]F. Su and B. Babych. Measuring comparability of doc-uments in non-parallel corpora for efﬁcient extractionof (semi-)parallel translation equivalents. InProc. ofthe Joint Workshop on ESIRMT and HyTra, EACL2012, pages 10–19, 2012.[36]K. Sumi, K. Itoyama, K. Yoshii, K. Komatani,T. Ogata, and H. G Okuno. Automatic chord recogni-tion based on probabilistic integration of chord transi-tion and bass pitch estimation. InProc. of the 9th IS-MIR, pages 39–44, 2008.[37]X. Wan and J. Yang. Multi-document summarizationusing cluster-based link analysis. InProc. of SIGIR ’08,pages 299–306, 2008.[38]D. Wu and P. Fung. Inversion transduction grammarconstraints for mining parallel sentences from quasi-comparable corpora. In R. Dale, K. Wong, J. Su, andO. Kwong, editors,Natural Language ProcessingʕIJCNLP 2005, LNCS 3651, pages 257–268, 2005.[39]L. Zhao, L. Wu, and X. Huang. Using query expan-sion in graph-based approach for query-focused multi-document summarization.Information Processing &Management, 45(1):35–41, 2009.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 377"
    },
    {
        "title": "MIREX Grand Challenge 2014 User Experience: Qualitative Analysis of User Feedback.",
        "author": [
            "Jin Ha Lee 0001",
            "Xiao Hu 0001",
            "Kahyun Choi",
            "J. Stephen Downie"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417103",
        "url": "https://doi.org/10.5281/zenodo.1417103",
        "ee": "https://zenodo.org/records/1417103/files/LeeHCD15.pdf",
        "abstract": "Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evalua- tion eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algo- rithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the quali- tative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opin- ions, not fully captured by score ratings on the given cri- teria, and demonstrated the challenge of evaluating a va- riety of systems with different user goals. We conclude with a discussion on the implications of findings and rec- ommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility.",
        "zenodo_id": 1417103,
        "dblp_key": "conf/ismir/LeeHCD15",
        "keywords": [
            "Evaluation",
            "Music Information Retrieval",
            "MIREX",
            "User Experience",
            "Grand Challenge 2014",
            "System Interaction",
            "Qualitative Analysis",
            "Free Text Responses",
            "User Opinions",
            "Score Ratings"
        ],
        "content": "MIREX GRAND CHALLENGE 2014 USER EXPERIENCE: QUALITATIVE ANALYSIS OF USER FEEDBACK Jin Ha Lee Xiao Hu Kahyun Choi, J. Stephen Downie University of Washington jinhalee@uw.edu University of Hong Kong xiaoxhu@hku.hk University of Illinois {ckahyu2,jdownie}@illinois.edu ABSTRACT Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evalua-tion eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algo-rithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the quali-tative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opin-ions, not fully captured by score ratings on the given cri-teria, and demonstrated the challenge of evaluating a va-riety of systems with different user goals. We conclude with a discussion on the implications of findings and rec-ommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility. 1. INTRODUCTION Since 2005, the Music Information Retrieval (MIR) community has benefited from the Music Information Re-trieval Evaluation eXchange (MIREX), an annual evalua-tion event led by researchers at University of Illinois [7]. MIREX has had a significant contribution to the field as it allows system developers to test and improve their MIR algorithms. However, as the field matures, the current state of the art is increasingly deemed sufficient to sup-port an acceptable degree of efficiency and effectiveness in various conventional MIREX tasks, resulting in the glass ceiling effect [1,2,11]. A number of researchers have also pointed out the limitations of MIREX, includ-ing the dominance of a system-centered approach and the lack of consideration for real users [11,12,14,19].  In response to the feedback received from the MIR community, the MIREX grand challenge was held in 2014. This was substantially different from any of the past evaluation tasks in two respects: 1) the focus of evalua-tion shifted to include the front end of the system (i.e., how users interact with the system), and 2) the submis-sions were complete MIR systems that can employ vari-ous MIR techniques rather than individual algorithms. This marks a shift of the evaluation paradigm, since all the MIREX evaluation tasks have been focused on the back end, with the front end being largely ignored [11,15]. Three different MIR systems participated in the Grand Challenge 2014 User Experience (GC14UX). In this pa-per, we present the findings from analyzing the results of GC14UX, focusing on the free-text user responses. The goal of the paper is twofold: 1) understanding how users reacted to which aspects of the systems in their responses, and 2) using that knowledge to improve the design of fu-ture MIR UX evaluation tasks. In particular, we seek to answer the following research questions: Q1. Which aspects of MIR systems were most im-portant to users, as evidenced by the responses?   Q2. Based on users’ responses, are there any evalua-tion criteria we should consider revising or adding for fu-ture iterations of MIR evaluation of user experience?  2. BACKGROUND 2.1 User-centered Evaluation in MIR As pioneers in user-centered evaluation in MIR, Pauws et al. [17,18,20] conducted a series of user evaluation tasks to examine an interactive playlist generation system. Sev-eral user-centered measures were considered, including time on tasks, number of actions, preference, ease of use, and usefulness. Although the evaluation was confined to one specific MIR system, it is noteworthy that they con-sidered the front-end interface and the user’s interaction in the earlier days of MIR system evaluation. Hoashi et al. [9] also conducted a user evaluation of visualization interfaces for MIR systems based on subjective measures such as perceived accuracy and enjoyability. Despite such efforts, most MIR evaluation research is still based on a system-centered approach without involv-ing users. While this makes sense for some of the micro-level tasks, ultimately many algorithms that are being evaluated will be implemented as features in complete MIR systems. Therefore it is important to consider how users determine the usefulness and value of the systems. Hu and Kando [10] also emphasized the need for user-centered evaluation in MIR based on their finding that only a weak correlation existed between user-centered measures and system-centered measures in their evalua-tion experiment of MIR systems.  Leaving aside the shortage of user-centered evaluation in our field, the evaluation in the few aforementioned studies has been mostly limited to specific algorithms or *             © Jin Ha Lee, Xiao Hu, Kahyun Choi, J. Stephen Downie. Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0). Attribution: Jin Ha Lee, Xiao Hu, Kahyun Choi, J. Stephen Downie. “MIREX Grand Challenge 2014 User Experience: qualitative analysis of user feedback”, 16th International Society for Music Information Retrieval Conference, 2015. \n779   \n Figure 1. Screenshots of Thank You for the Music, Moody, and Tonic.  functions such as playlist generation algorithms and rec-ommender systems [14]. This has been attributed to the lack of complete MIR systems ready for evaluation with-in the MIREX framework [11,15]. Consequently, at-tempts to conduct a holistic user-centered evaluation of MIR systems had to be done with existing commercial music services. For example, Lee and Price [15] exam-ined how Nielsen’s usability heuristics [16] can be ap-plied to evaluate multiple aspects of user experience for services like Pandora, Spotify, etc. As the MIR field is maturing, there is also a growing recognition that we are ready to evaluate complete and full-featured systems in-corporating various sub-components with helpful inter-faces [6,11]. Therefore, GC14UX was held, aiming to in-spire the development of complete MIR systems and a holistic evaluation of user experience with those systems.  2.2 GC14UX Evaluation Framework and Process1 The dataset used in GC14UX was a sample of 10,000 tracks with the CC-BY (Creative Commons Attribution) license from the Jamendo collection2, for the purpose of avoiding any potential copyright issues. All tracks had song and album titles, artist name, and at least two genre tags. To guide the evaluators, a user task was created based on several criteria: 1) a common and realistic MIR task, 2) a task specific enough to help evaluators judge how successful the results are, 3) a task not tied to a par-ticular MIR technique, and 4) a task that can be reasona-bly accomplished with the given dataset. The final task was determined as follows: “You are creating a short vid-eo about a memorable occasion that happened to you re-cently, and you need to find some (copyright-free) songs to use as background music.” An online evaluation platform was set up so that eval-uators could easily access the MIR systems through a web browser. The invitations were circulated through mailing lists within the MIR community. Evaluators were asked to interact with the systems and rate their scores on a seven-point Likert scale for the following criteria: x Overall Satisfaction: How would you rate your overall satisfaction with the system? x Learnability: How easy was it to figure out how to use the system? x Robustness: How good is the system’s ability to warn you when you are about to make a mistake, al-low you to recover, or retrace your steps?                                                            1 For more detailed information on the framework, see [11]. 2 https://www.jamendo.com/en/welcome x Affordance: How well does the system allow you to perform what you want to do? x Feedback: How well does the system communicate what is going on? Evaluators were also given an opportunity to provide their comments in an open text field.  2.3 Participating Systems and Quantitative Ratings There were a total of three systems that participated in GC14UX: Thank You for the Music (hereinafter, Thank You), Moody, and Tonic (Figure 1)3. The design and func-tionality of the three MIR systems varied to some extent. Thank You provides users with access to a music collec-tion through a more traditional music digital library inter-face, offering music search by title, album, genre, and art-ist. Moody is a recommender system in which a music collection can be browsed based on mood and genre. Tonic is a tag-based discovery system with a highly inter-active interface utilizing pre-defined tags to find songs. The three systems received mean scores between 4.15 and 5.37 across all criteria [11]. Tonic received the best score in Affordance (4.71), Feedback (4.79), and Overall Satisfaction (OS) (5.11). Thank You scored the highest in Learnability (5.37) with an OS of 4.15. Moody led in Ro-bustness (4.53) with an OS of 4.63. However, the results of the Kruskal-Wallis test [5] showed that only the OS category had significant differences across systems [11]. 3. ANALYSIS OF USER FEEDBACK 3.1 Codebook and Coding Process We employed content analysis, a widely used qualitative data analysis method as described in [13], to uncover and code common themes in the 99 user responses. On aver-age, there were 69 words in a response (median=51, max=259, min=2). The codebook was developed through an iterative process involving test-coding a subset of data and revising the codes for clarity. Table 1 presents de-tailed information on all the codes that emerged from the user responses. Each user response contained an average of 3.17 excerpts, each representing a particular code. The codes were organized into seven higher-level categories based on topical similarity. The count of excerpts for each code and the percentage calculated over the total number of excerpts (314) are also reported in the table.                                                             3Accessible at: http://bit.ly/1zqz1m0 (Thank You), http://bit.ly/1R3rNdr (Moody), http://bit.ly/1GU7GLO (Tonic) \n780 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015    Categories Codes Definition  # % \nEvaluation Criteria Aesthetics attractiveness The user specifically talks about the visual appeal of the interface.  27 8.6 Affordance  access  The user specifically comments on an ability to access original music files within the system.  7 2.2 play function The user specifically talks about the music play function in the system, in-cluding various aspects of the player such as the interface and features.   25 8.0 save function The user specifically talks about some kind of save function like a book-mark function allowing users to revisit the page, ability to save the selected songs, or preservation of specific system settings set by the user. 12 3.8 search/ browse The user specifically mentions topics related to searching or browsing mu-sic based on metadata (e.g., artist name, song/album title, genre, mood la-bels), including advanced search, auto-complete, and finding similar items.  91 29.0 Feedback clarity The user specifically talks about the clarity of functions or labels provided. 39 12.4 Learnability  ease of use The user talks about how easy, intuitive, and user-friendly it is to use the system and complete their desired task. 40 12.7 help The user comments on help provided in the system such as guidelines, tuto-rials, or instructions.  9 2.9 Performance bugs/ glitches  The user specifically talks about bugs/glitches in the system that cause it to produce incorrect or unexpected results, or behave in unintended ways.   5 1.6 response time The user specifically talks about the response time (i.e., the length of time taken for a system to react to a given event). 8 2.5 search  results The user specifically talks about the quality of search results and how they are presented to the user. 32 10.2 Utility usefulness The user talks about the overall usefulness of the system, as well as its use-fulness for the given evaluation task. 13 4.1 \nAdditional aspects External  factor dataset The user specifically notes the effects and/or limitations of using a particu-lar dataset for the evaluation task.  6 1.9 Sentiment positive The user expresses positive feelings in terms of a particular code. 107 34.1 negative The user expresses negative feelings or desires for specific func-tions/features in terms of a particular code. 198 63.1 Table 1. Summary of codebook. \n The first six categories correspond to particular eval-uation criteria. We can observe that three of these catego-ries were used as evaluation criteria in GC14UX (in bold). Codes matching the criterion Robustness did not emerge from coding user responses. The External factor category contains the code dataset that was used to mark the responses noting limitations of the experience due to variables that were not controllable by system developers. We also had an “Other” code used for uncommon but rel-evant part of responses that did not fit into existing codes (e.g., comments on scalability issues, mobile device com-patibility, etc.). Codes in the Sentiment category (i.e., positive and negative) were used in conjunction with an-other code to note users’ feelings regarding that code. 3.2 Inter-coder Reliability To ensure consistent application of codes, two coders were recruited. The coders independently coded a subset of user excerpts (42% of all excerpts) and Cohen’s kappa coefficient [4] was calculated to measure their agreement. Table 2 shows that all the kappa coefficients for each code fall in the range of good (.60-.74) or excellent agreement (.75-1.0) [3,8]. The Pooled Kappa statistic summarizing the overall results across all the codes [21] was .884, suggesting an excellent agreement.  Code Kappa value Agreement level save function 1.00 excellent bugs/glitches 1.00 excellent negative 0.98 excellent positive 0.97 excellent play function 0.95 excellent response time 0.92 excellent help 0.91 excellent dataset 0.88 excellent attractiveness 0.87 excellent clarity 0.86 excellent usefulness 0.85 excellent ease of use 0.82 excellent search/browse/metadata 0.80 excellent search results 0.80 excellent access  0.66 good Table 2. Kappa coefficients for each code. 3.3 Tabulation of Codes Table 3 shows the counts of positive excerpts for each system, sorted by the sum of all counts for each code. We can observe that participants liked Thank You for more functional reasons (e.g., search/browse, access to music files, search results) whereas they liked Tonic for aesthet-ics and usability aspects (e.g., attractiveness, ease of use, usefulness) in addition to functional reasons (e.g., play function, save function). Moody’s scores were fair across most of the codes except save function, access to music files, and search results. Overall, Tonic had the highest number of positive excerpts, with Thank You and Moody having approximately the same numbers.  Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 781    Thank You Moody Tonic Sum search/browse 14 10 4 28 ease of use  8 9 10 27 attractiveness  0 7 11 18 usefulness  1 1 6 8 play function  1 2 3 6 save function  0 0 6 6 access to music files  4 0 0 4 clarity  1 1 1 3 help  0 1 2 3 response time  1 1 1 3 search results 1 0 0 1 Total 31 32 44 107 Table 3. Tabulation of positive codes. We also tallied up the counts of negative excerpts for each system (Table 4). Negative excerpts also include de-sires for additional features/functions, so a high count does not necessarily mean that participants disliked the system. Moody had the highest number of negative ex-cerpts, mostly for search/browse, which was also the most commonly mentioned aspect across all three sys-tems. Evaluators had strong opinions about the search function in Moody, also evidenced by the highest number of counts in search results. For Tonic, improving the clar-ity and help was important, in addition to play function.     Thank You Moody Tonic Sum search/browse 19 34 10 63 clarity  5 10 20 35 search results  3 11 9 23 play function  3 7 9 19 ease of use  4 2 7 13 attractiveness  4 4 1 9 save function 2 4 0 6 dataset 2 3 1 6 help  0 1 5 6 bugs/glitches  2 1 2 5 response time  3 1 1 5 usefulness 2 3 0 5 access to music files 0 1 2 3 Total 49 82 67 198 Table 4. Tabulation of negative codes. When we tabulate the counts based on the top-level categories and compare the counts for positive and nega-tive excerpts for each category, we can observe with which aspects evaluators were most satisfied and dissatis-fied (Table 5). Across all three systems, Affordance, Per-formance, and Feedback had more negative excerpts, suggesting these aspects need to be improved upon. Learnability, Aesthetics, and Utility had more positive excerpts overall, although notably Thank You had no pos-itive excerpt for Aesthetics. Top level Thank You Moody Tonic Sum Affordance + 19 12 13 44 Affordance - 24 46 21 91 Learnability + 8 10 12 30 Learnability - 4 3 12 19 Feedback + 1 1 1 3 Feedback - 5 10 20 35 Performance + 2 1 1 4 Performance - 8 13 12 33 Aesthetics + 0 7 11 18 Aesthetics - 4 4 1 9 Utility + 1 1 6 8 Utility - 2 3 0 5 Table 5. Tabulation of codes at the top level categories. 4. DISCUSSION OF CATEGORIES AND CODES 4.1 Aesthetics Aesthetics consists of a single code regarding the overall attractiveness of the system. While this aspect was not included in the GC14UX evaluation criteria, it may be appropriate to consider adopting it for future iterations. Most excerpts coded with attractiveness were about how appealing the visual interface was, with a few comments about the use of white space, clean interface, use of ani-mation, and background color. The importance of this aspect is well-captured in the following response: “What's funny is that while [Thank You] allows me to search and browse, I really liked the graphic nature of the previous two interfaces. I don't necessarily think this interface performs any less well than the others--” 4.2 Affordance Affordance consists of four codes related to particular features or functions of the system. For access, most of the excerpts mentioned that Thank You was the only sys-tem where users could download the songs. To some par-ticipants, that meant that the system was “complete,” and gave them “real results.”  Excerpts coded with play function tended to be more negative, mentioning evaluators’ desires to have more control in which part of the song they are playing. Some evaluators did appreciate that Tonic plays the selected songs starting in the middle (e.g., “I like the fact that the selected pieces start playing from the middle, giving an immediate sense of the general mood and texture of the piece”), but more evaluators wanted to be able to select from multiple options themselves: “I would like to have an [sic] checkable option, “start playing from beginning”/”start playing from the middle\" (or 25%, 30%, 40%), because sometimes [what] is im-portant [is] the beginning, and sometimes (mostly) the mood of whole song.”  Evaluators also commented negatively on the fact that they had to go through another step for playing the music in Thank You and Moody (e.g., “I’d expected to start playing a track whenever I clicked on its cover, instead of having to wait for the pop-up and click ‘play.’ ”). The lack of visibility of the play button/slider was also noted for Moody and Tonic (e.g., “The ‘play’-slider is a bit small”; “difficult to find play button for the next song”).  782 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   With regard to the save function, Tonic had multiple positive excerpts on the usefulness of the bookmark func-tion, which was missing in other systems:  “there is a function at the top left corner for users to save their favorite results, it is convenient for user to compare the music later and choose the best result…” [Tonic] “i wish there were a way for me to create[,] like a list, collection, playlist, or save or favorite multiple songs for comparison or reconsideration.” [Moody] The system remembering user settings was also im-portant; evaluators noted that in Thank You, the player does not keep the selected volume level when a new song is loaded, and in Moody, switching between the mood and genre tab discards the selected search parameters. The save function code is somewhat related to the Robustness criterion in GC14UX; users want features that will help them trace back and return to previous results, although no excerpts were related to recovering from an error.   Overall, the search/browse code was applied most of-ten, excluding the sentiment codes. Thank You had the highest number of positive excerpts due to the fact that multiple search options were provided (i.e., text search, form search, and advanced search) and users had the most control over how the search could be conducted (e.g., “Its searching technique is very comprehensive and fully de-veloped, which is excellent for users to carry out detailed and accurate search”). The auto-complete features in Moody and Tonic were also appreciated by multiple eval-uators. However, there was still a lot to be desired from the search/browse functions in all three systems. For Thank You, the lack of a browsing mechanism and inabil-ity to get recommendations were noted. Evaluators also commented on the limitation of genre categorization:  “...about 255 songs are identified as unknown, it may cause inconvenience to the users as they do not know the type of song, they must spend time to listen [to] it first.” For Moody, nine excerpts specifically asked for an ability to combine both mood and genre for search. Some wanted more labels for mood and genre, and others noted the lack of a free-text search option. For Tonic, a few evaluators commented on the inaccuracy of certain labels and a lack of vocabulary control: “…the connection from tags to audio content does not always seem to be ‘correct’…Especially if more than two tags are combined, there seem to be some problems.” “Moreover, maybe due to the vocabulary control, when I type ‘cheerful,’ no result is found, I have to type ‘happy’ instead, so the system is not flexible enough.” 4.3 Feedback This category consists of a code “clarity” that is about how intuitive and clear the functions and labels were. Tonic had 20 negative excerpts that were primarily about evaluators having trouble understanding what information the different design constructs are trying to convey (e.g., meaning of the histogram, size of the bubble) or what the result of a particular user action was: “For instance, I noticed some bars on the left side, each corresponding to one of the search terms, that varied in height along with the bubble, which also resized. What is that? What does it mean when I move bubbles around?” Similar concern was also raised for Moody (e.g., “what does the size of the image mean?”). In addition, a couple of evaluators pointed out that they had a hard time figuring out what the “discover music similar to” function was supposed to do. For Thank You, several evaluators commented on misunderstanding genre ID as the count of items under a particular category. 4.4 Learnability This category contained two codes: ease of use and help. Overall, there were a lot more positive than negative ex-cerpts regarding the ease of use across all three systems. Simple, intuitive, and user-friendly interface design was appreciated for Moody and Tonic. In general, evaluators also found the basic search interface in Thank You easy to use. Negative excerpts were on issues like the page layout or too much text (Thank You) or opinions based on a comparison with other systems (e.g., “Tonic is not that easy to use when comparing with Moody”). For the help code, evaluators commented positively on the usefulness of a short introduction on how to use the system for Tonic but still desired more explanation on the meaning of design elements. For Thank You and Moody, clear searching guidelines and limitations (e.g., “Maybe it should say somewhere that the similarity search only works for artists in the database”) were desired. 4.5 Performance Of the three codes belonging to the Performance category (i.e., bugs/glitches, response time, search results), search results was most commonly used, and primarily with negative sentiment. For Thank You, the ability to sort the results was appreciated but different sorting criteria were desired. The lack of a sorting mechanism was also men-tioned for Moody. In addition, three evaluators stated that they wanted to know how many results there are for a particular search, as well as an option to switch between AND and OR connectors. For Moody and Tonic, several evaluators commented that they did not agree with or could not understand the results:   “The returned music doesn't really fit the moods, espe-cially ‘romantic.’ ” [Moody]  “I wrote: ‘piano’ and ‘jazzy,’ and just in the middle be-tween these two main bubbles I found the song ‘Salmacis – Arkangel,’ which is not piano nor jazzy at all.” [Tonic] The evaluators’ reactions to response time tended to vary, even for the same system, possibly due to varying Internet connection speeds and different levels of expec-tation. Bugs and glitches in scrolling, music playback, and entering data were also mentioned a few times, but they could also depend on the resolution setting or other configurations of the evaluators’ machines and browsers. Therefore, it is important to note that what we are seeing Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 783   is simply users’ interpretation of how well the system performed rather than the objective performance level.  4.6 Utility “Usefulness” was the only code in this category, noting the general usefulness of the system as well as its appro-priateness for the specified user task. Tonic had all posi-tive excerpts as evaluators deemed that the tag-based browsing interface worked well for unknown music. The negative excerpts on Thank You and Moody mostly showed that evaluators wanted more features and func-tions. For Thank You, one evaluator noted that the search interface is limiting for the given evaluation task, which is about finding music for editing a personal video, since there are no content-based features. 4.7 External Factor Comments on the limitation of the dataset were captured using the code dataset in this category. Six excerpts marked with this code were all negative, mostly stating that evaluators’ unfamiliarity with the songs hindered their ability to effectively use the systems. This was espe-cially true for Thank You, as evaluators could not issue searches using metadata such as artist name or song title. One evaluator also noted the difficulty in ascertaining the cause of unsuccessful results:  “…maybe the Jamendo collection is not very good for the task because of its variability: do we really not have good results or are systems unable to find them?” 5. IMPLICATIONS ON UX EVALUATION IN MIR Based on the user responses and the experience of run-ning GC14UX, we discuss three main implications for future UX evaluation tasks in MIR:  1) Adjustment of evaluation criteria We recommend considering new criteria, Aesthetics, Performance, and Utility, in future UX evaluation tasks. The quantitative ratings showed that the difference of the scores in the “Overall Satisfaction” was statistically sig-nificant, but the differences in the other four criteria were not. This suggests that perhaps there are additional evalu-ation criteria affecting users’ overall satisfaction. Based on the responses, the visual aesthetics of the system seem especially important; it is noteworthy that a large propor-tion of positive excerpts for Tonic, the most highly rated system, were based on “Aesthetics”. “Aesthetics” might be the missing piece that can explain the differences ob-served in the “Overall Satisfaction”. We also recommend rethinking the criterion “Robustness”; this may be diffi-cult to evaluate given the limited time evaluators have to interact with the systems in the MIREX framework.  2) A better dataset and more user tasks  As some users pointed out, lack of familiarity with the songs in the dataset hindered their search/browse experi-ence. In addition, a single user task for evaluation seems limiting, as MIR systems can serve a wide variety of use cases and scenarios. This was in fact the case in GC14UX as the three evaluated systems were designed to serve dif-ferent goals (e.g., Thank You for known-item searches, Moody for mood and genre-based search/browsing, and Tonic for exploring new music based on tags). For future UX evaluation, it might be worthwhile to consider estab-lishing multiple user tasks, and perhaps something more common (e.g., playlist generation, recommendation) ra-ther than trying to creating a task suitable for the dataset.    3) Focus on evaluation rather than competition  In addition to a common user task for evaluation, it may be fruitful to consider asking system developers to define a user task for which they want their system to be evaluated, as a secondary task. This makes sense consid-ering that many commercial MIR systems are often tar-geted to support specific MIR tasks (e.g., Pandora for online radio function, Shazam for music identification), which was also the case for the three evaluated systems from GC14UX. We do acknowledge that this means we will not be able to directly compare the evaluation results of multiple systems. However, we strongly believe that the community should move away from considering this evaluation as a competition where ranking the systems is the primary goal. If we treat this as an opportunity to evaluate the systems in order to improve the design of all participating systems rather than being able to claim one system is better than the other, this issue will naturally dissolve. In case of GC14UX, the differences in scores for the three systems were not substantial; even for the single category where there was a statistically significant difference among the scores (i.e., Overall Satisfaction), the difference between the best- and the worst-performing systems is less than one point in a seven-point Likert scale (5.11 vs. 4.15). What would truly benefit our com-munity as a whole is learning from the feedback about what users need and want, which will inform us on how to improve the design of MIR systems in general.        6. CONCLUSION AND FUTURE WORK GC14UX was the very first attempt in conducting a holis-tic evaluation of user experience for complete MIR sys-tems in the history of MIREX. Therefore, reflecting on our experience and deliberating on how to improve future UX evaluation is critical. Our findings indicate which as-pects of the systems most concerned users, and how we can use that knowledge to improve the design of and cri-teria for future UX evaluation. We discussed three key implications for future UX evaluation: 1) consider three new criteria in future UX evaluation tasks, 2) seek a bet-ter dataset to improve evaluators’ ability to effectively use the features and judge the quality of the results, and select more user tasks to reflect the diversity of the sys-tems, and 3) focus on evaluation for the improvement of systems rather than competition. We hope to continue UX evaluation as a regular task within MIREX, and redesign the task with new use scenarios and datasets in the future. We also plan to widen our pool of evaluators so that we can do a comparative analysis of how MIR experts and general users evaluate their experiences. 784 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   7. REFERENCES [1] J.-J. Aucouturier and F. Pachet: “Improving timbre similarity: how high’s the sky?” Journal of Negative Results in Speech and Audio Sciences, Vol. 1, No. 1, pp. 1-13, 2004. [2] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and A. Klapuri: “Automatic music transcription: breaking the glass ceiling,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 379-384, 2012. [3] D. V. Cicchetti: “Guidelines, criteria, and rules of thumb for evaluating normal and standardized assessment instruments in psychology,” Psychological Assessment, Vol. 6, pp. 284-290, 1994. [4] J. Cohen: “A coefficient of agreement for nominal scales,” Educational and Psychological Measurement, Vol. 20, No. 1, pp. 37-46, 1960. [5] G. W. Corder and D. I. Foreman: Nonparametric Statistics for Non-Statisticians. Hoboken: John Wiley & Sons, 2009. [6] J. S. Downie, D. Byrd, and T. Crawford: “Ten years of ISMIR: reflections on challenges and opportunities,” Proceedings of the International Conference on Music Information Retrieval, pp. 13-18, 2009. [7] J. S. Downie, X. Hu, J. H. Lee, K. Choi, S. J. Cunningham, Y. Hao, and D. Bainbridge: “Ten years of MIREX (Music Information Retrieval Evaluation eXchange): reflections, challenges and opportunities,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 657-662, 2014. [8] J. L. Fleiss: “Measuring nominal scale agreement among many raters,” Psychological Bulletin, Vol. 76, No. 5, pp. 378-382, 1971.  [9] K. Hoashi, S. Hamawaki, H. Ishizaki, Y. Takishima, and J. Katto: “Usability evaluation of visualization interfaces for content-based music retrieval systems,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 207-212, 2009. [10] X., Hu and N. Kando: “Evaluation of music search in casual-leisure situations,” Proceedings of the 5th Information Interaction in Context Symposium on-IIiX'14, pp. 1-4, 2014. [11] X. Hu, J. H. Lee, D. Bainbridge, K. Choi, P. Organisciak, and J. S. Downie: “The MIREX Grand Challenge: a framework of holistic user experience evaluation in music information retrieval,” Journal of the Association for Information Science and Technology, under review. [12] X. Hu and J. Liu: “Evaluation of music information retrieval: towards a user-centered approach,” Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval (HCIR), 2010. [13] K. H. Krippendorff: Content analysis: an introduction to its methodology. Thousand Oaks: Sage, 2013. [14] J. H. Lee and S. J. Cunningham: “Toward an understanding of the history and impact of user studies in music information retrieval,” Journal of Intelligent Information Systems, Vol. 41, No. 3, pp. 499-521, 2013. [15] J. H. Lee and R. Price: “User experience with commercial music services: an empirical exploration,” Journal of the Association for Information Science and Technology, DOI: 10.1002/asi.23433, 2015. [16] J. Nielsen: “Heuristic evaluation,” In J. Nielsen, and R. L. Mack (Eds.), Usability Inspection Methods. John Wiley & Sons, New York, NY, 1994. [17] S. Pauws and B. Eggen: “PATS: Realization and user evaluation of an automatic playlist generator,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 222-230, 2002 [18] S. Pauws and S. van de Wijdeven: “User evaluation of a new interactive playlist generation concept,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 638-643, 2005. [19] M. Schedl, A. Flexer, and J. Urbano: “The neglected user in music information retrieval research,” Journal of Intelligent Information Systems, Vol. 41, No. 3, pp. 523-539, 2013. [20] F. Vignoli and S. Pauws: “A music retrieval system based on user driven similarity and its evaluation,” Proceedings of the International Society for Music Information Retrieval Conference, pp. 272-279, 2005. [21] H. De Vries, M. N. Elliott, D. E. Kanouse, and S. S. Teleki: “Using pooled kappa to summarize interrater agreement across many items,” Field Methods, Vol. 20, pp. 272-282, 2008.    Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 785"
    },
    {
        "title": "Automatic Mashup Creation by Considering both Vertical and Horizontal Mashabilities.",
        "author": [
            "Chuan-Lung Lee",
            "Yin-Tzu Lin",
            "Zun-Ren Yao",
            "Feng-Yi Lee",
            "Ja-Ling Wu"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416712",
        "url": "https://doi.org/10.5281/zenodo.1416712",
        "ee": "https://zenodo.org/records/1416712/files/LeeLYLW15.pdf",
        "abstract": "In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by over- laying music segments on one single base track, the pro- posed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: “harmonic change balance” and “volume weight” have been considered. On the horizontal side, the meth- ods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encoun- tered and found the proper solution to each of them. Sub- jective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listen- ing enjoyment. Besides, by taking the newly proposed ver- tical mashability measurement into account, the improve- ment in user satisfaction is statistically significant.",
        "zenodo_id": 1416712,
        "dblp_key": "conf/ismir/LeeLYLW15",
        "keywords": [
            "music mashups",
            "re-created music",
            "mixing parts of multiple existing music pieces",
            "proposed system",
            "multiple background and lead track segments",
            "suitability between vertically overlaid tracks",
            "horizontal mashability",
            "harmonic change balance",
            "volume weight",
            "four levels of mashability"
        ],
        "content": "AUTOMATIC MASHUP CREATION BY CONSIDERING BOTH VERTICALAND HORIZONTAL MASHABILITIESChuan-Lung Lee1Yin-Tzu Lin1Zun-Ren Yao1Feng-Yi Lee2Ja-Ling Wu1Communications and Multimedia Laboratory, National Taiwan University, Taiwan1{kane0986,known,yyy110011,wjl}@cmlab.csie.ntu.edu.tw,2milkycc1111@gmail.comABSTRACTIn this paper, we proposed a system to effectively createmusic mashups – a kind of re-created music that is madeby mixing parts of multiple existing music pieces. Unlikeprevious studies which merely generate mashups by over-laying music segments on one single base track, the pro-posed system creates mashups with multiple background(e.g. instrumental) and lead (e.g. vocal) track segments.So, besides the suitability between the vertically overlaidtracks (i.e. vertical mashability) used in previous studies,we proposed to further consider the suitability between thehorizontally connected consecutive music segments (i.e.horizontal mashability) when searching for proper musicsegments to be combined. On the vertical side, two newfactors: “harmonic change balance” and “volume weight”have been considered. On the horizontal side, the meth-ods used in the studies of medley creation are incorporated.Combining vertical and horizontal mashabilities together,we deﬁned four levels of mashability that may be encoun-tered and found the proper solution to each of them. Sub-jective evaluations showed that the proposed four levels ofmashability can appropriately reﬂect the degrees of listen-ing enjoyment. Besides, by taking the newly proposed ver-tical mashability measurement into account, the improve-ment in user satisfaction is statistically signiﬁcant.1. INTRODUCTIONA Mashup is a kind of popular music what is made byoverlaying, connecting, digitally modifying parts of twoor more existing audio recordings [29]. The most commonway to create a mashup is to overlay the vocal track of onesong on the instrumental track of another [29]. With theaid of high-speed Internet, users are more easily to trademusic materials and ﬁnd related information through so-cial websites [1, 3]. The development and availability ofdigital audio editing techniques and software also reducedthe entry barrier for creating mashups. For example, thec\u0000Chuan-Lung Lee, Yin-Tzu Lin, Zun-Ren Yao, Feng-YiLee and Ja-Ling Wu. Licensed under a Creative Commons Attribution4.0 International License (CC BY 4.0).Attribution:Chuan-Lung Lee,Yin-Tzu Lin, Zun-Ren Yao, Feng-Yi Lee and Ja-Ling Wu. “AutomaticMashup Creation by Considering Both Vertical and Horizontal Mashabil-ities”, 16th International Society for Music Information Retrieval Confer-ence, 2015.BBLL……B…LLLL\u0001MFigure 1. Common strucuture of mashup songs. Eachblock labeled with “L” or “B” represents a segment in leadtrack (e.g. vocal track) or the background track (e.g. in-strumental track) from the same songs, respectively.MandNdenote the number of lead track segments per back-ground segment and the total number of background tracksegments in the resultant mashup, respectively.loop-based music sequencers such as Sony ACID Pro andAbleton Live make it easier for users to match the beats andshift the keys of the audio samples. As a result, mashupsare now more often created by music lovers without formalmusical training [29]. However, with the aforementionedtools, users still need to rely on their own experiences andmusical training to ﬁnd out proper music clips to be com-bined together. As the amount of currently available digitalmusic explosively goes up, ﬁnding suitable clips becomestime-consuming and labor-intensive. How to automaticallyﬁnd out and create pleasant mashups becomes an challeng-ing and interesting issue.Some previous studies have proposed automaticschemes to create mashups. But those approaches [8, 9]merely focused on the vertical suitability of the chosenmusic segments, that is, “they considered only about howsuitable are the music segments to be overlaid”, which wasdeﬁned as the term “mashability” in [8]. By observation,many human made mashups1are not created just by over-laying different music segments on one single base track,as proposed in [9]. A Mashup can also be composed ofsegments of multiple background tracks (e.g. instrumentaltracks) segments from different songs. Each backgroundtrack segment is overlaid with several lead track segments(e.g. vocal tracks). As shown inFigure 1, when the back-ground track segments changed, the lead tracks on top ofthem may still remain in the same song. So, while ﬁndingproper segments for generating mashups, we need to con-sider not only the vertical mashability between lead andbackground track segments but also the horizontal relation-1https://www.youtube.com/watch?v=If5MF4wm1T8399ships between consecutive lead/background segments – wedeﬁned this relation as the “horizontal mashability”.In this work, a framework is proposed to automaticallycreate mashups by considering both the vertical and thehorizontal mashabilities. Besides, two additional factors:“harmonic change balance” and “volume weighting” tothe vertical mashability are also considered and investi-gated. Subjective evaluation shows, by taking these fac-tors into account, the users’ listening pleasance of the cre-ated mashups will be enhanced as compared with that ofthe original counterparts created in [9]. Moreover, by in-tegrating with the horizontal mashability, various degreesof listening enjoyment of mashups can be achieved. As aresult, given a set of multitrack songs with structural seg-ment labels, the ﬁrst background track users want to ex-tracted from and some desired structure factors (such as thenumber of background track segmentsN, and the numberof lead track segments per background segmentM), thesystem will then automatically generate a pleasant mashupwith the structure as illustrated inFigure 1.We assume that the multitrack songs should at least con-tain two kinds of tracks: background and lead. This as-sumption is reasonable because multitrack songs can beeasily retrieved from mashup-related social websites [1,3].The unit of input segments depends on the granularity ofuser speciﬁed song changes in a mashup. The unit could beas large as a structural section (e.g. verse, chorus), or be assmall as a musical phrase (e.g. half or quarter of a verse),but we assume that all segment boundaries are aligned withbars. If users are not willing to provide segment bound-aries, we can still detect the boundaries by using currentstructural segmentation techniques [14]. These input seg-ments are regarded as the basic units to create mashups. Todistinguish “input segment” from the generally used term“segment”, in the rest of the paper, we will term the it as“unit”. Therefore, in this paper, a lead unit stands for asegment in the lead track of an input song, and so on.2. RELATED WORKAs compared with other music genres, mashup music isstill young, so there is still a few academic studies focusedon automatic mashup creation. Grifﬁn et al. [13] pro-posed an efﬁcient way to adjust the tempi of user-speciﬁedtracks and combine them after synchronizing their beats.The commercial software – Mixed in Key Mashup [2]uses the global harmonic compatibility among tracks inthe users’ music collection as the cue for track screen-ing and provides tools to help users match the beats ofthe chosen tracks. In other words, users still need to ﬁndout proper segments in the chosen tracks by themselves.AutoMashupper [8, 9] is the ﬁrst study that provided athorough investigation on measurement for ﬁnding propermusic segments to be overlaid together and an automaticmashup generation scheme. In AutoMashupper [9], an in-put song is regarded as a base track, and is segmented intoshort segments. For each segment, segments from othersongs that are with the highest mashability–on the basisof chromagram similarity, rhythmic similarity, and spectralbalance – will be overlaid with the corresponding segmentin the base track to create the ﬁnal mashup. The subse-quent studies [7, 27] also followed this structure. In [7],a live input audio is regarded as the base track, and theaccompanied music segments are overlaid upon the inputaudio. Tsuzuki et al. [27] focused on helping users over-lay voices from different singers who had sung the samesong along the common accompanied track. The proposedsystem, in contrast, is capable of creating mashups frommultiple background and lead segments.Besides mashup creation, there are other studies fo-cused on mixing parts from existing music recordings, bymeans of concatenating instead of overlaying the musicsegments, such as the automatic DJ [6, 15] [16, p. 97-101], the medley creation systems [18, 20] and concate-native synthesis [4, 24] [16, p. 101-102,109-111]. The for-mer two types of studies focused on concatenating longeraudio segments such as phrases or sections, and studies ofconcatenative synthesis focused on audio snippets that areas short as musical notes/onsets. To select proper units tobe concatenated, the existing systems may pick up propercandidates by comparing the similarity/distance betweenthe candidates and the given unit according to various au-dio features (e.g. tempo, rhythm, pitch, harmonic, andtimbre) [15, 16, 18], pre-cluster the all the units and thenchoosing among them according to some statistical mod-els [4, 20, 24], or align them with user speciﬁed condi-tions [4, 6, 20]. For short units (e.g. notes), the unitsmay be concatenated directly or accompanied with shortcross-fade. For long units (e.g. sections or phrases), theabove-mentioned systems may ﬁrst decide the transitionpositions between consecutive music segments on the ba-sis of rhythm [16] or chroma [18] similarity. And then,they adjusted the tempi (e.g. by phase vocoder [12]) andaligned the beats in the music segments with various meth-ods and then concatenated the segments by cross-fading.In this study, the pre-described methods used to ﬁnd propersegments and to smoothly connect them will be well-incorporated in the horizontal stage of the proposed sys-tem.3. PROPOSED FRAMEWORKThe proposed system framework is illustrated inFigure 2.In the preprocessing step, the system will ﬁrst extract audiofeatures and pre-compute vertical and horizontal masha-bilities for each possible pair of units in the given musicset. Then, according user speciﬁed structure factors (e.g.the ﬁrst song, the number of background track segmentsN, the number of lead track units per background seg-mentM, etc. ), we will determine (i)which and wherethe audio segment should locate in the resultant mashup– mashup composition (ii)how these segments are trans-formed to generate the resultant mashup. – mashup gener-ation. In the “mashup composition” step, we will ﬁrst pickMconsecutive background units from the user speciﬁedsong. We termed these units as a group of background unit(GBU). If users did not specify the ﬁrst song, our systemwill randomly choose a GBU for them. Then, in the verti-400 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Pre-processingMashup compositionMashup generationInput : Lead track segments & background track segmentsInput : User specified structural parametersVerticalHorizontalSegment modificationMixing Output\u0001 Resultant mashupBLBBLLL…\nBBLLL…Figure 2. Proposed system Framework.cal stage, we focus on ﬁnding proper lead units (the grayblocks marked with “L” inFigure 2) to be overlaid with theinput GBU via vertical multiple mashabilities. After that,in the horizontal stage, we aim at ﬁnding a proper subse-quent GBU (the gray block marked with “B” inFigure 2)by considering both vertical and horizontal mashabilities.The two processes, vertical and horizontal mashup stages,will be run iteratively until the resultant mashup reachesuser desired length – the number of GBUN. Finally, inthe mashup generation step, the tempo, loudness and pitchof each unit will be ﬁrst modiﬁed to the desired values, andthen the units will be mixed and concatenated to generatethe ﬁnal mashup song.4. PREPROCESSINGIn this step, the system will ﬁrst extract audio featuresand pre-compute vertical and horizontal mashabilities foreach possible pair of music units. The used featuresare beat/tempo [11], beat-synchronous chromagram [21],chord [22], MFCC [10], and volume [23]. For the easeof understanding, we will describe the used mashabilities,and how the above mentioned features are combined witheach mashability in the following sections.5. MASHUP COMPOSITIONIn mashup composition, our system will determine whichand where the basic units should locate. First, we will picka GBU as our starting point (user speciﬁed or randomlypicked by the system). The GBU should be withMcon-secutive background units in a song. Besides, all the back-ground units in a GBU should contain exactly2beats, for2N,\u00002. The reasons are (i)most popular songs arein 4/4 meter – 4 beats in a bar. (ii)most musical phrases inpop songs are multiples of four bars long [28]. (iii)mostverse or chorus sections contain 2 to 4 phrases [28].5.1 Vertical StageIn the vertical stage, our system will ﬁnd proper multi-ple lead units for each of the background unit in the inputiPitch classPitch classBeat\n(a)(b)Figure 3. Chromagrams of the segment unit with (a) sim-ple texture and (b) complex texture .GBU based on vertical mashabilities. Mashabilities usedin previous studies [9] include, harmonic matching, rhyth-mic matching and spectral balance. We do not use rhyth-mic matching in this stage because most lead tracks haveno kick or snare sounds, so rhythmic pattern becomes un-reliable in ﬁnding lead units. Spectral balance is also elim-inated because the sounds in lead tracks often spread in themid-band (220-1760 Hz), then spectral balance becomesindistinguishable. As a result, we adopt harmonic match-ing, and propose two new vertical mashabilities: harmonicchange balance, and volume weighting.5.1.1 Harmonic MatchingIn harmonic matching part, we use a similar method to thatof the AutoMashUpper [9]. The major difference is that wedirectly calculate the chroma similarity between each leadunit and background unit instead of shifting a window inthe whole song. The reason is that the original method cannot guarantee to get a complete lead unit. This may causeproblems for the subsequent horizontal stage process, es-pecially when it is the last unit in a GBU, because we willneed to ﬁnd consecutive lead units near GBU boundaries(please refer toSection 5.2for details). The mashabilityscore calculated by harmonic matching is denoted asSc.5.1.2 Harmonic Change Balance WeightingHarmonic change balance is a newly proposed mashability.The idea comes from the observation that chroma similar-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 401Pitch class(a)(b)\nBeatδ6 beats remain stableDid not cross the threshold —> not stableChroma similarity: chroma change pointsFigure 4. (a) Chromagrams of a segment unit and (b) thecorresponding plot of chroma similarity between consecu-tive beats.ities are not always proportional to the suitability for over-laying two segments. For instance, a given GBU composedof only one long note or long chord, the highest chroma-similar lead unit to it will highly probably be lead units thatare also composed of the same texture. Then, the pickedlead units for the GBU will all sound alike – long notesor long chords, which makes the resultant mashup soundboring and meaningless, even it sounds quite harmonic be-cause of high chroma-similarity. As a result, we proposedto match the input unit to the one that is composed of op-posite harmonic change rate, e.g., a background unit withsimple texture (such as the chromagram illustrated inFig-ure 3(a) ) should match with a lead unit with complex tex-ture (c.g.Figure 3(b) ), and vice versa. The harmonicchange rate can be calculated according to how many beatsremain stable on the chroma in a unit.Figure 4illustratesthe chromagram and the chroma similarities between con-secutive beats in a unit. The local minima of the chromasimilarity plot below the threshold\u0000can be deﬁned as thechroma change points. Then, the beats lie between any twochange points and contain exactly two crossing points to\u0000are regarded as stable beats. The percentage of stable beatswill be mapped to a sigmoid function to get a smooth scorefrom 0 to 1 (0% stable beat is mapped to 1 while, 100% sta-ble beats are mapped to 0), i.e. the harmonic change rate⇠. Then, the harmonic change balance weightswtcan becalculated as:wt=1\u0000|⇠p\u0000(1\u0000⇠q)|,(1)where⇠pand⇠qare the harmonic change rate of unitspandq, respectively. If harmonic change rate of a backgroundunit is 0.7, we tend to ﬁnd a lead unit whose harmonicchange rate is closer to 0.3.5.1.3 Volume WeightingThere are many inaudible (less than -40db) lead units ina lead track because the lead vocal or instruments oftenrest in sections such as intro, intermezzo, and outro. Toeliminate lead units contain too many inaudible parts, weincluded the volume weightingwvin our vertical masha-bility computation.wvcan be calculated according to theportion of the lead units that can be heard. That is,wv=⇢1,i f a\u000012⌘12+a⌘,i f a <12⌘,(2)\nFigure 5. Schematic diagrams showing how to generatemashups by considering (a) horizontal mashability and (b)vertical mashability, respectively.whereaand⌘are the numbers of audible and total beatsin a unit, respectively.Finally, we combine the aforementioned measurementstogether to ﬁnd the ﬁnal vertical mashabilitySv, that isSv=Sc·wt·wv+w⌧,(3)wherew⌧is an additional bonus to the pair of units withclose tempo, it is similar to the parameter↵adopted inEqn. (9) of [9].5.2 Horizontal StageIn this stage, we aim at ﬁnding a proper subsequent GBUTfor the input GBUIby considering both vertical andhorizontal mashabilities. A perfect subsequent GBUTshould satisfy two properties: (i)it can smoothly be con-catenated with the previous GBUI(cf.Figure 5(a)), and(ii)one can ﬁnd a proper leader unit on top of the ﬁrstbackground unit in this GBUTand the found leader unitcan be smoothly concatenated with the previous leader unit(cf.Figure 5(b)). To achieve property (i), we incorporatedan approach similar to the concept described in [20, Sec.7.2] and [19, Sec. 4.1]. We adopted the same similaritymeasurements and weights as [20] to compute the similar-ity between the GBU subsequent toIin the original trackand all the candidate GBUs, which was deﬁned as the hor-izontal mashabilitySh. Then, sort according toSh, we canget a rank list,Rh. A threshold↵is applied to cut off therank list: the GBUs withShthat are lower than↵are elim-inated. For dealing with the pre-described property (ii), wetake the opposite direction. That is, we ﬁrst check if thenext unit in the original track of the lead unitLImexists. Ifit is, we will temporally choose it as the ﬁrst lead unit forGBUT. Then, we can get another rank listRvof GBUsby sorting the vertical mashabilitiesSvbetween the ﬁrstbackground units of the GBUs and the picked lead unit. Asimilar threshold\u0000is also applied to the rank list to elim-inate inappropriate GBUs. After the above steps, we mayencounter four cases in the transitions between two GBUs.402 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Case 1.BothRhandRvexist, and{Rh\\Rv}6=?. Thisis the perfect case. Then, we can pick the ﬁrst GBUin{Rh\\Rv}as our result.Case 2.OnlyRvexists. We pick the ﬁrst GBU inRv.In case 2, the two background units at the transitionhave no correlation but they are bridged via the leadunits taken from the same song on top of them.Case 3.The opposite of case 2. OnlyRhexists. Wechoose the ﬁrst GBU fromRh. In this situation, twobackground units at the transition have high correla-tion but the lead units on top of them cannot stay inthe same track.Case 4.BothRhandRvdo not exist. We randomlychoose a GBU. In case 4, the two background unitshave no correlation and the lead units on top of themcannot stay in the same track. We can also pro-vide an optional self-repairing mechanism for case4. That is, instead of random selection, we choose aGBU that its next transition will ﬁt the condition ofcase 1 via pre-computation of all the possible casesof all the GBUs in the collection.A more complex situation is that, bothRhandRvexist,but{Rh\\Rv}=?. Which rank list should we choosefrom? According to the user evaluation results inSec-tion 7.2, most users prefer case 2 than case 3. So we willchoose a GBU fromRvﬁrst.6. MASHUP GENERATIONMashup generation can be divided into two steps: segmentmodiﬁcation and mixing. In segment modiﬁcation, we ﬁrstshift the pitches of the lead units to a target key, foundin the harmonic matching step (Section 5.1.1). The sameas [9], we use Rubberband library [5] to shift the pitches.Then, the volume of the lead units are also re-scaled tomatch that of the background unit by Replay Gain [23].After that, to match to beats of the units, we apply phasevocoder [12] to stretch the beats. Finally, we extend all ofthe units one beat long and apply cross fade technique toall of the transitions to create the resultant mashup.7. EXPERIMENT: SETTINGS AND RESULTSWe conducted two subjective listening tests. The ﬁrsttest is to evaluate the impact and the user’s acceptabil-ity of the four approaches, we proposed to deal withvarious transition conditions and also ﬁnd the proper-connecting priority of these four cases. The second testis to compare the compatibilities of lead units which areprovided by the mashability in AutoMashUpper [9] andby the vertical mashability in our system. The generatedmashups can be found inhttp://cmlab.csie.ntu.edu.tw/˜kane0986/ISMIR2015.html.7.1 DatasetWe use the multi-track audio dataset in [14] with structuralsegment labels. The dataset contains 104 pop songs, eachsong contains about 5 tracks on average. In the experi-ment, for each song, we take the lead vocal track as thelead track, and then we mix all the rest tracks into a singletrack and regard it as the background track. Examples ofbackground tracks are drum and bass tracks or chordal in-struments such as piano, guitar, or string. The vocal chorustrack is eliminated because it has different properties to ei-ther the lead or the background track. We take0.8478asthe threshold\u0000in the harmonic change balance weightingstep. The threshold is obtained by ﬁnding the intersectionof distribution of the chroma similarity values of consec-utive beats in 73 simple textured units and 156 complexunits. The other two thresholds,↵and\u0000we used in thehorizontal stage are set to0.6422and0.5740, respectively.These two thresholds are obtained through observations onthe ﬁrst derivatives of the sorted scores of all the unit pairs.7.2 Subjective Evaluations on Horizontal MashabilityIn the ﬁrst test, given the same background unitBand leadunits on top ofBas inputs, we then got four mashupswhich have dedicated conﬁgurations as those of pre-described case 1 to case 4, respectively. We also added theoriginal track of unitB, which has no transition, as a ref-erence, and is denoted as case 0. The user evaluations areconducted through the aid of a web interface, and the testedmashups are presented in random order. For each partic-ipant, he or she needs to listen ﬁve groups of mashups,and each group contains ﬁve mashups which respect tothe ﬁve cases we mentioned above. The questionnairesare designed based on a 7-point Likert scale [17]. Usersare asked to report their opinions about the degrees of en-joyment of the mashups from the following options: verypleasing (7), pleasing (6), somewhat pleasing (5), neutral(4), not so pleasing (3), not pleasing (2), and very unpleas-ing (1). 21 males and 6 females aged around 20⇠60 par-ticipated in this test. All of our participants have listeningtest experience, but most of them are not major in music(less than ﬁve participants have educational background inmusic) since our target consumer is general public.Figure 6shows the mean opinion score of each case.The paired Wilcoxon signed rank test [25] is applied toanalyze the results, where the corresponding p-values arereported inFigure 6. The overall result shows that the fourproposed cases did impact the human feeling of the resul-tant mashups under a conﬁdence level of 95%2. Case 1 israted second only to case 0. The score of case 2 is lowerthan case 1, but commonly higher than case 3. This in-dicates that users commonly prefer case 2 to case 3, i.e.,bridging two GBUs with no relation via one lead track ismore acceptable than concatenating two GBUs with highcorrelation but with the lead units do not stay in the sametrack. Case 3 is rated higher than case 4 commonly, but notas signiﬁcant as other cases. Case 4 gets the lowest scoregenerally, this veriﬁes that when there is signiﬁcant changein both of the lead and the background transitions, a greatimpact to user’s acceptability will result.2By Bonferroni correction, to preserve the total conﬁdence level as 95%, the p value for each paired comparison should be<0.05C52=0.005Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 40301234567\nNo trans.No trans.No trans.No trans.No trans.No trans.Case 1Case 1Case 1Case 1Case 1Case 1Case 2Case 2Case 2Case 2Case 2Case 2Case 3Case 3Case 3Case 3Case 3Case 3Case 4Case 4Case 4Case 4Case 4Case 4Mean scores of each methodScore  Total  (p=0.001)(p=0.000)(p=0.000)(p=0.004)   group 1   (p=0.046)(p=0.283)(p=0.001)(p=0.297)   group 2   (p=0.146)(p=0.022)(p=0.039)(p=0.708)   group 3   (p=0.010)(p=0.913)(p=0.059)(p=0.004)   group 4   (p=0.000)(p=0.207)(p=0.019)(p=0.037)   group 5   (p=0.554)(p=0.001)(p=0.545)(p=0.046)\nFigure 6. Mean opinion scores of total and each test sam-ple, in which the relevant p-values of paired Wilcoxonsigned rank test [25] on “case 0 (no transition) vs. case1”, “case 1 vs. case 2”, “case 2 vs. case 3”, and “case 3vs. case 4” are displayed above the corresponding bars ofeach one of the experiments, respectively.\n01234567\nAMUAMUAMUAMUAMUAMUProposedProposedProposedProposedProposedProposedMean scores of each methodScore  Total  (p=0.000)   group 6   (p=0.001)   group 7   (p=0.011)   group 8   (p=0.009)   group 9   (p=0.069)   group 10   (p=0.001)\nFigure 7. Mean opinion scores of total and each test sam-ple, in which the relevant p-values of paired Wilcoxonsigned rank test [25] on “AMU vs. our system” are dis-played above the corresponding bars of each experiment.Counter-intuitively, there are two groups in our listeningtest showing that case 1 is rated higher than case 0 slightlythough they did not statistically signiﬁcant. Possible rea-son would be that the two GBUs in case 0 happen to befrom verse and chorus sections of different styles, respec-tively. Then our system may have chance to ﬁnd anotherbackground unit which can be concatenated after the versesegment more smoothly than it’s own chorus counterpart.7.3 Subjective Evaluations on Vertical MashabilityIn the second test, we aim at comparing the vertical masha-bility provided by our system and the AutoMashUpper [9](denoted as AMU). We create the mashups by our methodand AMU’s method from the same input GBUIof 6 back-ground units. Besides, we force the chosen lead units tobe picked from different songs. As we mentioned inSec-tion 5.1, rhythmic matching and spectral balance are notreliable for the current dataset, so we only used the har-monic matching part in AMU3, i.e. the version in [8].3We implemented AMU’s methods by ourselves.Then, it is easily to pick lead units that are nearly inaudiblesince only harmonic matching is considered in the adoptedAMU version. To make a fair test, we also apply ourvolume weighting (Section 5.1.3) to AMU. As a result,the target component we compared here is the harmonicchange balance weighting. A similar evaluation procedureto the previous experiment was conducted. Users are in-vited to listen to ﬁve groups of mashups per time, and eachgroup has two mashups – generated by AMU and by oursystems, in random order. 18 males and 6 females agedaround 20⇠60 with similar background to the previous ex-periment participated in this test. The result of this test isgiven inFigure 7, and the corresponding p-values are alsoreported. The lead units generated by our system are com-monly rated higher than those created by AMU, under aconﬁdence level of 95%. This again veriﬁed the advantageof taking the harmonic change balance weighting into con-sideration. In fact, most of the GBUs are simple textured.So the lead units generated by AMU is more likely to picksimple textured lead units.8. CONCLUTION AND FUTURE WORKIn this paper, a novel system is proposed to effectively cre-ate music mashups. There are two main contributions donein our system. First, both vertical and horizontal mashabil-ities are taken into consideration. Through this, our sys-tem can create a mashup with multiple background andlead track segments, which provides much higher ﬂexibil-ity in making mashups than the systems proposed in previ-ous studies. Second, by taking the newly proposed verticalmashability measurement into account, user study showsthat the improvement in user satisfaction is statistically sig-niﬁcant. The subjective evaluations also show that the fourconcatenation cases we analyzed play a critical role in gen-erating enjoyable mashups.Many aspects of our system can be extended. First, inthe vertical stage, we could alternatively match the unitbased on the compatibility of pitch of lead unit and thechord of the background unit [26] instead of the chromasimilarity between the lead and the background units di-rectly. Second, sometimes we found that the lead unitschosen by our system are too different from one another,so that the created mashups would sound very abrupt. Toprevent this situation, we may restrict the chosen lead unitsto be with certain characteristics analyzed in advance, e.g.timbre, style, and emotion. Finally, we could further in-vestigate the effect of overlapping the lead units and thechorus track units. Even more, the background units canalso be separated into instrumental track units and drumtrack units, etc.. Toward the study about how to combineall kinds of units reasonably may provide true solutions tocreate music mashups in all conditions.9. ACKNOWLEDGMENTSThe authors are grateful to all the participants (CMLabDSP group members) who helped evaluate the results.404 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 201510. REFERENCES[1]DJ Mix Generator.http://www.djprince.no/site/DMG.aspx.[2]Mixed in Key. Mashup.http://mashup.mixedinkey.com/HowTo.[3]Mixter.http://ccmixter.org.[4]Gilberto Bernardes.Composing Music by Selection:Content-Based Algorithmic-Assisted Audio Composi-tion. Phd thesis, University of Porto, 2014.[5]C. Cannam. Rubber Band Audio Time StretcherLibrary, 2012.http://breakfastquay.com/rubberband/.[6]Dave Cliff. Hang the DJ : Automatic Sequencing andSeamless Mixing of Dance-Music Tracks. Technicalreport, HP Labs, 2000.[7]Matthew Davies, Adam Stark, Fabien Gouyon, andMasataka Goto. Improvasher: A Real-Time MashupSystem for Live Musical Input. InProc. NIME, pages541–544, 2014.[8]Matthew E P Davies, Philippe Hamel, KazuyoshiYoshii, and Masataka Goto. AutoMashUpper : An Au-tomatic Multi-Song Mashup System. InProc. ISMIR,Curitiba, PR, Brazil, 2013.[9]Matthew E. P. Davies, Philippe Hamel, KazuyoshiYoshii, and Masataka Goto. AutoMashUpper: Auto-matic Creation of Multi-Song Music Mashups.IEEETrans. ASLP, 22(12):1726–1737, December 2014.[10]Steven B. Davis and Paul Mermelstein. Comparisonof Parametric Representations for Monosyllabic WordRecognition in Continuously Spoken Sentences.IEEETrans. ASLP, 28(4):357–366, 1980.[11]Simon Dixon. Evaluation of the Audio Beat Track-ing System BeatRoot.J. New Music Res., 36(1):39–50,2007.[12]Mark Dolson. The Phase V ocoder: a Tutorial.Com-puter Music Journal, 10(4):14–27, 1986.[13]Garth Grifﬁn, YE Kim, and Douglas Turnbull. Beat-Sync-Mash-Coder: a Web Application for Real-TimeCreation of Beat-Synchronous Music Mashups. InProc. ICASSP, pages 2–5, Dallas, Texas, USA, 2010.[14]Steven Hargreaves, Anssi Klapuri, and Mark Sandler.Structural Segmentation of Multitrack Audio.IEEETrans. ASLP, 20(10):2637–2647, 2012.[15]Hiromi Ishizaki, Keiichiro Hoashi, and Yasuhiro Tak-ishima. Full-Automatic DJ Mixing System with Opti-mal Tempo Adjustment based on Measurement Func-tion of User Discomfort. InProc. of ISMIR, pages 135–140, Kobe, Japan, 2009.[16]Tristan Jehan.Creating Music by Listening. Phd disser-tation, Massachusetts Institute of Technology, 2005.[17]Rensis Likert. A Technique for the Measurement of At-titudes.Archives of Psychology, 22(140):1–55, 1932.[18]Heng-Yi Lin, Yin-Tzu Lin, Ming-Chun Tien, and Ja-Ling Wu. Music Paste: Concatenating Music ClipsBased on Chroma and Rhythm Features. InProc. IS-MIR, Kobe, 2009.[19]Yin-Tzu Lin, Chuan-Lung Lee, Jyh-Shing Roger Jang,and Ja-Ling Wu. Bridging Music via Sound Effect In-sertion.IEEE Multimedia. (to appear).[20]Yin-Tzu Lin, I-Ting Liu, Jyh-Shing Roger Jang, andJa-Ling Wu. Audio Musical Dice Game: A User-preference-aware Medley Generating System.ACMTOMM, 11(4), 2015.[21]Matthias Mauch and Simon Dixon. Approximate NoteTranscription for the Improved Identiﬁcation of Difﬁ-cult Chords. InProc. ISMIR, pages 135–140, 2010.[22]Yizhao Ni, Matt McVicar, Paul Santos-Rodriguez, andTijl De Bie. An End-to-End Machine Learning Systemfor Harmonic Analysis of Music.IEEE Trans. ASLP,20(6):1771–1783, 2012.[23]D. Robinson.Perceptual Model for Assessment ofCoded Audio. PhD thesis, University of Essex, 2002.[24]Diemo Schwarz.Data-driven concatenative soundsynthesis. Phd thesis, University of Paris 6 – Pierre etMarie Curie, 2004.[25]S. Siegel and N.J. Castellan.Nonparametric statisticsfor the behavioral sciences. McGraw-Hill, Inc., 1956.[26]Ian Simon, Dan Morris, and Sumit Basu. MySong:Automatic Accompaniment Generation for V ocalMelodies. InProc. SIGCHI, pages 725–734, 2008.[27]Keita Tsuzuki, Tomoyasu Nakano, Masataka Goto,Takeshi Yamada, and Shoji Makino. Unisoner : An In-teractive Interface for Derivative Chorus Creation fromVarious Singing V oices on the Web. InProc. ICMC,number September, pages 790–797, 2014.[28]Stephen Webber.DJ Skills: The Essential Guide toMixing and Scratching. Focal Press, 2007.[29]Melissa Hok Cee Wong. Mash-up. In Charles HiroshiGarrett, editor,The Grove Dictionary of American Mu-sic. 2 edition, 2013.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 405"
    },
    {
        "title": "Understanding Users of Commercial Music Services through Personas: Design Implications.",
        "author": [
            "Jin Ha Lee 0001",
            "Rachel Price"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.232222",
        "url": "https://doi.org/10.5281/zenodo.232222",
        "ee": "https://zenodo.org/records/232222/files/12_Paper.pdf",
        "abstract": "Most of the previous literature on music users needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control.",
        "zenodo_id": 232222,
        "dblp_key": "conf/ismir/LeeP15",
        "keywords": [
            "qualitative user study",
            "emerging personas",
            "music information retrieval systems",
            "user engagement",
            "context-switching",
            "system design implications",
            "privacy concerns",
            "design for specialized uses",
            "user interactions",
            "demographic considerations"
        ],
        "content": "UNDERSTANDING USERS OF COMMERCIAL MUSIC SERVICES THROUGH PERSONAS: DESIGN IMPLICATIONS Jin Ha Lee Rachel Price University of Washington jinhalee@uw.edu University of Washington rachelpr@uw.edu ABSTRACT Most of the previous literature on music users’ needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specif-ic interfaces/systems. In order to improve our understand-ing of how users’ personalities and characteristics affect their needs and interactions with MIR systems, we con-ducted a qualitative user study across multiple commer-cial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have de-veloped seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user en-gagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control. 1. INTRODUCTION Designing music information retrieval (MIR) systems such as music recommenders or music management sys-tems is challenging due to the wide variety of organiza-tional and listening strategies of music users [3]. Alt-hough the number of studies on music users, specifically related to their needs and interactions with MIR systems, has been increasing since the early 2000s [15], our under-standing on how to understand and model these users for system design is still lacking. Previous studies of MIR system users tend to focus on investigating needs, perceptions, and opinions of general users (represented by subjects recruited online or in aca-demic settings) or specific user groups. Studies involving specific user groups tend to investigate users based on particular demographic information or users of particular MIR systems. However, few studies attempt to categorize the “personalities” of music listeners surrounding their interaction behavior on multiple MIR systems. In addi-tion to demographic information, what kinds of personal characteristics can we use to model commercial MIR sys-tem users for system design? Our study aims to fill this gap in prior research and answer the following questions: RQ1. What kinds of user personas can we identify from real users of commercial MIR systems? RQ2. What are the expressed needs and behavior of each of these user personas, and what are the implica-tions for system design for each persona? Our research will contribute by providing a framework for understanding users of MIR systems based on their needs and interaction behavior, beyond typical demo-graphic information. This will help inform system de-signers to develop systems that are better targeted for their user groups representing particular personas, rather than creating a “one size fits all” mass production model. 2. RELEVANT WORK 2.1 HCI Studies Related to Music A number of studies in the human computer interaction (HCI) domain explore different user behavior related to music discovery or sharing. Most of the literature focuses on testing the usability of a particular system interface, or investigating user behavior related to music discovery or sharing within a particular application.  The literature reflects a growing understanding that current music listening habits are changing. Voong & Beale [26] highlight the fact that playlist generation is done differently now than in the past, whether users cre-ate playlists by mood, theme, or other criteria. In our re-search, we aim to understand these criteria that are rele-vant to users when generating playlists and judging the playlists created by music services, and how to use those criteria to influence user experience (UX) design. The social aspect of music consumption also seems to be a key area for investigation. Research around social playlists illustrates how friends can learn more about each other and can strengthen relationships through under-standing the preferences of others ([18][21]). Bonhard et al. [4] further illustrate that “friends from whom we seek recommendations are not just a source of information for us: we know their tastes, views and they provide not only recommendations, but also justification and explanations for them. (p. 1064)” The impact of new online music re-positories to people’s music discovery and sharing has also been discussed in [18].  Some studies looked at the problem of how personality affects recommenders. Researchers have borrowed theo-ries from psychology literature about personality, as in [6], exploring the impact of personality values on users’ needs for recommendation diversity. Their preliminary research shows a causal relationship between personality attributes, including openness, conscientiousness, extro-version, agreeableness, and neuroticism, and users’ diver-sity preferences when using a recommender system. In our work, we take a more empirical approach, looking at  © Jin Ha Lee, Rachel Price. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribu-tion: Jin Ha Lee, Rachel Price. “Understanding users of commercial music services through personas: design implications”, 16th Internation-al Society for Music Information Retrieval Conference, 2015. \n476   user data to understand various types of personas present in music services users and how the user experience can be designed to better accommodate these personas. 2.2 User Studies in MIR Prior studies of MIR system users can be categorized in-to: 1) empirical investigation of music information needs, behavior, perceptions, and opinions of humans, 2) exper-iments, usability testing, interface design involving hu-mans focusing on a particular MIR system, and 3) analy-sis of user-generated data such as queries or tags [15].  Of the first category, a few studies focus on “general music users,” often represented by queries in search en-gines, or human subjects recruited on various websites or a game (e.g., [8][17]). A majority of them, however, fo-cus on a particular group of users based on demographic information. Several researchers have investigated the effects of age (e.g., young adults in [14][27]) and nation-ality [10][12][23]. These studies revealed that age group and cultural background do affect how people perceive, use, and search for music. A number of studies also re-search needs and behaviors of users in specific music-related professions (e.g., musicologists [2], DJs [20], film-makers [9]). In order to complement the findings from these studies, we look beyond demographic infor-mation and model users based on their goals/behavior within MIR systems.    A few studies focused on investigating users’ experi-ences with existing commercial music services, and thus are more closely related to the current paper. Barrington et al. [1] and Lamere [13] evaluated the quality of provid-ed music recommendations or system-generated playlists. Barrington et al. [1] compare Apple iTunes’ Genius to two canonical music recommender systems: one based on artist similarity, and the other on acoustic similarity. They demonstrate the strength of collaborative filtering com-bined with musical cues for similarity (similar artists and other display metadata) and discuss factors that influence playlist evaluation, such as familiarity, popularity, trans-parency, and perceived expertise of the system. Lamere [13] also compares the playlists generated from Google’s Instant Mix, Apple iTunes, and the Echo Nest Playlist engine, and notes how personal preference of music or the context of music can affect the user experience with music services. Some factors that influence users’ evalua-tions of playlist (e.g., familiarity, popularity, transparen-cy) as well as the overall perception of the quality of mu-sic service (e.g., inexpensiveness, convenience, customi-zability) were also identified in [1] and [17], respectively. Celma [5] discusses varied recommendation needs for four different types of listeners (i.e., savants, enthusiasts, casuals, indifferents) based on their degrees of interest in music. Lee & Price [16] also evaluated commercial music services based on Nielsen’s ten usability heuristics, advo-cating for more holistic evaluation of MIR systems.  Some studies focused on investigating the factors that impact people’s music listening or sharing behavior. Baur et al. [3] analyzed a sample of 310 music listening histo-ries collected from Last.fm and 48 variables describing user and music characteristics. They found that temporal aspects such as seasons and the degree of users’ interests in novelty were important factors affecting people’s mu-sic listening behaviors. Additionally, a number of pat-terns regarding users’ music seeking and consumption behavior were observed in a large-scale survey [17]: an increased consumption in mobile streaming services, an increased desire for serendipitous music discovery and music videos, as well as a strong desire to customize and personalize their music experiences.  The scope and approach of our work differ from these studies on user experience with music services in that we investigate users of ten different MIR systems (Spotify, Pandora, YouTube, Songza, SoundCloud, Grooveshark, Bandcamp, Rdio, Last.fm, iTunes), and we take a qualita-tive approach, asking questions and observing users’ in-teractions with MIR systems. Our work aims to build up-on these studies and provide more detailed information about how user contexts or characteristics affect actual usage of music services. 3. RESEARCH DESIGN AND METHODS Table 1 provides an overview of the methods and activi-ties used for different phases for this study. The user data were collected through interviews and think-aloud ses-sions. All recruited participants were over 18 years old, and actively use at least one music service/application. All participants were undergraduate or graduate students at University of Washington. All the interviews were conducted between January and March 2014, either in-person or via Adobe Connect video conferencing. A total of 40 participants were interviewed and compensated with a $15 Amazon gift card. Methods Activities User  interview Semi-structured interview asking about how participants use music services and how they evaluate the quality of the services. Think-aloud  sessions Participants narrate their actions out loud as they use their preferred music service as they would in a typical session. Card  sorting Identify task-based user segments and create personas for each segment. Table 1. Overview of the study design The study session consists of two parts: first, subjects were interviewed about their preferred music services, discussing their interactions with the service, how they navigate the system, why they prefer one service over others, frustrations they experience with the service, and how they interact with the service in a typical session. Secondly, participants were asked to “think-aloud” or narrate their actions out loud to an investigator as they use their preferred music service in a typical session. These tasks include known-item search, browsing al-bums, artists, or genres, interacting with recommenda-tions, playlists, and radio stations, and other tasks as they arose. Each study session, consisting of the interview and think-aloud, lasted for approximately an hour.  The user data was used to generate a list of behaviors exhibited around MIR systems. A card sorting activity Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 477   was used to identify user groups with similar behaviors as a basis for deriving useful personas. Personas are “hypo-thetical archetypes of actual users (p.124)” representing their needs, behavior, and goals which allows for a goal-directed design of a system [7]. Persona development has been used to aid design and gain user insights across many fields [22], and can be beneficial for prioritizing audiences’ and users’ goals in product development [24].   We created a comprehensive list of user activities from the interview transcripts and think-aloud activities as well as the notes taken during observation. A total of 77 user behaviors related to music services were identified (e.g., read reviews, judge others’ tastes, seek recommenda-tions). Through a card sorting activity, similar behaviors were grouped, organized, and named. We then attempted to identify which types of users would show these kinds of behaviors and tentatively named these user groups (e.g., genre fans, tech savvy). Afterwards, we identified two relevant dimensions to express the differences among these user groups organized by their common behavior, or “task-based audience segments” [28]: Companionship (willingness to engage in social aspects of music recom-mendation and listening: social - neutral - private) and Investment (willingness to invest time/effort to interact with the system: positive - neutral - none). As a result, we derived these seven personas: x Active Curator: Neutral companionship + Positive investment x Music Epicurean: Social + Positive investment x Guided Listener: Neutral companionship + No in-vestment x Music Recluse: Private + Neutral investment  x Wanderer: Neutral companionship + Neutral in-vestment x Addict: Private + No investment x Non-believer: Social + Neutral investment Any user may exhibit a combination of these personas as they are not mutually exclusive. Each of these per-sonas is explained in detail in the following section. 4. USER PERSONAS 4.1 Active Curator This persona takes great pride in their music listening, and enjoys seeking new music and curating music he/she is already familiar with. This may come in the form of playlist creation, “saving” albums in online collections, or light music “research”, such as previewing songs or tak-ing recommendations from friends, blogs, and live shows. Of all the personas, this one is the most actively engaged with music services (“I’m definitely an active listener 98% of the time.” (P21)). This persona tends to utilize known-item search along-side other discovery tools, often searching rather than browsing (“I [search for song or artist] at least once a day.” (P26)). An active curator may often find discovery tools to be disappointing (“I feel like I end up listening to stuff I already know. It’s a little frustrating” (P1)). They tend to have higher expectations for music recommenda-tion services and may not always trust a service to make good recommendations. “One of the reasons I use these services is because I’m looking for linkages from music to music to music…I’m a little bit pedantic...In fact, I would love to have a little bit more information [about recommendations].” (P1) “I would love to see the metadata that goes into choosing each song…[I’d love to] be able to pick and choose those attributes, so I could say, ‘ok, I do like those smooth jazz elements, but I don’t like the saxophone solos.’” (P30) 4.2 Music Epicurean This persona may be considered a “music snob.” Music epicureans take an immense amount of pride in the music they collect and listen to, although they may not neces-sarily own all that music. Although streaming music is still an acceptable form of listening, this persona is more inclined to purchase music after listening to it than other personas as he/she genuinely cares about sound quality. A great amount of time is spent “hunting” for new music. This persona tends to focus on relationships between bands that may not be typically identified by a music rec-ommender, such as similar “scene”, overlapping band members, and a nuanced understanding of genre relation-ships, and thus expresses dissatisfaction towards the giv-en recommendations (“It looks like it’s only making rec-ommendations of artists based on artists.” (P23)). The music epicurean persona is unlikely to use music system recommendations; users representing this persona tend to also represent “The Non-believer” persona de-scribed below. The Music Epicurean leans on trusted sources for recommendations, whether it is a small group of friends with trusted taste or other “vetted” sources. “I’m very self-directed in listening to music. When I listen to the radio, it’s KEXP, and it’s usually a really short amount of time in the morning. I know what I want to listen to, why am I just going to let a random radio station tell me?” (P8) “For me it’s not really worth the time. I think it’s just going to recommend stuff that’s also tagged [similarly]...I do my own ways of [finding], and I rely on my friends and people I write with to recommend stuff...” (P6) 4.3 Guided Listener The Guided Listener’s most prominent quality is the de-sire to hand over control of the music to someone else. This persona mildly enjoys radio’s serendipitous nature, may have slight preferences over genre or artist, but ulti-mately just wants to hear something playing. This perso-na is not picky; he/she may occasionally interact with a service to indicate preferences or dislikes but will not go out of his/her way to curate albums or playlists. This persona may provide “seed” songs or artists to help a system generate a playlist or radio station, and in-frequently, will browse new music or artists for fun or out of boredom. For the most part, the guided listener is a “set it and forget it” kind of person. “It’s definitely ‘log in’, get to where I’m going, and it even goes back to the default station that I was listening to before. 478 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   I mean, I can get this thing booted up and going within seconds, and then I’m off doing dishes or whatever, which contributes to my satisfaction. It’s going to do what I want it to do immediately. Boom. Off I go.” (P17) 4.4 Music Recluse The primary characteristic of the Music Recluse is that he/she is a very private listener; this persona does not need to discuss his/her music listening habits with many people, and guards his/her privacy when using a music recommendation service. The music recluse actively avoids the social functions of music services like Spotify or Pandora and considers listening to be very personal. This persona may have sporadic listening habits, may lis-ten to music he/she is not proud of or would not want others to know about. Music recluses do not want people making assumptions about them based on the music they listen to. “I would allow zero information. I already think YouTube is too invasive. They’re already forcing users to create Google Plus accounts to comment on videos.” (P25) “I turned off sharing functionality. I made sure that I wasn’t putting it up on Facebook or sharing it...I definitely listen to a lot of embarrassing stuff and I don’t want every-body to know that. And I’m not really part of musical com-munities or anything, so I don’t feel like scrolling through my friends’ music gives me any useful information or songs to listen to.” (P34) 4.5 Non-believer The non-believer is a persona who does not believe that a machine can make adequate music recommendations for a variety of reasons: they do not understand how an algo-rithm can make “good” recommendations, they are able to see the limitations of recommendation algorithms, they prefer getting recommendations from friends, or they simply have not had good past experience with music recommendation services. Non-believers also have a ten-dency to dislike sharing personal information or listening histories with the service/system because they do not see the benefit of doing so. This persona often uses human-curated music services such as Songza or 8-Track, friends’ playlists, or their own collections, which may or may not be heavily curated. “Pandora will give me mainstream blues because it’s simi-lar rhythmically and in instrumentation, but that’s not the vibe I’m looking for. It seems like they go off of something really mechanical. They’re missing out on something and I don’t know what it would be called, like context, and how the music makes me feel.” (P23) 4.6 Wanderer The Wanderer primarily enjoys serendipitous music dis-covery, and listens to new music with an open mind (“…when it recommends me things that I never would have thought of, so I think, ‘yeah, I’ll give it a shot’.” (P11)). This persona enjoys the discovery process in general as a fun pastime, and is willing to put in some effort to dis-cover new music. The wanderer will likely accept rec-ommendations from a system as equally as she will ac-cept them from a friend, a blog, or a stranger. The wanderers tend to listen to music from a wider va-riety of music genres, although they may also have pre-ferred favorites. They enjoy discovering music/artists that are less popular and are willing to listen to new artists or genres. Wanderers may like recommendations based on “playful” themes such as “Monday morning” or “Coffee music.” They are more likely to use a variety of tools and also new features in the tools they regularly use. “Honestly, the serendipity of finding new music is what I enjoy the most. Generally if I’m listening to new music it will be because a friend recommended it or I came across it on YouTube through NPR Tiny Desk or something like that. I prefer that model...I listen to pretty diverse things.” (P13) 4.7 Addict The Addict exemplifies a known-item searcher and strongly utilizes a service that features search. This per-sona may listen to the same song multiple times in a row, or for a whole week (e.g., “I sort of fixate.” (P1)). This persona tends to use services like YouTube or Spotify where it is easy to repeat albums or songs. Their musical tastes may be all over the map, and they tend to listen to things on a whim, rather than curating any collections. They may listen sporadically, for short periods of time, and rely on easy access to music (web-based) from a va-riety of devices. The addict typically does not save his/her preferences by creating playlists for later access.  “I prefer Grooveshark…because I have a tendency to listen to a song, and then listen to it on repeat until I hate it for-ever, and Pandora doesn’t let you do that at all, whereas in Grooveshark you can do that.” (P23) 5. THEMES AND DESIGN IMPLICATIONS 5.1 Engagement, Ownership, and Specialization Our user data suggest that we may need to rethink the concept of “engagement” and how that affects peoples’ preferences for music services. If we consider engage-ment as users interacting with the system by exploring available features, then while it may be counter-intuitive, some users have no desire to engage with their preferred system. The way these users measure the success of the system is based on how little they have to interact with it. “As soon as I figured out the basics...as soon as I found that I could look at some friends’ playlists, and that I could find a few artists and make a radio station, I just, I was like, I’m done. I’m done learning how to make this work.” (P1) “There’s nothing I don’t like about Pandora...It might just be because I’m content enough...And I think I’m old enough, you know, I’m 45, I'm not into that 'music is my world' type of mentality. So it’s not high on my list.” (P17) A strong satisficing theme was identified among these users, consistent with the finding in [16]. As long as the system does what it is “supposed to do”, then it is “good enough” and users do not expect much more. This is es-pecially exhibited by participants representing the “guid-ed listener” persona, who tends to prefer music services like Pandora. The “addict” also tends to exhibit shallow engagement with the services. During the interview, it Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 479   became evident that most participants who can be catego-rized as guided listeners had never gone beyond the sur-face level of system. In fact, many participants discov-ered some of the features offered by their preferred ser-vice for the first time during the think-aloud sessions. They tend to have very specific needs and do not explore the service beyond their immediate needs. Personas such as active curator and music epicurean showed higher levels of engagement with the systems and seemed to have a stronger sense of ownership over their music collections. Active curators in particular would spend much time curating playlists even though they do not technically “own” the music. While guided listeners would most likely be satisfied with a streaming or sub-scription-based model, active curators and music epicure-ans hesitate to abandon the collection-based model. For this reason, we expect that cloud-based music services will appeal more to the latter group of users. For them, providing a way of creating their own access points into their collection will become an important issue, as the size of their collection will continue to grow. Organizing and accessing their collection by play frequency, name of the person who recommended a track, release date, or us-er in households where multiple members share the music service, were some examples that respondents specifical-ly mentioned as potentially useful.  In order to meet the needs of different personas, it may make sense to release different versions of the ser-vice/app so users can decide the appropriate version based on how much interaction they desire (“If [Spotify] had a light version then I would use that more. Like iTunes had a little mini-player, for example.” (P13)). Based on general observation, it does seem like speciali-zation works better than generalization; each service def-initely tends to attract particular types of user personas. For example, Pandora tends to attract users who do not want to spend time and effort curating music collections or listening experiences. On the other hand, Spotify users tend to invest more time in organizing their collections and providing input to improve their listening experience. Although users also rely on Spotify for music recommen-dations, they tend to be more critical about the results due to higher expectations. Websites like YouTube also serve a specific purpose, which is to stream videos, rather than attempting to work as some sort of Web portal that offers a variety of services. Many users, especially with need for known-item searches, will go to YouTube. Users’ strong desire to customize and personalize their music experi-ences was also noted in [17].  5.2 Awareness and Preserving User Trails Another theme emerged around a user’s general aware-ness within a system. Most users expressed a habit of “digging” and following “wormholes” while using mid- to high-level curation tools such as Spotify, Grooveshark, and YouTube. Many of these systems do a poor job of indicating the user’s location within the site, or helping them retrace their steps, which often results in users feel-ing the sense of “being lost.” “It’s constant digging. Click, click, scroll...wait, where am I? Click, scroll. For almost everything I want to do, I can never get there on the first try, or even if I get there on the first try, it feels like an accomplishment. Most of the time, I have an idea of where I am, but I don’t always know how to get back to where I was.” (P11) “I feel like I’m not as adventurous in wormholing sometimes as I can be or want to be, because I’m afraid of getting lost. If it were a little bit easier to just go back to where you started from or some sort of chain-of-command of what you had just done that you could click through (like a breadcrumb trail), then I probably would feel a little bit more comfortable.” (P3) This was also related to the general lack of error ex-planation in the systems, which would ideally help users recognize and prevent errors (“It just says, ‘There was an error.’ I almost never know what's going on when some-thing goes wrong.” (P11)). Users who discussed digging, wormholes, and the like, tended to be those who actively engaged with the service. This may span across any persona, but there appears to be a correlation between concern for user trails and engaged personas like the active curator and the music epicurean. Ideally the system should support the expression and preservation of a user trail and use breadcrumb trails to give users locational clues. Users also indicated that more transparency over rec-ommendations would improve their likelihood of trusting the system. Not knowing why the system wants them to listen to a particular song made them less inclined to fol-low the recommendation, especially for the active cura-tor, non-believer, and music epicurean personas.  “Sometimes I wonder why things are on there. I guess I need more insight on why I should choose to click on this thing...if it’s a band I’ve never heard of, I’m not going to click on it unless there’s a reason for me to...A lot of times it’s like, ‘You listened to this song by Rihanna once. All of a sudden we think you should listen to Justin Bieber.’ That doesn’t work for me.” (P31) 5.3 Privacy Concerns Several participants discussed privacy concerns around using music services. Our data suggest that the levels of privacy concerns are possibly affected by the following three factors: a) user’s interest in/belief of a machine’s ability to accurately recommend music, b) level of under-standing of privacy issues, and c) overall tech savviness. A user who has a higher interest in/belief of a machine’s ability, a better understanding of privacy issues, and is more tech savvy, tended to be more concerned about sharing their personal information. This trait was exhibit-ed across personas regardless of music listening habits, and most dominantly in non-believers. “When you download the software, the automatic preference is that Spotify will open every time you turn on your com-puter. I don’t like that. The first time I ever downloaded Spotify, that was the reason I didn’t use it [right away]. I felt like it was hijacking my computer.” (P1) “I wouldn’t want to give a system more information about me even if it would provide a perfect playlist, because I still 480 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   want to have control of that [information]…It’s creepy…I like having some degree of control and privacy.” (P13) “I'm split between 'that's really cool' and 'that's kind of creepy'. If I had the option to control it then that might be something I accept.” (P30) Being transparent about information collection and allow-ing more user control over privacy may help alleviate fears. This desire for control was also observed in [11], where us-ers wanted to be in control of logging what they considered as the most private information. They found that “users pre-fer sharing some information automatically such as listening history, sharing some information at will and keeping some information private (p. 171)” [11]. This aligns with concerns that arose during our interviews about privacy of infor-mation or activities. While listeners may be willing to share listening history, either discretely or publicly, those same users may be concerned about other information being shared without their knowledge. In addition to “what” is being shared, two other aspects worth noting are the different reaction to “who” is accessing users’ personal information and the directionality in sharing information. There seemed to be a distinction between keep-ing private information from the system versus from other people. Users exhibiting the music recluse persona, for in-stance, were much more concerned with the latter aspect. Music epicureans seemed interested in sharing their music listening history in a limited social circle (“I talk to about five people who like the same music as me. I just feel weird about posting videos on Facebook like ‘Listen to this’.” (P31)). Also a number of users acted like “lurkers” in that they wanted to see what other people listen to but did not want to share their own listening habits with others. During our work identifying the personas, we initially thought there may be a persona “Public broadcaster,” some-one who is very social and publicizes his or her listening choices. Careful examination of the transcripts, however, revealed that none of the users interviewed were “public broadcasters” themselves, but many made mention of that characteristic in friends or acquaintances who also use digi-tal music services. Most of the comments alluding to the ex-istence of this persona described how people have seen this kind of “broadcasting” behavior on social media (and were often annoyed by it). We believe that this persona may still exist, as previous research such as [11] found that their users were willing to share and seek shared information such as music listening habits, and some were already publicly do-ing so on websites such as Last.fm. Although users did want to keep some information private, music listening history was not such information. However, it may also be the case that we are simply seeing other’s music listening history be-cause of the default setting in some music services to public-ly share such information, and as previously discussed, many users do not spend much time trying to master their service’s feature settings. We plan to further explore this through a survey with a larger number of music service us-ers. 5.4 Context-switching  In addition to the different personas, the user’s context seemed critical in determining which services they use. “It really depends. If I’m upstairs in the office and coding data, I generally listen to music that I already know and like, because I don’t want it to take my focus away. If I am taking my dog for a walk or going for a drive, I may use the recommendations just to listen to new songs.” (P13) This resonates with previous MIR studies discussing how perceived qualities of music are affected by the con-text of the user [19], and how mood, activities, and social context among other factors influence music perception [25]. There were several aspects of user’s context that seemed particularly relevant: 1) Level of attention: This was often dependent on other activities in which users were concurrently engaged (e.g., driving or working). 2) Level of energy/motivation: This is closely related to users’ willingness to interact with the system. General-ly, tech savvy music listeners were more willing to do so, but depending on the time of the day, this also seemed to change (e.g., acting passively while fatigued after work).  3) Mood: The user’s mood constantly changes based on different events he/she is experiencing, and thus, the user may want to listen to songs with different “feels”. 4) Temporal aspect: This can be seasonal or about the time of day. Depending on work schedules, the early morning or evening may be the best time for users to in-teract with a system. Seasonality also means that users are engaged in different activities or in seasonal moods. User needs appear to continually shift depending on these contextual elements. A system allowing context-switching based on a combination of system logs of geo data, device usage, etc. (for attention level and temporal aspects) and users’ input (for level of energy/motivation and mood) would be desirable. 6. CONCLUSION AND FUTURE WORK In this paper, we present seven personas surrounding the use of commercial music information systems, derived from user interview data and observation of use sessions. These personas, each representing specific traits and atti-tudes of users, will be helpful in designing music infor-mation systems that are more highly tailored to specific user groups. Analyzing the user data made it clear that there is a relationship between persona placements on spectrums and types of services preferred. For instance, a user who is an active curator and music recluse would be more likely to use a “fringe” service such as Songza, whereas a guided listener user would likely end up rely-ing on an online radio service like Pandora. Based on us-ers’ opinions and observations of their interactions with the services, we discussed several design implications. In our future work, we plan to expand this study and test the applicability of these personas with a larger user population since they were derived from a relatively small sample. We will verify our results obtained from a qualitative approach by surveying a larger number of us-ers to identify appropriate personas reflecting their char-acteristics, using a stratified user sample based on their most preferred commercial music service.  Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 481   7. REFERENCES [1] L. Barrington, O. Reid, and G. Lanckriet: “Smarter than Genius? human evaluation of music recommender systems,” Proc. ISMIR, pp. 357-362, 2009. [2] M. Barthet and S. Dixon: “Ethnographic observations of musicologists at the British Library: implications for music information retrieval,” Proc. ISMIR, pp. 353-358, 2011. [3] D. Baur, J. Büttgen  and A. Butz: “Listening factors: a large-scale principal components analysis of long-term music listening histories,” Proc. CHI ’12, pp. 1273-1276, 2012. [4] P. Bonhard, C. Harries, J. McCarthy and M. A. Sasse: “Accounting for taste: using profile similarity to improve recommender systems,” Proc. CHI ’06, pp. 1057-1066, 2006. [5] Ò. Celma: “Music recommendation and discovery in the long tail,” Ph.D. dissertation, Dept. Information & Communication Tech., UPF, 2008. [6] L., Chen, W. Wu and L. He: “How personality influences users’ needs for recommendation diversity?” Proc. CHI EA ’13, pp. 829-834, 2013. [7] A. Cooper: The inmates are running the asylum. Sams, Indianapolis, 1999. [8] D. P. W. Ellis, B. Whitman, A. Berenzweig and S. Lawrence: “The quest for ground truth in musical artist similarity,” Proc. ISMIR, pp. 72-79, 2002. [9] C. Inskip, A. Macfarlane and P. Rafferty: “Music, movies and meaning: communication in film-makers’ search for pre-existing music, and the implications for music information retrieval,” Proc. ISMIR, pp. 477-482, 2008. [10] X. Hu and J. H. Lee: “A cross-cultural study of music mood perception between American and Chinese listeners,” Proc. ISMIR, pp. 535-540, 2012. [11] T. Kärkkäinen, T. Vaittinen and K. Väänänen-Vainio-Mattila: “I don't mind being logged, but want to remain in control: a field study of mobile activity and context logging,” Proc. CHI ’10, pp. 163-172, 2010. [12] K. Kosta, Y. Song, G. Fazekas and M. B. Sandler: “A study of cultural dependence of perceived mood in Greek music,” Proc. ISMIR, pp. 317-322, 2013. [13] P. Lamere. How good is Google’s Instant Mix? http://musicmachinery.com/2011/05/14/how-good-is-googles-instant-mix/ [14] A. Laplante and J. S. Downie: “The utilitarian and hedonic outcomes of music information seeking in everyday life,” Library and Information Science Research, Vol. 33, No. 3, pp. 202-210, 2011. [15] J. H. Lee and S. J. Cunningham: “Toward an understanding of the history and impact of user studies in music information retrieval,” Journal of Intelligent Information Systems, Vol. 41, pp. 499-521, 2013. [16] J. H. Lee and R. Price: “User experience with commercial music services: an empirical exploration,” JASIST, DOI: 10.1002/asi.23433, 2015.  [17] J. H. Lee and N. M. Waterman: “Understanding user requirements for music information services,” Proc. ISMIR, pp. 253-258, 2012. [18] T. W. Leong and P. C. Wright: “Revisiting social practices surrounding music,” Proc. CHI ’13, pp. 951-960, 2013. [19] M. Lesaffre, B. De Baets, H. De Meyer and J-P. Martens: “How potential users of music search and retrieval systems describe the semantic quality of music,” JASIST, Vol. 59, No. 5, pp. 695-707, 2008.   [20] J. Lingel: “When organization becomes an extension of your brain: DJs, music libraries and information practices,” Proc. iConference, pp. 366-376, 2013. [21] K. Liu and R. A. Reimer: “Social playlist: enabling touch points and enriching ongoing relationships through collaborative mobile music listening,” Proc. MobileHCI 2008, pp. 403-406, 2008. [22] T. Matthews, T. Judge and S. Whittaker: “How do designers and user experience professionals actually perceive and use personas?” Proc. CHI ’12, pp. 1219-1228, 2012.  [23] E. Nettamo, M. Nirhamo and J. Häkkilä: “A cross-cultural study of mobile music: retrieval, management and consumption,” Proc. OZCHI ’06, pp. 87-94, 2006. [24] L. Nielsen and K. S. Hansen: “Personas is applicable: a study on the use of personas in Denmark,” Proc. CHI ’14, pp. 1665-1674, 2014. [25] M. Schedl: “Ameliorating music recommendation,” Proc. MoMM’13, pp. 3-9, 2013. [26] M. Voong and R. Beale: “Music organisation using colour synaesthesia,” Proc. CHI EA ’07, pp. 1869-1874, 2007 [27] J. Woelfer and J. H. Lee: “The role of music in the lives of homeless young people: a preliminary report,” Proc. ISMIR, pp. 367-372, 2012. [28] I. Young: Mental models: aligning design strategy with human behavior. Rosenfeld Media, 2007. 482 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Monaural Blind Source Separation in the Context of Vocal Detection.",
        "author": [
            "Bernhard Lehner",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415940",
        "url": "https://doi.org/10.5281/zenodo.1415940",
        "ee": "https://zenodo.org/records/1415940/files/LehnerW15.pdf",
        "abstract": "In this paper, we evaluate the usefulness of several monau- ral blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the re- sults of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source sep- aration algorithms. In order to assess the performance of those strategies on an internal data set, we use two differ- ent feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post- processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of tra- ditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals.",
        "zenodo_id": 1415940,
        "dblp_key": "conf/ismir/LehnerW15",
        "keywords": [
            "monaural",
            "blind source separation",
            "vocal detection",
            "state-of-the-art",
            "source separation algorithms",
            "vocal detection",
            "feature sets",
            "classifiers",
            "post-processing technique",
            "source signals"
        ],
        "content": "MONAURAL BLIND SOURCE SEPARATION IN THE CONTEXT OFVOCAL DETECTIONBernhard Lehner, Gerhard WidmerDepartment of Computational PerceptionJohannes Kepler University of Linz{bernhard.lehner,gerhard.widmer}@jku.atABSTRACTIn this paper, we evaluate the usefulness of several monau-ral blind source separation (BSS) algorithms in the contextof vocal detection (VD). BSS is the problem of recoveringseveral sources, given only a mixture. VD is the problem ofautomatically identifying the parts in a mixed audio signal,where at least one person is singing. We compare the re-sults of three different strategies for utilising the estimatedsinging voice signals from four state-of-the-art source sep-aration algorithms. In order to assess the performance ofthose strategies on an internal data set, we use two differ-ent feature sets, each fed to two different classiﬁers. Afterselecting the most promising approach, the results on twopublicly available data sets are presented. In an additionalexperiment, we use the improved VD for a simple post-processing technique: For the ﬁnal estimation of the sourcesignals, we decide to use either silence, or the mixed, or theseparated signals, according to the VD. The results of tra-ditionally used BSS evaluation methods suggest that this isuseful for both the estimated background signals, as wellas for the estimated vocals.1. INTRODUCTIONMonaural Blind Source Separation (BSS) is a technique forthe separation of at least two components from a single-channel signal without using additional information, likethe instrumentation or the notation of a musical piece. It isextremely challenging, since we have to deal with the fact,that less mixtures than sources are at hand.The result of BSS could be useful for many tasks likeremixing, creating karaoke songs, manipulate isolated in-struments, and so on. Certain Music Information Re-trieval (MIR) tasks could also beneﬁt from a BSS as a pre-processing step, e.g. vocalist similarity, pitch detection, au-tomatic transcription, keyword spotting, . . .Unfortunately, it is hard to estimate the usefulness ofa certain BSS algorithm for a speciﬁc task beforehand.Metrics usually used for evaluating BSS (see Section 4.1)c\u0000Bernhard Lehner, Gerhard Widmer.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Bernhard Lehner, Gerhard Widmer.“Monaural Blind Source Separation in the context of V ocal Detection”,16th International Society for Music Information Retrieval Conference,2015.are certainly useful for comparison purposes, but haveonly limited meaningfulness when it comes to the ultimatequestion if BSS is actually useful for a speciﬁc task.To give an example, Schuller et al. had no success withachieving better results for tempo and key detection by util-ising drum beat separation in [20], despite the fact that theaudible results seemed good enough to be used for musicremixing. On the other hand, Weninger et al. achieved asigniﬁcant performance gain in the 3-class task of detect-ing singing voice segments and simultaneously recognis-ing the vocalist gender in [21].Therefore, we evaluate the usefulness of several state-of-the-art BSS algorithms in the context of vocal detec-tion (VD), also referred to as singing voice detection. Forthis task, usually several features are extracted frame-wisefrom the audio signal and fed to a classiﬁer [9, 13, 14, 19,21,22], or even to a speech-recogniser [1] in order to obtainthe vocal/non-vocal decision. Given this use case, we aremainly interested in separating the signal into two sources:vocals and background. In order to ﬁnd the best usage ofthe BSS results, we discuss the outcome of three differentstrategies.Furthermore, we investigate if the quality of the sep-arated sources can be improved by using VD as a post-processing technique: For the estimated vocals we mutethe parts which are not classiﬁed as such by our VD to re-duce non-voiced artifacts. For the estimated background,we replace the parts which are classiﬁed as non-vocal byour VD with the original mixed audio signal.2. SELECTED BSS ALGORITHMSWe selected four state-of-the-art BSS algorithms, all ofthem were already used to extract the singing voice froma mixed audio signal, and reference implementations areprovided by the authors. Due to limited space, we can dis-cuss the methods only brieﬂy, and refer to the original pa-pers.The adaptive REpeating Pattern Extraction Technique(aREPET) is a method, where repeating patterns (back-ground) are identiﬁed and used to separate non-repeating(foreground) elements. Those elements are often the vary-ing vocals, and it was shown in [18], that this techniquecan be used for Music/V oice Separation. There are sev-eral variants of the REPET algorithm [11,16–18], whereasaccording to the results from Liutkus et al. in [12] the309aREPET yielded the best\u0000SDRfor vocals out of three vari-ants. Therefore, we consider the aREPET the most promis-ing variant and choose this for our comparison.The FASST toolbox by Ozerov et al. [15] allows tospecify prior information and implement arbitrary sepa-ration problems. Therefore, it is not merely a method,but more a general framework. However, a baseline im-plementation is included in the toolbox, which separatesa song into the four sources drums, bass, main melody,and the rest. It comes with pre-trained models for severalsources, incl. singing voice, which is in our case used toextract the main melody source.The Kernel Additive Modelling (KAM) approach [10,12] uses source-dependent proximity kernels to describelocal dynamics like periodicity (similar to REPET),smoothness, stability over time or frequency, and more.The different sources are then separated by an algorithmcalled iterative kernel backﬁtting.Huang et al. use in [8] Robust Principal ComponentAnalysis (RPCA) for the separation of singing voice. Theirbasic assumptions are, that singing voice is relativelysparse within songs, and accompaniment is in a low-ranksubspace due to its repetitive structure. Their method usessolely the spectrogram as input, and neither training norparticular features are required.Another interesting approach, which we didn’t includein our experiments (because of the results reported in [12]),is suggested by Durrieu et al. in [3], where a source-ﬁltermodel is used for the vocals, and non-negative matrix fac-torisation (NMF) for the background.3. EXPERIMENTSIn this Section, we discuss the outcome of three differentstrategies to utilise the results of the selected BSS algo-rithms.3.1 Internal Data SetFor the ﬁrst experiments, we use a set of 149 annotatedrock songs by 149 different artists. All songs are recordedat a sampling rate of 22 kHz with 16-bit resolution andconverted to mono. Background and vocal tracks are sepa-rately available to allow for a more complex evaluation ofthe results. Approximately 52% of the frames are anno-tated as vocal, and the amount of pure singing, i.e. with-out instrumental accompaniment, is negligible. This set issplit into a 75 song train set, and a 74 song test set, ap-proximately 5h each. It is challenging for BSS algorithms,because it contains lots of guitar soli, where singing voicecharacteristics are mimicked.3.2 Feature Sets and ClassiﬁerFor the following experiments we choose the features from[9], which we refer to asIC14for the remainder of this pa-per. The IC14 feature vector comprises 116 attributes intotal. This method was already compared to several oth-ers in [9], and turned out to deliver the best VD results inalmost every testing scenario.For new insights, we compare this feature set to theone used by Weninger et al. in [22], henceforth referredto asOS11. This feature vector comprises 46 attributes. Itwas used along with a BLSTM-RNN classiﬁer to achievestate-of-the-art performance for several singing voice re-lated classiﬁcation tasks, among them gender recognitionand VD.In our implementation, both feature sets are extractedwith a ﬁxed frequency of ﬁve observations per second (200ms frames). Therefore, the units of audio to be classiﬁedare 200 ms frames. In the original implementation of OS11in [22], the features were extracted beat-wise, hence usinga variable framesize. As classiﬁer, we choose the RandomForest (RF) as well as the Support Vector Machine (SVM)implementations of the Weka toolkit [7]. To be able tofocus on the performance of feature set and classiﬁer, nopost-processing is applied.3.3 Foreground Separation EvaluationIn this Section we present the results of the ﬁrst strategy,were we extract the features (IC14 from [9], OS11 from[22]) from just the separated foreground audio signals.The results are presented in Table 1, where we ﬁrst seethe performance of a model trained from the original audio,and tested with the original audio (row MIX). To simulatea perfect BSS, we additionally extract the features from thereal vocal track (containing only vocals and silence) of thesong, and test with the same model as before (row VOC).Clearly, the results improve by just using pure vocals as testdata, e.g. from 83.7% to 91.6% accuracy for the IC14 fea-ture set and the Random Forest (col. RF-accuracy-IC14).For the upcoming results, we use the placeholderMETHOD to refer to the four BSS algorithms{aREPET,FASST, KAM, RPCA}in general. For METHODmixclas-siﬁcation, we always use the model trained from the mixedaudio signals. For METHODsepclassiﬁcation, we use themodel trained from the separated vocal signals to incorpo-rate BSS characteristics.The test data presented to the classiﬁer is extracted fromthe separated vocals. As can be seen in Table 1, con-sistently and regardless of the feature set and classiﬁer,both accuracy and F-measure improve when the model istrained with the separated vocals instead of the mixed au-dio data.Nevertheless, there is quite some room for improve-ment, since all methods show a substantial performancedecrease relative to testing with pure vocals (row VOC).Compared to the results of training and testing withthe mixed audio data (row MIX), only the aREPETsepandRPCAsepmethods, where both training and testing is donewith the separated foreground, yields slightly better results(e.g. for RF-accuracy-IC14: 83.7% vs. 84.1% and 84.5%respectively).Interestingly, the feature set from [22] in combinationwith the SVM (col. SVM-accuracy-OS11) is only in thepure vocals scenario (row VOC) superior to the feature setfrom [9] (col. SVM-accuracy-IC14) (94.9% vs. 93.9% ac-curacy). It seems that the feature set from [22] is quite ca-310 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Internal Data Set (framesize=200ms)RFSVMaccuracyF-measureaccuracyF-measureIC14 OS11IC14 OS11IC14 OS11IC14 OS11MIX.837 .795.846 .814.855 .807.863 .819VOC.916 .910.920 .905.939 .949.943 .951aREPETmix.768 .756.800 .781.783 .742.797 .789aREPETsep.841 .796.850 .810.861 .811.866 .822FASSTmix.732 .670.682 .603.751 .711.756 .686FASSTsep.826 .778.835 .795.845 .791.854 .803KAMmix.752 .736.773 .738.631 .577.728 .709KAMsep.826 .786.835 .798.849 .805.855 .815RPCAmix.752 .691.788 .763.620 .563.704 .703RPCAsep.845 .797.851 .809.861 .820.867 .828Table 1. Results of Foreground Separation. F-measure re-lates to the classvocal. MIX: trained and tested with mixedaudio; VOC: trained with mixed audio, tested with pure vo-cals. METHODmix: trained with mixed audio, tested withseparated vocals; METHODsep: trained and tested withseparated vocals. The columns IC14 and OS11 refer tothe feature sets used in [9] and [22].pable to model singing voice, but less robust to backgroundnoise.Generally, comparing the performance of the classiﬁers,SVM delivers better results than the Random Forest. Re-garding the feature set, IC14 seems to be the better choice.This can also be observed in the following experiments.3.4 Foreground Concatenation EvaluationHere, we concatenate the features extracted from the mixto the features extracted from the separated foreground intoa single feature vector, hence doubling its size.In Table 2 we can see that this strategy leads to bet-ter results regardless of BSS method, classiﬁer and featureset. In order to assess the upper bound of this strategy,we include the results when using the real vocals also (rowMIX+VOC), simulating perfect separation. Similar to Sec-tion 3.3, the results from utilising RPCA are the best, eventhough the absolute differences between the BSS methodsare within 1 percentage point (ppt).Compared to the previous strategy (see Section 3.3), thecomputational effort is much higher. This is especially truefor training the SVM, due to the increased size of the fea-ture vector. Therefore, we evaluate another strategy in thenext Section, where the size of the feature vector stays thesame instead of being doubled.3.5 Foreground Enhancement EvaluationIn this Section we present the results of the third strategy toimprove VD. In order to enhance the vocals (i.e. increasethe SNR), we remix the separated foreground with the orig-inal signal. The mixes were made with different levels ofthe separated track, ranging from\u00006dB to6dB in3dBsteps. The results from0dB indicate, that the remix wasdone without any gain changes.Training as well as testing was done by using the fea-tures extracted from the remixed signals. Again, we in-clude the results when using the real vocals also. In Ta-ble 3 we can see that different gain changes for remixingdo not make a big difference for the results, regardless ofInternal Data Set (framesize=200ms)RFSVMaccuracyF-measureaccuracyF-measureIC14 OS11IC14 OS11IC14 OS11IC14 OS11MIX.837 .795.846 .814.855 .807.863 .819MIX+VOC.960 .985.962 .986.976 .984.977 .985MIX+aREPET.845 .800.853 .817.865 .825.872 .834MIX+FASST.842 .798.850 .816.863 .825.871 .835MIX+KAM.844 .800.853 .815.871 .830.877 .839MIX+RPCA.850 .806.858 .822.870 .833.877 .841Table 2. Results of Foreground Concatenation. MIX:trained and tested with mixed audio. For training and test-ing of the methods aREPET, FASST, KAM, and RPCA,the classiﬁer is given a double-sized vector containing thefeatures from the mixed and the separated audio signal.MIX+VOC: concatenating features from the real vocals tosimulate perfect separation.the BSS method, classiﬁer and feature set, except whenusing the real vocals (rows VOC). However, only the fea-ture set from [9] allows for results at least as good as forthe previous experiment in Section 3.4. Since those resultsare achieved without the additional computational burdendue to a two-fold feature extraction, and the increased sizeof the feature vector, the enhancing-by-remixing strategyseems to be the best choice.Again, RPCA based results are slightly better comparedto the other source separation methods.3.6 Final MethodConsidering the results from the previous experiments,we choose the following setting for the upcoming ex-periments: For source separation, we choose the RPCAmethod, and we use the result to enhance the singing voiceby remixing it with the original signal with an increasedgain of 6 dB. The 116-attribute feature set IC14 as sug-gested in [9] is used, and fed to a SVM classiﬁer with aradial basis function (RBF) kernel (C=2,\u0000=0.35). Theremixed audios are used both for training and testing.A very simple post-processing, where we use a medianﬁlter (order=5) for majority voting is also applied, whichimproves the results slightly from 87.3% to 87.8% accu-racy.3.7 Results on Public Data SetsIn this Section we compare the results of our suggestedmethod as described in Section 3.6 to previously publishedresults.3.7.1 JamendoIn [19], the authors presented results on a precisely deﬁnedsplit of the Jamendo data set, where the training set com-prises 61 songs, and validation and test sets comprise 16songs each. This allows for a fair comparison.Table 4 lists the results reported by Lehner et al. in [9],compared with our new method. While the untouched out-put of the classiﬁer (col. NEW) is on par with the (post-processed) baseline (col. LEH), the simple post-processingProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 311Internal Data Set (framesize=200ms)RFSVMaccuracyF-measureaccuracyF-measureIC14 OS11IC14 OS11IC14 OS11IC14 OS11MIX.837 .795.846 .814.855 .807.863 .819VOC\u00006dB.880 .861.886 .868.907 .869.911 .874VOC\u00003dB.895 .883.900 .888.922 .888.925 .892VOC0dB.909 .905.914 .907.937 .907.939 .911VOC3dB.923 .926.926 .927.949 .927.951 .929VOC6dB.937 .943.940 .944.960 .945.961 .946aREPET\u00006dB.844 .792.852 .809.862 .807.869 .818aREPET\u00003dB.843 .792.852 .809.863 .807.871 .818aREPET0dB.845 .794.853 .811.865 .809.872 .820aREPET3dB.845 .795.854 .811.866 .812.873 .822aREPET6dB.845 .799.854 .813.867 .813.874 .823FASST\u00006dB.844 .795.852 .812.861 .805.868 .817FASST\u00003dB.842 .796.851 .813.862 .807.870 .819FASST0dB.844 .799.852 .816.864 .809.871 .820FASST3dB.843 .799.851 .816.865 .811.872 .822FASST6dB.844 .799.852 .815.864 .811.871 .822KAM\u00006dB.845 .801.854 .816.866 .815.873 .825KAM\u00003dB.846 .803.855 .817.868 .818.874 .827KAM0dB.847 .804.855 .817.868 .819.874 .828KAM3dB.846 .803.855 .816.870 .820.875 .829KAM6dB.845 .803.854 .816.870 .821.876 .829RPCA\u00006dB.847 .803.855 .817.868 .817.874 .826RPCA\u00003dB.848 .805.856 .819.870 .818.876 .827RPCA0dB.851 .806.858 .819.871 .819.877 .828RPCA3dB.850 .807.858 .819.872 .820.877 .829RPCA6dB.850 .809.858 .821.873 .821.878 .829Table 3. Results of Foreground Enhancement. MIX:trained and tested with mixed audio. For training and test-ing of the methods aREPET, FASST, KAM, and RPCA,the classiﬁer is given the features extracted from a signal,where the separated vocals are remixed with the originalaudio signal. VOC: using the real vocals instead of theseparated.LEHNEWNEW+accuracy.882.882.896recall.862.873.892precision.880.872.884F-measure.871.873.888Table 4. Jamendo corpus results. LEH: results reportedin [9]. NEW: our new classiﬁer (SVM) with RPCA basedvocal enhancement. NEW+: incl. post-processing withmedian ﬁlter.(see Section 3.6) helps to reach better results, with an ac-curacy of 89.6% (col. NEW+).3.7.2 RWCIn [13], Mauch et al. report 87.2% accuracy with a 5-foldcross validation (CV) on a 102 song data set that is com-posed of 90 songs from the RWC music database [5], and12 additional songs. Since we had just access to the 100RWC songs, our results are only comparable to a certainextent. Therefore, we also include the (post-processed) re-sults reported from Lehner et al. in [9] (col. LEH), wherewe could use exactly the same splits for the 5-fold CV .In Table 5 we can see an improvement of 2.3 ppt accu-racy by comparing LEH and NEW (87.5% vs. 89.8%), de-spite the lack of any post-processing. The post-processing(col. NEW+) did not improve the accuracy on this data set.However, the increased recall (0.928 vs. 0.939) could stillbe desired for certain use cases, even when it comes withreduced precision (0.905 vs. 0.898).MODEMAUCHMODELEHNEWNEW+accuracy.654.872.604.875.898.898recall1.00.9211.00.926.928.939precision.654.887.604.875.905.898F-measure.791.904.753.900.917.918Table 5. Results on the RWC data set. MAUCH: results re-ported in [13]. NEW: our new classiﬁer (SVM) with RPCAbased vocal enhancement. NEW+: incl. post-processingwith median ﬁlter. LEH and NEW were trained on the 100RWC songs, MAUCH on 90 RWC + 12 additional (un-known) songs. MODE: baseline achievable by always pre-dicting the majority class (vocals); MODE of classiﬁcationaccuracy thus tells the percentage of vocals in the data set.4. IMPROVING BACKGROUND AND VOCALESTIMATESIn this Section, we discuss the results of BSS algorithmsin more detail regarding the amount of non-vocal artifactsin the estimated vocals, and vocal artifacts in the estimatedbackground.All of the four presented BSS methods have one char-acteristic in common: they do not incorporate VD results.In [18], the authors even state that their REPET methoddoes not require any explicit handling of singing voice seg-ments. Although, by listening to the results of all presentedBSS algorithms in this paper, we believe there is neverthe-less room for improvement. Our internal data set containsa lot of instrumental soli, played by a guitarist. Consider-ing the basic principle of e.g. the REPET method, it comesas no surprise that the estimates of the vocals have pas-sages containing those solo instruments only, and no vocalswhatsoever. This is especially troublesome for use caseslike artist recognition. On the other hand, the estimates ofthe instrumental background often contain artifacts fromthe singing voice. This is problematic for tasks like auto-matic karaoke track creation.In [2, 16], the vocal frames were already successfullyused to improve the results of the source separation, butaccording to the annotated ground truth, and not to an au-tomatic classiﬁcation. Therefore, we investigate the im-pact of VD on the results of the BSS with respect to met-rics traditionally used to evaluate BSS algorithms. Eventhough we consider only RPCA henceforth, the remainingthree BSS methods show a very similar characteristic inthat matter. Concerning the VD, we use the one improvedby RPCA as described in Section 3.6.We suggest a simple post-processing strategy to im-prove the estimates: Regarding the estimated vocals, wesimply ﬁlter out (i.e. mute) the non-vocal frames. In otherwords, for the ﬁnal estimates of the separated vocals, wedecide whether to use the vocal estimates from RPCA orsilence – according to our VD.Figure 1 illustrates this principle, where we can see inthe upper plot a time signal of vocals (dark) embedded inthe mixture (bright). The lower plot shows the estimated312 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1. Example of RPCA separated singing voice. Inthe upper subplot we can see the mixed signal (bright) andthe embedded singing voice (dark). In the lower subplotwe can see the result from RPCA (bright) and the same re-sult combined with singing voice detection (dark). Clearly,the latter approach is closer to the true singing voice.vocals (bright) from the RPCA method, and the vocals af-ter the VD based post-processing (dark). Obviously, theamount of non-vocal artifacts in the vocal estimate is re-duced by applying this simple post-processing.The same principle is applied in order to improve theestimates of the background. Here, we decide for the ﬁnalestimates, whether to use the separated background or theoriginal mix. This means, the separated background is onlychosen, where the VD classiﬁes the audio signal as vocal.Nevertheless, it is not certain, if metrics traditionallyused to evaluate BSS algorithms also reﬂect any improve-ment. A recall of vocals below an unknown – depending onthe current situation – threshold would cause too much ofthe vocal estimates to be muted. At the same time, the esti-mated background would suffer from too much presence ofvocals, since we would often wrongly opt for the originalmixture instead of the separated background. Therefore, athorough evaluation of the aforementioned post-processingis necessary in order to shed light on how useful it actuallyis.4.1 Evaluation MetricsIn order to get meaningful evaluation results, we use themeasurements proposed by Gribonval et al. in [6], wherethe overall estimation error is decomposed intotarget dis-tortion,interference, andartifacts. Based on this compo-nents, the following energy ratios are deﬁned: source Im-age to Spatial distortion Ratio (ISR), Source to InterferenceRatio (SIR), and Source to Artifacts Ratio (SAR). Sourceto Distortion Ratio (SDR) is based on the three aforemen-tioned measures, and serves as a global measure of distor-tion. For all metrics applies, that higher values indicatebetter performance.Additionally, a set of measures that was proposed byEmiya et al. in [4] is used. Compared to the previouslypresented set, they better correlate with the perceived audioquality judged by human listeners. The overall distortionis also decomposed into the same three components, andbased on them, the following measures are deﬁned: Target-related Perceptual Score (TPS), Interference-related Per-ceptual Score (IPS), and Artifacts-related Perceptual Score(APS). The Overall Perceptual Score (OPS) is based on thethree aforementioned scores, and serves as a global mea-sure of perceived audio quality. Similar to the aforemen-tioned metrics, higher values indicate better performance.All measures were extracted with the PEASS toolkit [4].4.2 Evaluation ResultsIn this Section, we present box plots of the evaluationresults on our internal data set (see Section 3.1) regard-ing the VD based post-processing method, which we de-scribed in Section 4. Audio examples are available athttp://www.cp.jku.at/misc/ismir2015bss.4.2.1 BackgroundIn Figure 2, we can see the evaluation results of the back-ground, separated with RPCA. For each metric, we can seethree results: raw RPCA output (A), RPCA output post-processed with VD (B), and RPCA output post-processedwith ground-truth annotations (C). By adding the resultsfrom a ground-truth based post-processing, we assess thepotential beneﬁt of the suggested post-processing method,and how far away we are from this optimum.Compared to the raw RPCA outputsA, the post-processed resultsBandCimprove for all metrics, exceptIPS. The median of the global measure of distortion (SDR)improves by 2 dB for post-processingB, and 1.9 dB forpost-processingC(A: 1.3 dB,B: 3.3 dB,C: 3.2 dB). Thissuggests, that our VD performs on par with using ground-truth.The median of the global measure of perceived audioquality (OPS) improves by 6.5 points for post-processingB, and 8.3 points for post-processingC(A: 26.5,B: 33.0,C: 34.8). Even though the median OPS is approximatelythe same for post-processingBandC, we can see still roomfor improvement, since the distribution of the ground-truthbased resultsChas a tendency towards higher values.Interestingly, compared to the raw RPCA outputA, themedian of the IPS results drops for both post-processingmethods. For the VD based resultsB, we assume, this isdue to some missed vocals, where the original mix is cho-sen instead of the separated background. This causes thevocals to be moved back into the ﬁnal background estima-tion, and deteriorates the result. For the ground-truth basedresultsCwe assume, this is due to the fact, that the vocaltrack that we use for evaluation, contains not complete si-lence, but rather some noise. But our ﬁnal estimation ofthe vocals replaces non-vocal segments with silence.Based on the results, we consider it useful to incorpo-rate VD in order to yield better estimations of the back-ground. This could be especially useful for generatingkaraoke tracks, where for the non-vocal segments the orig-inal mixture can be used, without any loss in quality due toBSS characteristics. Obviously, for songs with high vocalcontent, the impact will be rather small.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 313Figure 2. RPCA background estimation evaluation results.A: raw RPCA output; B: VD post-processed output; C:post-processed using ground truth. Higher values indicatebetter performance. In general, the performance increasesfor all metrics, except IPS. We assume, this is due to somemissed vocals from our VD, where the original mix (incl.vocals) is chosen instead of the separated background.4.2.2 VocalsIn Figure 3, we can see the evaluation results of the vo-cals, separated with RPCA. Compared to the raw RPCAoutputsA, the median of SDR indicate better performancefor the post-processed outputB(-7.2 dB vs. -4.9 dB), andno improvement comparing post-processingBtoC.The impact of silencing all non-vocal segments for theﬁnal vocal estimates can be seen in the interference re-lated SIR (A: -2.0 dB,B: 0.2 dB,C: 0.6 dB). The per-ceptually motivated IPS reveals this relationship even bet-ter, where we can see an improvement of 11.2 points forpost-processingBand 12.5 points for post-processingC(A: 41.2,B: 52.4,C: 53.7).The median of the OPS improves by 8.3 points for post-processingB, and 9.7 points for post-processingC(A:10.9,B: 19.2,C: 20.6).Similar to the background estimates, the results ofthe metrics indicate improvement, when VD based post-processing is applied. Especially for tasks like artist recog-nition it could be useful to only use the parts which areclassiﬁed as vocals, even when some are missed by the VD.5. CONCLUSION AND OUTLOOKIn this paper we ﬁrst presented the outcome of three strate-gies of utilising different monaural BSS techniques to im-prove VD: foreground separation, foreground concatena-tion, and foreground enhancement. According to the re-sults on an internal data set, foreground enhancement isthe best strategy. The difference of the usefulness betweenthe four techniques aREPET, FASST, KAM, and RPCA isrelatively small, and the latter usually performs best. Wecompared the results achieved with the best approach onpublicly available data sets, and could show an improve-ment of 2.3 ppt relative to the baseline, reaching an accu-racy of 89.8% on the RWC data set. Compared to the same\nFigure 3. RPCA vocal estimation evaluation results.A: raw RPCA output; B: VD post-processed output; C:post-processed using ground truth. The global measuresSDR and OPS indicate better performance for the post-processed output. The higher performance regarding in-terferences SIR and IPS are caused by the parts, that aremuted, when our VD classiﬁes them as non-vocal.baseline, the results on the Jamendo data set have also im-proved by 1.4 ppt, with an accuracy of 89.6%. However,approximately half of the improvement is due to using aSVM instead of a Random Forest. Depending on the usecase, the effort of employing a BSS might therefore notalways be justiﬁed. Nevertheless, by adding the results ob-tained by using the real vocals, we could show that VDwould principally beneﬁt from better separation results.Our second contribution addressed the issue, that allof the four separation techniques produce vocal estimates,where many segments contain only instrumental back-ground, and no singing voice at all. We suggested to usethe results of the VD to simply mute the non-singing parts.Regarding the vocal estimates, we could see an improve-ment of 2.3 dB SDR when applying this post-processing(-7.2 dB vs. -4.9 dB).For the ﬁnal background estimates, we suggested to usethe original mixed audio signal, where the VD classiﬁesthe signal as non-vocal. Regarding the background esti-mates, we could see an improvement of 2.0 dB SDR whenapplying this post-processing (1.3 dB vs. 3.3 dB).We think it is safe to conclude that VD based post-processing improves the results of BSS vocal and back-ground estimates, although not by much regarding tradi-tional evaluation metrics. However, in the context of vocal-ist recognition, it could be helpful to only use the classiﬁedvocal parts, especially when solo instruments like guitarscause the BSS algorithm to produce lots of non-vocal arti-facts in the vocal estimates. As one of the next steps, weplan to investigate the usefulness of our approach in thistopic.6. ACKNOWLEDGEMENTSThis research is supported by the Austrian Science Fund(FWF) under grants TRP307-N23 and Z159.314 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]A. L. Berenzweig and D. P. W. Ellis. Locating singing voicesegments within music signals. InWorkshop on the Appli-cations of Signal Processing to Audio and Acoustics, pages119–122. IEEE, 2001.[2]T-S Chan, T-C Yeh, Z-C Fan, H-W Chen, L. Su, Y-H Yang,and R. Jang. V ocal activity informed singing voice separa-tion with the ikala dataset. InProceedings of the Interna-tional Conference on Acoustics, Speech, and Signal Process-ing (ICASSP 2014). IEEE, 2014.[3]J-L Durrieu, B. David, and G. Richard. A musically moti-vated mid-level representation for pitch estimation and mu-sical audio source separation.Journal of Selected Topics inSignal Processing, 5(6):1180–1191, 2011.[4]V . Emiya, E. Vincent, N. Harlander, and V . Hohmann. Sub-jective and objective quality assessment of audio source sep-aration.Transactions on Audio, Speech, and Language Pro-cessing, 19(7):2046–2057, 2011.[5]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWCmusic database: Popular, classical, and jazz music databases.InProceedings of the 3rd International Conference on MusicInformation Retrieval (ISMIR 2002), volume 2, pages 287–288, 2002.[6]R. Gribonval, L. Benaroya, E. Vincent, and C. F´evotte. Pro-posals for performance measurement in source separation.In4th International Symposium on Independent ComponentAnalysis and Blind Signal Separation (ICA2003), pages 763–768, 2003.[7]M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reutemann,and I. H. Witten. The WEKA data mining software: an up-date.ACM SIGKDD Explorations Newsletter, 11(1):10–18,2009.[8]P-S Huang, S. D. Chen, P. Smaragdis, and M. Hasegawa-Johnson. Singing-voice separation from monaural recordingsusing robust principal component analysis. InProceedings ofthe International Conference on Acoustics, Speech, and Sig-nal Processing (ICASSP 2012), pages 57–60. IEEE, 2012.[9]B. Lehner, G. Widmer, and R. Sonnleitner. On the reductionof false positives in singing voice detection. InProceedingsof the International Conference on Acoustics, Speech, andSignal Processing (ICASSP 2014), pages 7530–7534. IEEE,2014.[10]A. Liutkus, D. Fitzgerald, Z. Raﬁi, B. Pardo, and L. Daudet.Kernel additive models for source separation.Transactionson Signal Processing, 62(16):4298–4310, 2014.[11]A. Liutkus, Z. Raﬁi, R. Badeau, B. Pardo, and G. Richard.Adaptive ﬁltering for music/voice separation exploiting therepeating musical structure. InProceedings of the Interna-tional Conference on Acoustics, Speech, and Signal Process-ing (ICASSP 2012), pages 53–56. IEEE, 2012.[12]A. Liutkus, Z. Raﬁi, B. Pardo, D. Fitzgerald, L. Daudet, et al.Kernel spectrogram models for source separation. In4th JointWorkshop on Hands-free Speech Communication and Micro-phone Arrays (HSCMA), pages 6–10. IEEE, 2014.[13]M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Timbre andMelody Features for the Recognition of V ocal Activity andInstrumental Solos in Polyphonic Music. InProceedings ofthe 12th International Conference on Music Information Re-trieval (ISMIR 2011), pages 233–238, 2011.[14]T. L. Nwe, A. Shenoy, and Y . Wang. Singing voice detectionin popular music. InProceedings of the 12th annual ACM in-ternational conference on Multimedia, pages 324–327. ACM,2004.[15]A. Ozerov, E. Vincent, and F. Bimbot. A general ﬂexibleframework for the handling of prior information in audiosource separation.Transactions on Audio, Speech, and Lan-guage Processing, 20(4):1118–1133, 2012.[16]Z. Raﬁi and B. Pardo. A simple music/voice separationmethod based on the extraction of the repeating musicalstructure. InProceedings of the International Conference onAcoustics, Speech, and Signal Processing (ICASSP 2011),pages 221–224. IEEE, 2011.[17]Z. Raﬁi and B. Pardo. Music/voice separation using the sim-ilarity matrix. InProceedings of the 13th International Con-ference on Music Information Retrieval (ISMIR 2012), pages583–588, 2012.[18]Z. Raﬁi and B. Pardo. Repeating pattern extraction tech-nique (REPET): A simple method for music/voice separation.Transactions on Audio, Speech, and Language Processing,21(1):73–84, 2013.[19]M. Ramona, G. Richard, and B. David. V ocal detection inmusic with support vector machines. InProceedings of theInternational Conference on Acoustics, Speech, and SignalProcessing (ICASSP 2008), pages 1885–1888. IEEE, 2008.[20]B. Schuller, A. Lehmann, F. Weninger, F. Eyben, andG. Rigoll. Blind enhancement of the rhythmic and harmonicsections by nmf: Does it help. InProceedings of the Inter-national Conference on Acoustics (NAG/DAGA 2009), pages361–364, 2009.[21]F. Weninger, J-L Durrieu, F. Eyben, G. Richard, andB. Schuller. Combining monaural source separation with longshort-term memory for increased robustness in vocalist gen-der recognition. InProceedings of the International Confer-ence on Acoustics, Speech, and Signal Processing (ICASSP2011), pages 2196–2199. IEEE, 2011.[22]F. Weninger, M. W¨ollmer, and B. Schuller. Automatic assess-ment of singer traits in popular music: Gender, age, heightand race. InProceedings of the 12th International Conferenceon Music Information Retrieval (ISMIR 2011), pages 37–42,2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 315"
    },
    {
        "title": "Score Following for Piano Performances with Sustain-Pedal Effects.",
        "author": [
            "Bochen Li",
            "Zhiyao Duan"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418127",
        "url": "https://doi.org/10.5281/zenodo.1418127",
        "ee": "https://zenodo.org/records/1418127/files/LiD15.pdf",
        "abstract": "One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the us- age of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration nat- urally ceases. This makes the notes longer than their no- tated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes” in the audio repre- sentation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previ- ous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Ex- periments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness.",
        "zenodo_id": 1418127,
        "dblp_key": "conf/ismir/LiD15",
        "keywords": [
            "score following",
            "audio frames",
            "score positions",
            "piano performances",
            "sustain pedal",
            "notes",
            "audio representation",
            "partial notes",
            "sustain-pedal effects",
            "alignment accuracy"
        ],
        "content": "SCORE FOLLOWING FOR PIANO PERFORMANCES WITHSUSTAIN-PEDAL EFFECTSBochen Li Zhiyao DuanAudio Information Research (AIR) Lab,University of Rochester, Department of Electrical and Computer Engineeringbli23@ur.rochester.edu, zhiyao.duan@rochester.eduABSTRACTOne challenge in score following (i.e., mapping audio framesto score positions in real time) for piano performances isthe mismatch between audio and score caused by the us-age of the sustain pedal. When the pedal is pressed, notesplayed will continue to sound until the string vibration nat-urally ceases. This makes the notes longer than their no-tated lengths and overlap with later notes. In this paper,we propose an approach to address this problem. Giventhat the most competitive wrong score positions for eachaudio frame are the ones before the correct position due tothe sustained sounds, we remove partials of sustained notesand only retain partials of “new notes” in the audio repre-sentation. This operation reduces sustain-pedal effects byweakening the match between the audio frame and previ-ous wrong score positions, hence encourages the system toalign to the correct score position. We implement this ideabased on a state-of-the-art score following framework. Ex-periments on synthetic and real piano performances fromthe MAPS dataset show signiﬁcant improvements on bothalignment accuracy and robustness.1. INTRODUCTION1.1 Audio-Score AlignmentAudio-score alignment is the problem of aligning (syn-chronizing) a music audio performance with its score [8].It can be addressed either ofﬂine or online. Ofﬂine algo-rithms may “look into the future” when aligning the currentaudio frame to the score. Online algorithms (also calledscore following), on the other hand, may only use the pastand current audio data to align the current audio frame tothe score. Provided with enough computational resources,online algorithms can be applied in real-time scenarios.As online algorithms utilize less input data than ofﬂine al-gorithms, they can support broader applications includingthose in ofﬂine scenarios. However, they are also morechallenging to achieve the same alignment accuracy androbustness as ofﬂine algorithms do.c\u0000Bochen Li, Zhiyao Duan. Licensed under a CreativeCommons Attribution 4.0 International License (CC BY 4.0).Attribu-tion:Bochen Li, Zhiyao Duan. “Score Following for Piano Perfor-mances with Sustain-Pedal Effects”, 16th International Society for MusicInformation Retrieval Conference, 2015.Audio-score alignment has many existing and poten-tial applications. Ofﬂine algorithms have been used foraudio indexing to synchronize multiple modalities (video,audio, score, etc.) of music to build a digital library [28].Other applications include a piano pedagogical system [3]and an intelligent audio content editor [11]. Online algo-rithms further support online or even real-time applica-tions, including automatic accompaniment of a soloist’sperformance [8], automatic coordination of audio-visualequipment [18], real-time score-informed source separa-tion and remixing [11], and automatic page turning for mu-sicians [1]. Potential applications of audio-score alignmentinclude musicological comparison of different versions ofmusical performances, automatic lyrics display, and stagelight/camera management.1.2 Related WorkIn this section, we brieﬂy review existing approaches toaudio-score alignment with an emphasis on score follow-ing for piano performances, which is the problem addressedin this paper.Audio-score alignment has been an active research topicfor two decades. Early researchers started with monophonicaudio performances. Puckette [25], Grubb and Dannen-berg [16], and Cano et al. [4] proposed systems to followvocal performances. Orio and Dechelle [23] used a Hid-den Markov Model (HMM)-based method to follow dif-ferent monophonic instruments and voices. Raphael [26]applied a Bayesian network to follow and accompany amonophonic instrument soloist.For polyphonic audio, a number of ofﬂine systems usingDynamic Time Warping (DTW) have been proposed fordifferent kinds of instruments, including string and windensembles [24] and pop songs [17]. For online algorithms,Duan and Pardo [12] proposed a 2-dimensional state spacemodel to follow an ensemble of string and wind instru-ments. All the abovementioned methods, however, havenot been tested on piano performances.There are a few systems that are capable of aligning pi-ano performances. Joder and Schuller [20] proposed anHMM system with an adaptive-template-based observa-tion model to follow piano performances. In [19], Joderet al. further improved the system by exploring differentfeature functions for the observation model and using aConditional Random Field (CRF) as the alignment frame-469work. Wang et al. [29] employed DTW to achieve align-ment in three passes of the audio performance and usedscore-driven NMF to reﬁne the audio and score representa-tions in later passes. All the abovementioned systems havebeen systematically evaluated and shown with good perfor-mance on about 50 classical piano performances from theMAPS dataset [14], however, they are ofﬂine algorithmsand require the entire audio piece to ﬁnd the alignment.Dixo and Widmer [10] developed a toolkit to align differ-ent versions of music audio performances including pianobased on an efﬁcient DTW algorithm. However, this isagain an ofﬂine algorithm, although an extension to onlinescenarios can be made through online DTW algorithms [9].For online algorithms capable of following piano per-formances, Cont [5] proposed a hierarchical HMM approachwith Nonnegative Matrix Factorization (NMF). However,this system was not quantitatively evaluated. Later, Cont[6] proposed another probabilistic inference framework withtwo coupled audio and tempo agents to follow general poly-phonic performances. This algorithm has been systemati-cally evaluated on 11 monophonic and lightly polyphonicpieces played by wind and string instruments, but just 1polyphonic piano performance (a Fugue by J.S. Bach).1.3 Our ContributionIn this paper, we are interested in following piano perfor-mances. Their speciﬁc properties, such as sustain pedaleffects, the sympathetic vibration of strings, and the widepitch range, may impose challenges to systems that are de-signed to follow ensembles of voices, strings, and wind in-struments. In particular, we argue that the sustain-pedal ef-fects are especially challenging. When the pedal is pressed,notes played will continue to sound until the string vibra-tion naturally ceases. This makes the notes longer thantheir notated lengths and overlap with later notes, whichcauses potential mismatch between audio and score.Note that Niedermayer et al. reported negligible inﬂu-ence of sustain-pedal effects on alignment results in theirexperimental study on audio-score alignment [22]. How-ever, they further reasoned that this might be because thedataset used for evaluation contains only Mozart pieces, inwhich “the usage of pedals plays a relatively minor role”.In fact, the sustain pedal has been commonly used sincethe Romantic era (after Mozart) in the Western music his-tory, and is widely used in modern piano performances ofmany different styles. Another reason for Niedermayer etal.’s observation, we argue, is that the algorithm used forevaluation was an ofﬂine algorithm, which is more robustto the local mismatch between audio and score as a globalalignment is employed. For online algorithms, however,they are more sensitive to local audio-score mismatch andthey can be totally lost during the following process.In this paper, we build a system to follow piano per-formances, based on the state-space framework proposedby Duan and Pardo [12]. More speciﬁcally, we proposean approach to deal with the mismatch issue caused bysustain-pedal effects. In each inter-onset segment of theaudio, we remove partials of all notes extended from theprevious segment and only retain partials of the new notes.This operation reduces sustain-pedal effects by weakeningthe match between an audio frame and the previous wrongscore positions, which are the most competitive wrong can-didates. But we need to mention another case that thematch between this audio frame and the current correctscore position may be also reduced, if notes in previousframes are actually extended because they are not releasedyet according to the score instead of due to the sustainpedal. Nevertheless, as explained in detail in Section 3.4,this operation still favors the correct position even in thiscase. We conduct experiments on 25 synthetic and 25 realpiano performances randomly chosen from the MAPS dataset[14]. Results show that the proposed system signiﬁcantlyoutperforms the baseline system [12] on both alignmentaccuracy and robustness.2. SYSTEM FRAMEWORKWe build our system based on the state-space model pro-posed in [12], which follows polyphonic audio with itsscore. Music audio is segmented into time frames and fedinto the system in sequence. Each frameynis associatedwith a 2-dimensional state vectorsn=(xn,vn)T, repre-senting its underlying score position (in beats) and tempo(in beats-per-minute), respectively. The goal of score fol-lowing is to infer the score positionxnfrom current andprevious audio observationsy1,···,yn. This is formu-lated as an online inference problem of hidden states of ahidden Markov process, which is achieved through particleﬁltering. The hidden Markov process contains two parts: aprocess model and an observation model.The process model describes state transition probabili-tiesp(sn|sn\u00001)by two dynamic equations forxnandvn,respectively. The score position advances from the previ-ous position according to the tempo. The tempo changesthrough a random walk or does not change at all, depend-ing on where the position is.The observation modelp(yn|sn)evaluates the matchbetween an audio frame and the hypothesized state on thepitch content. A good match is achieved when the audioframe contains exactly the pitches described on the scoreat the hypothesized score position in the state. Otherwise, abad match is achieved. This is calculated using the multi-pitch likelihood model proposed in [13], which evaluatesthe likelihood of a hypothesized pitch set in explaining themagnitude spectrum of an audio frame.The multi-pitch likelihood model detects prominent peaksin the magnitude spectrum of the audio frame and repre-sents them as frequency-amplitude pairs:P={hfi,aii}Ki=1,(1)whereKis the total number of peaks detected in the frame.The likelihood would be high if the harmonics of the hy-pothesized pitch set match well with the detected peaksin terms of both frequency and amplitude. The likelihoodwould be low otherwise, for example, if many harmonicsare far away from any detected peak.470 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20153. PROPOSED METHOD3.1 Properties of Piano MusicThere are many speciﬁc properties of piano music, such asthe wide pitch range and the inharmonicity of note partials.In this section, we discuss two properties considered in theproposed approach: strong onset with exponential decay ofthe note waveform, and the sustain-pedal effects.\nFigure 1. Waveform and energy envelope of a piano note.Figure 1 shows the waveform and energy envelope ofa piano note. We see a sudden energy increase at the on-set followed by an exponential decay. When a piano keyis pressed, its damper is released and its hammer strikesthe strings, which yields an impulse-like articulation. Thedamper continues to be released as the key is being pressed.This lets the string vibration decay naturally, which maytake as long as 10 seconds. The damper comes back tothe strings when the key is released, and the string vibra-tion ceases quickly. However, when the sustain pedal ispressed, all dampers of all keys are released no matter if akey is pressed or not. This allows all active notes to con-tinue to sound, and even activate some inactive notes due tosympathetic vibrations, which enriches the sound timbre.\nFigure 2. Mismatch between audio and score caused bythe sustain-pedal effects.A detailed analysis of the sustain-pedal effects is givenby Lehtonen et al. in [21]. Here we focus on its resultedmismatch problem between audio and score. Figure 2 showsthe MIDI score (in pianoroll) and waveforms of four notes.According to the score, the ﬁrst two notes are supposed toend when the latter ones start. However, due to the sustainpedal, the waveforms of the ﬁrst two notes are extendedinto those of the latter. This causes potential mismatch be-tween the audio and the score, especially in frames rightafter the onset of the latter notes. In other words, the audiois unfaithful to the score in those frames. The degree andthe length of the unfaithfulness, however, is not notated inthe score. It depends on the the notes being played as wellas how hard the performer presses the pedal. If the pedalis pressed partially, then the damper will slightly touch thestrings and the effects are slighter. While some composersand music editors use pedal marks to notate it, appropriateuse of the sustain pedal is more often left to the performer.The main idea of the proposed approach to deal withthe sustain-pedal effects is to ﬁrst detect audio onsets tolocate the potentially unfaithful frames. Then partials ofthe extended notes are removed in the peak representationof these frames. We describe the two steps in the following.3.2 Onset DetectionAlthough not all frames right after an onset are unfaith-ful, as notes could be extended because their keys are stillpressed according to the score, many unfaithful frames doappear right after onsets. Therefore, onset detection helpsto locate potentially unfaithful frames. Many onset detec-tion methods have been proposed in the literature [2]. Inthis paper, we adopt the widely used spectral-based ap-proach, since it is effective for polyphonic signals. Weadapt it to online scenarios for our score following system.\nFigure 3. Illustration of onset detection. (a) Spectrogram.(b) Spectrogram after compression. (c) Spectral ﬂux. (d)Normalized spectral ﬂux by signal energy.Figure 3 illustrates the onset detection process. We ﬁrstcalculate the audio magnitude spectrogramY(n, k)throughShort-time Fourier Transform (STFT) in Figure 3(a), wherenandkare frame and frequency bin indices, respectively.We then apply logarithmic compression on it to enhancethe high-frequency content by˜Y(n, k) = log (1 +\u0000·Y(n, k)),(2)where\u0000controls the compression ratio. This is becausehigh frequency content is indicative for onsets but rela-tively weak in the original spectrogram [27]. Figure 3(b)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 471shows the enhanced magnitude spectrogram with\u0000=0.2.We then compute the spectral ﬂux\u0000Y(n)by summingpositive temporal differences across all frequency bins as\u0000Y(n)=Xk\u0000\u0000\u0000˜Y(n, k)\u0000˜Y(n\u00001,k)\u0000\u0000\u0000\u00000,(3)where|·|\u00000denotes half-wave rectiﬁcation, i.e., keepingnon-negative values while setting negative values to 0. Thecalculated spectral ﬂux is shown in Figure 3(c). We can seethat all onsets in the example are associated with a clearpeak, however, peak heights vary much. Spurious peaksin the middle of louder notes are as high as true peaks ofsofter notes. One could set an adaptive threshold whichvaries with the moving average of the spectral ﬂux, but thiswould make the onset detection algorithm ofﬂine. Instead,we normalize the spectral ﬂux by the energy of the audiosignal in the current frame by˜\u0000Y(n)=\u0000Y(n)/E(n),(4)whereE(n)is the Root-Mean-Squre (RMS) value of then-th frame of the audio. After this operation, a simplethreshold can detect the onsets, as shown in Figure 3(d).Note that onset detection has been used in several on-line [5] and ofﬂine [15] alignment algorithms, where a spe-cial matching function is used to match audio and scoreonsets. In our system, however, onset detection is to locatepotentially unfaithful audio frames. Their audio represen-tations are modiﬁed but no special matching function isdeﬁned.3.3 Reduce Pedal Effects by Spectral Peak RemovalFrames within a period after a detected onset are poten-tially unfaithful frames due to the sustain pedal. Conser-vatively, without knowledge of the degree and length ofthe effects, we just reduce them in the ﬁrst 200ms (i.e.,20 frames) following an onset. As described in Section 2,each audio frame is represented by a set of signiﬁcant spec-tral peaks in Eq. (1). The match between the audio frameand a hypothesized score location is evaluated through themulti-pitch likelihood model on how well the harmonics ofthe score notes match with spectral peaks in the audio. Asthe spectrum of an unfaithful audio frame contains unex-pected peaks corresponding to partials of notes extendedby the sustain pedal, we propose to remove these peaks toreduce the mismatch between audio and score.Figure 4 illustrates the idea. For each potentially un-faithful frame (e.g., then-th frame), we compare its spec-tral peaks with those in a frame before the onset (e.g.,them-th frame), and remove peaks that seem to be ex-tended from the earlier frame. LetPm={hfmi,amii}Kmi=1be the totalKmpeaks detected in them-th frame, andPn=\u0000⌦fnj,anj↵ Knj=1be the totalKnpeaks detected in then-th frame. A peak in then-th frame whose frequency isvery close to and whose amplitude is smaller than those ofa peak in them-th frame is considered as an extension andis removed. Note that repeated notes will not be removedin this way as the amplitude criterion is not met. Extended\nFigure 4. Illustration of the spectral peak removal idea. (a)Audio performance representation before and after peakremoval. (b) Magnitude spectra with spectral peaks in them-th andn-th frames. Peaks marked by crosses corre-spond to the ﬁrst two notes. Peaks marked by circles cor-respond to the latter two notes.partials that are overlapped with a partial of a new notewill not be removed either due to the same reason. Af-ter peak removal, a new spectral peak representation of then-th frame is obtained asP⇤n=Pn\u0000\u0000hfni,anii:9js.t.|fni\u0000fmj|<d ,ani<amj ,(5)wherehfni,anii2Pm.dis the threshold for the allowablefrequency deviation, which is set to a quarter tone in thispaper. Finally, the match between then-th frame and a hy-pothesized score position is evaluated through the multi-pitch likelihood of score-indicated pitches in explainingthe modiﬁed peak representation of the spectrum. Notethat this operation only modiﬁes the peak representation ofthe audio instead of the audio itself.The peak removal operation emphasizes new notes inthe representation and discards old ones. This is in accor-dance to music perception, as we always pay more atten-tion to new notes even though the old notes are as loud.3.4 New Mismatch Introduced by Peak RemovalThe peak removal operation removes notes extended by thesustain pedal in the audio representation, however, it alsoremoves notes that should remain according to the score,e.g, D4 in Figure 5(a). This causes new mismatch betweenaudio and score. Ideally, we could differentiate these twokinds of notes from the note offset information in a well-aligned score, which we do not have during score follow-ing. Nevertheless, we explain in the following that the newmismatch actually still helps with score following.472 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 5. Illustration of mismatch reduced and introducedby the peak removal operation. (a) MIDI score and itspiano-roll representation. (b) Audio performance repre-sentation before peak removal. (c) Audio performance rep-resentation after peak removal.Figure 5 illustrates the mismatch reduced and introducedby the peak removal operation. A MIDI score with twointer-onset segments↵1and↵2is shown in Figure 5(a).Notes 1 and 2 are supposed to end when Notes 4 and 5start, while Note 3 is supposed to span both segments. Foran audio frame right after the onset (e.g., then-th frame)in Figure 5(b), we can see that it contains all the ﬁve notes,including Notes 1 and 2 due to the sustain pedal. It istherefore unfaithful to the correct segment↵2in the score.Which segment is a better match to this audio frame? Outof the 5 notes in then-th frame,↵1contains 3 (Notes 1,2, and 3) and↵2also contains 3 (Notes 4, 5, and 3). Thecorrect segment↵2does not show a better match than↵1.Suppose the audio onset of Note 4 and 5 is detected,then the peak removal operation will remove spectral peakscorresponding to Notes 1, 2, and 3 in then-th frame. Themismatch between then-th frame and the correct segment↵2due to the sustain pedal is reduced, while new mis-match is introduced as Note 3 is supposed to stay in↵2inthe score but is removed in the audio. This leaves 2 notes(Notes 4 and 5) shared by the score and the audio, althoughthe score has 1 more note (Note 3). The mismatch betweenthen-th frame and↵1, on the other hand, is increased sig-niﬁcantly. There becomes no intersection at all betweennotes remained in then-th frame (Notes 4 and 5) and notesin↵1(Notes 1, 2, and 3). Therefore, the correct segment↵2is clearly a better match to then-th frame.In general, the peak removal operation may introducemismatch between an audio frame and its correct score lo-cation as it may remove peaks that are supposed to stay,but the mismatch between the audio frame and the previ-ous wrong score location will be increased much more. Infact, there will be no match at all. This is true even if allnotes in↵1stay in↵2according to the score. Therefore,the mismatch introduced by the peak removal operation isnot harmful to but actually helps with score following.In Figure 5, we only consider the previous segment↵1as a wrong segment to compete with↵2. This is becauseit is the most common error caused by the sustain pedalin score following. The peak removal operation, however,can help eliminate non-immediate segments that are priorto the current segment as well.4. EXPERIMENTS4.1 Data Set and Evaluation MeasuresWe use the MAPS dataset [14] to evaluate the proposedapproach. In this dataset, performers ﬁrst play on a MIDIkeyboard, then the MIDI performances are rendered intoaudio by a software synthesizer or a Yamaha Disklavier.The former are synthetic recordings while the latter are realacoustic recordings. Both have exactly the same timing asthe MIDI performances. We randomly select 25 syntheticpieces and 25 real pieces from the dataset. The syntheticpieces simulate the “Bechstein D 280” piano in a con-cert hall, and the real pieces are recorded with an uprightDisklavier piano. Approximately 18 synthetic pieces and10 real pieces are played with substantial sustain pedal us-age. We then download their MIDI scores fromhttp://piano-midi.de/. Note that the MIDI performanceshave minor differences from the MIDI scores besides theirtempo difference. These include occasionally missed oradded notes, different renderings of trills, and slight desyn-chronization of simultaneous notes. We therefore performan ofﬂine DTW algorithm to align the MIDI performancesto the MIDI scores and then manually correct minor errorsto obtain the ground-truth alignment.We calculate the time deviation (in ms) between theground-truth alignment and the system’s output alignmentof the onset of each score note. This value ranges from 0msto the total length of the audio. We deﬁne its average overall notes in a piece as theAverage Time Deviation (ATD).We also calculate theAlign Rate (AR)[7] for all pieces.It is deﬁned as the percentage of correctly aligned notes,those whose time deviation is less than a threshold. Com-monly used thresholds range from 50ms to 200ms depend-ing on the application. For an automatic accompanimentsystem, a deviation less than 50ms would be required, whilefor an automatic page turner, 200ms would be ﬁne.4.2 Implementation DetailsOur score following system is built upon the system pro-posed in [12], whose source code can be downloaded atthe authors’ website. We therefore take it as the baselinesystem for comparison. We use the authors’ original codeand parameter settings in both the baseline system and theproposed system. The multi-pitch likelihood model in [12]was trained on thousands of randomly mixed chords usingnotes of 16 kinds of Western instruments excluding piano.We stick with this model in the proposed system for a faircomparison. For unique parameters of the proposed sys-tem, we set\u0000to 0.2 in Eq. (2), the threshold in Figure 3to 225, the length of unfaithful region to 200ms after eachdetected onset, the frame to compare with to the 5-th framebefore the onset, and the peak frequency deviationdin Eq.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 473(5) to a quarter tone. All these parameters are ﬁxed forall pieces. Due to the probabilistic nature of the baselinesystem and the proposed system, we run 10 times of eachsystem on each piece for the comparison.4.3 Results\n(a) The 25 synthetic pieces.\n(b) The 25 real pieces.Figure 6. Align Rate comparisons between the baseline[12] (grey) and the proposed (white) systems using differ-ent time deviation tolerrances. Numbers above the ﬁguresshow medians of the boxes.Figure 6 shows box plots of align rates of the two sys-tems with different onset deviation tolerance values on bothsynthetic and real pieces. Each box in Figure 6(a) repre-sents 250 data points (10 runs on 25 pieces) and each boxin Figure 6(b) represents 250 data points. We can see thatfor the synthetic pieces, the median align rate is signiﬁ-cantly improved for all tolerance values. The dispersion ofthe distribution is also signiﬁcantly shrunk, making the im-provement on some low-performing piece-runs especiallysigniﬁcant. For the real pieces, the median align rate is sig-niﬁcantly improved for all tolerance values except 200ms.The dispersion of the distribution is shrunk signiﬁcantlyfor all tolerances except 50ms. This shows that the pro-posed approach improves the alignment accuracy and ro-bustness signiﬁcantly on both synthetic and real pieces.The improvement on synthetic pieces is more remarkablebecause there are more synthetic pieces with a substantialpedal usage. However, the proposed system also has morelow-performing outliers on the real pieces, some of whichcorrespond to piece-runs when the system is lost.Figure 7 compares the Average Time Deviation (ATD)between the two systems on all piece-runs. Again, eachbox in the synthetic setting contains 250 points and eachbox in the real setting contains 250 points. We can see\nFigure 7. Average time deviation comparison between thebaseline [12] and the proposed system. Outliers that ex-ceed 500ms are not shown in this ﬁgure. Several outliersare higher than 3 seconds. Numbers above the ﬁgure showmedians of the boxes.that the median ATD in both cases is reduced by the pro-posed system. The reduction on the synthetic pieces iseven more signiﬁcant. The dispersion of the distribution isalso shrunk signiﬁcantly, reducing the worst ATD (exclud-ing outliers) from 200-300ms to the range under 200ms.After the improvement, a fair amount of synthetic and realpiece-runs have ATD under 50ms, which would enable real-time applications such as automatic accompaniment.Examples of alignment results can be found athttp://www.ece.rochester.edu/users/bli23/projects/pianofollowing.5. CONCLUSIONSIn this paper we proposed an approach to follow piano per-formances with sustain-pedal effects. The usage of thesustain pedal extends notes even if their keys have beenreleased, hence causes mismatch between audio and score,especially in frames right after note onsets. To address thisproblem, we ﬁrst detect audio onsets to locate these po-tentially unfaithful frames. We then remove spectral peaksthat correspond to the extended notes in these frames. Thisoperation reduces the mismatch caused by the sustain-pedal effects at the expense of introducing potential newmismatch caused by the removal of notes whose keys havenot been released. However, we analyzed that this opera-tion still helps the system to favor the correct score positioneven in this case. Experimental results on both syntheticand real piano recordings show that the proposed approachimproved the alignment accuracy and robustness signiﬁ-cantly over the baseline system.For future work, we plan to consider other speciﬁc prop-erties of piano music to improve the alignment perfor-mance. For example, alignment of audio and score onsetscan provide “anchors” for the alignment, and we can deﬁnea special matching function that models the transient-likeproperty to align onsets. In addition, for the sustain part,a time-varying matching function that considers the expo-nential energy decay would improve the alignment accu-racy within a note.474 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20156. REFERENCES[1]A. Arzt, G. Widmer, and S. Dixon. Automatic pageturning for musicians via real-time machine listen-ing. InProc. European Conference on Artiﬁcial Intel-ligence (ECAI), 2008.[2]J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,M. Davies, and M. Sandler. A tutorial on onset detec-tion in music signals.IEEE Trans. Speech and AudioProcessing, 13(5):1035–1047, 2005.[3]E. Benetos, A. Klapuri, and S. Dixon. Score-informedtranscription for automatic piano tutoring. InProc.European Signal Processing Conference (EUSIPCO),2012.[4]P. Cano, A. Loscos, and J. Bonada. Score-performancematching using HMMs. InProc. ICMC, 1999.[5]A. Cont. Realtime audio to score alignment for poly-phonic music instruments using sparse non-negativeconstraints and hierarchical HMMs. InProc. ICASSP,2006.[6]A. Cont. A coupled duration-focused architecture forreal-time music-to-score alignment.IEEE Trans. Pat-tern Analysis and Machine Intelligence, 32(6):974–987, 2010.[7]A. Cont, D. Schwarz, N. Schnell, and C. Raphael. Eval-uation of real-time audio-to-score alignment. InProc.ISMIR, 2007.[8]R. Dannenberg and C. Raphael. Music score alignmentand computer accompaniment.ACM Communications,49(8):39–43, 2006.[9]S. Dixon. Live tracking of musical performances usingonline time warping. InProc. International Conferenceon Digital Audio Effects (DAFx), 2005.[10]S. Dixon and G. Widmer. Match: A music alignmenttool chest. InProc. ISMIR, 2005.[11]Z. Duan and B. Pardo. Soundprism: An online sys-tem for score-informed source separation of music au-dio.Journal of Selected Topics in Signal Processing,5(6):1205–1215, 2010.[12]Z. Duan and B. Pardo. A state space model for onlinepolyphonic audio-score alignment. InProc. ICASSP,2011.[13]Z. Duan, B. Pardo, and C. Zhang. Multiple fundamen-tal frequency estimation by modeling spectral peaksand non-peak regions.IEEE Trans. Audio Speech andLang. Process, 18(8):2121–2133, 2010.[14]V . Emiya, R. Badeau, and B. David. Multipitch estima-tion of piano sounds using a new probabilistic spectralsmoothness principle.IEEE Trans. Audio, Speech, andLanguage Process., 18(6):1643–1654, 2010.[15]S. Ewert, M. Muller, and P. Grosche. High resolutionaudio synchronization using chroma onset features. InProc. ICASSP, 2009.[16]L. Grubb and R.B. Dannenberg. A stochastic methodof tracking a vocal performer. InProc. ICMC, 1997.[17]N. Hu, R.B. Dannenberg, and G. Tzanetakis. Poly-phonic audio matching and alignment for music re-trieval. InProc. IEEE Workshop on Applications ofSignal Processing to Audio and Acoustics (WASPAA),2003.[18]T. Itohara, K. Nakadai, T. Ogata, and H.G. Okuno. Im-provement of audio-visual score following in robot en-semble with human guitarist. InProc. IEEE-RAS Inter-national Conference on Humanoid Robots, 2012.[19]C. Joder, S. Essid, and G. Richard. Learning op-timal features for polyphonic audio-to-score align-ment.IEEE Trans. Audio, Speech, Language Process.,21(10):2118–2128, 2013.[20]C. Joder and B. Schuller. Off-line reﬁnement of audio-to-score alignment by observation template adaptation.InProc. ICASSP, 2013.[21]H. M. Lehtonen, H. Penttinen, J. Rauhala, and V . Val-imaki. Analysis and modeling of piano sustain-pedaleffects.Journal of the Acoustical Society of America,122(3):1787–1797, 2007.[22]B. Niedermayer, S. Bck, and G. Widmer. On the impor-tance of real audio data for mir algorithm evaluation atthe note-level - a comparative study. InProc. ISMIR,2011.[23]N. Orio and F. Dechelle. Score following using spectralanalysis and hidden markov models. InProc. ICMC,2001.[24]N. Orio and D. Schwarz. Alignment of monophonicand polyphonic music to a score. InProc. ICMC, 2001.[25]M. Puckette. Score following using the sung voice. InProc. ICMC, 1995.[26]C Raphael. A bayesian network for real-time musicalaccompaniment. InProc. Advances in Neural Informa-tion Processing Systems (NIPS), 2001.[27]X. Rodet and F. Jaillet. Detection and modeling of fastattack transients. InProc. ICMC, 2001.[28]V . Thomas, C. Fremerey, D. Damm, and M. Clausen.Slave: a score-lyrics-audio-video-explorer. InProc. IS-MIR, 2009.[29]T. M. Wang, P.Y . Tsai, and A.W.Y . Su. Score-informedpitch-wise alignment using score-driven non-negativematrix factorization. InProc. IEEE International Con-ference on Audio, Language and Image Processing(ICALIP).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 475"
    },
    {
        "title": "Analysis of Expressive Musical Terms in Violin Using Score-Informed and Expression-Based Audio Features.",
        "author": [
            "Pei-Ching Li",
            "Li Su 0002",
            "Yi-Hsuan Yang",
            "Alvin W. Y. Su"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416784",
        "url": "https://doi.org/10.5281/zenodo.1416784",
        "ee": "https://zenodo.org/records/1416784/files/LiSYS15.pdf",
        "abstract": "The manipulation of different interpretational factors, in- cluding dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer- aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specif- ically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music infor- mation retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expres- sive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction.",
        "zenodo_id": 1416784,
        "dblp_key": "conf/ismir/LiSYS15",
        "keywords": [
            "interpretational factors",
            "dynamics",
            "duration",
            "vibrato",
            "expressive musical terms",
            "computer- aided music education",
            "novel task",
            "direct means",
            "score-informed scheme",
            "classification accuracy"
        ],
        "content": "ANALYSIS OF EXPRESSIVE MUSICAL TERMS IN VIOLIN USINGSCORE-INFORMED AND EXPRESSION-BASED AUDIO FEATURESPei-Ching Li1Li Su2Yi-Hsuan Yang2Alvin W. Y. Su11SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan2MAC Lab., CITI, Academia Sinica, Taiwanp78021015@mail.ncku.edu.tw, lisu@citi.sinica.edu.twyang@citi.sinica.edu.tw, alvinsu@mail.ncku.edu.twABSTRACTThe manipulation of different interpretational factors, in-cluding dynamics, duration, and vibrato, constitutes therealization of different expressions in music. Therefore,a deeper understanding of the workings of these factors iscritical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the noveltask of automatic expressive musical term classiﬁcation asa direct means to study the interpretational factors. Specif-ically, we consider up to 10 expressive musical terms, suchasScherzandoandTranquillo, and compile a new datasetof solo violin excerpts featuring the realization of differentexpressive terms by different musicians for the same set ofclassical music pieces. Under a score-informed scheme,we design and evaluate a number of note-level featurescharacterizing the interpretational aspects of music for theclassiﬁcation task. Our evaluation shows that the proposedfeatures lead to signiﬁcantly higher classiﬁcation accuracythan a baseline feature set commonly used in music infor-mation retrieval tasks. Moreover, taking the contrast offeature values between an expressive and its correspondingnon-expressive version (if given) of a music piece greatlyimproves the accuracy in classifying the presented expres-sive one. We also draw insights from analyzing the featurerelevance and the class-wise accuracy of the prediction.1. INTRODUCTIONThe expressive meaning of music is generally related totwo inter-dependent factors: thestructureestablished bythe composer (e.g., mode, pitch, or dissonance) and thein-terpretationof the performer (e.g., expression) [21]. GlennGould could phrase thetrillsin a way different from otherpianists. Mozart’sGraziososhould be interpreted unaliketo Brahms’. Although the interplay between the structuraland interpretational factors makes it difﬁcult to character-ize musical expressiveness from audio signals, it has beenc\u0000Pei-Ching Li, Li Su, Yi-Hsuan Yang, Alvin W. Y . Su. Li-censed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Pei-Ching Li, Li Su, Yi-Hsuan Yang, AlvinW. Y . Su. “ANALYSIS OF EXPRESSIVE MUSICAL TERMS IN VI-OLIN USING SCORE-INFORMED AND EXPRESSION-BASED AU-DIO FEATURES”, 16th International Society for Music Information Re-trieval Conference, 2015.pointed out that such analysis is valuable in emerging ap-plications such as automatic music transcription, computer-aided music education, or expressive music synthesis [2,4,7,19]. Accordingly, computational analysis of the interpre-tational aspects in music expression has been studied for awhile. For example, Bresinet al.analyzed the statisticalbehaviors oflegatoandstaccatoplayed with 9 expressiveadjectives (not expressive musical terms) [3]. Grachtenet al.made both predictive and explanatory modeling onthe dynamic markings (e.g.,f,p,fz, andcrescendo) [10].Ramirezet al.considered an approach of evolutionarycomputing for general timing and energy expressiveness[18]. Marchiniet al.analyzed the performance of stringquartets by the following three terms:mechanical,nor-malandexaggerated[14]. Recently, Rod`aet al.furtherconsidered expressive constants as affective dimensions ofmusic [20]. Related works also include the identiﬁcationof performers, singers and instrument playing techniquesin the context of musical expression [1, 6, 12, 15].To model speciﬁc aspects of the complicated music ex-pression quantitatively, a machine learning based approachis usually taken. Given an audio input, features are ex-tracted to characterize the interpretational aspects of mu-sic, such as the dynamics, tempo and vibrato [3,9,12,14].1If the symbolic or score data such as the MIDI or Mu-sicXML are available, one can further introduce more struc-tural aspects including tonality, pitch, note duration andmeasure, amongst others [10, 15, 16]. In [14], the syn-chronized audio, score and even motion data are utilizedto generate 4 sets of features, including sound level, notelengthening, vibrato extent and bow velocity, in an attemptto reveal human behaviors while playing the instrumentor indicate the structural information of music. This way,the features investigated have music meanings, and can beadopted for speciﬁc applications such as the prediction andthe generation of expressive performances [10, 18].Among all the objects of music expression, we noticethat theexpressive musical terms(EMT)2have garneredless attention in the literature, although they have been1Here we assume that any real-world interpretation of an expressivemusical term performed by a musician can be “atomized” into several(independent) factors such as dynamics, tempo, and vibrato.2In this paper, the expressive musical term is deﬁned as the Italianmusical term which describes an emotion, feeling, image or metaphor,rather than merely an indication of tempo or dynamics. It includes, butnot limited to the emotional terms (see Table 1).809Violin piecesMeasureExpressionsW. A. Mozart -Variationen1-24None, Scherzando, Tranquillo, Con Brio, Maestoso, RisolutoT. A. Vitali -Chaconne1-9None, Scherzando, Affettuoso, Con Brio, Agitato, CantabileG. Faure -Elegie2-9None, Scherzando, Grazioso, Agitato, Espressivo, CantabileP. I. Tchaikovsky -String Quartet, No. 1, Mov. II1-16None, Affettuoso, Tranquillo, Con Brio, Cantabile, RisolutoM. Bruch -Violin Concerto, No. 1, Mov. I6, 10(solo. ad lib.)None, Affettuoso, Tranquillo, Agitato, Maestoso, CantabileA. Vivaldi -La primavera, Mov. I1-13None, Scherzando, Affettuoso, Grazioso, Con Brio, RisolutoA. Vivaldi -La primavera, Mov. II2-11None, Grazioso, Agitato, Espressivo, Maestoso, CantabileE. Elgar -Salut d’Amour3-17None, Affettuoso, Grazioso, Agitato, Espressivo, MaestosoA. Vivaldi -L’autunno, Mov. I1-13None, Tranquillo, Grazioso, Con Brio, Espressivo, RisolutoA. Vivaldi -L’autunno, Mov. III1-29None, Scherzando, Tranquillo, Espressivo, Maestoso, RisolutoTable 1: The proposed dataset contains 10 different classical music pieces and each with 6 distinct expressions.widely used in specifying expressions of classical musicfor hundreds of years. How the interpretational factors (dy-namics, duration or vibrato) are taken for a musician to in-terpret the terms is still not well understood. This might bedue to the lack of a dataset containing various interpreta-tions for a ﬁxed set of classical music pieces.In this paper we address these issues, and particularly,focus on the classiﬁcation of expressive musical terms inviolin solo music. We compile a new dataset of solo vio-lin excerpts featuring the realization of 10 expressive termsand 1 non-expressive term (e.g., no expression) by 11 dif-ferent musicians for 10 classical music pieces (Section 2).After collecting the MIDI and MusicXML data for the mu-sic pieces, we design a number of dynamic-, duration- andvibrato-based features under a score-informed scheme (Sec-tion 3.2). Moreover, we also consider a baseline featureset comprising of standard audio features that can be com-puted without score information, such as the Mel-frequencycepstral coefﬁcients (MFCCs), spectral ﬂux, spectral cen-troid, and the zero-crossing rate (Section 3.1). As suchfeatures have been widely used in music information re-trieval tasks like the classiﬁcation of mood, genre or in-struments [25], we want to know whether they are also use-ful for classifying the expressive musical terms. However,we should note that many of the baseline features do notbear clear music meanings as the proposed features do. Inour experiments, we will evaluate the performance of thesefeatures for expressive musical term classiﬁcation, and an-alyze the importance of such features (Section 4).The dataset is referred to as the SCREAM-MAC-EMTdataset. For reproducibility and for calling more attentionto this research problem, we have made the audio ﬁles ofthe recordings publicly available online.32. THE SCREAM-MAC-EMT DATASETTo ﬁnd out how a violinist interprets the expressive mu-sical terms, the scope of the music data, the differencein personal interpretation, and the suitability between themusic piece and the musical term are all considered. Westarted by listing 20 typical violin pieces ranging across theBaroque, Classical, and Romantic eras, such as Vivaldi’sThe Four Seasons, Beethoven’sSpring, and Schubert’sAveMaria, to name a few. Then, we consulted with 3 profes-3https://sites.google.com/site/pclipatty/scream-mac-emt-dataset\nFigure 1: Flowchart of the proposed system.sional violinists, who are active in classical music perfor-mance, to select 10 pieces from the list and assign 5 suit-able expressive musical terms for each of them. The ma-jor criterion of selecting the music pieces, as it turns out,requires that an excerpt has a simple melody that can beeffectively manipulated to exhibit different characteristicswhen being interpreted with different expressions.The following 10 expressive terms are considered:Tranquillo(calm),Grazioso(graceful),Scherzando(play-ful),Risoluto(rigid),Maestoso(majestic),Affettuoso(af-fectionate),Espressivo(expressive),Agitato(agitated),Con Brio(bright), andCantabile(like singing).4In orderto have a balanced dataset, we require that each expressivemusical term is associated with 5 pieces. This is not easy,because not all of the 20 pieces can be interpreted withdiverse expressions. Eventually, some compromises haveto be made. For example, we choseMaestosoinstead ofCantabilefor Elgar’sSalut d’Amour, although the formeris somewhat awkward for this music piece. The resultingselection of the music pieces and the assigned expressionsis shown in Table 1.After selecting the music pieces, we recruited 11 profes-sional violinists to perform them one by one in a real-worldenvironment. In addition to the 5 assigned terms, everymusician performed a non-expressive (denoted asNone)version for each piece. Here,Nonemeans mechanical in-terpretation [14] by which the music is of constant dynam-ics, constant tempo and no vibrato. The dataset thereforecontains 660 excerpts as there are 10 classical music piecesand each piece is interpreted by 6 different versions by allthe 11 violists. We have 110 excerpts ofNone, and 55 ex-cerpts for each of the 10 expressions.3. METHODFigure 1 shows the proposed system diagram. At the ﬁrststage of the system, the input audio signal is aligned with4For more information, seehttp://www.musictheory.org.uk/res-musical-terms/italian-musical-terms.php810 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015NameAbbreviation Note-level aggregation Song-level aggregationDynamicsD M, Max, maxPos M, S,CMDurationND, 1MD, 2MD, 4MD — M, S,CMFPD — —,CMVibrato rateVR M, S, M\u0000,S\u0000, Max, Min, Diff M, S,CMVibrato extentVE M, S, M\u0000,S\u0000, Max, Min, Diff M, S,CMGlobal vibrato extentGVE — M, S,CMVibrato ratiovibRatio — —Table 2: Proposed features, the note-level and song-level aggregation methods.its corresponding MIDI ﬁle in order to ﬁnd the onset andoffset positions and the pitch of each note in the audiosignal. To do this, we adopt a chromagram-based audio-score alignment algorithm proposed in [23]. The posi-tions of the bar lines are extracted from the MusicXML-formatted score sheets by using an XML parser.5Then, tobetter characterize the attributes of the basic temporal ele-ments (note or bar) of music, frame-level features are ag-gregated over time to generate note-level or bar-level fea-tures according to the desired segmentation. Furthermore,the note-level and bar-level features are aggregated againinto a song-level representation, which allows us to mapa variable-length sequence into a ﬁxed-size feature vectorthat can be fed into a classiﬁer. Finally, in the classiﬁcationstage, we use radial-basis function (RBF) kernel SupportVector Machine (SVM) implemented by LIBSVM [5].For the feature aggregation process from note-level (orbar-level) to song-level, we consider 3 different ways: (1)taking mean value over all notes in the excerpt (M), (2)taking standard deviation over all notes in the excerpt (S),and (3) taking the contrast of M between the expressiveand its corresponding non-expressive version (CM):CM=Mexpressive/MNone.CMhere is designed to “calibrate” theeffect ofNone, which can be regarded as a baseline for theother 10 expressive musical terms. That is,CMcan some-how tell how different the expressive feature is from itsnon-expressive version. For the feature aggregation meth-ods from frame-level to note-level, we will introduce themseparately since they are different for each feature.We introduce below the baseline feature set and the pro-posed feature set.3.1 Baseline FeaturesThe baseline features are a rich set of audio features cover-ing dynamics, rhythm, tonal, and timbre. In particular, thebaseline features are a rich set of temporal, spectral, cep-stral and harmonic descriptors. It contains the mean andstandard deviation of spectral centroid, brightness, spread,skewness, kurtosis, roll-off, entropy, irregularity, ﬂatness,roughness, inharmonicity, ﬂux, zero-crossing rate, low en-ergy ratio, attack time, attack slope, dynamics and themean and standard deviation of ﬁrst-order temporal dif-ference for all the above features, totaling 4⇥17=68 fea-tures. Besides, it involves the mean of ﬂuctuation peakand centroid, tempo, pulse clarity and event density, gen-erating 5 features; the mean and standard deviation of5For more details about MusicXML, please refer tohttp://www.musicxml.com/mode and key clarity, resulting 4 features. Furthermore,it includes the mean and standard deviation of the 40-DMFCCs,\u0000MFCCs (ﬁrst-order temporal difference) and\u0000\u0000MFCCs (second-order temporal difference), totaling2⇥120=240 features. In sum, we have 317 features ex-tracted by the MIRtoolbox (version 1.3.4) [13].3.2 Proposed Features3.2.1 Dynamic FeaturesThe dynamics of each note is estimated from the short-time Fourier transform (STFT). Given a segmented notex(n)and the Hanning window functionw(n), the STFT isrepresented asXw(n, k)=Mw(n, k)ej\u0000w(n,k), whereMw(n, k)is the magnitude part,\u0000w(n, k)is the phasepart,nis the time index, andkis the frequency index. Thedynamic level functionD(n)is computed by the summa-tion of the magnitude spectrogram over the frequency binsand is expressed in dB scale:D(n) = 20 log10 XkM(n, k)!.(1)Three note-level dynamic features are computed fromD(n). Each of them are the mean value ofD(n)(D-M),the maximal value ofD(n)(D-Max) and the proportion ofthe maximum position to the note length (D-maxPos):maxPos =arg maxnD(n)length (D(k))⇥100%.(2)D-maxPos therefore measures the time a note reachesits maximal energy from its beginning, normalized to thelength of the note. All of these three note-level features arethen aggregated to song-level by M, S, andCM, totaling9 features (see the second row of Table 2). For theD(n)calculation, frames of 23ms (1014 samples) with an 82%overlap (832 samples), as used in [14], are adopted.3.2.2 Duration FeaturesAfter score alignment and note segmentation, we take thefollowing values as the features: the duration of everysingle note (ND), measure (1MD), two-measure segment(2MD), four-measure segment (4MD), and the full piece(FPD) (see the third row of Table 2). We expect that thesefeatures can capture the interpretation of local tempo vari-ations measured by single notes, downbeats, and phrases.We take M and S on ND, 1MD, 2MD and 4MD to obtainsong-level features. FPD itself is already a song-level fea-ture so no aggregation is needed. Moreover, all of theseProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 811Figure 2: Pitch contours of the ﬁrst crotchet (C5)of Mozart’sVariationenwith 6 expressions:None,Scherzando,Tranquillo,Con Brio,MaestosoandRisoluto.ﬁve features are processed byCM. Figure 2 shows exam-ples where the same note (a crotchet C5) is interpreted indifferent ND for distinct expressions.There are some more implementation details about theduration features. If a music piece has an incomplete mea-sure in the beginning (e.g., Vivaldi’sLa primavera Mov. I)then the incomplete measure is merged into the next oneand features are computed starting from the ﬁrst completemeasure. If the length of a phrase is not the multiple ofthe 2 or 4 measures then the remainders are combined asa group. Bruch’sViolin Concerto No. 1 Mov. I(the 5thpiece) is an unusual instance that has twoad libitummea-sures. In this case, 4MD is set at zero. In the parser pro-cess, a special part is to eliminate rests and ties becausethey do not have a unique sound. The former means an in-terval of silence and the latter has a curved line connectingto its previous note of the same pitch, indicating that theyshould be played as a single note.3.2.3 Vibrato FeaturesVibrato is an expressive manipulation of pitch correspond-ing to a frequency modulation of F0 (fundamental fre-quency) [17]. Because the vibrato is characterized by therate and extent of the frequency modulation of F0, a preciseestimation of the instantaneous pitch contour is needed.Since the frequency resolution in the STFT representationmay not be high enough to represent the instantaneous fre-quency, we compute the instantaneous frequency deviation(IFD) [11] to estimate the instantaneous frequency:IFDw(n, k)=@\u0000w@t=I m✓XDw(n, k)Xw(n, k)◆,(3)whereDw(n)=w0(n). Given the pitch of each notefrom the score, instantaneous frequency is computed bysumming the IFD and the bin frequency of the bin whichis nearest to the pitch frequency. Figure 2 also sketchesexamples of the vibrato contours. We can see large differ-ences in both duration and vibrato among them. For the\nFigure 3: Illustration of vibrato rate, vibrato extent, andglobal vibrato extent in a single note.IFD calculation, a window of 1025 samples at 44.1 kHzsampling rate and a hop size of 64 samples are applied.After obtaining the vibrato contour of each note, weadopt a moving-average ﬁlter with length of one-hundredthof the note length to reduce the spurious variation of thepitch contour. The ﬁlter length is empirically set so asnot to avoid much distortion and to remove high-frequentnoise. Based on the smoothed pitch contour, we considerthevibrato rate(VR) and thevibrato extent(VE). Theformer means the reciprocal of the time duration of twoconsecutive peaks, while the latter means the frequencydeviation between a peak and its nearby valley. Follow-ing [8], we require that a vibrato chain contains more than3 points and VR is between 3 and 12 Hz; otherwise, thevibrato chain is excluded. For each note, we compute themean, standard variation, mean of difference (M\u0000), stan-dard variation of difference (S\u0000), maximum (Max), mini-mum (Min) and difference (Diff) between the maximal andminimal values of both VR and VE over all frames withina note [24]. These note-level features are also aggregatedto song-level features by means of M, S, andCM.In addition, we consider a note-level feature calledglobal vibrato extent (GVE), meaning the difference of themaximal peak value and the minimal valley value within avibrato note as shown in Figure 3. GVE is also aggregatedto song-level features through M, S, andCM. Finally, weconsider a song-level feature called vibrato ratio (vibRa-tio), deﬁned as:vibRatio =# vibrato notes# notes in a violin piece⇥100%.(4)When no vibrato note is detected or the ND is shorter than125ms [14], the vibrato features are set at zero.3.3 Feature Selection and ClassiﬁcationTo evaluate the importance of the adopted features in ourtask, we perform feature selection on both the baseline andthe proposed feature sets. Here, the ReliefF routine of theMATLAB statistics toolbox6is employed in the featureselection process [22]. In the training process, ReliefFsorts the features in descending order of relevance (impor-tance). Then, the top-n0most relevant features are takenfor SVM modeling. The optimal feature numbernoptwhich results in the best accuracy is obtained by brute-force searching.6http://www.mathworks.com/products/812 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Baselinennoptc\u0000ACC317 107 1 2\u000080.473ProposedwithoutCMwithCMnnoptc\u0000ACCnnoptc\u0000ACCDynamics6 6 16 2\u000060.3189 9 16 2\u000080.398Duration9 8 16 2\u000040.33114 13 64 2\u000060.380Vibrato31 6 256 2\u000060.26446 15 64 2\u000060.258All46 22 4 2\u000060.42569 36 1 2\u000060.531Fusion363 148 1 2\u000080.498386 68 1 2\u000060.589Table 3: Performance of the baseline and the proposed fea-ture sets. ‘All’ indicates the combination of dynamics, du-ration and vibrato; ‘fusion’ represents the combination ofbaseline and ‘all.’nandnoptare the original and the op-timized number of features respectively;cand\u0000are SVMparameters; ACC indicates the average accuracy.The RBF-kernel SVM is adopted for classiﬁcation.Since the dataset is recorded by 11 violinists, we simplytake 11-fold cross validation, by using the data of 10 vio-linists as the training set and the other as the testing set.Then the feature selection is performed in each fold ofthe cross validation individually. After sending the top-noptmost relevant features into classiﬁcation, the resultingperformance is obtained from optimizing the parameterscand\u0000of the SVM. In this work, the SVM parameters areset according to the highest average classiﬁcation accuracyacross the 11 folds. In the future, we will consider otherdata splitting settings, for example using an independentheld-out set for parameter tuning.In our classiﬁcation experiment, we exclude the caseofNoneand consider a 10-class multi-class classiﬁca-tion problem, because the calculation ofCMaggregationmethod requires that the non-expressive version is knowna priori. The classiﬁcation accuracy of random guess is0.152 on average.As we want to ﬁnd out the relevant interpretational fac-tors, we only report below the results obtained by the top-noptrelevant features selected by ReliefF.4. EXPERIMENT RESULTS4.1 OverviewTable 3 lists the original feature numbern, the optimalfeature numbernopt, the average accuracy (the ratio oftrue positives and the number of data) computed over the11 folds, and the corresponding optimalcand\u0000for eachexperimental setting. The upper part of the table showsthe result of the baseline feature set, where ReliefF selectsnopt= 107out of 317 features and achieves an accuracyof 0.473. From the lower part of the table, whenCMag-gregation method is considered, the proposed feature setachieves an accuracy of 0.531 when choosingnopt= 36out of 69 features, showing a signiﬁcant improvement fromthe baseline feature set as validated by a one-tailed t-test(p<0.05, d.f.=20). Finally, after fusing the baseline fea-tures and all the proposed features, the average accuracycomes to 0.589, usingnopt= 68out of 386 features.#BaselineProposed (‘all’)Fusion124th MFCC-M4MD-CMvibRatio218th MFCC-SvibRatio24th MFCC-M326th MFCC-MD-Max-CMND-M431st MFCC-MD-M-CM18th MFCC-S525th MFCC-SFPD-CMVR-Min-M615th MFCC-SND-M31st MFCC-M721st MFCC-SD-maxPos-M26th MFCC-M831st MFCC-SVR-Min-M4MD-CM99th MFCC-S1MD-CM25th MFCC-S10entropy-S\u00002MD-CM9th MFCC-S1117th MFCC-SFPDFPD-CM1224th MFCC-S2MD-M24th MFCC-S1316th MFCC-S1MD-M15th MFCC-S1423rd MFCC-MND-CM23rd MFCC-M1522nd MFCC-SVR-M-M31st MFCC-S1615th MFCC-M4MD-SD-maxPos-M1730th MFCC-M4MD-M16th MFCC-S1810th MFCC-SD-maxPos-S21st MFCC-S1916th MFCC-MD-maxPos-CM10th MFCC-S2029th MFCC-SD-M-Mentropy-S\u0000Table 4: The ﬁrst 20 ranked features of the feature sets.4.2 The contrast valueCMTable 3 also shows how important using the contrast be-tween the expressive and non-expressive version improvesthe performance. Comparing the left-hand side (withoutCM) and the right-hand side (withCM) of the table, usingCMconstantly improves the average accuracy. Salient im-provement can be observed for dynamic features (p<0.05)and duration features (p'0.05), implying that the changeof dynamics, note duration, downbeat or phrase might beimportant interpretation factors when comparing the ex-pressive and non-expressive performance. The improve-ment is not signiﬁcant for vibrato (p>0.5), possibly be-cause the ratio of strong vibrato (expressive) and “almostno vibrato” (non-expressive) is not a stable feature. Ta-ble 3 also shows that usingCMon the proposed (‘all’) andthe fusion feature sets leads to signiﬁcant improvement forboth cases (p<0.005). Taking the contrast of feature val-ues between expressive and non-expressive performanceseems to be critical in modeling musical expression.4.3 Feature importance analysisTable 4 lists the top-20 relevant features for the baseline,proposed (‘all’) and fusion feature sets. The list is gener-ated by summing the rank of each feature over the resultsof 11 folds, and by sorting the summarized rank again.From the leftmost column, we can see that most of therelevant features in the baseline set are MFCCs. Despiteits accuracy is inferior to the proposed features, this resultshows the generality of MFCCs in audio classiﬁcation.From the middle column, we see that the top-20 pro-posed features include 11 duration features, 6 dynamicones, and 3 vibrato ones. Over half of them are durationfeatures. However, we note that the second feature is aboutvibrato (vibRatio) and the next two are both dynamic fea-tures (D-Max-CMand D-M-CM). It is not trivial to con-clude that which factor is the most relevant. Dynamics,duration and vibrato all have contribution on music inter-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 813Figure 4: Average accuracy (in F-scores) of expression classiﬁcation using individual feature sets.predicted classF-Sc Af Tr Gr Co Ag Es Ma Ca Riscoreactual classScherzando45021 320 0 020.763Affettuoso122343344 830.444Tranquillo114630001300.800Grazioso232314063400.544Con Brio8400251 2 0 3 120.431Agitato0101 24344000.748Espressivo1019 252365 30.451Maestoso0234 15434110.618Cantabile1935 214 12900.532Risoluto4 2 0 1 19 0 0 2 1260.510Table 5: Confusion matrix of musical term classiﬁcationusing the fusion feature set.pretation in various perspective. What this list providesis the signal-level details useful in synthesizing expressivemusic, or in education software for teaching expressiveperformance, something that cannot be achieved using thebaseline features such as the MFCCs.Finally, from the rightmost column we ﬁnd that the top-20 fusion features contain a blending of the baseline andthe proposed features. This shows that the two feature setsare indeed complementary, and it is advisable to exploitboth of them if classiﬁcation accuracy is of major concern.4.4 Class-wise performanceTable 5 illustrates the confusion matrix of the fusion fea-ture set summarized over the 11-fold outputs. In this con-fusion matrix, rows correspond to the actual class and colu-mns correspond to the predicted class. The column onthe right of the confusion matrix lists the average F-score,which is the harmonic mean of precision and recall.We see thatScherzando,TranquilloandAgitatoat-tain relatively high F-scores because the ﬁrst two havelighter dynamics than other expressions and the last onehas shorter duration in most cases, all are fairly easy to berecognized. Interestingly, the low F-scores and the highconfusion between the two pairsAffettuoso/GraziosoandCantabile/Espressivoclearly reveal their semantic similar-ity. The most serious confusion occurs betweenRisolutoandCon Brio, perhaps due to their similar tempo and dy-namics; the slightly difference of vibrato extent betweenthem is not discriminated in our system, unfortunately.Figure 4 shows the class-wise F-scores obtained by dif-ferent feature sets. From the ﬁrst three feature sets, we canﬁnd that using dynamic features outperforms other two forsix expressions. Using duration features attains the bestresults forAgitato,Con BrioandMaestoso; the ﬁrst twotend to use relatively fast tempo and the last one is proneto use a little slow and stable tempo. Lastly, we see that vi-brato features perform slightly better than dynamic and du-ration features forEspressivo, possibly becauseEspressivois similar toCantabilein dynamic features and is similartoGraziosoin duration features.Comparing the baseline to the proposed (‘all’) fea-ture sets, the baseline feature set performs better only forGraziosoandCon Brio. The fusion set generally im-proves F-scores for all expressions except forCon Brio.For all settings, the four expressions,Affettuoso,Grazioso,EspressivoandCantabile, are not easy to be distinguishedfrom each other due to their similar meaning.5. CONCLUSION AND FUTURE WORKIn this study, we have presented a method for analyzing theinterpretational factors of expressive musical terms imple-mented on a new dataset comprising of rich expressive in-terpretations of violin solos. The proposed features, moti-vated from the basic understanding of dynamics, duration,vibrato, and the information of score, give better perfor-mance than the standard feature set in classifying expres-sive musical terms. Particularly, the contrast of feature val-ues between expressive and non-expressive performance isfound critical in modeling musical expression. The impor-tance of the features is also reported. This provides insightsinto the design of new expression-based features, whichmay include features for the possible glissando betweentwo adjacent notes, or the variation of the note/measure du-ration proportion with respect to its measure/excerpt. Forfuture work, we will consider to expand the dataset, to ex-periment with other features and machine learning tech-niques, and to devise a mechanism that does not require anon-expressive reference to compute the contrast values.6. ACKNOWLEDGMENTSThe authors would like to thank the following three profes-sional violinists for consulting: Chia-Ling Lin (Doctor ofMusical Arts, City University of New York; concertmas-ter of Counterpoint Ensemble), Liang-Chun Chou (Mas-ter of Music, Manhattan School of Music; concertmasterof Tainan Symphony Orchestra), and Hsin-Yi Su (Mas-ter of Music, New England Conservatory of Music; majorviolin performance of Tainan Symphony Orchestra). Weare also grateful to the 11 professional violinists for theircontribution to the development of the new dataset. Thepaper is partially funded by the Ministry of Science andTechnology of Taiwan, under contracts MOST 103-2221-E-006-140-MY3 and 102-2221-E-001-004-MY3, and theAcademia Sinica Career Development Award.814 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]J. Abeßer, H. Lukashevich, and G. Schuller. Feature-based extraction of plucking and expression styles ofthe electric bass guitar. InICASSP, pages 2290–2293,2010.[2]M. Barthet, P. Depalle, R. Kronland-Martinet, andS. Ystad. Analysis-by-synthesis of timbre, timing, anddynamics in expressive clarinet performance.MusicPerception, 28(3):265–278, 2011.[3]R. Bresin and G. U. Battel. Articulation strategies inexpressive piano performance analysis of legato, stac-cato, and repeated notes in performances of the andantemovement of mozart’s sonata in g major (k 545).Jour-nal of New Music Research, 29(3):211–224, 2000.[4]A. Camurri, G. V olpe, G. De Poli, and M. Le-man. Communicating expressiveness and affect inmultimodal interactive systems.IEEE Multimedia,12(1):43–53, 2005.[5]C.-C. Chang and C.-J. Lin. LIBSVM: A library for sup-port vector machines.ACM Transactions on IntelligentSystems and Technology, 2(3):27:1–27:27, 2011.[6]J. Charles.Playing Technique and Violin Timbre: De-tecting Bad Playing. PhD thesis, Dublin Instititute ofTechnology, 2010.[7]R. L. De Mantaras. Playing with cases: Rendering ex-pressive music with case-based reasoning.AI Maga-zine, 33(4):22, 2012.[8]A. Friberg, E. Schoonderwaldt, and P. N. Juslin.CUEX: An algorithm for automatic extraction of ex-pressive tone parameters in music performance fromacoustic signals.Acta acustica united with acustica,93(3):411–420, 2007.[9]R. Gang, G. Bocko, J. Lundberg, S. Roessner, D. Head-lam, and M. F. Bocko. A real-time signal processingframework of musical expressive feature extraction us-ing MATLAB. InISMIR, pages 115–120, 2011.[10]M. Grachten and G. Widmer. Linear basis models forprediction and analysis of musical expression.Journalof New Music Research, 41(4):311–322, 2012.[11]S. Hainsworth and M. Macleod. Time frequency re-assignment: A review and analysis. Technical report,Cambridge University Engineering Department, 2003.[12]N. Kroher and E. G´omez. Automatic singer identiﬁca-tion for improvisational styles based on vibrato, timbreand statistical performance descriptors. InICMC-SMC,2014.[13]O. Lartillot and P. Toiviainen. A matlab toolbox formusical feature extraction from audio. InDAFx, 2007.[14]M. Marchini, R. Ramirez, P. Papiotis, and E. Maestre.The sense of ensemble: a machine learning approachto expressive performance modelling in string quartets.Journal of New Music Research, 43(3):303–317, 2014.[15]M. Molina-Solana, J. L. Arcos, and E. G´omez. Usingexpressive trends for identifying violin performers. InISMIR, pages 495–500, 2008.[16]K. Okumura, S. Sako, and T. Kitamura. Stochas-tic modeling of a musical performance with expres-sive representations from the musical score. InISMIR,pages 531–536, 2011.[17]E. Prame. Vibrato extent and intonation in professionalwestern lyric singing.The Journal of the Acoustical So-ciety of America, (102):616–621, 1997.[18]R. Ramirez, E. Maestre, and X. Serra. A rule-basedevolutionary approach to music performance model-ing.Evolutionary Computation, IEEE Transactions on,16(1):96–107, 2012.[19]C. Raphael. Representation and synthesis of melodicexpression. InIJCAI, pages 1474–1480, 2009.[20]A. Rod`a, S. Canazza, and G. De Poli. Clustering affec-tive qualities of classical music: beyond the valence-arousal plane.Affective Computing, IEEE Transactionson, 5(4):364–376, 2014.[21]S. Vieillard, M. Roy, and I. Peretz. Expressiveness inmusical emotions.Psychological research, 76(5):641–653, 2012.[22]M. R.ˇSikonja and I. Kononenko. Theoretical andempirical analysis of ReliefF and RReliefF.MachineLearning, 53:23–69, 2003.[23]T.-M. Wang, P.-Y . Tsai, and A. W. Y . Su. Note-basedalignment using score-driven non-negative matrix fac-torisation for audio recordings.IET Signal Processing,8:1–9, February 2014.[24]L. Yang, E. Chew, and K. Z. Rajab. Vibrato perfor-mance style: A case study comparing erhu and violin.InCMMR, 2013.[25]Y .-H. Yang and H. H. Chen. Ranking-based emotionrecognition for music organization and retrieval.IEEETrans. Audio, Speech & Lang. Processing, 19(4):762–774, 2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 815"
    },
    {
        "title": "Musical Offset Detection of Pitched Instruments: The Case of Violin.",
        "author": [
            "Che-Yuan Liang",
            "Li Su 0002",
            "Yi-Hsuan Yang",
            "Hsin-Ming Lin"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416834",
        "url": "https://doi.org/10.5281/zenodo.1416834",
        "ee": "https://zenodo.org/records/1416834/files/LiangSYL15.pdf",
        "abstract": "Musical offset detection is an integral part of a music sig- nal processing system that requires complete characteriza- tion of note events. However, unlike onset detection, off- set detection has seldom been the subject of an in-depth study in the music information retrieval community, possi- bly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. More- over, we conduct a case study of offset detection in vio- lin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset cover- ing 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly anno- tated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well- known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based",
        "zenodo_id": 1416834,
        "dblp_key": "conf/ismir/LiangSYL15",
        "keywords": [
            "Musical offset detection",
            "Note events",
            "Music signal processing",
            "Pitched non-percussive instruments",
            "Annotating offset times",
            "Evaluating offset detection",
            "Vio- lin recordings",
            "Energy-based methods",
            "Spectral flux-based methods",
            "Pitch-based methods"
        ],
        "content": "MUSICAL OFFSET DETECTION OF PITCHED INSTRUMENTS: THECASE OF VIOLINChe-Yuan Liang, Li Su, Yi-Hsuan YangAcademia Sinica{mister2dot4,lisu,yang}@citi.sinica.edu.twHsin-Ming LinUniversity of California, San Diegohsl040@ucsd.eduABSTRACTMusical offset detection is an integral part of a music sig-nal processing system that requires complete characteriza-tion of note events. However, unlike onset detection, off-set detection has seldom been the subject of an in-depthstudy in the music information retrieval community, possi-bly because of the ambiguity involved in the determinationof offset times in music. This paper presents a preliminarystudy aiming at discussing ways to annotate and to evaluateoffset times for pitched non-percussive instruments. More-over, we conduct a case study of offset detection in vio-lin recordings by evaluating a number of energy, spectralﬂux, and pitch based methods using a new dataset cover-ing 6 different violin playing techniques. The new dataset,which is going to be shared with the research community,consists of 63 violin recordings that are thoroughly anno-tated based on perceptual loudness and note transition. Theoffset detection methods, which are adapted from well-known methods for onset detection, are evaluated using anonset-aware method we propose for this task. Result showsthat the accuracy of offset detection is highly dependenton the playing techniques involved. Moreover, pitch-basedmethods can better get rid of the soft-decaying behavior ofoffsets and achieve the best result among others.1. INTRODUCTIONIn the literature, offset detection has been frequently men-tioned in the context of performance analysis [14], auto-matic music transcription (AMT) [4, 13, 21, 24, 29], notesegmentation [10, 15, 18, 26], and computational auditoryscene analysis (CASA) [19]. In these systems, offset detec-tion is required for complete measurements of duration, in-tonation, vibrato, dynamics, and other kinds of note-basedproperties of music [14]. However, to date, offset detec-tion is mostly treated as a component in a large system.Few studies, if any, are dedicated to offset detection.The challenges of offset detection can be illustrated bythe attack-decay-sustain-release (ADSR) model of musicsignals. First, consider the ADSR envelope of a pluckedc\u0000Che-Yuan Liang, Li Su, Yi-Hsuan Yang, Hsin-Ming Lin.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Che-Yuan Liang, Li Su, Yi-HsuanYang, Hsin-Ming Lin. “Musical Offset Detection of Pitched Instruments:The Case of Violin”, 16th International Society for Music InformationRetrieval Conference, 2015.\n(a) Plucked string\n(b) Bowed stringFigure 1. The ADSR envelops of a plucked string (upper)and a bowed string signal (lower). The gray blocks showthe ambiguity of onset (dark) and offset (light) due to thevariation of hearing threshold. The bold-line segments ofthe envelopes are the possible regions to detect an offset.string signal in Figure 1(a). The envelope of such signalsusually consists of a short attack, unobservable sustain, anda gradual decay right before the release. Due to the dif-ference in hearing threshold among human listeners, thepossible region of perceptual offset time (i.e.medium grayregion) can be fairly wide due to the gentle slope of therelease. Because of this, offset detection may slip into thegame of comparing the subjective listening thresholds. Incontrast, there is little ambiguity associated with the onsettime (i.e.dark gray region) due to the short attack.Figure 1(b), on the other hand, shows the possible ADSRenvelope of a bowed string signal, which contains four dis-cernible parts. Because the release time is shorter, the tem-poral uncertainty of the perceptual threshold of such sig-nals should be less than that of plucked string signals. Inpractice, however, computationally estimating the percep-tual threshold in bowed string signals may not be easy, dueto the similar shapes of the decay and the release parts.Things are more complicated in real-world signals that con-tain rich variation in the employed instruments and playingtechniques, which would shape the ADSR envelope in to-tally different ways. Indeed, the challenges of offset detec-tion can be attributed to the gentle slope of the release partand the rich variation in timbre in music signals.281This paper presents a preliminary attempt focusing onmusical offset detection. Speciﬁcally, this paper discussesvarious aspects of offset detection research, from buildinga dataset, designing an algorithm informed by the afore-mentioned challenges, to evaluating the performance ofoffset detection. We restrict our discussion on the violin,and investigate the offset detection of its six different play-ing techniques. This way, we exclude musical signals withvery long releases, such as the pedaled piano.Speciﬁcally, we discuss possible approaches to manu-ally annotating offset times in music and then propose anew one (see Section 3). The proposed approach is adoptedto construct a new offset detection dataset, which we havemade available to the research community online.1Withthe new dataset, we present and evaluate a number of off-set detection algorithms based on the spectral ﬂux, energyand pitch attributes of music (see Section 4). To investigatethe effect of playing techniques, an in-depth technique-by-technique discussion is also presented (see Section 5). Asanother contribution of this paper, a new evaluation mea-sure for offset detection is also proposed and discussed.2. RELATED WORKMost of the offset detection algorithms are implemented intwo main directions: thresholding on energy salience, andthresholding on pitch salience. The energy salience can bethe physical, perceptual or pitch-wise sound levels [15,16,20]. The thresholding on pitch salience is often seen in thecontext of AMT [13, 24], where the offset can be regardedas the falling position of a pitch salience function for a spe-ciﬁc pitch. In Non-negative matrix factorization (NMF)-based AMT, the offset is usually determined by a thresh-old on the activation matrix [29]. Other approaches, suchas novel features like correntropy [10], data-driven modelssuch as the hidden Markov models (HMM) [4, 14], sup-port vector machines (SVM) [18], have also been appliedto offset detection. Spectral ﬂux-based approaches (i.e.using temporal difference of spectrum-based representa-tions) [3, 6, 7, 28], despite being a conventional method inonset detection, are rarely used in offset detection exceptfor some studies [19, 21, 26]. Post-processing with knownonset information is sometimes used [10, 15].3. DATASET CONSTRUCTION3.1 Annotating the OffsetThere are several possible ways to annotate the offset ofmusical notes and build a dataset, depending on the dataformat of the music content. For example, one can takethe timestamps of note-off message in MIDI as the groundtruth for offset. Although audio data for experiments canbe generated by MIDI efﬁciently, this method cannot ac-curately indicate the perceptual offset time in many cases.For example, a control message “sustain pedal” makes thesynthesizer prolong the amplitude envelope even after the1http://mac.citi.sinica.edu.tw/offset_detection/note-off message. In this case, the perceptual offset canfall far behind the note-off message. Alternatively, onecan also construct a music dataset from video recordings.Video can plausibly provide visual clues to a performer’smovement which are sometimes helpful to estimate theoffset time. Inaccuracy, however, may result from audio-visual asynchrony and low frame rates.Another useful way to specify the offset times is to an-notate on the spectrogram of waveform with the aid of au-dio visualization and musical signal analysis tools such asSonic Visualiser. This method, however, may not be re-liable due to the mismatch between the physical and per-ceptual offset. For example, human has varied audibilitythreshold in different pitch frequency ranges. Perceptuallimitations, such as simultaneous masking and temporalmasking, may also affect. Therefore, a more practical wayis to incorporate visualization software and the hearing per-ception of musicians, despite the cost may be higher. Sincetheir is no procedure for such a perception-based offset an-notation, we propose a new one below.3.2 Proposed Offset Annotation ProcedureConsidering the perceptual aspects of pitched instruments,the validity of our annotation is based on two assumptions:First, if a note onset and its fundamental frequency (F0) areboth retrieved, its offset time is the ﬁrst moment when thesound intensity level is below the auditory threshold for acertain period of time. Second, for continuous notes, thesound intensity level may always be above the threshold.Therefore, the offset time of preceding note should be ex-actly or very close to the onset time of the subsequent noteunless there are polyphnic notes.With the aid of a visualization tool such as the Audacity,we propose the following steps for annotating offset times.1.Remove DC offset (bias) and normalize maximumamplitude to -1.0 dB (software default value). Thisis done by the “normalize” function in Audacity.2.Transcribe all identiﬁable pitches, excluding unsta-ble overtones and unidentiﬁable sound resulting fromplaying faults or speciﬁc playing techniques (e.g.ﬂa-geoletorsul ponticello).3.Carefully and repeatedly listen to a short part of soundsample as well as zoom in the display of waveformin order to catch the onset position.4.Identify the position within a pitch where we ﬁnd thestart of “attack” of amplitude envelope in the wave-form. The timestamp corresponds to the note onset.5.Catch the ﬁrst perceived disappearance (i.e.belowthe audibility threshold) of that given pitch. The cor-responding timestamp is the note offset time.6.For continuous notes, we simply ﬁnd consequent noteonset time and use it as the preceding note offsettime. However, in case of a clear note overlapping,we annotate the onset and the offset independently.7.If the time is still not assured, we play the sound atslower speeds and repeats steps 3–6. This is helpfulin estimating note onset or offset precisely.282 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Technique# of clips# of offsetsPizzicato13144Spiccato5168Sordino10539Flageolet848Sul tasto12140Sul ponticello15187Total631,226Table 1. Detailed information of the proposed dataset.3.3 Proposed DatasetThe dataset contains 63 violin solo excerpts with a total of1,226 notes derived from the YouTube video clips in [27]and several sound clips from the website “CompositionTo-day.com” [1]. This dataset, however, does not include in-formation about music score, ﬁngering, dynamics, vibrato,recording environment acoustics, etc. The excerpts covers6 playing techniques, namelyﬂageolet(harmonic),pizzi-cato(pluck the string),sordino(mute),spiccato(bouncethe bow),sul ponticello(bow nearing the bridge) andsultasto(bow nearing the ﬁngerboard), all of which are widelyused in orchestration [2]. These techniques produce var-ious patterns of temporal envelopes, thereby providing apractical reference set for evaluating offset detection algo-rithms. Detailed information about the number of clips andnotes for each playing techniques is listed in Table 1. Weconsider these techniques because the dataset is intendedto be used as an extension of our previous work [27]. Formore comprehensive experiments, people need to includemore playing techniques such as legato and detache.We hired a professional musician to annotate the dataset.The musician has profession-level training in music schooland has more than 20 years of experience in playing musi-cal instruments. He also has long experience in composingstring quartet and orchestral work, and in sound mixingand recording technology. From the musician’s feedback,ﬁnishing a precise note annotation and double-check costs1 to 2 minutes through the above process.4. METHODIn our study, features are extracted from three different as-pects of music, including fundamental frequency, energyenvelope and magnitude spectrum. We evaluate the threeaspects separately to investigate their feasibility for offsetdetection. Revising a few previous approaches for onsetdetection based on these aspects, we discuss ﬁve possibleoffset detection algorithms in the following subsections.4.1 Fundamental frequencyIn what follows, we denotef0nas the fundamental fre-quency at the frame indexn. The corresponding MIDInumbermncan be obtained by the relationmn=b12·log2(f0n/440)e+ 69.We adopt the spectral-domain YIN algorithm [9] to es-timate the fundamental frequency. The algorithm reducesthe computation complexity of the original, time-domainYIN algorithm [12], and can produce efﬁcient and robustestimate of fundamental frequency. It estimates the funda-mental frequency by ﬁnding the minimum of the taperedsquare difference functiondn(⌧)below a certain thresh-old. The functiondn(⌧)is formulated as:dn(⌧)=2NN/2+1Xk=0|(1\u0000e2j⇡k⌧/N)Xn(k)|2,(1)where⌧is the time lag, andXn(k)is the short-time Fouriertransform (STFT) spectrum at frame indexn. The windowsize of STFT is set toN= 2048in our implementation.The minimum of Eq.(1) indicates the periodicity. Thesmaller thedn(⌧)is, the higher the conﬁdence that the in-put signal has a fundamental frequency at1/⌧. Conversely,ifdn(⌧)is too high then the input signal is considered non-pitched. The fundamental frequencyf0is represented as:f0n=✓arg min⌧dn(⌧)◆\u00001s.t.1\u0000dn(⌧)>\u0000c.(2)We consider the termcn=1\u0000mindn(⌧)as thepitchconﬁdence; as it measures whether an input is periodic andtherefore can determine whether it is a pitch signal [9, 25].In our implementation, we set the pitch to zero (i.e.mn=0) if the conﬁdence is below a threshold\u0000c. We set\u0000c=0.7empirically.4.1.1 Pitch changePitch change has been known as a useful onset detectorfor pitched non-percussive instruments like bowed strings,where the input signal is usually excited constantly andexhibits no obvious amplitude or phase variation [11, 17].Pitch change is a clear indicator of a note transition, whichtypically contains an offset of the previous note and theonset of the latter note. When the pitch contour changesfrom one pitch to another, we expect that there should beone note ending and another note starting. We note that thelimitation of this idea is that it cannot deal with the case ofrepeating notes.Based on the above observation, we propose the fol-lowing offset detection method using pitch change infor-mation. We consider there is an offset event at framen, ifthe following two rules are satisﬁed:mod12(mn\u0000mn\u00001)\u00001^cn\u0000cn\u00001<0.(3)Similar to the onset detector proposed in [17], the mod-ulo operator in the ﬁrst rule is applied to prevent octaveerrors, although it also hinders the detection of transitionsof octave(s). Becausemn=0whencn\u0000c, the ﬁrstrule also captures voice/unvoice transition. We also ob-serve that the falling moment of conﬁdence function canindicate the chance of a stable pitch fading that enables usto distinguish offset from onset.4.1.2 Pitch conﬁdenceAnother perspective is to directly use the pitch conﬁdencefunction as an offset detector. In this case, errors of pitchProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 283detection would not inﬂuence the performance. The ba-sic idea is that the time instant when the pitch conﬁdencechanges from pitched to non-pitched is considered as theoffset time. Therefore, this method searches for the mo-ment that the pitch conﬁdence falls below the threshold. Inother words, there is an offset event atn, if the followingconditions meet:cn\u00001>\u0000c^cn<\u0000c.(4)Please note that this method is conceptually similar tothe way many NMF-based automatic transcription algo-rithms detect offsets: they usually detect offsets by thresh-olding on the activation matrix [29]. While our methodusescnto measure pitch conﬁdence, NMF-based methodsuse the value of activation to measure pitch conﬁdence.4.2 Energy envelopeThe energy envelope as used by the human auditory sys-tem [6] has been proven to be a robust feature in many on-set detection tasks [7, 8, 22]. Here we compute the energy-like temporal envelope based on this feature. The pre-processing step starts from raw STFT spectra with framesize 2048, then map into 141 sub-bands by a set of trian-gular ﬁlter bank equally spaced in log-scale ranging from30 Hz to 17000 Hz. Then, the feature is scaled by the log-arithmx7!log(1 +x). Finally, the energy-like envelopeis formulated as:En=Pk|¯Xn(k)|2, where¯Xn(k)isthe pre-processed spectra magnitude of bink. Since theperceptual offset is a subjective threshold lies between thedecaying phase of energy envelope, and in most cases noteoffset is interrupted by succeeding onset, that make thesetting an absolute thresholding infeasible. Therefore, weemploy the relative threshold peak-picking algorithm [5]to ﬁnd the valley of energy envelope as offset.4.3 Spectral ﬂuxSpectral ﬂux is one of the most common, easy-to-implementyet powerful methods for onset detection [3, 7]. It canbe formulated as:SFn=PkH(|¯Xn(k)|\u0000|¯Xn\u00001(k)|),whereH(x)=|x|+x2is the rectiﬁer function, and the pre-processed spectral bins¯Xn(k)are the ones that are de-scribed in Section 4.2.We are interested in whether the idea of spectral ﬂuxcan be adopted for offset detection. Tworeversedvariantsof spectral ﬂux are considered:•Reverse rectiﬁcation (SFrr):The rectiﬁer functionHin onset detection selects only the positive ﬂuxwhile suppresses the negative ﬂux. Conversely, foroffset detection,His replaced byH0=|x|\u0000x2, whichsuppress all the positive ﬂux.•Reverse coding (SFrc):The other setting is to com-pute spectral ﬂux in the opposite direction,i.e., fromthe future to the past:PkH(|¯Xn(k)|\u0000|¯Xn+3(k)|),to reverse the raw audio signal, and apply the normalspectral ﬂux method to the reversed signal as look-ing for onset in the opposite direction.\nOffset annotation\u0001Onset annotation\u0001i\u0001ii\u0001iii\u0001Figure 2. A case when an offset and its succeeding onsetare very close, the right margin of the tolerance windowfor offset (the solid line) might fall behind the right marginof the tolerance window for onset (the dash line) of onset.Regions i–iii are all within the tolerance window for offset,while region ii is within the tolerance window for onset.5. EVALUATION5.1 Onset-aware evaluation metricWe employ the standard measures for evaluation: preci-sion, recall and F-score. In evaluation, the offset estimatethat falls within a tolerance window of length2\u0000toleranceofthe groundtruth offset time is considered to be a true posi-tive. Moreover, the estimate and the groundtruth can onlybe matched at most once, based on maximum cardinalitybipartite matching [23]. The remaining estimates are con-sidered false positives. The tolerance window (centeredat the groundtruth annotation) can be written as\u0000W=[\u0000\u0000tolerance,+\u0000tolerance]. This is referred to as theconven-tionaltolerance window.A typical problem of this evaluation method is depictedin Fig. 2. As mentioned in Section 1, the tolerance win-dow for offset detection is often set to be wider than thatfor onset detection in most previous work.2In Fig. 2, theright margin of the tolerance window for offset of the cur-rent note (i.e.the solid line in Fig. 2) falls behind the rightmargin of the tolerance window for onset of the succeedingnote (i.e.the dash line in Fig. 2). Such a situation occursfor more than 80% of notes in our dataset, when\u0000toleranceisset to 100ms. If the offset is annotated given the transitionoffset annotation rule that we suggest, region iii should notbe considered as a possible true positive area.In light of this observation, we further deﬁne a newtolerance window by\u0000W0=[\u0000\u0000tolerance,+\u0000posttolerance],making\u0000posttolerancedependent on the succeeding onset. Inthis paper, we set\u0000posttolerance=m i n (\u0000t+50ms,\u0000tolerance),where\u0000tdenotes the timestamp of the next onset, and 50msis a commonly adopted value for\u0000tolerancefor onset.To give a deep insight of the onset-aware tolerance win-dow, let’s ﬁrst consider this: if the offset and succeedingonset are located far apart, the post tolerance would be thesame as the conventional tolerance, so the evaluation re-sult will be the same as the result of the conventional met-ric. But, as the distance becomes closer, post tolerance willshrink to the tolerance of succeeding onset when they arefully overlapped, resulting in a shortened tolerance win-2http://www.music-ir.org/mirex/wiki/2014:Multiple_Fundamental_Frequency_Estimation_\\%26_Tracking284 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Playing techniquePerformancePitch conﬁdencePitch changeEnergySFrcSFrrmeasureMAMBMAMBMAMBMAMBMAMBPizzicatoF-score0.689 0.6710.578 0.5570.695 0.6950.576 0.5560.587 0.567Spiccato0.778 0.7240.740 0.6870.759 0.7590.610 0.3080.584 0.271Sordino0.727 0.7180.701 0.6860.555 0.5120.650 0.5980.652 0.596Flageolet0.381 0.3810.321 0.3010.414 0.4020.292 0.2620.290 0.254Sul tasto0.531 0.5220.544 0.5240.463 0.4330.448 0.4330.440 0.424Sul ponticello0.522 0.5180.44 0.4340.338 0.3090.314 0.3020.310 0.299OverallPrecision0.688 0.6730.514 0.4980.422 0.3980.364 0.3260.361 0.320Recall0.623 0.6090.669 0.6480.639 0.6040.758 0.6770.759 0.674F-score0.654 0.6400.582 0.5630.508 0.4800.492 0.4400.489 0.434Table 2. Comparison of evaluation metrics to offset detection methods. MA: the conventional evaluation metric. MB: theproposed onset-aware evaluation metric.\n/tolerance0.050.070.090.110.130.15F-score0.30.40.50.60.7Con-dence(MA)Con-dence(MB)SFrr(MA)SFrr(MB)Figure 3. Comparison of evaluation using conven-tional metric (solid line) and proposed onset-aware metric(dash line) on two offset detection methods. Horizontalaxis shows the tolerance\u0000tolerance(ranging from 50ms to150ms), and vertical axis shows average F-score.dow without region iii. In other words, the right margin ofthe tolerable window for offset would not exceed the rightmargin of the tolerable window for the succeeding onset.5.2 Experiment resultTable 2 shows the evaluation of detection algorithms per-formed by both metrics. First of all, we see that pitch-based methods signiﬁcantly outperform the others in theoverall result according to both metrics. This is perhapsnot surprising, given that pitch-based methods have beenshown effective for onset detection for notes with slow at-tack phase. For example, Holzapfelet al.[17] have shownthat pitch-based methods work much better than SF-basedmethods for onset detection for bow-string instrument andwind instrument. The decaying phase exhibits similar sig-nal characteristics as soft onsets when “looking reversely”from the end of the signal. This may explain why pitch-based methods also work better than SF-based methods foroffset detection.We expect that the result of onset-aware evaluation (us-ing\u0000W0) would be equal or less than conventional metric(using\u0000W). The interesting ﬁnding is that, while mostmethods we considered have similar result for the two eval-uation metrics, the result of SF-based methods degrades alot when the onset-aware metric is adopted. For the overallresult, the result of the two SF methods decreases by 11%and 13%, respectively. The most severe degradation is seeninspiccato. This result indicates that SF-based methodsmay be prone to produce many estimations within regioniii of Fig. 2.Fig. 3 compares the result of the pitch conﬁdence methodand theSFrrmethods using the two metrics. As it will beshown later in Section 5.3, spectral ﬂux exhibits temporalalignment issues while the pitch conﬁdence method doesnot. It can be seen that the pitch conﬁdence method doesnot suffer from the penalty of proposed metric whileSFrrdoes. We note that the bipartite matching mechanism weadopted may have also avoided some of the estimation in-side region iii of Fig. 2. But, by using the proposed metric,we can ensure region iii is fully eliminated. This is im-portant because the conventional metric may give us over-optimistic result.Another important ﬁnding is that the pitch conﬁdencemethod consistently outperforms the pitch change method,when the onset-aware metric is adopted. Results showthat the pitch change method has higher recall but muchlower precision, possibly due to the ﬂuctuation of conﬁ-dence above and below threshold causes some false alarms.It is possible to mitigate the issue by proper post-processing,such as by padding the continuous note or using median ﬁl-ter, but if the pitch conﬁdence method is employed we donot have to deal with such an issue.5.3 IllustrationThe upper part of Fig. 4 shows the spectrogram and the off-set detection functions ofpizzicatoandspiccato.3Thoughboth techniques produce sound by pulse-like excitation, wecan see the envelope ofspiccatois much smoother thanspiccato in terms of attack and decay phase possibly, be-cause of the elasticity of bow cause the striking contactsthe string slightly longer (i.e.leading to longer sustain)than the plucking string. SF based methods typically takethe the beginning of decay as the offset position, as shownin Fig. 2, while the estimates of other methods appear tobe closer to the ground truth. SF-based methods are proneto produce temporal detection errors largely inpizzicatoandspiccato, making the conventional evaluation metricfor onset less appropriate for evaluate the result for offset.However, some estimations ofpizzicatois a lot earlier than3We only put one of the spectral ﬂux based methods due to their highsimilarity of detection curve.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 285(a)Pizzicato(pluck the string)\n(b)Spiccato(bounce the bow)\n(c)Con sordino(mute)\n(d)Sul tasto(bow nearing the ﬁngerboard)\n(e)Sul ponticello(bow nearing the bridge)\n(f)Flageolet(harmonic)Figure 4. Comparison of the signal characteristic of six playing techniques. From top to bottom are spectrogram, spectralﬂux, energy envelope, and pitch-based offset detection curves. The right-pointing triangle denotes the onset annotation andthe left-pointing triangle denotes the offset annotation.spiccatothat it is even located within the onset tolerance(i.e.region ii). In that extreme situation, we may have toshorten the onset tolerance by a few milliseconds in ourevaluation metric.The low part of Fig. 4 shows the other four bowing tech-niques. For the lower two techniques, all of the detectionfunctions exhibit the ﬂuctuating curve due to the noise-likeovertones, leading to inferior result forsul ponticelloandﬂageolet. In such case, energy relatively remains in the thesame level of performance. On the other hand, from themiddle part of Fig. 4, we can see the pitch conﬁdence isstill a good indicator of offset forcon sordinoandsul tasto.6. CONCLUSIONIn this paper, we have discussed the challenges of offsetdetection, the methodology of constructing an offset de-tection dataset, some detection algorithms, and a few con-siderations in evaluation. Based on the newly constructedviolin dataset, we have ﬁrstly investigated the behaviors ofmusical offsets in the signals generated by various kindsof mechanism. We ﬁnd that, in general, the pitch con-ﬁdence based offset detection function outperforms algo-rithms based on energy and spectral ﬂux. For the playingtechniques having sharp envelopes such aspizzicatoandspiccato, energy-based method can be competitive. Wehave also proposed an onset-aware evaluation metric that ismore reliable than the conventional ones in avoiding over-estimation of true positives. We hope that these ﬁndingscan contribute to the advance of research on automatic mu-sic transcription and melody tracking.7. ACKNOWLEDGMENTThis work was supported by the Ministry of Science andTechnology of Taiwan under the contracts MOST 102-2221-E-001-004-MY3, MOST 104-2221-E-001-029-MY3, andthe Academia Sinica Career Development Program.286 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Compositiontoday.com - sound bank – violin.http://www.compositiontoday.com/sound_bank/violin/.[2]S. Adler.The Study of Orchestration–3rd Edition. WW Nor-ton, 2002.[3]J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies,and M. B. Sandler. A tutorial on onset detection in musicsignals.IEEE Trans. Speech Audio Proc., 13(5):1035–1047,2005.[4]E. Benetos and S. Dixon. Polyphonic music transcription us-ing note onset and offset detection. InProc. IEEE Int. Conf.Acoust. Speech Signal Proc., pages 37–40. IEEE, 2011.[5]S. B¨ock, A. Arzt, F. Krebs, and M. Schedl. Online real-timeonset detection with recurrent neural networks. InProc. Int.Conf. Digital Audio Effects, pages 1–4, 2012.[6]S. B¨ock, F. Krebs, and M. Schedl. Evaluating the online ca-pabilities of onset detection methods. InProc. Int. Soc. MusicInformation Retrieval Conf., pages 49–54, 2012.[7]S. B¨ock and G. Widmer. Maximum ﬁlter vibrato suppressionfor onset detection. InProc. Int. Conf. Digital Audio Effects,2013.[8]Sebastian B¨ock and Florian Krebs. MIREX onset detectiontask. InMusic Information Retrieval Evaluation eXchange,2012. [Online]http://www.music-ir.org/mirex/abstracts/2012/BK2.pdf.[9]P. M. Brossier.Automatic annotation of musical audio for in-teractive applications. PhD thesis, Queen Mary, Universityof London, 2006.[10]S. Chang and K. Lee. A pairwise approach to simultaneousonset/offset detection for singing voice using correntropy. InProc. IEEE Int. Conf. Acoust. Speech Signal Proc., pages629–633. IEEE, 2014.[11]N. Collins. Using a pitch detector for onset detection. InProc.Int. Soc. Music Information Retrieval Conf., pages 100–106,2005.[12]A. De Cheveign´e and H. Kawahara. Yin, a fundamental fre-quency estimator for speech and music.The Journal of theAcoustical Society of America, 111(4):1917–1930, 2002.[13]A. Degani, R. Leonardi, P. Migliorati, and G. Peeters. Apitch salience function derived from harmonic frequency de-viations for polyphonic music analysis. InProc. Int. Conf.Digital Audio Effects, 2014.[14]J. Devaney, M. I. Mandel, and I. Fujinaga. A study of into-nation in three-part singing using the automatic music perfor-mance analysis and comparison toolkit (AMPACT). InProc.Int. Soc. Music Information Retrieval Conf., pages 511–516,2012.[15]A. Friberg, E. Schoonderwaldt, and P. N. Juslin. Cuex: Analgorithm for automatic extraction of expressive tone param-eters in music performance from acoustic signals.Acta acus-tica united with acustica, 93(3):411–420, 2007.[16]J. Glover, V . Lazzarini, and J. Timoney. Real-time segmenta-tion of the temporal evolution of musical sounds. InProc.Meetings on Acoustics, volume 15. Acoustical Society ofAmerica, 2014.[17]A. Holzapfel, Y . Stylianou, A. C. Gedik, and B. Bozkurt.Three dimensions of pitched instrument onset detection.IEEE Trans. Audio, Speech, Language Proc., 18(6):1517–1527, 2010.[18]L.-C. Hsu, Y .-L. Wang, Y .-J. Lin, C. D. Metcalf, and A. W.-Y . Su. Detection of motor changes in violin playing by emgsignals. InProc. Int. Soc. Music Information Retrieval Conf.,2014.[19]G. Hu and D. Wang. Auditory segmentation based on onsetand offset analysis.IEEE Trans. Audio, Speech, Lang. Proc.,15(2):396–405, 2007.[20]C. Kehling, J. Abeßer, C. Dittmar, and G. Schuller. Automatictablature transcription of electric guitar recordings by estima-tion of score- and instrument-related parameters. InProc. Int.Conf. Digital Audio Effects, 2014.[21]A. Kobzantsev, D. Chazan, and Y . Zeevi. Automatic tran-scription of piano polyphonic music. InProc. Int. Symp. Im-age and Signal Processing and Analysis, pages 414–418,2005.[22]E. Marchi, G. Ferroni, F. Eyben, L. Gabrielli, S. Squartini,and B. Schuller. Multi-resolution linear prediction based fea-tures for audio onset detection with bidirectional lstm neuralnetworks. InProc. IEEE Int. Conf. Acoustics, Speech and Sig-nal Processing, pages 2164–2168, 2014.[23]C. Raffel, B. McFee, E. J. Humphrey, J. Salamon, O. Nieto,D. Liang, and D. P. W. Ellis. mireval: A transparent imple-mentation of common MIR metrics. InProc. Int. Soc. for Mu-sic Information Retrieval Conf., 2014.[24]J. Salamon and E. G´omez. Melody extraction from poly-phonic music signals using pitch contour characteristics.IEEE Trans. Audio, Speech, Lang. Proc., 20(6):1759–1770,2012.[25]J. Serra, G. K. Koduri, M. Miron, and X. Serra. Assessing thetuning of sung indian classical music. InProc. Int. Soc. MusicInformation Retrieval Conf., pages 157–162, 2011.[26]R. Sridhar and T. V . Geetha. Raga identiﬁcation of carnaticmusic for music information retrieval.International Journalof recent trends in Engineering, 1(1):571–574, 2009.[27]L. Su, H.-M. Lin, and Y .-H. Yang. Sparse modelingof magnitude and phase-derived spectra for play-ing technique classiﬁcation.IEEE/ACM Trans. Audio,Speech and Language Proc., 22(12):2122–2132, 2014.[Online]http://mac.citi.sinica.edu.tw/violin-playing-technique/.[28]L. Su and Y .-H. Yang. Power-scaled spectral ﬂux and peak-valley group-delay methods for robust musical onset detec-tion. InProc. Sound and Music Computing Conf., 2014.[29]E. Vincent, N. Bertin, and R. Badeau. Enforcing harmonicityand smoothness in bayesian non-negative matrix factoriza-tion applied to polyphonic music transcription.IEEE Trans.Audio, Speech, Lang. Proc., 18(3):528–537, 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 287"
    },
    {
        "title": "Content-Aware Collaborative Music Recommendation Using Pre-trained Neural Networks.",
        "author": [
            "Dawen Liang",
            "Minshu Zhan",
            "Daniel P. W. Ellis"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416308",
        "url": "https://doi.org/10.5281/zenodo.1416308",
        "ee": "https://zenodo.org/records/1416308/files/LiangZE15.pdf",
        "abstract": "Although content is fundamental to our music listening preferences, the leading performance in music recommen- dation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user’s listening his- tory rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known “cold-start” prob- lem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content informa- tion into collaborative filtering methods have shown suc- cess in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative fil- tering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evalu- ated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case.",
        "zenodo_id": 1416308,
        "dblp_key": "conf/ismir/LiangZE15",
        "keywords": [
            "content",
            "collaborative-filtering",
            "cold-start",
            "semantic tagging",
            "neural network",
            "Million Song Dataset",
            "user listening data",
            "prior",
            "performance",
            "recommendation"
        ],
        "content": "CONTENT-AWARE COLLABORATIVE MUSIC RECOMMENDATIONUSING PRE-TRAINED NEURAL NETWORKSDawen Liang, Minshu Zhan, and Daniel P. W. EllisLabROSA, Dept. of Electrical EngineeringColumbia University, New York{dliang@ee., mz2468@, dpwe@ee.}columbia.eduABSTRACTAlthough content is fundamental to our music listeningpreferences, the leading performance in music recommen-dation is achieved by collaborative-ﬁltering-based methodswhich exploit the similarity patterns in user’s listening his-tory rather than the audio content of songs. Meanwhile,collaborative ﬁltering has the well-known “cold-start” prob-lem, i.e., it is unable to work with new songs that no onehas listened to. Efforts on incorporating content informa-tion into collaborative ﬁltering methods have shown suc-cess in many non-musical applications, such as scientiﬁcarticle recommendation. Inspired by the related work, wetrain a neural network on semantic tagging information asa content model and use it as a prior in a collaborative ﬁl-tering model. Such a system still allows the user listeningdata to “speak for itself”. The proposed system is evalu-ated on the Million Song Dataset and shows comparablybetter result than the collaborative ﬁltering approaches, inaddition to the favorable performance in the cold-start case.1. INTRODUCTIONMusic recommendation is an important yet difﬁcult taskin music information retrieval. A recommendation systemthat accurately predicts users’ listening preferences baresenormous commercial value. However, the high complex-ity and dimensionality of music data and the scarcity ofuser feedback makes it difﬁculty to create a successful mu-sic recommendation system.Two primary approaches exist in recommendation: col-laborative ﬁltering and content-based methods. For mu-sic, the state-of-the-art recommendation results have beenachieved by collaborative ﬁltering methods, which requiresonly information on users’ listening history rather than themusical content for recommendation. The central assump-tion of this model is that a user is likely to accept a songthat is liked by users who have similar taste. A major cate-gory of collaborative ﬁltering approaches is based on latentc\u0000Dawen Liang, Minshu Zhan, Daniel P. W. Ellis.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Dawen Liang, Minshu Zhan, DanielP. W. Ellis. “Content-Aware Collaborative Music Recommendation Us-ing Pre-trained Neural Networks”, 16th International Society for MusicInformation Retrieval Conference, 2015.factor model. It assumes that a low-dimensional represen-tation exists for both users and songs such that the compat-ibility between a user and a song, modeled as their innerproduct in this latent space, predicts the user’s fondness ofthe song. In the case that user feedback isimplicit(e.g.,whether or not the user has listened to a particular song),the weighted matrix factorization from Huet al.[6] worksparticularly well. Details regarding collaborative ﬁlteringwill be further discussed in Section 2.1.On the other hand, modeling musical content for thepurpose of taste prediction is difﬁcult due to the structuralcomplexity present in music data which is hard to captureby simple models. Deep learning has shown its power invarious pattern recognition tasks with its capability of ex-tracting hierarchical representations from raw data. In mu-sic recommendation, van den Oordet al.[13] have experi-mented with neural networks on predicting the song latentrepresentation from musical content.It is natural to combine collaborative ﬁltering and con-tent models in recommendation to utilize different sourcesof information. A successful attempt from Wang and Blei[14], which joins a content model on article with collab-orative ﬁltering, achieves good performance on scientiﬁcarticle recommendation.Inspired by these mentioned above, we create a content-aware collaborative music recommendation system. As thename suggests, the system has two components: the con-tent model and the collaborative ﬁltering model. To obtaina powerful content model, we pre-train a multi-layer neu-ral network to predict semantic tags from vector-quantizedacoustic feature. The output of the last hidden layer istreated as a high-level representation of the musical con-tent, which is used as a prior for the song latent represen-tation in collaborative ﬁltering. We evaluate our systemon the Million Song Dataset and show competitive perfor-mance to the state-of-the-art system.2. RELATED WORKIn this section we review important relevant work. Firstwe give an overview of matrix factorization model for rec-ommendation, especially forimplicit feedback. Then wedescribe two models which are closely related to ours: col-laborative topic model for article recommendation and deepcontent-based music recommendation.2952.1 Recommendation by matrix factorizationA widely used approach to recommendation is collabora-tive ﬁltering, where items are recommended to a user basedon other users with similar patterns of item consumption.Matrix-factorization-based latent factor models [6, 8] areamong the most successful collaborative ﬁltering methods.In a matrix factorization recommendation model, werepresent both users and items in a shared low-dimensionalspace of dimensionK, where useruis represented by a la-tent factor✓u2RKand itemiis represented by a latentfactor\u0000i2RK. To make a prediction about the prefer-ence of useruon itemi, we simply take the dot productbetween the twoˆrui=✓Tu\u0000i. To estimate user and itemfactors, we can minimize the squared loss between the es-timated preference and actual responsesPu,i(rui\u0000ˆrui)2,with`2regularization on the factors to prevent overﬁtting.Alternating least squares (ALS) can be employed for efﬁ-cient optimization. Equivalently, we can formulate a prob-abilistic matrix factorization model [12] with the followinggenerative process:•For each useru, draw user latent factor:✓u⇠N(0,\u0000\u00001✓IK),•For each itemi, draw item latent factor:\u0000i⇠N(0,\u0000\u00001\u0000IK),•For each user-item pair(u, i), draw feedback:rui⇠N(✓Tu\u0000i,c\u00001ui),and obtain the same estimates via maximuma posteriori.Herecuirepresents our conﬁdence on the correspondingresponserui, i.e., larger value ofcuiindicates that thereis less uncertainty about the responserui, and vice versa.This is especially crucial in the case of implicit feedback(e.g., whether userulistened to songi), because of itsnoisy nature. Huet al.[6] propose a simple heuristic forsetting the values ofcuifor implicit feedback1:cui=1+↵log(1 +rui/✏)where↵and✏are tunable hyperparameters. This methodachieves the state-of-the-art recommendation performancein the implicit feedback case.2.2 Collaborative topic modelDue to its content-free nature, collaborative ﬁltering ap-proaches can be applied in a wide range of domains. Theyperform well on what is calledin-matrixpredictions, i.e.,recommending items that have been consumed by someusers. However, this approach suffers from the well-knownproblem that it is unable to recommend new items that nouser has consumed, or makingout-of-matrixpredictions,1In [6], the observational model is on the binary indicator variablepui=1{rui>0}rather thanrui, i.e.,pui⇠N(✓Tu\u0000i,c\u00001ui). How-ever, in this paper the responseruiis itself binary, indicating whetheruseruhas listened to songi. Thus we treatruiandpuiinterchangeably.where content-based models are better suited. Many ef-forts have been made to incorporate content into collabora-tive ﬁltering. Wang and Blei [14] propose the collaborativetopic regression (CTR) model for scientiﬁc article recom-mendation, which is particularly relevant to our proposedmethod.There are two components in CTR: a matrix factoriza-tion collaborative ﬁltering model (as described in Section2.1) and a latent Dirichlet allocation (LDA) article contentmodel. LDA [2] is a mixed-membership model on docu-ments. Assuming there areKtopics\u0000=\u00001:K, each ofwhich is a distribution over a ﬁxed set of vocabulary, LDAtreats each document as a mixture of these topics wherethe topic proportion⇡iis inferred from the data. Onecan understand LDA as representing documents in a low-dimensional “topic” space with the topic proportion beingtheir coordinates. With this interpretation, the generativeprocess of CTR is as follows:•For each useru, draw user latent factor:✓u⇠N(0,\u0000\u00001✓IK),•For each documenti,–Draw topic proportion⇡i⇠Dirichlet(↵)2,–Draw latent factor\u0000i⇠N(⇡i,\u0000\u00001\u0000IK),•For each user-document pair(u, i), draw feedback:rui⇠N(✓Tu\u0000i,c\u00001ui).We can see CTR differs from [6] in that CTR assumesthat the item latent factor\u0000iis close to the topic propor-tion⇡ibut could deviate from it if necessary. This allowsthe user-item interaction data to “speak for itself”. An at-tractive characteristic of CTR is its capability of makingout-of-matrixpredictions. This is done by using the topicproportion⇡ialone as the item latent factor:ˆrui=✓Tu⇡i,which is not possible in the traditional collaborative ﬁlter-ing model.Although CTR achieves better recommendation perfor-mance than pure collaborative ﬁltering, it does not scalewell with large data. Since the model is not condition-ally conjugate: the prior on\u0000icomes from a Dirichlet-distributed random variable⇡i, topic proportion⇡icannotbe updated analytically and slower numerical optimizationmethod is required. To address this problem, Gopalanetal.[5] propose the collaborative topic Poisson factoriza-tion (CTPF). This model replaces the Gaussian likelihoodand Gaussian prior in CTR with Poisson likelihood andgamma prior, thus becoming conditionally conjugate withclosed-form updates. Experiments on large-scale scientiﬁcarticle recommendation demonstrate that CTPF performssigniﬁcantly better than CTR.The main difference that sets our method apart fromcollaborative topic model is the content model. As a fea-ture extractor, LDA can only produce linear factors due toits bilinear nature. On the other hand, multi-layer neuralnetwork used by in our system is capable of capturing thenon-linearities in the feature space.2The generative process for words is omitted for brevity throughoutthe paper. Please refer to [14] for details.296 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20152.3 Deep content-based music recommendationPrevious attempts on content-based music recommenda-tion have achieved promising results. van den Oordetal.[13] utilize a neural network to map acoustic featuresto the song latent factors learned from the weighted ma-trix factorization [6]. As a result, given a new song thatno one has ever listened to, a latent factor can still be pre-dicted from the network and recommendation can be donein the same fashion as with a regular collaborative ﬁlteringmodel.Our method is very similar to this approach, but we willpoint out two major differences:•First, the neural network is used for different pur-poses. We use it as a content feature extractor, justlike LDA in the collaborative topic model. The neu-ral network in [13] maps content directly to the la-tent factors learned from pure collaborative ﬁltering,and the resulting model is expected to operate simi-larly to collaborative ﬁltering even when usage datais absent.•Since the neural network is trained to map contentto the latent factors learned from the weighted ma-trix factorization, the performance of [13] is unlikelyto surpass that of the weighted matrix factorization.What we propose in this paper, on the other hand,uses content as anadditionto the weighted matrixfactorization, in a similar manner as the collabora-tive topic model described in Section 2.2. As weshow in the experiment, we are able to achieve betterresult than the weighted matrix factorization whenwe only have limited amount of user feedback.Other approaches that hybridize content and collabora-tive models include Yoshiiet al.[17], McFeeet al.[11],and Wang and Wang [15]. [17] train a three-way proba-bilistic model that joins user, item, and content by a latent“topic” variable; the model focuses on explicit feedback(user ratings). [11] take a similar approach to [13] and learna content-based similarity function from collaborative ﬁl-tering via metric learning. [15] also use a neural networkto incorporate music content into the collaborative ﬁlteringmodel. The major difference is that in [15] the output of theneural network is treated as item factor and the neural net-work is trained to minimize a collaborative-ﬁltering-basedloss function. Therefore the content model itself does nothave explicit musicological meaning.3. PROPOSED APPROACHAdopting the same structure as that of CTR, our systemconsists of two components: a content model which isbased on a pre-trained neural network and a collaborativeﬁltering model based on matrix factorization.3.1 Supervised pre-trainingInspired by the success of transfer learning in computervision which exploits deep convolutional neural networks[9], in our system we pre-train a multi-layer neural networkin a supervised semantic tagging prediction task and use itas the content model.Our training data comes from Lianget al.[10] whichconsists of 370Ktracks from the Million Song Datasetand the pre-processedlast.fmdata with a vocabulary of561 tags, including genre, mood, instrumentation, etc. Weuse the Echonest’s timbre feature, which is very similar toMFCC. To get the song-level features, we vector-quantizeall the timbre features following the standard procedure:We run thek-means algorithm on a subset of randomly se-lected training data to learnJ= 1024cluster centroids(codewords). Then for each song, we assign each segment(frame) to the cluster with the smallest Euclidean distanceto the centroid. We aggregate the VQ feature of songi(xi2RJ+) by counting the number of assignments to eachcluster across the entire song and then normalize it to haveunit`1norm to account for the various lengths.We treat music tagging as a binary classiﬁcation prob-lem: For each tag, we make independent predictions onwhether the song is tagged with it or not. We ﬁt the outputof the networkf(xi)2R561into logistic regression clas-siﬁers. Therefore, given tag labelsyit2{\u00001,1}for songiand tagt, the network is trained to minimize the followingloss:Ltag=Pi,tlog(1 + exp(\u0000yitft(xi))Here we use a network with three fully-connected hid-den layers and ReLU activations with dropout. Each layerhas 1,200 neurons. Stochastic gradient descent with mini-batch of size 100 is used with AdaGrad [3] for adjustingthe learning rate3. We notice that both dropout and Ada-Grad are crucial for getting the good performance. Thetagging performance is reported in Section 4.1.3.2 Content-aware collaborative ﬁlteringWe can interpret the output of the last hidden layerhi2RFh(hereFh= 1200) as a latent content representationof songi. Because of the way the network is trained,this latent representation is supposed to be highly corre-lated to the semantic tags (“topics” of music). Therefore,we can take a similar approach to the collaborative topicmodel and use this representation in a collaborative ﬁlter-ing model.The generative process for the proposed model is as fol-lows:•For each useru, draw user latent factor:✓u⇠N(0,\u0000\u00001✓IK).•For each songi, draw song latent factor:\u0000i⇠N(Whi,\u0000\u00001\u0000IK).•For each user-song pair(u, i), draw implicit feed-back (whether userulistened to songi):rui⇠N(✓Tu\u0000i,c\u00001ui).3The source code for training the neural network is available at:https://github.com/dawenl/deep_taggingProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 297Here the weight matrixW2RK⇥Fhtransforms the learnedcontent representation from the neural networks into thecollaborative ﬁltering latent space viaWhi. The precisionparameter\u0000\u0000balances how the song latent vector\u0000idevi-ates from the content feature. We set the conﬁdencecuiinthe same way as in Section 2.1.We want to emphasize that our proposed model iscontent-awareinstead ofcontent-based. Just like collaborative topicmodel, our proposed model is still fundamentally based oncollaborative ﬁltering. The content model is only used as aprior and can be deviated if the model thinks it is necessaryto explain the data.For notational convenience, we deﬁne the concatenateduser latent factors matrix⇥4=[✓1|···|✓U]2RK⇥Uandsong latent factors matrixB4=[\u00001|···|\u0000I]2RK⇥I.W eestimate the model parameters{⇥,B,W}via maximumaposteriori.The complete log-likelihood is written as:L=\u0000Xu,icui2(rui\u0000✓Tu\u0000i)2\u0000\u0000✓2Xu✓Tu✓u\u0000\u0000\u00002Xi(\u0000i\u0000Whi)T(\u0000i\u0000Whi)Take the gradient of the complete log-likelihood with re-spect to the model parameters and set it to 0, we can obtainthe following closed-form coordinate updates:✓u (BCuBT+\u0000✓IK)\u00001BCuru(1)\u0000i (⇥Ci⇥T+\u0000\u0000IK)\u00001(⇥Ciri+\u0000\u0000Whi)(2)WT (HTH+\u0000WIFh)\u00001HTBT(3)whereCu2RI⇥Iis a diagonal matrix withcui,i=1,···,Ias its diagonal elements, andru2RIis the feed-back for useru.Ciandriare similarly deﬁned.H2RI⇥Fhis the concatenated output from the last hidden layer[h1|···|hI]T. When updatingW, we add a small ridgeterm\u0000Wto the diagonal of the matrix to regularize andavoid numerical problems when inverting. Alternating be-tween updating⇥,B, andW, we are guaranteed to reacha stationary point of the complete log-likelihood.The same technique used in [6] to speed up computationcan be applied here. This enables us to apply our model tolarge-scale music corpus and user-item interaction, whichis not possible for CTR.After the model is trained, we can makein-matrixpre-diction byˆrui=✓Tu\u0000i. Similar to the collaborative topicmodel, we can also makeout-of-matrixprediction for songsthat no one has listened to by only using the contentˆrui=✓Tu(Whi).4. EVALUATIONWe ﬁrst evaluate our system on the pre-training tag predic-tion task to ensure the quality of the extracted features, andthen measure its recommendation performance in compar-ison with related models4.4https://github.com/dawenl/content_wmfcontains thesource code for training the proposed model and reproducing the experi-ModelPrec Recall F-scoreAROC MAPSPMF0.127 0.146 0.1360.712 0.120NNet0.184 0.207 0.1950.781 0.178Table 1: Annotation and retrieval performance on the Mil-lion Song Dataset from Poisson matrix factorization withstochastic inference (SPMF) [10] and the pre-trained neu-ral network (NNet) described in Section 3.1. The standarderror is on the order of0.01, thus not included here.4.1 Tag predictionEvaluation tasks and metricsWe evaluate the pre-trainedneural network on semantic tags with an annotation taskand a retrieval task. We use the same dataset in Lianget al.[10] from the Million Song Dataset [1] and compare withtheir result which, to our knowledge, is the state-of-the-artperformance on large-scale tag prediction. Note that weonly use tag prediction as a proxy to measure the qualityof the content model and do not argue for our approach asan optimal one to automatic music tagging.For the annotation task we seek to automatically tag un-labeled songs. To evaluate the model’s ability to annotatesongs, we compute the average per-tag precision, recall,and F-score on the held-out test set. For the retrieval task,given a query tag we seek to provide a list of songs whichare related to that tag. To evaluate retrieval performance,for each tag in the vocabulary we ranked each song in thetest set by the predicted probability. We then calculate thearea under the receiver-operator curve (AROC) and meanaverage precision (MAP) for each ranking.Tagging performance and discussionThe results are re-ported in Table 1, which show that the pre-trained neu-ral network performs signiﬁcantly better than the Poisson-factorization-based approach. This is not surprising fortwo reasons: 1) Here we treat tag prediction as a super-vised task and train a multi-layer neural network, whilein [10] the problem is formulated as an unsupervised learn-ing task to account for the uncertainty in the user-generatedtags (which incidentally can be considered as a typical ex-ample of implicit feedback). 2) Similar to LDA, Poissonfactorization can only capture linear factor, whose expres-sive power is much weaker than that of a multi-layer neuralnetwork.Nevertheless, the results conﬁrm that our pre-trainedneural network can be considered as an effective contentfeature extractor and we will use the output of the last hid-den layer as the content feature.Note that our neural network is relatively simple anddoes not directly use raw acoustic features (e.g., log-melspectrograms) as input. It is reasonable to believe that witha more complex network structure and low-level acousticfeature, we should be able to achieve better tagging per-formance and obtain a more powerful content feature ex-tractor, which could further boost the performance of ourproposed recommendation method.mental results for recommendation in Section 4.2.298 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015ModelR@40R@80R@120R@160R@200NDCGPMF [4]0.1021 0.1533 0.1908 0.2206 0.24560.2419CTPF [5]0.1031 0.1511 0.1861 0.2138 0.23700.2395WMF [6]0.1722 0.2367 0.2803 0.3133 0.33970.2881CF + shallow0.1724 0.2368 0.2803 0.3131 0.33960.2883CF + deep0.1722 0.2365 0.2800 0.3129 0.33940.2882Table 2:In-matrixperformance on the DEN subset with proposed and competing methods.4.2 RecommendationData preparationWe use the Taste Proﬁle Dataset whichis part of the Million Song Dataset to evaluate the recom-mendation performance. It contains listening history in theform of play counts from one million users with more than40 million (user, song, play count) triplets. We ﬁrst bina-rize all the play counts5and create two complementarysubsets (denoted asDENandSPR):For the DEN subset, we intend to create a reasonablydense subset so that the traditional collaborative ﬁlteringmodel will have good performance. We remove the userswho have less than 20 songs in their listening history andsongs that are listened to by less than 50 users, obtaining asubset with 613,682 users and 97,414 songs with more than38 million user-song pairs (sparsity level0.064%). For theSPR subset, on the contrary, we only keep the users whohave less than 20 songs in their listening history and songsthat are listened to by less than 50 users, yielding a highlysparse (0.002%) subset with 564,437 users and 260,345songs.We select5%of the songs from DEN (4,871) forout-of-matrixprediction. For both subsets we split20%and10%as test and validation sets, respectively. Validationset is used to select hyperparameters, as well as monitorconvergence by computing predictive likelihood.Competing methodsWe compare our proposed method(denoted asCF + deep) with weighted matrix factorization(WMF) [6], as well as the following three methods:CF + shallow: A simple baseline where we directly usethe normalized VQ featurexiin place of the feature ex-tracted from the neural networkhi. This baseline is mainlyused to demonstrate the necessity of an effective featureextractor forout-of-matrixprediction.Poisson matrix factorization (PMF) [4]: Just like WMF,PMF is a matrix factorization model for collaborative ﬁl-tering. Instead of Gaussian likelihood and priors on the la-tent factors, it utilizes Poisson likelihood model and gammapriors. The biggest advantage of PMF is computational.As shown in [4], the inference algorithm has complexitythat scales linearly with the number of non-zero entries inthe user-item matrix.Collaborative topic Poisson factorization (CTPF) [5]:This model incorporates the content information into PMFin the same way as CTR. Additionally, it is conditionallyconjugate with closed-form updates and enjoys the same5In practice, we ﬁnd that the performances using actual play countsand binarized indicators are very close for our model.computational efﬁciency as PMF. Therefore, it can be ap-plied to large-scale dataset without delicate engineering.Based on our argument in Section 2.3, we do not di-rectly compare with [13] because it is sufﬁcient to comparewith WMF. Forout-of-matrixrecommendation evaluation,we can only compare with CTPF and CF + shallow. Inall the experiments, the dimensionality of the latent spaceK= 50. We select↵=2and✏= 10\u00006to compute theconﬁdencecui. For WMF, CF + shallow, and CF + deep,the model parameters⇥,BandW(if any) are initializedto the same values.Evaluation metricsTo evaluate different algorithms, weproduce a ranked list of all the songs (excluding those inthe training and validation sets) for each user based on thepredicted preferenceˆru.Precision and recall are commonly used evaluation met-rics. However, for implicit feedback, the zeros can meaneither the user is not interested in the song or more likely,the user does not know the song. This makes the precisionless interpretable. However, since the non-zerorui’s areknown to be true positive, we instead reportRecall@M,which only considers songs within the topMin the rankedlist. For each user, the deﬁnition ofRecall@MisRecall@M=# songs that the user listened to in topMtotal # songs the user has listened to.In addition toRecall@M, we also report (untruncated)normalized discounted cumulative gain (NDCG) [7]. Un-likeRecall@Mwhich only focuses on topMsongs in thepredicted list, NDCG measures the global quality of rec-ommendation. In the meantime, it also prefers algorithmsthat place held-out test items higher in the list by applyinga discounted weight. Given a ranked list of songs from therecommendation algorithm, for each user NDCG can becomputed as follows:DCG=IXi=12reli\u00001log2(i+ 1);NDCG=DCGIDCG.Given our binarized data, the reverencereliis also binary:1 if songiis in the held-out user listening history and 0otherwise. IDCG is the optimal DCG score where all theheld-out test songs are ranked top in the list. Therefore,larger NDCG values indicate better performance.Results on the DEN subsetThe model hyperparameters\u0000✓=\u0000W= 10and\u0000\u0000= 100are selected from the valida-tion set based on NDCG. Thein-matrixandout-of-matrixperformances are reported in Table 2 and 3, respectively.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 299ModelR@40R@80R@120R@160R@200NDCGCTPF [5]0.0256 0.0700 0.1440 0.1869 0.20860.1271CF + shallow0.0503 0.0894 0.1218 0.1514 0.17780.1429CF + deep0.0910 0.1461 0.1881 0.2241 0.25500.1605Table 3:Out-of-matrixperformance on the DEN subset with proposed and competing methods.ModelR@40R@80R@120R@160R@200NDCGWMF [6]0.1137 0.1286 0.1378 0.1449 0.15050.1415CF + shallow0.1138 0.1286 0.1377 0.1449 0.15040.1416CF + deep0.1140 0.1289 0.1378 0.1451 0.15070.1417Table 4:In-matrixperformance on the SPR subset with proposed and competing methods.All the metrics are averaged across 612,232 users in theheld-out test user-item pairs.We can see that with sufﬁcient amount of user feedback,there is almost no difference in performance among WMF,CF + shallow, and CF + deep6– there is not a single modelwhich is consistently better. This is understandable, sinceboth CF + shallow and CF + deep are fundamentally col-laborative ﬁltering models. With enough user feedback,the model is able to produce meaningful recommendationwithout resorting to the content features. Moreover, CF+ shallow, which has access to more content information,does slightly better than CF + deep.One observation from Table 2 is that adding content fea-tures does not necessarily improvement the performance.Unlike CF + deep, CTPF falls behind its content-free coun-terpart PMF on bothRecall@Mand NDCG. This is pos-sibly due to the insufﬁcient feature extraction capability ofthe topic model (LDA) on the rich musical data.The superiority of CF + deep is more obvious on theout-of-matrixpredictions performance shown in Table 3.We can see a larger margin between CF + deep and CF+ shallow, as compared to their close performance onin-matrixpredictions. This suggests the importance of a pow-erful feature extractor in the absence of usage data. Evena simple linear LDA model in CTPF can be more effec-tive than CF + shallow at predicting songs that the userslistened to in the held-out test set.Results on the SPR subsetWe repeat thein-matrixeval-uation on the highly sparse SPR subset. The model hyper-parameters\u0000✓=\u0000W= 10\u00002and\u0000\u0000=1are selectedfrom the validation set. The performance is reported in Ta-ble 4. All the metrics are averaged across 564,437 users inthe held-out test user-item pairs.Again, the overall differences among all three methodsare relatively minor. However, with very limited user feed-back, both CF + shallow and CF + deep outperform thecontent-free WMF. More importantly, CF + deep consis-tently improves over CF + shallow, which indicates theimportance of an effective feature extractor.6There is little point in arguing for the statistical signiﬁcance of thedifference, since given the number of users to average over, the standarderror is vanishingly small.5. CONCLUSIONIn this paper we present a content-aware collaborative mu-sic recommendation system that joins a multi-layer neu-ral network content model with a collaborative ﬁlteringmodel. The system achieves the state-of-the-art perfor-mance in music recommendation given content and im-plicit feedback data.A possible future direction is to incorporate ranking-based loss function, e.g., the weighted approximate-rankpairwise (WARP) loss in [16] into the collaborative ﬁl-tering model. We normally evaluate recommendation al-gorithms using ranking-based metrics (e.g.Recall@Mand NDCG), but the model is trained using squared lossfunction. It would be more natural to directly optimize aranking-based loss function.6. ACKNOWLEDGEMENTSWe gratefully acknowledge the support of NVIDIA Cor-poration with the donation of the Tesla K40 GPU used forthis research.7. REFERENCES[1]Thierry Bertin-Mahieux, Daniel P.W. Ellis, BrianWhitman, and Paul Lamere. The Million Song Dataset.InISMIR, 2011.[2]David M. Blei, Andrew Y . Ng, and Michael I. Jor-dan. Latent Dirichlet allocation.the Journal of machineLearning research, 3:993–1022, 2003.[3]John Duchi, Elad Hazan, and Yoram Singer. Adaptivesubgradient methods for online learning and stochas-tic optimization.The Journal of Machine Learning Re-search, 12:2121–2159, 2011.[4]Prem Gopalan, Jake M. Hofman, and David M. Blei.Scalable recommendation with poisson factorization.arXiv preprint arXiv:1311.1704, 2013.[5]Prem K. Gopalan, Laurent Charlin, and David Blei.Content-based recommendations with Poisson factor-ization. InAdvances in Neural Information ProcessingSystems 27, pages 3176–3184. 2014.300 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[6]Yifan Hu, Yehuda Koren, and Chris V olinsky. Collab-orative ﬁltering for implicit feedback datasets. InDataMining, 2008. ICDM’08. Eighth IEEE InternationalConference on, pages 263–272. IEEE, 2008.[7]Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulatedgain-based evaluation of IR techniques.ACM Transac-tions on Information Systems (TOIS), 20(4):422–446,2002.[8]Yehuda Koren, Robert Bell, and Chris V olinsky. Ma-trix factorization techniques for recommender systems.Computer, (8):30–37, 2009.[9]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-ton. ImageNet classiﬁcation with deep convolutionalneural networks. InAdvances in neural informationprocessing systems, pages 1097–1105, 2012.[10]Dawen Liang, John Paisley, and Daniel P. W. Ellis.Codebook-based scalable music tagging with Poissonmatrix factorization. InProceedings of the Interna-tional Society for Music Information Retrieval Confer-ence, pages 167–172, 2014.[11]Brian McFee, Luke Barrington, and Gert Lanckriet.Learning similarity from collaborative ﬁlters. InIS-MIR, pages 345–350, 2010.[12]Andriy Mnih and Ruslan Salakhutdinov. Probabilisticmatrix factorization. InAdvances in neural informationprocessing systems, pages 1257–1264, 2007.[13]A¨aron van den Oord, Sander Dieleman, and BenjaminSchrauwen. Deep content-based music recommenda-tion. InAdvances in Neural Information ProcessingSystems, pages 2643–2651, 2013.[14]Chong Wang and David M. Blei. Collaborative topicmodeling for recommending scientiﬁc articles. InPro-ceedings of the 17th ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 448–456. ACM, 2011.[15]Xinxi Wang and Ye Wang. Improving content-basedand hybrid music recommendation using deep learn-ing. InProceedings of the 22nd ACM InternationalConference on Multimedia. ACM Press, 2014.[16]Jason Weston, Samy Bengio, and Nicolas Usunier. Ws-abie: Scaling up to large vocabulary image annotation.InProceedings of the International Joint Conferenceon Artiﬁcial Intelligence, IJCAI, 2011.[17]Kazuyoshi Yoshii, Masataka Goto, Kazunori Ko-matani, Tetsuya Ogata, and Hiroshi G. Okuno. Hy-brid collaborative and content-based music recommen-dation using probabilistic model with latent user pref-erences. InISMIR, 2006.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 301"
    },
    {
        "title": "How Music Alters Decision Making - Impact of Music Stimuli on Emotional Classification.",
        "author": [
            "Elad Liebman",
            "Peter Stone",
            "Corey N. White"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414908",
        "url": "https://doi.org/10.5281/zenodo.1414908",
        "ee": "https://zenodo.org/records/1414908/files/LiebmanSW15.pdf",
        "abstract": "Numerous studies have demonstrated that mood can af- fect emotional processing. The goal of this study was to explore which components of the decision process are af- fected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, partici- pants decided whether words were emotionally positive or negative while listening to music that was chosen to in- duce positive or negative mood. The behavioral results show that the music manipulation was effective, as par- ticipants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumula- tion, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individ- ual aspects of music affect response patterns. Our results have implications for future studies of the connection be- tween music and mood.",
        "zenodo_id": 1414908,
        "dblp_key": "conf/ismir/LiebmanSW15",
        "keywords": [
            "mood",
            "emotional processing",
            "decision process",
            "stochastic sequential model",
            "drift-diffusion model",
            "positive or negative",
            "music manipulation",
            "bias",
            "starting point of evidence accumulation",
            "response patterns"
        ],
        "content": "HOW MUSIC ALTERS DECISION MAKING - IMPACT OF MUSICSTIMULI ON EMOTIONAL CLASSIFICATIONElad LiebmanComputer Science DepartmentThe University of Texas at Austineladlieb@cs.utexas.eduPeter StoneComputer Science DepartmentThe University of Texas at Austinpstone@cs.utexas.eduCorey N. WhiteDepartment of PsychologySyracuse Universitycnwhite@syr.eduABSTRACTNumerous studies have demonstrated that mood can af-fect emotional processing. The goal of this study was toexplore which components of the decision process are af-fected when exposed to music; we do so within the contextof a stochastic sequential model of simple decisions, thedrift-diffusion model (DDM). In our experiment, partici-pants decided whether words were emotionally positive ornegative while listening to music that was chosen to in-duce positive or negative mood. The behavioral resultsshow that the music manipulation was effective, as par-ticipants were biased to label words positive in the positivemusic condition. The DDM shows that this bias was drivenby a change in the starting point of evidence accumula-tion, which indicates an a priori response bias. In contrast,there was no evidence that music affected how participantsevaluated the emotional content of the stimuli. To betterunderstand the correspondence between auditory featuresand decision-making, we proceeded to study how individ-ual aspects of music affect response patterns. Our resultshave implications for future studies of the connection be-tween music and mood.1. INTRODUCTIONThere is robust evidence that one’s mood can affect howone processes emotional information. This phenomenonis often referred to as mood-congruent processing or bias,reﬂecting the ﬁnding that positive mood induces a relativepreference for positive emotional content (and vice versa).The goal of the present study was to use a popular modelof simple decisions, the drift-diffusion model (DDM; [9]),to explore how music-induced mood affects the differentcomponents of the decision process that could drive mood-congruent bias. The model, described below, can differen-tiate two types of bias: a) Bias due to an a priori preferencefor one response over the other; and b) Bias due to a shiftin how the stimuli are evaluated for decision making. Thisc\u0000Elad Liebman, Peter Stone, Corey N. White.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Elad Liebman, Peter Stone, Corey N.White. “How Music Alters Decision Making - Impact of Music Stimulion Emotional Classiﬁcation”, 16th International Society for Music Infor-mation Retrieval Conference, 2015.class of models has been successfully employed to differ-entiate these biases in perceptual and memory tasks, but toour knowledge has never been used to investigate effects ofmusic on emotional classiﬁcation. We consider the follow-ing to be our key contributions: a) We provide meaningfulevidence that decision making is indeed affected by musicstimuli, and analyze the observed effects; b) we study evi-dence of how speciﬁc auditory features are correlated withaspects of decision making.Studies that induce mood, either through listening tohappy/sad music or having participants write passages orsee pictures based on a particular emotion, have shownmood-congruent bias across a range of tasks. Behen etal. [4] showed participants happy and sad faces whilethey listened to positively- or negatively valenced musicand underwent fMRI. Participants rated the happy facesas more happy while listening to positive music, and thefMRI results showed that activation of the superior tempo-ral gyrus was greater when the face and music were con-gruent with each other. In a study of mood and recall,De l’Etoile [3] found that participants could recall signiﬁ-cantly more words when mood was induced (through mu-sic) at both encoding and retrieval. Similarly, Kuhbandnerand Pekrun [6] had participants study emotional words thatwere printed in either black, red, green, or blue, with thehypothesis that congruent words (e.g., negative words inred, positive words in green) would show enhanced mem-ory at test. Their ﬁndings supported the hypothesis, asmemory was better for negative words shown in red andpositive words shown in green.Previous work at the intersection of musicology andcognitive science has also studied the connection betweenmusic and emotion. As Krumhansel points out [5], emo-tion is a fundamental part of music understanding and ex-perience, underlying the process of building tension andexpectations. There is neurophysical evidence of musicbeing strongly linked to brain regions linked with emotionand reward [1], and different musical patterns have beenshown to have meaningful associations to emotional affec-tations [8]. Similarly, studies have indicated that mood alsoaffects the perception of music [12]. Not only is emotiona core part of music cognitive processing, it can also havea resounding impact on people’s mental state, and aid inrecovery, as shown for instance by Zumbansen et al. [15]in the case of people suffering from Brocas aphasia. Peo-ple regularly use music to alter their moods, and evidence793has been presented that music can alter the strength ofemotional negativity bias [2]. All this evidence indicatesa deep and profound two-way connection between musicand emotional perception.The structure of the paper is as follows. In Section 2we outline the characteristics of the drift-diffusion model,which we use in this study. In Section 3 we discuss ourexperimental design and how data was collected from par-ticipants. In Section 4 we present and analyze the results ofour behavioral study. In Section 5 we further analyze howindividual auditory components correlate with the behav-ioral patterns observed in our human study. In Section 6we recap our results and discuss them in a broader context.2. THE DRIFT-DIFFUSION MODELThis study employs the DDM of simple decisions to re-late observed decision behavior to the underlying decisioncomponents. The DDM, shown in Figure 1, belongs to abroader class of evidence accumulation models that positsimple decisions involve the gradual sequential accumu-lation of noisy evidence until a criterial level is reached.In the model, the decision process starts between the twoboundaries that correspond to the response alternatives.Evidence is accumulated over time to drive the process to-ward one of the boundaries. Once a boundary is reached,it signals a commitment to that response. The time takento reach the boundary denotes the decision time, and theoverall response time is given by the decision time plus thetime required for processes outside the decision processlike encoding and motor execution. The model includes aparameter for this nondecision time (Ter), to account forthe duration of these processes.The primary components of the decision process in theDDM are the boundary separation, the starting point, andthe drift rate. Boundary separation provides an index ofresponses caution or speed/accuracy settings; wide bound-aries indicate a cautious response style where more evi-dence needs to be accumulated before the choice is made.The need for more evidence makes the decision processslower, but also more accurate as it is less likely to hit thewrong boundary by mistake. The starting point of the dif-fusion process (z), indicates whether there is a responsebias. If z is closer to the top boundary, it means less evi-dence is required to reach that boundary, so “positive” re-sponses will be faster and more probable than “negative”responses. Finally, the drift rate (v) provides an index ofthe evidence from the stimulus that drives the accumu-lation process. Positive values indicate evidence for thetop boundary, and negative values for the bottom bound-ary. Further, the absolute value of the drift rate indexesthe strength of the stimulus evidence, with larger valuesindicating strong evidence and leading to fast and accurateresponses.In the framework of the DDM, there are two mecha-nisms that can drive behavioral bias. Changes in the start-ing point (z) reﬂect a response expectancy bias, wherebythere is a preference for one response even before the stim-ulus is shown [7,14]. Experimentally, response expectancy\nFigure 1. An Illustration of the Drift-Diffusion Model.bias is observed when participants have an expectation thatone response is more likely to be correct and/or rewardedthan the other. In contrast, changes in the drift rate (v) re-ﬂect a stimulus evaluation bias, whereby there is a shiftin how the stimulus is evaluated to extract the decisionevidence. Experimentally, stimulus evaluation bias is ob-served when there is a shift in the stimulus strength and/orthe criterion value used to classify the stimuli. Thus re-sponses expectancy bias, reﬂected by the starting point inthe DDM, indicates a shift in how much evidence is re-quired for one response relative to the other, whereas stim-ulus evaluation bias, reﬂected by a shift in the drift rates inthe DDM, indicates a shift in what evidence is extracted bythe stimulus under consideration. Importantly, both mech-anisms can produce behavioral bias (faster and more prob-able responses for one choice), but they differentially af-fect the distribution of response times. In brief, responseexpectancy bias only affects fast responses, whereas stim-ulus evaluation bias affects both fast and slow responses(see [14]). It is this differential effect on the RT distribu-tions that allow the DDM to be ﬁtted to behavioral datato estimate which of the two components, starting pointor drift rates, is driving the bias observed in the RTs andchoice probabilities. The DDM has been shown to suc-cessfully differentiate these two bias mechanisms from be-havioral data in both perceptual and recognition memorytasks [14].This study used the DDM approach described above toinvestigate how music-induced mood affects the differentdecision components when classifying emotional informa-tion. Participants listened to happy or sad music while de-ciding if words were emotionally positive or negative. TheDDM was then ﬁtted to each participant’s behavioral datato determine whether the mood induction affected responseexpectancy bias, stimulus evaluation bias, or both.3. METHODSParticipants were shown words on the computer screen andasked to classify them as emotionally positive or negativewhile listening to music. The words were emotionally pos-itive, negative, or neutral. After a ﬁxation cue was shownfor 500 ms, each word was presented in the center of the794 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015screen and remained on screen until a response was given.If no response was given after 3 seconds, the trial endedas a “no response” trial. Responses were indicated withthe “z” and “/” keys, and mapping between the key and re-sponse was counterbalanced across participants. The taskconsisted of 4 blocks of 60 trials with 20 stimuli from eachword condition (positive, negative, neutral). A differentsong was played during each block, alternating from posi-tive to negative music across blocks. The order of the songswas counterbalanced across subjects. The entire experi-ment lasted less than 30 minutes. To ensure that the resultswere not speciﬁc to the particular choice of songs, the en-tire experiment was replicated with a new set of music.The stimuli consisted of emotionally positive (e.g., suc-cess, happy), negative (e.g., worried, sad), and neutralwords (e.g., planet, sipped) taken from a previous study[13]. There were 96 words for each stimulus condition,which were matched for word frequency and letter length.From each wordpool, 80 items were randomly chosen foreach participant to use in the task. Words were randomlyassigned to appear in the positive or negative music blockswith the constraint that 20 of each word type appeared inevery block of trials.Publicly available music was surveyed to isolate twoclear types - music that is characterized by slow tempo,minor keys and somber tones, typical to traditionally “sad”music, and music that has upbeat tempo, major scales andcolorful tones, which are traditionally considered to be typ-ical to “happy” music. Our principal concern in selectingthe musical stimuli, rather than their semantic categoriza-tion as either happy or sad, was to curate two separate“pools” of music sequences that were broadly character-ized by a similar temperament (described above), and showthey produced consistent response patterns.To ensure that the selected music was effective for in-ducing the appropriate mood, a separate set of participantsrated each piece of music on a 7-point Likert scale, with 1indicating negative mood and 7 indicating positive mood.There were 21 participants that rated the songs for Exper-iment 1, and 19 participants for Experiment 2. This moodassessment was done outside of the main experiment toeliminate the possibility that the rating procedure wouldinﬂuence the participants’ classiﬁcation behavior in theprimary task. The ratings showed that the music choiceswere appropriate. The positive songs in Experiment 1 ledto more positive ratings than the negative songs. Simi-lar results were found for the songs in Experiment two,with higher ratings for the positive songs than the negativesongs. The differences between the positive and negativesong ratings were highly signiﬁcant for both experiments(p-values<.001using a paired t-test, witht(20)>7.3).The means and standard deviations of the scores for thesongs in the two experiments are presented in table1.The DDM was ﬁtted to each participant’s data, sepa-rately for positive and negative music blocks, to estimatethe values of the decision components. The data enteredinto the ﬁtting routine were the choice probabilities andresponse time (RT) distributions (summarized by the .1,—Experiment 1— —Experiment 2—song average SD average SDhappy 1 5.14 1.24 5.15 1.29happy 2 5.00 1.22 5.42 1.17sad 1 2.24 1.00 2.26 1.24sad 2 2.33 0.97 2.11 0.99Table 1. Aggregated Likert scale results for the 8 songsused in the two experiments..3, .5, .7, and .9 quantiles) for each response option andstimulus condition. The parameters of the DDM were ad-justed in the ﬁtting routine to minimize the\u00002value, whichis based on the misﬁt between the model predictions andthe observed data (see [10]). For each participant’s dataset, the model estimated a value of boundary separation,nondecision time, starting point, and a separate drift ratefor each stimulus condition. Because of the relatively lownumber of observations used in the ﬁtting routine, the vari-ability parameters of the full DDM were not estimated(see [9]). This resulted in two sets of DDM parametersfor each participant, one for the positive music blocks andone for the negative music blocks.4. RESULTSThe RTs and choice probabilities in Figure 2 show thatthe mood-induction successfully affected emotional bias.The left column shows the response probabilities, and theright column shows an RT-based measure of bias, whichis taken as the median RT for negative responses minusthe median RT for positive responses for each condition.Thus RT values above 0 indicate faster positive than neg-ative responses for that condition, and vice-versa. In Ex-periment 1 (top row), happy music led to more “positive”responses overall. This difference was signiﬁcant for neu-tral words and positive words, but not for negative words.For RTs, positive responses were generally faster than neg-ative responses in the happy compared to sad music condi-tions, though the difference was only signiﬁcant for posi-tive words . The results from Experiment 2 largely mirrorthose from Experiment 1. Participants were more likely torespond “positive” in the happy music condition. This dif-ference was signiﬁcant for the negative and neutral words,but not the positive words (though there is a trend in thatdirection). Likewise, positive responses were relativelyfaster than negative responses in the happy compared tosad music conditions, though the difference was only sig-niﬁcant for neutral and positive words.Overall, the behavioral data show that the mood induc-tion was effective in inﬂuencing participants’ emotionalclassiﬁcation: positive responses were more likely andfaster in the happy compared to sad music condition. Thesebehavioral data are next decomposed with the DDM.Figure 3 shows the DDM parameters for each exper-iment. Although the two bias-related measures (startingpoint and drift rates) are of primary interest, all of theDDM parameters were compared across music conditions.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 795Figure 2. Response patterns for the two experiments. 1st column shows proportions of classiﬁcation for the three wordtypes. 2nd column shows normalized response time difference between positive and negative classiﬁcations for the threeword types. X marks the sad music condition, O marks the happy music condition.\nFigure 3. DDM ﬁtted parameters - boundary separation, nondecision time, response bias and stimulus bias. X marksthe sad music condition, O marks the happy music condition. Response bias indicates a statistically signiﬁcant differencebetween the sad and happy music conditions.796 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015It is possible that the different music conditions could af-fect response caution and nondecision time. For example,the slower tempo of the sad songs could lead participantsto become more cautious and have slower motor executiontime. Thus all parameters were investigated. As the leftcolumns of Figure 3 shows, the music conditions did notdifferentially affect response caution or encoding/motortime, as neither boundary separation nor nondecision timediffered between happy and sad music blocks. Of primaryinterest were the starting point and drift rate parameters,which provide indices of response expectancy and stimu-lus evaluation bias, respectively. For starting point, therewas a signiﬁcant shift in response bias for both experi-ments, with participants favoring the “positive” responsemore heavily in the happy compared to sad music. Thisindicates that the music induced an a priori bias for oneresponse over the other. In contrast, the music conditionshad no reliable effect on the drift rates for positive, neg-ative, or neutral words. Thus music did not inﬂuence thestimulus evaluation of the items. The DDM results showthat the music-based manipulation of mood had a targetedeffect on the starting point measure, which reﬂects an apriori response expectancy bias. There were no effects ofmusic on response caution, nondecision time, or drift rates(stimulus evaluation bias). Thus the results show that themood-congruent bias was driven by a change in partici-pants’ expectancy about the appropriate response, ratherthan a change in how the emotional content of the wordswas evaluated.5. CORRELATING RESPONSES AND MUSICALFEATURESThe partition between “positive” and “negative” mood-inducing songs is intuitively understandable, and is indeedsufﬁcient in order to observe the effects discussed in theprevious section. This partition, however, is still some-what arbitrary. It is of interest then to identify, on a morefundamental auditory level, how speciﬁc aspects of mu-sic affect response patterns. To this end, we consideredthe 8 musical segments used in this experiment, extractedkey auditory features which we assume are relevant to themood partitioning, and examined how they correlate withthe participant responses we observed.5.1 Extracting Raw Auditory FeaturesWe focused on three major auditory features: a) overalltempo; b) overall “major” vs. “minor” harmonic character;c) average amplitude. Features (a) and (c) were computedusing the Librosa library [11]. To compute feature (b), weimplemented the following procedure. For each snippet of20 beats an overall spectrum was computed and individualpitches were extracted. Then, for that snippet, accordingto the amplitude intensity of each extracted pitch, we iden-tify whether the dominant harmonic was major or minor.The major/minor score was deﬁned to be the proportion ofmajor snippets out of the overall song sequence. We caneasily conﬁrm that these three features were indeed asso-ciated with our identiﬁcation as “positive” vs. “negative”.Having labeled “positive” and “negative” as 1 and 0 re-spectively, we observed a Pearson correlation of0.7\u00000.8with p-values0.05between these features and the la-bel. Signiﬁcance was further conﬁrmed when we appliedan unpaired t-test for each feature for positive vs. negativesongs (p-values<.05,|t(3)|>3).5.2 Processing Participant ResponsesFor each observed song we ﬁrst aggregated all relevantsubject responses. We focused on three measurements -time delay for classifying positive words as positive, timedelay for classifying negative words as negative, and likeli-hood of classifying neutral words as positive. Time delayswere normalized to a z-score per user. This alternative per-spective helps verify the robustness of the effects observedin the previous section. Following this analysis step, weproceeded to ﬁt the DDM parameter decomposition as wedid in sections 3 and 4, but rather than for each song con-dition (“sad”/“happy”), to each song separately.5.3 Observed CorrelationsIn this section we consider the effects observed whenanalyzing response patterns with respect to each of thethree auditory features discussed in the previous subsec-tions. Only statistically signiﬁcant correlations are re-ported, though it’s worth noting that with a relatively smallsample size in terms of songs, potentially meaningful ef-fects might be missed due to outliers.5.3.1 Correlation with Response Times and BiasWhen we consider how the three auditory features corre-spond with the normalized delays when classifying posi-tive or negative words as such, we see an interesting pat-tern. For all three features, there was a statistically sig-niﬁcant negative correlation (p-value0.05) between theaverage normalized response time and the feature values.Intuitively speaking, the faster the song was, the louder itwas, or the more it was major in mode overall, the fasterpeople classiﬁed positive words as positive (see Figures 4a-4c). However, no such clear correlation was observed fornegative songs. This observation supports our key ﬁnd-ing when using the drift-diffusion model, that participantswere biased to label words positive in the positive musiccondition. When we analyzed the likelihood of associatingneutral words as positive with respect to each auditory fea-ture, the only effect that is borderline signiﬁcant (p-value0.1) is the correspondence between major mode domi-nance and the likelihood of associating a neutral word aspositive (the more major-mode oriented the song is, themore likely people are to associate neutral words as posi-tive) - see Figure 4d.5.3.2 Correlation with DDM DecompositionWe analyzed the correlation between the extracted audi-tory features and the DDM parameters ﬁtted for each songseparately: nondecision time, response caution, responsebias, and stimulus evidence (drift rate) for each word type.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 797Figure 4. Scatter plots reﬂecting the correlation betweenmusical features and response patterns: (a) average tempo(BPM) vs the normalized average delay in classifying pos-itive words; (b) average amplitude vs. normalized averagedelay in classifying positive words; (c) % of major-modeharmonies vs. normalized average delay in classifying pos-itive words. (d) % of major-mode harmonies vs. the likeli-hood of associating neutral words as positive.We found a statistically signiﬁcant correlation (r=0.7\u00000.8,p <0.05) between the major dominance feature andthe bias and positive drift rate parameters (see Figures 5a,5b). A borderline correlation (r=0.62,p <0.1) was ob-served between major dominance and the neutral drift rate.These ﬁndings support the previous observations in the pa-per. Interestingly, we’ve also observed a borderline signif-icant negative correlation (r=\u00000.67;p<0.1) betweenmean amplitude and response caution, implying people areless cautious the louder the music gets (see Figure 5c).6. DISCUSSIONThere is great interest in understanding how music affectsemotional processing. This study advances our under-standing of this relationship through the use of the drift-diffusion model, which was used to decompose the behav-ioral data into meaningful psychological constructs. Par-ticipants classiﬁed words as emotionally positive or nega-\nFigure 5. (a) Scatter plot of the correlation between thepercentage of major-mode harmonies (major dominance)in a song and the bias component of the DDM. (b) Scatterplot of the correlation between the percentage of major-mode harmonies (major dominance) in a song and the stim-ulus evidence component (drift rate) for positive words inthe DDM. (c) Scatter plot of the correlation between theaverage amplitude of a song and the response caution com-ponent of the DDM.tive while listening to music that induced a happy or sadmood. The behavioral data showed small, but reliable ef-fects of mood congruent emotional bias based on the mu-sic conditions. The DDM analysis of those data showedthat music-induced mood had a targeted effect on the de-cision components, affecting response expectancy bias butnot stimulus evaluation bias, response caution, or encod-ing/motor time. Further analysis of how speciﬁc musicaltraits correspond with response patterns conﬁrmed theseﬁndings and led to interesting additional observations.These results suggest that music-induced mood does notsigniﬁcantly affect how participants evaluate the emotionalcontent of the stimuli, but rather it affects how they favorone response option independent of the actual stimulus un-der consideration. In other words, a negative word is justas negative while listening to sad compared to happy mu-sic, even though the classiﬁcation behavior differs. Thusthe mood-congruent bias appears to be driven more by theselection of the response, rather than the emotional pro-cessing of the stimulus. The distinction between these twoprocesses is only identiﬁable through the DDM analysis,as it can capitalize on the RT distributions to dissociate thetwo decision components.AcknowledgmentsThis work has taken place in the Brain and Behavior Lab,The Department of Psychology, Syracuse University, andin the Learning Agents Research Group (LARG) at the Ar-tiﬁcial Intelligence Laboratory, The University of Texasat Austin. LARG research is supported in part by grantsfrom the National Science Foundation (CNS-1330072,CNS-1305287), ONR (21C184-01), AFRL (FA8750-14-1-0070), AFOSR (FA9550-14-1-0087), and Yujin Robot.798 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1] Anne J Blood and Robert J Zatorre. Intensely pleasur-able responses to music correlate with activity in brainregions implicated in reward and emotion.Proceedingsof the National Academy of Sciences, 98(20):11818–11823, 2001.[2] Jie Chen, Jiajin Yuan, He Huang, Changming Chen,and Hong Li. Music-induced mood modulates thestrength of emotional negativity bias: An erp study.Neuroscience Letters, 445(2):135–139, 2008.[3] Shannon K de lEtoile. The effectiveness of music ther-apy in group psychotherapy for adults with mental ill-ness.The Arts in Psychotherapy, 29(2):69–78, 2002.[4] Jeong-Won Jeong, Vaibhav A Diwadkar, Carla DChugani, Piti Sinsoongsud, Otto Muzik, Michael E Be-hen, Harry T Chugani, and Diane C Chugani. Con-gruence of happy and sad emotion in music and facesmodiﬁes cortical audiovisual activation.NeuroImage,54(4):2973–2982, 2011.[5] Carol L Krumhansl. Music: A link between cognitionand emotion.Current Directions in Psychological Sci-ence, 11(2):45–50, 2002.[6] Christof Kuhbandner and Reinhard Pekrun. Joint ef-fects of emotion and color on memory.Emotion,13(3):375, 2013.[7] Martijn J Mulder, Eric-Jan Wagenmakers, Roger Rat-cliff, Wouter Boekel, and Birte U Forstmann. Bias inthe brain: a diffusion model analysis of prior probabil-ity and potential payoff.The Journal of Neuroscience,32(7):2335–2343, 2012.[8] S´ebastien Paquette, Isabelle Peretz, and Pascal Belin.The musical emotional bursts: a validated set of mu-sical affect bursts to investigate auditory affective pro-cessing.Frontiers in psychology, 4, 2013.[9] Roger Ratcliff and Gail McKoon. The diffusion deci-sion model: theory and data for two-choice decisiontasks.Neural computation, 20(4):873–922, 2008.[10] Roger Ratcliff and Francis Tuerlinckx. Estimating pa-rameters of the diffusion model: Approaches to dealingwith contaminant reaction times and parameter vari-ability.Psychonomic bulletin & review, 9(3):438–481,2002.[11] Brian McFee ; Matt McVicar ; Colin Raf-fel ; Dawen Liang ; Douglas Repetto. Librosa.https://github.com/bmcfee/librosa, 2014.[12] Jonna K Vuoskoski and Tuomas Eerola. The role ofmood and personality in the perception of emotionsrepresented by music.Cortex, 47(9):1099–1106, 2011.[13] Corey N White, Aycan Kapucu, Davide Bruno,Caren M Rotello, and Roger Ratcliff. Memory biasfor negative emotional words in recognition memoryis driven by effects of category membership.Cognition& emotion, 28(5):867–880, 2014.[14] Corey N White and Russell A Poldrack. Decompos-ing bias in different types of simple decisions.Journalof Experimental Psychology: Learning, Memory, andCognition, 40(2):385, 2014.[15] Anna Zumbansen, Isabelle Peretz, and Sylvie H´ebert.The combination of rhythm and pitch can accountfor the beneﬁcial effect of melodic intonation therapyon connected speech improvements in brocas aphasia.Frontiers in human neuroscience, 8, 2014.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 799"
    },
    {
        "title": "Comparative Analysis of Orchestral Performance Recordings: An Image-Based Approach.",
        "author": [
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416248",
        "url": "https://doi.org/10.5281/zenodo.1416248",
        "ee": "https://zenodo.org/records/1416248/files/LiemH15.pdf",
        "abstract": "Traditionally, the computer-assisted comparison of mul- tiple performances of the same piece focused on perfor- mances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and artic- ulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While dif- ferences between interpretations in this genre may include commonly studied features on timing, dynamics and ar- ticulation, the timbre of the orchestra and choices of bal- ance within the ensemble are other important aspects dis- tinguishing different orchestral interpretations from one another. While it is hard to model these higher-level as- pects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Compo- nents Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pair- wise comparison, relative differences can be studied be- tween multiple performances in a corpus at once.",
        "zenodo_id": 1416248,
        "dblp_key": "conf/ismir/LiemH15",
        "keywords": [
            "computer-assisted comparison",
            "multiple performances",
            "same piece",
            "single instruments",
            "strong bias",
            "piano performances",
            "expressive performance features",
            "orchestra performances",
            "different orchestras",
            "different conductors"
        ],
        "content": "COMPARATIVE ANALYSIS OF ORCHESTRAL PERFORMANCERECORDINGS: AN IMAGE-BASED APPROACHCynthia C. S. LiemDelft University of TechnologyMultimedia Computing Groupc.c.s.liem@tudelft.nlAlan HanjalicDelft University of TechnologyMultimedia Computing Groupa.hanjalic@tudelft.nlABSTRACTTraditionally, the computer-assisted comparison of mul-tiple performances of the same piece focused on perfor-mances on single instruments. Due to data availability,there also has been a strong bias towards analyzing pianoperformances, in which local timing, dynamics and artic-ulation are important expressive performance features. Inthis paper, we consider the problem of analyzing multipleperformances of the same symphonic piece, performed bydifferent orchestras and different conductors. While dif-ferences between interpretations in this genre may includecommonly studied features on timing, dynamics and ar-ticulation, the timbre of the orchestra and choices of bal-ance within the ensemble are other important aspects dis-tinguishing different orchestral interpretations from oneanother. While it is hard to model these higher-level as-pects as explicit audio features, they can usually be notedvisually in spectrogram plots. We therefore propose amethod to compare orchestra performances by examiningvisual spectrogram characteristics. Inspired by eigenfacesin human face recognition, we apply Principal Compo-nents Analysis on synchronized performance fragments tolocalize areas of cross-performance variation in time andfrequency. We discuss how this information can be usedto examine performer differences, and how beyond pair-wise comparison, relative differences can be studied be-tween multiple performances in a corpus at once.1. INTRODUCTIONA written notation is not the ﬁnal, ultimate representa-tion of music. As Babbitt proposed, music can be rep-resented in the acoustic (physical), auditory (perceived)and graphemic (notated) domain, and as Wiggins noted,in each of these, projections are observed of the abstractand intangible concept of ‘music’ [29]. In classical mu-sic, composers usually write down a notated score. Subse-quently, in performance, multiple different musicians willpresent their own artistic reading and interpretation of it.c\u0000Cynthia C. S. Liem, Alan Hanjalic.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Cynthia C. S. Liem, Alan Hanjalic.“Comparative analysis of orchestral performance recordings: an image-based approach”, 16th International Society for Music Information Re-trieval Conference, 2015.Nowadays, increasing amounts of digital music record-ings become available. As a consequence, for musicalpieces, an increasing amount of (different) recorded per-formances can be found. Therefore, in terms of dataavailability, increasing opportunities emerge to study andcompare different recordings of the same piece. Beyondthe Music Information Retrieval (Music-IR) domain, thiscan serve long-term interests in psychology and cogni-tion on processes and manifestations of expressive playing(e.g. [6, 21, 26]), while the analysis of performance stylesand schools also is of interest to musicologists [5, 16].In this paper, we mostly are interested in the analysisof multiple performances of the same piece from a searchengine and archive exploration perspective. If one is look-ing for a piece and is confronted with multiple alterna-tive performances, how can technology assist in givingoverviews of main differences between available perfor-mances? Given a corpus, are certain performances verysimilar or dissimilar to one another?In contrast to common approaches in automated analy-sis of multiple performances, we will not depart from ex-plicit modeling of performance parameters from a signal.Instead, we take a more holistic approach, proposing toconsider spectrogram images. This choice has two rea-sons: ﬁrst of all, we are particularly interested in ﬁndingmethods for comparative analysis of orchestra recordings.We conjecture that the richness of orchestra sounds is bet-ter captured in spectrogram images than in mid-level audiofeatures. Secondly, as we will demonstrate in this paper,we believe spectrogram images offer interpretable insightsinto performance nuances.After discussing the state of the art in performance anal-ysis in Section 2, in Section 3, we will further motivateour choice to compare performances through visual com-parison of spectrogram images. Subsequently, Section 4details our chosen comparison method, after which wepresent the experimental setup for this paper in Section 5.We will then illustrate our approach and its outcomesthrough a case study in Section 6, with a detailed discus-sion of selected musically meaningful examples. This isfollowed by a discussion on how our method can assistcorpus-wide clustering of performances in Section 7, af-ter which the Conclusion will be presented.3022. STATE-OF-THE-ART REVIEWA lot of work exists on analyzing musical performanceexpressivity. In several cases, establishing models forcomputer-rendered expressive performances was the ulti-mate goal (e.g. see [10,11]). Other works focused on iden-tifying reasons behind performance expressivity, includinglower-level perceptual processes [21]; varying score edi-tions, individual treatments of ornamentation and pedaling,and music-theoretic notions of expectation and tension-relaxation [20]; generative rules, emotional expression,random variability, motion principles and stylistic unex-pectedness [14]; and musical structure [9, 13, 20]. His-torically, the analysis of musical performance strongly fo-cused on expressivity in piano playing (e.g. [6, 20–22]).The few exceptions to this rule focused on violin perfor-mance (e.g. [4]), movement in clarinet players (e.g. [8]),and performance of trained and untrained singers (e.g. [7],inspired by [26]), but to the best of our knowledge, no sys-tematic comparative studies have been performed consid-ering larger ensembles.A reason for the general bias towards piano perfor-mance may be that digital player pianos (e.g. the YamahaDisklavier) allow a very precise recording of mechanicalperformance parameters. When such parameters are avail-able, inter-onset-intervals (IOIs), expressing the time be-tween subsequent onsets, are frequently studied. Other-wise, performance parameters have to be extracted or an-notated from the audio signal. As a piano has a discretepitch set and percussive mechanics, expressive possibil-ities for a pianist are restricted to timing, dynamics andarticulation. As a consequence, audio-based performanceanalysis methods usually focus on local timing and dynam-ics. Since it is not trivial to ﬁnd a suitable time unit forwhich these parameters should be extracted, supervised orsemi-supervised methods often have been applied to ob-tain this, e.g. by departing from manually annotating beatlabels (e.g. [24, 25]). However, it is hard (if not infeasi-ble) to realize such a (semi-)supervised approach at scale.Therefore, while a very large corpus of recorded ChopinMazurkas exists, in practice only the Mazurkas for whichannotated beat information exists have been studied in fur-ther depth (e.g. [15, 19, 24, 25]).Alternatively, in [17, 18] an unsupervised approach forcomparing Mazurka recordings was proposed which doesnot rely on explicitly modeled higher-level performanceparameters or semantic temporal units, but rather on align-ment patterns from low-level short-time frame analyses.As such, this approach would be scalable to a larger cor-pus. Furthermore, while the choice of not adopting explicitperformance parameters makes evaluation of a clear-cutground truth less trivial, at the same time it allows for anysalient variations to emerge automatically from the analy-sis. The work of this paper follows a similar philosophy.3. MOTIVATION FOR SPECTROGRAM IMAGESIn this paper, we focus on the comparative analysis of or-chestra recordings. An orchestra involves a mix of many\n(a) Georg Solti, Chicago Symphony Orchestra, 1973.\n(b) Nikolaus Harnoncourt, Chamber Orchestra of Europe, 1990.Figure 1. Beethoven’s Eroica symphony, 2nd movement,spectrogram of bars 56-60 for two different interpretations.instruments. Hence, the overall orchestral sound is richerthan that of a piano, although individual beat placings andnote onsets will be much smoother. Given the multitudeof involved players, an orchestra needs guidance by a con-ductor. Due to this coordinated setup, there is less roomfor individual freedom in both local dynamics and tempothan in Romantic piano music repertoire. Thus, while lo-cal tempo deviations still occur in orchestral recordings,one cannot expect these to reﬂect performer individualityas strongly as for example in the case of Chopin Mazurkas.At the same time, in terms of timbre, balance and phras-ing articulation, a conductor has a much richer palette thanisolated instruments can offer. These aspects are not trivialto explicitly model or interpret from audio signals. How-ever, relevant information may be reﬂected in recordingspectrograms, as illustrated in Figure 1. While it is hardto point out individual instruments, a spectrogram can vi-sually reveal how rich the overall sound is, where signalenergy is concentrated, and if there are any salient soundquality developments over time, such as vibrato notes.Indeed, spectrograms are commonly used in audio edit-ing tools for visualization, navigation and analysis pur-poses. In an ethnographic study of musicologists studyinghistorical recordings, it further was shown that examina-tion of the spectrogram helped musicologists in discover-ing and listening to performance nuances [1]. Therefore,regarding potential end users of performance analysis andexploration tools, spectrogram images may be more fa-miliar and interpretable than reduced mid-level represen-tations such as chroma.4. METHODOur proposed analysis method for spectrogram imagesis inspired by the eigenfaces method of Turk and Pent-land [27], which was originally proposed in the contextof human face recognition. Since human faces sharemany common features, by applying Principal Compo-nents Analysis (PCA) on a dataset of aligned facial im-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 303ages, a set of basis images (‘eigenfaces’) can be found,explaining most of the variability found in the face dataset.While PCA has previously been applied as a tool in musi-cal performance analysis [23], this analysis was performedon annotation-intensive IOI data. In contrast, our analy-sis considers information which only requires alignmentof different fragments (as will described in Section 5), butno further manual annotation effort.We apply the same principle to a set ofNspectrogramimages for a time-aligned music fragment, as representedbyNdifferent recordings. Each spectrogram imagexis(i·j)pixels in size. We treat each pixel in the image as afeature; as such,xis a vector of lengthi·j. We collect allspectrogram images in an(N⇥(i·j))matrixX.By applying PCA, we decomposeXinto an(N⇥N)matrix of principal component loadingsWand an((i·j)⇥N)matrix of principal components scoresT.Xcanbe reconstructed by performingX=T·WT.Since the PCA is constructed such that principal com-ponents are ordered in descending order of variance, di-mension reduction can be applied by not using the fullTandW, but only the ﬁrstLcolumns of both.The component scores inTcan now be interpreted andvisualized as basis images, each representing a linear com-ponent explaining part of the variability in the dataset.5. EXPERIMENTAL SETUPUnfortunately, no standardized corpora on multiple per-formances of the same orchestra piece exist.1Further-more, no clear-cut ground truth exists of performance sim-ilarity. We therefore consider a dataset collected for thePHENICX2project, consisting of 24 full-length record-ings of Beethoven’s Eroica symphony, as well as 7 record-ings of the Alpensinfonie by Richard Strauss. In theBeethoven dataset, 18 different conductors and 10 orches-tras are featured (with a major role for the recording cat-alogue of the Royal Concertgebouw Orchestra (RCO)),meaning that the same conductor may conduct multiple or-chestras, or even the same orchestra at different recordingmoments. While metadata and audio content are not fullyidentical, in two cases in the dataset (Harnoncourt, Cham-ber Orchestra of Europe (COE) 1990 and 1991; Haitink,London Symphony Orchestra (LSO) 2005 (⇥2)), thereare suspicions that these near-duplicates pairs consider thesame original recording. In the Strauss dataset, 6 con-ductors and 6 orchestras are featured: Haitink conductsboth the RCO and LSO, and the RCO is represented oncemore with Mariss Jansons as conductor. The oldest (Men-gelberg, RCO, 1940) and newest (Fischer, RCO, 2013)recordings are both featured in the Beethoven dataset.We will demonstrate insights from the PCA spectro-gram analysis in two ways: (1) by highlighting severalanalysis examples in detail in Section 6, based on manualselection of musically relevant fragments and (2) by dis-cussing generalization opportunities in Section 7, based on1While a dataset of orchestral recordings with multiple renditions ofthe same piece was used in [2], these recordings are not publicly available.2http://phenicx.upf.edu\nFigure 2. Eroica 1st movement, score bars 3-10.aggregation of 4-bar analysis frames.In both cases, a similar strategy is taken: ﬁrst, a mu-sical fragment is designated, for which all recordings ofthe piece should be aligned. Alignment is performed au-tomatically using the method described in [12]. Then, theaudio fragments, which are all sampled at Fs = 44.1 kHz,are analyzed using a Hann window of 1024 samples anda hop size of 512, and the corresponding magnitude spec-trum is computed using the Essentia framework [3]. Com-bining the spectra for all frames results in a spectrogramimage. To ensure that all images have equal dimensions, aconstant heigth of 500 pixels is imposed, and the longestfragment in terms of time determines a ﬁxed width of theimage, to which all other spectrograms are scaled accord-ingly. While all recordings are offered at 44.1 kHz, theoriginal recordings sometimes were performed at a lowersampling rate (particularly in more historical recordings).Therefore, a sharp energy cut-off may exist in the higherfrequency zones, and for analysis, we try to avoid this asmuch as possible by only considering the lower 90% ofthe image. In general, by using raw spectrogram images, arisk is that recording quality is reﬂected in this spectrum;nonetheless, in the next sections we will discuss how mu-sically relevant information can still be inferred.6. CASE STUDYIn this case study, to illustrate the information revealedby PCA analysis, we will look in detail at informationobtained on two selected fragments: the start of the ﬁrstmovement of the Eroica symphony, ﬁrst theme (bars 3-15),and the ‘maggiore’ part of the Eroica symphony, secondmovement (bars 69-104).6.1 Eroica ﬁrst movement, bars 3-15A score fragment for bars 3-10 of the ﬁrst movement ofthe Eroica is given in Figure 2. In our case, we considerthe full phrase up to bar 15 in our analysis.The ﬁrst three basis images (component scores) result-ing from PCA analysis are shown in Figure 3. The ﬁrstcomponent of the PCA analysis gives a smoothed ‘basic’performance version of the fragment. For this very gen-eral component, it is rather hard to truly contrast perfor-mances. However, a more interesting mapping can be donein higher-order components. As an example, Figure 4 dis-304 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015(a) First component\n(b) Second component\n(c) Third componentFigure 3. Eroica, 1st movement, 1st theme start (bars 3-15); ﬁrst three principal component basis images.\n−0.3−0.2−0.100.10.20.30.40.50.60.7−0.4−0.3−0.2−0.100.10.20.30.40.50.6Fischer−RCO−2013\nBernstein−VP−1978Furtwangler−BP−1952\nGardiner−ORR−1993Haitink−LSO−2005Harnoncourt−COE−1991Karajan−BP−1963Klemperer−PO−1959Solti−CSO−1973Toscanini−NBC−1953\nZinman−TOZ−1998Haitink−RCO−1987Harnoncourt−RCO−1988Jochum−RCO−1978Kleiber−RCO−1950\nKondrashin−RCO−1979Mengelberg−RCO−1940\nMonteux−RCO−1962Sawallisch−RCO−1993Jochum−RCO−1969Harnoncourt−COE−1990Kleiber−VP−1953Haitink−LSO−2005\nvanBeinum−RCO−19572nd component loadings3rd component loadings\nFigure 4. 2nd and 3rd PCA component scatter plot for Eroica 1st movement, bars 3-15.\n(a) Fisher, RCO, 2013\n(b) Haitink, RCO, 1987Figure 5. Spectrogram image examples for Fisher and Haitink interpretations of Eroica 1st movement, bars 3-15.plays a scatter plot of the second and third principal com-ponent loadings for this fragment.While as expected, several historical (and acousticallynoisy) recordings cause outliers, by comparing the com-ponent scores and loadings to corresponding data samples,we still note interpretable differences. For example, theRCO recordings of Fischer and Haitink, of which respec-tive spectrogram images for the excerpt are shown in Fig-ure 5, have contrasting loadings on the third PCA com-ponent. Judging from the principal component image inFigure 3, this component indicates variability at the start ofthe fragment (when the celli play), and in between the frag-ments highlighted by the second component; more specif-ically, a variability hotspot occurs at the sforzato in bar10. When contrasting two opposite examplars in termsof scores, such as Fischer and Haitink, it can be heardthat in the opening, Haitink emphasizes the lower stringsmore strongly than Fischer, while at the sforzato, Haitinkstrongly emphasizes the high strings, and lets the sound de-velop over the a-ﬂat played by violin 1 in bar 10. Fischermaintains a ‘tighter’ sound over this sforzato.6.2 Eroica second movement, maggioreTo illustrate ﬁndings on another manually selected (andslightly longer) relevant fragment, we also consider theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 305−0.4−0.3−0.2−0.100.10.20.30.4−0.4−0.200.20.40.60.81\n3rd component loadings4th component loadingsFischer−RCO−2013Bernstein−VP−1978Furtwangler−BP−1952Gardiner−ORR−1993\nHaitink−LSO−2005Harnoncourt−COE−199Karajan−BP−1963Klemperer−PO−1959Solti−CSO−1973Toscanini−NBC−1953Zinman−TOZ−1998Haitink−RCO−1987Harnoncourt−RCO−1988Jochum−RCO−1978Kleiber−RCO−1950Kondrashin−RCO−1979Mengelberg−RCO−1940Monteux−RCO−1962Sawallisch−RCO−1993Jochum−RCO−1969Harnoncourt−COE−19Kleiber−VP−1953\nHaitink−LSO−2005vanBeinum−RCO−1957Figure 6. 3rd and 4th PCA component scatter plot for Eroica 2nd movement, maggiore. Jochum’s 1969 and 1978 record-ings occur within the marked rectangular border.‘maggiore’ part of the second movement of the Eroica.Analyses of scatter plots and component images show thatthe second principal component is affected by historicalrecording artefacts. However, this is less so for the thirdand fourth component, of which the scatter plot is dis-played in Figure 6. It can be seen that the suspected near-duplicates of Harnoncourt’s two COE recordings havenear-identical loadings on these components. Next to this,another strong similarity is noted between the recordingsof Jochum with the RCO in 1969 and 1978. While theseboth recordings acoustically are clearly different and alsoseem to be explicitly different interpretations, there still areconsistencies in Jochum’s work with the same orchestra forthese two recordings.7. CORPUS-WIDE CLUSTERINGAs demonstrated in the previous section, PCA analysiscan be used as an exploratory tool to reveal differencesbetween selected fragments in recordings. However, se-lecting incidental manual examples will not yet allow forscalable analysis of information over the full timeline ofa piece. To do this, instead of pre-selecting designatedfragments, we perform a 4-bar sliding window PCA anal-ysis on full synchronized recordings, where bar bound-aries are obtained through the score-to-performance map-ping obtained in the alignment procedure. Instead of ex-amining individual component images, in each 4-bar anal-ysis frame, we consider vectors of component loadingsfor the minimum amount of components required to ex-plain 95% of the variance observed. From these compo-nent loading vectors, we compute the Euclidean distancebetween recordings within a frame, and aggregate these atthe recording track level.33Note that component loadings obtained for different frames cannotbe directly averaged, as the components are different per frame. How-ever, observed distances between recordings still remain valid and can beaggregated.Based on distances found between performances, clus-tering can be performed. This reveals whether stable per-former clusters can found for different movements withina piece, and to what extent clusterings found in local frag-ments match those found for a full piece.Regarding the ﬁrst question, for each of the Eroicamovements, we calculated the average between-performerdistances per movement, and then made 5 clusters of per-formers based on Ward’s linkage method [28]. Whilespace does not allow a full cluster result report, severalclusters co-occur consistently:•The two Harnoncourt COE recordings consistentlyform a separate cluster. These are highly likely to beduplicate recordings.•Haitink’s two LSO recordings also consistently co-occur, and like Harnoncourt are highly likely to beduplicate recordings. However, Bernstein’s 1978 Vi-enna Philharmonic recording co-occurs with thesetwo Haitink recordings in the ﬁrst three Eroicamovements, and thus may be similar in terms of in-terpretation. It is striking that Haitink’s 1987 record-ing with the RCO never co-occurs in this cluster.•In the ﬁrst three movements, a consistent cluster oc-curs with recordings by Klemperer (PhilharmoniaOrchestra, 1959), Toscanini (NBC Symphony Or-chestra, 1953) and Van Beinum (RCO, 1957). Whilethis may be due to recording artefacts, other histor-ical recordings (e.g. Kleiber, RCO 1950 / ViennaPhilharmonic 1953) do not co-occur.•Surprisingly, Gardiner’s historically informedrecording with the Orchestre R´evolutionaire etRomantique (1993) clusters with Kleiber’s 1950RCO recording for the ﬁrst and last movement ofthe Eroica. Upon closer listening, Gardiner’s choiceof concert pitch matches the pitch of Kleiber’srecording, and the sound qualities of the orchestras306 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Karajan−BP−1980Jaervi−SNO−1987Luisi−SD−2007Haitink−RCO−1985Previn−PHO−1980Jansons−RCO−2007Haitink−LSO−20080.40.60.811.21.4\n(a) ‘Sonnenaufgang’ fragment (bars 46-63).\nHaitink−RCO−1985Jaervi−SNO−1987Luisi−SD−2007Karajan−BP−1980Previn−PHO−1980Jansons−RCO−2007Haitink−LSO−20080.60.70.80.911.11.21.31.4\n(b) Average over full Alpensinfonie.Figure 7. Dendrogram images for performer distances inthe Alpensinfonie.are indeed similar (although in case of Kleiber, thisis caused by recording artefacts).•The 1969 and 1978 Jochum recordings with theRCO always co-occur, though in the largest clusterof recordings. As such, they are similar, but no clearoutlier pair compared to the rest of the corpus.Regarding consistent clusterings over the course of apiece, we further illustrate an interesting ﬁnding fromthe Alpensinfonie, in which we compare a clustering ob-tained on 18 bars from the ‘Sonnenaufgang’ movement tothe clustering obtained for average distances over the fullpiece, as visualized in the form of dendrograms in Fig-ure 7. As can be noted, the clusterings are very close, withthe only difference that within the ‘Sonnenaufgang’ move-ment, Karajan’s interpretation is unusually close to J¨arvi’sinterpretation, while Haitink’s interpretation is unusuallydifferent.8. CONCLUSIONIn this paper, we proposed to analyze differences betweenorchestral performance recordings through PCA analysisof spectrogram images. As we showed, PCA analysis iscapable of visualizing areas of spectral variation betweenrecordings. It can be applied in a sliding window setupto assess differences between performers over the timelineof a piece, and ﬁndings can be aggregated over interpre-tations of multiple movements. While spectrograms in-evitably have sensitivity to recording artefacts, we showedthat near-duplicate recordings in the corpus could be iden-tiﬁed, and historical recordings in the corpus do not con-sistently form outliers in the different analyses.While certain interesting co-occurrences were foundbetween recordings, no conclusive evidence was found re-garding consistent clustering of the same conductor withdifferent orchestras, or the same orchestra with differentconductors. This can either be due to interference fromartefacts and different recording setups, but at the sametime may suggest that different conductors work differ-ently with different orchestras.Several directions of future work can be identiﬁed. Firstof all, further reﬁnement regarding the generation and anal-ysis of the spectrogram images should be performed. Atthe moment, given the linear way of plotting and highsample rate, the plain spectrogram may be biased towardshigher-frequency components, and risks to be inﬂuencedby sharp frequency cut-offs from lower original recordingsample rates.Furthermore, it would be interesting to study moredeeply if visual inspection of spectrograms can indeed as-sist people in becoming more actively aware of perfor-mance differences. While the spectrogram images are ex-pected to already be understandable to potential end-users,appropriate techniques should still be found for visualiz-ing differences between multiple performers in a corpus.In the current paper, this was done with scatter plots anddendrograms, but for non-technical end-users, more intu-itive and less mathematically-looking visualizations maybe more appropriate.One concern that may come up with respect to ourwork, is that it may be hard to fully associate our reportedﬁndings to expressive performance. As indicated, record-ing artefacts are superimposed on the signal, and effectsof different halls and choices of orchestra instruments andconcert pitch may further inﬂuence acoustic characteris-tics, which will in turn inﬂuence our analysis. Further-more, since we are dealing with commercial recordings,we are dealing with produced end results which may havebeen formed out of multiple takes, and as such do not re-ﬂect ‘spontaneous’ performance.However, our main interest is not in analyzing per-formance expressivity per se, but in providing novelways for archive and search engine exploration, andmaking general sense of larger volumes of unannotatedperformance recordings. In such settings, the data understudy will mostly be produced recordings with the abovecharacteristics. For this, we believe our approach isuseful and appropriate, offering interesting applicationopportunities.Acknowledgements:The research leading to these results hasreceived funding from the European Union Seventh FrameworkProgramme FP7 / 2007–2013 through the PHENICX project un-der Grant Agreement no. 601166.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 3079. REFERENCES[1]M. Barthet and S. Dixon. Ethnographic observations of musi-cologists at the British Library: implications for Music Infor-mation Retrieval. InProceedings of the International Societyfor Music Information Retrieval Conference (ISMIR), Miami,USA, 2011.[2]J. P. Bello. Measuring structural similarity in music.IEEETransactions on Audio, Speech and Language Processing,19(7):2013–2025, 2011.[3]D. Bogdanov, N. Wack, E. G´omez, S. Gulati, P. Herrera,O. Mayor, G. Roma, J. Salamon, J. Zapata, and X. Serra. ES-SENTIA: an Audio Analysis Library for Music InformationRetrieval. InProceedings of the International Society for Mu-sic Information Retrieval Conference, pages 493–498, 2013.[4]E. Cheng and E. Chew. Quantitative Analysis of PhrasingStrategies in Expressive Performance: Computational Meth-ods and Analysis of Performances of Unaccompanied Bachfor Solo Violin.Journal of New Music Research, 37:325–338,December 2008.[5]N. Cook. Towards the compleat musicologist? InProceed-ings of the International Symposium on Music InformationRetrieval (ISMIR) [invited talk], London, UK, 2005.[6]P. Desain and H. Honing. Does expressive timing in musicperformance scale proportionally with tempo?PsychologicalResearch, 56(4):285–292, July 1994.[7]J. Devaney, M. I. Mandel, D. P. W. Ellis, and I. Fujinaga.Automatically extracting performance data from recordingsof trained singers.Psychomusicology: Music, Mind & Brain,21:108–136, 2011.[8]M. M. Wanderley E. C. F. Teixeira, M. A. Loureiro and H. C.Yehia. Motion Analysis of Clarinet Performers.Journal ofNew Music Research, July 2014.[9]A. Friberg and J. Sundberg. Does music performance alludeto locomotion? A model of ﬁnalritardandiderived frommeasurements of stopping runners.Journal of the AcousticSociety of America, 105(3):1469–1484, March 1999.[10]W. Goebl, S. Dixon, G. De Poli, A. Friberg, R. Bresin, andG. Widmer. “Sense” in expressive music performance: Dataacquisition, computational studies, and models. In P. Polottiand D. Rocchesso, editors,Sound to sense, sense to sound: astate of the art in sound and music computing. Logos Verlag,2007.[11]W. Goebl and G. Widmer. On the use of computational meth-ods for expressive music performance. In T. T. Crawfordand L. Gibson, editors,Modern Methods for Musicology:Prospects, Proposals and Realities, Digital Research in theArts and Humanities, pages 93–113. Ashgate, 2009.[12]M. Grachten, M. Gasser, A. Arzt, and G. Widmer. AutomaticAlignment of Music Performances with Structural Differ-ences. InProceedings of the International Society for MusicInformation Retrieval Conference (ISMIR), pages 607–612,2013.[13]M. Grachten and G. Widmer. Who is who in the end? Recog-nizing pianists by their ﬁnal ritardandi. InProceedings of theInternational Society for Music Information Retrieval Con-ference (ISMIR), Kobe, Japan, October 2009.[14]P. N. Juslin. Five facets of musical expression: a psycholo-gist’s perspective on music performance.Psychology of Mu-sic, 31(3):273–302, July 2003.[15]K. Kosta, O. F. Bandtlow, and E. Chew. Practical implicationsof dynamic markings in the score: Is piano always piano?InProceedings of the 53rd International AES Conference onSemantic Audio, London, UK, January 2014.[16]E. Liebman, E. Ornoy, and B. Chor. A Phylogenetic Ap-proach to Music Performance Analysis.Journal of New Mu-sic Research, 41:195–222, June 2012.[17]C. C. S. Liem and A. Hanjalic. Expressive timing from cross-performance and audio-based alignment patterns: An ex-tended case study. InProceedings of the International Societyfor Music Information Retrieval Conference (ISMIR), Miami,Florida, USA, October 2011.[18]C. C. S. Liem, A. Hanjalic, and C. S. Sapp. Expressivity inmusical timing in relation to musical structure and interpre-tation: A cross-performance, audio-based approach. InPro-ceedings of the 42nd International AES Conference on Se-mantic Audio, pages 255–264, Ilmenau, Germany, July 2011.[19]M. M¨uller, P. Grosche, and C. S. Sapp. What makes beattracking difﬁcult? a case study on chopin mazurkas. InPro-ceedings of the International Society for Music InformationRetrieval Conference (ISMIR), Utrecht, The Netherlands, Au-gust 2010.[20]C. Palmer. Anatomy of a performance: Sources of musicalexpression.Music Perception, 13:433–453, Spring 1996.[21]A. Penel and X. Drake. Sources of timing variations in musicperformance: a psychological segmentation model.Psycho-logical Research, 61(1):12–32, March 1998.[22]B. Repp. A microcosm of musical expression. I. Quantitativeanalysis of pianist’s timing in the initial measures of Chopin’sEtude in E major.Journal of the Acoustic Society of America,104(2):1085–1100, August 1998.[23]B. Repp. A microcosm of musical expression. I. Quantitativeanalysis of pianist’s timing in the initial measures of Chopin’sEtude in E major.Journal of the Acoustic Society of America,104(2):1085–1100, August 1998.[24]C. S. Sapp. Comparative analysis of multiple musical per-formances. InProceedings of the International Conferenceon Music Information Retrieval (ISMIR), Vienna, Austria,September 2007.[25]C. S. Sapp. Hybrid numeric/rank similarity metrics for mu-sical performance analysis. InProceedings of the Interna-tional Conference on Music Information Retrieval (ISMIR),Philadelphia, USA, September 2008.[26]C. Seashore.Psychology of music. University of Iowa Press,Iowa City, 1938.[27]M. A. Turk and A. P. Pentland. Face recognition using eigen-faces. InProceedings of IEEE Computer Society Conferenceon Computer Vision and Pattern Recognition (CVPR), Maui,Hawaii, USA, June 1991.[28]J. H. Ward Jr. Hierarchical grouping to optimize an objec-tive function.Journal of the American Statistical Association,58(301):236–244, 1963.[29]G. A. Wiggins. Computer-representation of music in the re-search environment. In T. T. Crawford and L. Gibson, editors,Modern Methods for Musicology: Prospects, Proposals andRealities, Digital Research in the Arts and Humanities, pages7–22. Ashgate, Aldershot, UK, 2009.308 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Detection of Common Mistakes in Novice Violin Playing.",
        "author": [
            "Yin-Jyun Luo",
            "Li Su 0004",
            "Yi-Hsuan Yang",
            "Tai-Shih Chi"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415968",
        "url": "https://doi.org/10.5281/zenodo.1415968",
        "ee": "https://zenodo.org/records/1415968/files/LuoSYC15.pdf",
        "abstract": "Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musi- cal instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset com- prising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are gener- ated from the same feature set with different scales, in- cluding two note-level representations and three segment- level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vec- tor machine for classification. Results show that the F- measures using different feature representations can vary up to 20% for two types of playing mistakes. It demon- strates the different sensitivities of each feature represen- tation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed.",
        "zenodo_id": 1415968,
        "dblp_key": "conf/ismir/LuoSYC15",
        "keywords": [
            "novice violin players",
            "four types of mistakes",
            "novel dataset",
            "violin experts",
            "Fisher score",
            "support vector machine",
            "legato notes",
            "different scales",
            "feature representations",
            "playing mistakes"
        ],
        "content": "DETECTION OF COMMON MISTAKES IN NOVICE VIOLIN PLAYING \nABSTRACT Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musi-cal instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset com-prising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are gener-ated from the same feature set with different scales, in-cluding two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vec-tor machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demon-strates the different sensitivities of each feature represen-tation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed. 1. INTRODUCTION With advances in music technology, the development of computer-aided music learning and automatic scoring systems has attracted wide attention. Such systems pro-vide self-learning experiences to users through computer-aided platforms. Despite numerous efforts have been made, however, the performance of current systems still leaves plenty of space for improvement. A review of the music learning system and the main challenge can be found in [1].   For a novice player, three common basic aspects, into-nation, rhythm and timbre, are often used to evaluate his/her performance [2]. Intonation refers to the pitch of the tone, rhythm specifies the duration of the tone, and timbre characterizes the overall quality of the tone. Con-ventionally, a novice player uses a tuner for correcting the intonation and a metronome for following the rhythm during the practice. In traditional music education,  there is no hardware device capable of automatically evaluating the timbre quality.   Up to date, most of the computer-aided music learning systems also focus on intonation and rhythm only [1]. These studies mainly coped with learning intonation and rhythm in music in the context of automatic music tran-scription (AMT). For example, the pitch played by the violin learner was automatically detected and visually presented to evaluate the pitch intonation [3]. A fusion of audio and video cues improved the onset detection of non-percussive instruments, such as violin, and thereby enhanced the performance of AMT [4]. Automatic sing-ing quality assessment is achieved by measuring the dis-similarity between singing voices of beginners and of trained singers [5]. Besides intonation and rhythm, timbre plays an essential role in identifying the skill (or profi-ciency) level of a player but has not attracted much atten-tion in computer-aided music learning platforms. Some timbre-related research studies considered instrumental expression to recognize the techniques in playing musical notes by violin [6] and by electric bass guitar [7], respec-tively. Other studies aimed to evaluate the played notes, for example, using spectral parameters from long tones to evaluate the technical level of saxophone players [8]. Re-cently, a hierarchical approach combining deterministic signal processing and deep learning was employed to identify different common mistakes made by novice flute players [9]. Machine learning techniques were also adopted to distinguish good trumpet tones from bad ones [10]. The first attempt to detect bad violin playing in [11] is the most relevant work to the proposed study. One of the two tasks conducted in [11] classifies violin tones into binary clusters, i.e., good or bad, using k-nearest neigh-borhood algorithm. The other task examined the promi-nent feature sets for detecting individual playing mistakes. Similarly, in this paper, we explore the capability of tim-bre in detecting playing mistakes produced by novice vi-olin players during practice. However, since the dataset and the algorithm codes in [11] are not publicly available, it is difficult to compare our approach with the approach in [11].  Yin-Jyun Luo1,2 Li Su2 Yi-Hsuan Yang2 Tai-Shih Chi1 1Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan 2Research Center for Information Technology Innovation, Academia Sinica, Taiwan  fredom.smt02g@nctu.edu.tw, lisu@citi.sinica.edu.tw, yang@citi.sinica.edu.tw, tschi@mail.nctu.edu.tw \n © Yin-Jyun Luo, Li Su, Yi-Hsuan Yang and Tai-Shih Chi. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Yin-Jyun Luo, Li Su, Yi-Hsuan Yang and Tai-Shih Chi. “Detection of Common Mistakes in Novice Violin Playing”, 16th International Society for Music Information Retrieval Conference, 2015. 316     The first contribution of this paper is to build and re-lease a new dataset1 for such a research problem. To be as realistic as possible, we recorded four successive legato notes, which require smooth, round and continuous flow of tones [12], as a unit and then trimmed it into individual notes rather than simply recording single note at a time as in [11]. The resulting dataset comprises of 981 individual legato notes played by several players across different skill levels. The playing mistakes associated with each note were annotated by violin experts using the following four pre-defined classes: scratching bow, crooked bow, bouncing bow, and inappropriate arm height. More de-tails of the annotations and dataset are elaborated in Sec-tions 2 and 3, respectively.    The second contribution of this paper is to evaluate a number of features capturing the acoustic characteristics of different segments of a musical note for the task of au-tomatic playing mistake classification. A set of spectral features is extracted from either the whole note or the segment of onset, sustain and offset, partitioned using the output of an optical sensor installed on the violin. The approach leads to five different feature representations: Note, Onset, Sustain, Offset and Cascade. They refer to features extracted from the whole note, from the corre-sponding segments only and the concatenation of seg-ment-level features, respectively. More details about the partitioning method and feature extraction can be found in Sections 3 and 4, respectively.    In our approach, the Fisher score is used for feature se-lection and the support vector machine (SVM) is used for classification. Feature selection is done as a preprocess-ing step of classification. The performance of classifica-tion is assessed in terms of the F-measure. Experimental details are presented in Section 5. Exploration of insights to link specific feature representations to playing mis-takes is presented in Section 6, before we conclude the paper in Section 7. 2.  VIOLIN PLAYING MISTAKES We defined four common playing mistakes made by vi-olin novices. These mistakes are mainly related to the bow arm and the bow hand which dominantly control vi-olin timbre for novice players and cause most of the trou-ble for violinists [2]. 2.1 Scratching Bow (SB) The pressure of the bow applied on the string can either come from the weight of the bow, arm and hand, from controlled muscular action, or from a combination of the-se factors [2]. Excessive bowing pressure without enough bowing speed to complement with can hinder the vibra-tions of the string and produce coarse sound with inferior quality. Without the support of bowing speed, extreme                                                              1 The audio clips and annotations of playing mistakes can be found in http://perception.cm.nctu.edu.tw/sound-demo/. pressure of the bow on the string results in sound with scratching effect. 2.2 Crooked Bow (CB) Drawing a straight bow from the frog to the tip is the foundation of the bowing technique [2]. If the bow is crooked, not parallel to the bridge, the sound quality will vary due to change of the contact position of the bow on the string. Severe inclination even causes sudden dis-placement of the bow from the bridge and produces sound with skating effect.  2.3 Bouncing Bow (BB) Lack of muscular control of either the bow arm or the bow grip reduces strength to the bow. It might prevent the bow from properly laying on the string, thereby the bow bounces naturally due to its elasticity. 2.4 Inappropriate Arm Height (IAH) Appropriate tilt of the arm relative to the bow is required in order to play on each string without touching the other strings. With inconsistent height or tilt of the arm when drawing the bow across the string, pitch produced by ad-jacent string might be heard. 3. DATASET All notes in the dataset were played by ten players across different skill levels using the same violin in a semi-anechoic chamber. Four players are relatively more ex-perienced in violin or similar string instruments such as cello, while the other six players have learned to play vi-olin for less than one month. Each player was asked to play four successive notes as a clip at the speed of 60 beat-per-minute (BPM). Each clip was directed to start with down-bow and end with up-bow. In total, 26 clips containing 104 legato notes were played by each player. This style of successive playing is more similar to actual practicing than the style of playing an individual note at once. In our recordings, analysis of transition between notes is also feasible though we leave it as future work. We limit the study to consider legato notes only because legato is the essence of all cantabile playing [12] and one can hardly master other advanced techniques before playing it well.   Segmentation between notes and within each note was achieved using a photo resistor and four rings of surface-mounted light-emitting diodes (SMD LEDs) installed respectively underneath the violin bridge and on the bow stick. Two of the four rings were installed at the posi-tions close to the frog and the tip on the bow stick, while the other two were placed at both ends of the middle of the bow. Segmenting a violin note can benefit the analy-sis, as the time domain signal varies in characteristic over a bow draw. The purpose of installing the optical sensor was to segment the time domain signal in a more direct way rather than the approach in [13]. When a lega-to note was played, the optical sensor was capable of marking the time instants, at which those ring-located Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 317   positions of the bow stick passed through the violin bridge, without influencing playing. As our main pur-pose was to simply divide the bow draw into three seg-ments, we can tolerate the small accuracy errors of the sensors on the longitudinal bow position [13].   Based on the marked time instants, we divided each clip into four individual notes and segmented each note into three different segments, i.e., onset, sustain and offset, as playing mistakes can occur at any instant of the drawing. The two ends of each clip, the start of the first note and the end of the fourth note, were manually determined by an energy threshold. The edges between successive notes and within each individual note were automatically de-fined by the marked time instants. At the end, we col-lected 981 notes in total and the corresponding segments after discarding notes containing accidently made dis-tinct noise during the recording.    We employed the hardware-assisted approach instead of automatic approaches proposed in the literature [14, 15] because automatic approaches usually segment an individual musical note according to the temporal evolu-tion of amplitude envelope and spectral centroid [14, 15]. Since we are dealing with notes produced by the violin novices, the algorithms developed for well-played musi-cal notes are not applicable in our case. For instance, simply dividing the note into three equal-duration seg-ments would not produce same results as our hardware-assisted approach since novice players cannot draw the bow with a constant speed. Therefore, without the assis-tance of the optical sensors, automatic segmentation of violin notes performed by novice players should be a dif-ficult task, which is beyond the scope of this paper and deserves further research in the future.   The notes were then annotated by violin experts using the four pre-defined mistakes. Note that a single note could possess multiple playing mistakes. Fig. 1 shows the duration distributions of notes and the corresponding segments. One can observe that although players were asked to play each note at the speed of 60 BPM, begin-ners, especially those who lack of musical background, weren’t necessarily able to perform accurately. Table 1 summarizes the numbers of instances of the playing mis-takes in the first row. Dividing the first row by the total number of the collected notes gives the percentages in the second row.  4. METHOD 4.1 Preprocessing  All of the notes in the dataset were resampled to 44.1 kHz and saved in the mono-channel WAV format. Before feature extraction, each time domain signal was first normalized to zero mean and unit variance and then di-vided into three segments as described in Section 3 for further analysis.  Figure 1. Duration distributions of 981 notes (the first column), the corresponding onset, sustain and offset segments (the second to the last column). Table 2. The number of instances of each mistake and the corresponding percentage.  4.2   Feature Extraction A set of 30 frame-level spectral features, including high frequency content (HFC) [16], 13 Mel-frequency cepstral coefficients (MFCCs), spectral centroid, spectral crest, spectral flatness, spectral flux, spectral roll-off, de-scriptors of spectral distribution (i.e., spectral variance, skewness and kurtosis), tristimulus [17], odd-to-even harmonic energy ratio (OER) [18], the estimated pitch, zero crossing rate and the instant power, were extracted from either the waveform or the spectrum using the ES-SENTIA open-source library (version 2.0.1) [19]. The feature extraction was performed in each Hanning-windowed frame with the frame duration of 46 ms and the frame shift of 50%. These features are capable of characterizing timbre and regularly employed in audio signal processing applications [20]. The six temporal functionals, including mean, variance, skewness, kurtosis, mean and variance of the derivative, of all the frame-level features were derived to generate clip- or segment-level features. The outcome of the feature extraction stage is a feature vector of 180 dimensions.   The feature extraction process was done on different segments of notes resulting in five feature representations: Note, Onset, Sustain, Offset and Cascade. The Note rep-resentation was extracted from each intact note while the Onset, Sustain and Offset representations were extracted from corresponding segments of each note. These four representations consist of feature vectors of 180 dimen-sions. The Cascade representation was produced by con-catenating the Onset, Sustain and Offset representations \n SB CB BB IAH Numbers 265 133 154 53 Percentage 27.0% 13.6% 15.7% 5.4% 318 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   to give a 540-dimentional feature vector for each note. All feature representations were derived from 981 record-ed notes and used for the task of playing mistake classifi-cation. 4.3   Feature Selection and Classification The Fisher score was considered for selecting prominent features in a pre-processing step prior to classification to reduce amounts of computation [21]. It is defined as [22]  !!≡(!!(+)−!!)2+(!!(−)−!!)!21!(+)−1(!!,!(+)−!!(+))2!+!=1+1!(−)−1(!!,!(−)−!!(−))2!+!=1 , where !(!) and !(!) are the numbers of positive and neg-ative instances, respectively; !!, !!(!), !!(!) are the aver-ages of  the !th feature over the whole, positive, and negative instances, respectively; !!,!(!) is the !th feature of the !th positive instance, and !!,!(!) is the !th feature of the !th negative instance.   We followed the framework in [22] which selects fea-tures with high Fisher scores and uses the support vector machine (SVM), implemented by LIBSVM [23], for classification. The performance was evaluated in terms of the averaged F-measure, which is the harmonic mean of precision and recall, for each mistake using each feature representation with 100 repetitions of stratified five-fold cross-validation (CV). 5. EXPERIMENTS The goal of the experiments is to investigate the capabil-ity of used features to detect playing mistakes and bridge the relation between playing mistakes and feature repre-sentations from different segments of notes.    Detection experiments were carried out through all fea-ture representations after completing feature extraction. Following the procedures in [22], we first adopted a nest-ed stratified five-fold CV to find the best percentage threshold to retain features based on Fisher scores, and then used the selected features for grid searching the op-timized hyper-parameters C and γ, from the choices of {10-6, 10-5, 10-4, 10-3} and {1, 10, 100, 1000} respectively, of the radial-basis function (RBF) kernel based SVM. Fi-nally, the selected threshold and the hyper-parameters were fed into another stratified five-fold CV. The overall performance was evaluated by averaging the F-measures of 100 repetitions of the final CV. Note that the above experiments were conducted for each feature representa-tion and for each mistake. In other words, we trained M binary SVMs on each feature representation, where M is the number of types of playing mistakes.   To further have subset analysis, the experiments were conducted using three sets of data: All, Down-bow, and Up-bow, which respectively refer to the full data set of 981 notes, the set of 480 notes played with down-bow and the set of 481 notes played with up-bow. Moreover, we performed the same experiment on the 570 notes rec-orded by the six beginners who have played violin less than one month. Experiment results on these subset data and related discussions will be given in the next section.  6. RESULTS The averaged F-measure using each feature representa-tion for identifying each playing mistake in the All dataset is shown in Fig. 2. One can see that Cascade performs slightly better than Note in terms of the F-measure across all the mistakes, which is verified by the two-tailed t-test (p<0.01). It is probably because Cascade contains more detailed information of each individual segment. Except for the BB mistake, Cascade performs better than each of its constituents, i.e., Onset, Sustain and Offset. Note that the F-measures of the playing mistakes by the random guess would be 35.0%, 21.3%, 23.7% and 9.7%, respec-tively, equivalent to the prior probabilities p(m) of the mistake m as shown in the second row of Table 1 divided by p(m) + 0.5. It is because we preserved the prior distri-bution of the dataset in all partitions during the stratified five-fold CV procedures for each playing mistake. For comparison, we show in Fig 3 the performance of using the original 180 features without feature selection. Simi-lar results between Figs. 2 and 3 suggest that the selected features sufficiently capture information embedded in the original 180 features for our experiments.   To explore more connections between playing mistakes and feature representations, one can re-arrange the F-measures of Onset, Sustain and Offset against mistakes as in Table 2.  Results in Table 2 show that Onset has ad-vantage in detecting SB over the others. It means that the onset segment is more sensitive for detecting SB, which somehow implies that the 10 players tended to have ex-cessive bow pressure at the beginning of the bow draw. In contrast, Sustain surpasses the others in both CB and BB by up to 8% and 20%, respectively, which suggests CB and BB have higher chance to emerge during the middle of a drawing bow. Lastly, Offset dominates the IAH mis-take. Such “favor” of a specific playing mistake in a par-ticular segment of a note reveals the tendency of players to make that mistake at certain moment of a bow draw. This kind of information is helpful to novice players dur-ing their practice.   As shown in Table 2, SB and BB are prone to happen in the onset segment and sustain segment, respectively. Fur-thermore, it is commented by violin experts that such “favor” of SB and BB would be even more obvious in down-bow notes based on their teaching experiences. Figs. 4 and 5 compare the F-measures between the Up-bow and Down-bow subsets for the SB and BB mistakes, respectively. Obviously, these two figures indicate the down-bow notes are more associated with the mistakes than the up-bow notes, which is consistent with experi-ences of the violin experts.   Moreover, Fig. 6 shows the results on notes only played by the six beginners. It shows better overall performance Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 319   than results in Fig. 2, which suggests notes played by the beginners reveal more obvious characteristics of mistakes than the ones played by experienced players. In other words, the adopted features might be incapable of captur-ing slight mistakes made by experienced players.   The inferior performance in classifying the IAH mistake, as shown in Figs. 2 and 6, might result from the severe imbalance of the dataset. In addition, pitch-related fea-tures are overwhelmed by timbre-related features in our adopted feature set. If more pitch features are considered, it is possible to further improve the performance for IAH detection, since it is about the mistake of playing unde-sired pitch.   \n Figure 2. Averaged F-measures of playing mistake clas-sification on all recorded notes using different feature representations.   \n Figure 3. Average F-measures of playing mistake classi-fication on all recorded notes using different feature rep-resentations from the original 180 features.   Table 2.  Averaged F-measures (in %) of Onset, Sustain and Offset. The feature representation with the highest F-measure for each mistake is highlighted.    Figure 4. Averaged F-measures of the playing mistake ‘scratching bow’ (SB) using different feature representa-tions within the up-bow and down-bow subsets.  \n Figure 5. Averaged F-measures of the playing mistake ‘bouncing bow’ (BB) using different feature representa-tions within the up-bow and down-bow subsets.   \n Figure 6. Averaged F-measures of playing mistake clas-sification on notes played by beginners using different feature representations.  7. CONCLUSION AND FUTURE WORK In this study, we first recorded a new dataset of violin legato notes played by novice players. Then we defined four common playing mistakes mainly made by bow arm \n SB CB BB IAH Onset 53.4 36.1 39.2 16.5 Sustain 45.0 44.3 52.9 24.4 Offset 47.6 38.8 37.9 33.5 320 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   and performed automatic playing mistake classification using spectral and temporal features extracted from dif-ferent segments of the notes.   Our evaluation on different feature representations sug-gests concatenation of segment-level features provides more information than the note-level features in identify-ing playing mistakes. Furthermore, by exploring connec-tions between playing mistakes and feature representa-tions, we found SB, CB, BB, and IAH mistakes are prone to happen in the onset, sustain, sustain, and offset seg-ments, respectively. These findings would serve pedagog-ical purpose and benefit novice violin players. Our future work will focus on improving the overall classification performance by enriching the dataset and seeking more relevant features, using either feature design or feature learning techniques [24, 25]. 8. ACKNOWLEDGEMENTS This research is supported by the National Science Coun-cil, Taiwan under Grant No NSC 102-2220-E-009-049, the Biomedical Electronics Translational Research Center, NCTU, and the Academia Sinica Career Development Award. 9.  REFERENCE [1] C. Dittmar, E. Cano, J. Abeßer and S. Grollmisch: “Music information retrieval meets music educa-tion,” Multimodal Music Processing. Dagstuhl Fol-low-Ups M. Müller, M. Goto and M. Schedl, Eds., vol. 3, pp. 95–120, 2012. [2] I. Galamian: Principals of Violin Playing and Teaching, London, Prentice Hall, 1985. [3] J. Wang, S. Wang, W. Chen, K. Chang and H. Chen: “Real-Time Pitch Training System for Violin Learners,” Proc. IEEE Int. Conf. Multimedia and Expo Workshops, pp. 163–168, 2012. [4] B. Zhang and Y. Wang: Automatic music transcrip-tion using audio-visual fusion for violin practice in home environment, Tech. Report TRA7/09, School of Computing, National University of Singapore, 2009. [5] E. Molina et al.: “Fundamental frequency align-ment vs. note-based melodic similarity for singing voice assessment,” Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, pp. 744-748, 2013. [6] I. Barbancho, C. Bandera, A.M. Barbancho and L. J. Tarón: “Transcription and Expressiveness Detec-tion System for Violin Music,” Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, pp. 189-192, 2013.  [7] J. Abeßer et al.: “Feature-based extraction of pluck-ing and expression styles of the electric bass guitar,” Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, pp. 2290-2293, 2010.  [8] M. Robine and M. Lagrange: “Evaluation of the Technical Level of Saxophone Performers by Con-sidering the Evolution of Spectral Parameters of the Sound,” Proc. Int. Society for Music Information Retrieval Conference, pp. 79-84, 2006.  [9] Y.  Han and K. Lee: \"Hierarchical approach to de-tect common mistakes of beginner flute players,\" Proc. Int. Society for Music Information Retrieval Conference, pp. 77-82, 2014  [10] T. Knight, F. Upham, and I. Fujinaga: “The poten-tial for automatic assessment of trumpet tone quali-ty,” Proc. Int. Society for Music Information Re-trieval Conference, pp. 573–578, 2011. [11] J. Charles: Playing Technique and Violin Timbre: Detecting Bad Playing, Ph.D. dissertation, Dublin Institute of Technology, 2010. [12] L. Auer: Violin Playing As I Teach It, Dover Publi-cations Inc., New York, 1980. [13] T. Grosshauser, and T. Gerhard: “Optical bow posi-tion, speed and contact point detection,” Proc\u0001 ACM Int. Conf. Pervasive and Ubiquitous Compu-ting Adjunct Publication, 2013. [14] M. Hajda: “A New Model for Segmenting the En-velope of Musical Signals: The Relative Salience of Steady State Versus Attack, Revisited,” Audio Eng. Soc, paper No. 4391, 1996.  [15] M. Caetano, J.J. Burred and X. Rodet: “Automatic Segmentation of the Temporal Evolution of Iso-lated Acoustic Musical Instrument Sounds Using Spectro-Temporal Cues,” Proc. Int. Conf. Digital Audio Effects, 2010. [16] P. Masri and A. Bateman: “Improved modellling of attack transients in music analysis-resynthesis,” Proc. Int. Computer Music Conference, pp. 100–103, 1996. [17] G. Peeters: A large set of audio features for sound description (similarity and classification) in the CUIDADO project, CUIDADO I.S.T. Project Re-port, 2004. [18] K. D. Martin and Y. E. Kim: “Musical instrument identification: A pattern-recognition approach,” The Journal of the Acoustical Society of America, vol. 104, no. 3, pp. 1768–1768, 1998. [19] D. Bogdanov, N. Wack, E. Gómez, S. Gulati, P. Herrera and O. Mayor: “ESSENTIA: an audio analysis library for music information retrieval,” Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 321   Proc. Int. Society for Music Information Retrieval Conference, pp. 493-498, 2013. [20] M. Müller et al: “Signal processing for music anal-ysis,” IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 6,pp. 1088-1110, 2011. [21] I. Guyon and A. Elisseeff: “An introduction to vari-able and feature selection,” Journal of Machine Learning Research, vol. 3\u0002\u0001pp. 1157-1182, 2003. [22] Y.-W. Chen and C.-J. Lin: “Combining SVMs with various feature selection strategies,” Feature ex-traction foundations and applications, pp. 315-324, 2006. [23] C.-C. Chang and C.-J. Lin: “LIBSVM : a library for support vector machines,” ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011. [24] P.-C. Li, L. Su, Y.-H. Yang and A. W. Y. Su: “Analysis of expressive musical terms in violin us-ing score-informed and expression-based audio fea-tures,” Proc. Int. Society for Music Information Re-trieval Conf., 2015. [25] L. Su, L.-F. Yu and Y.-H. Yang: “Sparse cepstral and phase codes for guitar playing technique classi-fication,” Proc. Int. Society for Music Information Retrieval Conf., pp. 9-14, 2014.     322 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Beat Histogram Features from NMF-Based Novelty Functions for Music Classification.",
        "author": [
            "Athanasios Lykartsis",
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418145",
        "url": "https://doi.org/10.5281/zenodo.1418145",
        "ee": "https://zenodo.org/records/1418145/files/LykartsisWL15.pdf",
        "abstract": "In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classification task. Musical excerpts are analyzed using an optimized, partially fixed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are eval- uated on two widely used genre datasets (GTZAN and Ball- room) using standard classification methods, concerning the achieved overall classification accuracy. Furthermore, their suitability in distinguishing between rhythmically sim- ilar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classification systems, while considering strictly drum patterns.",
        "zenodo_id": 1418145,
        "dblp_key": "conf/ismir/LykartsisWL15",
        "keywords": [
            "drum tracks",
            "polyphonic music",
            "genre classification",
            "Non-Negative Matrix Factorization",
            "beat histogram",
            "genre datasets",
            "GTZAN",
            "Ball-room",
            "rhythm features",
            "classification accuracy"
        ],
        "content": "BEAT HISTOGRAM FEATURES FROM NMF-BASED NOVELTYFUNCTIONS FOR MUSIC CLASSIFICATIONAthanasios LykartsisTechnische Universit¨at BerlinAudio Communication Groupalykartsis@mail.tu-berlin.deChih-Wei WuGeorgia Institute of TechnologyCenter for Music Technologycwu307@gatech.eduAlexander LerchGeorgia Institute of TechnologyCenter for Music Technologyalexander.lerch@gatech.eduABSTRACTIn this paper we present novel rhythm features derived fromdrum tracks extracted from polyphonic music and evaluatethem in a genre classiﬁcation task. Musical excerpts areanalyzed using an optimized, partially ﬁxed Non-NegativeMatrix Factorization (NMF) method and beat histogramfeatures are calculated on basis of the resulting activationfunctions for each one out of three drum tracks extracted(Hi-Hat, Snare Drum and Bass Drum). The features are eval-uated on two widely used genre datasets (GTZAN and Ball-room) using standard classiﬁcation methods, concerningthe achieved overall classiﬁcation accuracy. Furthermore,their suitability in distinguishing between rhythmically sim-ilar genres and the performance of the features resultingfrom individual activation functions is discussed. Resultsshow that the presented NMF-based beat histogram featurescan provide comparable performance to other classiﬁcationsystems, while considering strictly drum patterns.1. INTRODUCTIONThe description of musical rhythm remains an importantand challenging topic in Music Information Retrieval (MIR)with applications in several areas [12, 16]. The difﬁculty ofrhythm extraction lies in its multifaceted character, whichinvolves periodicity and structural patterning in the signal aswell as perceptual components such as musical meter [19].An approach which has achieved some popularity over thelast years is based on the creation of a periodicity represen-tation — commonly called the beat histogram (BH) — andthe subsequent extraction of features from this histogramto be used, e.g., in genre classiﬁcation [4, 13, 33]. A com-mon ﬁrst processing step of all approaches is the extractionof a so-called novelty function [2] or its derivatives as thestarting point for further analysis. Since a complete rhythmrepresentation of a musical track results from the superposi-tion of the temporal progressions of different instrumentsor voices [12, 16], it makes sense to include features takinginto account individual temporal and spectral properties.c\u0000Athanasios Lykartsis, Chih-Wei Wu, Alexander Lerch.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Athanasios Lykartsis, Chih-Wei Wu, Alexan-der Lerch. “Beat histogram features from NMF-based novelty functionsfor music classiﬁcation”, 16th International Society for Music InformationRetrieval Conference, 2015.In western popular music (which is the focus of this pa-per), rhythm is most often carried from the drum section,providing the temporal grid on which other instruments canunfold their melodic or harmonic patterns. This makes theanalysis of the drum track appealing for the description ofrhythmic character. In order to obtain the rhythmic proper-ties of the drum section, the extraction of temporal noveltyfunctions per instrument is necessary. Although such meth-ods for the extraction of speciﬁc voices or instruments havebeen commonly used in the area of source separation orautomatic instrument transcription (the most notable beingnon-negative matrix factorization (NMF) [31]), their appli-cation to rhythm extraction problems is, to the best of ourknowledge, sparse. We therefore propose to use a techniquefor source separation and drum transcription based on par-tially ﬁxed NMF using the resulting activation functions as asource material for the extraction of rhythmic features basedon beat histograms. This paper investigates the suitabilityof the proposed features in the context of rhythm-basedgenre classiﬁcation for dance music and other styles.The paper is structured as follows. In the second section,an overview of previous work and the goals of the currentpaper are presented. In section 3, the drum transcriptionprocedure and the feature extraction are described. In thefourth section, the evaluation of the proposed features andthe results are given. After discussing the results in sec-tion 5, we close by giving conclusions and suggestions forfuture work (sect. 6).2. PREVIOUS WORK AND GOALSBeat histograms have been used for a long time as rhythmicdescriptions. Initially introduced in studies on beat track-ing and analysis [11, 29] as a useful very low frequencyperiodicity representation, they were only later referred toas thebeat histogram[33] orperiodicity histogram[13].The histogram is useful as an intermediate representationthat can be used to extract musical parameters such astempo as well as low-level features (e.g., statistical prop-erties of the histogram). Traditionally, a measure of thesignal amplitude envelope or its change over time is utilizedas the novelty function for the extraction of a beat his-togram [4, 13, 33]. However, in the ﬁeld of onset detection,the proposed novelty functions take into account spectralcontent changes [3, 10, 15, 27]. Genre classiﬁcation sys-tems based on such representations have generally shown434promising results, although rhythm features do usually notperform as well as features from other domains such as tim-bre descriptors [4,28,33]. However, studies have shown thatfor highly rhythmical music, beat histogram features canachieve very high performance [13], a fact which has beenconﬁrmed in current studies investigating the role of usingmultiple novelty functions as a basis for beat histogramfeatures [20].Since drum tracks convey essential information abouttempo, rhythm and possibly genre, they could potentiallyprovide better representation for extracting rhythm features.To extract drum tracks from complete mixtures of music, adrum transcription system for polyphonic music would benecessary. Gillet and Richard divide systems for the drumtranscription from mixtures into three categories [9]: (i) seg-ment and classify, (ii) separate and detect, and (iii) matchand adapt. Here, we focus on the second type of approaches(separate and detect). Based on the assumption that themusic signal is a superposition of different sound sources,the music content could be transcribed by ﬁrst decomposingthe signal into source templates with corresponding acti-vation functions, and then detecting the activities of eachtemplate. Different methods such as Independent SubspaceAnalysis [7], Prior Subspace Analysis [6], and Non-negativeMatrix Factorization [1, 21] fall into this category. Theseapproaches are usually easy to interpret since most of thedecompositions result in spectrum-like representations. Fur-thermore, these approches do not require additional classesfor simultaneous events, which could potentially reduce themodel complexity.In the context of NMF for music transcription, the fol-lowing issues have to be taken into consideration: First, thenumber of sound sources and notes within a music record-ing is usually unknown. It is therefore difﬁcult to determinea suitable rankrin order to obtain a clear differentiation ofthe decomposed components in the dictionary matrix. Sec-ond, after the unsupervised NMF decomposition process,it is difﬁcult to identify the associated instrument of eachcomponent in the dictionary matrixWwhen rank is toohigh or too low. Third, when multiple similar entries exist inthe dictionary matrix, the corresponding activation matrixcould be activated at these entries simultaneously, which inturn increases the difﬁculty of intuitively interpreting theresults.To address the above issues, Yoo et al. proposed a co-factorization algorithm [35] to simultaneously factorize aprior drum track and a target signal, and use the basis ma-trix from the drum track to identify the drum componentsin the target signal. This method ensures that the drumcomponents in both dictionary matrices remain percussiononly over the iterations, and thus proper isolation of theharmonic components from the drum components. Sincethey focus on drum separation rather than drum transcrip-tion, their selection of ranks can be higher, but the approachis not directly applicable to the transcription problem be-cause of the probable lack of interpretability of the dictio-nary matrix. Wu and Lerch proposed a variant of the co-factorization algorithm using partially ﬁxed NMF (PFNMF)\nFigure 1. Illustration of the factorization process.W: dic-tionary matrix.H: activation matrix. SubscriptD: drumcomponents, SubscriptH: harmonic components.for drum transcription in polyphonic signals [34]. Insteadof co-factorization, this method uses a pre-determined drumdictionary matrix during the decomposition process, andextracts one activation function for each of the three drums(Hi-Hat, Snare Drum, and Bass Drum).In this paper, we apply PFNMF to transcribe drum eventsin polyphonic signals, and use the activation functions asthe basis for the extraction of beat histogram features. Theidea of using NMF with prior knowledge of targeting sourcewithin the mixture has been applied in source separationtasks [32], multi-pitch analysis [26] and drum transcrip-tion [34]. Furthermore, the use of multiple novelty func-tions for the extraction of beat histograms has been pro-posed in [20]. Here, we combine both approaches for thegeneration of rhythmic features which are descriptive ofthe percussive rhythmic content of polyphonic tracks andtherefore of their general rhythmic character. We focus ontwo tasks: the investigation of their overall performance,in order to determine the salience of the features for genreclassiﬁcation; and their performance for each percussivecomponent (drum track) separately, attempting to extractconclusions regarding the importance of drum based rhythmfeatures and the salience of NMF activation functions.3. METHODThe basic concept of NMF is to approximate a matrixVwith matricesWandHasV⇡WHwith non-negativityconstraints. Given am⇥nmatrixV, NMF will decomposethe matrix into the product of am⇥rdictionary (or basis)matrixWand anr⇥nactivation matrixH, withrbeingthe rank of the NMF decomposition. In most audio applica-tions,Vis the spectrogram to be decomposed,Wcontainsthe magnitude spectra of the salient components, andHindicates the activation of these components with respect totime [31]. The matricesWandHare estimated through aniterative process that minimizes a distance measure betweenthe target spectrogramVand its approximation [30].To effectively extract drum activation functions from thepolyphonic signals, PFNMF is used in this study.Figure 1visualizes the basic concept from the work of Yoo et al.:the matricesWandHare split into the matricesWDandWH, andHDandHH, respectively. Instead of using co-factorization, PFNMF initializes the matrixWDwith drumcomponents and to not modify it during the factorizationprocess. MatricesWH,HH, andHDare initialized withrandom numbers. The distance measure used in this paper isthe generalized KL-divergence (or I-divergence), in whichProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 435Figure 2. Flowchart of NMF and beat histogram featureextraction and classiﬁcation system.DKL(x|y)=x·log (x/y)+(y\u0000x). The cost functionas shown in (1) is minimized by applying gradient descentand multiplicative update rules, the matricesWH,HH, andHDwill be updated according to Eqs. (2)–(4).J=DKL(V|WDHD+WHHH)(1)HD HDWTD(V/(WDHD+WHHH))WTD(2)WH WH(V/(WDHD+WHHH))HTHHTH(3)HH HHWTH(V/(WDHD+WHHH))WTH(4)PFNMF can be summarized in following steps:1.Construct anm⇥rDdictionary matrixWD, withrDbeing the number of drum components to be detected.2.Given a pre-deﬁned rankrH, initialize anm⇥rHmatrixWH, anrD⇥nmatrixHDand anrH⇥nmatrixHH.3.NormalizeWDandWH.4.UpdateHD,WH, andHHusing (2)–(4).5.Calculate the cost of the current iteration using (1).6.Repeat step 3 to step 5 until convergence.In our current setup, the STFT of the signals is calcu-lated using a window size and a hop size of 2048 and 512,respectively. A pre-trained dictionary matrix is constructedfrom the training set, consisting of isolated drum sounds.The templates are extracted for the three classes Hi-Hat(HH), Bass Drum (BD) and Snare Drum (SD) as the me-dian spectra of all individual events of one drum class inthe training set. Next, the PFNMF will be performed withrankrH= 10on the test ﬁles. More details of the trainingprocess and the selection of rankrHcan be found in [34].Finally, the activation MatrixHDcan be extracted from theaudio signals through the decomposition process.Once the activation functions of the three drum trackshave been extracted as described above, they are used asnovelty functions for the calculations of beat histograms,similar to [20]. The complete procedure for the genera-tion of a feature vector representing each track includesthe following steps: For each activation function, the beathistogram is extracted through the calculation of an Auto-correlation Function (ACF) and the retaining of the areabetween30and240BPM. For each beat histogram, the sub-features listed in Table 1 are extracted. The concatenationDistributionPeakMean (ME)Salience of Strongest Peak (A1)Standard Deviation (SD)Salience of 2nd Stronger Peak (A0)Mean of Derivative (MD)Period of Strongest Peak (P1)SD of Derivative (SDD)Period of 2nd Stronger Peak (P2)Skewness (SK)Period of Peak Centroid (P3)Kurtosis (KU)Ratio of A0 to A1 (RA)Entropy (EN)Sum (SU)Geometrical Mean (GM)Sum of Power (SP)Centroid (CD)Flatness (FL)High Frequency Content (HFC)Table 1. Subfeatures extracted from beat histograms.of all subfeature groups for each novelty function producesthe ﬁnal feature vector for an audio excerpt. Similar sub-features as listed in Table 1 can be found in the literature,e.g., in [33] (Peak), and [4, 13] (Distribution). In total,3novelty functions are used for the production of as manybeat histograms, from each of which19subfeatures areextracted, resulting in a total count of57features.4. EVALUATION4.1 Dataset DescriptionIn order to evaluate the features for multiple track kindspossessing different rhythmic qualities, two datasets wereconsidered: the Tzanetakis Dataset (GTZAN) [33], as anexample of a dataset which is widely used, comprising10030 sec excerpts for each of10diverse musical genres;and the Ballroom Dataset [5, 13] (Ballroom), comprising698 very rhythm/dance-oriented tracks of length 10 sec andtherefore suitable for the evaluation of our NMF-based beathistogram features. Both datasets contain tracks with adrum section and others with only non-percussive instru-ments. This does not only allow to investigate if the ex-tracted features are also suitable for music where a drumsection is present and if they can generalize to other musicstyles, but also allows conclusions as to what genres in par-ticular are represented satisfactory or insufﬁciently by thefeatures.4.2 Evaluation ProcedureThe features were tested using the Support Vector Machine(SVM) algorithm for supervised classiﬁcation. For our mul-ticlass setting, an RBF kernel was used and the optimalparameters (C,\u0000) were determined through grid search. Wechose the SVM classiﬁer since it has been frequently usedin similar genre classiﬁcation experiments, shows gener-ally good results (see [8]) and allows for comparabilitywith those studies. Since the focus here lay on the featuresand not the classiﬁcation algorithms, we refrained from us-ing more state-of-the-art approaches such as deep learningalgorithms. All experiments took place with a10-fold cross-validation (using90%of the data for training and10%fortesting over10randomly selected folds, taking the averageaccuracy over the folds for each dataset) and standardiza-tion (z-score) of the training and testing data. After the full436 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015GTZANBallroom01020304050607080\nDatasetsAccuracy (%)Classification Performance\n  NMF−BasedMult. Nov. (baseline)CombinedBass DrumSnare DrumHi−HatPrior\nFigure 3. Classiﬁcation results for both datasets.NMF-based feature set (i.e., the features originating fromall three drum activation functions) was tested, the featuresfrom each individual activation function were evaluated inturn in order to study the importance of each drum trackseparately. Finally, the NMF-based features are combinedwith other beat histogram features from a current study [20],extracted from novelty functions of amplitude (RMS), spec-tral shape (spectral ﬂux, centroid, ﬂatness and the ﬁrst13MFCCs) and tonal components (pitch chroma coefﬁcientsand tonal power ratio) on3second-long frames. Thosefeatures resulted from a similar procedure as the one usedhere, where30different novelty functions were extractedand their beat histograms computed through the calcula-tion of an ACF. A subsequent two-stage feature selectionscheme (mutual information with target data [14] using theCMIM metric [25], followed by a sequential forward selec-tion with an SVM wrapper [17]) was applied to retain thebest-performing features, resulting in a total of20featuresin each case.4.3 ResultsThe results are shown in Figure 3. On both datasets, the fullNMF feature set (comprising features from all three drumactivation functions) performs better than the individualones (BD, SD, HH), with an attained accuracy of36.6%and51.9%for GTZAN and Ballroom, respectively. Thosevalues lie considerably above the average priors of bothdatasets. The differences between the accuracies of thefeature sets are not large (especially between the individualdrum based feature sets) but are signiﬁcant at the0.05levelin all cases (based on a comparison test of the Cohen’sKappa extracted from the confusion matrices). Due totheir small values (ranging from0.2%to0.6%), standarddeviations between accuracies of the folds for each featureset are not presented in Figure 3.The multiple novelty feature set (from [20]) outperformsthe NMF-based features, reaching an accuracy of59.8%forthe GTZAN and67.7%for the Ballroom dataset, whereasthe combined set (NMF and multiple novelty) demonstratesthe best performance (accuracy of65.1%(GTZAN) and75.5%(Ballroom)). The individual feature sets from eachdrum track provide performance inferior to that of theCh.Ji.Qu.Ru.Sa.Ta.Vw.Wa.Ch.5410101114318Ji.17135610225Qu.1024453873Ru.8325347219Sa.1548250313Ta.216625577Vw.5396061719Wa.5041614476Acc.4922545458642669Pr.15.98.611.714.012.312.39.315 .8Table 2.Confusion matrix for Ballroom dataset, averageaccuracy:51.9%. Accuracy and Prior are given in%.Bl.Cl.Co.Di.Hi.Ja.Me.Po.Re.Ro.Bl.151116154799113Cl.463311145315Co.6638124566116Di.1316436185125Hi.84542181020137Ja.817507389772Me.311772651229Po.7665145533127Re.636664111534Ro.641010171011111011Acc.15633843213851335311Pr.10101010101010101010Table 3.Confusion matrix for GTZAN dataset, averageaccuracy:36.6%. Accuracy and Prior are given in%.full NMF-based set, but still considerably higher than theprior. The best individual drums are the BD and SD for theGTZAN and Ballroom datasets, respectively. The worstindividual percussion instrument is in both cases the HH.For the full NMF-based feature set, confusion matrices re-sulting from the classiﬁcation can be seen in Tables 2 and 3.In general, features achieved better average performanceon the Ballroom dataset than on the GTZAN. In order toevaluate the misclassiﬁcations and the performance of theindividual genres, a closer observation of the confusionmatrices of each dataset should be taken.For theBallroomdataset, confusions between genres ap-pear to be plausible based on what one would expect whenextracting rhythm features only from drums tracks: gen-res with strongly pronounced, stable rhythm played from adrum section such as samba andchachacha (Ch.)are con-fused with each other, whereas thewaltz (Wa.)andtango(Ta.)genres, having no drum section (but still a succinctrhythm) are not confused much with other genres. Thelatter are the two genres which also achieve the best individ-ual performance, followed bychachacha,quickstep (Qu.),rumba (Ru.)andsamba (Sa.).Jive (Ji.)andviennese waltz(Vw.)display the worse performance, and are confused withchachachaandwaltzrespectively, a result which is alsoexpected when one considers the rhythmic proximity ofthose genres, whether they possess a drum section or not.For theGTZANdataset, misclassiﬁcations present amore mixed picture: On the one hand, genres which pos-sess tracks featuring a well articulated, distinct rhythmicperformed by a drum section (such asreggae (Re.),metalProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 437(Me.)anddisco (Di.)) as well as the only genre withoutdrums (classical (Cl.)) achieve satisfactory performanceand are confused with genres which are rhythmically rela-tively close (classicalwithjazz (Ja.),metalwithrock (Ro.),discowithreggae, andreggaewithpop (Po.)). On theother hand, genres possessing tracks with a more “generic”rhythm (such ascountry (Co.)andpop) are confused withmultiple other genres. Finally,hiphop (Hi.),blues (Bl.)androckattain the last places in individual performance and areconfused with multiple other genres.5. DISCUSSIONThe results show that beat histogram features based onNMF activation functions of speciﬁc drums can be help-ful in rhythm-based genre classiﬁcation, as their accuracyfor the used datasets is comparable to that achieved byother rhythmic feature sets used up to date (59.8%[20] and28%[33] for the GTZAN,67.7%[20] and56.7%[13] forthe Ballroom dataset). When taking into account that thefeatures are solely based on drum novelty functions, theirperformance, especially for the Ballroom dataset, can beseen as satisfactory. It is clear, though, that for this rea-son, our results cannot achieve as high accuracy as otherstudies which use very sophisticated methods [8,18,22–24].Our results are somewhat lower than the state of the artusing rhythm [22,24] or combined features [8,23], howeverstaying in the same range. For the sake of comparison, wereport here the highest performances reached when usingadvanced rhythmic features: on the GTZAN dataset an ac-curacy of92.4%[22] has been achieved, for the Ballroomdataset one of96.1%[24]. The advantage of our proposedmethods and features lies in the ability to pinpoint the im-portance of the rhythm patterns from speciﬁc drums forspeciﬁc genres.The misclassiﬁcations (reported in Tables 2 and 3) showthat genres which do not feature genre-speciﬁc rhythm pat-terns, even if those are clearly articulated by the drum sec-tion (e.g., a 4/4 BD and SD alternating beat), tend to beconfused with other similar genres (especially when drumtracks are present, such as inrock). Genres containingnon-percussive tracks (such asclassicalandwaltz) or veryspeciﬁc rhythmic patterns (reggae) are more easily distin-guished from others. Those results indicate that the NMF-based beat histogram features indeed capture rhythmic prop-erties related to the drum section and the regularities of theirperiodicities, pointing towards the suitability of those fea-tures for the extraction of drum-based rhythmic propertiesand the use in the discrimination of musical tracks whichcontain drums from ones which do not.With regards to the feature sets, the satisfactory accuracyof the NMF-based feature set is a hint towards the appro-priateness of the features for the analysis of the rhythmiccharacter of a musical track. However, it is clear that thosefeatures, being derived only from drum tracks, cannot rep-resent as much information as features resulting from theuse of multiple novelty functions covering many aspects ofthe signal temporal progress. The improved performanceof the combined set (NMF and multiple novelty based)is a consequence of incorporating speciﬁc, drum-relatedrhythm information in the feature base, showing that theNMF-based rhythm feature set can contribute informationnot provided by more general rhythm features and lead tosigniﬁcant improvement for the two evaluated datasets. Theanalysis of the features derived from the activation functionof a speciﬁc drum track showed that mainly the snare drumand to a lesser extent the kick drum are the most impor-tant components. The tendency is strong for the Ballroomdataset, where the SD outperforms the BD, whereas for theGTZAN dataset the result is reversed but with a smallerdifference. In all cases (also between the individual drumsets), the differences in accuracies between the feature setsare signiﬁcant at the5%level. Those results can be dueto the very pronounced sound texture and greater power ofthose drums which leads to a salient activation function, aswell as their role in providing the basic metric positions inmost of western popular music. However, the accuracy ofeach subset lies below that of their combination, leadingto the conclusion that the activation functions of all threepercussion instruments contribute valuable information tothe feature description of musical genre.Concerning the datasets, the poorer classiﬁcation perfor-mance observed for the GTZAN dataset is a sign of the morediverse character of tracks and genres in this set, containingmusic styles which lack a speciﬁc rhythmic character andcan therefore not be distinguished effectively through beathistogram features derived from drum activation functions.Results were still better than the ones reported in [33],but their inferiority compared to the ones in other stud-ies [13, 20] shows that when considering a multitude ofdifferent genres, solely drum based activation functions cannot provide a complete rhythmic characterization. This,however, points towards the possible goal of using NMF inorder to transcribe not only drums but also other instrumentsin order to use their activation functions as a basis for beathistogram features. The Ballroom dataset shows better per-formance, which was to be expected since the tracks thereinare selected for belonging to different dance styles, requir-ing a special rhythmic pattern which is mostly conveyedby the drum section. The results are in the same range asthose provided in [13] (56.7%) when using only periodicityhistogram features. Furthermore, in the same study it wasshown that using the tempo of the given tracks as a featurethey could achieve very high results using a simple 1-NNclassiﬁer (51.7%for the “naive tempo” derived from the pe-riodicity histogram and82.3%for the ground-truth tempoprovided with the recordings), reaching as much as90%when combining the correct tempo with other descriptors(MFCCs) from the periodicity histogram. This shows thatbeat histograms (from which the tempo can be extracted)are a good tool for rhythmic analysis in datasets containingdance music such as the Ballroom.Regarding speciﬁc genres, it is clear from the resultsthat the NMF-based features have a twofold use: ﬁrst, inrepresenting genres which are characterized by distinct pat-terns in their drum sections (e.g.,reggaeorsamba) andsecond, in characterizing genres which lack a drum section438 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015at all (waltz,classical) in contrast to genres which do; theactivation functions transcribed in this case are maximallydifferent, leading to beat histogram features which can beeasily discriminated by a classiﬁer. Such a ﬁnding showsthat drum-based rhythm features can be very helpful forrhythmic characterization of speciﬁc genres, which couldbe an argument for their further application when a speciﬁckind of music is involved. As a general remark, it can beseen that genres possessing a stable rhythm articulated bya drum section such asreggaeandsambaor genres lack-ing drums in general (waltzandclassical) perform better,whereas genres which have a very uncharacteristic rhythm(such asrockorblues) get more easily confused.6. CONCLUSIONSThe work presented in this paper focuses on the creationof novel, NMF-based beat histogram features for rhythm-based musical genre classiﬁcation and rhythmic similarity.The difference in comparison to other well-known studiesfor rhythm features based on beat histograms [4, 13, 24, 33]is the use of the activity functions of speciﬁc drums pro-vided through NMF as a basis for the calculation of thebeat histogram. We showed that the classiﬁcation accuracyusing these beat histogram features is comparable to thatof other rhythm features, whereas our proposed featuresare better especially for characterizing tracks with speciﬁcrhythmic patterns or for distinguishing between songs withand without a drum section. It was observed that the mostimportant percussion patterns for dance music classiﬁcationwere generated by the snare and the kick drum, which un-derlines the importance of its activation function for furthertasks.One future goal is the expansion of the use of NMF toidentify more instruments or voices and use them as pos-sible novelty functions. The goal would be to thereforecapture the rhythmic patterns of every instrument, essen-tially joining source transcription and rhythm feature extrac-tion into one module. Another possibility is the use of ourproposed features for larger and more speciﬁc datasets, inorder to further investigate their suitability for speciﬁc gen-res, as well as the strengths and weaknesses of the patternsextracted from individual drums in discriminating betweenmusical genres. As an expansion of the feature selectionprocedure, a further idea would be to proﬁt from the combi-nation of NMF-based features and other acoustic featuresusing a classiﬁer that is capable of learning feature impor-tance (e.g. random forest) to quantitatively investigate theimportance of NMF-derived features. While NMF-basedbeat histogram features have been evaluated only in thecontext of rhythmic genre classiﬁcation, we believe thatthey can prove useful in other tasks. Future research willfocus on adjusting and using the proposed features for MIRtasks such as rhythmic similarity computation and structuralanalysis.7. REFERENCES[1]David S Alves, Jouni Paulus, and Jos´e Fonseca. Drumtranscription from multichannel recordings with non-negative matrix factorization. InProceedings of theEuropean Signal Processing Conference (EUSIPCO),Glasgow, Scotland, UK, 2009.[2]Juan P. Bello, Laurent Daudet, Samer Abdallah, ChrisDuxbury, Mike Davies, and Mark B Sandler. A tutorialon onset detection in music signals.IEEE Transactionson Speech and Audio Processing, 13(5):1035–1047,2005.[3]Juan P. Bello, Chris Duxbury, Mike Davies, and MarkSandler. On the use of phase and energy for musicalonset detection in the complex domain.IEEE SignalProcessing Letters, 11(6):553–556, 2004.[4]Juan Jos´e Burred and Alexander Lerch. A hierarchi-cal approach to automatic musical genre classiﬁcation.InProceedings of the 6th international conference ondigital audio effects, pages 8–11, 2003.[5]Simon Dixon, Elias Pampalk, and Gerhard Widmer.Classiﬁcation of dance music by periodicity patterns.InProceedings of the 4th International Conference onMusic Information Retrieval (ISMIR), 2003.[6]Derry FitzGerald, Bob Lawlor, and Eugene Coyle.Drum transcription in the presence of pitched instru-ments using prior subspace analysis. InProceedings ofthe Irish Signals and Systems Conference (ISSC), 2003.[7]Derry FitzGerald, Robert Lawlor, and Eugene Coyle.Sub-band independent subspace analysis for drum tran-scription. InProceedings of the Digital Audio EffectsConference (DAFX), pages 65–59, 2002.[8]Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Deng-sheng Zhang. A survey of audio-based music classiﬁca-tion and annotation.IEEE Transactions on Multimedia,13(2):303–319, 2011.[9]Olivier Gillet and Ga¨el Richard. Transcription and sep-aration of drum signals from polyphonic music.IEEETransactions on Audio, Speech, and Language Process-ing, 16(3):529–540, 2008.[10]Masataka Goto and Yoichi Muraoka. Music understand-ing at the beat level – real-time beat tracking for au-dio signals. InComputational auditory scene analysis,pages 157–176, August 1995.[11]Masataka Goto and Yoichi Muraoka. A real-time beattracking system for audio signals. InProceedings ofthe International Computer Music Conference (ICMC),pages 171–174, 1995.[12]Fabien Gouyon and Simon Dixon. A review of auto-matic rhythm description systems.Computer MusicJournal, 29(1):34–35, 2005.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 439[13]Fabien Gouyon, Simon Dixon, Elias Pampalk, and Ger-hard Widmer. Evaluating rhythmic descriptors for musi-cal genre classiﬁcation. InProceedings of the AES 25thInternational Conference, pages 196–204, 2004.[14]Isabelle Guyon and Andr´e Elisseeff. An introduction tovariable and feature selection.The Journal of MachineLearning Research, 3:1157–1182, 2003.[15]Stephen Hainsworth and Malcolm Macleod. Onset de-tection in musical audio signals. InProceedings ofthe International Computer Music Conference (ICMC),2003.[16]Enric Guaus i Termens. New approaches for rhythmicdescription of audio signals. Technical report, MusicTechnology Group, Universitat Pompeu Fabra, 2004.[17]Ron Kohavi and George H John. Wrappers for featuresubset selection.Artiﬁcial intelligence, 97(1):273–324,1997.[18]Chang-Hsing Lee, Jau-Ling Shih, Kun-Ming Yu, andHwai-San Lin. Automatic music genre classiﬁcationbased on modulation spectral analysis of spectral andcepstral features.Multimedia, IEEE Transactions on,11(4):670–682, 2009.[19]Justin London.Hearing in time. Oxford UniversityPress, 2012.[20]Athanasios Lykartsis. Evaluation of accent-based rhyth-mic descriptors for genre classiﬁcation of musi-cal signals. Master’s thesis, Audio CommunicationGroup, Technische Universit¨at Berlin, (www.ak.tu-berlin.de/menue/abschlussarbeiten/), 2014.[21]Arnaud Moreau and Arthur Flexer. Drum transcriptionin polyphonic music using non-negative matrix factori-sation. InProceedings of the 8th International Confer-ence on Music Information Retrieval (ISMIR), pages353–354, 2007.[22]Yannis Panagakis, Constantine Kotropoulos, and Gon-zalo R Arce. Music genre classiﬁcation using local-ity preserving non-negative tensor factorization andsparse representations. InProceedings of the 10th In-ternational Conference on Music Information Retrieval(ISMIR), 2009.[23]Yannis Panagakis, Constantine L Kotropoulos, and Gon-zalo R Arce. Music genre classiﬁcation via joint sparselow-rank representation of audio features.IEEE/ACMTransactions on Audio, Speech and Language Process-ing (TASLP), 22(12):1905–1917, 2014.[24]Geoffroy Peeters. Spectral and temporal periodicity rep-resentations of rhythm for the automatic classiﬁcationof music audio signal.IEEE Transactions on Audio,Speech and Language Processing, 19(5):1242–1252,2011.[25]Hanchuan Peng, Fulmi Long, and Chris Ding. Featureselection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.IEEETransactions on Pattern Analysis and Machine Intelli-gence, 27(8):1226–1238, 2005.[26]Stanisław A Raczy´nski, Nobutaka Ono, and ShigekiSagayama. Multipitch analysis with harmonic nonnega-tive matrix approximation. InProceedings of the 8th In-ternational Conference on Music Information Retrieval(ISMIR), 2007.[27]Axel Roebel. Onset detection in polyphonic signals bymeans of transient peak classiﬁcation. InProceedings ofthe 6th International Conference on Music InformationRetrieval (ISMIR), 2005.[28]Nicolas Scaringella, Giorgio Zoia, and Daniel Mlynek.Automatic genre classiﬁcation of music content: a sur-vey.IEEE Signal Processing Magazine, 23(2):133–141,2006.[29]Eric D Scheirer. Tempo and beat analysis of acousticmusical signals.The Journal of the Acoustical Societyof America, 103(1):588–601, 1998.[30]D Seung and L Lee. Algorithms for non-negative ma-trix factorization. InAdvances in neural informationprocessing systems, pages 556–562, 2001.[31]Paris Smaragdis and Judith C Brown. Non-negativematrix factorization for polyphonic music transcription.InIEEE Workshop on Applications of Signal Processingto Audio and Acoustics., pages 177–180, 2003.[32]Paris Smaragdis, Bhiksha Raj, and MadhusudanaShashanka. Supervised and semi-supervised separationof sounds from single-channel mixtures. InIndepen-dent Component Analysis and Signal Separation, pages414–421. Springer, 2007.[33]George Tzanetakis and Perry Cook. Musical genre clas-siﬁcation of audio signals.IEEE transactions on Speechand Audio Processing, 10(5):293–302, 2002.[34]Chih-Wei Wu and Alexander Lerch. Drum transcriptionusing partially ﬁxed non-negative matrix factorization.InProceedings of the European Signal Processing Con-ference (EUSIPCO), 2015.[35]Jiho Yoo, Minje Kim, Kyeongok Kang, and SeungjinChoi. Nonnegative matrix partial co-factorization fordrum source separation. InProceedings of the IEEE In-ternational Conference on Acoustics Speech and SignalProcessing (ICASSP), pages 1942–1945, 2010.440 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Probabilistic Modular Bass Voice Leading in Melodic Harmonisation.",
        "author": [
            "Dimos Makris",
            "Maximos A. Kaliakatsos-Papakostas",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416374",
        "url": "https://doi.org/10.5281/zenodo.1416374",
        "ee": "https://zenodo.org/records/1416374/files/MakrisKC15.pdf",
        "abstract": "Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmoni- sation, since they capture statistical rules of the music id- ioms they are trained with. Proposed methodologies fo- cus either on specific aspects of harmony (e.g., generat- tion of many harmonic characteristics in a single proba- bilistic generative scheme. This paper addresses the prob- lem of assigning voice leading focussing on the bass voice, chord sequence, under the scope of a modular melodic har- monisation system where different aspects of the genera- tive process are arranged by different modules. The pro- posed technique defines the motion of the bass voice ac- cording to several statistical aspects: melody voice con- tour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are mod- ular, i.e., each criterion is defined by independent statisti- cal learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The pro- posed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic har- monisations).",
        "zenodo_id": 1416374,
        "dblp_key": "conf/ismir/MakrisKC15",
        "keywords": [
            "probabilistic methodologies",
            "automated music composition",
            "melodic harmonization",
            "voice leading",
            "bass voice",
            "chord sequence",
            "modular melodic harmonisation system",
            "voice motion",
            "statistical aspects",
            "novel blended melodic harmonisations"
        ],
        "content": "PROBABILISTIC MODULAR BASS VOICE LEADING IN MELODICHARMONISATIONDimos MakrisDepartment of Informatics,Ionian University,Corfu, Greecec12makr@ionio.grMaximos Kaliakatsos-PapakostasSchool of Music Studies,Aristotle University ofThessaloniki, Greecemaxk@mus.auth.grEmilios CambouropoulosSchool of Music Studies,Aristotle University ofThessaloniki, Greeceemilios@mus.auth.grABSTRACTProbabilistic methodologies provide successful tools forautomated music composition, such as melodic harmoni-sation, since they capture statistical rules of the music id-ioms they are trained with. Proposed methodologies fo-cus either on speciﬁc aspects of harmony (e.g., generat-ing abstract chord symbols) or incorporate the determina-tion of many harmonic characteristics in a single proba-bilistic generative scheme. This paper addresses the prob-lem of assigning voice leading focussing on the bass voice,i.e., the realisation of the actual bass pitches of an abstractchord sequence, under the scope of a modular melodic har-monisation system where different aspects of the genera-tive process are arranged by different modules. The pro-posed technique deﬁnes the motion of the bass voice ac-cording to several statistical aspects: melody voice con-tour, previous bass line motion, bass-to-melody distancesand statistics regarding inversions and note doublings inchords. The aforementioned aspects of voicing are mod-ular, i.e., each criterion is deﬁned by independent statisti-cal learning tools. Experimental results on diverse musicidioms indicate that the proposed methodology capturesefﬁciently the voice layout characteristics of each idiom,whilst additional analyses on separate statistically trainedmodules reveal distinctive aspects of each idiom. The pro-posed system is designed to be ﬂexible and adaptable (forinstance, for the generation of novel blended melodic har-monisations).1. INTRODUCTIONIn melodic harmonisation systems harmony is expressedas a sequence of chords, but an important aspect is alsothe relative placement of the notes that comprise chord se-quence, which is known as thevoice leadingproblem. Asin many aspects of harmony, in voice leading there are cer-tain sets of diverse conventions for different musicidiomsc\u0000Dimos Makris, Maximos Kaliakatsos-Papakostas, Emil-ios Cambouropoulos.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Dimos Makris, Maximos Kaliakatsos-Papakostas, Emilios Cambouropoulos. “Probabilistic modular bass voiceleading in melodic harmonisation”, 16th International Society for MusicInformation Retrieval Conference, 2015.that need to be taken under consideration. Such rules havebeen hand-coded by music experts for the development ofrule-based melodic harmonisation systems (see [15] for areview of such methods). Similarly, such hand-coded ruleshave been utilised as ﬁtness criteria for evolutionary sys-tems (see [4, 18] among others). However, the speciﬁca-tion of rules that are embedded within these systems arevery complex with many variations and exceptions. Ad-ditionally, the formalisation of such rules has not yet beenapproached for musical idioms that have not hitherto beenthoroughly studied. Most of the works so far, have focusedon either ﬁnding a satisfactory chord sequence for a givenmelody (performed by the soprano voice), or on complet-ing the remaining three voices that constitute the harmonyfor a given melodic or bass line (known as the “four-partharmony” task) [5, 14, 18, 24]. Experimental evaluationof methodologies that utilise statistical machine learningtechniques demonstrated that an efﬁcient way to harmonisea melody is to add the bass line ﬁrst [22]. To this end, themotivation behind the work presented in the paper at handis further enforced by the ﬁndings in the aforementionedpaper.This study, is based on the following underlying melodicharmonisation strategy: 1) analyse a give melody in termsof segmentation, scale/pitch hierarchy, harmonic/embellish-ment notes, harmonic rhythm (this can be achieved auto-matically or, at this stage, manually), 2) assign abstractchords to the given melody from learned ﬁrst-order chordtransition tables, 3) select concrete pitches from abstractchords for the bass-line based on learned melody-to-bass-line movement (discussed in this paper), 4) select concretepitches for inner voices (steady or varied number of notesper chord). This scheme would seem to be adequate for alarge body of non-monophonic music, but not all. For in-stance, even the mere concept of chords (with inversions)is rather controversial in European music before the mid-eighteenth century and in other traditional polyphonic mu-sics; more so, the idea of melody with chords and func-tional bass line is untenable in such music.However, as the aim of this project is not individualfully-ﬂeshed harmonic models of different idioms, but rathera general-as-possible method to ‘extract’ basic componentsof harmonic content in various harmonic textures, it is pos-sible to employ the above strategy in any non-monophonictexture. It is known that outer voices tend to stand out per-323ceptually (e.g. in [6]); additionally, note simultaneities canbe encoded in a more abstract manner (e.g., GCT represen-tation). Employing a computational methodology basedon such generic concepts, can enable the construction of a‘generic’ melodic harmoniser that can use harmonic com-ponents from various idioms, without claiming to emulatethe idioms themselves.This paper proposes a modular methodology for deter-mining the bass voice leading, to be integrated in a melodicharmonisation system under development. The effective-ness of the proposed methodology that performs bass voiceleading according to statistics describing the overall voic-ing layout (i.e. arrangement of pitches) of given chord se-quences in the General Chord Type (GCT) [2] represen-tation is examined. This methodology is extending thebass voice leading scheme presented in [12], by harnessingvoicing layout information through additional voicing lay-out statistical, independently trained,modulesconcerningthe chords that constitute the harmonisation. Those charac-teristics include distributions on the distance between thebass and the melody voice and statistics regarding the in-versions and doublings of the chords in the given chordsequence. By training these modules on multiple diverseidioms, a deeper study is pursued within the context ofthe COINVENT project [20], which examines the devel-opment of a computationally feasible model for conceptualblending. Thereby, blending different modules from dif-ferent idioms will expectedly lead to harmonisations withblended characteristics.2. PROBABILISTIC MODULAR BASS VOICELEADINGGiven the fact that a melody is available in systems thatperform melodic harmonisation, the methodology presentedin [12] derives information from the melody voice in or-der to calculate the most probable movement for the bassvoice, named as thebass voice leading(BVL). This ap-proach, in combination with information regarding thevoicelayout(Section 2.2), is incorporated into alarger mod-ular probabilistic framework. In the integrated modularmelodic harmonisation system under development, the se-lection of chords (in GCT form [2]) is performed by an-other probabilistic module [10] not discussed in this pa-per. Therefore, the herein discussed modules have beendeveloped to provide indications about possible movementof the bass as well as to deﬁne speciﬁc notes for the bassvoice, providing a ﬁrst step to complete information re-garding speciﬁc voices from the chords provided by thechord selection module.To this end, both the bass and the melody voice stepsare represented by abstract notions that describe generalquantitative information on pitch direction. In [12] sev-eral scenarios for voice contour reﬁnement were exam-ined, providing different levels of accuracy for describ-ing the bass motion in different datasets. In the paper athand, the selected methodology is the one with the great-est level of detail, i.e. the scenario where the melody andbass note changes are divided in seven steps, as exhibitedin Table 1. While different range schemes could have beenselected, the rationale behind the utilised one is that theperfect fourth is considered as a small leap and the perfectﬁfth as a big leap.reﬁnementshort namerange (semitones)steady voicestvx=0step upsup16x62step downsdown\u000026x6\u00001small leap upslup36x65small leap downsldown\u000056x6\u00003big leap upblup5<xbig leap downbldownx<\u00005Table 1. The pitch step and direction reﬁnement scale con-sidered for the development of the utilised bass voice lead-ing system.2.1 The hidden Markov model moduleThe primary module for deﬁning bass motion functionsunder the ﬁrst order Markov assumption in combinationwith the fact that it depends on the piece’s melody. Tothis end, the next step of the bass voice contour (bass di-rection descriptor) is dependent on the previous one andon the current melody contour (melody direction descrip-tor). This assumption, based on the fact that a probabilisticframework is required for the harmonisation system, mo-tivates the utilisation of thehidden Markov model(HMM)methodology. According to the HMM methodology, a se-quence of observed elements (melody direction descrip-tor) is given and a sequence of (hidden) states (bass di-rection descriptor) is produced as output. The “order” ofthe HMM utilised in the presented work, i.e. how manyprevious steps are considered to deﬁne the current, is 1.In melodic harmonisation literature different orders havebeen examined, e.g. [19], where it is shown that order 1might not be the most efﬁcient. In the context of the pre-sented work, this investigation is part of future research.The HMM training process extracts four probability val-ues for each bass motion: 1) to begin the sequence, 2) toend the sequence, 3) to follow another bass motion (transi-tion probability) and 4) to be present given a melody step(observation probability). The probabilities extracted bythis process for each possible next bass motion is denotedwith a vector of probabilities~pm(one probability for eachpossible bass motion step) and will be utilised in the prod-uct of probabilities from all modules in Equation 1.2.2 The voicing layout information moduleIn order to assign a bass voice to a chord, additional in-formation is required that is relevant to the chords of theharmonisation. The voicing layout statistics that are con-sidered for the modules of the presented methodology aretheinversionsand thedoublingsof chords. The inver-sions of a chord play an important role in determining howeligible is a chord’s pitch class to be a bass note, whilethe doublings indicate if additional “room” between the324 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015bass and the melody is required to ﬁt doublings of spe-ciﬁc pitch classes of the chords. For instance, the chordwith pitch classes[0,4,7]has three inversions, with eachone having a bass note that corresponds to a different pitchclass, e.g.[60,64,67],[64,67,72]or[67,72,76], while,by considering the inversion prototype[60,64,67]of the[0,4,7]chord, there are four scenarios of single note dou-blings:[60,64,67,72],[60,64,67,76],[60,64,67,79]and[60,64,67](no-doubling scenario).The voicing layout module of the harmonic learningsystem regarding chord inversions and note doublings, istrained through extracting relevant information from every(GCT) chord in pieces from a music idiom. Speciﬁcally,consider a GCT chord in the formg=[r,~t], whereristhe root of the chord in relation to the root of the key and~tis the vector describing the type of the chord. For instance,the I chord in any key is expressed asg=[ 0,[0,4,7]]inthe GCT representation, where 4 denotes the major thirdand 7 the perfect ﬁfth. This GCT type is a set of integers,~t=[t1,t2,. . .,tn], wherenis the number of type ele-ments, that can be directly mapped to relative pitch classes(PCs). The statistics concerning chord inversion are ex-pressed as the probability that each type element ingis thebass note of the chord, orpi=(v1,v2,. . .,vn),wherevi,i2{1,2,...,n}, is the probability that the ele-menttiis the bass note. Similarly, probabilities about notedoublings are expressed through a probability vectorpd=(d1,d2,. . .,dn,s),wheredi,i2{1,2,...,n}, is the probability that the pitchclasstigets doubled, while there is an additional value,s, that describes the probability that there is no doublingof pitch classes. Table 2 exhibits the extracted statisticsfor inversions and note doublings for the most often metchords of the major Bach Chorales.2.3 The melody-to-bass distance moduleAn important aspect of voice layout has to do with abso-lute range of chords in the chord sequences of an idiom,i.e. the absolute difference between the bass voice and themelody. Different idioms encompass different constraintsand characteristics concerning this voicing layout aspect,according to several factors, e.g., the utilised instruments’range. The proposed methodology addresses this voicinglayout aspect by capturing statistics about theregionthatthe bass voice is allowed to move according to the melody.Therefore, histograms are extracted that describe the fre-quency of all melody-to-bass intervals found in a trainingdataset, as illustrated by the bars in the example in Fig-ure 1.However, interval-related information in the discussedcontext are used only as approximate indicators about theexpected pitch height of the bass voice, while the exactintervals (bars in Figure 1) are referring to speciﬁc inter-vals and, additionally, they are scale-sensitive, e.g. differ-\nFigure 1. Histogram of pitch interval distances betweenmelody and bass for a set of major Bach Chorales.ent scales potentially produce different distributions of me-lody-to-bass intervals. Therefore, the “expected” bass pitchheight is approximated by a normal distribution that is ad-justed to ﬁt the distribution of the melody-to-bass intervalsobserved in the dataset. Figure 1 illustrates the normal dis-tribution that is approximates the distributions of intervalsfor a collection of major Bach Chorales.2.4 Combining all modulesThe probabilities gathered from all the modules describedhitherto are combined into a single value, computed as theproduct of all the probabilities from all the incorporatedmodules. To this end, for each GCT chord (C) in the com-position every possible scenario of chord inversions, dou-blings and bass note pitch height, denoted by an indexx,is generated. For each scenario (x), the product (bx(C))of all the modules discussed so far is computed, i.e. thebass motion (pmx(C)), the inversions (pix(C)), doublings(pdx(C)) and melody-to-bass intervalphx(C):bx(C)=pmx(C)pix(C)pdx(C)phx(C). (1)Therefore, the best scenario (xbest) for the bass voice ofchordCis found by:xbest= arg maxx(bx(C)). The bassnote motion probability is obtained by the HMM moduleanalysed in Section 2.1 and it takes a value given by thevector~pmaccording to the bass step it leads to.3. EXPERIMENTAL RESULTSThe aim of the experimental process is to evaluate whetherthe proposed methodology efﬁciently captures the bass voiceleading according to several factors related to the voice lay-out characteristics of each training idiom. Additionally, itis examined whether the separate trained modules, whichconstitute the overall system, statistically reveal aspects ofeach idiom that are more distinctive. A collection of eightdatasets has been utilised for training and testing the capa-bilities of the proposed methodology, exhibited in Table 3.These pieces are included in a music database with manydiverse music idioms and it is developed for the purposesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 325GCT chordrelative PCinversionsdoublings[0,[0,4,7]][0,4,7][0.74,0.23,0.02][0.68,0.15,0.08,0.09][7,[0,4,7]][7,11,2][0.78,0.22,0.00][0.83,0.02,0.09,0.06][5,[0,4,7]][5,9,0][0.65,0.34,0.01][0.46,0.30,0.11,0.13]Table 2. Probabilities for chord inversion (pi) and note doublings (pd) in the three most frequently used chords in the majorChorales of Bach.Name (number)DescriptionBach Chorales (35)a set of Bach choralesBeatles (10)set of songs from the band BeatlesEpirus (29)traditional polyphonic songs fromMedieval (12)fauxbourdon and organum piecesModal chorales (34)15th-16th century modal choralesRembetika (22)folk Greek songsStravinsky (10)pieces composed by Igor StravinskyTango (24)pieces of folk tango songsTable 3. Dataset description.of the COINVENT project. For the presented experimen-tal results, each idiom set includes from around 50 to 150phrases. The Bach Chorales have been extensively utilisedin automatic probabilistic melodic harmonisation [1, 7, 13,16], while the polyphonic songs of Epirus [9,11] and Rem-betika [17] constitute datasets that have hardly been usedin studies.3.1 Cross-entropies for training and testing in allidiom combinationsThe cross-entropy tests include the statistical modules thatare independent of the GCT chords, i.e. HMM model andthe melody-to-bass distance ﬁtted distribution (will herebybe symbolised asmbd). Additionally, to examine the ef-fect of the transition and the observation probabilities, theprobabilities related to transitions of the bass (states transi-tions and will hereby be symbolised astr) and the melodyvoice (observation transitions and will hereby be symbol-ised asmel) will be examined separately. The statisti-cal combinations examined during the experimental eval-uation process are: 1) the HMM model and the melody-to-bass distance ﬁtted distribution probabilities (Mall), 2)only the bass voice transition probabilities from the HMM(Mtr), 3) only the melody observation probabilities fromthe HMM (Mmel) and 4) only the Melody-to-bass distancedistributions (Mmbd).Each idiom’s dataset is divided in two subsets, a trainingand a testing subset, with a proportion of 90% to 10% ofthe entire idiom’s pieces. The training subset of an idiomXis utilised to train the aforementioned modules, form-ing the trained modelMX, while the testing subset of thesame idiom will be hereby denoted asDX. For instance,the HMM trained with the Bach Chorales will be symbol-ised asMBachwhile its testing pieces will be symbolisedasDBach. The evaluation of whether a modelMXpredictsa subsetDXbetter than a subsetDYis achieved throughthe cross-entropy measure. The measure of cross-entropyis utilised to provide an entropy value for a sequence froma dataset,{Si,i2{1,2,...,n}} 2DX, according to thecontext of each sequence element,Si, denoted asCi, asevaluated by a modelMY. The value of cross-entropy un-der this formalisation is given by\u00001nnX1logPMY(Si,Ci,MY), (2)wherePMY(Si,Ci,MY)is the probability value accordingto the examined scenarios of probabilities.By comparing the cross-entropy values of a sequenceXas predicted by two models,DXandDY, we can assumewhich model predictsSbetter: the model that produces thesmallercross entropy value [8]. Smaller cross entropy val-ues indicate that the elements of the sequenceS“move ona path” with greater probability values. Tables 4 exhibitsthe cross-entropy values produced by the proposed modelfor the examined scenarios. The presented values are av-erages across 100 repetitions of the experimental process,with different random divisions in training and testing sub-sets (preserving a ratio of 90%-10% respectively for allrepetitions). In every repetition the average cross entropyof all the testing sequences is calculated. The effective-ness of the combined proposed modules is indicated by thefact that most of the minimum values per row are on themain diagonal of the upper part of the matrix, i.e. wheremodelMallXpredictsDXbetter than any otherDY. A 10-fold cross-validation routine was also tested for splittingthe dataset, however, replications of the experiment wheredifferent pieces in training and testing sets were used, gaveconsiderably different results. The utilised experimentalsetup was providing similar results in several replicationsof the experiment.It is evident that each module isolated does not producelower values in the diagonal. Among the clearest isolatedcharacteristics is the melody observations part of the HMM(Mmel), where 5 out of 8 diagonal elements are the lowestin their row. Thereby, these results indicate that the com-bination of all modules is a vital part for achieving betterresults.3.2 Diversity in inversions and doublings of GCTchordsA straightforward comparison in statistics related to inver-sions and doublings between GCTs of different idioms isnot possible for all idioms and all GCTs, since this infor-mation is harnessed on GCT sets that are in many casesdifferent for different idioms. The differences in character-istics about voicing layout between different sets of GCTsthat could be envisaged, relate to thediversityof the voic-ing layout scenarios that are used across different idioms.326 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015DBachDBeattlesDEpirusDMedievalDModalDRembetikaDStravinskyDTangoMallBach7.1711.0715.7510.797.419.7711.868.88MallBeattles9.757.8215.9714.869.778.277.649.01MallEpirus16.6419.626.9910.5413.1114.3016.1116.46MallMedieval10.9617.567.687.478.4912.4616.1812.63MallModal9.2715.9415.0410.968.3910.8915.3210.72MallRembetika8.738.5613.6511.798.227.117.808.29MallStravinsky14.1910.8217.4519.8815.8410.999.7613.88MallTango8.278.7814.6211.337.987.629.357.70MtrBach2.092.613.162.252.242.992.972.62MtrBeattles3.512.332.473.302.881.822.282.20MtrEpirus5.393.172.044.904.312.062.643.78MtrMedieval2.732.921.972.332.332.492.743.11MtrModal2.872.922.822.413.322.792.733.07MtrRembetika4.112.661.903.533.211.671.882.62MtrStravinsky5.443.982.514.514.732.633.504.50MtrTango3.112.162.822.983.021.882.552.12MmelBach1.792.142.281.951.852.342.442.15MmelBeattles2.341.922.092.261.931.651.871.86MmelEpirus2.722.431.422.212.431.721.742.59MmelMedieval2.543.322.152.132.502.362.513.04MmelModal2.682.602.572.642.362.122.552.59MmelRembetika2.812.131.862.392.201.372.172.00MmelStravinsky3.773.122.293.853.392.832.533.77MmelTango2.331.861.942.361.901.482.171.72MmbdBach3.586.5110.506.773.554.455.654.25MmbdBeattles4.904.2412.1710.135.634.723.905.38MmbdEpirus9.0314.893.514.146.8310.3112.0410.34MmbdMedieval6.1013.053.773.934.577.7210.827.15MmbdModal4.4411.5310.356.483.476.189.705.63MmbdRembetika3.794.8010.596.803.924.114.324.20MmbdStravinsky5.874.5612.9112.088.006.184.676.73MmbdTango3.645.3510.386.563.704.124.784.19Table 4. Mean values of cross-entropies for all pairs of datasets, for all the combination of all probabilities, as well as inisolation concerning previous bass motion, melody motion and bass-to-melody distance.Along these lines, the question would be: are there morediverse chord expressions regarding inversions and dou-blings – regardless of which chords (GCTs) – in the choralesof Bach, than in the modal chorales? Thediversityin a dis-crete probability distribution (like the ones displayed in theexamples of Table 2) is measured by the Shannon informa-tion entropy [21] (SIE). The SIE reﬂects the diversity inpossibilities described by discrete probability distribution,with higher SIE values indicating a more random distribu-tion with more diverse / less expectable outcomes. There-fore, by measuring the SIE values of all GCTs and com-paring them for every pair of idioms, it can be concludedwhether some idioms have richer possibilities for the voic-ing layouts of chords than others.Table 5 exhibits the results of a test in the statisticalsigniﬁcance in differences between the SIE values in ev-ery pair of idioms. The upper-diagonal elements concerninversions, while lower-diagonal elements doublings. Avalue of+1indicates that the GCTs in the idiom of the roware statistically signiﬁcantly more diverse in their voicinglayout – according to the mean SIE values – than the onesin the idiom of the column. A\u00001value indicates the op-posite, while a0value indicates no statistically signiﬁcantdifference. The statistical signiﬁcance is measured througha two–sided Wilcoxon [23] rank sum test, which is appliedon the SIE values of all GCT voicing layout distributionsfor every idiom. The statistical signiﬁcance test in statisticsrelated to voice layout reveal that few datasets are signiﬁ-cantly superior or inferior regarding their diversity.3.3 Example compositionsThe proposed bass voice leading methodology was utilisedin an “off-line” mode to produce two examples. The term“off-line” indicates the fact that the system was used togenerate a single description for the bass voice leading on agiven set of chords (in GCT representation [2] produced bya probabilistic chord-generation model [10]). This meansthat if no inversion of the predetermined chord can satisfythe requirements of the bass voice leading, then the systemsimply selected the most probable inversion of this chord,regardless of the bass voice leading indication. The bassvoice for the generated examples was selected using theargmaxfunction mentioned in Section 2.4, which allowsthe reﬂection of some typical idiom characteristics, eventhough such an approach does not necessarily guaranty in-terestingness [3] (since the most “expected” scenario is fol-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 327SBachSBeattlesSEpirusSMedievalSModalSRembetikaSStravinskySTangoSBach011\u000011111SBeattles000\u000010000SEpirus000\u000010000SMedieval\u00001\u00001\u0000101111SModal00010110SRembetika0\u00001010000SStravinsky00101100STango00010000Table 5. Statistical signiﬁcance of differences in the diversity of inversions (upper diagonal) and doublings (lower diago-nal). Statistically signiﬁcant superiority of diversity in the row dataset is exhibited with a+1, of the column dataset with\u00001, while0indicates no statistical signiﬁcance in diversity differences.lowed). The intermediate voices where manually adjustedby a music expert.The presented examples (Figure 2) include two alterna-tive harmonisations of a Bach Chorale melody with boththe chord generation and the bass voice leading systemstrained on sets of (a) the Bach Chorales and (b) polyphonicsongs from Epirus. In the case of the Bach chorale, the sys-tem made erroneous bass voice assignments in the secondbar that create consecutive anti-parallel octaves betweenthe outer voices (due to the chord incompatibility problemdiscussed above)1. The harmonisation in the style of thepolyphonic songs from Epirus indeed preserves an impor-tant aspect of these pieces: the drone note.\n(a) Bach Chorale style\n(b) Polyphonic Epirus songs styleFigure 2. Harmonisation examples in two different styles.Chord sequences in the GCT representation were previ-ously produced by another probabilistic system.4. CONCLUSIONSThis paper presented a modular methodology for determin-ing the bass voice leading in automated melodic harmoni-sation given a melody voice and a sequence of chords. Inthis work it is assumed that harmony is not solely the ex-pression of a chord sequence, but also of harmonic move-ment for all voices that comprise the harmonisation. Thepresented work focuses on generating the bass voice on agiven sequence of chords by utilising information from the1Another voice-leading issue occurs at the ﬁrst beat of the 3rd bar,where the D in the 2nd voice is introduced as unprepared accented disso-nance. Note that the parenthesised pitches in the 3rd voice (bar 2) wereintroduced manually (not by the system) to create imitation.soprano /melody voice and other statistics that are relatedto the layout of the chords, captured by different statisti-cal modules. Speciﬁcally, a hidden Markov model (HMM)is utilised to determine the most probable movement forthe bass voice (hidden states), by observing the sopranomovement (set of observations), while additional voicinglayout characteristics of the incorporated chords are con-sidered that include distributions on the distance betweenthe bass and the melody voice and statistics regarding theinversions and doublings of the chords in the given chordsequence.Experimental results evaluate that the learned statisti-cal values from an idiom’s data are in most cases efﬁ-cient for capturing the idiom’s characteristics in compar-ison to others. Additionally, similar tests were performedfor each statistical module of the model in isolation, a pro-cess that revealed whether some characteristics of the ex-amined idioms are more prominent than others. Further-more, preliminary music examples indicate that the pro-posed methodology indeed captures some of the most promi-nent characteristics of the idioms it is being trained with,despite the fact that further adjustments are required for itsapplication in melodic harmonisation.5. ACKNOWLEDGEMENTSThis work is founded by the COINVENT project. Theproject COINVENT acknowledges the ﬁnancial support ofthe Future and Emerging Technologies (FET) programmewithin the Seventh Framework Programme for Researchof the European Commission, under FET-Open grant num-ber: 611553. The authors would like to thank Costas Tsougrasfor his assistance in preparing the presented musical exam-ples.6. REFERENCES[1]Moray Allan and Christopher K. I. Williams. Harmon-ising chorales by probabilistic inference. InAdvancesin Neural Information Processing Systems 17, pages25–32. MIT Press, 2004.[2]Emilios Cambouropoulos, Maximos Kaliakatsos-Papakostas, and Costas Tsougras. An idiom-independent representation of chords for compu-328 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015tational music analysis and generation. InProceedingof the joint 11th Sound and Music Computing Confer-ence (SMC) and 40th International Computer MusicConference (ICMC), ICMC–SMC 2014, 2014.[3]Tom Collins.Improved methods for pattern discoveryin music, with applications in automated stylistic com-position. PhD thesis, The Open University, 2011.[4]Patrick Donnelly and John Sheppard. Evolving four-part harmony using genetic algorithms. InProceed-ings of the 2011 International Conference on Applica-tions of Evolutionary Computation - Volume Part II,EvoApplications’11, pages 273–282, Berlin, Heidel-berg, 2011. Springer-Verlag.[5]Kemal Ebcioglu. An expert system for harmonizingfour-part chorales.Computer Music Journal, 12(3):43–51, 1988.[6]David Huron. V oice denumerability in polyphonic mu-sic of homogeneous timbres.Music Perception, pages361–382, 1989.[7]Michael I. Jordan, Zoubin Ghahramani, andLawrence K. Saul. Hidden markov decision trees.In Michael Mozer, Michael I. Jordan, and ThomasPetsche, editors,NIPS, pages 501–507. MIT Press,1996.[8]Dan Jurafsky and James H. Martin.Speech and lan-guage processing. Prentice Hall, New Jersey, USA,2000.[9]M. Kaliakatsos-Papakostas, A. Katsiavalos,C. Tsougras, and E. Cambouropoulos. Harmonyin the polyphonic songs of epirus: Representation,statistical analysis and generation. In4th InternationalWorkshop on Folk Music Analysis (FMA) 2014, June2011.[10]Maximos Kaliakatsos-Papakostas and Emilios Cam-bouropoulos. Probabilistic harmonisation with ﬁxedintermediate chord constraints. InProceeding of thejoint 11th Sound and Music Computing Conference(SMC) and 40th International Computer Music Con-ference (ICMC), ICMC–SMC 2014, 2014.[11]Kostas Liolis.To Epir´otiko Polyphonik´o Trago´udi(Epirus Polyphonic Song). Ioannina, 2006.[12]Dimos Makris, Maximos Kaliakatsos-Papakostas, andEmilios Cambouropoulos. A probabilistic approach todetermining bass voice leading in melodic harmonisa-tion. In Tom Collins, David Meredith, and Anja V olk,editors,Mathematics and Computation in Music, vol-ume 9110 ofLecture Notes in Computer Science, pages128–134. Springer International Publishing, 2015.[13]Leonard C. Manzara, Ian H. Witten, and Mark James.On the entropy of music: An experiment with bachchorale melodies.Leonardo Music Journal, 2(1):81–88, January 1992.[14]Francois Pachet and Pierre Roy. Formulating constraintsatisfaction problems on part-whole relations: The caseof automatic musical harmonization. InProceedingsof the 13th European Conference on Artiﬁcial Intelli-gence (ECAI 98), pages 1–11. Wiley-Blackwell, 1998.[15]Francois Pachet and Pierre Roy. Musical harmoniza-tion with constraints: A survey.Constraints, 6(1):7–19,January 2001.[16]Jean-Franc ¸ois Paiement, Douglas Eck, and Samy Ben-gio. Probabilistic melodic harmonization. InProceed-ings of the 19th International Conference on Advancesin Artiﬁcial Intelligence: Canadian Society for Compu-tational Studies of Intelligence, AI’06, pages 218–229,Berlin, Heidelberg, 2006. Springer-Verlag.[17]Risto Pekka Pennanen. The development of chordalharmony in greek rebetika and laika music, 1930s to1960s.British Journal of Ethnomusicology, 6(1):65–116, 1997.[18]Somnuk Phon-amnuaisuk and Geraint A. Wiggins. Thefour-part harmonisation problem: A comparison be-tween genetic algorithms and a rule–based system. InIn proceedings of the AISB99 symposium on musicalcretivity, pages 28–34. AISB, 1999.[19]Martin Rohrmeier and Thore Graepel. Comparingfeature-based models of harmony. InProceedings ofthe 9th International Symposium on Computer MusicModelling and Retrieval, pages 357–370, 2012.[20]M. Schorlemmer, A. Smaill, K.U. K¨uhnberger,O. Kutz, S. Colton, E. Cambouropoulos, and A. Pease.Coinvent: Towards a computational concept inventiontheory. In5th International Conference on Computa-tional Creativity (ICCC) 2014, June 2014.[21]C. E Shannon. A mathematical theory of communica-tion.ACM SIGMOBILE Mobile Computing and Com-munications Review, 5:3–55, January 2001.[22]Raymond P. Whorley, Geraint A. Wiggins, ChristopheRhodes, and Marcus T. Pearce. Multiple viewpoint sys-tems: Time complexity and the construction of do-mains for complex musical viewpoints in the harmo-nization problem.Journal of New Music Research,42(3):237–266, September 2013.[23]F. Wilcoxon. Individual comparisons by ranking meth-ods.Biometrics Bulletin, 1(6):80–83, 1945.[24]Liangrong Yi and Judy Goldsmith. Automatic gen-eration of four-part harmony. In Kathryn B. Laskey,Suzanne M. Mahoney, and Judy Goldsmith, editors,BMA, volume 268 ofCEUR Workshop Proceedings.CEUR-WS.org, 2007.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 329"
    },
    {
        "title": "Specter: Combining Music Information Retrieval with Sound Spatialization.",
        "author": [
            "Bill Z. Manaris",
            "Seth Stoudenmier"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415596",
        "url": "https://doi.org/10.5281/zenodo.1415596",
        "ee": "https://zenodo.org/records/1415596/files/ManarisS15.pdf",
        "abstract": "Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile en- vironment to experiment with sound spatialization for music composition and live performance. Through vari- ous interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information re- trieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real- time generation, manipulation, and storing of sound tra- jectory scores. Finally, through Glaser, a sound manipula- tion instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vec- tor Based Amplitude Panning.  Various interfaces are dis- cussed, including a Kinect-based sensor system, a Leap- Motion-based hand-tracking interface, and a smartphone- based OSC controller.  Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware.",
        "zenodo_id": 1415596,
        "dblp_key": "conf/ismir/ManarisS15",
        "keywords": [
            "Ambisonics",
            "JythonMusic",
            "Iannix",
            "Glaser",
            "Ambisonics",
            "Vector Based Amplitude Panning",
            "Migrant",
            "Specter",
            "Specter",
            "Specter"
        ],
        "content": "SPECTER: COMBINING MUSIC INFORMATION RETRIEVAL WITH SOUND SPATIALIZATION Bill Manaris Seth Stoudenmier Computer Science Dept. College of Charleston, USA manarisb@cofc.edu Computer Science Dept. College of Charleston, USA stoudenmiersh@g.cofc.edu ABSTRACT Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile en-vironment to experiment with sound spatialization for music composition and live performance. Through vari-ous interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information re-trieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real-time generation, manipulation, and storing of sound tra-jectory scores. Finally, through Glaser, a sound manipula-tion instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vec-tor Based Amplitude Panning.  Various interfaces are dis-cussed, including a Kinect-based sensor system, a Leap-Motion-based hand-tracking interface, and a smartphone-based OSC controller.  Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware. 1. INTRODUCTION Spec•ter (ˈspɛk t!r) n. 1. a visible incorporeal spirit, esp. one of a terrifying na-ture; ghost; phantom; apparition. 2. some object or source of terror or dread. Also, esp. Brit., spectre. [1595–1605; < Latin spectrum; see spectrum] (http://www.thefreedictionary.com)  Sound spatialization offers the ability to composers and performers to specify how sounds are positioned in the listener’s audio field.  Most consumer-quality audio sys-tems allow for stereo fields (i.e., the ability to pan sound from left to right channel), becoming a standard house-hold item in the 1970s.  Around the same time, quadro-phonic systems were introduced (i.e., making use of 4 channels), but did not meet with much commercial suc-cess, due to the industry’s indecision to create a standard. Within the 1980s and 1990s, surround sound was intro-duced into the consumer market with Cinema 5.1 (6 channels), and 7.1 (8 channels), e.g., DTS, Dolby Digital, etc.  Still, these systems have proprietary formats, they require specialized software and hardware, and are not as readily available (as stereo systems).  Additionally, vari-ous research techniques for sound spatialization were de-veloped during this time, including Ambisonics and Vec-tor Based Amplitude Panning (VBAP), which have not yet received commercial acceptance. We present Specter, an open-source, scalable, easy to customize and deploy sound spatialization system, devel-oped to facilitate music composition and live perfor-mance.   Specter was initially developed in the context of Time Jitters, a four-projector interactive installation (see Figure 1), designed by Los-Angeles-based visual artist Jody Zellen for the Halsey Institute of Contemporary Art in Charleston, SC, USA.1  Time Jitters includes two walls displaying looping video animation, and two walls with                                                              1 See a video of Time Jitters, http://goo.gl/TIfpPl \n Figure 1. Specter system deployment at Time Jitters exhibit. Photo shows two people moving through the installation. Two of the four speakers are visible (top-left and center).  Also visible is one of the two Kinect sensors used in the installation (top right), and one of the four projectors (each projecting to one of the four walls enclosing the installation). \n © Bill Manaris and Seth Stoudenmier. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Bill Manaris and Seth Stoudenmier. “Specter: Combining Music Information Retrieval with Sound Spatialization”, 16th International Society for Music Information Retrieval Conference, 2015. 288   interactive elements. The concept is to create an immer-sive experience for participants, which confronts them with a bombardment of visual and sound elements.  As participants enter the space, images and sounds are as-signed to them.  As participants move freely through the space, these images and sounds follow them. The result is an immersive, dynamic experience that unfolds in real-time as different people navigate the space.  Several indi-viduals contributed to this installation (including visual materials and overall concept design, interaction design, and sound design).  In this paper, we focus on the design and implementation of Specter.  Other aspects of this in-stallation are presented elsewhere (e.g., [1, 2]). Specter was designed to offer composers and perform-ers a simple, expandable, and versatile environment to experiment with sound spatialization for music composi-tion and live performance. It combines music information retrieval (MIR) with minimal hardware through the Open Sound Control (OSC) protocol to produce a low-cost, easily configurable and transportable system. Through various interfaces, users may position sounds at arbitrary locations and trajectories on a three-dimensional plane.   The system incorporates JythonMusic, an environment for symbolic music processing, music information re-trieval, and live audio manipulation.  It also utilizes Iannix, a graphical open-source sequencer for digital art for real-time generation, manipulation, and storing of sound trajectory scores. Finally, it uses Glaser, a sound manipulation instrument to render the various sounds in space by quickly manipulating their attributes.   The rest of the paper is organized as follows:  Section 2 describes related background in sound spatialization.  Section 3 defines the Specter system architecture; this includes a description of JythonMusic, the underlying music programming environment, used to implement Specter; Iannix, a graphical open-source sequencer for digital art, utilized to represent Specter trajectory scores; and Glaser, a sound rendering instrument.  Section 4 pre-sents a case study utilizing an MIR approach to generate a musical composition involving sound spatialization, which includes both static (pre-composed) and dynamic (interactive) sound trajectories, rendered with Specter.  Finally, section 5 provides concluding remarks. 2. BACKGROUND Although sound spatialization is a very promising field for developing new music and related composition tech-niques, it is highly underutilized (versus, say, timbre composition) because of the difficulty in exploring pos-sibilities and performing existing compositions.  While there is much development already in timbre technologies for both analog and digital timbre, spatial computer music is being held back because composers have limited access to performance techniques and spaces with installed multi-channel systems [3].   Additionally, the software tools for spatial composi-tion are few, compared to those for symbolic music, and for timbre compositions.  Moreover, there is not a stand-ard high-level format for representing spatial composi-tions or storing spatial recordings.  As a result, spatial compositions may loose integrity and content as they are transferred from one technology to another, in order to be performed, given that the available performance spaces for spatial music are few, quite expensive to set up and maintain, have different architectures, and support differ-ent formats [4].  Nevertheless, sound spatialization is a rich field with many decades of research and development.  Johnson et al. [5] provide a thorough overview of the early history of the field, starting with Schafer and Henry, who in 1951 performed the first pre-composed electroacoustic piece of music with dynamic spatialization at performance time.  This was accomplished through a special interface con-trolling gain of individual speakers on a tetrahedral speaker array.  Other important examples include con-struction of expensive performance spaces, during the 1970s and 1980s, such as the GRM Acousmonium, IMEB’s Gmebaphone, and the Univ. of Birmingham’s BEAST.  Additionally, various spatialization algorithms have been developed for creating dynamic trajectories, and for spatial rendering for diffusion performance.  They can be classified into two general categories:  (a) room-based diffusion, which involves program-ming autonomous spatial trajectories and complex spatial distribution patterns, through large numbers of speakers; one popular approach is Higher Order Ambisonics, e.g., see [6], and (b) phantom-source positioning, which places less emphasis on amount of speakers, and focuses more on improving accuracy of sound object placement in the sound field, providing more control of sound trajectory rendering through better algorithms and data structures, and provision of interactive tech-niques for improved dynamic control of sound tra-jectories at performance time; one popular ap-proach is Vector Based Amplitude Panning (VBAP), e.g. see [7].  Our system’s high-level architecture supports both ap-proaches. Lopez-Lezcano [8] discusses development of open, general-purpose sound diffusion systems.  He identifies several important characteristics of such systems, in order for them to be more usable than the current state-of-the-art.  These characteristics include simplicity, transparen-cy, versatility, using commodity hardware, using free software, and having a small footprint.  Our system is de-signed with these characteristics in mind, as described in the next section. A significant research trend in sound spatialization in-volves gestural control (e.g., [9-12]) at composition time Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 289   (such as through algorithms exploring dynamical sys-tems, e.g., swarms and boids), but also at performance time (through specialized interfaces, such as data gloves, Kinect, and LeapMotion sensors, among others).  Specter, through the underlying JythonMusic environment pro-vides similar capabilities (a) through development of ar-bitrary algorithms to drive (or guide aspects of) music composition, and (b) through a variety of devices that can communicate with it via MIDI or OSC protocols. 3. SPECTER ARCHITECTURE Specter incorporates three major components, JythonMusic, Iannix, and Glaser – all communicating via OSC to pass data and synchronize / coordinate their ac-tions.  The following sections describe each of the sub-systems, and how they are combined to provide an envi-ronment to experiment with sound spatialization for mu-sic composition and live performance.  Through various interfaces and sensors, users may po-sition sounds at arbitrary locations and trajectories on a two- or three-dimensional plane.  3.1 Music Information Retrieval MIR functionality is available to Specter through JythonMusic, an environment for music analysis, com-position and performance (see http://jythonmusic.org).  JythonMusic provides libraries for music making, im-age manipulation, building graphical user interfaces (GUIs), and for connecting computers to external MIDI and OSC devices, such as digital pianos, smartphones, and tablets. JythonMusic is an outcome of a decade-long project exploring various aspects of music information retrieval, including investigation of fractals in music, and their rela-tionship to human aesthetics (e.g., [13]).  This on-going project explores Zipf’s Law (and related power laws) in music data mining, in music recommendation, and in mu-sic analysis, composition, and performance [14-16]. JythonMusic incorporates the following libraries.  Primitives from each of these libraries are used in con-junction with Specter (as explained below) to create sound spatialization trajectories and related processes: • Music library - provides primitives for creating music notes, phrases, parts, and scores, and for playing them live, as well as reading and writing them as MIDI or XML files. • Audio library - provides primitives for loading and looping audio files, and for recording and looping live audio. • Zipf library - provides primitives for extracting meas-urements from musical data (e.g., [13]). • MIDI library - provides primitives for loading and looping MIDI files, and for connecting to external MIDI devices (e.g., pianos, guitars, synthesizers, etc.). • OSC library - provides primitives for connecting to other devices via Open Sound Control (e.g., smartphones, tablets, computers, synthesizers, etc.). Additionally, JythonMusic provides libraries for graph-ical interactivity, image manipulation and sonification, and event scheduling.  Finally, it encapsulates various cross-platform libraries for MIR and music / sound ma-nipulation, such as jMusic and jSyn.   In summary, JythonMusic provides the glue code through which Specter is implemented, and, through its libraries facilitates arbitrary possibilities for data map-ping, sonification, and interaction.   3.2 Sound Spatialization Trajectories Sound spatialization trajectories in Specter are modeled via Iannix, an open-source, a 3D graphical sequencer for real-time generation, manipulation, and storing of musi-cal and other scores (see http://www.iannix.org).  As shown in Figure 2, Iannix scores consist of: • Curves, which define spatial trajectories.  These tra-jectories support cursors and triggers.  Curves can be circular, straight lines, Bézier curves, free-form curves (drawn via mouse), or prescribed through math equa-tions.   • Cursors, which follow the trajectories defined by curves, moving at a constant speed, when the score is played.  Cursors report (via MIDI or OSC messages) their current coordinates in XYZ space (as defined by the Iannix score); also they can be set externally (via MIDI or OSC messages). • Triggers, which report (again, via MIDI or OSC) when a cursor crosses them. Additionally, through JythonMusic arbitrary math func-tions and algorithms may be implemented (such as boids, swarms, and other dynamic particle systems), which may     \n  Figure 2. Example of an Iannix score consisting of three curves, each with a cursor and a trigger, drawn in 3D space.  When such a score is played, cursors move automatically on prescribed trajectory, continu-ously reporting their XYZ coordinates, and when they cross triggers (via OSC and MIDI protocols). 290 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   provide Specter with 3D trajectory information.  This in-formation may be stored as an Iannix score, or used in real-time to render sound spatialization trajectories.  3.3 Audio Rendering Audio rendering in Specter is done through the Glaser subsystem.  Glaser is an audio rendering instrument im-plemented using JythonMusic.  It was developed for the TimeJitters exhibit (see section 1), and has been adapted to implement the functionality needed by Specter. Glaser, in its basic form, allows exploring various sounds for sound design by manipulating their attributes (frequency, volume, and spatialization).  It consists of three GUI displays with several sliders each, one per au-dio file. Through these, it allows a sound designer, com-poser, or performer to interactively control volume, fre-quency, and spatialization of an arbitrary number of audio files simultaneously. Within Specter, the Glaser architecture has been ex-tended to support both the Ambisonics and Vector Based Amplitude Panning approaches (e.g., see [6, 7]).  This allows taking into account the spatial configuration (number of individual channels available) and geometry of the space, through existing algorithms, such as the ones used in [17]. 3.4 Music Representation Specter, through JythonMusic, utilizes a common-practice-based notation to represent audio to be rendered.  This notation consists of: • Notes, which specify pitch, duration, dynamic, and panning. For stereo, panning ranges from 0.0 to 1.0 (where 0.0 is left, 0.5 is center, and 1.0 is right of ste-reo field).  Panning values greater than 1.0 are treated as identifiers for Iannix trajectories, which are used when the corresponding note is rendered).  Pitches are used for sound frequency shifting (sounds are as-signed a default / reference pitch, i.e., A4), durations are in seconds, and volume ranges from 0 to 127, fol-lowing the MIDI standard. • Phrases, which serve as containers for sequences of notes. Additional, higher-level containers include parts and scores (again, see http://jythonmusic.org).  3.5 Expandable Architecture The design concept behind Specter allows using easily accessible, low-cost equipment to render multi-channel audio, in an expandable architecture.  This is facilitated by computer programming to account for the modular architecture. Through the use of audio aggregates and low-cost, 2-channel USB audio interfaces, such as the Behringer UCB222 (approx. $30, at the time of this writing), it is possible to assemble a wide variety of sound spatializa-tion architectures (e.g., obviously stereo, quadrophonic, 9-channel, 16-channel, and so on).  Theoretically, any number of speakers / channels is possible for arbitrary sound spatialization installations.  3.6 Interfaces for Sound Spatialization  Given the underlying functionality provided to Specter via JythonMusic, a wide variety of interfaces may be used (or developed) to generate and/or capture sound tra-jectories.  These trajectories may be stored (in a Iannix score) for later use in audio rendering, or be used imme-diately for real-time sound placement (this is exemplified in the case study presented in section 4).  Possible inter-faces include: • Kinect motion sensor: Utilizing Kinect sensors with JythonMusic has already been implemented in the context of Kuatro, a motion-based framework for de-veloping interactive music installations [1]. • LeapMotion:  We have also developed a LeapMotion interface to capture fine movement of both hands. This inexpensive sensor, and its versatile API, allow for a wide variety of interfaces and associated gestures to be developed for natural, intuitive control of sound spatialization.  • Smartphone: Using smartphone sensors, e.g., gyro and accelerometer readings, one may develop various programs to control aspects of musical performance.  One such example is presented in the next section. Various other possibilities exist, utilizing any type of sen-sor that supports MIDI and OSC protocols. 4. MIGRANT - A CASE STUDY Migrant is a cyclic piece for piano and computer, origi-nally composed for Undomesticated, a public-art installa-tion by Vassiliki Falkehag at Moore Farms Botanical Gardens, in the context of ArtFields 2015, held in Lake City, SC, USA (http://www.artfieldssc.org).  It is used here as an example of combining music in-formation retrieval techniques and sound spatialization for music composition and live performance. Migrant is part of the ISMIR 2015 music program to be performed on Wednesday, October 28, 2015 at the Sa-la Unicaja de Conciertos María Cristina in Málaga, Spain. 4.1 Composition Techniques In terms of composition, Migrant integrates data sonifica-tion, interactivity, and sound spatialization.   The data used in the piece comes from migrant worker statistics, including migration patterns, age, wages, fami-ly dependents, and other elements of the migrant life ex-perience. This data was collected from 56,976 in-person interviews with hired crop farm workers.  The interviews Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 291   were conducted in 545 US counties and 43 states during fiscal years 1989-2012.2  Each note in the piece represents a single person.   Melody, harmony and dynamic are all driven by the data.  Data from 120 people were randomly selected.  Notes were carefully spatialized – set to \"fly around\" to reflect the nomadic lifestyle.  Different timescales were combined.  A few notes were manually adjusted by the composer to reflect his own aesthetic and migrant experi-ence.  Figures 3a and b show photos used by the compos-er to provide aesthetic inspiration for composing the soni-fication scheme.   The composition makes use of the golden ratio to af-fect the piece’s harmonic density (e.g., see Figure 4).   The sonification code was written in JythonMusic using ideas presented in [20].  A preview of the piece (one cycle, mixed for two speakers) is available here – http://goo.gl/iYOVmY . The composition employs interactivity to control tem-po and spatialization of notes, via a smartphone-based controller manipulated by one of the performers.  This controller sends gyro readings via OSC to a JythonMusic program, similar to this – http://goo.gl/dsTWFM . 4.2 Performance Needs In terms of performance, the piece requires one piano, one computer, a video projector, two (or more) speakers (as described above), and a smartphone. The original composition envisioned eight pianos ar-ranged in 45-degree increments, with the audience seated in the middle.  For ISMIR 2015, in order to demonstrate Specter, a single piano and a computer with sound spati-alization are used. Ideally, four speakers in a square configuration (as seen in Figure 5) allow the audience to fully experience the “flying around” of notes.  However, two speakers in a stereo configuration work also, albeit losing one of the sound spatialization dimensions; in this case, the outcome is similar to the preview of the piece above. 4.3 Performance Instructions Migrant is a cyclic piece for piano and computer, using a smartphone-based OSC controller, during the perfor-mance, to send these notes “flying around” in the sound field.  Each cycle of the piece lasts 4 minutes and 52 seconds.  It is meant to be played in a continuous loop - minimally two times.  For the ISMIR 2015 performance, the piece will be cycled exactly twice, for a total duration of 9 minutes and 44 seconds.  During the first cycle, the pianist plays the notes in the score verbatim.  The computer layers identical notes in a computer-enhanced timbre and diffuses them in a circular                                                              2 See National Agricultural Workers Survey, Public Access Data, Octo-ber 1, 1988 to September 30, 2012 – http://goo.gl/xWsQe8  pattern (or left-to-right pattern, for 2 speakers), thus gen-erating a “flying-around” of notes. The computer also displays images (using precise, scripted timings), via the video projector (again, see the piece preview provided here –  http://goo.gl/iYOVmY).   During the second cycle, the computer plays the com-plete first cycle (i.e., both the notes originally played by the pianist, as well as the enhanced timbres spatialized in the sound field). The pianist is instructed to improvise additional notes, guided by the score notes.  The only constraint is that an A natural minor scale is used.  No constrains are given in terms of note start times, durations, or harmony.  The pi-  \n  Figure 3a. One of several photos used by the composer to provide aesthetic inspiration for composing the sonfication scheme.  Photo Credit: Dorothea Lange (1936), “Destitute pea pickers in California. Mother of seven children. Age thirty-two. Nipomo, California” [18].    \n  \n    Figure 3b. Another of the photos used by the composer to provide aesthetic inspiration for composing the sonfication scheme.  Photo Credit: Dorothea Lange (1938), “Aban-doned farm with windmill and farm equipment. Dalhart, Texas. June 1938” [19].  292 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   anist is encouraged to create a musical narrative (to the best of their musical abilities – a challenge!), which aes-thetically complements the sonified “narratives” of the people / notes in the data.  In essence, this provides an opportunity for the pianist to interweave his or her own experience, aesthetic, and improvisatory skills into the piece.  During this, the computer performer utilizes the smartphone-based OSC controller to affect timing and spatialization of the computer-generated notes.  This way, he or she controls aspects of the musical expression of the combined performance, through the following gestures: • Ready Position:  Smartphone is held facing up, paral-lel with the floor. • Controlling Tempo: The phone is used in a percus-sive gesture (moving downward) to play the next note.  When the phone pitch (see Figure 6) crosses the neu-tral (parallel to the floor) position, the next note is played. • Controlling Volume:  Device shake corresponds with loudness of notes. The more intensely one shakes or vibrates the phone as notes are generated, the louder the notes are. • Controlling Spatialization:  The yaw of the phone corresponds to placement of notes on the periphery of the sound field. (System is calibrated before the per-formance so magnetic north corresponds with Spec-ter’s virtual north, as far as sound placement is con-cerned). The interactive aspects of Migrant allow both human performers to musically interact with each other, and to-gether, to interact with the musical “narrative” generated from the data.   5. CONCLUSION Sound spatialization / diffusion systems normally require expensive, specialized equipment, which is usually hard to transport.  We presented Specter, a simple, yet versa-tile environment to experiment with sound spatialization for music composition and live performance.  By com-bining readily available hardware and software, through a simple, customizable architecture, we offer an inexpen-sive alternative to existing sound spatialization systems.   Specter may be used by MIR practitioners, as well as music composers and artists to explore and experiment with sound spatialization / diffusion more easily. Addi-tionally, this project may facilitate development of inno-vative art installations, as well as new gaming experienc-es.  Through the underlying JythonMusic system, devel-opers may connect various MIR techniques to music composition and sound spatialization, open the door for new sonification applications, and develop innovative, immersive interactive applications (e.g., [21]).  ACKNOWLEDGEMENTS We would like to thank Blake Stevens, Yiorgos Vass-ilandonakis, David Johnson, and Chris Benson for con-tributing to the development of some of the ideas and concepts presented herein.  The first author would like to thank Vassiliki Falkehag for providing the inspiration for composing Migrant.  Also, Mark Sloan and the Halsey Institute, in Charleston, SC, USA provided funding and space for the first implementation of Specter.  Funding for this project has been provided in part by NSF (DUE-1323605). \n  Figure 4. Pianoroll excerpt of the piece demonstrating result of sonification and use of the golden ratio to affect harmonic density.  \n  Figure 5. Migrant performance 4-channel speaker ar-rangement.  Audience is seated in the middle.  However, a regular two-speaker (stereo) setup is possible (for con-venience).  \n  Figure 6. Smartphone Pitch, Roll, and Yaw directions.  Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 293   REFERENCES [1] D. Johnson, B. Manaris, Y. Vassilandonakis, and S. Stoudenmier, “Kuatro: A Motion-Based Framework for Interactive Music Installations,” Proc. of the 40th  International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 355-362. [2] J. Drucker, “Pause Effect,” http://goo.gl/kLwgbY , accessed July 16, 2015. [3] E. Lyon: “The Future of Spatial Computer Music,” Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 850-854. [4] M. Baalman: “Spatial Composition Techniques and Sound Spatialisation Technologies,” Organized Sound, 15(3), pp. 209-218. [5] B. Johnson, M. Norris, and A. Kapur: “Diffusing Diffusion: A History of the Technological Advances in Spatial Performance,” Proc. of the 40th  International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 126-132. [6] Blue Ripple Sound, “HOA Technical Notes – Introduction to Higher Order Ambisonics,” http://goo.gl/srrKC6 , accessed July 16, 2015. [7] V. Pulkki: “Generic panning tools for MAX/MSP,” Proc. of the 26th  International Computer Music Conference (ICMC 2000), Berlin, Germany, 2000. [8] F. Lopez-Lezcano: “Towards Open 3D Sound Diffusion Systems,” Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 869-876. [9] T. Davis and O. Karamanlis: “Gestural Control of Sonic Swarms: Composing with Grouped Sound Objects,” Proc. of the Sound and Music Computing Conference, Lefkada, Greece, 2007, pp. 192-195. [10] M. Marshall, J. Malloch, and M.M. Wanderley: “Gesture Control of Sound Spatialization for Live Musical Performance,” Gesture-Based Human-Computer Interaction and Simulation, Lecture Notes in Computer Science, vol. 5085, 2009, pp. 227-238. [11] E. Soria and R. Morales-Manzanares: “Multidimensional sound spatialization by means of chaotic dynamical systems,” Proc. of the 13th International Conference on New Interfaces for Musical Expression (NIME 2013), Daejeon, Korea, 2013, pp. 79-83. [12] B. Johnson, M. Norris, and A. Kapur: “tactile.motion: An iPad Based Performance Interface For Increased Expressivity in Diffusion Performance,” Proc. of the 40th International Computer Music Conference (ICMC 2014), Athens, Greece, 2014, pp. 798-801. [13] B. Manaris, P. Roos, D. Krehbiel, T. Zalonis, and J.R. Armstrong, “Zipf’s law, power laws and music aesthetics,” in T. Li, M. Ogihara, G. Tzanetakis (eds.), Music Data Mining, pp. 169-216, CRC Press - Taylor & Francis, 2011. [14] B. Manaris, D. Krehbiel, P. Roos, and T. Zalonis, “Armonique: Experiments in content-based similarity retrieval using power-law melodic and timbre metrics,” Proc. of the 9th International Conference on Music Information Retrieval (ISMIR 2008), Philadelphia, PA, pp. 343-348, 2008. [15] B. Manaris, P. Roos, P. Machado, D. Krehbiel, L. Pellicoro, and J. Romero, “A Corpus-Based Hybrid Approach to Music Analysis and Composition,” Proc. of 22nd Conference on Artificial Intelligence (AAAI-07), Vancouver, BC, pp. 839-845, Jul. 2007. [16] B. Manaris, D. Hughes, and Y. Vassilandonakis, “Monterey Mirror: Combining Markov models, genetic algorithms, and power laws,” Proc. of 1st Workshop in Evolutionary Music, 2011 IEEE Congress on Evolutionary Computation (CEC 2011), New Orleans, LA, pp. 33-40, Jun. 2011. [17] R. Graham, “SEPTAR: Audio Breakout Design for Multichannel Guitar,” Proc. of the 15th International Conference on New Interfaces for Musical Expression (NIME 2015), Baton Rouge, Louisiana, 2015. [18] D. Lange, “Destitute pea pickers in California. Mother of seven children. Age thirty-two. Nipomo, California,” Mar. 1936. https://en.wikipedia.org/wiki/Florence_Owens_Thompson  [19] D. Lange, “Abandoned farm with windmill and farm equipment. Dalhart, Texas,” Jun. 1938. http://www.pbs.org/kenburns/dustbowl/photos [20] B. Manaris and A. Brown, Making Music with Computers: Creative Programming in Python, Chapman & Hall/CRC Textbooks in Computing, 2014. [21] B. Manaris, D. Johnson, and M. Rourk, “Diving into Infinity: A Motion-Based, Immersive Interface for M.C. Escher’s Works,” Proc. of the 21st  International Symposium on Electronic Art (ISEA 2015), Vancouver, Canada, Aug. 2015 (to appear). 294 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "New Sonorities for Early Jazz Recordings Using Sound Source Separation and Automatic Mixing Tools.",
        "author": [
            "Daniel Matz",
            "Estefanía Cano",
            "Jakob Abeßer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416568",
        "url": "https://doi.org/10.5281/zenodo.1416568",
        "ee": "https://zenodo.org/records/1416568/files/MatzCA15.pdf",
        "abstract": "In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a pre- processing step of the mixing process. In addition to an ini- tial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive mul- ti-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different pro- cessing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound bal- ance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework.",
        "zenodo_id": 1416568,
        "dblp_key": "conf/ismir/MatzCA15",
        "keywords": [
            "framework",
            "automatic mixing",
            "early jazz recordings",
            "sound source separation",
            "pre-processing step",
            "mixing process",
            "six processing blocks",
            "cross-adaptive multi-track scaling",
            "cross-adaptive equalizer",
            "cross-adaptive dynamic spectral panning"
        ],
        "content": "NEW SONORITIES FOR EARLY JAZZ RECORDINGS USING SOUNDSOURCE SEPARATION AND AUTOMATIC MIXING TOOLSDaniel MatzUniversity of Applied SciencesDüsseldorf, Germanydaniel.matz@hotmail.deEstefanía CanoFraunhofer IDMTIlmenau, Germanycano@idmt.fhg.deJakob AbeßerFraunhofer IDMTIlmenau, Germanyabr@idmt.fhg.deABSTRACTIn this paper, a framework for automatic mixing of earlyjazz recordings is presented. In particular, we proposethe use of sound source separation techniques as a pre-processing step of the mixing process. In addition to an ini-tial solo and accompaniment separation step, the proposedmixing framework is composed of six processing blocks:harmonic-percussive separation (HPS), cross-adaptive mul-ti-track scaling (CAMTS), cross-adaptive equalizer(CAEQ), cross-adaptive dynamic spectral panning(CADSP), automatic excitation (AE), and time-frequencyselective panning (TFSP). The effects of the different pro-cessing steps in the ﬁnal quality of the mix are evaluatedthrough a listening test procedure. The results show thatthe desired quality improvements in terms of sound bal-ance, transparency, stereo impression, timbre, and overallimpression can be achieved with the proposed framework.1. INTRODUCTIONWhen early jazz recordings are analyzed from a modernaudio engineering perspective, clear stylistic differencescan be identiﬁed with respect to modern recording tech-niques. These characteristics mainly evidence the techno-logical and stylistic differences between the two eras. Forexample, solo instruments such as the saxophone or thetrumpet often completely dominate the audio mix in earlyjazz recordings. At the same time, the rhythm section, i.e.,double bass, piano, drums, and percussion, often falls ina secondary place, recorded or mixed with much lower in-tensity and often perceived as unclear and undifferentiated.Additionally, from today’s perspective, early jazz record-ings often present an unusual stereo image. Instrumentgroups are sometimes assigned to extreme stereo positionswhich can cause the solo instrument to be panned to theleft and the accompaniment band panned to the right. As aconsequence, the energy distribution over the stereo widthis unbalanced and is often perceived today as irritating anddisturbing, especially when listened through headphones.c\u0000Daniel Matz, Estefanía Cano, Jakob Abeßer.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Daniel Matz, Estefanía Cano, JakobAbeßer. “New sonorities for early jazz recordings using sound sourceseparation and automatic mixing tools”, 16th International Society forMusic Information Retrieval Conference, 2015.Several initiatives have arisen that attempt to give suchearly recordings a more modern sonority. Remastering andAutomatic Mixing (AM) techniques offer various meth-ods for a sonic redesign of such recordings. However,given that the original individual stems of the instrumentsin the recordings are usually not available, these techniquescan only achieve minor modiﬁcations to the sound charac-teristics of mono and stereo mixtures. In-depth remixingusually requires the original multi-track recordings to beavailable. For this purpose, sound source separation meth-ods developed in the Music Information Retrieval (MIR)community can be useful tools to retrieve individual in-struments from a given mix.2. GOALSThe main goal of this study is to identify suitable signalprocessing methods to modify the above-mentioned char-acteristics in a selection of early jazz recordings. Thesemethods are combined in a fully automatic mixing frame-work. In particular, we focus on modifying the audio mixin terms of transparency, stereo impression, frequency re-sponse, and acoustic balance in order to improve the over-all perception of sound and the quality of the mix with re-spect to the original recording.Our main approach for remixing is to modify the char-acteristic of the backing track to make it more present inthe mix. We also aim at improving the acoustical and spa-tial balance of th audio mix. The solo signal is balancedwith respect to its loudness and spectral energy to mini-mize spectral masking as well as to improve its position inthe stereo image.3. RELATED WORKIn the ﬁeld of automatic mixing, several approaches havebeen presented in the literature. In [1], a method is pro-posed to automatically adjust gain and equalizer param-eters for multi-track recordings using a least-squares op-timization. In [12] the idea of modifying the magnitudespectrogram of a signal towards a target spectrogram calledtarget mixing, is presented. Other approaches for auto-matic mixing of multi-track recordings have incorporatedmachine learning algorithms to perform the mixing pro-cess [16, 17].749In [14] and [19], several cross-adaptive signal process-ing methods for automatic mixing such as source enhancer,panner, fader, equalizer, and polarity and time offset cor-rection are proposed. These modules can be combinedinto a full mixing application. In [4], the authors proposea knowledge-engineered autonomous mixing system andpropose to include expert knowledge within an automaticmixing system. The included audio effects are automat-ically controlled based on extracted low-level and high-level features such as musical genre, instrumentation, andthe type of sound sources. The authors evaluated the sys-tem using short four bar audio signals with vocals, bass,guitar, keyboard, and other instruments.Harmonic-percussive source separation was used as pre-processing step for manual remixing in [6], in particular toadjust the sound source levels of the signals. To the au-thors’ best knowledge, a framework for automatic remix-ing that suits the requirements discussed in section 2 hasnot been proposed so far.4. PROPOSED METHODFor our mixing framework, we propose the use of soundsource separation techniques as a pre-processing step ofthe mixing process. For this purpose, we ﬁrst isolate thesolo instrument from the audio mix by applying an algo-rithm for pitch-informed solo and accompaniment separa-tion [2]. The two separated signals, i.e., thesoloand theresidual/backingsignal, are the starting point for the au-tomatic remixing process. Additionally, based on the re-quirements discussed in section 2, our proposed frameworkcomprises six subcomponents:1.Harmonic-percussive separation (HPS)2.Cross-adaptive multi-track scaling (CAMTS)3.Cross-adaptive equalization (CAEQ)4.Cross-adaptive dynamic spectral panning (CADSP)5.Automatic excitation (AE)6.Time-frequency selective panning (TFSP)Figure 1 presents a block diagram of the proposed frame-work. There are three main signal pathways A, B, and C. Ifthe CADSP is activated, pathway A is chosen. If CADSPis not activated, pathway B and C are chosen dependingon whether the harmonic-percussive separation (HPS) isused. All signal paths output a stereo mix. In the followingsections, the individual subcomponents are ﬁrst described,followed by a description of the three proposed signal path-ways.4.1 Solo and Backing track SeparationThe algorithm as proposed in [2] automatically extractspitch sequences of the solo instrument and uses them asprior information in the separation scheme. In order tofalseCADSP ? (true or false)true\nDownmixHPSCAEQ\nMixdownCADSPCAMTS\nAEStereo SplitTFSPMixdownDownmix\nNormalize\nStereo Remix424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzA\nStereo? (true or false)\ntruefalseCBHPS ? (true or false)true\nHPSCAEQMixdownAE\nCAMTS\nfalse\nCAEQ\nAE\nCAMTS\n424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzBackingSolo424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzBackingSolo424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzBackingSolo\nTFSPMixdownNormalize\nStereo Split\nNormalize\nMixdown424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzSolo424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel Matz424 Konzeption und UmsetzungAM-DAFX IMPLEMENTATIONS 543xm(n)xe(n)he0(n)h0,m(n)hK–1,m(n)heK–1(n)RMS(he0(n))RMS(h0,m(n))RMS(h0,m(n)) > RMS(he0(n))Gateh0,m(n)xe(n)hg0,m(n)Avr(xl0,m(n))STable ofloudnessweightingcoefficientsFiltersThe sameforall K filtersSPLmeasurementMulti-bandadaptivegateFilter bank (with K bands)Filter bank (with K bands)\nFixed SPL1001Probability mass functionOutput most probable loudness per band: fvk,m(n)hg0,m(n)*w(SP(n))xl0,m(n)fv0,m(n)SP(n)tr0,m(n)\nFigure 13.11Block diagram of an automatic equaliser.probable loudness per band of a given input and is denoted byfvk,m(n).T h eb l o c kd i a g r a mo fthis multi-band loudness feature extraction method is shown in Figure 13.11.Cross-adaptive feature processingCross-adaptive feature processing consists of mapping the perceptual loudness of each spectral bandto its amplitude level so that, by manipulating its gain per band level, we can achieve a desiredtarget loudness level. We aim to achieve an average loudness valuel(n),t h e r e f o r ew em u s td e c r e a s ethe loudness of the equalisation bands for signals above this average and increase the band gain forsignals below this average. This results in a system in which we have a multipliercvak,m(n)perband, such that we scale the input bandsxk,m(n)in order to achieve a desired average loudnessl(n).The function of the system can be approximated byHlk,m(n)=l(n)/(cvk,m(n)xk,m(n)),w h e r et h econtrol vectorcvk,m(n)is given bycvk,m(n)=l(n)Hlk,m(n)xk,m(n),(13.18)wherecvk,m(n)is the control gain variable per band used to achieve the target average loudnessl(n)andHlk,m(n)is the function of the desired system. The feature extraction block has a functionAbbildung 4.4:Blockdiagramm der Merkmalsgewinnung des implementiertencross-adaptiven EqualizersCross-adaptive MerkmalsverarbeitungInnerhalb des CAFPB geht es nun um die Zuordnung der häuﬁgsten Lautheitfvk,m(n)jedes Frequenzbandes zu einem Verstärkungsfaktorcvk,m(n)des korrespondierendenEQ-Bandes für alleTracks. Hierdurch kann die Verstärkung eines speziﬁschen Bandesbeeinﬂusst werden, sodass dieses letztendlich die beabsichtigte Lautheit zu besitzt.Dies erfolgt in Anlehnung an die cross-adaptive Verarbeitung des CAMTS, wobeizunächst ebenfalls eine mittlere Ziel-Lautheitl(n)mit Hilfe der Mittelwertbildungüber allefvk,m(n)für alleKFrequenzbänder undMSignale bestimmt wird:l(n)=M\u00001\u0000m=0\u0000K\u00001\u0000k=0fvk,m(n)/K\u0000/M(4.11)Bachelorarbeit Daniel MatzBackingSoloBacking\nStereo RemixStereo RemixFigure 1: Signal ﬂow-chart of the developed automaticremixing frameworkobtain more accurate spectral estimates of the solo instru-ment, the algorithm creates tone objects from the pitch se-quences, and performs separation on a tone-by-tone basis.Tone segmentation allows more accurate modeling of thetemporal evolution of the spectral parameters of the soloinstrument. The algorithm performs an iterative searchin the magnitude spectrogram in order to ﬁnd the exactfrequency locations of the different partials of the tone.A smoothness constraint is enforced on the temporal en-velopes of each partial. In order to reduce interferencefrom other sources caused by overlapping of spectral com-ponents in the time-frequency representation, a commonamplitude modulation is required for the temporal enve-lopes of the partials. Additionally, a post-processing stagebased on median ﬁltering is used to reduce the interferencefrom percussive instruments in the solo estimation.4.2 Harmonic-percussive Separation (HPS)We use the algorithm for harmonic-percussive separationproposed in [6]. The algorithm is based on median ﬁlter-ing of the magnitude spectrogram to split the original au-dio signal into its horizontal (harmonic sources) and verti-cal elements (percussive sources). In an automatic mixingcontext, these components can be understood as separatesubgroups which can be processed individually and ﬁnallyremixed.750 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20154.3 Cross-adaptive Multi-track Scaling (CAMTS)The method proposed in [19] which is commonly referredto asautomatic fader control, is used for automatic scalingof the sound sources. The algorithm is used to automati-cally modify the ampliﬁcation of separate sound sources.A psychoacoustic model based on the EBU R-128 stan-dard [9] is used to compute the loudness of each track us-ing a histogram-based approach. All tracks are individu-ally ampliﬁed to be perceived as equally loud.4.4 Cross-adaptive Equalizer (CAEQ)We use the cross-adaptive equalizing algorithm proposedin [19] to obtain a spectrally balanced mixture. The mainapproach is to modify the spectral envelopes of the au-dio signals and to minimize the spectral masking betweenthe solo signal and the backing track. The algorithm isa multi-band extensions of the CAMTS algorithm as dis-cussed in section 4.3. The spectral characteristics of theseparated signals are modiﬁed by enhancing or attenuat-ing pre-deﬁned frequency bands depending on the signal’sperceived loudness with respect to the overall loudness. Incontrast to the CAMTS algorithm, the loudness model pro-posed in [19] is used since it outperformed the loudnessmodel based on EBU R-128 during informal testing. Inparticular, the mix results based on EBU-R 128 showed toostrong of an emphasis on treble frequencies while lackingenergy in the lower frequency range. We use a 10-band oc-tave equalizer with second-order biquad IIR ﬁlters follow-ing [19] and frequency bands uniformly distributed overthe audible frequency range. Standard frequency valuesbased on [8] are used to adjust the center frequencies of thepeak ﬁlter as well as the cutoff frequencies of the shelvingﬁlters.4.5 Cross-adaptive Dynamic Spectral Panning(CADSP)Dynamic spectral panning is a technique that allows thecreation of a stereophonic impression in a given mono-phonic multi-track recording. We use the algorithm pro-posed in [15] to create a spatialization effect given multi-track signals. The method dynamically assignstime-frequency bins of the original tracks towards azimuthpositions. The assignment reduces masking due to sharedazimuth positions between multiple sound sources. Thisimproves the overall transparency of an audio mix. In thecases where the original audio mix is a stereo track, it isﬁrst down-mixed to mono and then up-mixed to a newstereo image using the CADSP algorithm.4.6 Automatic Exciter (AE)The exciting algorithm improves the assertiveness of thebacking track. The digital signal processing methods areimplemented following theAPHEX Aural Exciterdescri-bed in [18]. The audibility of the mixed signal is enhancedby adding harmonic distortions in the upper frequency range.These distortions create additional harmonic signal com-ponents which improve the presence, clarity, and bright-ness of the audio signal.The automation of the exciting step is implemented fol-lowing atarget mixingapproach. Based on [5], the mix-ing parameters are iteratively adjusted to atarget energyratio. The target energy ratio is computed from the rela-tionship between the energy of the high-pass ﬁltered signaland the energy of the target signal. In theside chain, anasymmetric soft clipping characteristic,harmonic genera-tor block, with adaptive threshold was used. This allows alevel-independent distortion as well as the preservation ofthe signal dynamics [5].4.7 Time-frequency selective Panning (TFSP)Time-frequency selective panning improves the stereo im-age as well as the overall spatial impression of an audiomix. In our framework, the method for time-frequency se-lective panning presented in [3] was used. The azimuth po-sitions of the sound sources are modiﬁed using a non-linearwarping function. The stereo image is widened while theinitial arrangement of the sound sources, as well as thesound quality of the original source is maintained. Withinthe proposed automatic remixing framework, the TFSP al-gorithm can be interpreted as an extension of the CADPSalgorithm. The panning algorithm is only applied to theresidual signal (see section 4.8.1). We set the aperture pa-rameter⇢to a ﬁxed value based on initial informal testing.4.8 Processing Pathways4.8.1 Signal path A (Main Path)The main processing path includes all system components.Stereo ﬁles must be down-mixed to mono ﬁrst due to con-straints of thecross-adaptive dynamic spectral panning(CADSP) algorithm as detailed in section 4.5. All soundsources, which are initially distributed in the stereo pano-rama, are ﬁrst centered to the mono channel and later re-distributed over the stereo panorama again based on theharmonic-percussive sound separation [6]. This up-mixingstep that can involve a modiﬁcation of the stereo arrange-ment is only possible in this signal path.Thecross-adaptive equalization(CAEQ) andmulti-trackscaling(CAMTS) are the ﬁrst processing steps in all threepathways. After applying thedynamic spectral panning(CADSP) to the percussive and harmonic signal compo-nents, all stereophonic signals are summed up to a backingtrack with a more homogeneous distribution of the soundsources. The backing track can now be processed with theautomatic excitation(AE) and thetime-frequency selectivepanning(TFSP) algorithms. The solo signal is split intostereo channels in theStereo Splitstage and scaled suchthat the overall gain remains constant. In the ﬁnalmix-downstep, the backing track is mixed with the solo trackby adjusting the individual ampliﬁcation factors as givenby the CAMTS stage. If the cross-adaptive equalization(CAEQ) was performed, the spectral envelope of the back-ing track is perceivably modiﬁed due to the minimizationProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 751of the spectral masking. The stereo sum signal is ﬁnallynormalized.4.8.2 Signal path BSignal path B resembles signal path A, however, the equal-ization (CAEQ) and scaling (CAMTS) steps offer moreways to modify parameters due to the prior harmonic per-cussive separation stage.4.8.3 Signal path CIn the signal path C, no harmonic-percussive separation isperformed. The equalization (CAEQ) and scaling(CAMTS) are applied to both the backing and the solotrack. However, the automatic excitation is only appliedto the backing track since we particularly want to enhancethe presence, clarity, and brightness of the backing track.As shown in ﬁgure 1, the time-frequency selective panning(TFSP) can only be applied to the backing track if it is astereo signal. For monaural signals, the signal is split tothe stereo channels (Stereo Split) and scaled such that theoverall gain remains constant. Similar to signal path B, thesignals are ﬁnally mixed down and normalized.5. EVALUATION5.1 Experimental DesignTo evaluate the proposed framework, a listening test pro-cedure was conducted following the guidelines of theMultiStimulus Test with Hidden Reference and Anchor(MUSHRA) described in the ITU-R BS.1534-2 recommen-dation [11], and modifying them to ﬁt the characteristics ofthis study. The main difference of our test with respect tothe original MUSHRA is that a reference signal, which inour case would be an ideal mix of the original recording,is not available. Moreover, the notion of an ideal mix isill-posed in the automatic remixing context.The listening test was conducted in a quiet room andall signals were played using open headphones (AKG K701). A total of 19 participants conducted the listeningtest. The participants included audio signal processing ex-perts, professional audio engineers, music students (jazz,classical music), musicologists, as well as amateur musi-cians and regular music consumers. The average age ofthe participants was 30.7 years old. Further demographicinformation such as gender, hearing impairments, listen-ing test experience, and educational background were alsocollected. A summary of the demographic information ispresented in table 1.The listening test was divided into ﬁve evaluation tasks,each focusing on a different subjective quality parameter.The following parameters were selected based on the ITU-R BS.1248-1 recommendation [10], and were adopted toour requirements: (QP1) Sound Balance, (QP2) Transpa-rency, (QP3) Stereo/Spatial Impression, (QP4) Timbre, and(QP5) Overall Impression. In each evaluation task, atrain-ing phasewas ﬁrst conducted to allow the participants tofamiliarize themselves with the test material and to adjustplayback levels to a comfortable one.GenderM16F3Hearing impairment?Yes0No19Listening test experience?Yes9No10Expert in audio engineering?Yes11No8Educational background in music?Yes15No4Table 1: Demographics of the listening test participantsFollowing the training phase, anevaluation phasewasconducted for each task. Five audio tracks as described inTable 2 with ten mixtures each were rated by the partic-ipants. The ﬁve tracks used in this study are part of theJazzomat Database1. Among the presented mixtures, theoriginal signal, eight mixes created with different conﬁg-urations of the proposed framework, and an anchor signal(rhythm section reduced by 6 dB, the sum signal low-passﬁltered at 3.5 kHz) were used. Table 3 gives an overviewof all the remix conﬁgurations.TitleSoloist (Instrument)StyleYearBody and SoulChu Berry (ts)Swing1938Tenor MadnessSonny Rollins (ts)Hardbop1956Crazy RhythmJ.J. Johnson (tb)Bebop1957Bye Bye BlackbirdBen Webster (ts)Swing1959Adam’s AppleWayne Shorter (ts)Postbop1966Table 2: Dataset descriptionMixHPSCAEQCAMTSCADSPAETFSP1offonoffoffonoff2offoffonoffonoff3offononoffonoff4ononoffoffonoff5onoffonoffonoff6onoffoffononon7onononononon8onononoffonoff(mono)Table 3: Conﬁgurations of the eight remixes used in thelistening testThe automatic exciting (AE) component is active in allthe mixes. The panning (TFSP) algorithm is only acti-vated in conjunction with the cross-adaptive dynamic spec-tral panning (CADSP). This way, a further stereo expan-sion of critical stereo recordings with an unbalanced stereopanorama is avoided. Mixture 8 was added to investigatethe inﬂuence of the stereo effects (CADSP and TFSP) ontothe input signals in the pre-processing step of pathway Bthat are mixed monophonic.1A description of the Jazzomat Database is available at:http://jazzomat.hfm-weimar.de/dbformat/dbcontent.html752 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Ben WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Stereofoner und räumlicher Eindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Frequenzgang und Klangfarbe\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Stereofoner und räumlicher Eindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Frequenzgang und Klangfarbe\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Stereofoner und räumlicher Eindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Frequenzgang und Klangfarbe\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Stereofoner und räumlicher Eindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Transparenz\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Frequenzgang und Klangfarbe\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorCAEQCAMTSCAEQ+CAMTSHPS+CAEQHPS+CAMTSHPS+CADSP+TFSPHPS+CAEQ+CAMTS+CADSP+TFSPHPS+CAEQ+CAMTS(mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\n12345678910−1−0.8−0.6−0.4−0.200.20.40.60.81\nSound Balance12345678910−1−0.8−0.6−0.4−0.200.20.40.60.81\nTransparency12345678910−1−0.8−0.6−0.4−0.200.20.40.60.81\nStereo/Spatial Impression12345678910−1−0.8−0.6−0.4−0.200.20.40.60.81\nStereo/Spatial ImpressionBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Stereofoner und räumlicher Eindruck\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Frequenzgang und Klangfarbe\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustischer Gesamteindruck\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\n12345678910−1−0.8−0.6−0.4−0.200.20.40.60.81\nTimbre\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\nBen WebsterChu BerryJ.J.JohnsonSonny RollinsWayne ShorterAll0Bad(Schlecht)20Poor(Mangelhaft)40Fair(Ausreichend)60Good(Gut)80Excellent(Sehr Gut)100Akustisches Gleichgewicht\n  OriginalAnchorMix 1 (CAEQ)Mix 2 (CAMTS)Mix 3 (CAEQ+CAMTS)Mix 4 (HPS+CAEQ)Mix 5 (HPS+CAMTS)Mix 6 (HPS+CADSP+TFSP)Mix 7 (HPS+CAEQ+CAMTS+CADSP+TFSP)Mix 8 (HPS+CAEQ+CAMTS, mono)\n11.522.533.544.55−1−0.8−0.6−0.4−0.200.20.40.60.81\nOverall ImpressionFigure 2: Listening test results for the ﬁve evaluated parameters.5.2 Results5.2.1 GeneralFigure 2 shows the results of the listening test for the ﬁveacoustic quality parameters. The ﬁgure legend summarizesall the system conﬁgurations that were evaluated. It is ev-ident from the plot that the anchor stimulus was alwayscorrectly identiﬁed. Results also suggest that the use ofharmonic-percussive separation does not bring perceptualquality gains for HPS+CAEQ (mix 4) when compared tothe CAEQ (mix 3). Unexpectedly, results even got worsefor the parameters timbre and overall impressions. Simi-larly, the combined settings in HPS+CAMTS (mix 5) donot show an improvement in the ratings when compared toCAMTS (mix 2).To facilitate analysis of results, table 4 lists the percent-age improvement obtained for each of the ﬁve quality pa-rameters (QP), subject to the presence or absence of theindividual framework components compared to the origi-nal signal. Mixes 4 and 5, which include the harmonic-percussive separation, were not listed due to the reasonspreviously described. The ﬁve mixtures listed in the tableare further analyzed in the following sections.QP1QP2QP3QP4QP5Mix 1 (CAEQ)18 %17 %4%--Mix 2 (CAMTS)15 %19 %10 %16 %9%Mix 3 (CAEQ+CAMTS)10 %12 %6%-4%Mix 6 (HPS+CADSP+TFSP)9%16 %18 %-8%Mix 7 (All components)29 %24 %43 %3%6%Table 4: Percentage improvement of the remixed signalcompared to the original audio recording subject to thepresence (or absence) of the individual framework com-ponents shown for each of the ﬁve perceptual quality pa-rameters.5.2.2 Mix 1 (CAEQ)Mix 1 does not include a prior separation of the residualcomponent and outperforms the original mix for most ofthe quality parameters. The highest improvements were18% for sound balance and 17% for transparency. How-ever, for timbre and overall impressions, no improvementwas observed.5.2.3 Mix 2 (CAMTS)Despite the absence of the harmonic percussive separationstep, mix 2 showed improvements for transparency (19 %),sound balance (15%), and overall impression (9 %). Thereason for the improvement in timbre by 16% is not en-tirely clear in this case; however, a possible explanationis that the increased loudness of the rhythm section led tomore balanced dynamic levels and a clearer perception ofthe instrument and overall timbres.5.2.4 Mix 3 (CAEQ+CAMTS)The combination of the CAEQ and CAMTS componentsshowed inferior results compared to the exclusive appli-cation of both components. However, the ratings are stillslightly higher than the ratings of the original audio ﬁle.5.2.5 Mix 6 and Mix 7Both mixtures 6 and 7 outperformed the original audio ﬁle.The highest ratings were achieved with mixture 7 whichwas extracted with the full processing chain. In particular,the improvements compared to the original audio ﬁle were29 % for sound balance, 24 % for transparency, as well as43 % for stereo and spatial impression. The small improve-ments with respect to the overall impression are likely dueto the individual aesthetic preferences of the listening testparticipants.Additionally, to analyze the inﬂuence of the stereo ef-fects to the input signals of pathway B (which are initiallyProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 753downmixed to mono), Table 5 presents the percentage im-provement obtained with mix 7 (all components active) incomparison to mix 8 (mono).QP 1QP 2QP 3QP 4QP 539 %16 %33 %12 %18 %Table 5: Mean ratings of the ﬁve quality parameters forthe additional usage of the stereo effects (CADSP+TFSP)in mix 7 compared with the non-processed monophonicinput signal in the same framework setting of mix 8 (HPS,CAEQ, CAMTS, AE).As can be observed in the table, the use of the CADSPand TFSP modules improved the ratings for all ﬁve qualityparameters. The improvement was statistically signiﬁcantfor sound balance (39 %) and stereo/spatial impression (33%).6. CONCLUSIONSIn this paper, we proposed a prototype implementation ofanautomatic remixing frameworkfor tonal optimization ofearly jazz recordings. The main focus was on improvingthe balance between the solo instrument and the rhythmsection. The framework consists of six components whichinclude different processing steps to modify the loudness,frequency response, timbre, and stereophonic perceptionof the separated sound sources. We compared differentconﬁgurations of the framework and evaluated the improve-ment of the transparency of the backing track as well asthe acoustic balance, stereophonic homogeneity, and over-all quality perception. The evaluation was performed witha MUSHRA-like listening test based on the ratings givenby 19 participants.The usage of automatic equalization (CAEQ) and multi-track scaling (CAMTS) showed clear improvement in thequality parameter ratings, whereas the combination of bothled to a smaller improvements than the independent ap-plication of each approach. The improvement based onharmonic-percussive separation (HPS) within the automaticmixing framework is not easy to assess. The usage of HPSin conjunction with CAEQ and CAMTS did not improvethe ratings. On the other hand, HPS is a basic requirementfor the application of CADSP on the backing track of mix7, and therefore contributes to its consistent high ratings.HPS is irrelevant for the automatic excitation (AE) step,since it is applied to the full residual track.Particularly with mix 7 (all components), the initiallytargeted improvements in sound balance, stereo and spatialimpression, and transparency with respect to the originalaudio recording were achieved.In future work, the most relevant processing modulesmust be further investigated and improved with respect tothe aforementioned quality parameters. Modules thatshowed none or only minor improvements must be replacedand alternative algorithms must be evaluated for the giventasks. Promising algorithms seem to be a mastering equal-izer [7] or dynamic range compression [13]. The additionaluse of semantic information of music genre and instrumen-tation seems to be another fruitful approach as discussed insection 3.Finally, the integration of audio restoration methods suchas denoising will likely help to remove unwanted back-ground noise and spurious signals from the main signal tobe processed.7. REFERENCES[1]Daniele Barchiesi and Joshua D. Reiss. Automatic tar-get mixing using least-square optimization of gains andequalization settings. InProceedings of the 12th Inter-national Conference on Digital Audio Effects (DAFx-09), Como, Italy, 2009.[2]Estefanía Cano, Gerald Schuller, and ChristianDittmar. Pitch-informed solo and accompaniment sep-aration: towards its use in music education applica-tions.EURASIP Journal on Advances in Signal Pro-cessing, 23:1–19, 2014.[3]Maximo Cobos and Jose J. Lopez. Interactive enhance-ment of stereo recordings using time-frequency selec-tive panning. InProceedings of the 40th AES Inter-national Conference on Spatial Audio, Tokyo, Japan,2010.[4]Brecht De Man and Joshua D. Reiss. A semantic ap-proach to autonomous mixing.Journal of the Art ofRecord Production, 8, 2013.[5]Brecht De Man and Joshua D. Reiss. Adaptive controlof amplitude distortion effects. InProceedings of the53rd AES International Conference on Semantic Au-dio, London, UK, 2014.[6]Derry FitzGerald. Harmonic/percusssive separation us-ing median ﬁltering. InProceedings of the 13th Inter-national Conference on Digital Audio Effects (DAFx),Graz, Austria, 2010.[7]Marcel Hilsamer and Stefan Herzog. A statistical ap-proach to automated ofﬂine dynamic processing inthe audio mastering process. InProceedings of the17th International Conference on Digital Audio Effects(DAFx-14), Erlangen, Germany, 2014.[8]ISO International Organization for Standardization.Acoustics - preferred frequencies, August 1997.[9]ITU Radiocommunication Bureau. Algorithms to mea-sure audio programme loudness and true-peak audiolevel (rec. itu-r bs.1770-3), August 2012.[10]ITU Radiocommunication Bureau. General methodsfor the subjective assessment of sound quality (rec. itu-r bs.1248-1), December 2003.[11]ITU Radiocommunication Bureau. Method for the sub-jective assessment of intermediate quality level of au-dio systems (rec. itu-r bs.1534-2), June 2014.754 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[12]Zheng Ma, Joshua D. Reiss, and Black, Dawn A. A.Implementation of an intelligent equalization tool us-ing yule-walker for music mixing and mastering. InProceedings of the 134th AES Convention, Rome andItaly, 2013. AES.[13]Stylianos-Ioannis Mimilakis, Konstantinos Drossos,Andreas Floros, and Dionysios Katerelos. Automatedtonal balance enhancement for audio mastering appli-cations. InProceedings of the 134th AES Convention,Rome, Italy, 2013. AES.[14]Enrique Perez Gonzales.Advanced Automatic MixingTools for Music. PhD thesis, Queen Mary University ofLondon, London. UK, 30.09.2010.[15]Pedro D. Pestana and Joshua D. Reiss. A cross-adaptive dynamic spectral panning technique. InPro-ceedings of the 17th International Conference on Dig-ital Audio Effects (DAFx-14), Erlangen, Germany,2014.[16]Jeffrey Scott and Youngmoo E. Kim. Analysis of ac-coustic features for automated multi-track mixing. InProceedings of the 12th International Society for Mu-sic Information Retrieval Conference (ISMIR), Miami,USA, 2011.[17]Jeffrey Scott, Matthew Prockup, Erik M. Schmidt, andYoungmoo E. Kim. Automatic multi-track mixing us-ing linear dynamical systems. InProceedings of the8th Sound and Music Computing Conference (SMC),Padova, Italy, 2011.[18]Priyanka Shekar and Smith, III, Julius O. Modelingthe harmonic exciter. InProceedings of the 135th AESConvention, New York, USA, 2013.[19]U. Zölzer.DAFX: Digital Audio Effects. John Wiley &Sons Ltd., second edition, 2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 755"
    },
    {
        "title": "A Software Framework for Musical Data Augmentation.",
        "author": [
            "Brian McFee",
            "Eric J. Humphrey",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418365",
        "url": "https://doi.org/10.5281/zenodo.1418365",
        "ee": "https://zenodo.org/records/1418365/files/McFeeHB15.pdf",
        "abstract": "Predictive models for music annotation tasks are practi- cally limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a train- ing set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated per- turbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals.",
        "zenodo_id": 1418365,
        "dblp_key": "conf/ismir/McFeeHB15",
        "keywords": [
            "Predictive models",
            "pauca well-annotated training data",
            "data augmentation",
            "large-scale machine learning",
            "concept of data augmentation",
            "important component",
            "general software framework",
            "musically motivated perturbations",
            "task of recognizing instruments",
            "mixed signals"
        ],
        "content": "A SOFTWARE FRAMEWORK FOR MUSICAL DATA AUGMENTATIONBrian McFee1,2,*, Eric J. Humphrey2,3, and Juan P. Bello21Center for Data Science, New York University2Music and Audio Research Laboratory, New York University3MuseAmi, Inc.ABSTRACTPredictive models for music annotation tasks are practi-cally limited by a paucity of well-annotated training data.In the broader context of large-scale machine learning, theconcept of “data augmentation” — supplementing a train-ing set with carefully perturbed samples — has emerged asan important component of robust systems. In this work,we develop a general software framework for augmentingannotated musical datasets, which will allow practitionersto easily expand training sets with musically motivated per-turbations of both audio and annotations. As a proof ofconcept, we investigate the effects of data augmentationon the task of recognizing instruments in mixed signals.1. INTRODUCTIONMusical audio signals contain a wealth of rich, complex,and highly structured information. The primary goal ofcontent-based music information retrieval (MIR) is to ana-lyze, extract, and summarize music recordings in a human-friendly format, such as semantic tags, chord and melodyannotations, or structural boundary estimations. Model-ing the vast complexity of musical audio seems to requirelarge, ﬂexible models with many parameters. By the sametoken, parameter estimation in large models often requiresa large number of samples: big models require big data.Within the past few years, this phenomenon of increas-ing model complexity has been observed in the computervision literature. Currently, the best-performing models forrecognition of objects in images exploit two fundamentalproperties to overcome the difﬁculty of ﬁtting large, com-plex models: access to large quantities of annotated data,and label-invariant data transformations [14]. The beneﬁtsof large training collections are obvious, but unfortunatelydifﬁcult to achieve for most musical annotation tasks dueto the complexity of the label space and need for expertannotators. However, the idea of generating perturbationsof a training set — known asdata augmentation— can bereadily adapted to musical tasks.⇤Please direct correspondence tobrian.mcfee@nyu.educ\u0000Brian McFee, Eric J. Humphrey, Juan P. Bello.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Brian McFee, Eric J. Humphrey, JuanP. Bello. “A software framework for musical data augmentation”, 16thInternational Society for Music Information Retrieval Conference, 2015.Conceptually, data augmentation consists of the appli-cation of one or more deformations to a collection of (an-notated) training samples. Data augmentation is motivatedby the observation that a learning algorithm may general-ize better if it is trained on instances which have been per-turbed in ways which are irrelevant to their labels. Someconcrete examples of deformations drawn from computervision include translation, rotation, reﬂection, and scaling.These simple operations are appealing because they typi-cally do not affect the target class label: an image of a catstill contains a cat, even when it is ﬂipped upside-down.More generally, deformations apply not only to observ-able features, but the labels as well. Continuing with theimage example, if an image is rotated, then any pixel-wiselabel annotations (e.g., bounding boxes) should be rotatedaccordingly. This observation opens up several interestingpossibilities for musical applications, in which the targetconcept space typically exhibits a high degree of structure.A musical analog to the image rotation example would betime-stretching, where time-keyed annotation boundaries(e.g., chord labels or instrument activations) must be ad-justed to ﬁt the stretched signal [16].Many natural, musically-inspired deformations wouldnot only change the position of annotations, but theval-uesthemselves. For instance, if a time-stretched track hastempo annotations, the annotation values should be scaledaccordingly. Similarly, pitch-shifting a track should inducetranspositions of annotated fundamental frequency curves,and if the transposition is sufﬁciently large, chord labels orsymbolic annotations may change as well. Because the an-notation spaces for music tasks often exhibit a high degreeof structure, successful application of data augmentationmay require a more sophisticated approach in MIR than inother domains.1.1 Our contributionsIn this work, we describe the MUDA software architecturefor applying data augmentation to music information re-trieval tasks.1The system is designed to be simple, mod-ular, and extensible. The design enables practitioners todevelop custom deformations, and combine multiple sim-ple deformations together into pipelines which can gener-ate large volumes of reliably deformed, annotated musicdata. The proposed system is built on top of JAMS [12],1https://bmcfee.github.io/muda248which provides a simple container for accessing and trans-porting multiple annotations for a given track.We demonstrate the proposed data augmentation archi-tecture with the application of recognizing instruments inmixed signals, and show that simple manipulations canyield improvements in accuracy.2. RELATED WORKThe ﬁrst step in developing a solution to an MIR problem isoften to design features which discard information thoughtto be irrelevant to the target concept. For example, chromafeatures are designed to capture pitch class information andsuppress timbre, loudness, or octave height [18]. Simi-larly, many authors interested in modeling timbre use Mel-frequency cepstral coefﬁcients (MFCCs) and discard theﬁrst component to achieve invariance to loudness [19]. Thisgeneral strategy makes intuitive sense, but it carries manylimitations. First, it is not necessarily easy to identify allrelevant symmetries in the data: if it was, the modelingproblem would be essentially solved. Second, even if suchproperties are easy to identify, it may still be difﬁcult to en-gineer appropriately invariant features without discardingpotentially useful information. For example, 2-D Fouriermagnitude coefﬁcients achieve invariance to time- and pitch-transposition, but discard phase coherence [8].As an alternative to custom feature design, some authorsadvocate learning or optimizing features directly from thedata [11]. Not surprisingly, this approach typically requireslarge model architectures, and much larger (annotated) datasets than had previously been used in MIR research. Dueto the high cost of acquiring annotated musical data, it hasso far been difﬁcult to apply these techniques in most MIRtasks. While some authors have advocated leveraging unla-beled data to “pre-train” feature representations [6], recentstudies have shown that comparable or better performancecan be achieved with random initialization and fully su-pervised training [9, 22]. Our goal in this work is to pro-vide data augmentation tools which may ease the burdenof sample complexity, and make data-driven methodologymore accessible to the MIR community.Speciﬁc instances of data augmentation can be foundthroughout the MIR literature, though they are not oftenidentiﬁed as such, nor are they treated systematically in auniﬁed framework. For example, it is common to apply cir-cular rotations to chroma features to achieve key invariancewhen modeling chord quality [15]. Alternately, syntheticmixtures of monophonic instruments have been used togenerate more difﬁcult examples when training polyphonictranscription engines [13]. Some authors even leave the au-dio content unchanged and only modify labels during train-ing, as exempliﬁed by thetarget smearingmethod of Ull-richet al.for training structural boundary detectors [21].Finally, recent studies have used degraded signals toevaluate the stability of existing methods for MIR tasks.The Audio Degradation Toolbox (ADT) was developed forthis purpose, and was used to measure the impact of nat-uralistic deformations of audio on several tasks, includingbeat tracking, score alignment, and chord recognition [16].Similarly, Sturm and Collins proposed the “Kiki-BoubaChallenge” as a way to determine whether statistical mod-els of musical concepts actually capture the deﬁning char-acteristics of the concept (e.g., genre), or are over-ﬁtting tospurious correlations [20].In both of the studies cited above, models are ﬁt to un-modiﬁed data, and evaluated in degraded conditions un-der the control of the experimenter. Data augmentationprovides the converse of this setting: models are ﬁt to de-graded data, and evaluated on unmodiﬁed examples. Thedistinction between the two approaches is critical. The for-mer attempts to measure the robustness of a system undersynthetic conditions, while the latter attempts to improverobustness bytrainingunder synthetic conditions. Notethat with data augmentation, the evaluation set is left un-touched by the experimenter, so the resulting comparisonsare unbiased with respect to the underlying distributionfrom which the data are sampled. While this does not di-rectly measure robustness, it has been observed that dataaugmentation can improve generalization [10, 14].3. DATA AUGMENTATION ARCHITECTUREOur implementation takes substantial inspiration from theAudio Degradation Toolbox [16]. In principle, the ADTcan be used directly for some forms of data augmentationsimply by applying it to the training set rather than testset. However, we opted for an independent, Python-basedimplementation for a variety of reasons.First, Python enables object-oriented design, allowingfor structured, extensible, and reusable code. This in turnfacilitates a simple interface shared across alldeformationobjects, and makes it easy for practitioners to combine orextend existing deformations.Second, we use JAMS [12] both to transport and storetrack annotations, and as an internal data structure for pro-cessing. JAMS provides a uniﬁed interface to different an-notation types, and a convenient framework to manage allannotations for a particular track. This simpliﬁes the tasksof maintaining synchronization between audio and annota-tions, and implementing task-dependent annotation defor-mations. We also adapt JAMS sandbox ﬁelds to providedata provenance and facilitate reproducibility.Finally, we borrow familiar software design patterns fromthe scikit-learn package [4], such astransformers,pipelines,and model serialization. These building blocks allow prac-titioners to quickly and easily assemble complex pipelinesfrom small, conceptually simple components.In the remainder of this section, we will describe thesoftware architecture in more detail. Without loss of gen-erality, we assume that an annotation (e.g., instrument ac-tivations) is encoded as a collection of tuples:(time, du-ration, value, conﬁdence). Note that instantaneous eventscan be represented with zero duration, while track-level an-notations have full-track duration. Thevalueﬁeld dependson the annotation type, and may encode strings, numericquantities, or fully structured objects.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2493.1 Deformation objectsAt the core of our implementation is the concept of ade-formation object. We will ﬁrst describe deformation ob-jects in terms of their methods and abstract properties. Sec-tion 3.1.1 follows with a concrete, but high-level example.A deformation object implements one or moretrans-formationmethods, each of which applies to either audio,meta-data, or annotations. Parameters of the deformationare shared through astateobjectS. For example,Smightcontain the speed-up factor of a time-stretch, or the numberof semi-tones in a pitch-shift. Each transformation methodtakes as input a pair(S, x)and returns the transformed au-dio, meta-data, or annotationx0. Decoupling the defor-mation object’s instantiation from its state allows multipletracks to be processed in parallel by the same object. More-over, as described in Section 3.3, state objects are reusable,which promotes reproducibility.Data augmentation often requires sampling or sweepinga set of deformation parameters, and instantiating a sepa-rate deformation object for each parameterization can beinefﬁcient, especially when theScontains non-trivial data(e.g., tuning estimates or noise signals). Instead, a defor-mation object implements astate generator, which mayexecute arbitrary transition logic to produce a sequence ofstates(S1,S2,...). This is implemented efﬁciently usingPythongenerators.Finally, deformation objects may register transforma-tion functions against thetypeof an annotation, as de-scribed by regular expressions. This allows different trans-formation procedures to be applied to different annotationtypes. During execution, the JAMS object is queried forall annotations matching the speciﬁed expression, and theresults are processed by the corresponding transformationmethod. For example, the expression “.*” matches allannotation types, while “chord.*” matches only chord-type annotations. These patterns need not be unique ordisjoint, though care must be taken to ensure consistentbehavior. Deformations are always applied following theorder in which they are registered.The abstract transformation algorithm is described inAlgorithm 1. For each stateS, the input dataJis copied,transformed intoJ0, and yielded back to the caller. EachJ0can then be exported to disk, provided as a sample to aniterative learning algorithm, or passed along to another de-formation object in a pipeline for further processing. Whenall subsequent processing ofJ0has completed, Algorithm 1may resume computation at line 10 and proceed to the nextstate at line 2. Note that because deformation objects areboth iterative (per track) and can be parallelized (acrosstracks), batches of deformed data can be generated onlinefor stochastic learning algorithms.3.1.1 Example: randomized time-stretchingTo illustrate the deformation object interface, we will de-scribe the implementation of a randomizedtime-stretchde-formation object. In this case, eachstateobject contains asingle quantity: the stretch factorr. Algorithm 2 illustratesthe state-generation logic for a randomized time-stretcher,Algorithm 1Abstract transformation pseudocodeInput:Deformation objectD, JAMS objectJOutput:Sequence of transformed JAMS objectsJ01:functionD.TRANSFORM(J)2:forstatesS2D.STATES(J)do3:J0 COPY(J)4:J0.audio D.AUDIO(S, J0.audio)5:J0.meta D.METADATA(S, J0.meta)6:fortransformationsginDdo7:forannotationsA2J0which matchgdo8:J0.A g(S, A)9:J0.history APPEND(J0.history,S)10:yieldJ0Algorithm 2Randomized time-stretch state generatorInput:JAMS objectJ, number of deformationsn, rangebounds(r\u0000,r+)Output:Sequence of statesS1:functionRANDOMSTRETCH.STATES(J,{n, r\u0000,r+})2:foriin1,2,...,ndo3:Sampler⇠U[r\u0000,r+]4:yieldS={r}in which somenexamples are generated by samplingruniformly at random from an interval[r\u0000,r+].2The JAMS objectJover which the deformations willbe applied is also provided as input to the state generator.Though not used in this example, access toJallows thestate generator to pre-compute quantities of interest, suchas track duration — necessary to ensure well-deﬁned out-puts from target-smearing deformations — or tuning esti-mates, which are used by pitch-shift deformations to deter-mine when a shift is large enough to alter note labels.Once a stateShas been generated, theAUDIO() de-formation method —D.AUDIO(S, J.audio)— applies thetime-stretch to the audio signal, which is stored within theJAMS sandbox upon instantiation.3Similarly, track-levelmeta-data can be modiﬁed by theMETADATA() method. Inthis example, time-stretching will change the track dura-tion, which is recorded in the JAMS meta-data ﬁeld.Next, a genericannotationdeformation would be reg-istered to the pattern “.*” and apply the stretch factor toalltimeanddurationﬁelds of all annotations. This defor-mation would leave the annotationvaluesuntouched, sincenot all annotation types have time-dependent values.Finally, any annotations whosevalueﬁelds depend ontime, such astempo, can be modiﬁed directly by regis-tering the transformation function against the appropriatetype pattern,e.g.,“tempo”. Other time-dependent typedeformations would be registered separately as needed.The time-stretching example is simple, but it serves toillustrate the ﬂexibility of the architecture. It is straight-forward to extend this example into more sophisticated de-2The parametersn, r\u0000,r+are actually properties of the deformationobject, but are listed here as method parameters to simplify exposition.3Thesandboxprovides unstructured storage space within a JAMS ob-ject, which is used in our framework as a scratch space for audio signals.250 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015formations with structured state generators to sweep overdeterministic parameter grids. For example, an additivebackground noise deformation could be parameterized bya collection of noise sources and a range of gain parame-ters, and generate one example for each unique combina-tion of source and gain.3.2 Pipelines and bypassesAlgorithm 1 describes the process by which a deformationobject turns a single annotated audio example into a se-quence of deformed examples. If we were interested in ex-perimenting with only a single type of augmentation (e.g.,time stretching), this would sufﬁce. However, some appli-cations may require combining or cascading multiple typesof deformation, and we prefer a uniﬁed interface that obvi-ates the need for customized data augmentation scripts.Here, we draw inspiration from scikit-learn in deﬁn-ingpipelineobjects. The general idea is simple: two ormore deformation objectsDican be chained together, andtreated as a single, integrated deformation object. Moreprecisely, for a deformation pipelinePcomposed ofkstages:P=(D1,D2,...,Dk),examples are generated by a depth-ﬁrst traversal of theCartesian product of the corresponding state spaces⌃i:⌃P=⌃1⇥⌃2⇥···⇥⌃k.One input example therefore produces|⌃P|=Qki=1|⌃i|outputs. By using generators rather than explicit lists ofstates, we ensure that onlyk+1examples (counting theinput) are ever in memory at any time. In most cases,kis much smaller than|⌃P|, which provides substantial im-provements to memory efﬁciency.Finally, we introduce thebypassobject, which is usedto mark individual pipeline stages as optional. Bypassesare useful when it is difﬁcult to encode a specialno trans-formationstate within a deformation object, such as in therandomized time-stretch example of Algorithm 2. The in-ternal logic of a bypass object is simple: ﬁrst, pass the in-put directly through unmodiﬁed, and then generate sam-ples from the contained deformation object as usual. By-passes can be used to ensure that the original examples arepropagated through the pipeline unscathed, and the result-ing augmented data set is a strict superset of the clean data.3.3 Reproducibility and data provenanceWhen modifying data for statistical modeling purposes,maintaining transparency is of utmost importance to en-sure reproducibility and accurate interpretation of results.This ultimately becomes a question of data provenance [5]:a record of all transformations should be kept, preferablyattached as closely as possible to the data. Rather thanforce practitioners to handle book-keeping, we automatethe process from within the deformation engine. This isaccomplished at line 9 of Algorithm 1 by embedding thestateobjectS(and, in practice, the parameters used to con-struct the deformation objectD) within the JAMS objectTable 1. The 15 instrument labels used in our experiments.Instrument # Tracks # Artistsdrum set 65 57electric bass 64 53piano 42 23male singer 38 34clean electric guitar 37 32vocalists 27 25synthesizer 27 21female singer 25 17acoustic guitar 24 16distorted electric guitar 21 20auxiliary percussion 18 17double bass 16 13violin 14 5cello 11 8ﬂute 11 6after each deformation is applied. EachJ0generated atline 10 thus contains a full transactional history of all mod-iﬁcations required to transformJintoJ0. For this reason,stochastic deformations are designed so that all random-ness is contained within the state generator, and transfor-mations are all deterministic.In addition to facilitating reproducibility, maintainingtransformation provenance allows practitioners to computea wide range of deformations, and later ﬁlter the results toderive subsets generated by different augmentation param-eters.To further facilitate reproducibility and sharing of ex-perimental designs, the proposed architecture supportsse-rializationof deformation objects and pipelines into a sim-ple, human-readable JavaScript object notation (JSON) for-mat. Once a pipeline has been constructed, it can be ex-ported, edited as plain text, shared, and reconstructed. Thisfeature also simpliﬁes the process of applying several dif-ferent sets of deformation parameters, and eliminates theneed for writing a custom script for each setting.4. EXAMPLE: INSTRUMENT RECOGNITIONWe applied data augmentation to the task of instrumentrecognition in mixed audio signals. For this task, we usedthe MedleyDB dataset, which consists of 122 tracks, span-ning a variety of genres and instrumentation [3]. Eachtrack is strongly annotated with time-varying instrumentactivations derived from the recording stems. MedleyDBis a small, but well-annotated collection, which we selectedbecause it should be possible to over-ﬁt with a reasonablycomplex model. Our purpose here is not to achieve thebest possible recognition results, but to investigate utilityof data augmentation for improving generalization. How-ever, because of the small sample size, we limited the ex-periment to cover only the 15 instruments listed in Table 1.For evaluation purposes, each test track is split into dis-joint one-second clips. The system is then tasked withrecognizing the instruments active within each clip. Thesystem is evaluated according to the average track-wisemean (label-ranking) average precision (LRAP), and per-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 251instrumentF-score over one-second clips.4.1 Data augmentationThe data augmentation pipeline consists of four stages:Pitch shiftbyn2{\u00001,0,+1}semitones.Time stretchby a factor ofr2\u00002\u00001/2,1.0,21/2 .Background noise (bypass)under three conditions: sub-way,crowded concert hall,and night-time city noise.Noise clips were randomly sampled and linearly mixedwith the input signalyusing random weights↵⇠U[0.1,0.4]:y0 (1\u0000↵)·y+↵·ynoise.Dynamic range compression (bypass)under two settingsdrawn from the Dolby E standards [7]:speech, andmusic (standard).Pitch-shift and time-stretch operations were implementedwith Rubberband [1], and dynamic range compression wasimplemented using thecompandfunction of sox [2]. Notethat the ﬁrst two stages include null parameter settingsn=0andr=1. Bypasses on the ﬁnal two stages ensure thatall combinations of augmentation are present in the ﬁnalset. The full pipeline produces|⌃P|=3⇥3⇥(3 + 1)⇥(2 + 1) = 108variants of each input track. To simplify the experiments,we only compare the cumulative effects of the above aug-mentations. This results in ﬁve training conditions of in-creasing complexity:•(N) no augmentation,•(P) pitch shift,•(PT) pitch shift and time stretch,•(PTB) pitch shift, time stretch, and noise,•(PTBC) all stages.4.2 Acoustic modelThe acoustic model used in these experiments is a deepconvolutional network. The input to the network consistsof log-amplitude, constant-Q spectrogram patches extractedwith librosa [17]. Each example spans approximately onesecond of audio, corresponding to 44 frames at a hop lengthof 512 samples and sampling rate of22050Hz. Constant-Q spectrograms cover the range of C2 (65.41Hz) to C8(4186Hz) at 36 bins per octave, resulting in time-frequencypatchesX2R216⇥44. Instrument activations are aggre-gated into a single binary label vector, such that an instru-ment is deemed active if its on-time within the sample ex-ceeds 0.25 seconds.Constant-Q representations are linear in both time andpitch, a property that can be exploited by convolutionalneural networks to achieve translation invariance. Thus afour-layer model is designed to estimate the presence ofzero or more instruments in a time-frequency patch. For-mally, an inputX, is transformed into an outputZ, via acomposite nonlinear functionF(·|⇥)with parameters⇥.This is achieved as a sequential cascade ofL=4opera-tions,f`(·|✓`), referred to aslayers, the order of which isgiven by`:Z=F(X|⇥) =fL(···f2(f1(X|✓1)|✓2)|✓L)(1)The ﬁrst two layers,`2{1,2}, are convolutional, ex-pressed by the following:Z`=f`(X`|✓`)=h(W~X`+b),✓`=[W, b](2)Here, the valid convolution,~, is computed by convolvinga 3D input tensor,X`, consisting ofNfeature maps, witha collection ofM3D-kernels,W, followed by an additivevector bias term,b, and transformed by a point-wise ac-tivation function,h(·). In this formulation,X`has shape(N,d0,d1),Whas shape(M,N,m0,m1), and the output,Z`, has shape(M,d0\u0000m0+1,d1\u0000m1+1). Max-poolingis applied in time and frequency, to further accelerate com-putation by reducing the size of feature maps, and allowinga small degree of scale invariance in both time and pitch.The ﬁnal two layers,`2{3,4}, are fully-connectedmatrix products, given as follows:Z`=f`(X`|✓`)=h(WX`+b),✓`=[W, b](3)The input to the`thlayer,X`, is ﬂattened to a columnvector of lengthN, projected against a weight matrixWofshape(M,N), added to a vector bias term,b, of lengthM,and transformed by a point-wise activation function,h(·).The network is parameterized thusly:`1usesWwithshape(24,1,13,9), followed by(2,2)max-pooling overthe last two dimensions, and a rectiﬁed linear unit (ReLU)activation function:h(x)··= max(x,0);`2has ﬁlter pa-rametersWwith shape(48,24,9,7), followed by(2,2)max-pooling over the last two dimensions, and a ReLU ac-tivation function;`3usesWwith shape(17280,96)anda ReLU activation function; ﬁnally,`4usesWwith shape(96,15)and a sigmoid activation function.During training, the model optimizes cross-entropy lossvia mini-batch stochastic gradient descent, using batchesofn= 64randomly selected patches and a constant learn-ing rate of0.01. Dropout is applied to the activations ofthe penultimate layer,`=3with dropout probability0.5.Quadratic regularization is applied to the weights of the ﬁ-nal layer,`=4, with a penalty factor of0.02. This helpsprevent numerical instability by keeping the weights fromgrowing arbitrarily large. The model is check-pointed af-ter every1000batches (up to50000batches), and a vali-dation set is used to select the parameter setting achievingthe highest mean LRAP.4.3 EvaluationFifteen random artist-conditional partitions of the Med-leyDB collection were generated with a train/test artist ra-tio of 4:1. For the purposes of this experiment,MusicDeltatracks were separated by genre into a collection of distinct252 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1. Test-set score distributions (mean track-wiselabel-ranking average precision), over all train-test splits.Mean scores are indicated by•. Boxes cover the25–75percentiles, and whiskers cover the5–95percentiles.pseudo-artists. This results in 75 unique artist identiﬁersfor the 122 tracks. For each train/test split, the trainingset was further partitioned into training and validation sets,again at a ratio of 4:1. To evaluate performance, we com-pute for each test track the mean label-ranking average pre-cision (LRAP) over all disjoint one-second patches.4.3.1 Label ranking resultsFigure 1 illustrates the distribution of test-set performanceacross splits. Between the no-augmentation condition (N)and pitch-shifting augmentation (P), there is a small, butconsistent improvement in performance from an averageof 0.655 to 0.677. This is in keeping with the motivationfor this work, and our expectations when training a (pitch)-convolutional model on a small sample. If the amount ofclean data is too small, the model may easily over-ﬁt bycapturing irrelevant, correlated properties. (For example,if all of thepianorecordings are in one key, the modelmay simply capture the key rather than the characteristicsofpiano.) Adding pitch-shifted examples should help themodel disambiguate these properties.Subsequent deformations do not appear to improve overcondition (P). In each case, no signiﬁcant difference fromthe pitch-shift condition could be detected by a Bonferroni-corrected Wilcoxon signed-rank test. However, all defor-mation conditions consistently outperform the baseline (N).Although the difference in average performance is rel-atively small, the upper and lower quantiles are notablyhigher in (P), (PT), and (PTB) conditions. This indicatesa reduction in the tendency to over-ﬁt the relatively smalltraining sets used in these experiments.4.3.2 Frame-tagging resultsTo investigate the effects of augmentation on each instru-ment class, we computed theF-score of frame-level instru-ment recognition under each training condition. Resultswere averaged ﬁrst across test tracks in a split, and thenacross all splits. Figure 2 depicts the change inF-scorerelative to the baseline condition (N):\u0000F=(F\u0000FN).The trend is primarily positive: in all but three classes,all augmentation conditions provide consistent improve-ment. The three exceptions aresynthesizer,female singer,andviolin. In the latter two cases, negative change is onlyobserved after introducing time-stretch deformations, which\nFigure 2. Per-class change in mean test-setF-scorefor each augmentation condition (F), relative to the no-augmentation baseline (FN).may unnaturally distort vibrato characteristics and renderthese classes more difﬁcult to model. The effect is partic-ularly prominent forviolin, which has the fewest uniqueartists, and produces the fewest training examples.The reduction inF-score forsynthesizerin the (PT)condition may explain the corresponding reduction in Fig-ure 1, and may be due to a conﬂuence of factors. First,many of the synthesizer examples in MedleyDB have lowamplitudes in the mix, and may be difﬁcult to model ingeneral. Second, the class itself may be ill-deﬁned, assyn-thesizerencompasses a range of instruments and timbreswhich may be artist-dependent and idiosyncratic. Simpleaugmentations can have adverse effects if the perturbed ex-amples are insufﬁciently varied from the originals, whichmay be the case here for (P) and (PT). However, the in-clusion of background noise (PTB) results in a slight im-provement over the baseline.5. CONCLUSIONThe data augmentation framework provides a simple andﬂexible interface to train models on distorted data. Theinstrument recognition experiment demonstrates that evensimple deformations such as pitch-shifting can improve gen-eralization, but that some care should be exercised whenselecting deformations depending on the characteristics ofthe problem. We note that these results are preliminary,and do not fully exploit the capabilities of the augmenta-tion framework. In future work, we will investigate thedata augmentation for a variety of MIR tasks.6. ACKNOWLEDGEMENTSBM acknowledges support from the Moore-Sloan Data Sci-ence Environment at NYU. This material is partially basedupon work supported by the National Science Foundation,under grant IIS-0844654.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2537. REFERENCES[1]Rubber band library v1.8.1, October 2012.http://rubberbandaudio.com/.[2]sox v14.4.1, February 2013.http://sox.sourceforge.net/.[3]Rachel Bittner, Justin Salamon, Mike Tierney,Matthias Mauch, Chris Cannam, and Bello, JuanPablo. MedleyDB: a multitrack dataset for annotation-intensive mir research. In15th International Societyfor Music Information Retrieval Conference, ISMIR,2014.[4]Lars Buitinck, Gilles Louppe, Mathieu Blondel, FabianPedregosa, Andreas Mueller, Olivier Grisel, Vlad Nic-ulae, Peter Prettenhofer, Alexandre Gramfort, JaquesGrobler, et al. API design for machine learningsoftware: experiences from the scikit-learn project.InEuropean Conference on Machine Learning andPrinciples and Practices of Knowledge Discovery inDatabases, 2013.[5]Peter Buneman, Sanjeev Khanna, and Wang-ChiewTan. Data provenance: Some basic issues. InFST TCS2000: Foundations of software technology and theoret-ical computer science, pages 87–93. Springer, 2000.[6]Sander Dieleman, Phil´emon Brakel, and BenjaminSchrauwen. Audio-based music classiﬁcation with apretrained convolutional network. In12th internationalsociety for music information retrieval conference, IS-MIR, 2011.[7]Dolby Laboratories, Inc.Standards and practices forauthoring Dolby Digital and Dolby E bitstreams, 2002.[8]Daniel PW Ellis and Thierry Bertin-Mahieux. Large-scale cover song recognition using the 2d fourier trans-form magnitude. InThe 13th international society formusic information retrieval conference, ISMIR, 2012.[9]Xavier Glorot, Antoine Bordes, and Yoshua Ben-gio. Deep sparse rectiﬁer networks. InProceedings ofthe 14th International Conference on Artiﬁcial Intelli-gence and Statistics. JMLR W&CP Volume, volume 15,pages 315–323, 2011.[10]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation.arXivpreprint arXiv:1502.01852, 2015.[11]Eric J Humphrey, Juan Pablo Bello, and Yann LeCun.Moving beyond feature design: Deep architectures andautomatic feature learning in music informatics. InThe13th international society for music information re-trieval conference, ISMIR, 2012.[12]Eric J Humphrey, Justin Salamon, Oriol Nieto, JonForsyth, Rachel M Bittner, and Bello, Juan Pablo.JAMS: A JSON annotated music speciﬁcation for re-producible MIR research. In15th International Societyfor Music Information Retrieval Conference, ISMIR,2014.[13]Holger Kirchhoff, Simon Dixon, and Anssi Klapuri.Multi-template shift-variant non-negative matrix de-convolution for semi-automatic music transcription. InThe 13th international society for music informationretrieval conference, ISMIR, 2012.[14]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-ton. ImageNet classiﬁcation with deep convolutionalneural networks. InAdvances in neural informationprocessing systems, NIPS, pages 1097–1105, 2012.[15]Kyogu Lee and Malcolm Slaney. Acoustic chord tran-scription and key extraction from audio using key-dependent HMMs trained on synthesized audio.Audio,Speech, and Language Processing, IEEE Transactionson, 16(2):291–301, 2008.[16]Matthias Mauch and Sebastian Ewert. The audiodegradation toolbox and its application to robustnessevaluation. In14th International Society for Music In-formation Retrieval Conference, ISMIR, 2013.[17]Brian McFee, Matt McVicar, Colin Raffel, DawenLiang, Dan Ellis, Douglas Repetto, Petr Viktorin, andJoo Felipe Santos. librosa: 0.4.0rc1, March 2015.[18]Meinard M¨uller and Sebastian Ewert. Chroma tool-box: Matlab implementations for extracting variantsof chroma-based audio features. In12th InternationalConference on Music Information Retrieval, ISMIR,2011.[19]Elias Pampalk. A matlab toolbox to compute musicsimilarity from audio. InInternational Symposium onMusic Information Retrieval (ISMIR2004), 2004.[20]Bob L Sturm and Nick Collins. The Kiki-Bouba Chal-lenge: Algorithmic composition for content-basedMIR Research & Development. InInternational Sym-posium on Music Information Retrieval, 2014.[21]Karen Ullrich, Jan Schl¨uter, and Thomas Grill. Bound-ary detection in music structure analysis using convo-lutional neural networks. In15th International Societyfor Music Information Retrieval Conference, ISMIR,2014.[22]Matthew D Zeiler, M Ranzato, Rajat Monga, M Mao,K Yang, Quoc Viet Le, Patrick Nguyen, A Senior, Vin-cent Vanhoucke, Jeffrey Dean, et al. On rectiﬁed linearunits for speech processing. InAcoustics, Speech andSignal Processing (ICASSP), 2013 IEEE InternationalConference on, pages 3517–3521. IEEE, 2013.254 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Hierarchical Evaluation of Segment Boundary Detection.",
        "author": [
            "Brian McFee",
            "Oriol Nieto",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414866",
        "url": "https://doi.org/10.5281/zenodo.1414866",
        "ee": "https://zenodo.org/records/1414866/files/McFeeNB15.pdf",
        "abstract": "Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and refined down to the short melodic ideas at the motivic level. However, typ- ical algorithmic approaches to structural annotation pro- duce flat temporal partitions of a track, which are com- monly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierar- chical structure annotations have been recently published, no techniques yet exist to measure an algorithm’s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detec- tion with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical an- notations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset.",
        "zenodo_id": 1414866,
        "dblp_key": "conf/ismir/McFeeNB15",
        "keywords": [
            "hierarchical",
            "structure",
            "analysis",
            "algorithmic",
            "approaches",
            "flat",
            "temporal",
            "partitions",
            "motivic",
            "level"
        ],
        "content": "HIERARCHICAL EVALUATION OF SEGMENT BOUNDARY DETECTIONBrian McFee1,2, Oriol Nieto2, and Juan P. Bello21Center for Data Science, New York University2Music and Audio Research Laboratory, New York University1,2{brian.mcfee, oriol, jpbello}@nyu.eduABSTRACTStructure in music is traditionally analyzed hierarchically:large-scale sections can be sub-divided and reﬁned down tothe short melodic ideas at the motivic level. However, typ-ical algorithmic approaches to structural annotation pro-duce ﬂat temporal partitions of a track, which are com-monly evaluated against a similarly ﬂat, human-producedannotation. Evaluating structure analysis as represented byﬂat annotations effectively discards all notions of structuraldepth in the evaluation. Although collections of hierar-chical structure annotations have been recently published,no techniques yet exist to measure an algorithm’s accuracyagainst these rich structural annotations. In this work, wepropose a method to evaluate structural boundary detec-tion with hierarchical annotations. The proposed methodtransforms boundary detection into a ranking problem, andfacilitates the comparison of both ﬂat and hierarchical an-notations. We demonstrate the behavior of the proposedmethod with various synthetic and real examples drawnfrom the SALAMI dataset.1. INTRODUCTIONThe analysis of structure in music is a principal area ofinterest to musicologists. Its goal is to identify and char-acterize the form of a musical piece by investigating theorganization of its components, such as sections, phrases,melodies, or recurring motives. Traditional analyses usu-ally provide multiple levels of annotation (e.g., Schenke-rian analysis), which suggest that music is structured hier-archically [3], and can be modeled and analyzed using treerepresentations [2].In the music information research literature,music seg-mentation(also known asmusic structure analysis) is atask that aims to automatically identify the structure of amusical recording [6]. The segmentation task has histori-cally been geared toward algorithms which produce a ﬂatpartition of the recording into disjoint segments. This for-malization contrasts with our intuition that music exhibitshierarchical structure [7,8]. Even though a large dataset ofc\u0000Brian McFee, Oriol Nieto, Juan P. Bello.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Brian McFee, Oriol Nieto, Juan P.Bello. “Hierarchical Evaluation of Segment Boundary Detection”, 16thInternational Society for Music Information Retrieval Conference, 2015.hierarchically-structured human annotations is now pub-licly available [8], current evaluation methodologies aredeﬁned only forﬂatsegmentations. As a result, the dimen-sion ofdepthhas been practically ignored in the evaluationof music segmentation algorithms.In contrast to segmentation, thepattern discoverytaskformulation allows output segments to overlap, and the an-notation is not required to cover the entire piece. These twotasks share multiple attributes [5], and steps toward a gen-eral formulation musical structure analysis could be madeby accounting for depth in segmentation. Numerous met-rics to evaluate pattern discovery have been proposed [1].However, they are designed to capture repeated patterns,and would be inappropriate for evaluating non-repeating,hierarchical structure.1.1 Our contributionsWe present theTree Measures(T-measures): an evaluationframework designed to measure the accuracy of boundarydetection in hierarchical segmentations. TheT-measuresinfer frame-wise similarity from a hierarchical annotation,and then compare the induced rank-orderings to assessagreement between reference and estimated annotations.TheT-measures integrate information from all layers ofa hierarchy, trivially specialize to handle ﬂat annotations,and require no explicit correspondence between the depthof the estimated and reference hierarchies. Thus, theT-measures encourage the development of new algorithmsto produce richer representations of structure. Althoughnot all music can necessarily be modeled using trees [11],we argue that tree-based evaluation represents a ﬁrst steptoward moving beyond ﬂat structure analyses. We demon-strate the properties ofT-measures with multiple synthetic,human, and algorithmic examples.2. SEGMENT BOUNDARY EVALUATIONSegmentation algorithms are typically evaluated for twodistinct goals. The ﬁrst goal,boundary detection, evalu-ates the algorithm’s ability to detect the times of transitionsbetween segments. The second goal,structural grouping,evaluates the labeling applied to the estimated segmenta-tion, and thus quantiﬁes the ability of an algorithm to detectrepeated forms, such as verses or refrains. In this paper, wefocus exclusively on the boundary detection task.Boundary estimates are typically evaluated by precisionand recall [10]. Estimated and reference boundaries are406matched within a speciﬁed tolerance window — typicallyeither 0.5 or 3 seconds — and the hit ratenh(number ofmatches) is used to deﬁne precision and recall scores:P··=nhne,R··=nhnr,(1)whereneandnrdenote the number of boundaries in theestimated and reference annotations, respectively.PandRare typically combined into a singleF-measure by com-puting their harmonic mean.Boundary detection has also been evaluated bydevi-ation[10]. This is done by measuring the median time(absolute) differential between each reference boundaryand the nearest estimated boundary (R2E), and vice versa(E2R). Boundary deviation is useful for quantifying thetemporal accuracy of a detection event. However, it canbe sensitive to the number of estimated boundaries.2.1 The limitations of ﬂat evaluationThe precision-recall paradigm has been critical to quanti-fying improvements in segmentation algorithms, but it hasnumerous limitations with hierarchical annotations. Themost obvious limitation is that both the reference and esti-mated annotations must have ﬂat structure. This is some-times resolved by collecting multiple ﬂat reference annota-tions for each track, each corresponding to different levelsof analysis [8].When only the estimation is ﬂat, it is still not obvioushow to compute accuracy against multiple layers. Aggre-gating reference boundaries across layers prior to evalua-tion would imply that all boundaries are equally informa-tive. However, high-level boundaries often convey moreinformation about the overall structure of the piece, buttheir contribution to the total score may be diluted by theabundance of low-level boundaries, which necessarily out-number high-level boundaries in hierarchical annotations.Flat evaluation followed by aggregation across layerscan be similarly problematic, since it discards the rela-tional structure between layers in the reference annotation.This can complicate interpretation of the scores by con-ﬂating inaccurate boundary detection with mismatch be-tween the target levels of the estimate and reference anno-tations [9].Finally, the above strategies provide no means to di-rectly compare two hierarchical annotations. While onemay imagine simple comparison strategies when both hi-erarchies have a small number of layers with an obviouslayer-wise correspondence —e.g., SALAMI’slarge- andsmall-scale annotations — it is unclear how to proceed inmore general settings.3. THE TREE MEASURESIn this section, we derive thetree measuresfor evaluatingmulti-level segment boundary detection. The evaluationis based on a reduction to ranking evaluation, which wedescribe in detail below.\nFigure 1:An example of a three-level hierarchical segmentation.Framesi, j, andkare indicated along thex-axis, and their con-taining segments are indicated within the ﬁgure,e.g.,H(j, k).3.1 PreliminariesLetXdenote a set of sample frames generated from thetrack at some ﬁxed resolutionfr(e.g., 10Hz).1LetSde-note a ﬂat, temporally contiguous partition ofX, and letS(i)identify the segment containing theith frame inX.We will use the subscriptsSRandSEto denotereferenceandestimatedannotations, respectively.Ahierarchical segmentationHis deﬁned as a tree ofﬂat segmentations(S0,S1,...,Sd)where each layer is areﬁnementof the preceding layer.2LetH(i, j)identifythe smallest (most reﬁned) segment containing framesiandj. We will denote precedence (containment) of seg-ments by\u0000:e.g.,H(j, k)\u0000H(i, k). Note that ﬂatsegmentations are a special case of hierarchical segmen-tations, where there are only two levels of segmentation,and the ﬁrst layer contains no boundaries.As illustrated in Figure 1, hierarchical segmenta-tions can be represented as tree structures. Here,H(i, i),H(j, j)andH(k,k)denote the most speciﬁc seg-ments containing framei, jandk, respectively. From theﬁgure, we observe thatH(j, k)identiﬁes the least commonancestor of framesjandk. We can generally infer mem-bership and precedence relations from the hierarchy,e.g.,j2H(j, j)\u0000H(j, k)\u0000H(i, j)=H(i, k).(2)3.2 Flat segmentation and bipartite rankingSegmentation evaluation can be reduced to a ranking eval-uation problem as follows. Letqdenote an arbitraryframe, and letiandjdenote any two frames such thatSR(q)=SR(i)andSR(q)6=SR(j). In this case,imaybe consideredrelevantforq, andjis consideredirrelevant.This leads to the following per-frame recall metric:f(q;SE,SR)··=Xi2SR(q)\\{q},j/2SR(q)JSE(q)=SE(i)6=SE(j)KZq(3)Zq··=(|SR(q)|\u00001)·(n\u0000|SR(q)|+ 1),whereJ·Kis the indicator function,n=|X|denotes thetotal number of frames, andZqcounts the number of termsin the summation. The score for frameqis the fraction of1Non-uniform samplings (e.g., beat- or onset-aligned samples) arealso easily accommodated.2A partitionSi+1is a reﬁnement of partitionSiif each member ofSi+1is contained within exactly one member ofSi.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 407pairs(i, j)for whichSEagrees withSRwith respect toq.Averaging over allqyields a mean recall score:⇢(SE,SR)··=1nXqf(q;SE,SR).(4)3.3 Hierarchies and partial rankingEquation (3) is deﬁned in terms of segment membershipequality, but it has a straightforward generalization to hier-archical segmentations. If we restrict attention to a querysampleq, thenH(q,·)induces a partial ranking over theremaining samples. Frames contained inH(q,q)are con-sidered maximally relevant, followed by those inH(q,q)’simmediate ancestor, and so on.Rather than compare framesq,i, andjwhereS(q)=S(i)6=S(j), we can instead compare whereH(q,i)\u0000H(q,j):i.e., the pair(q,i)merge deeper inthe hierarchy than do(q,j). This leads to the followinggeneralization of Equation (3):g(q;HE,HR)··=X(i,j),i6=q,HR(q,i)\u0000HR(q,j)JHE(q,i)\u0000HE(q,j)KZq,(5)whereZqis suitably modiﬁed to count the number of termsin the summation. This deﬁnition is equivalent to Equa-tion (3) for ﬂat hierarchies, but it applies more generally tohierarchies of arbitrary (and unequal) depth.Just as in Equation (3),gcan be viewed as a classiﬁca-tion accuracy of correctly predicting pairs(i, j)as positive(qandimerge ﬁrst) or negative (qandjmerge ﬁrst). Ties(H(q,i)=H(q,j)) are precluded by the strict precedenceoperator in the summation. Equation (5) can be alternatelybe viewed as a generalized area under the curve (AUC)over the partial ranking induced by the hierarchical seg-mentation, where depth within the estimated hierarchyHEplays the role of the detection threshold.Averaging overqyields thetree-recallT-measure:TR(HE,HR)··=1nXqg(q;HE,HR).(6)Thetree-precisionmetricTP(HE)is deﬁned analogouslyby swapping the roles ofHEandHR:TP(HE,HR)··=TR(HR,HE).(7)Intuitively,TRmeasures how many triplets generated bythe referenceHRcan be found in the estimateHE, whileTPcomputes the converse. TheT-measures retain inter-pretation as recall and precision scores, albeit at the levelof frame triplets rather than boundaries. Finally, an anal-ogousF-measureTFcan be deﬁned in the usual way bycomputing the harmonic mean ofTPandTR.3.4 Windowing in TimeTheT-measures deﬁned above capture the basic notion ofhierarchically nested, frame-level relevance, but they posethree technical limitations. First, the score for each querywill generally depend on the track durationn, which makescomparisons between tracks of differing length problem-atic. Second, for large values ofn(long tracks), Equa-tion (5) can be dominated by trivial comparisons wherejlies far fromqin time,i.e.,|q\u0000i|⌧|q\u0000j|. Longer trackswill produce inﬂated scores compared to shorter tracks,simply by having more “easy” comparisons. Finally, thecalculation of Equation (6) can be expensive, takingO(n3)time using a direct implementation.To resolve these issues, we introduce a time window ofwseconds to both simplify the calculation of the metricand normalize its range. This is achieved by restrictingthe triples(q,i,j)in the summation such thatiandjbothlie within a window ofwseconds centered atq. Addingthis windowing property to equations (5, 6) yields the win-dowedT-measures:g(q;HE,HR,w)··=Xi,j2{x:|q\u0000x|w/2}i6=q,HR(q,i)\u0000HR(q,j)JHE(q,i)\u0000HE(q,j)KZq(w),(8)TR(HE,HR;w)··=1nXqg(q;HE,HR,w),(9)andZq(w)is again modiﬁed to count the terms in thesummation. This reduces computational complexity fromO(n3)toO(nw2). Each query frameqnow operates overa bounded number of comparisons, so the windowedT-measures are calibrated across tracks of different lengths.This property is useful when compiling score statistics overa test collection.3.5 Transitive reductionJust as Equation (5) can be dominated by long-range inter-actions in the absence of windowing, deep hierarchies canalso pose a problem. To see this, consider the sequenceHR(q,i)\u0000HR(q,j)\u0000HR(q,k). Since the summationin Equation (5) ranges over all precedence comparisons,andi2HR(q,j), the triple(q,i,k)is double-counted.Since segments grow in size at higher levels in the hierar-chy, over-counting can dominate the evaluation.To counteract this effect, the summation can be re-stricted to include only direct precedence relations. Thisis accomplished by comparing samples only from succes-sive levels in the hierarchy,i.e., replacing the partial rank-ing generated byqwith its transitive reduction. This botheliminates redundant comparisons and increasesg’s effec-tive range. We refer to the resulting metrics asreducedT-measures.4. SYNTHETIC EXAMPLESIn this section we discuss the behavior of theT-measuresby showing various synthetic examples, and comparingthem against other existing methods when possible. Foreach example in this section, we illustrate the behaviorof our proposed metric under different window timesw.This section is subdivided by the types of annotations un-der consideration.408 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015T-measures Hit RatewTRTPRP0.5 0.40 1.00 0.40 1.003 0.40 1.00 0.40 1.0015 0.39 0.5330 0.69 0.5010.80 0.50Figure 2:Flat vs. ﬂat boundaries (top),T-measures and bound-ary detection (hit-rate) scores (bottom).4.1 Flat vs. ﬂat annotationsWe ﬁrst compare two ﬂat boundary annotations to demon-strate how theT-measures behave compared to standardboundary detection. When both annotations are ﬂat, the re-ducedT-measures behave identically to the full measures,so we omit them from this section. The synthesized ﬂatboundaries are displayed on the top of Figure 2, and theyaim to capture a situation where an algorithm correctly de-tects a subset of the reference boundaries.The hit rate scores obtain a recall of 0.40 and a pre-cision of 1.0, since all estimated boundaries are also inthe reference, but only two out of ﬁve boundaries were re-trieved.3Whenwdoes not exceed the minimum segmentduration, theT-measures coincide exactly with the bound-ary detection metrics. For largerw,TPdecreases, whileTRincreases aswapproaches the track duration. The de-pendency onwis further explored in Section 5.1.To understand the relationship betweenTPandw, con-sider the example(q,i,j)=( 5,15,25). The estimationconsidersito be relevant forq(since they belong to thesegment[0,20]), andjto be irrelevant forq. Meanwhile,the reference considers bothiandjto be equally irrelevantforq, so this triple contributes 0 to the precision metric.Note that this comparison is counted only whenwis largeenough to span multiple segments.In general, sensitivity to long-range interactions in-creases withw. This illustrates how the window size de-pends on the duration and scale of structure that the practi-tioner wishes to capture.4.2 Flat vs. hierarchical annotationsHere we present four examples of ﬂat estimations against aﬁxed hierarchical reference, but note that the reverse com-parisons can be inferred by swappingTPandTR.4.2.1 Large-scale and under-segmentationFigure 3 illustrates a ﬂat estimation corresponding to thehighest layer of a hierarchical reference. We reportT-3The ﬁrst and last boundaries (0and60s) mark the beginning and endof the track, and since they are constant across all estimates, we suppressthem during the evaluation to avoid score inﬂation.\nReduced FullwTRTPTRTP0.5 0.00 1.00 0.40 1.003 0.00 1.00 0.40 1.0015 0.37 1.00 0.51 1.0030 0.70 1.00 0.82 1.0010.80 1.00 0.89 1.00Figure 3:Hierarchical reference vs. ﬂat (large-scale) estima-tion (top) andT-measures (bottom).Reduceduses the transi-tive reduction method of section 3.5, whileFulluses comparisonsacross all layers.\nReduced FullwTRTPTRTP0.5 0.0 1.00 0.20 1.003 0.0 1.00 0.20 1.0015 0.19 0.94 0.26 0.9430 0.37 0.71 0.44 0.7110.53 0.67 0.59 0.67Figure 4:Hierarchical reference vs. ﬂat under-segmentation(top) andT-measures (bottom).measures with and without the transitive reduction strategydescribed in Section 3.5. TheT-measures behave as ex-pected: the tree-precision scoreTPis always 100%, sincethe reference contains the estimation. We also observe thegeneral trend thatfullscores exceedreducedscores.For small time windows (w3), the full tree-recallscore is 40%, just as in the previous example. Thereducedrecall scores in this case are 0 because no frameqin theestimation has two framesi, jboth withinw3secondsthat merge within one layer of each-other in the reference.Figure 4 illustrates an example of under-segmentation:the estimation misses a high-level structural change at 20s.Again, smallwyieldsT-measures which coincide withstandard boundary detection metrics. Largerwincreasesthe tree-recall (and decreases precision) since only long-range interactions are well represented in the estimation.4.2.2 Small-scale and over-segmentationFigure 5 illustrates an example comparable to Figure 3,except that the estimation now corresponds to the bottomlayer of the reference annotation. Again, since the refer-ence contains the estimation, precision is maximal for allw. However, the reference provides strictly more informa-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 409Reduced FullwTRTPTRTP0.5 1.00 1.00 1.00 1.003 1.00 1.00 1.00 1.0015 0.63 1.00 0.76 1.0030 0.30 1.00 0.59 1.0010.20 1.00 0.55 1.00Figure 5:Hierarchical reference vs. ﬂat, small-scale estimation(top) andT-measures (bottom).\nReduced FullwTRTPTRTP0.5 1.00 0.56 1.0 0.563 0.98 0.56 0.98 0.5615 0.46 0.86 0.53 0.8630 0.22 0.92 0.40 0.9210.13 0.94 0.37 0.94Figure 6:Hierarchical reference vs. ﬂat over-segmentation (top)andT-measures (bottom).tion: namely, it encodes structure over the low-level seg-ments. TheT-measures quantify the missing informationin the estimation Whenwexceeds the smallest segmentduration (10s),TRdecreases. This information would beobscured by independent, layer-wise boundary evaluation.Similarly, Figure 6 illustrates anover-segmentationwhere the estimation predicts more boundaries than thedeepest layer of the reference. Again, theTRdecays whenthe window captures multiple short segments. Unlike theunder-segmented example in Figure 4, long-range interac-tions derived fromHEare mostly satisﬁed byHR, soTPincreases rather than decreases.4.3 Hierarchical vs. hierarchicalFigure 7 compares two different hierarchical segmenta-tions. The estimation contains an additional high-levellayer, but is otherwise identical to the reference. At smallw, bothT-measures agree perfectly, since the window isnot large enough to resolve differences. Aswincreases,TPdecreases as expected, since the estimation has foundan additional structural element not captured in the refer-ence. TheTRscores remain at 100% for allw.\nReduced FullwTRTPTRTP0.5 1.00 1.00 1.00 1.003 1.00 1.00 1.00 1.0015 1.00 0.98 1.00 0.9930 1.00 0.79 1.00 0.8911.00 0.62 1.00 0.79Figure 7:2-layer vs. 3-layer hierarchical boundaries (top) andT-measures scores (bottom).\nReduced FullwTRTPTRTP0.5 0.76 0.77 0.81 0.793 0.95 0.95 0.96 0.9315 0.75 0.75 0.80 0.8430 0.62 0.83 0.71 0.8910.57 0.96 0.68 0.98Figure 8:Hierarchical annotations for SALAMI track #636 fromthe two different human annotators. Top: annotations; bottom:T-measures scores.5. LARGE-SCALE EVALUATIONIn this section, we apply theT-measures to quantify inter-annotator agreement in the SALAMI corpus, and evaluatethe hierarchical predictions of the agglomerative clusteringmethod (OLDA) of McFee and Ellis [4].5.1 Human annotator agreementFigure 8 illustrates hierarchical annotations obtained fromtwo human annotators on one track in the SALAMIdataset. While the two annotators tend to agree at the smallscale, they differ at the large scale. This is reﬂected in theT-measures: at largew, the recall skews low because thereference’s large-scale annotations are coarser than thoseof the estimation.To further investigate inter-annotator agreement, wecomputedT-measure scores between hierarchical refer-ence annotations for the 410 tracks in the SALAMI datasetwhere two annotations are available and both mark the startand end times of the song equally at both levels. To sim-plify exposition, we summarize agreement byTF. Figure 9410 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 9:TFscores between human annotators for SALAMItracks over a range of window sizesw.illustrates the distribution of per-trackTFscores as a func-tion ofw. We observe that the score distribution is rela-tively stable forw\u000015.4The example in Figure 8 is gen-erally representative of inter-annotator agreement, achiev-ingTF=0.75atw= 15. The out-lying low scores tendto be examples where one annotator ignored structure an-notated by the other:e.g., in track #68, one annotator onlymarkedsilenceboundaries.This analysis quantitatively substantiates prior obser-vations that humans do not perfectly agree upon struc-tural annotations [9], and suggests an accuracy ceiling near70% for hierarchical annotation. Similarly, it suggeststhatw= 15provides a reasonable default value for theSALAMI dataset. This setting is large enough to capturemultiple small-scale segments: in the tracks considered forthis evaluation, the median small-scale segment durationwas 6.66s, with a 95th percentile of 15.69s.5.2 Annotator vs. algorithmFinally, we evaluated the quality of hierarchical segmenta-tions produced by OLDA [4].5Figure 10 illustrates oneexample output of OLDA and the resultingT-measures.The reference provides two levels of segmentation (largeand small), while the estimation produces several layerswith generally large segments. For sufﬁciently largew, theestimation achieves high recall and low precision. This be-havior is typical of the OLDA method, which constructshierarchies in a bottom-up fashion by agglomerative clus-tering, adding only a single boundary at each layer. Dueto the depth of the estimated boundaries, thefullscores areinﬂated compared to thereducedscores.Figure 11 displays theTFscore distribution forOLDA, measured against annotator1on 726 tracks fromSALAMI. These results reveal a gap of around 30% be-tween inter-annotator agreement (Figure 9) and the perfor-mance of OLDA. This suggests that there is substantialroom for improvement in hierarchical boundary estimationalgorithms.4The analogous plots forTPandTRare omitted for brevity, but illus-trate the same trend.5To the authors’ knowledge, this is the only published method for hi-erarchical boundary detection.\nReduced FullwTRTPTRTP0.5 0.14 1.00 0.28 0.553 0.20 1.00 0.34 0.7215 0.62 0.56 0.66 0.7030 0.76 0.53 0.80 0.5810.90 0.16 0.93 0.42Figure 10:Hierarchical reference annotation vs. OLDA onSALAMI track #636. (top) andT-measures (bottom).\nFigure 11:TFscores between OLDA and human reference an-notations on the SALAMI dataset.6. DISCUSSION AND CONCLUSIONSThe implementation ofT-measures depends upon two crit-ical parameters: the time windoww, and whether to use thereducedorfullmetrics. While the setting ofwultimatelydepends upon the practitioner’s preference and character-istics of the dataset, the results on SALAMI suggest thatw= 15provides a reasonable balance between captur-ing high-level structure and resilience to long-range inter-actions. As illustrated in section 4.2.1, whenwis largeenough to capture multiple short segments, the transitivereduction approach can also be used to enhance the rangeof the metrics while eliminating redundant comparisons.In this paper, we focused only on the problem of eval-uating estimated boundaries. In future work, we plan toextend general ideas behindT-measures to other structuralannotation problems, such as segment label agreement.7. ACKNOWLEDGEMENTSBM acknowledges support from the Moore-Sloan DataScience Environment at NYU.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4118. REFERENCES[1]Tom Collins. Discovery of Repeated Themes & Sec-tions, 2013.[2]Fred Lerdahl and Ray Jackendoff.A Generative Theoryof Tonal Music. MIT Press, 1983.[3]Fred Lerdahl and Ray Jackendoff. An Overview of Hi-erarchical Structure in Music.Music Perception: AnInterdisciplinary Journal, 1(2):229–252, 1983.[4]Brian McFee and Daniel P. W. Ellis. Learning to Seg-ment Songs with Ordinal Linear Discriminant Analy-sis. InProc. of the 39th IEEE International Conferenceon Acoustics Speech and Signal Processing, Florence,Italy, 2014.[5]Oriol Nieto and Morwaread M. Farbood. IdentifyingPolyphonic Patterns From Audio Recordings UsingMusic Segmentation Techniques. InProc. of the 15thInternational Society for Music Information RetrievalConference, pages 411–416, Taipei, Taiwan, 2014.[6]Jouni Paulus, Meinard M¨uller, and Anssi Klapuri.Audio-Based Music Structure Analysis. InProc of the11th International Society of Music Information Re-trieval, pages 625–636, Utrecht, Netherlands, 2010.[7]Geoffroy Peeters and Emmanuel Deruty. Is MusicStructure Annotation Multi-Dimensional? A Proposalfor Robust Local Music Annotation . InProc. of the3rd International Worskhop on Learning Semantics ofAudio Signals, pages 75–90, Graz, Austria, 2009.[8]Jordan B. Smith, J. Ashley Burgoyne, Ichiro Fujinaga,David De Roure, and J. Stephen Downie. Design andCreation of a Large-Scale Database of Structural An-notations. InProc. of the 12th International Society ofMusic Information Retrieval, pages 555–560, Miami,FL, USA, 2011.[9]Jordan B. L. Smith and Elaine Chew. A Meta-Analysisof the MIREX Structure Segmentation Task. InProc.of the 14th International Society for Music InformationRetrieval Conference, Curitiba, Brazil, 2013.[10]Douglas Turnbull, Gert RG Lanckriet, Elias Pampalk,and Masataka Goto. A supervised approach for detect-ing boundaries in music using difference features andboosting. InISMIR, pages 51–54, 2007.[11]GeraintA. Wiggins and Jamie Forth. Idyot: A com-putational theory of creativity as everyday reasoningfrom learned information. In Tarek R. Besold, MarcoSchorlemmer, and Alan Smaill, editors,ComputationalCreativity Research: Towards Creative Machines, vol-ume 7 ofAtlantis Thinking Machines, pages 127–148.Atlantis Press, 2015.412 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Put the Concert Attendee in the Spotlight. A User-Centered Design and Development Approach for Classical Concert Applications.",
        "author": [
            "Mark S. Melenhorst",
            "Cynthia C. S. Liem"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416820",
        "url": "https://doi.org/10.5281/zenodo.1416820",
        "ee": "https://zenodo.org/records/1416820/files/MelenhorstL15.pdf",
        "abstract": "As the importance of real-life use cases in the music in- formation retrieval (MIR) field is increasing, so does the importance of understanding user needs. The develop- ment of innovative real-life applications that draw on MIR technology requires a user-centered design and de- velopment approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applica- tions to enrich classical symphonic concerts. A user- driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications.",
        "zenodo_id": 1416820,
        "dblp_key": "conf/ismir/MelenhorstL15",
        "keywords": [
            "real-life use cases",
            "music information retrieval",
            "user-centered design",
            "innovative real-life applications",
            "user needs",
            "MIR technology",
            "user-driven approach",
            "orchestras",
            "concert experience",
            "informed user requirements"
        ],
        "content": "PUT THE CONCERT ATTENDEE IN THE SPOTLIGHT.  A USER-CENTERED DESIGN AND DEVELOPMENT APPROACH FOR CLASSICAL CONCERT APPLICATIONS  Mark S. Melenhorst Cynthia C. S. Liem Delft University of Technology Multimedia Computing Group m.s.melenhorst@tudelft.nl Delft University of Technology Multimedia Computing Group c.c.s.liem@tudelft.nl ABSTRACT As the importance of real-life use cases in the music in-formation retrieval (MIR) field is increasing, so does the importance of understanding user needs. The develop-ment of innovative real-life applications that draw on MIR technology requires a user-centered design and de-velopment approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applica-tions to enrich classical symphonic concerts. A user-driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications.   1. INTRODUCTION While the Music Information Retrieval (MIR) field his-torically has mostly been algorithmically oriented, in re-cent years the community increasingly gained interest in the use and consequences of MIR technology for real-life applications rooted in user needs. Cases for a ‘mentality shift’ into this direction have been made in [22], [4], [20], [6], [15], and the ISMIR community includes a limited amount of active work on real-world user requirements (e.g. [3], [10], [12], [13]). However, it still seems hard to connect real-world user needs and requirements to con-crete technological system and algorithmic advances [14]. When the needs and characteristics of the users are left unaddressed in technological applications, the end user remains an abstract entity, which becomes manifest in the absence of a requirements analysis and untargeted partic-ipant recruitment for formative or summative evaluations of systems involving MIR technology.  In this paper, we focus on technological application opportunities targeted at (Western) classical symphonic concert attendance. Orchestras are increasingly worried about audience sustainability. A Flemish study confirmed the common belief that concert attendees are typically highly educated and over the age of 45 [19]. Concerns about an aging audience motivate orchestras to find crea-tive ways to involve new audiences [11], not only with new attractive concert formats, but also with technologi-cal innovations that allow users interested in classical music to become engaged in an easy way. Examples in-clude online concert broadcasting (e.g. Digital Concert Hall1), smartphone-supported live program notes (e.g.  LiveNote2), and enriched tablet e-magazines with second screen content (e.g. RCO Editions3). As argued in [9], MIR technology has the potential of enriching the cus-tomer experience for the users of these applications. Once users become more engaged, they are more likely to buy concert tickets, which ultimately would lead to a more diverse classical concert audience. However, there is a trade-off between the need for in-novations that attract new audiences and the risk of avoiding the alienation of the traditional audience. Since technology is not naturally associated with the classical concert experience and the allegedly conservative audi-ence might be reluctant towards the use of technology in and around the concert hall, the importance of user ac-ceptance cannot be underestimated. Therefore, an innova-tion approach is needed that combines a technology push from the MIR community with a strong technology pull from user audiences. User-centered design is an im-portant pillar of this approach, addressing user needs from existing and new audiences, and evaluating solu-tions with end-users in every stage of the design process.  In this paper, we therefore demonstrate how a user-centered design approach can be used to identify oppor-tunities for the use of (MIR) technology in classical con-cert applications that are grounded in the needs of differ-ent audience segments. More specifically, our study seeks to answer the following research questions: 1. What are the motivators and obstacles for different audience segments to (not) attend classical concerts? 2. How can the needs of the audience segments be translated into opportunities for the enrichment of the classical concert experience by means of technology?  Consistent with [9], we argue that the classical concert experience not only involves the concert itself, but also                                                            1 https://www.digitalconcerthall.com/en/home 2 https://www.philorch. org/introducing-livenote%E2%84%A2-nights#/ 3 www. concertgebouworkest.nl/en/rco-editions  © Mark S. Melenhorst, Cynthia C. S. Liem. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Melenhorst, M.S. & Liem, C.C.S. “Put the Concert Attendee in the Spotlight. A User-Centered Design and Development Approach for Classical Concert Applications”, 16th International Society for Music Information Retrieval Conference, 2015. \n800   the preparation beforehand, and reflection and re-experience afterwards. The envisioned applications are intended to appeal to new audiences by yielding a strong-er hedonic response on four sources of stimulation (emo-tions, senses, imagination, and intellect) [18], for both current and new audiences. After discussing related work on the needs of different classical music audience segments, we outline the user-centered design approach that was taken. Subsequently, we present the results with respect to the first steps in our approach: the user requirements elicitation process that was preceded by the construction of user stories [16].  User requirements are derived from focus groups that ad-dress motivators and obstacles for classical concert at-tendance and that collect feedback on a set of user stories, containing ideas for the use of (MIR) technology to en-rich the classical concert experience before, during and after the concert.  2. AUDIENCE SEGMENTS While the classical concert audience sometimes perceives itself as homogeneous, in fact this is not the case [17]. To develop applications that support the needs of the classi-cal concert audience, it is therefore important to distin-guish different audience segments. Roose [19] suggests a tripartite audience segmentation. First, passers-by are in-cidental – typically younger - visitors that are not moti-vated by the concert performance itself, but rather by ex-trinsic motivations such as an evening out with friends. Participants comprise the core of the audience, consisting of well-informed, well-interested people that generally are not formally trained in music. In contrast, the inner circle consists of audience members that are professional-ly involved in the arts who frequently attend concerts and form a peer group for the performers. A large-scale sur-vey conducted by [19] demonstrated that the average age for all participants was between almost 55 and 57. The educational level for all segments is higher than for the general population (above bachelor level or higher). Inner circle members are better educated than participants, who in turn are better educated than passers-by. This tripartite segmentation is used as the basis for the audience seg-mentation that will be used in this paper as the basis for application development.   3. MOTIVATORS AND OBSTACLES The development of concert experience enrichment ap-plications requires a solid understanding of why people enjoy classical concerts (motivators) and what obstacles they experience towards concert attendance. This section discusses existing literature on these, with focus on North-American and European audiences.1 In a Flemish study, Roose [19] distinguishes between extrinsic and intrinsic motivations for concert attendance                                                            1 To the best of our knowledge, no cross-cultural comparisons involving audiences with other cultural backgrounds have been made; however, also in this paper, we will focus on Western audience. and five classes of aesthetic dispositions. Even though the definitions of and relationships between motivations and dispositions are not precisely defined, they can shed light on why classical concerts appeal to different audiences. Intrinsic and extrinsic motivators. Intrinsic motiva-tions include the performers (e.g. a soloist or an orches-tra), the programming, or a concert being part of a sea-sonal ticket. Extrinsic motivators are social motivators (advice from others, invitation from others, or spending time with friends), or attention in the media. Radbourne et al. [18] further elaborate on the social part of the expe-rience, referred to as ‘collective engagement’. They argue that this an important determinant of the audience experi-ence. Collective engagement can take three forms: be-tween the audience and the performers, among audience members, and between attendees and non-attendees. So-cial interactions stimulate discussion about the music [18], which would facilitate learning. This in turn would improve the audience experience. Aesthetic dispositions. [19] distinguishes five aesthetic dispositions that influence one’s inclination to attend classical concerts: emotional, escapist (e.g. change of set-ting to escape everyday concerns), familiarity (e.g. music one is familiar with), normative (e.g criticize society), and innovative (e.g. experiments with the tonal system, complex rhythmic patterns, etc., with the purpose of en-couraging the listener to discover new meanings in the music). The innovative disposition primarily is particular-ly present in well-educated, experienced audiences.  In comparison to motivators for classical concert at-tendance, relatively little is known about the obstacles preventing people from attending classical concerts. [11] and [4] invited participants to attend a classical concert for the first time. Responses of first-time classical concert attendees can shed light on the preconceptions with which they enter the concert hall, and the difficulties they face. From these studies three classes of obstacles can be derived: limited sense of belonging, knowledge about classical music, and richness of the experience.  Limited sense of belonging. Classical concert novices might feel overwhelmed when they enter a concert hall for the first time due to the social conventions, the eti-quette, and the social interactions that occur. [4] and [11] have shown that first-time attendees have trouble with adjusting to these. [4] reported a lack of a sense of be-longing as a result of age differences and differences in clothing. The limited sense of belonging because of social distance and the unknown social conventions is amplified by a limited understanding of the music. Additionally, [11] found that the lack of interaction between audience members and between the audience and performers nega-tively impacted the experience of first-time concert at-tendees, corroborating the importance of collective en-gagement that was suggested by [18].  Knowledge about classical music. Respondents in [4] and [11] articulated the importance of acquiring a certain level of knowledge to enjoy the concerts more. Knowledge about classical music is also related to emo-tions. While the emotional response is an important de-terminant of the audience experience [18], these emotions Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 801   are more likely to be evoked when the attendee has a cer-tain level of knowledge. Currently available information sources prove to be ill-adjusted to non-regular audiences, imposing an obstacle to the learning process. Respond-ents in [4] complained about the program notes, which were ill-adjusted to first-time attendees in terms of vo-cabulary and required general background knowledge. Richness of the experience. Classical concerts are ra-ther different for first-time attendees compared to popular music concerts. Kolb [11] indicated that their respondents were able to pay attention during about 10 minutes per piece. They also felt that there were little opportunities for interaction between the audience and the performers, while the setting did not allow for interaction between audience members. Participants in [11] indicated that the lack of visual stimuli on the stage caused the time to go slow. Participants in both [11] and [4] reported a lack of visual stimuli, caused by both the stage set up, and the way the performers dress (referred to as ‘funeral attire’). To the best of our knowledge, no prior academic stud-ies exist which comprehensively address both motivators and obstacles on classical concert attendance for multiple audience segments. In the following sections, we will de-scribe how we investigated this, with the ultimate goal of developing innovative classical concert applications.  4. DESIGN APPROACH  The development of applications that are well-aligned with the needs and preferences of the users requires a multi-stakeholder approach. Orchestras characterize their target audiences through marketing research. New appli-cations need to be aligned with their business model and their marketing strategy. Existing and new audience members need to provide input on their needs and expec-tations. Throughout the development cycle, they provide feedback on prototypes of increasing fidelity. Technology providers (businesses and research institutes) develop the actual applications, based on academic or business ambi-tions, balancing technology-push with technology-pull. In Figure 1, a high-level user-centered design and de-velopment process for classical concert applications is displayed, involving the aforementioned stakeholders.    Figure 1 User-centered development process  Audience segmentation. In the work described in this pa-per, applications need to be tested for all parts of the con-cert experience: before, during, and after a concert. Based on (unpublished) marketing research from a Dutch or-chestra, three audience segments were targeted: outsiders, casual consumers, and heavy consumers. In comparison to [19], outsiders (OS) are comparable to the passers-by or the ‘culturally-aware non-attenders’. The casual con-sumers (CC) are in between the participants and the pass-ers-by. While they have serious interest in attending clas-sical music, compared to participants, their concert at-tendance frequency is lower, as is their average age, mu-sical knowledge level, and less natural engagement with classical music. Heavy consumers (HC) comprise both the inner circle and the participants.  User stories. User stories describe specific functionali-ties, written from the perspective of an end-user. They function as data collection probes [1] – artefacts “contain-ing open-ended, provocative and oblique tasks to support early participant engagement responses with the design process” (p. 1077). In our work, eight user stories were constructed that each describe a set of features for smartphone or tablet applications, expected to enrich the concert experience before, during and after event attend-ance. The user stories – described in [16] – address the needs of all three relevant target audiences (OS, CC, HC), while at the same time, they build on opportunities from the technological and MIR domain. The insights gained from feedback on the user stories shape the user requirements in a way we will describe in the remainder of this paper. In their turn, the requirements form the basis for the iterative development and evalua-tion of these apps.  5. USER REQUIREMENTS  ELICITATION METHODOLOGY 5.1 General approach For user requirements elicitation, five focus groups were held: two in the Netherlands (one for HC consumers, one for CC consumers) and three in Austria (targeting HC, CC and OS consumes, respectively). After signing in-formed consent forms a project introduction was given. Participants then introduced themselves, focusing on their music preferences. Subsequently, motivators and obsta-cles for attending classical concerts were discussed, in-troduced by the questions “What makes a classical con-cert such a unique experience for you?” and “What pre-vents you from going to classical concerts more often”?, respectively. Afterwards, participants received a booklet with the user stories, which the participants read, annotat-ed on sticky notes, and discussed. The focus group was concluded with a questionnaire, addressing technology use, music and concert behavior, and demographics.  5.2 Participants In the Netherlands, participants were recruited via a mail-ing of the Royal Dutch Concertgebouw Orchestra’s cus-tomer association, whose members fitted our CC and HC criteria. In Austria, a recruitment e-mail was sent to all students of a university. A sign-up form with questions about classical music involvement was used to divide participants over the three audience segments. Table 1 reports participant characteristics for all focus groups.  5.3 Data analysis After transcription, the data were analyzed using thematic analysis, a form of pattern recognition within the data, where emerging themes become analysis categories [8]. Data were analyzed with the purpose of identifying moti-\n802 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   vators and obstacles for concert attendance. Feedback on the user stories was analyzed with the purpose of deriving opportunities for applications to enrich the classical con-cert experience. Note that even though the differences between the Netherlands and Austria are of interest to the goal of our study, other differences between the samples (e.g. age, occupational status, income, experience) pre-vent us from doing a valid cross-cultural comparison.   Measure The Netherlands CC HC N 6 13 Age 27.7 (.8) 54.7 (15.2) Concert attendance   > once/ month   once/month   once/quarter   once/year    1  4 1  5  8  Measure Austria  OS CC HC N 7 10 4 Age 29.4 (8.2) 27.8 (11.3) 27.5 (3.8) Concert attendance   > once/month   once a month   once/quarter   once every year      3 4  1  3 6   1 2 1 Table 1. Focus group participant characteristics 6. MOTIVATORS AND OBSTACLES In this section, we present the results of devising general motivators and obstacles from the user requirements elici-tation process. Transcription, analysis, and coding of the results has led to the definition of 17 motivators and 16 obstacles, of which we will discuss the most important ones, backed with statements from the discussions. Statement quotes use the following abbreviations: #n=participant ID; OS=outsiders, CC=casual consumers, HC=heavy consumers; NL=the Netherlands, AT=Austria. Statements from sticky notes do not have a participant ID, as they were collected all at once on a flip-over sheet. 6.1 Intrinsic motivators Concert experience and musical quality. Across target groups, participants appreciate the uniqueness of the con-cert as a one-time event during which high-quality music is played. Participants clearly see the added value of a live concert in comparison to a recording. They felt that this was not only applicable to classical music, but also to concerts in other genres (CC-NL, OS/CC/HC-AT). The discussions revealed that in classical concerts, at-tendees are motivated by the interaction between the con-ductor and the orchestra, between the audience and the performers, and by the orchestra members themselves. Tension and suspense fascinated the participants: “You can see tension with musicians, feeling is transmitted through the way they look and move. You can also see this from the conductor. (…) you can feel the emotion, not just audio. You don’t get this in a recording.” (#9-CC-AT). From the OS-AT group it became apparent that this fascination not only applies to classical concerts, but also to other genres.  Escapism. For casual consumers and heavy consumers in both AT and NL, escapism – an aesthetic disposition mentioned by [19] – is an important motivator for classi-cal concert attendance. Participants indicated that sub-merging themselves in an environment in which they cannot do anything else but focus on the music allows them to disconnect from their daily concerns (“At a clas-sical concert I forget all my problems, I am not stressed, #6-CC-AT). In that sense, a classical concert was com-pared to a church service: “A moment to be quiet” (#2-CC-NL). Another participant emphasized the difference to listening to classical music at home: “It’s an obligation to listen to a concert in peace and quiet. I don’t succeed in doing that at home” (#5-CC-NL).  Need for cognition. People differ in the extent to which they desire to engage in cognitively effortful activities [2]. In the CC-NL and HC-NL groups, opportunities for cognitive engagement and learning motivated several par-ticipants to attend classical concerts. Curiosity about the musicians, the piece, and the performers was expressed (referred to as ‘hunger for information’; #5-CC-NL). However, this need for cognition and learning was not expressed by participants in the outsider group. One participant in the CC-AT group connected the es-capist motivator and the resulting focus on the music to an increased level of processing: “You start thinking about things. You discover new pieces”. Another partici-pant noticed a difference in attitude with respect to learn-ing: “Awareness and qualitative enjoyment of a piece is more important than entering the hall snobbishly, pre-tending that you know everything” (#5-CC-NL).   6.2 Extrinsic motivators Social influences. Participants reported that having peers or family members with the same interests, helps to get motivated for classical concerts. One participant com-mented: “I notice that it works well when you know a couple of people in the orchestra. It makes things more personal. And lowers the barrier to join in” (#6-CC-NL).  Furthermore, in particular the younger CC-NL group pre-ferred a concert experience to encompass more than just the performance itself (e.g. appreciating “A drink at a bar with young people afterwards”; #5-CC-NL). 6.3 Intrinsic obstacles Importance of classical music. The discussions revealed substantial differences between audience segments con-cerning the role classical music plays in people’s lives. Consistent with findings from [19] and [11], we found that participants are not exclusively focused on classical music, but are ‘culturally mobile’ [7]. One participant ex-plained: “You just don’t visit 10 classical concerts. There is more than classical music. It’s interesting if something comes up. And that’s what our generation likes” (#3-CC-NL). Participants also mentioned that their interest in classical concerts is mood-dependent.  Preparation and risk. Substantial differences were found with respect to the effort audience segments were Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 803   willing to invest in concert preparations. While the CC-NL group requested easily consumable information, the HC-NL group was motivated to invest more time (“I can spend hours on YouTube watching videos about what a singer has done before”; HC-NL), and considered prepa-ration as part of the pre-concert anticipation. The discus-sion in the OS-AT group revealed that the risk of buying an expensive ticket can be too high (“It’s expensive for just something you don’t know”, #4-HC-AT). To reduce this risk, participants felt they needed to invest time in finding information about the performers and the piece. This factor was more important in AT than NL, probably because due to the AT participants being students.  Concert setting and conventions. Consistent with [11] participants in the younger groups (OS/CC-AT; CC-NL) felt disconnected from other concert attendees, primarily as a function of the age difference “What stops me? That there are very, very, very many seniors in the hall. Some-times that disturbs me” (#2/3/5, CC, NL). Participants also mentioned the complexity of concert conventions for novices (“a classical concert can be intimidating. Un-known. They don’t know the rules”, #4+6-CC-NL).  Richness of the experience. Results suggested that the perceived richness of the experience was dependent on both the age group and the level of engagement with clas-sical music. Younger groups (CC-NL, OS-AT, HC-AT) noted that “The experience is richer in other genres, for classical it’s more about the music itself” (#3-HC-AT). One participant (#3) in the OS-AT group commented on the lack of surprises, knowing already what the playlist is. Interestingly, the HC-NL group considered the surprise element to be a motivator (“At every performance you become surprised by something…you hear things you won’t hear elsewhere”, #10-HC-NL). Outsiders (OS-AT) and casual consumers (CC-NL, CC-AT) commented on the lack of opportunities for physical expression. “I miss standing up. Being engrossed in music you also experi-ence physically.” (#2-CC-NL). This radically differs from their experience with non-classical concerts.  6.4 Extrinsic obstacles Social influences. Younger participants – most present in the OS and CC groups – indicated that their peers were less interested in classical music, causing a lack of com-pany. This prevents the respondents from going more of-ten, both in Austria and in the Netherlands. (#3-CC-NL, “You have to know people that also like classical music”; #9-CC-AT, “It’s easier to find friends who want to join me to a rock concert”. Other extrinsic obstacles included ticket costs, and the long time attendees needed to plan ahead when they want to attend a concert.  7. APPLICATION AND MIR OPPORTUNITIES In this section, we aggregate user story feedback under several clustered themes. We discuss relevant feedback per theme, formulate exemplary requirements for tech-nology-supported concert applications, and discuss inte-gration opportunities for MIR technology. 7.1 Support with preparation The discussion on motivators and obstacles has high-lighted the importance of concert preparation across ex-pertise levels. User stories facilitating concert preparation were well-received. The CC groups appreciated the con-venience of having information in one place (“We are part of a generation that is used to large amounts of in-formation, but also to get it presented in an easy way”; #2-CC-NL). Both the HC-NL and the CC-NL group ap-preciated the added value of the information, particularly historic context, for preparation before the concert, but also for better understanding during and after the concert. When working towards concrete applications, this leads to the requirement that the applications should offer information about the composer, the musicians, the piece, and its historical context. MIR technology can support this by developing cross-modal and cross-performance synchronization methods, and techniques for analyzing and combining hybrid music information resources. 7.2 Need for support to understand the music. While participants wanted to avoid overemphasizing cognitive aspects, across groups a need was expressed for understanding the music, learning about what parts one should pay attention to, and discovering unexpected new elements. Participants recognized the difficulty for novice listeners to understand and then enjoy the music “because music is hard to grasp/decipher” (CC-NL). They ex-pressed interest in the structure of the music, the compo-ser’s intention, the conductor’s interpretation, and the discovery of style differences in comparison to recor-dings. User stories that provide this support were as-sessed positively, in terms of their educational potential and the potential to lower the barriers for outsiders to start attending classical concerts. In terms of application requirements, two main re-quirements can be extracted: the applications should offer representations of the musical structure and the user's attention should be attracted to parts of the music which wouldn't have been noticed otherwise. These interests confirm the relevance of MIR work on automated music description, performance analysis, and visualization. 7.3 Audience expansion by sharing relevant moments The user stories included application features allowing users to annotate particularly interesting moments, to re-view the notes and related audiovisual content after the concert, and to share notes and their corresponding frag-ments through social media. Participants felt that the sharing of small fragments could function as an “'opener' for people unacquainted with this type of music” (HC-NL). By sharing the experience, users can motivate their friends to attend a classical concert (“if you share this, you can tag someone along”; CC-NL).  While the opportunity to review and share particularly interesting moments after the concert was generally eval-uated positively, taking notes during a concert was per-ceived as distracting. Participants were concerned with the impact on the concert experience (“It’s not a lecture”; HC-NL). They felt that the cognitive effort of taking notes “destroys magic of non-repeatable live experience”. 804 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   A one-button marker was frequently mentioned as a light-weight solution: (“Annotations for a specific moments, ok, but not with text, only with a marker. Which means: I want to hear this again”; CC-NL). This leads to several application requirements: applica-tions should enable the user to set a marker at a particu-lar moment during the concert by pressing a single but-ton and enable the user to listen to marked fragments af-ter a concert. While the concepts of marking, annotating and sharing are somewhat related to work on social media and autotagging in the MIR community, many open ques-tions are raised regarding temporal aspects of ‘interesting moments’, and especially the type of information to be displayed and marked. 7.4 Personalization and control The results revealed substantial differences between and within audience segments, concerning their expectations of the concert experience, attitude towards technology, and level of classical music experience. Considering these differences, participants expressed interest in per-sonalized information. Here, ‘personalization’ had two meanings: first, participants preferred to only receive in-formation that is relevant to them, notwithstanding their need for a certain level of surprise in the information of-fered. Second, they wanted to switch on and off different layers of information to personalize their user experience.  These notions lead to two corresponding requirements formulations for applications: the user must be able to receive personalized content by filling out a limited num-ber of questions and the user must have control over the layers of information that are displayed. Regarding the first point, an explicit questionnaire is suggested, as this provides both most transparency to a user, and avoids da-ta sparsity issues. Still, it is useful to assess the potential of automated MIR profiling and recommendation tech-niques, in terms of usefulness and feasibility.  7.5 Caveat: interference with the concert experience. One important caveat was brought up in every focus group: applications should refrain from interfering with the live concert experience. Participants wanted to enjoy the music without engaging in cognitive activities. This is in line with the escapism disposition from [19], which also emerged from the focus groups as a motivator. In terms of [18], an overemphasis on cognitive stimulation potentially prevents sensory or emotional stimuli from contributing to the concert experience. (“How can you combine a tablet with emotions?” HC-NL).  When tablets were discussed as a possible medium in concert halls, participants were worried about distraction by messages about everyday affairs (“You might receive a work-related e-mail that makes you tense up”, CC-NL).  Second, tablets might also distract other audience mem-bers due to the light emittance of tablets in an otherwise (semi-)dark concert hall. The strongest rejection of these ideas came from the participants in the HC-NL group who wanted to keep the concert experience as it is. This leads to a very clear and strong requirement that applications must not distract the user or other concert-goers while listening to a live concert. In terms of MIR technology, this poses open challenges with respect to user experience design of in-concert applications. 8. CONCLUSIONS In this paper we discussed a user-centered design ap-proach to identify opportunities for technological enrich-ment of the classical concert experience. Departing from a tripartite audience segmentation and common motiva-tors and obstacles for concert attendance from literature, five focus groups were conducted in which these motiva-tors and obstacles were further refined and connected to application and MIR technology inclusion opportunities.  A trade-off was found between offering cognitive sup-port to users and allowing users to enjoy the concert without disturbance. Light emittance, required attention by the user, and the impact on other concertgoers are the most important concerns that were voiced by the partici-pants. In contrast, stronger support was found for ideas that improve the understanding of the music. Participants also supported ideas to relive marked interesting mo-ments of the concert, although the marking effort during the concert should be limited to pressing a single button. The reported results support our plea for a detailed as-sessment of end-user needs and user characteristics. Our results reveal differences between individual participants with respect to their aesthetic dispositions [19], cultural mobility [7], and also the type of stimulation participants expect from a concert [18]. Furthermore, consistent with [21], the results suggest that age affects user acceptance of technology in the concert hall – with older participant being more reluctant towards changes of the concert ex-perience. In sum, the results emphasize that what is a mo-tivator for one attendee, can be an obstacle for another.  Classical concert applications for such a heterogeneous audience require a personalized user experience, with many opportunities to integrate advances from the MIR research agenda. At the same time, the success of result-ing applications will depend on their connection to end-user needs and expectations. The chosen presentation and contextualization of information is a critical factor in this, which is not yet thoroughly examined with true end-user involvement in MIR. Follow-up steps in our approach are to iteratively de-sign and evaluate application wireframes for prototypical applications, while simultaneously developing the backend (MIR) technology. Results of consecutive evalu-ations will then refine and extend the requirements and opportunities presented in this paper.  9. ACKNOWLEDGMENT This study is part of the PHENICX project, which is funded by the European Union Seventh Framework Pro-gramme FP7 / 2007- 2013 under grant agreement no. 601166. Additionally, the authors want to express their gratitude towards Marcel van Tilburg (Royal Concertge-bouw Orchestra) and Marko Tkalčič (Johannes Kepler University) who have offered substantial support with the practical organization of the focus groups and the re-cruitment of participants.   Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 805   10. REFERENCES [1] Boehner, K., Vertesi, J., Sengers, P., and Dourish, P.: ‘How HCI Interprets the Probes’. Proc. CHI, San Jo-se, CA, April 28-May 3, 2007 [2] Cacioppo, J.T., and Petty, R.E.: ‘The need for cogni-tion’, Journal of Personality and Social Psychology, 1982, 42, (1), pp. 116–131 [3] Cunningham, S.J., Downie, J.S., and Bainbridge, D.: ‘“The pain, the pain”: modelling music information behavior and the songs we hate’, in: 6th International Conference on Music Information Retrieval (ISMIR 2005). London, UK; 2005, pp. 474-477. [4] Dobson, M.C., and Pitts, S.E.: ‘Classical Cult or Learning Community? Exploring New Audience Members’ Social and Musical Responses to First-time Concert Attendance’, Ethnomusicology Forum, 2011, 20, (3), pp. 353-383 [5] Downie, J.S., Byrd, D., and Crawford, T.: ‘Ten Years of ISMIR: Reflections on Challenges and Opportunities’, in:  10th International Society for Music Information Retrieval Conference (ISMIR 2009). Kobe, Japan; 2009, pp. 13-18. [6] Downie, J.S., Hu, X., Lee, J.H., Choi, K., Cunningham, S.J., and Hao, Y.: ‘Ten Years of MIREX: Reflections, Challenges and Opportunities’, in: 15th International Society for Music Information Retrieval Conference (ISMIR 2014). Taipei, Taiwan; 2014, pp. 657-662. [7] Emmison, M.: ‘Social Class and Cultural Mobility: Reconfiguring the Cultural Omnivore Thesis’, Jour-nal of Sociology, 2003, 39, (3), pp. 211-230 [8] Fereday, J., and Muir-Cochrane, E.: ‘Demonstrating Rigor Using Thematic Analysis: A Hybrid Approach of Inductive and Deductive Coding and Theme De-velopment’, International Journal of Qualitative Methods, 2006, 5, (1), pp. 80-92 [9] Gómez, E., Grachten, M., Hanjalic, A., Janer, J., Jordà, S., Julià, C.F., Liem, C.C.S., Martorell, A., Schedl, M., and Widmer, G.: ‘PHENICX: Perfor-mances as Highly Enriched aNd Interactive Concert Experiences’, in: SMC Sound and Music Computing Conference. Stockholm, Sweden; 2013. [10] Inskip, C., MacFarlane, A., and Rafferty, P.: ‘Upbeat and quirky, with a bit of a build: interpretive reper-toires in creative music search’, in: 11th Internation-al Society for Music Information Retrieval Confer-ence (ISMIR 2010). Utrecht, The Netherlands; 2010, pp. 655-660. [11] Kolb, B.: ‘You Call This Fun? Reactions of Young First-time Attendees to a Classical Concert’, Music & Entertainment Industry Educators Association (MEIEA) Journal, 2000, 1, (1), pp. 13-28  [12] Laplante, A.: ‘Users’ relevance criteria in music retrieval in everyday life: an exploratory study’, in: 11th International Society for Music Information Retrieval Conference (ISMIR 2010). Utrecht, The Netherlands; 2010, pp. 601-606. [13] Lee, J.H.: ‘Analysis of user needs and information features in natural language queries seeking music information’, Journal of the Association for Information Science and Technology, 2010, 61, (5). pp. 1025-1045. [14] Lee, J.H., and Cunningham, S.J.: ‘The impact (or non-impact) of user studies in music information retrieval’, Journal of Intelligent Information Systems, 2013, 41, (3). pp. 499-521. [15] Liem, C.C.S., Mueller, M., Eck, D., Tanetakis, G. and Hanjalic, A.: ‘The need for music information retrieval with user-centered and multimodal strate-gies’, in: 1st International ACM Workshop on Music Information Retrieval with User-Centered Multi-modal Strategies (MIRUM 2011). Scottsdale, AZ, USA; 2011, pp 1-6. [16] Liem, C.C.S, Van Der Sterren, R., Van Tilburg, M., Sarasúa, Á., Bosch, J.J., Melenhorst, M.S., Gómez, E., and Hanjalic, A.: ‘Innovating the Classical Music Experience in the PHENICX Project: Use Cases and Initial User Feedback’, in: 1st International Work-shop on Interactive Content Consumption (WSICC) at EuroITV. Como, Italy; 2013. [17] Pitts, S.E., Dobson, M.C., Gee, K., and Spencer, C.P.: ‘Views of an audience: Understanding the or-chestral concert experience from player and listener perspectives’, Journal of Audience and Reception studies, 2013, 10, (2), pp. 65-95 [18] Radbourne, J., Johanson, K., Glow, H., and Tabitha, W.: ‘The Audience Experience: Measuring Quality in the Performing Arts’, International Journal of Arts Management, 2009, 11, (3), pp. 16-29 [19] Roose, H.: ‘Many-Voiced or Unisono?: An Inquiry into Motives for Attendance and Aesthetic Disposi-tions of the Audience Attending Classical Concerts’, Acta Sociologica, 2008, 51, (3), pp. 237-253 [20] Schedl, M., Flexer, A., and Urbano, J.: ‘The neglected user in music information retrieval research’, Journal of Intelligent Information Systems, 2013, 41, (3), pp. 523-539. [21] Venkatesh, V., Thong, J.Y.L., and Xin, X.: ‘Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology’, MIS Quarterly, 2012, 36, (1), pp. 157-178 [22] Wiering, F.: ‘Meaningful music retrieval’, in: 1st Workshop on the Future of MIR (f(MIR)). Kobe, Japan; 2009. 806 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Improving Score-Informed Source Separation for Classical Music through Note Refinement.",
        "author": [
            "Marius Miron",
            "Julio José Carabias-Orti",
            "Jordi Janer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417689",
        "url": "https://doi.org/10.5281/zenodo.1417689",
        "ee": "https://zenodo.org/records/1417689/files/MironCJ15.pdf",
        "abstract": "Signal decomposition methods such as Non-negative Ma- trix Factorization (NMF) demonstrated to be a suitable ap- proach for music signal processing applications, including sound source separation. To better control this decompo- sition, NMF has been extended using prior knowledge and parametric models. In fact, using score information con- siderably improved separation results. Nevertheless, one of the main problems of using score information is the mis- alignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, on- set and offset for each note present in the score. Note re- finement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) ma- trix. Decomposition is performed in a supervised way, us- ing training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory re- sults, especially in terms of SIR.",
        "zenodo_id": 1417689,
        "dblp_key": "conf/ismir/MironCJ15",
        "keywords": [
            "Non-negative Matrix Factorization",
            "Sound source separation",
            "Alignment between score and performance",
            "Audio-to-score alignment systems",
            "Note-level refinement",
            "Instrument-wise time activation matrix",
            "Supervised decomposition",
            "Training instrument models",
            "Coarsely-aligned score information",
            "Satisfactory results"
        ],
        "content": "IMPROVING SCORE-INFORMED SOURCE SEPARATION FORCLASSICAL MUSIC THROUGH NOTE REFINEMENTMarius Miron Julio Jos´e Carabias-Orti Jordi JanerMusic Technology Group, Universitat Pompeu Fabramarius.miron,julio.carabias,jordi.janer@upf.eduABSTRACTSignal decomposition methods such as Non-negative Ma-trix Factorization (NMF) demonstrated to be a suitable ap-proach for music signal processing applications, includingsound source separation. To better control this decompo-sition, NMF has been extended using prior knowledge andparametric models. In fact, using score information con-siderably improved separation results. Nevertheless, oneof the main problems of using score information is the mis-alignment between the score and the actual performance.A potential solution to this problem is the use of audio toscore alignment systems. However, most of them rely on atolerance window that clearly affects the separation results.To overcome this problem, we propose a novel method toreﬁne the aligned score at note level by detecting both, on-set and offset for each note present in the score. Note re-ﬁnement is achieved by detecting shapes and contours inthe estimated instrument-wise time activation (gains) ma-trix. Decomposition is performed in a supervised way, us-ing training instrument models and coarsely-aligned scoreinformation. The detected contours deﬁne time-frequencynote boundaries, and they increase the sparsity. Finally, wehave evaluated our method for informed source separationusing a dataset of Bach chorales obtaining satisfactory re-sults, especially in terms of SIR.1. INTRODUCTIONSound source separation has been actively addressed dur-ing the recent years with various applications ranging frompredominant melody transcription [10], to interference re-moval in close microphone recordings [4]. State of theart systems particularly target the separation of the pre-dominant harmonic instrument from the accompaniment[3, 4, 10, 18], and less often the separation of various in-struments in classical music [9, 15].Besides [18](recurrent neural networks), and [9](parti-cle ﬁlters), the aforementioned systems are based on non-negative matrix factorization (NMF) [19], a technique thatefﬁciently decomposes a magnitude spectrogram into a setc\u0000Marius Miron, Julio Jos´e Carabias-Orti, Jordi Janer. Li-censed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Marius Miron, Julio Jos´e Carabias-Orti,Jordi Janer. “Improving score-informed source separation for classicalmusic through note reﬁnement”, 16th International Society for Music In-formation Retrieval Conference, 2015.of template (basis) and activation (gains) vectors. How-ever, when dealing with a non-convex problem, the NMFcan converge to a local minima solution for which thesources are not well separated. Towards a better separa-tion, the system can beneﬁt from prior knowledge. On thisaccount, a set of musical meaningful variables are intro-duced into the parametric model and estimated jointly.Furthermore, important improvements are reportedwhen score information is added to guide the decompo-sition process [3,9,12,15,17]. In this case, the best perfor-mance is achieved when the audio is perfectly aligned withthe score [23]. However, in a real-case scenario, a perfectaligned score is not available, and a score-alignment algo-rithm is needed [5, 8, 9, 13].Conversely, as enounced in [3], besides the global mis-alignments, ﬁxed by score-alignment systems, we can alsoencounter local misalignments. With respect to this prob-lem, source separation systems propose to estimate the on-set implicitly into the parametric NMF model, by increas-ing the time boundaries for the onsets in the gains matrix atthe initialization stage [12,15,17]. However, an interestingquestion is whether such an initialization results in a betterseparation than reﬁning the gains and correcting the localmisalignments prior to the source separation.Several methods deal with explicitly correcting localmisalignments [21, p. 103], [20,27]. The latter ﬁnds shapesand contours (blobs) in a pitch salience function, obtainedby pre-processing the spectrogram of the signal and thenﬁltering the spectral peaks for each instrument. However,this method does not use any information regarding thetimbre, which is more desirable when distributing energybetween different instruments.The goal of this paper is to use the note reﬁnement in-formation in order to improve score-informed source sepa-ration of harmonic instruments. Speciﬁcally, we have twocontributions: we adapt the source separation frameworkin [24] to the score-informed case, and, notably, we cor-rect the local misalignments in the score and reﬁne thetime-frequency zones of the gains used in source separa-tion. First, we compute the initial gains by distributingthe energy among instruments with the source separationNMF algorithm proposed in [24]. The computed gainsoffer a more robust representation than the pitch salienceused in [20], because timbre information is used to dealwith the problem of overlapping partials between the in-struments, and because the gains are represented on log-frequency scale and are less noisy than the pitch salience448in [20]. As a result, detecting and assigning blobs to notesin the gains matrix can be done more robustly. Second, wecan use the processed gains to reiterate the NMF sourceseparation. Consequently, instead of initializing the NMFwith the MIDI information, we can use the blobs associ-ated with each note. On this account, we restrict the po-tential interferences not only in time but also in frequency,and achieve better separation.We evaluate the note reﬁnement and the source separa-tion on the Bach10 dataset [9]. Accordingly, note reﬁne-ment is performed on an artiﬁcially generated score withlocal misalignments, and on the output the DTW basedscore alignment algorithm [5]. Furthermore, we evaluatethe score-informed source separation, as we want more in-sight on which initialization method yields better sourceseparation.The remainder of this paper is structured as follows.First, we describe the existing source separation frame-work and then, in Section 3, the note reﬁnement methodand its application to monaural score informed source sep-aration. Then, we evaluate score alignment and source sep-aration. Finally, we discuss the results and restate the con-tributions to prior work.2. NMF FOR SOURCE SEPARATIONIn this section we explain the Source Separation Frame-work used for sound source separation. Further informa-tion can be found in [24].2.1 Signal ModelTechniques based on Non-negative Matrix Factorization(NMF) can be used to efﬁciently decompose an audio spec-trogram as a linear combination of spectral basis functions.In such a model, the short-term magnitude (or power) spec-trum of the signalx(f,t)in time-frametand frequencyfis modeled as a weighted sum of basis functions as:x(f,t)⇡ˆx(f,t)=NXn=1bn(f)gn(t),(1)wheregn(t)is the gain of the basis functionnat framet, andbn(f),n=1,. . . ,Nare the bases. Note that model ineq. (1) only holds under the assumption of a) strong spar-sity (only one source active per time-frequency(TF) bin) orb) local stationarity (only for power spectrogram) [2].When dealing with musical instrument sounds, it is nat-ural to assume that each basis function represents a sin-gle pitch, and the corresponding gains contain informa-tion about the onset and offset times of notes having thatpitch [4]. Besides, restricting the model in (1) to be har-monic is particularly useful for the analysis and separationof musical audio signals since each basis can deﬁne a sin-gle fundamental frequency and instrument. Harmonicityconstrained basis functions are deﬁned as:bj,n(f)=HXh=1aj,n(h)G(f\u0000hf0(n)),(2)wherebj,n(f), are the bases for each notenof instru-mentj,n=1,. . . ,Nis deﬁned as the pitch range for in-strumentj=1,. . . ,J, whereJis the total number of in-struments present in the mixture,h=1,. . . ,His the num-ber of harmonics,aj,n(h)is the amplitude of harmonichfor notenand instrumentj,f0(n)is the fundamental fre-quency of noten,G(f)is the magnitude spectrum of thewindow function, and the spectrum of a harmonic com-ponent at frequencyhf0(n)is approximated byG(f\u0000hf0(n)). Therefore, the harmonic constrained model forthe magnitude spectra of a music signal is deﬁned as:ˆx(f,t)=JXj=1NXn=1HXh=1gj,n(t)aj,n(h)G(f\u0000hf0(n)),(3)where the time gainsgj,n(t)and the harmonic ampli-tudesaj,n(h)are the parameters to be estimated.2.2 Augmented NMF for Parameter EstimationNon-negativity of the parameters is a common restrictionimposed to the signal decomposition method for music sig-nal processing applications. Furthermore, the factorizationparameters of equation (3) are estimated by minimizing thereconstruction error between the observedx(f,t)and themodeledˆx(f,t)spectrograms, using a cost function, whichis this case the Beta-divergence [14]:D\u0000(x|ˆx)=8>>><>>>:1\u0000(\u0000\u00001)\u0000x\u0000+(\u0000\u00001)ˆx\u0000\u0000\u0000xˆx\u0000\u00001\u0000\u00002(0,1)[(1,2]xlogxˆx\u0000x+ˆx\u0000=1xˆx+l o gxˆx\u00001\u0000=0(4)For particular values of\u0000, Beta-divergence includes inits deﬁnition the most popular cost functions, Euclidean(EUC) distance (\u0000=2), Kullback-Leibler (KL) diver-gence (\u0000=1) and the Itakura-Saito (IS) divergence(\u0000=0). The parameters in (1) are estimated with an it-erative cost minimization algorithm based on multiplica-tive update (MU) rules, as discussed in [19]. Under theserules,D(x(f,t)|ˆx(f,t))does not increase with each iter-ation while ensuring the non-negativity of the bases andthe gains. These MU rules are obtained applying diagonalrescaling to the step size of the gradient descent algorithm(see [19] for further details).Lets denote as✓lthe parameter to be estimated. Then,the MU rule for✓lis obtained by computing the derivativer✓lDof the cost function with respect to✓l. This deriva-tive can be expressed as a difference between two positivetermsr+✓lDandr\u0000✓lD[25] and thus, the update rule forparameter✓lcan be expressed as:✓l ✓lr\u0000✓lD(x(f,t)|ˆx(f,t))r+✓lD(x(f,t)|ˆx(f,t)).(5)2.3 Timbral Informed Signal ModelAs showed in [6], when appropriate training data are avail-able, it is useful to learn the instrument-dependent bases inProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 449advance and keep them ﬁxed during the analysis of the sig-nals. In the commented work, the amplitudes of each noteof each musical instrumentaj,n(h)are learnt by using theRWC database [16] of solo instruments playing isolatednotes together with their ground-truth transcription. Thus,gains are set to unity for each pitch at those time frameswhere the instrument is active while the rest of the gainsare set to zero. Note that gains initialized to zero remainzero because of the multiplicative update rules, and there-fore the frame is represented only with the correct pitch.The MU rules are computed from equation (5) for theamplitude coefﬁcients and the gains asaj,n(h) aj,n(h)Pf,tx(f,t)ˆx(f,t)\u0000\u00002gj,n(t)G(f\u0000hf0(n))Pf,tˆx(f,t)\u0000\u00001gj,n(t)G(f\u0000hf0(n))(6)gj,n(t) gj,n(t)Pf,mx(f,t)ˆx(f,t)\u0000\u00002aj,n(h)G(f\u0000hf0(n))Pf,mˆx(f,t)\u0000\u00001aj,n(h)G(f\u0000hf0(n))(7)Finally, the training procedure is summarized in Algo-rithm 1.Algorithm 1Instrument modeling algorithm1Computex(f,t)from a solo performance for each instru-ment in the training database2Initialize gainsgj,n(t)with the ground truth transcriptionRj,n(t)andaj,n(h)with random positive values.3Update the gains using eq. (6).4Update the bases using eq. (7).5Repeat steps 2-3 until the algorithm converges (or maximumnumber of iterations is reached).6Compute basis functionsbj,n(f)for each instrumentjusingeq. (2).The training algorithm obtains an estimation of the ba-sis functionsbj,n(f)required at the factorization stage foreach instrument. Since the instrument dependent basis func-tionsbj,n(f)are held ﬁxed, the factorization can be re-duced to the estimation of the gainsgj,n(t)for each of thetrained instrumentsj.2.4 Gains estimationHere, the classical augmented NMF factorization with MUrules is applied to estimate the gains corresponding to eachsourcejin the mixture. The process is detailed in Algo-rithm 2.Algorithm 2Gain Estimation Method1Initializebj,n(f)with the values learned in section 2.3. Userandom positive values to initializegj,n(t).2Update the gains using eq. (7).3Repeat step 2 until the algorithm converges (or maximumnumber of iterations is reached)2.5 From the estimated gains to the separated signalsIn this work, we assume that the individual sourcesyj(t),j=1...Jthat compose the mixed signalx(t)arelinearly mixed, sox(t)=JPj=1yj(t). Lets denote thepower spectral density of sourcejat TF bin(f,t)as|Xj(t, f)|2,j=1...J, then, each ideally separated sourceyj(t)can be estimated from the mixturex(t)using a gen-eralized time-frequency Wiener ﬁlter over the Short-TimeFourier Transform (STFT) domain as in [14, 15].Here we use the Wiener ﬁlter soft-masking strategy asin [24]. In particular, the soft-mask↵jof sourcejrepre-sents the relative energy contribution of each source to thetotal energy of the mixed signalx(t)and is obtained as:↵j(t, f)=ˆYj(t, f)2PjˆYj(t, f)2(8)whereˆYj(t, f)is the estimated source magnitude spec-trogram computed asˆYj(t, f)=gn,j(t)bj,n(f),gn,jarethe gains estimated in Section 2.4 andbj,n(f)are the ﬁxedbasis functions learnt in Section 2.3.Then, the magnitude spectrogramˆXj(t, f)is estimatedfor each sourcejas:ˆXj(t, f)=↵j(t, f)·X(t, f)(9)whereX(t, f)is the complex-valued STFT of the mixtureat TF bin(t, f).Finally, the estimated sourceˆyj(t)is computed with theinverse overlap-add STFT overˆXj(f,t), with the phasespectrogram of the original mixture.3. PROPOSED METHODWe adapt the source separation framework described inSection 2 to the score-informed scenario. The frameworkis initialized with the gainsginitj,n(t)derived from a MIDIscore having alignment errors. Next, the resulting gainsafter the NMF separationgj,n(t)are reﬁned with a set ofimage processing heuristics which we describe in the Sec-tion 3.2. Finally, the reﬁned gainspj,n(t)are used to reini-tialize the framework and reiterate the separation, towardsa better result.3.1 Score-informed gains computationWe use as input a coarsely aligned score and the associatedaudio recording. The MIDI score has local misalignmentsup todframes for the onset and the offset times. Thus, weinitialize the source separation system in Section 2 with theMIDI notes by addingdframes before the onset and afterthe offset. Consequently, for an instrumentj, and all thebins in a semitonenassociated with a MIDI note (Figure1B), we set the matrixginitj,n(t)to 1 for the frames wherethe MIDI note is played as well as for thedframes aroundthe onset and the offset of the MIDI note. The other valuesinginitj,n(t)are set to 0 do not change during computation,while the values set to 1 evolve according to the energydistributed between the instruments. The ﬁnal gains arecomputed with the algorithm described in Section 2.4, ob-taininggj,n(t), the gains which will be used during the notereﬁnement stage (Figure 1C).450 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1.A. The reconstructed signal can be seen as theproduct between the several harmonic components (A) andthe gains (B). After NMF , the resulting gains (C) are splitin submatrices and used to detect blobs (D).3.2 Note reﬁnementThe shape and contours detected in an image, and asso-ciated with meaningful objects, are commonly known asblobs [22, p. 248]. Additionally, if we consider the matrixassociated with a grayscale image, an image patch is anysubmatrix of the corresponding matrix.During the note reﬁnement stage we apply image pro-cessing on the gains matrixgj,n(t)in order to associate theentities in an image, namely the blobs, with notes. Thechosen blobs give the onset and offset times. Additionally,the areas of the blobs are used to reiterate the separation.The reﬁnement of the gains occurs for each note sep-arately. Hence, for each notesfrom the input score wechoose an image patch centered at the semitonencorre-sponding to its associated MIDI note value. Precisely, weprocess a submatrix ofgj,n(t), namelyˇgsj,n(t), fors=1...S, whereSis the total number of notes in the score foran instrumentj. The size of submatrixˇgsj,n(t), as seen inFigure 1D, is equal to the one of the submatrices which hasbeen set to 1 at the initialization for the corresponding notes. Thus,ˇgsj,n(t)has a width of two semitones and a lengthcorresponding to the prolonged duration of the notes.3.2.1 Image binarizationEach image patch is preprocessed in two steps before bina-rization. Initially, each row vector of the submatrixˇgsj,n(t)is convolved with a smoothing gaussian ﬁlter to removenoise and discontinuities. Then each column of the samesubmatrix is multiplied with a gaussian centered at the cen-tral frequency bin, in order to penalize the values far fromthe central bin, but still to preserve vibratos or transitionsbetween notes.First, we apply a smoothing ﬁlter [22, p. 86] on theimage patch. We choose a one dimension Gaussian ﬁlter:w(t)=1p2⇡\u0000e\u0000\u0000t22\u00002(10)wheretis the time axis and\u0000=3is the standard devia-tion . The ﬁrst and the last\u0000elements of each row vectornof the matrixˇgsj,n(t)are mirrored at the beginning, respec-tively at the end of the vector. Then each row vector ofˇgsj,n(t)is convolved withw(t), and the result is truncatedin order to preserve the dimensions of the initial matrix byremoving the mirrored frames.Second, we multiplyˇgsj,n(t)with a 1-dimensional gaus-sian centered in the central frequency bin:v(n)=1p2⇡⌫e\u0000(n\u0000)22⌫2(11)wherenis the frequency axis,=4is the position ofthe central frequency bin and the standard deviation⌫=4(one semitone). Then, each column vector ofˇgsj,n(t)ismultiplied withv(n).Image binarization assumes calculating a submatrixˇpsj,n(t), associated with notes:ˇpsj,n(t)=(0ifˇgsj,n(t)< mean(ˇgsj,n(t))1ifˇgsj,n(t)\u0000mean(ˇgsj,n(t))(12)3.2.2 Blob selectionFor a noteswe detect blobs the corresponding binary sub-matrixˇpsj,n(t), using the connectivity rules described in[22, p. 248] and [20].Furthermore, we need to determine the best blob foreach note. A simple solution is to compute a score foreach blob by summing all the values inˇgsj,n(t)includedin the area associated with the blob. However, we wantto penalize parts of the blobs which overlap in time withother blobs from different notess\u00001,s ,s+1. Basically,we want to avoid picking the same blobs for two adjacentnotes. Thus, we weight each element inˇgsj,n(t)with a fac-tor\u0000, depending on the amount of overlapping with blobsfrom adjacent notes, and we build a score matrix:ˇqsj,n(t)=8><>:\u0000⇤ˇgsj,n(t)ifˇpsj,n(t)^ˇps\u00001j,n(t)=1\u0000⇤ˇgsj,n(t)ifˇpsj,n(t)^ˇps+1j,n(t)=1ˇgsj,n(t)otherwise(13)where\u0000is a value in the interval0..1.Note that we do not use the dynamic programmingmethod in [20] because the images patches are small, thuswe have to choose between very few blobs and, to that re-spect, the Dijkstra algorithm is superﬂuous.Furthermore, we compute a score for each notesandfor each blob associated with the note, by summing up theelements in the score matrixˇqsj,n(t)which are a part of ablob. Furthermore, the selected blob for a notesis the onehaving the maximum score. The boundaries of the selectedblob give the note onset and offset. Additonally, the areaof the blob can be used to reiterate source separation.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4513.3 Extension to score informed source separationOur assumption is that better alignment gives a more sparseinitialization of the gainsgj,n(t), which limits the way en-ergy distributes along instruments during the NMF, andyields better source separation. Additionally, we can fur-ther increase sparsity by knowing the frequency bound-aries of the notes and by initializing the gains with the de-tected blob contours. However, by limiting the areas inthe activations to the area of the chosen blobs, we discardenergy from the unchosen blobs. This energy which is dis-carded from an instrument can be redistributed between theother instruments by reiterating the factorization.Letpsj,n(t)be the matrices derived from the submatri-cesˇpsj,n(t), containing1for the elements associated withthe selected blob for the notesand0otherwise. Then,the new matrixgj,n(t)can be formed with the submatricespsj,n(t). For the corresponding binsnand time framesfof a notes, we initialize the values ingj,n(t)with the val-ues inpsj,n(t). Subsequently, we repeat the factorizationusing the timbre-informed algorithm described in Section2.4, this time initializing it with the reﬁned gains. More-over, the calculate the spectrogram of the separated sourceswith the method described in Section 2.5.4. EXPERIMENTAL SETUPa) Time-Frequency representation:In this paper we usea low-level spectral representation of the audio data whichis generated from a windowed FFT of the signal. A Han-ning window with the size of92ms, and a hop size of11ms are used (for synthetic and real-world signals). Here,a logarithmic frequency discretization is adopted. Further-more, two time-frequency resolutions are used. First, to es-timate the instrument models and the panning matrix, a sin-gle semitone resolution is proposed. In particular, we im-plement the time-frequency representation by integratingthe STFT bins corresponding to the same semitone. Sec-ond, for the separation task, a higher resolution of1/4ofsemitone is used, which has proven to achieve better sep-aration results [4]. These time-frequency representationsare obtained by integrating the short-term Fourier trans-form (STFT) bins corresponding to the same semitone, or1/4semitone, interval. Note that in the separation stage,the learnt basis functionsbj,n(f)are adapted to the1/4semitone resolution by replicating 4 times the basis at eachsemitone to the4samples of the1/4semitone resolutionthat belong to this semitone.b) Dataset:We evaluate the note reﬁnement and the sourceseparation on the Bach10 dataset presented in [9] and com-prising ten J.S. Bach chorales played by a quartet (violin,clarinet, tenor saxophone and bassoon), each piece hav-ing the duration⇡30seconds. The instruments wererecorded separately, then mixed to create a monaural au-dio sampled at44.1kHz. Moreover, the Bach10 datasethas certain traits which inﬂuence the note reﬁnement andsource separation. For instance, the chorales present a ho-mophonic texture which makes it more difﬁcult when per-forming source separation. Additionally, the results aredirectly related to the tempo of the recordings [9]. Forthis dataset, the tempo is slower than other classical mu-sic pieces, there are very few notes below the quarter notelevel, and we have prolonged notes, known as fermata.The audio ﬁles are accompanied by two MIDI scores:the perfectly aligned ground truth, and a score which hasglobal and local misalignments. Moreover, in order to testthe note reﬁnement we use two datasets. The datasetdisA,proposed in [20], introduces errors for the ground truth on-sets and offsets in the interval[100...200]ms. Addition-ally, we plan to reﬁne the alignment at the note level forthe score alignment method described in [5], denoted asdatasetdtwJ. The method offers solely note onset informa-tion, therefore we use the onset of the next note as the noteoffset for the current note.c) Evaluation metrics:For score aligment, we evaluatenote onsets and offsets in terms of alignment rate, similarlyto [7], ranging from0to1, deﬁned as the proportion of cor-rectly aligned notes in the score within a given threshold.For source separation, the evaluation framework and themetrics are described in [26] and [11]. Correspondingly,we useSource to Distortion Ratio(SDR),Source to Inter-ference Ratio(SIR), andSource to Artifacts Ratio(SAR).d) Parameters tuning:We picked50number of iterationsfor the NMF, and we experimentally determined value forthe beta-divergence distortion,\u0000=1.3.5. RESULTS5.1 Score alignmentWe measure the aligment rate of the input score present-ing misalignments (B), the alignment method described inSection 3.2 (E), and the one in [20] (D), on the two datasets”disA” and ”dtwJ”. We vary the threshold within the inter-val[15..200]. Subsequently, in Figure 2 we present theresults for the datasets ”disA” and ”dtwJ”. The errors ofthe original scores are presented with dotted and straightblack lines. For the aligned onsets, aligned rates are drawnwith dashed lines and for offsets with straight lines.We observe that both reﬁnement methods improve thealign rate of the scores with local misalignments (blackline). For lower threshold, the proposed method (red) im-proves the method in [20] (blue). Moreover, consideringthat offsets are more difﬁcult to align, the proposed align-ment outperforms the one in [20] when it comes to detect-ing offsets, within a larger threshold.5.2 Source separationWe use the evaluation metrics described in Section 4c. Weinitialize the gains of the separation framework with dif-ferent note information, as seen in Figure 3. Speciﬁcally,we evaluate the perfect initialization with the ground-truthMIDI, Figure 3(A), with the score having local misalign-ments (disA) or the output of a score aligment systemdtwJ,Figure 3(B), the common practice of NMF gains initial-ization in state of the art score-informed source separa-tion [12,15,17], Figure 3(C), and the reﬁnement aproaches:Figure 3(D,E,F). Note that in D and E we initialize the452 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2.Alignment rate for the two datasets; ”B” denotesthe score to be reﬁned; ”E” and ”D” are the scores reﬁnedwith the methods in Section 3.2 and [20].\nFigure 3.The test cases for initialization of score-informed source separation, for the submatrixpsj,n(t)gains prior to a note reﬁnement stage with the methods de-scribed in [20] (reﬁned [20]) and in the Section 3.2 (reﬁnedtime), and in F we further reﬁne the gains as proposed inSection 3.3 (reﬁned time frequency).The results for the test cases A-F, for the two datasetsdisAanddtwJare presented in Table 1 in terms of meansof SDR, SIR, SAR. Additionally, audio examples of theseparation can be listened online [1].The proposed system F improves over the other cases interms of SDR, for all the input scores. Particularly, whenwe reﬁne the gains in frequency we obtain higher SIR val-ues, hence less interference. Consequently, F yields betterresults than A, the initialization with ground-truth MIDIannotations, and than E, which is note reﬁnement in time,without tracking the shape of the blob. On the other hand,the ground-truth A has better SAR values, less artifacts,but has more interference, since F sets to zero some partsof the gains matrix for which the energy does not get re-distributed. Additionally, F improves over C, the implicitinitialization which extends the time span for the gains,which is the most used approach by the state of the artscore-informed source separation algorithms when dealingwith local misalignments. On the other hand, the worsedecision is not to do any reﬁnement, as in case B.Moreover, F achieves better results than A-E reﬁningthe alignment of [5] (datasetdtwJ). However, as this datasetdoes not have large local misalignments, the difference be-tween F and C, and even B, is not as high as for datasetdisA, and the improvement is not remarkable. Note thatF is better than A in this case as well, suggesting that ourdatasetdisAdatasetdtwJSDR SIR SAR SDR SIR SARA 6.31 7.1025.266.31 7.10 25.26B 3.72 4.04 15.20 6.19 6.99 24.59C 5.18 5.67 19.62 6.25 6.97 25.31D 5.89 6.80 22.41 5.79 6.67 23.69E 6.24 7.08 24.43 6.07 6.99 24.58F6.35 7.3724.186.37 7.23 25.45Table 1. Means of SDR, SIR, ISR for the datasetsdisAanddtwJfor test cases A-F, for all the instrumentsproposed method is robust with regards to different kindsof inputs: signiﬁcant local misalignments as the datasetdisA, or smaller as datasetdtwJ. Additionally, ground truthoffsets are close to the next note onsets, thusdtwJachievesbetter separation compared todisA.Furthermore, with respect to the performance achievedby other source separation frameworks, tested on the samedataset [9], the results in terms of SDR are similar. Themethod we propose in this paper is used with the sourceseparation framework [24], but can be adapted to otherNMF based frameworks. However, due to the TF repre-sentation used in the method, even for ideal TF masks, theseparated examples might exhibit cross-talk at high fre-quencies. This fact is reﬂected in the measures by lowerSIR values.6. CONCLUSIONSWe proposed a timbre-informed note reﬁnement method tocorrect local misalignments and to reﬁne the output of stateof the art audio-to-score alignment systems, for monauralclassical music recordings. We extended the source sepa-ration framework proposed in [24] for the case of monoau-ral score informed source separation by reﬁning the gains.The approach increases the sparseness of the gains initial-ization, achieving better performance than the implicit ap-proach of estimating the onset with a parametric model,as [12,15,17], especially for input scores having large localmisalignments. Particularly, the proposed system reducesthe interference, resulting in higher SIR values. Addition-ally, the method improves the alignment rate over the onein [20], and is more robust because it uses meaningful tim-bre information.As future work, the selection of the best blob and thebinarization threshold could be included into the factor-ization framework through the cost function. Moreover,we plan to test our method with more complex orchestralrecordings, and for multi-channel source separation.7. ACKNOWLEDGEMENTSThis work was supported by the European Commission,FP7 (Seventh Framework Programme), STREP project, ICT-2011.8.2 ICT for access to cultural resources, grant agree-ment No 601166. Phenicx ProjectProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4538. REFERENCES[1]Bach10 dataset source separation demo.https://dl.dropboxusercontent.com/u/80928189/demos/index.html.[2]L. Benaroya, F. Bimbot, and R. Gribonval. Audiosource separation with a single sensor.Audio, Speech,and Language Processing, IEEE Transactions on,14(1):191–199, 2006.[3]J.J. Bosch, K. Kondo, R. Marxer, and J. Janer. Score-informed and timbre independent lead instrument sep-aration in real-world scenarios. InSignal ProcessingConference (EUSIPCO), pages 2417–2421, Aug 2012.[4]J. J. Carabias-Orti, M. Cobos, P. Vera-Candeas, andF. J. Rodriguez-Serrano. Nonnegative signal factoriza-tion with learnt instrument models for sound sourceseparation in close-microphone recordings.EURASIPJ. Adv. Sig. Proc., 2013:184, 2013.[5]J. J. Carabias-Orti, F. J. Rodriguez-Serrano, P. Vera-Candeas, N. Ruiz-Reyes, and F. J. Ca˜nadas-Quesada.An audio to score alignment framework using spectralfactorization and dynamic time warping.ISMIR, 2015.[6]J. J. Carabias-Orti, T. Virtanen, P. Vera-Candeas,N. Ruiz-Reyes, and F. J. Canadas-Quesada. MusicalInstrument Sound Multi-Excitation Model for Non-Negative Spectrogram Factorization.IEEE Journal ofSelected Topics in Signal Processing, 5(6):1144–1158,October 2011.[7]A. Cont, D. Schwarz, N. Schnell, and C. Raphael. Eval-uation of real-time audio-to-score alignment. InISMIR,2007.[8]S. Dixon. Match: A music alignment tool chest. InIS-MIR, 2005.[9]Z. Duan and B. Pardo. Soundprism: An online systemfor score-informed source separation of music audio.IEEE Journal of Selected Topics in Signal Processing,pages 1–12, 2011.[10]J.L. Durrieu, A. Ozerov, and C. F´evotte. Main instru-ment separation from stereophonic audio signals usinga source/ﬁlter model.EUSIPCO, (1), 2009.[11]V . Emiya, E. Vincent, N. Harlander, and V . Hohmann.Subjective and Objective Quality Assessment of Au-dio Source Separation.IEEE Transactions on Audio,Speech, and Language Processing, 19(7):2046–2057,September 2011.[12]S. Ewert and M. M¨uller. Score-Informed V oice Sepa-ration For Piano Recordings.ISMIR, pages 245–250,2011.[13]S. Ewert, M. M¨uller, and P. Grosche. High resolutionaudio synchronization using chroma onset features. InICASSP, pages 1869–1872. IEEE, 2009.[14]C. F´evotte, N. Bertin, and JL. Durrieu. Nonnegativematrix factorization with the itakura-saito divergence:With application to music analysis.Neural computa-tion, 21(3):793–830, 2009.[15]J. Fritsch and M. Plumbley. Score informed audiosource separation using constrained non-negative ma-trix factorization andscore synthesis.ICASSP, pages888–891, 2013.[16]M. Goto. Development of the rwc music database. InICA, pages 553–556, 2004.[17]R. Hennequin, B. David, and R. Badeau. Score in-formed audio source separation using a paramet-ric model of non-negative spectrogram.ICASSP, (1),2011.[18]P.-S. Huang, M. Kim, M. Hasegawa-Johnson, andP. Smaragdis. Singing-voice separation from monau-ral recordings using deep recurrent neural networks. InISMIR, 2014.[19]D. D. Lee and H. S. Seung. Learning the parts ofobjects by non-negative matrix factorization.Nature,401(6755):788–791, 1999.[20]M. Miron, J.J. Carabias, and J. Janer. Audio-to-scorealignment at the note level for orchestral recordings. InISMIR, 2014.[21]B. Niedermayer.Accurate Audio-to-Score AlignmentData Acquisition in the Context of Computational Mu-sicology. PhD thesis, Johannes Kepler Universit¨at,2012.[22]M. Nixon.Feature Extraction and Image Processing.Elsevier Science, 2002.[23]C. Raphael. A classiﬁer-based approach to score-guided source separation of musical audio.Comput.Music J., 32(1):51–59, March 2008.[24]F. J. Rodriguez-Serrano, J. J. Carabias-Orti, P. Vera-Candeas, T. Virtanen, and N. Ruiz-Reyes. Multipleinstrument mixtures source separation evaluation us-ing instrument-dependent NMF models. InLVA/ICA,pages 380–387, 2012.[25]D.L. Sun and C. Fevotte. Alternating direction methodof multipliers for non-negative matrix factorizationwith the beta-divergence. InAcoustics, Speech andSignal Processing (ICASSP), 2014 IEEE InternationalConference on, pages 6201–6205, May 2014.[26]E. Vincent, R. Gribonval, and C. Fevotte. Performancemeasurement in blind audio source separation.Audio,Speech, and Language Processing, IEEE Transactionson, 14(4):1462–1469, July 2006.[27]S. Wang, S. Ewert, and S. Dixon. Compensatingfor asynchronies between musical voices in score-performance alignment. InICASSP, pages 589–593,Brisbane, Australia, 2015.454 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Autoregressive Hidden Semi-Markov Model of Symbolic Music Performance for Score Following.",
        "author": [
            "Eita Nakamura",
            "Philippe Cuvillier",
            "Arshia Cont",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417030",
        "url": "https://doi.org/10.5281/zenodo.1417030",
        "ee": "https://zenodo.org/records/1417030/files/NakamuraCCOS15.pdf",
        "abstract": "A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score fol- lowing. Stochastic modelling has been one of the most suc- cessful strategies in this field. We describe the performance as a hierarchical process of performer’s progression in the score and the production of performed notes, and repre- sent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and rea- sons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by com- paring the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more effi- cient and retains the advantages of the former model. The present model yields one of the state-of-the-art score fol- lowing algorithms for symbolic performance and can pos- sibly be applicable for other music recognition problems.",
        "zenodo_id": 1417030,
        "dblp_key": "conf/ismir/NakamuraCCOS15",
        "keywords": [
            "stochastic model",
            "symbolic performance",
            "polyphonic scores",
            "score following",
            "hidden semi-Markov model",
            "hidden Markov model",
            "performance progression",
            "produced notes",
            "accuracy comparison",
            "errors analysis"
        ],
        "content": "AUTOREGRESSIVE HIDDEN SEMI-MARKOV MODEL OF SYMBOLICMUSIC PERFORMANCE FOR SCORE FOLLOWINGEita Nakamura1Philippe Cuvillier2Arshia Cont2Nobutaka Ono1Shigeki Sagayama31National Institute of Informatics, Tokyo 101-8430, Japan2Inria MuTant Project-Team, Ircam/UPMC/CNRS UMR STMS, 75004 Paris, France3Meiji University, Tokyo 164-8525, Japaneita.nakamura@gmail.com,philippe.cuvillier@ircam.fr,Arshia.Cont@ircam.fronono@nii.ac.jp,sagayama@meiji.ac.jpABSTRACTA stochastic model of symbolic (MIDI) performance ofpolyphonic scores is presented and applied to score fol-lowing. Stochastic modelling has been one of the most suc-cessful strategies in this ﬁeld. We describe the performanceas a hierarchical process of performer’s progression in thescore and the production of performed notes, and repre-sent the process as an extension of the hidden semi-Markovmodel. The model is compared with a previously studiedmodel based on hidden Markov model (HMM), and rea-sons are given that the present model is advantageous forscore following especially for scores with trills, tremolos,and arpeggios. This is also conﬁrmed empirically by com-paring the accuracy of score following and analysing theerrors. We also provide a hybrid of this model and theHMM-based model which is computationally more efﬁ-cient and retains the advantages of the former model. Thepresent model yields one of the state-of-the-art score fol-lowing algorithms for symbolic performance and can pos-sibly be applicable for other music recognition problems.1. INTRODUCTIONFor the last thirty years the real-time matching of musicperformance to the corresponding score (called score fol-lowing) has been a popular ﬁeld of study motivated byapplications such as automatic music accompaniment andscore-page turning system [1, 2, 3, 4, 5, 6, 7, 8]. Westudy here score following of polyphonic symbolic (MIDI)performance. A central problem in score following is toproperly capture the variety of music performance in acomputationally efﬁcient manner. A commonly studiedway to capture this variety and develop an effective score-following algorithm is to use stochastic models of musicperformance (Sec. 2.1, see also [3]).c\u0000Eita Nakamura, Philippe Cuvillier, Arshia Cont, Nobu-taka Ono, Shigeki Sagayama. Licensed under a Creative Commons Attri-bution 4.0 International License (CC BY 4.0).Attribution:Eita Naka-mura, Philippe Cuvillier, Arshia Cont, Nobutaka Ono, Shigeki Sagayama.“ Autoregressive Hidden Semi-Markov Model of Symbolic Music Perfor-mance for Score Following ”, 16th International Society for Music Infor-mation Retrieval Conference, 2015.Hidden Markov models (HMMs) have been applied toscore following of symbolic performance and providedcurrently best results [4, 7, 9]. In these models, a musi-cal event in the score, i.e. note, chord, trill, etc., is repre-sented as a state, and the performed notes are describedas outputs of an underlying state transition process. Mem-oryless statistical dependence is assumed for both outputand transition probabilities for the sake of computationalefﬁciency. Due to these simpliﬁcations the models cannotwell describe signiﬁcant features of performance data suchas the number of performed notes per event and the totalduration of a trill.Phenomenologically, music performance can be re-garded as a hierarchical process of producing musicalnotes: The higher level describes performer’s progressionin the score in units of musical events, and the lower leveldescribes the production of individual notes [9, 10]. Wedescribe this process in terms of a hidden semi-Markovmodel (HSMM) [11] with an autoregressive extension [12](Sec. 2) and incorporate the above features into the model.With some simpliﬁcations, the model is reduced to a pre-viously studied HMM [9]. We compare these models inthe informational and algorithmic aspects and argue thatthe present model is advantageous for score following es-pecially for scores with trills, tremolos, and arpeggios(Sec. 3). Empirical conﬁrmation of this fact is given bycomparing the accuracy of score following and analysingthe errors (Sec. 4). Finally remaining problems and futureprospects are discussed (Sec. 5).2. AUTOREGRESSIVE HIDDEN SEMI-MARKOVMODEL OF SYMBOLIC PERFORMANCE2.1 Stochastic description of music performanceMusic performances based on a score have a wide vari-ety because of indeterminacies inherent in musical scoredescriptions and uncertainties in movements of perform-ers and musical instruments. These indeterminacies anduncertainties are included in tempos, noise in onset times,dynamics, articulations, ornaments, and also in the way ofmaking performance errors, repeats, and skips [7]. In orderto perform accurate and robust score following, we need392to incorporate (maybe implicit) rules into the algorithm tocapture this variety.A way to do this is to construct a stochastic model ofmusic performance and describe those indeterminacies anduncertainties in terms of probability. A score-following al-gorithm can be developed as an inference problem of themodel. We shall take this approach in the following, whichhas been proved to be successful in score following.2.2 Model of performer’s progression in the scoreLet us present the model. We model music performance asa combination of subprocesses in two levels. The higher-level (top-level) process describes the performer’s pro-gression in the score in units of musical events that arewell-ordered in performances without errors. We take achord (possibly arpeggiated), a trill/tremolo, a short ap-poggiatura, or an after note1as a unit and represent it witha state (top state). Letilabel a top state. Then the per-former’s progression can be described as successive transi-tions between these states denoted byi1:N=(i1,...,iN)(Nis the number of performed MIDI notes). We will usethe symboln(= 1,...,N)to index the performed notesthat are ordered according to the onset time, andinrepre-sents the corresponding musical event.The probabilityP(i1:N)describes statistical tenden-cies of performances. Simpliﬁcations are necessary toconstruct a performance model yielding a computation-ally tractable algorithm. A typical assumption is thatthe probability is decomposed into transition probabilities:P(i1:N)=⇧Nn=1P(in|in\u00001)(P(i1|i0)⌘P(i1)denotesthe initial distribution). The probabilityP(j|i)representsthe relative frequency of straight progressions to the nextevent (j=i+1), insertions of events (j=i), dele-tions of an event (j=i+2), and repeats or skips (if|j\u0000i\u00001|>1). These probability values can be estimatedfrom performance data. With the assumption thatP(i|j)isonly dependent oni\u0000j, the probability values have beenestimated with piano performance data in a previous study([7], Table 3).2.3 Model of production of performed notesThe lower-level process describes the production of per-formed notes during each musical event. Because dynam-ics and articulations are generically highly indeterminate,we focus on pitch and onset time which are denoted bypnandtn. For example, multiple notes are performed ata chord or a trill (Fig. 1). Note that whereas chords arewritten in musical scores as simultaneous notes, performedMIDI notes are serialised and never exactly simultaneous.Thuspnis always a single pitch.Let us ﬁrst consider the number of performed notes perevent. For “chords” (meaning a set of all simultaneousnotes in the score), short appoggiaturas, and after notes,the expected number of notes is determinate, but it can1Here ‘after notes’ are deﬁned as grace notes that are played in prece-dence over the associated beat. A typical example is grace notes after atrill.ScorePerformanceIOI3 IOI3 IOI2IOI1(a) An arpeggiated chord.\nIOI3IOI3 IOI2IOI1ScorePerformance(b) Trill with preceding short-appoggiaturas and after notes.Figure 1. Examples of musical events and performednotes. The three types of time intervals IOI1, IOI2, andIOI3 are explained in the text.be modiﬁed as a result of added or deleted notes by mis-take. For trills and (unmeasured) tremolos, the numberof notes are indeterminate since the speed of ornamentsvaries among realisations. We describe this situation witha probability distributiondi(s)wheresdenotes the num-ber of performed notes (⌃1s=1di(s)=1). For example, thefunctiondi(s)peaks at the indicated number of notes wheneventiis a chord. When eventiis a one-note trill, the peakcan be written asspeaki'⌫iv/\u0000ttrill, where\u0000ttrill,⌫i, andvdenote the average inter-onset time interval (IOI) of suc-cessive notes of a trill, the note value of eventi, and the(inverse) tempo in units of “second per unit note value”.Because currently we do not have a strong empirical basisfor determining the shape ofdi(s), we simply assume it isa normal distributiondi(s)=N(s;speaki,\u0000i)withspeakigiven in Sec. 2.3, and leave\u0000ias an adjustable parameter.Next the pitch of each performed note of eventican bedescribed with a probabilityPpitchi(p), which is assumedto be independent for each note for the sake of compu-tational efﬁciency. The probability values for incorrectpitches represent the possibility and frequencies of pitcherrors. An approximate distribution ofPpitchi(p)has beenestimated previously (Eq. (30) of [7]) with piano perfor-mance data, where the probability of pitch errors is as-sumed to be uniform for all score notes.Finally we consider the description of onset times.A natural assumption of time translational invariance re-quires the model to be only dependent of time intervals.There are (at least) three different kinds of time intervalsProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 393relevant in locally describing onset times of music perfor-mance: (IOI1) The time interval between the ﬁrst notesof succeeding events, which is typically the duration of anevent, (IOI2) the time interval between the ﬁrst note of anevent and the last note of its previous event, and (IOI3) thetime interval between succeeding performed notes withinan event (Fig. 1). Assuming that the probability of thesetime intervals depends only on the current and previousstates for simplicity and computational efﬁciency, it hasthe formP(\u0000t|in\u00001,in,v)(=I O I 1,IOI2,IOI3) where\u0000tandvdenote the relevant time interval and the tempo.Based on the experience that time interval IOI3 is mostlydependent on the relevant event and almost independentof tempo and other contexts, we further simplify the func-tional form asPIOI3(\u0000t|in). Note that the time intervalsIOI1 and IOI2 are not independent quantities if we retainall historical information on time, but they have differentimportance when we take the Markovian description ex-plained below.2.4 Autoregressive hidden semi-Markov modelThe integration of the models in Secs. 2.2 and 2.3 can bedescribed in terms of an extension of the HSMM. In one ofequivalent formulations [13] (also Sec. 3.3 of Ref. [11]), asemi-Markov model can be represented as a Markov modelon an extended state space. The extended state space isindexed by a pair(i, s)of the top statei(correspondingto a musical event) and a counter of performed notess=1,2,...2with a transition probabilityP(in,sn|in\u00001,sn\u00001)=\u0000sn,1P(in|in\u00001)Pexitin\u00001(sn\u00001)+\u0000sn,sn\u00001+1\u0000in,in\u00001⇣1\u0000Pexitin\u00001(sn\u00001)⌘(1)wherePexiti(s)=di(s)/⌃1s0=sdi(s0).(2)Here\u0000in Eq. (1) denotes Kronecker’s delta. The exitingprobability in Eq. (2) represents the probability that theperformer moves to another event given that she has al-ready playedsnotes at eventi. The ﬁrst term in the right-hand side of Eq. (1) describes the probability that the per-former moves to eventinafter having playedsn\u00001notes ofeventin\u00001. The second term describes the probability thatthe performer stays at eventinand sounds another noteafter having playedsn\u00001notes. In this way, this model de-scribes the integrated process of performer’s progressionin the score and the production of performed notes.The pitches and onset times of the performed notescan be described with output probabilities associated withthis semi-Markov process. We assume the statistical in-dependence of pitch and onset time for simplicity. Theoutput probability of pitch is given byP(pn|in,sn)=Ppitchin(pn).The output probability of the onset time of then-th note2Remark: In the present model,scounts the number of notes playedduring a musical event. This is not the durational time (in seconds) spenton that event, which is described with time interval IOI1....in\u00001sn\u00001insnin+1sn+1......tn\u00001tntn+1...pn\u00001pnpn+1\nFigure 2. Graphical representation of the autoregres-sive hidden semi-Markov model of symbolic music perfor-mance. The stochastic variables are explained in the text.is given asP(tn|in,sn,in\u00001,sn\u00001,v,t1:n\u00001)=(w1PIOI1+w2PIOI2,sn= 1;PIOI3,sn6=1(3)wherePIOI1=PIOI1(tn\u0000tn\u0000s[n\u00001]|in,in\u00001,v),(4)PIOI2=PIOI2(tn\u0000tn\u00001|in,in\u00001,v),(5)PIOI3=PIOI3(tn\u0000tn\u00001|in)\u0000inin\u00001.(6)(Here we have writtens[n\u00001] =sn\u00001to display the equa-tion with clarity.) The three cases correspond to the threekinds of time intervals explained in Sec. 2.3. Because bothprobabilities for IOI1 and IOI2 have relevance in scorefollowing, we have used a mixture probability of them(w1+w2=1). Such output probabilities with condi-tional dependence on the previous outputs have been con-sidered in some studies on speech processing, and we callthe model autoregressive semi-Markov model based on theconvention of previous studies [12]. A graphical represen-tation of the model is given in Fig. 2.The distributionsPIOI1,PIOI2, andPIOI3can be esti-mated by analysing performance data. The functionsPIOI2andPIOI3have previously been estimated with piano per-formance data [9]. It has been shown there that, in the mostimportant case thatin=in\u00001+1(straight transition to thenext event),PIOI2(\u0000t|i+1,i ,v)is well approximated by aCauchy distribution of the formCauchy(\u0000t;v(⌧endi\u0000⌧i)\u0000devi,0.4s ).(7)HereCauchy(x;µ,\u0000)denotes the Cauchy distributionwith medianµand width\u0000, and⌧iis the onset score timeof eventi,⌧endiis the score time after which no new onsetsof eventican occur, anddevidescribes the ‘stolen time’of eventiwhose expectation value is given as the numberof short appoggiaturas and arpeggiated notes times the av-erage IOI of the corresponding notes. Using this result, we394 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015can estimatePIOI1in the case thatin=in\u00001+1asPIOI1(\u0000t|i+1,i ,v)=C a u c h y (\u0000t;v⌫i,0.4s )(8)where⌫i=⌧i+1\u0000⌧iis the note value of eventi. The dis-tributionPIOI3was estimated with measurements on IOIsof chordal notes and ornaments (see Secs. 3.3 and 4.2 of[9]).Finally, tempovnis estimated online with a sepa-rate model, for which we use a method based on switch-ing Kalman ﬁlter (see Sec. 3.4 of [9]). In summary thecomplete-data probabilityP(i1:n,s1:n,t1:n,p1:n)is givenas the following recursive product:nYm=1hP(tm|im,sm,im\u00001,sm\u00001,vm\u00001,t1:m\u00001)·P(im,sm|im\u00001,sm\u00001)Ppitchim(pm)i.(9)3. COMPARISON WITH OTHER MODELS3.1 Relation to the HMM-based modelSo far the state-of-the-art method for symbolic score fol-lowing is developed with a performance model based ona standard HMM [9]. The current model can be seen asan extension of this performance model in two ways. Firstthe transition probability of the HMM is realised as a spe-cial case of the transition probability in Eq. (1) with exitingprobabilitiesPexiti(s)constant ins. Speciﬁcally, it is givenas the inverse of the expected number of performed notesin eventi. As is well known, this constraint leads to a geo-metrically distributeddi(s)with a peak ats=1, which is abad approximation for a large chord or a long trill/tremolo.The second difference is the structure of output proba-bilities for onset times. In the standard HMM, the Marko-vian condition is assumed on the output probability of on-set times. Thus the model describes only time intervalsIOI2 and IOI3, and the probability distribution for IOI1 inEq. (3) is ignored. In other words, the IOI output probabil-ity of the HMM assumesw1=0andw2=1in that equa-tion. This means that the total duration of a trill/tremolo oran arpeggio is poorly captured with the HMM.These differences have important effects when the mod-els are applied to score following. For score following,the pitch information is generically most important. Whenthere are musical events with similar pitch contents in suc-cession, however, the information on onset times and thenumber of performed notes play more signiﬁcant roles incorrectly matching notes. For example, to correctly matchperformed notes of succeeding trills/tremolos, the numberof notes and the duration of each trill/tremolo are impor-tant viewpoints. Since they are not well captured in theHMM, the autoregressive HSMM would work better in thiscase. Similar situations arise for successions of arpeggios,where the time intervals IOI2 and IOI3 are largely vari-able among realisations. On the other hand, the time inter-vals IOI1 and IOI2 are almost same for successive normalchords and these IOIs carry much information necessary tocluster them. Thus the models are expected to have similareffects for passages without ornaments.3.2 Comparison with the preprocessing methodTo solve the problems with ornaments for score follow-ing, a preprocessing method has been proposed long ago[14]. The idea is to preprocess performed notes so thatornamental notes are not sent to the matching module di-rectly. While the method can work for scores with not-heavy polyphonic ornamentation and performances withinfrequent errors, the preprocessing can fail when there areerrors or unexpected repeats or skips near ornaments. Be-cause a direct comparison showed that the HMM outper-formed the preprocessing method for piano performanceswith errors, repeats, and skips [9], we compare our modelonly with the HMM in Sec. 4.3.3 Computational costFor score following, we ﬁnd the most probable hidden statesequence given the input performance. In order to realisereal-time processing, the computational cost of the estima-tion algorithm must be sufﬁciently small. We here comparethe present model and the HMM discussed in Sec. 3.1 interms of the computational cost.The Viterbi algorithm can be applied for HMMs to es-timate states. Let us denote the product of the transi-tion probability and the output probability asaij(o)=P(j|i)·P(o|i, j)whereorepresents pitch and onset time.The Viterbi update equation can be expressed as the fol-lowing recursive equationˆpN(iN)⌘maxi1,...,iN\u00001NYn=1ain\u00001in(on)\u0000(10)= maxiN\u00001⇥ˆpN\u00001(iN\u00001)aiN\u00001iN(oN)⇤.(11)The number of states isNsince a state corresponds to amusical event in the score. If we allow arbitrary progres-sions in the score including repeats and skips, a direct ap-plication of the Viterbi algorithm requiresO(N2)compu-tations of probability for each update. When the probabil-ity matrixaij(o)can be represented as a sum of a bandmatrix↵ijof widthDand an outer product of two vec-torsSiandrj, the computational complexity can be re-duced toO(DN)with a recombination method [7]. Intu-itively,↵ijdescribes probabilities corresponding to transi-tions between neighbouring states, which have larger prob-abilities, andSiandrjrepresent probabilities correspond-ing to large repeats and skips, which typically have verysmall probabilities. Substitutingaij(o)=↵ij+Sirjinto Eq. (11), we see↵ijinducesO(DN)complexity andSirjinducesO(N)complexity by a recombination. Thissimpliﬁed transition probability matrix is used in previousstudies to enable real-time processing for long scores.It is clear from the formulation of the autoregressiveHSMM in Sec. 2.4 that the standard Viterbi algorithm canalso be applied to the model. In practice, we put an up-per bound on the number of performed notessmaxiforeach eventi, and the number of states of the HSMM is⌃ismaxi⌘SNwhereSis the average ofsmaxi. Becauseof the special form of transition probabilities in Eq. (1), theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 395Table 1. Error rates (%) of score following with the auto-regressive HSMM (“HSMM”), the hybrid model (“Hy-brid”), and the HMM [9]. The ﬁrst four pieces indicateCouperin’s Allemande`a deux clavecins, the solo piano partof Beethoven’s ﬁrst piano concerto, Beethoven’s secondpiano concerto, and Chopin’s second piano concerto [9],and the last two pieces are explained in the text.Piece # Notes HSMM Hybrid HMMCouperin 1763 5.50 6.02 6.66Beethoven 1 17587 3.16 3.13 3.16Beethoven 2 5861 2.01 2.20 2.35Chopin 16241 9.22 9.22 11.1Debussy 3294 3.64 3.58 4.66Tchaikovsky 2245 0.40 0.40 4.55computational complexity for one Viterbi update is generi-callyO(SN2). When we apply the recombination methodin Ref. [7], the complexity can be reduced toO(DSN)for the outer-product type transition probability. Note thatthe widthDin the top-level transition probability matrixinducesSDtransitions between HSMM states. Conse-quently the computational cost of the model is aboutStimes larger than its reduced HMM. For example, if wesetsmaxias twice the number of expected notes per event,S'3–10 for a score with a modest degree of polyphony,and it increases if there are many large chords or longtrills/tremolos.3.4 Hidden hybrid Markov/semi-Markov modelAs discussed in Sec. 3.1, there are reasons that the presentmodel yields better results for score following than theHMM, but it is at the cost of increased computational cost,which is unwanted for long scores. On the other hand,most of the musical events in scores are normal chords(or single notes) for which the HMM already yields goodresults. Therefore if we combine the HMM state repre-sentation for normal chords and the autoregressive HSMMstate representation for other ornamented events, it wouldbe possible to obtain an improved score-following algo-rithm with minimal increase in computational cost. Such acombination of HMM and HSMM can be achieved in theframework of hidden hybrid Markov/semi-Markov model[5, 15]. In the hybrid model, normal chords are rep-resented with HMM states and other events (i.e. trill,tremolo, arpeggio, short appoggiatura, and after notes) arerepresented with HSMM states. For this model the com-putational complexity of the Viterbi algorithm takes thesame form as the autoregressive HSMM, by substitutingsmaxi=1for HMM states inS=⌃ismaxi/N.4. COMPARING THE ACCURACY OF SCOREFOLLOWINGTo evaluate and compare the discussed models with re-spect to the accuracy of score following, we implementedTable 2. Number of mismatched notes of various types.Each type is explained in the text. The same abbreviationsfor the models as in Table 1 are used.Type # Notes HSMM Hybrid HMMTrill 8159 282 281 508Tremolo 2603 115 115 151Arpeggio 1081 36 33 127Otherornaments2401 340339362Other 32030 1580 1599 1673three score-following algorithms based on the autoregres-sive HSMM (Sec. 2.4), the hybrid model (Sec. 3.4), and theHMM [9], and run these algorithms for music performancedata containing various ornaments. In addition to the pianoperformance data used in Ref. [9] which contains perfor-mance errors, repeats and skips, we used collected pianoperformances of passages in Debussy’s En Blanc et Noirwith successions of tremolos (the ﬁrst piano part in the sec-ond movement) and the solo piano part of Tchaikovsky’sﬁrst piano concerto with his typical successions of widearpeggios (the last section of the second movement).The additional parameters\u0000ifor the autoregressiveHSMM and the hybrid model were set as follows:\u0000i=0.4speakifor trills and tremolos and\u0000i=1otherwise. Themixture weights for the output probability for time inter-vals IOI1 and IOI2 were set asw1=w2=1/2. Theseparameters were used as a benchmark and there is a roomfor further optimisation.For the evaluation measure, we calculated the error rate,which is deﬁned as the proportion of mis-matched notesto the total number of performed notes. There were per-formed notes that are difﬁcult to associate with any scorenotes even for humans, which naturally appear in real data.While they were included in the input data, they were notused in the calculation of error rates. Results are shown inTable 1, where we see that the autoregressive HSMM andthe hybrid model had similar accuracies, and the HMM hadthe worst accuracy overall. (Slight differences in the valuesfor the HMM compared to those in Ref. [9] are mainly dueto slight corrections of the implementation.) For detailederror analysis, we list the frequencies of classiﬁed match-ing errors in Table 2. Here the numbers indicate the totalnumber of matching errors in the whole data for each type.Ornaments are classiﬁed into the ﬁrst four types, and othernotes are gathered in the last type. Signiﬁcant reductionof matching errors is observed in the ﬁrst three types (trill,tremolo, and arpeggio), and other types of matching errorsare also reduced but rather slightly in the reduction rate.Two example results of score following are shown inFig. 3, which represent typical situations where the auto-regressive HSMM worked better than the HMM. In theﬁrst example, the passage includes a succession of tremo-los with similar pitch contents. We see some of the mis-matched notes with the HMM are correctly matched withthe autoregressive HSMM. Similarly the mismatched notes396 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015ScorePerformance(a) A passage from Debussy’s En Blanc et Noir with the autoregressiveHSMM.ScorePerformance(b) Same as (a) with the HMM.ScorePerformance(c) A passage from Tchaikovsky’s ﬁrst piano concerto with the autore-gressive HSMM.ScorePerformance(d) Same as (c) with the HMM.Figure 3. Example results of score following with theautoregressive HSMM and the HMM [9]. Mismatchednotes are indicated with bold red lines.Table 3. Averaged computation time (ms) required for oneViterbi update. The same abbreviations for the models andthe musical pieces as in Table 1 are used.Piece HSMM Hybrid HMMCouperin 1.6 1.1 0.3Beethoven 1 5.9 2.9 1.1Beethoven 2 7.0 3.0 1.6Chopin 7.1 3.5 1.2Debussy 0.9 0.8 0.1Tchaikovsky 1.2 1.0 0.1with the HMM are all correctly matched with the autore-gressive HSMM for a succession of wide arpeggios in thesecond example. These results are consistent with the dis-cussion in Sec. 3.1.We also measured the required computation time (Ta-ble 3). The computation time for each Viterbi update isconstant over time, and the algorithms were run on a lap-top with moderate computation power. The results con-ﬁrm our expectation that the use of hybrid model for scorefollowing has practical advantages over the autoregressiveHSMM in the computation time and the HMM in the ac-curacy.5. CONCLUSIONWe explained reasons that the present model of sym-bolic music performance based on autoregressive HSMMis more advantageous for score following than previouslystudied HMMs, and we have conﬁrmed this empirically bycomparing the accuracy of score following and analysingthe matching errors. Because a semi-Markov model canbe seen as a Markov model with an extended state space aswe have explained, we can apply to the present model themethods for HMMs to improve score following [7, 16]. Inparticular, this is important to reduce matching errors oc-curring after repeats and skips and those due to reorderednotes in the performance, which were the main factors ofremaining errors.It would be interesting to apply the present model formusic/rhythm transcription and related problems. Becausethe model describes both the total duration and the internaltemporal structure of ornaments, it would be possible todetect ornaments from performances without a score andintegrate the results into music transcription.6. ACKNOWLEDGEMENTSThis work is partially supported by NII MOU Grantin ﬁscal year 2014 and Grant-in-Aid for Scientiﬁc Re-search from Japan Society for the Promotion of Science,No. 26240025 (S.S. and N.O.) and No. 25880029 (E.N.).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 3977. REFERENCES[1]R. Dannenberg, “An on-line algorithm for real-time ac-companiment,”Proc. ICMC, pp. 193–198, 1984.[2]B. Vercoe, “The synthetic performer in the context oflive performance,”Proc. ICMC, pp. 199–200, 1984.[3]N. Orio, S. Lemouton and D. Schwarz, “Score fol-lowing: State of the art and new developments,”Proc. NIME, pp. 36–41, 2003.[4]B. Pardo and W. Birmingham, “Modeling form for on-line following of musical performances,”Proc. of the20th National Conf. on Artiﬁcial Intelligence, 2005.[5]A. Cont, “A coupled duration-focused architecture forrealtime music to score alignment,”IEEE Trans. PAMI,32(6), pp. 974–987, 2010.[6]A. Arzt, G. Widmer and S. Dixon, “Adaptive distancenormalization for real-time music tracking,”Proc. EU-SIPCO, pp. 2689–2693, 2012.[7]E. Nakamura, T. Nakamura, Y . Saito, N. Ono andS. Sagayama, “Outer-product hidden Markov modeland polyphonic MIDI score following,”JNMR,43(2),pp. 183–201, 2014.[8]P. Cuvillier and A. Cont, “Coherent time modelingof semi-Markov models with application to real-timeaudio-to-score alignment,”Proc. IEEE MLSP, 6 pages,2014.[9]E. Nakamura, N. Ono, S. Sagayama and K. Watanabe,“A stochastic temporal model of polyphonic MIDI per-formance with ornaments,” to appear inJNMR, 2015.[10]N. Orio and F. D´echelle, “Score following using spec-tral analysis and hidden Markov models,”Proc. ICMC,pp. 1708–1710, 2001.[11]S.-Z. Yu, “Hidden semi-Markov models,”Artiﬁcial In-telligence,174, pp. 215–243, 2010.[12]J. Bilmes, “Graphical models and automatic speechrecognition,” inMathematical foundations of speechand language processing(Springer New York),pp. 191–245, 2004.[13]M. Russel and A. Cook, “Experimental evaluation ofduration modelling techniques for automatic speechrecognition,”Proc. ICASSP, pp. 2376–2379, 1987.[14]R. Dannenberg and H. Mukaino, “New techniques forenhanced quality of computer accompaniment,”Proc.ICMC, pp. 243–249, 1988.[15]Y . Gu´edon, “Hidden Hybrid Markov/Semi-MarkovChains,”Computational Statistics and Data Analysis,49, pp. 663–688, 2005.[16]E. Nakamura, Y . Saito, N. Ono and S. Sagayama,“Merged-output hidden Markov model for score fol-lowing of MIDI performance with ornaments, desyn-chronized voices, repeats and skips,”Proc. JointICMC|SMC 2014, pp.1185-1192, 2014.398 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Beat and Downbeat Tracking Based on Rhythmic Patterns Applied to the Uruguayan Candombe Drumming.",
        "author": [
            "Leonardo O. Nunes",
            "Martín Rocamora",
            "Luis Jure",
            "Luiz W. P. Biscainho"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415728",
        "url": "https://doi.org/10.5281/zenodo.1415728",
        "ee": "https://zenodo.org/records/1415728/files/NunesRJB15.pdf",
        "abstract": "Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or poly- rhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper pro- poses a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of an- notated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper.",
        "zenodo_id": 1415728,
        "dblp_key": "conf/ismir/NunesRJB15",
        "keywords": [
            "Computational analysis",
            "rhythmic/metrical structure",
            "music information retrieval",
            "explicit modeling",
            "characteristic rhythmic patterns",
            "beat-tracking algorithms",
            "syncopated or poly-rhythmic music",
            "Uruguayan Candombe drumming",
            "afro-rooted rhythm",
            "basic rhythmic patterns"
        ],
        "content": "BEAT AND DOWNBEAT TRACKING BASED ON RHYTHMIC PATTERNSAPPLIED TO THE URUGUAYAN CANDOMBE DRUMMINGLeonardo NunesATL-Brazil, Microsoftlnunes@microsoft.comMart´ın RocamoraUniversidad de{rocamora,Luis Jurela Rep´ublicalj}@eumus.edu.uyLuiz W. P. BiscainhoFederal Univ. of Rio de Janeirowagner@smt.ufrj.brABSTRACTComputational analysis of the rhythmic/metrical structureof music from recorded audio is a hot research topic inmusic information retrieval. Recent research has exploredthe explicit modeling of characteristic rhythmic patterns asa way to improve upon existing beat-tracking algorithms,which typically fail on dealing with syncopated or poly-rhythmic music. This work takes the Uruguayan Candombedrumming (an afro-rooted rhythm from Latin America) asa case study. After analyzing the aspects that make thismusic genre troublesome for usual algorithmic approachesand describing its basic rhythmic patterns, the paper pro-poses a supervised scheme for rhythmic pattern trackingthat aims at ﬁnding the metric structure from a Candomberecording, including beat and downbeat phases. Then itevaluates and compares the performance of the methodwith those of general-purpose beat-tracking algorithmsthrough a set of experiments involving a database of an-notated recordings totaling over two hours of audio. Theresults of this work reinforce the advantages of trackingrhythmic patterns (possibly learned from annotated music)when it comes to automatically following complex rhythms.A software implementation of the proposal as well as theannotated database utilized are available to the researchcommunity with the publication of this paper.1. INTRODUCTIONMeter plays an essential role in our perceptual organiza-tion of music. In modern music theory, metrical structureis described as a regular pattern of points in time (beats),hierarchically organized in metrical levels of alternatingstrong and weak beats [15, 16]. The metrical structureitself is not present in the audio signal, but is rather in-ferred by the listener through a complex cognitive process.Therefore, a computational system for metrical analysisfrom audio signals must, explicit or implicitly, make im-portant cognitive assumptions. A current cognitive modelproposes that, given a temporal distribution of events, ac\u0000Leonardo Nunes, Mart´ın Rocamora, Luis Jure, Luiz W.P. Biscainho. Licensed under a Creative Commons Attribution 4.0 Inter-national License (CC BY 4.0).Attribution:Leonardo Nunes, Mart´ınRocamora, Luis Jure, Luiz W. P. Biscainho. “Beat and Downbeat Track-ing Based on Rhythmic Patterns Applied to the Uruguayan CandombeDrumming”, 16th International Society for Music Information RetrievalConference, 2015.competent listener infers the appropriate metrical structureby applying two sets of rules: Metrical Well-FormednessRules (MWFR), which deﬁne the set of possible metricalstructures, and Metrical Preference Rules (MPR), whichmodel the criteria by which the listener chooses the moststable metrical structure for a given temporal distributionof events [15]. While not strictly universal, most of theMWFR apply for a variety of metric musics of differentcultures [23]; MPR, on the other hand, are more subjectiveand, above all, style-speciﬁc. A listener not familiar witha certain type of music may not be able to decode it prop-erly, if its conventions differ substantially from usual tonalmetrical structures.This is why the computational analysis of rhythmic/met-rical structure of music from audio signals remains a dif-ﬁcult task. Most generic algorithms follow a bottom-upapproach with little prior knowledge of the music underanalysis [6,7,13], often including some kind of preferencerules—e.g. by aligning beats with onsets of stronger and/orlonger events [15]. Therefore, they usually fail on process-ing syncopated or polyrhythmic music, for instance, thatof certain Turkish, Indian or African traditions [22].For this reason, other approaches prefer a top-down pro-cess guided by high-level information, such as style-speciﬁccharacteristics [11]. Given that listeners tend to group mu-sical events into recurrent rhythmic patterns which givecues for temporal synchronization, the explicit modelingof rhythmic patterns has recently been proposed as a wayto improve upon existing beat-tracking algorithms [14, 24,25]. The identiﬁcation of challenging music styles and thedevelopment of sytle-speciﬁc algorithms for meter analy-sis and beat-tracking is a promising direction of researchto overcome the limitations of existing techniques.In this work, an afro-rooted rhythm is considered as acase of study: the Candombe drumming in Uruguay. Moti-vated by the fact that some characteristics of Candombe arechallenging for most of the existing rhythm analysis algo-rithms, a supervised scheme for rhythmic pattern trackingis proposed, aiming at ﬁnding the metric structure from anaudio signal, including the phase of beats and downbeats.The performance of the proposed method is assessed overa database of recordings annotated by an expert.The next section provides a brief description of the Can-dombe rhythm. Then, the proposed method for rhythmicpattern matching is presented in Section 3. Experimentsand results are described in Section 4. The paper ends withsome critical discussion and directions for future research.2642. AFRO-URUGUAYAN CANDOMBE2.1 Candombe drumming in contextCandombe is one of the most characteristic features of Uru-guayan popular culture, practiced by thousands of people.Its rhythm inﬂuenced and was incorporated into variousgenres of popular music. However, little known abroad, itmay be difﬁcult to understand for unfamiliar listeners.Although originated in Uruguay, Candombe has its rootsin the culture brought by the African slaves in the 18th cen-tury. It evolved during a long historical process, graduallyintegrating European immigrants and now permeating thewhole society [1, 8]. Candombe drumming, with its dis-tinctive rhythm, is the essential component of this tradi-tion. Its most characteristic manifestation is thellamadade tambores, a drum-call parade, when groups of playersmeet at speciﬁc points in the city to play while marchingon the street (Figure 1).\nFigure 1. Group of Candombe drummers.The instrument of Candombe is calledtambor(“drum”in Spanish), of which there are three different sizes:chico(small),repique(medium) andpiano(big). Each type hasa distinctive sound (from high to low frequency range) andits own speciﬁc rhythmic pattern. All three are played withone hand hitting the skin bare and the other with a stick,which is also used to hit the shell when playing theclavepattern. The minimal ensemble of drums (cuerda de tam-bores) must have at least one of each of the three drums;during allamada de tamboresit usually consists of around20 to 60 drums. While marching, the players walk for-ward with short steps synchronized with the beat ortactus;this movement, while not audible, is very important for theembodiment of the rhythm. Figure 1 shows in the ﬁrst row,from the front backwards, arepique,achicoand apiano.2.2 Rhythmic patterns and metrical structureThe Candombe rhythm orritmo de llamadaresults fromthe interaction between the patterns of the three drums. Anadditional important pattern is theclave, played by all thedrums as an introduction to and preparation for the rhythm(see Figure 21).The pattern of thechicodrum is virtually immutable,and establishes the lowest level of the metrical structure1Lower and upper line represent hand and stick strokes respectively.\nchicoclaverepiquepianoFigure 2. Interaction of mainCandombepatterns, and thethree levels of the resulting metric structure.Repiqueandpianopatterns are shown in a simpliﬁed basic form.(tatum). The period of the pattern is fourtatums, conform-ing the beat ortactuslevel in the range of about 110 to 150beats per minute (BPM). The interaction ofchicoandclavehelps to establish the location of the beat within thechicopattern (otherwise very difﬁcult to perceive), and deﬁnes ahigher metric level of four beats (sixteentatums).The resulting metrical structure is a very common one:a four-beat measure with a regular subdivision in 16tatums.However, two characteristic traits link the rhythmic con-ﬁguration of Candombe with the Afro-Atlantic music tra-ditions, differentiating it from usual tonal rhythms: 1) thepattern deﬁning the pulse does not articulate thetatumthatfalls on the beat, and has instead a strong accent on thesecond; 2) theclavedivides the 16-tatumcycle irregularly(3+3+4+2+4), with only two of its ﬁve strokes coincidingwith the beat. This makes the Candombe rhythm difﬁcultto decode for both listeners not familiar with it and genericbeat-tracking algorithms (see Table 1). The strong phe-nomenological accents displaced with respect to the metricstructure add to the difﬁculty.Therepiqueis the drum with the greatest degree of free-dom. During thellamadait alternates between theclavepattern and characteristically syncopated phrases. Figure 2shows its primary pattern, usually varied and improvisedupon to generate phrases of high rhythmic complexity [12].Thepianodrum has two functions: playing the base rhythm(piano base), and occasional more complex ﬁgurations akinto therepiquephrases (piano repicado). The pattern in Fig-ure 2 is a highly abstracted simpliﬁcation of thepiano base.It can be seen that it is essentially congruent with theclavepattern, and when correctly decoded it permits the infer-ence of the whole metric structure. In real performances,however, much more complex and varied versions of thispattern are played. It has been shown [21] that the anal-ysis ofpianopatterns may elicit the identity of differentneighborhoods (barrios)2and individual players.3. RHYTHMIC PATTERN MATCHINGIn this section, a rhythmic/metric analysis algorithm thatmatches a given rhythmic accentuation pattern to an audiosignal is described. It tries to ﬁnd the time of occurrence2The three more important traditional styles areCuareim(orbarrioSur),Ansina(orbarrio Palermo) andGaboto(orbarrio Cord´on).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 265of eachtatumknowing its expected accentuation inside thepattern, thus being able to track not only the beat but alsoother metrical information. Initially, a tempo estimationalgorithm is employed to obtain the beat period (tempo),assumed to be approximately stable throughout the signal.Then, the main algorithm is used to ﬁnd the phase of theaccentuation pattern within the observed signal.3.1 Audio feature extractionFor audio feature extraction, this work adopts a typical ap-proach based on the Spectral Flux. First, the Short-TimeFourier Transform of the signal is computed and mapped tothe MEL scale for sequential windows of 20 ms duration inhops of 10 ms. The resulting sequences are differentiated(via ﬁrst-order difference) and half-wave rectiﬁed.For tempo estimation, the feature values are summedalong all MEL sub-bands, in order to take into accountevents from any frequency range.Since its pattern is the most informative on bothtactusbeat and downbeat locations, the rhythmic pattern trackingis tailored towards thepiano(i.e. the lowest) drum. There-fore, the accentuation feature used for pattern matching isobtained by summing the Spectral Flux along the lowestMEL sub-bands (up to around 200 Hz) only. This functionis normalized by the8-norm of a vector containing its val-ues along±2estimatedtatumperiods around the currentframe. The resulting feature value is expected to be closeto one if a pulse has been articulated and close to zero oth-erwise. In addition, it also carries some information on thetype of articulation. For instance, an accented stroke pro-duces a higher feature value compared to a mufﬂed one,since in the former case the spectral change is more abrupt.3.2 Tempo EstimationFor tempo estimation, this work adopts a straightforwardprocedure based on locating the maximum of a suitablydeﬁned similarity function. As proposed in [20], the ba-sic function is the product between the auto-correlationfunction and the Discrete Fourier Transform of the featurescomputed for the whole signal. The result is weighted bythe function described in [17]. The period associated withthe largest value in this weighted similarity function is se-lected as the tempo of the signal. After the tempo is ob-tained, thetatumperiod used for pattern tracking can becomputed just by dividing the beat period by 4. Thistatumperiod is then used to deﬁne the variables in the patterntracking algorithm as described in the next sections.3.3 Variables deﬁnitionIn order to perform its task, the algorithm employs two dis-crete random variables. The ﬁrst one, calledtatum coun-ter,ck, counts how many frames have passed since the lasttatumhas been observed at framek. Assuming an esti-matedtatumperiod of⌧frames, thenck2{0,1,...,⌧\u00001+\u0000c}, where\u0000cis a parameter that allows for possibletiming inaccuracies in thetatum. The second, calledpat-tern index,ak, indicates the position inside a given rhyth-mic pattern at framekin the range{0,1,...,M\u00001},whereMis the length of the rhythmic pattern intatums.The rhythmic pattern will be expected to deﬁne a series ofaccents or lacks of accent in thetatums. Time evolution ofthese two variables will be described in the next section,where it is assumed that the sampling rate of the feature(typically less than 100 Hz) is much lower than that of theoriginal signal (usually 44.1 kHz). The model describesthe accentuation feature extracted at framekas a randomvariable,yk, with actual observed (extracted) valueyk.3.4 State TransitionIn this section, the probabilities of each value for the tworandom variables at framekgiven past frames are described.A ﬁrst-order Markov model will be assumed for the jointdistribution of the random variables, i.e., the probability ofeach possible value of a random variable at framekde-pends only on the values assumed by the variables at theprevious framek\u00001. Using this assumption, the two ran-dom variables will constitute a Hidden Markov Model [18].Thetatumcounter variable, as previously mentioned,counts how many frames have passed since the lasttatum.The stateck=0is considered the “tatum state” and in-dicates that atatumhas occurred at framek. This randomvariable is closely related to thephase stateproposed in [5]for beat tracking. Only two possible transitions from framek\u00001to framekare allowed: a transition to the “tatumstate” or an increment in the variable. The transition to the“tatum state” depends on both the past value of the vari-able and the (known)tatumperiod. The closer the valueof the variable is to thetatumperiod, the more probableis the transition to the “tatum state.” Mathematically, it ispossible to writepck(ck|ck\u00001)=8><>:h[ck\u00001\u0000⌧],ifck=01\u0000h[ck\u00001\u0000⌧],ifck=ck\u00001+10,otherwise,(1)whereh[.]is a tapering window withh[n]=0for|n|>\u0000cthat models possible timing inaccuracies on thetatum, andPnh[n]=1. Currently, a normalized Hann window isemployed to penalize farther values. The value\u0000c=2was set for the reported experiments, indicating that inac-curacies of up to 50 ms are tolerated by the algorithm.Since the accentuation pattern is deﬁned in terms of thetatum, its time evolution will be conditioned by the patternevolution. Assuming that the pattern indicates the expectedaccentuation of the nexttatum, the variable should onlychange value when a “tatum state” has been observed, in-dicating that a different accentuation should be employedby the observation model (described in the next section).Hence, mathematicallypak(ak|ck\u00001,ak\u00001)=8><>:1,if(ak=ak\u00001\u00001)^(ck\u00001= 0)1,if(ak=ak\u00001)^(ck\u000016= 0)0,otherwise,(2)266 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015where^is the logical AND,\u0000denotes a modulo-Msum-mation, andMis the length of the accentuation pattern. Ascan be gathered, given the previoustatumcounter value,the pattern index becomes deterministic, with its next valuecompletely determined by its value at the previous frameand the value of thetatumcounter. The transitions for thisvariable are inspired on the ones used in the family of al-gorithms based on [24] (i.e. [2,10,14]), except for deﬁningthe pattern in terms oftatumsinstead of an arbitrary unit.3.5 Observation ModelThis section describes the likelihood ofckandakgiven anobserved accentuationykin the signal. The main idea is tomeasure the difference between the expected accentuation(provided by the rhythmic pattern) and the observed one.The larger the difference, the less probable the observation.If the accentuation pattern is a vectorA2RM⇥1con-taining the expected feature values, then at framekthelikelihood forck=0(“tatum state”) can be deﬁned aspyk(yk|ck,ak)=N\u0000t(yk\u0000Aak),(3)whereN\u0000t(.)is a Gaussian function with zero mean andvariance\u00002tused to model possible deviations between ex-pected and observed accents. Forck6=0, the likelihood isgiven by:pyk(yk|ck,ak)=N\u0000d(yk),(4)whereN\u0000dis a zero-mean Gaussian with variance equal to\u00002d. Hence, the closer to zero the feature, the more proba-ble the observation. This is similar to the non-beat modeladopted in [5], and is not found in [14, 24].In the reported experiments,\u0000t=\u0000d=0.5, thus allow-ing for a reasonable overlap between expected and actualobserved values.3.6 InferenceA summary of the proposed model for rhythmic patterntracking can be viewed in Figure 3, where the statisticaldependencies among the variables are explicited. Differ-ent inference strategies can be employed to ﬁnd the mostprobable pattern index andtatumcounter values given theobserved accentuation [18]. In this work, the well-knownViterbi algorithm [18, 24] is employed to ﬁnd the mostprobable path among all possible combinations of valuesof each random variable given the observed featuresyk.\nak\u00001yk\u00001akykck\u00001ckFigure 3. Graphical representation of the statistical depen-dency between random variables and observations.At last, a uniform prior is chosen forc0anda0indi-cating that the counter and the pattern can start with anypossible value in the ﬁrst frame.4. EXPERIMENTS AND RESULTSA set of experiments was devised to assess the performanceof the proposed rhythmic pattern tracking system with re-spect to the problems of estimating the rate and phase ofbeats and downbeats, using a database of manually labeledCandombe recordings. Four state-of-the-art beat-trackingalgorithms [6, 7, 13, 19] were included in the experimentsin order to evaluate how challenging the rhythm at hand isfor typical general-purpose approaches.Two different strategies are explored: the rhythmic pat-terns to follow are either informed to the algorithm basedon a priori musical knowledge about the rhythm, or learnedfrom the labeled database itself.4.1 DatasetA dataset of Candombe recordings, totaling over 2 hoursof audio, was compiled and annotated for this work and itis now released to the research community.3It comprises35 complete performances by renowned players, in groupsof three to ﬁve drums. Recording sessions were conductedin studio, in the context of musicological research over thepast two decades. A total of 26tamborplayers took part,belonging to different generations and representing all theimportant traditional Candombe styles. The audio ﬁles arestereo with a sampling rate of 44.1 kHz and 16-bit preci-sion. The location of beats and downbeats was annotatedby an expert, adding to more than 4700 downbeats.4.2 Performance measuresSince tempo estimation is only an initialization step of therhythmic pattern tracking task, whose overall performancewill be examined in detail, it sufﬁces to mention that theestimated tempo was within the interval spanned by the an-notated beat periods along each of the ﬁles in the database,thus providing a suitable value for the respective variable.Among the several objective evaluation measures avail-able for audio beat tracking [4] there is currently no con-sensus over which to use, and multiple accuracies are usu-ally reported [2, 3]. In a recent pilot study, the highest cor-relation between human judgements of beat tracking per-formance and objective accuracy scores was attained forCMLt and Information Gain [3].In this work CMLt, AMLt and F-measure were adopted,as their properties are well understood and were consid-ered the most suitable for the current experiments. Thenon-inclusion of Information Gain was based on the ob-servation that it yielded high score values for estimatedbeat sequences that were deﬁnitely not valid. Speciﬁcally,in several instances when the beat rate (or a multiple ofit) was precisely estimated, even if the beat phase was re-peatedly misidentiﬁed, the Information Gain attained highvalues while other measures such as CMLt or F-measurewere coherently small. In the following, a brief description3Available fromhttp://www.eumus.edu.uy/candombe/datasets/ISMIR2015/.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 267of the adopted metrics4is provided (see [4] for details),along with the values selected for their parameters.The CMLt measure (Correct Metrical Level, continu-ity not required) considers a beat correctly estimated ifits time-difference to the annotated beat is below a smallthreshold, and if the same holds for the previous estimatedbeat. Besides, the inter-beat-interval is required to be closeenough to the inter-annotation-interval using another thresh-old. The total number of correctly detected beats is thendivided by the number of annotated beats and expressedas a percentage (0-100 %). Both thresholds are usually setto 17.5 % of the inter-annotated-interval, which was alsothe value adopted in this work. The AMLt measure (Al-lowed Metrical Levels, continuity not required) is the sameas CMLt but does not take into account errors in the metri-cal level and phase errors of half the period.The F-measure (Fmea) is the harmonic mean of preci-sion and recall of correctly detected beats, where preci-sion stands for the ratio between correctly detected beatsand the total number of estimated beats, while recall de-notes the ratio between correctly detected beats and the to-tal number of annotated beats. A beat is considered cor-rectly detected if its time-difference to the annotation iswithin±70 ms; this tolerance was kept in this work.Only CMLt and F-measure were used for assessing thedownbeat, since the loosening of metrical level and phaseconstraints in AMLt was considered inappropriate.4.3 Experiments with informed rhythmic patternsIn the ﬁrst type of experiment, the pattern to trackAis in-formed to the algorithm based on musical knowledge aboutthe rhythm, without any training or tuning to data. On onehand, this has a practical motivation: even when no labeleddata is available one could take advantage of the technique.On the other hand, it gives a framework in which musicalmodels can be empirically tested. In short, an informedrhythmic pattern based on musical knowledge is nothingbut a theoretical abstraction, and this type of experimentcould provide some evidence of its validity.To that end, based on the different ways thepianopat-tern is notated by musicology experts, a straightforwardapproach was adopted. Firstly, thepianopattern as in-troduced in Figure 2 (usually regarded as thepianoin itsminimal form) was considered. A binary patternAwas as-sembled by setting a value of 1 for thosetatumswhich areexpected to be articulated and 0 otherwise. Then, a morecomplex pattern was considered by adding two of the mostrelevant articulatedtatumswhich were missing, namely the6th and 15th, and also building the corresponding binarypattern. Hence, the binary informed patterns proposed arePattern 1:A= [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]Pattern 2:A= [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0].This is certainly an oversimpliﬁcation of the real rhyth-mic patterns, since it does not take into account the ac-cented and mufﬂed strokes that are an essential trait of a4Computed with standard settings using code athttps://code.soundsoftware.ac.uk/projects/beat-evaluation/.pianoperformance. It would be possible to encompass dy-namic variations into the informed pattern by consideringdistinct quantized values of the feature for different typeof strokes. However, the binary patterns were favoured forthe sake of simplicity and as a proof of concept.Table 15compares 4 general-purpose beat-tracking al-gorithms with the proposed algorithm using the binary in-formed patterns, and also (for conciseness) the experimentsdiscussed in the next section. Results are averaged over thewhole database and weighted by the number of beats anddownbeats of each audio ﬁle. Although the beat rate (or amultiple) is sometimes precisely estimated by the general-purpose beat-tracking algorithms, the correct metrical leveland/or the phase of the beat is usually misidentiﬁed.BEAT DOWNBEATCMLt AMLt Fmea CMLt FmeaGeneral-purposeEllis [7] 44.2 63.0 43.8 – –Dixon [6] 13.9 14.9 22.7 – –IBT [19] 9.1 27.6 16.7 – –Klapuri [13] 28.8 35.5 29.3 36.6 13.2Informed patterns– Section 4.3Pattern 1 80.2 80.5 81.3 84.7 79.1Pattern 2 79.0 81.0 79.8 81.2 77.5Learned patterns– Section 4.4 (leave-one-out)Median 79.9 79.9 80.8 82.4 76.9K-means 2 81.7 81.7 82.6 84.4 79.3K-means 5 82.5 82.5 83.6 85.2 80.6Table 1. Performance of the different algorithms considered.4.4 Experiments with learned rhythmic patternsThe labeled database allows the study of the rhythmic pat-terns actually present in real performances. There are dif-ferent possible approaches to extract a single rhythmic pat-tern to track from the annotated data. For eachtatum-gridposition in the bar-length pattern, all the feature values inthe dataset can be collected, and their distribution can bemodeled, e.g. by a GMM as in [14]. The distribution offeature values in the low-frequency range will be domi-nated by thebasepatterns of thepianodrum, albeit therewill be a considerable amount ofrepicadopatterns [21]. Inorder to cope with that, a simple model was chosen: themedian of feature values for eachtatumbeat, which is lessinﬂuenced by outliers than the mean.The problem with the median pattern is that it modelsdifferent beat positions independently. A better suited ap-proach is to group the patterns based on their similarityinto a given number of clusters, and select the centroid ofthe majority cluster as a good prototype of thebasepat-tern. This was applied in [21] to identifybasepatterns5Additional details can be found inhttp://www.eumus.edu.uy/candombe/papers/ISMIR2015/.268 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015of thepianodrum in a performance, and similarly in [10]to learn rhythmic patterns from annotated data to adapta beat-tracking model to speciﬁc music styles. Figure 4shows the patterns learned from the whole database, us-ing the median and the centroid of the majority cluster ob-tained with K-means for 2 and 5 clusters. It is remark-able that the differently learned patterns are quite similar,exhibiting the syncopated 4thtatumbeat as the most ac-cented one. The locations of articulated beats for the in-formed patterns of the previous section are also depicted,and are consistent with the learned patterns. The K-meansapproach turned out to be little sensitive to the number ofclusters, yielding similar patterns from 1 to 6.\nFigure 4. Comparison of the different patterns considered.(Median and K-means learned from the whole database.)For testing the performance of the learning approach aleave-one-out scheme was implemented and the results aredetailed in Table 1. Not surprisingly, performance is al-most the same for the different rhythmic patterns. Consid-ering different feature values instead of binary patterns didnot yield any notable performance increase.A detailed inspection of the performance attained foreach recording in the database, as depicted in Figure 5,shows there is still some room for improvement, given thatabout half-a-dozen ﬁles are deﬁnitely mistracked. Thismay indicate that the patternAto track simply does notproperly match the given performance. To check this hy-pothesis, a K-means (K=2) clustering was carried out onlywith the candidate patterns found within each target record-ing, whose tracking was then performed using the centroidof the majority cluster asA. Table 2 shows the new resultsobtained for the ﬁles with lower performance (CMLt<50%)in the dataset. Except for the ﬁrst one, performance was(sometimes notably) improved when the informed rhyth-mic pattern is the one that better matches the recording.Therefore, modeling several rhythmic patterns as in [10]can potentially improve the current results.\nFigure 5. Leave-one-out performance for each recordingof the database using the K-means pattern with K=2.BEAT DOWNBEATRecording # CMLt Fmea CMLt Fmea15 34.1 32.8 32.7 7.216 95.6 98.0 96.3 97.126 40.2 36.9 42.9 22.231 71.3 69.9 78.3 67.832 55.7 54.1 59.6 44.734 60.9 60.0 62.7 51.7Table 2. Scores attained when tracking the centroid of themajority cluster for each of the low performing ﬁles.5. DISCUSSION AND FUTURE WORKThis paper tackled the problem of automatic rhythmic anal-ysis of Candombe audio signals. A study of the rhythmicstructure of Candombe was described, along with a pat-tern tracking algorithm that could deal with the particularcharacteristics of this rhythm. From the rhythm descrip-tion and the presented experiments, it becomes clear thattypical assumptions of general-purpose beat-tracking algo-rithms (such as strong events at beat times) do not hold,which hinders their performance. In order to overcomethis problem, the proposed algorithm tracks a rhythmic pat-tern that informs when a beat with or without accentuationis expected to occur, which eventually can determine thecomplete metric structure. Indeed, experiments employingboth rhythmic patterns based on musical knowledge andothers learned from a labeled database, showed that theproposed algorithm can estimate the beat and downbeatpositions for Candombe whereas traditional methods failat these tasks. The attained CMLt score of about 80 % forbeat tracking is approximately what one can expect froma state-of-the-art algorithm in a standard dataset [2,9], andwhat is reported in [10] for a Bayesian approach adapted toa culturally diverse music corpus. The present work givesadditional evidence of the generalizability of the Bayesianapproach to complex rhythms from different music tra-ditions. The analysis of examples with low performancescores indicates that tracking several rhythmic patterns si-multaneously, as proposed in [10], is a promising alterna-tive for future work. Surely taking into account the timbrecharacteristics of different drums can be proﬁtable.Along with the annotated database employed, a soft-ware implementation of the proposal is being released withthe publication of this paper to foster reproducible research(the ﬁrst available implementation of the Bayesian approachfor beat tracking, to the best of our knowledge).66. ACKNOWLEDGMENTSThis work was supported by funding agencies CAPES andCNPq from Brazil, and ANII and CSIC from Uruguay.6Available fromgithub.com/lonnes/RhythmicAnalysis.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2697. REFERENCES[1]G. Andrews.Blackness in the White Nation: A His-tory of Afro-Uruguay. The University of North Car-olina Press, Chapel Hill, USA, 2010.[2]S. B¨ock, F. Krebs, and G. Widmer. A multi-model ap-proach to beat tracking considering heterogeneous mu-sic styles. InProc. of the 15th International Societyfor Music Information Retrieval Conference (ISMIR2014), pages 603–608, Taipei, Taiwan, Oct. 2014.[3]M. Davies and S. B¨ock. Evaluating the evaluation mea-sures for beat tracking. InProc. of the 15th Interna-tional Society for Music Information Retrieval Confer-ence (ISMIR 2014), pages 637–642, Taipei, Taiwan,Oct. 2014.[4]M. Davies, N. Degara, and M. Plumbley. Evaluationmethods for musical audio beat tracking algorithms.Technical Report C4DM-TR-09-06, Center for Digi-tal Music, Queen Mary University, London, UK, Oct.2009.[5]N. Degara, E. A. Rua, A. Pena, S. Torres-Guijarro,M. E. P. Davies, and M. D. Plumbley. Reliability-informed beat tracking of musical signals.IEEE Trans-actions on Audio, Speech, and Language Processing,20(1):290–301, Jan. 2012.[6]S. Dixon. Automatic extraction of tempo and beat fromexpressive performances.Journal of New Music Re-search, 30(1):39–58, Aug. 2001.[7]D. P. W. Ellis. Beat tracking by dynamic programming.Journal of New Music Research, 36(1):51–60, May2007.[8]L. Ferreira. An afrocentric approach to musical per-formance in the black south atlantic: The candombedrumming in Uruguay.TRANS-Transcultural MusicReview, 11:1–15, Jul. 2007.[9]A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. L.Oliveira, and F. Gouyon. Selective sampling for beattracking evaluation.IEEE Transactions on Audio,Speech, and Language Processing, 20(9):2539–2548,Nov. 2012.[10]A. Holzapfel, F. Krebs, and A. Rinivasamurthy. Track-ing the “odd”: Meter inference in a culturally diversemusic corpus. InProc. of the 15th International So-ciety for Music Information Retrieval Conference (IS-MIR 2014), pages 425–430, Taipei, Taiwan, Oct. 2014.[11]T. Jehan.Creating Music by Listening. PhD thesis, Pro-gram in Media Arts and Sciences, Massachusetts Insti-tute of Technology, Cambridge, USA, Sep. 2005.[12]L. Jure. Principios generativos del toque de repiquedel candombe. In C. Aharoni´an, editor,La m´usica en-tre´Africa y Am´erica, pages 263–291. Centro Nacionalde Documentaci´on Musical Lauro Ayestar´an, Montev-ideo, 2013. In Spanish.[13]A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysisof the meter of acoustic musical signals.IEEE Trans-actions on Audio, Speech, and Language Processing,14(1):342–355, Jan. 2006.[14]F. Krebs, S. Bck, and G. Widmer. Rhythmic patternmodeling for beat and downbeat tracking in musicalaudio. InProc. of the 14th International Conferenceon Music Information Retrieval (ISMIR 2013), pages227–232, Curitiba, Brazil, Nov. 2013.[15]F. Lerdahl and R. Jackendoff.A Generative Theory ofTonal Music. MIT Press, Cambridge, USA, 1985.[16]J. London. Cognitive constraints on metric systems:Some observations and hypotheses.Music Perception,19(4):529–550, Summer 2002.[17]M. F. McKinney and D. Moelants. Deviations from theresonance theory of tempo induction. InProc. of theConference on Interdisciplinary Musicology (CIM04),pages 1–11, Graz, Austria, Apr. 2004.[18]K. P. Murphy.Dynamic Bayesian Networks: Represen-tation, Inference and Learning. PhD thesis, ComputerScience Division, University of California, Berkeley,Berkeley, USA, Fall 2002.[19]J. L. Oliveira, M. E. P. Davies, F. Gouyon, and L. P.Reis. Beat tracking for multiple applications: A multi-agent system architecture with state recovery.IEEETransactions on Audio, Speech, and Language Pro-cessing, 20(10):2696–2706, Dec. 2012.[20]Geoffroy Peeters. Template-based estimation of time-varying tempo.EURASIP Journal on Advances in Sig-nal Processing, 2007(67215):1–14, Dec. 2006.[21]M. Rocamora, L. Jure, and L. W. P. Biscainho. Toolsfor detection and classiﬁcation of piano drum patternsfrom candombe recordings. InProc. of the 9th Confer-ence on Interdisciplinary Musicology (CIM14), pages382–387, Berlin, Germany, Dec. 2014.[22]A. Srinivasamurthy, A. Holzapfel, and X. Serra. Insearch of automatic rhythm analysis methods for turk-ish and indian art music.Journal of New Music Re-search, 43:94–114, Mar. 2014.[23]D. Temperley.The Cognition of Basic Musical Struc-tures. MIT Press, Cambridge, USA, 2001.[24]N. Whiteley, A. Cemgil, and S. Godsill. Bayesian mod-elling of temporal structure in musical audio. InProc.of the 7th International Conference on Music Informa-tion Retrieval (ISMIR 2006), pages 29–34, Victoria,Canada, Oct. 2006.[25]M. Wright, W. A. Schloss, and G. Tzanetakis. Ana-lyzing afro-cuban rhythms using rotation-aware clavetemplate matching with dynamic programming. InProc. of the 9th International Conference on MusicInformation Retrieval (ISMIR 2008), pages 647–652,Philadelphia, USA, Sep. 2008.270 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "FlaBase: Towards the Creation of a Flamenco Music Knowledge Base.",
        "author": [
            "Sergio Oramas",
            "Francisco Gómez 0001",
            "Emilia Gómez",
            "Joaquín Mora"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417183",
        "url": "https://doi.org/10.5281/zenodo.1417183",
        "ee": "https://zenodo.org/records/1417183/files/OramasGGM15.pdf",
        "abstract": "Online information about flamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to flamenco music is gath- ered from general knowledge bases (e.g., Wikipedia, DB- pedia), music encyclopedias (e.g., MusicBrainz), and spe- cialized flamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identifiers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 pa- los (flamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The sys- tem can recognize and disambiguate FlaBase entity refer- ences in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a fla- menco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base.",
        "zenodo_id": 1417183,
        "dblp_key": "conf/ismir/OramasGGM15",
        "keywords": [
            "Online",
            "information",
            "flamenco",
            "music",
            "gathered",
            "general",
            "knowledge",
            "bases",
            "music",
            "encyclopedias"
        ],
        "content": "FLABASE: TOWARDS THE CREATION OF A FLAMENCO MUSICKNOWLEDGE BASESergio Oramas1, Francisco G´omez2, Emilia G´omez1, Joaqu´ın Mora31Music Technology Group, Universitat Pompeu Fabra2Technical University of Madrid3Faculty of Psychology, University of Sevilla{sergio.oramas, emilia.gomez}@upf.edu, fmartin@eui.upm.es, mora@us.esABSTRACTOnline information about ﬂamenco music is scattered overdifferent sites and knowledge bases. Unfortunately, thereis no common repository that indexes all these data. Inthis work, information related to ﬂamenco music is gath-ered from general knowledge bases (e.g., Wikipedia, DB-pedia), music encyclopedias (e.g., MusicBrainz), and spe-cialized ﬂamenco websites, and is then integrated into anew knowledge base called FlaBase. As resources fromdifferent data sources do not share common identiﬁers, aprocess of pair-wise entity resolution has been performed.FlaBase contains information about 1,174 artists, 76pa-los(ﬂamenco genres), 2,913 albums, 14,078 tracks, and771 Andalusian locations. It is freely available in RDF andJSON formats. In addition, a method for entity recognitionand disambiguation for FlaBase has been created. The sys-tem can recognize and disambiguate FlaBase entity refer-ences in Spanish texts with an f-measure value of 0.77. Weapplied it to biographical texts present in Flabase. By usingthe extracted information, the knowledge base is populatedwith relevant information and a semantic graph is createdconnecting the entities of FlaBase. Artists relevance is thencomputed over the graph and evaluated according to a ﬂa-menco expert criteria. Accuracy of results shows a highdegree of quality and completeness of the knowledge base.1. INTRODUCTIONMusic context information is now playing a key role inMIR research. Multimodal approaches, semantic approaches,and text-IR approaches have shown important achievementsin typical MIR problems, such as music recommendationand discovery, genre classiﬁcation, or music similarity [17].Therefore, collecting and storing music context informa-tion may be extremely useful for the MIR research com-munity [13]. There are some broad repositories of musicc\u0000Sergio Oramas1, Francisco G´omez2, Emilia G´omez1,Joaqu´ın Mora3.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Sergio Oramas1, Francisco G´omez2,Emilia G´omez1, Joaqu´ın Mora3. “FlaBase: Towards the Creation of aFlamenco Music Knowledge Base”, 16th International Society for MusicInformation Retrieval Conference, 2015.context information such as MusicBrainz1or Discogs2.Although some of these repositories are very complete andaccurate, there is still a vast amount of music informa-tion out there, which is generally scattered among differ-ent sources on the Web. Hence, harvesting and combiningthat information is a crucial step in the creation of practicaland meaningful music knowledge bases. In addition, thecreation of genre-speciﬁc knowledge bases may be veryvaluable for research and dissemination purposes, and par-ticularly to non-western music traditions.In this paper, we propose a methodology for the creationof a genre-speciﬁc knowledge base; in particular, a knowl-edge base of ﬂamenco music. The proposed methodologycombines content curation and knowledge extraction pro-cesses. First, an important amount of information is gath-ered from different data sources, which are subsequentlycombined by applying pair-wise entity resolution. Next,new knowledge is extracted from unstructured harvestedtexts and employed to populate the knowledge base. Forthis purpose, an entity linking system has been expresslydeveloped. Finally, the content of the knowledge base isused to compute artist relevance and results are evaluatedaccording to ﬂamenco experts criteria. The content of theknowledge base is freely available and downloadable asdata dumps in RDF and JSON formats.The remainder of the paper is organized as follows. InSection 2, an introduction to ﬂamenco music is presented.In Section 3 some relevant prior work is brieﬂy surveyed.Section 4 describes the structure of the knowledge base.Next, in Section 5 the process of content curation is ex-plained. Section 6 shows the methodology applied for knowl-edge extraction. In Section 7 artist relevance is computedand some statistics about the content are laid out. Finally,Section 8 concludes the paper and points out for futurelines of work.2. FLAMENCO MUSICSeveral musical traditions contributed to the genesis of ﬂa-menco music as we know it today. Among them, the inﬂu-ences of the Jews, Arabs, and Spanish folk music are rec-ognizable, but indubitably the imprint of Andalusian Gyp-sies’ culture is deeply ingrained in ﬂamenco music. Fla-1http://musicbrainz.org2http://www.discogs.com/378menco occurs in a wide range of settings, including festivejuergas(private parties),tablaos(ﬂamenco venues), con-certs, and big productions in theaters. In all these settingswe ﬁnd the main components of ﬂamenco music:canteorsinging,toqueor guitar playing, andbaileor dance. Ac-cording to Gamboa [9], ﬂamenco music grew out of thesinging tradition, as a melting process of all the traditionsmentioned above, and therefore the role of the singer soonbecame dominant and fundamental.Toqueis subordinatedtocante, especially in more traditional settings, whereasbaileenjoys more independence from voice.In the ﬂamenco jargon styles are calledpalos. Crite-ria adopted to deﬁne ﬂamencopalosare rhythmic patterns,chord progressions, lyrics and its poetic structure, and geo-graphical origin. In ﬂamenco geographical variation is im-portant to classifycantesas often they are associated to aparticular region where they were originated or where theyare performed with gusto. Rhythm orcomp´asis a uniquefeature of ﬂamenco. Rhythmic patterns based on 12-beatcycles are mainly used. Those patterns can be classed asfollows: binary patterns, such astangosortientos; ternarypatterns, which are the most common ones, such asfan-dangosorbuler´ıas; mixed patterns, where ternary and bi-nary patterns alternate, such asguajira; free-form, wherethere is no a clear underlying rhythm, such aston´as. Forfurther information on fundamental aspects of ﬂamencomusic, see the book of Fern´andez [7]. For a comprehen-sive study of styles, musical forms and history of ﬂamencothe reader is referred to the books of Blas Vega and R´ıosRuiz [3], Navarro and Ropero [12], and Gamboa [9] andthe references therein.3. RELATED WORKA knowledge base is a centralized repository intended tostore both complex structured and unstructured informa-tion. Content in a knowledge base can be either curated orextracted, and knowledge bases can be classiﬁed accord-ing to those criteria [6]. Curated knowledge can be man-ually gathered by humans or automatically extracted froma structured data source. By contrast, extracted knowledgeis produced after the application of an information extrac-tion process over an unstructured data source. There areseveral well-known general purpose knowledge bases ei-ther extracted or curated. The most widely used are DBpe-dia3and Freebase4, and more recently WikiData5. Themost relevant extracted knowledge bases are NELL [5] andOpen IE [1].In the music ﬁeld, one of the most complete and broadlyused knowledge bases is MusicBrainz6, which has beencreated in a collaborative curated way. However, there isnot any extracted and open music knowledge base. More-over, little effort have been done in the creation of genre-speciﬁc knowledge bases. Most relevant initiatives in this3http://dbpedia.org4http://www.freebase.com5http://www.wikidata.com6http://musicbrainz.orgdirection have been done within the CompMusic project7.In this project, one of the main tasks has been the gather-ing of culture-speciﬁc corpora of non-western musical tra-ditions, combining expert information, audio recordings,features, music notation, lyrics, editorial metadata and com-munity information [18]. According to [19], a domain-speciﬁc corpora should be designed by satisfying the fol-lowing criteria: purpose, coverage, completeness, qualityand reusability. In [15], the architecture and applicationsof a system that exploits domain-speciﬁc corpora is pre-sented. Another interesting project is Linked Jazz [14],where the application of Linked Open Data (LOD) tech-nology to enhance discovery and visibility of jazz music isstudied.4. FLABASEFlaBase (Flamenco Knowledge Base) is the acronym of anew knowledge base of ﬂamenco music. Its ultimate aimis to gather all available online editorial, biographical andmusicological information related to ﬂamenco music. Aﬁrst version is just being released. Its content is the resultof the curation and extraction processes explained in Sec-tions 5 and 6. FlaBase is stored in RDF and JSON formats,and it is freely available for download8. Its RDF versionfollows the Linked Open Data principles, and it might bequeried by setting up a SPARQL endpoint. A JSON ver-sion is also available, thus facilitating the use of the contentby all the community of researchers and developers. Thisﬁrst release of FlaBase contains information about 1,174artists, 76palos(ﬂamenco genres), 2,913 albums, 14,078tracks, and 771 Andalusian locations.4.1 Ontology DeﬁnitionThe FlaBase data structure is deﬁned in an ontology schema.One of the advantages of using an ontology as a schema isthat it can be easily modiﬁed. Thus, our design is a ﬁrstbuilding block that can be enhanced and redeﬁned in thefuture. The initial ontology is structured around ﬁve mainclasses: MusicArtist, Album, Track, Palo and Place, andthree domain speciﬁc classes:cantaor(ﬂamenco singer),guitarist (ﬂamenco guitar player), andbailaor(ﬂamencodancer). These three classes were deﬁned because they arethe most frequent types of artists in the data. Other instru-ment players may be instantiated directly from the Musi-cArtist class. We have tried to reuse as much vocabularyas we could. We re-utilized most of the classes and someproperties from the Music Ontology9, a standard modelfor publishing music-related data. We selected the classesaccording to the ones used by the LinkedBrainz project10,which maps concepts from MusicBrainz to Music Ontol-ogy.7http://compusic.upf.edu8http://mtg.upf.edu/download/datasets/flabase9http://musicontology.com10https://wiki.musicbrainz.org/LinkedBrainzProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 379Figure 1. Selected data sources5. CONTENT CURATIONThe ﬁrst step towards building a domain-speciﬁc knowl-edge base is to gather all possible content from availabledata sources. This implies at least two problems, namely,the selection of sources, and the matching between enti-ties from different sources. In what follows we enumeratethe involved data sources and describe the methodologyapplied to entity resolution.5.1 Data AcquisitionOur aim is to gather an important amount of informationabout musical entities, including textual descriptions andavailable metadata. A schema of the selected data sourcesis shown in Figure 1. We started by looking at Wikipedia11,the free and multilingual Internet encyclopedia. It is the In-ternet’s largest and most popular general reference work.Each Wikipedia article may have a set of associated cat-egories. Categories are intended to group together pageson similar subjects and are structured in a taxonomicalway. To ﬁnd Wikipedia articles related to ﬂamenco mu-sic, we ﬁrst looked for ﬂamenco categories. The taxon-omy of categories can be explored by querying DBpedia,a knowledge base with structured content extracted fromWikipedia. In particularly, we employed the SPARQL end-point of the Spanish DBpedia12. We queried for categoriesrelated to the ﬂamenco category in the taxonomy. At theend, we obtained 17 different categories (e.g.,cantaores deﬂamenco, guitarristas de ﬂamenco).By querying again DBpedia, we gathered all DBpediaresources related to one of these categories. We obtaineda total number of 438 resources in Spanish, of which 281were also in English. Each DBpedia resource is associatedwith a Wikipedia article. Text and HTML code were thenextracted from Wikipedia articles in English and Spanishby using the WikiMedia API. Next, we classiﬁed the ex-tracted articles according to the ontology schema deﬁnedin our knowledge base (Section 4.1). For this purpose, weexploited classiﬁcation information provided by DBpedia11http://www.wikipedia.org12http://es.dbpedia.org(DBpedia ontology and Wikipedia categories). At the end,from all gathered resources, we only kept those related toartists andpalos, totalling 291 artists and 56palos.However, the amount of information present in Wikipediarelated to ﬂamenco music is somewhat scarce. Therefore,we decided to expand our knowledge base with informa-tion from two different websites. First,Andalucia.org, thetouristic web from the Andalusia Government13. It con-tains 422 artist biographies in English and Spanish, and thedescription of 76palosalso in both languages. Second, awebsite calledEl arte de vivir el ﬂamenco14, which in-cludes 749 artist biographies amongcantaores,bailaoresand guitarists. Both webs were crawled and their contentstored in our knowledge base.MusicBrainz is one of the biggest and more reliableopen music databases, which provides an unambiguous formof music identiﬁcation. Therefore, we turned to it in orderto ﬁll our knowledge base with information about ﬂamencoalbum releases and recordings. Artists present in FlaBasewere intended to be mapped with MusicBrainz artists. Forevery match, all content related to releases and recordingswas gathered. After doing so, we obtained a total numberof 814 releases and 9,942 recordings.The information gathered from MusicBrainz is a lit-tle part of the actual ﬂamenco discography. Therefore,to complement it we used a ﬂamenco recordings databasegathered by Rafael Infante and available at CICA web-site15(Computing and Scientiﬁc Center of Andalusia). Thisdatabase has information about releases from the early timeof recordings until present time, counting 2,099 releasesand 4,136 songs. For every song entry, acantaornameis provided, and most of the times also guitarist andpalo,which is a very valuable information to deﬁne ﬂamencorecordings.Finally, we supplied our knowledge base with informa-tion related to Andalusian towns and provinces. We gath-ered this information from the ofﬁcial database SIMA16(Multi-territorial System of Information of Andalusia).5.2 Entity ResolutionEntity Resolution (ER) is the problem of extracting, match-ing and resolving entity mentions in structured and un-structured data [10]. There are several approaches to tacklethe ER problem. For the scope of this research, we selecteda pair-wise classiﬁcation approach based on string similar-ity between entity labels.The ﬁrst issue after gathering the data is to decide whethertwo entities from different sources are referring to the sameone. Therefore, given two sets of entitiesAandB, the ob-jective is to deﬁne an injective and non-surjective mappingfunctionfbetweenAandBthat decides whether an en-titya2Ais the same as an entityb2B. To do that,a string similarity metricsim(a, b)based on the Ratcliff-Obershelp algorithm [16] has been deﬁned. It measures13http://andalucia.org14http://www.elartedevivirelflamenco.com/15http://flun.cica.es/index.php/grabaciones16http://www.juntadeandalucia.es/institutodeestadisticaycartografia/sima380 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2. F-measure for different values of✓the similarity between two entity labels and outputs a valuebetween 0 and 1. We consider thataandbare the same en-tity if their similarity is bigger than a parameter✓. If thereare two entitiesb, c2Bthat satisfy thatsim(a, b)\u0000✓andsim(a, c)\u0000✓, we consider only the mapping with thehighest score. To determine the value of✓, we tested themethod with several✓values over an annotated dataset ofentity pairs. To create this dataset, the 291 artists gatheredfrom Wikipedia were manually mapped to the 422 artistsgathered from Andalucia.org, obtaining a total amount of120 pair matches. As it is shown in Figure 2 the best F-measure (0,97) was obtained with✓=0.9. Finally, weapplied the described method with✓=0.9to all gath-ered entities from the three data sources. Thanks to the en-tity resolution process, we reduced the initial set of 1,462artists and 132palosto a set of 1,174 artists and 76palos.Once we had our artist entities resolved, we began togather their related discographic information. First, wetried to ﬁnd out the MusicBrainz ID of the gathered artists.Depending on the information about the entity, two differ-ent process were applied. First, every Wikipedia page, andits equivalent DBpedia resource, has a correspondent en-tity deﬁned in Wikidata. Wikidata is a free linked databasewhich acts as a structured data storage of Wikipedia. Thereare several properties in Wikidata that may link Wikidataitems with MusicBrainz items. Thus, the equivalent Wiki-data resource of a Wikipedia artist page may have a linkto its corresponding MusicBrainz artist ID. Therefore, welooked for these relations and mapped all possible entities.For those artists without a direct link to MusicBrainz, wequeried the MusicBrainz API by using the artist labels, andthen applied our entity resolution method to the obtainedresults.Finally, to integrate the discography database of CICAinto our knowledge base, we applied the entity resolutionmethod to the ﬁeldscantaor, guitarist andpaloof eachrecording entry in the database. From the set of 202can-taoresand 157 guitarists names present in the recordingentries, a total number of 78cantaoresand 44 guitaristswere mapped to our knowledge base. The number of mappedartists was low due to differences between the way of la-beling an artist. An artist name may be written using oneor two surnames, or using a nickname. In the case ofpalos,there were 162 differentpalosin the database, 54 of whichwere mapped with the 76 of our knowledge base. These 54paloscorrespond to an 80% ofpaloassignments present inthe recording entries.6. KNOWLEDGE EXTRACTIONOnce the process of data acquisition is ﬁnished, the knowl-edge base is ready for use. However, there is an impor-tant amount of knowledge present in the data that has notbeen fully exploited. Texts gathered contain a huge epis-temic potential that remains implicit. Consequently, to en-hance the amount of structured data in FlaBase, a processof knowledge extraction has been carried out. This implicitknowledge may vary from biographical data, such as placeand date of birth, to more complex semantic relations in-volving different entities. Three tasks play a key role inthe process of knowledge extraction from non-structuredtext: named entity recognition (NER), named entity dis-ambiguation (NED), and relation extraction (RE) [20]. Inthis research, we focus on the two ﬁrst tasks. In what fol-lows, a system for entity recognition and disambiguation isdescribed and evaluated. Lastly, an information extractionprocess is applied to populate the knowledge base.6.1 Named entity recognition and disambiguationTo extract implicit knowledge from a text, the ﬁrst stepis to semantically annotate it by identifying entity men-tions. Named entity recognition is a task that seeks andclassify words in text into pre-deﬁned categories (e.g., per-son, organization, or place). Named entity disambigua-tion, also called entity linking, aims to determine whatis actually a named entity present in a text. It generallydoes so by identifying it in a knowledge base of reference.NED can be addressed directly from the text, or applied tothe output of a NER system. We propose a method thatemploys a combination of both approaches, depending onthe category of the entity. For NER, we used the Stan-ford NER system [8], implemented in the library StanfordCore NLP17and trained on Spanish texts. For NED wetried two different approaches. First, we looked for exactstring matches between FlaBase entity labels and word n-grams extracted from the text. Second, we searched forexact string matches between FlaBase entity labels and theoutput of the NER system. In fact, we tried several com-binations of both approaches until we obtained the mostsatisfactory one.For the scope of this research, we focused on Spanishtexts, as ﬂamenco texts are mostly written in Spanish. Al-though there are many entity linking tools available, wedecided to develop ours because state-of-the-art systems(e.g., Tag-me or Babelfy) are well-tuned for English texts,but do not perform well on Spanish texts, and even lesswith music texts of a speciﬁc domain. In addition, wewanted to have a system able to map entities to our knowl-edge base. Therefore, we developed a system able to detectand disambiguate three categories of entities: person,paloand location. Three different approaches were deﬁned bycombining NER and NED in different ways according tothe category. First, directly applying NED to text. Sec-ond, disambiguating location and person entities from the17http://nlp.stanford.edu/software/corenlp.shtmlProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 381ApproachPrecisionRecallF-measure1) NED0.8290.6940.7562) NED + NER to PERS & LOC0.7390.3470.4723) NED + NER to LOC0.8920.6740.767Table 1. Precision, Recall and F-measure of NER+NEDNER output, andpalodirectly from text. Third, only dis-ambiguating location entities from the NER output, andlocation andpalodirectly from text.To determine which approach performs better, three artistbiographies coming from three different data sources weremanually annotated, having a total number of 49 annotatedentities. We followed an evaluation methodology similar tothe one used in KBP2014 Entity Linking Task18. Resultson the different approaches are shown in Table 1. We ob-serve that applying NER to entities of the person categorybefore NED worsens performance signiﬁcantly, as recallsuddenly decrease by half. After manually analysing falsenegatives, we observed that this is caused because manyartist names have deﬁnite articles between name and sur-name (e.g.,de, del), and this is not recognized by the NERsystem. In addition, many artists have a nickname that isnot interpreted as a person entity by the NER system. Thebest approach is the third (NED + NER to LOC), which isslightly better than the ﬁrst (only NED) in terms of preci-sion. This is due to the fact that many artists have a townname as a surname or as part of his nickname. Therefore,applying NED directly to text is misclassifying person en-tities as location entities. Thus, by adding a previous stepof NER to location entities we have increased overall per-formance, as it can be seen on the F-measure values.6.2 Knowledge base populationBiographical texts coming from different data sources havebeen stored in FlaBase. These texts are full of relevantinformation about FlaBase entities, but in an unstructuredway. Thus, a process of information extraction is necessaryto transform the unstructured information into structuredknowledge. For the scope of this research, we focused onextracting two speciﬁc data: birth year and birth place, asthey can be very relevant for anthropologic studies. Weobserved that this information is often in the very ﬁrst sen-tences of the artist biographies, and always near the wordnaci´o(Spanish translation of “was born”). Therefore, toextract this information, we looked for this word in the ﬁrst250 characters of every biographical text. If it is found, weapply our entity linking method to this piece of text. If alocation entity is found near the word ”naci´o”, we assumethat this entity is the place of birth of the biography sub-ject. In addition, by using regular expressions, we look forthe presence of a year expression in the neighborhood. Ifit is found, we assume it as the year of birth. If more thanone year is found, we select the one with the smaller value.To evaluate our approach, we tested the extraction ofbirth places in all texts coming from the web Andalucia.org(442 artists). We chose this subset because Andalucia.org18http://nlp.cs.rpi.edu/kbp/2014/also provides speciﬁc information about artist origin thathad been previously crawled and stored in FlaBase. How-ever, we observed that in many occasions the artist originprovided by the data source was wrong. Therefore, we de-cided to manually annotate the province of precedence ofthese 442 artists for building ground truth data. After theapplication of the extraction process on the annotated testset, we obtained a precision value of 0,922 and a recall of0,648. Therefore, we can state that our method is extract-ing biographic information with very high precision andquite reasonable recall. We ﬁnally applied the extractionprocess to all artist entities with biographical texts com-ing from any of the two ﬂamenco crawled websites. Thus,from a total number of 1,123 artists coming from these datasources (95% of the artists in the knowledge base), 743birth places and 879 birth years were extracted.7. LOOKING AT THE DATA7.1 Artist RelevanceWe assume that an entity mention inside an artist biog-raphy means a semantic relation between the biographysubject and the mentioned entity. Based on this assump-tion, we build a semantic graph by applying the followingsteps. First, each artist of the knowledge base is added tothe graph as a node. Second, entity linking is applied toartist’s biographical texts. For every linked entity, a newnode is created in the graph (only if it was not previouslycreated). Next, an edge is added by connecting the artistentity node with the linked entity node. This way, a di-rected graph connecting the entities of FlaBase is ﬁnallyobtained. Entities identiﬁed in a text can be seen as hyper-links. Hence, algorithms to measure the relevance of nodesin a network of hyperlinks can be applied to our semanticgraph [2]. In order to measure artist relevance, we appliedPageRank [4] and HITS [11] algorithms to the obtainedgraph.We built an ordered list with the top-10 entities of thedifferent artist categories (cantaor, guitarist andbailaor)for the two algorithms. For evaluation purposes, we askeda ﬂamenco expert to build a list of top-10 artists for eachcategory according to his knowledge and the available bib-liography. The concept of artist relevance is somehow sub-jective and there is no uniﬁed or consensual criteria for ﬂa-menco experts about who the most relevant artists are. De-spite that, there is a high level of agreement among themon certain artists that should be on such a hypotheticallist. Thus, the expert provided us with this list of hypo-thetical top-10 artists by category and we considered it asground truth. We deﬁne precision as the number of iden-tiﬁed artists in the resulting list that are also present in theground truth list divided by the length of the list. We eval-uated the output of the two algorithms by calculating pre-cision over the entire list (top-10), and over the ﬁrst ﬁve el-ements (top-5) (see Table 3). We observed that PageRankresults (see Table 2) show the greatest agreement with theﬂamenco expert. High values of precision, specially for thetop-5 list, indicates that the content gathered in FlaBase is382 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015highly complete and accurate (see Table 3).CantaorGuitaristBailaorAntonio MairenaPaco de Luc´ıaAntonio Ruiz SolerManolo CaracolRam´on MontoyaRosarioLa Ni˜na de los PeinesNi˜no RicardoAntonio GadesAntonio Chac´onManolo Sanl´ucarMario MayaCamar´on de la IslaSabicasCarmen AmayaTable 2. PageRank Top-5 artists by categoryTop-5Top-10PageRank0.9330.633HITS Authority0.60.4Table 3. Precision values7.2 StatisticsFor the sake of completeness, some statistics on the datastored in FlaBase were calculated. Data shown in Figure 3was produced out of the entity resolution process, whiledata shown in Figures 4 and 5 was calculated accordingto the populated data. In Figure 3 it is shown that the mostrepresentativepalosare represented in the knowledge base,with a higher predominance of fandangos. We can observein Figure 4 that most ﬂamenco artists are from the Andalu-sian provinces of Seville and Cadiz. Finally, in Figure 5we observe a higher number of artists in the data were bornfrom the 30’s to the 80’s of the 20th century.\nFigure 3. Songs bypalo8. CONCLUSIONS AND FUTURE WORKA new knowledge base that contains information about ﬂa-menco music has been created and released. A process ofautomatic knowledge curation has been applied to com-bine information coming from different data sources. Inaddition, the knowledge base has been enriched with con-tent extracted directly from texts by using a custom en-tity linking system. Using FlaBase data, artist relevancehas been computed and compared to the ﬂamenco experts’judgment. Precision values obtained reveals a high degree\nFigure 4. Artists by province of birth\nFigure 5. Artists by decade of birthof coverage and a good quality of the knowledge base con-tent.There are still many avenues to be explored for futurework. More websites can be exploited to increase cov-erage. The entity resolution step might be improved byincreasing the amount of entity labels used, or by apply-ing learning algorithms. A SPARQL endpoint might becreated, letting users query FlaBase directly. In addition,implementing a collaborative environment for knowledgemanagement would lead to an improvement in terms ofcompleteness and data accuracy, as content might be added,checked and corrected directly by a community of users.9. ACKNOWLEDGMENTSThis work was funded by the COFLA2 research project(Proyectos de Excelencia de la Junta de Andaluca, FEDERP12-TIC-1362). We thank Rafael Infante and Jos´e RuizFuentes for the provided content.10. REFERENCES[1]Michele Banko, Michael J Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni. Open Infor-mation Extraction from the Web. InInternational JointConferences on Artiﬁcial Intelligence, pages 2670–2676, 2007.[2]Francesco Bellomi and Roberto Bonato. NetworkProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 383Analysis for Wikipedia.Proceedings of Wikimania,2005.[3]Jose Blas Vega and Manuel R´ıos Ruiz.Diccionario en-ciclop´edico ilustrado del ﬂamenco. Cinterco, Madrid,1988.[4]Sergey Brin and Lawrence Page. The Anatomy of aLarge-Scale Hypertextual Web Search Engine.Com-puter Networks, 30:107–117, 1998.[5]Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R Hruschka Jr, and Tom M Mitchell.Toward an Architecture for Never-Ending LanguageLearning.In AAAI, 2010.[6]Anthony Fader, Luke Zettlemoyer, and Oren Et-zioni. Open question answering over curated and ex-tracted knowledge bases.Proceedings of the 20th ACMSIGKDD international conference on Knowledge dis-covery and data mining - KDD ’14, pages 1156–1165,2014.[7]Lola Fern´andez.Teor´ıa musical del ﬂamenco. AcordesConcert, Madrid, 2004.[8]Jenny Rose Finkel, Trond Grenager, and ChristopherManning. Incorporating Non-local Information intoInformation Extraction Systems by Gibbs Sampling.Proceedings of the 43nd Annual Meeting of the Associ-ation for Computational Linguistics (ACL 2005), pages363–370, 2005.[9]J. M. Gamboa.Una historia del ﬂamenco. Espasa-Calpe, Madrid, 2005.[10]Lise Getoor. Entity Resolution: Theory, Practice &Open Challenges.Tutorial at AAAI-12, pages 2018–2019, 2012.[11]Jon M. Kleinberg. Authoritative sources in a hyper-linked environment.Journal of the ACM (JACM),46:604–632, 1999.[12]J.L. Navarro and M. Ropero.Historia del ﬂamenco. Ed.Tartessos, Sevilla, 1995.[13]Sergio Oramas. Harvesting and Structuring Social Datain Music Information Retrieval.Extended SemanticWeb Conference (ESWC). Lecture Notes in ComputerScience, 8465:817–826, 2014.[14]M Cristina Pattuelli, Matt Miller, Leanora Lange, SeanFitzell, and Carolyn Li-Madeo. Crafting Linked OpenData for Cultural Heritage: Mapping and CurationTools for the Linked Jazz Project.Code4Lib Journal,page 4, 2013.[15]Alastair Porter, Mohamed Sordo, and Xavier Serra.Dunya: A System for Browsing Audio Music Collec-tions Exploiting Cultural Context.14th InternationalSociety for Music Information Retrieval Conference(ISMIR 2013), 2013.[16]John W Ratcliff and David Metzener. Pattern match-ing: The gestalt approach.Dr. Dobb’s Journal, 13:46–72, 1988.[17]Markus Schedl, Emilia G´omez, and Juli´an Urbano.Music Information Retrieval: Recent Developmentsand Applications.Foundations and TrendsR\u0000in Infor-mation Retrieval, 8(2-3):127–261, 2014.[18]Xavier Serra. Data gathering for a culture speciﬁc ap-proach in MIR.Proceedings of the 21st internationalconference companion on World Wide Web - WWW ’12Companion, page 867, 2012.[19]Xavier Serra. Creating Research Corpora for the Com-putational Study of Music : the case of the CompMusicProject.53rd International Conference: Semantic Au-dio (January 2014), pages 1–9, 2014.[20]Ricardo Usbeck, Axel-cyrille Ngonga Ngomo,R Michael, Daniel Gerber, Sandro Athaide Coelho,and Andreas Both. AGDISTIS - Graph-Based Disam-biguation of Named Entities using Linked Data.TheSemantic Web – ISWC 2014, 2014.384 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "A Semantic-Based Approach for Artist Similarity.",
        "author": [
            "Sergio Oramas",
            "Mohamed Sordo",
            "Luis Espinosa Anke",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415976",
        "url": "https://doi.org/10.5281/zenodo.1415976",
        "ee": "https://zenodo.org/records/1415976/files/OramasSAS15.pdf",
        "abstract": "This paper describes and evaluates a method for comput- ing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting men- tions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge rep- resentation from these mentions in the form of a seman- tic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similar- ity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors.",
        "zenodo_id": 1415976,
        "dblp_key": "conf/ismir/OramasSAS15",
        "keywords": [
            "entity linking",
            "semantic information",
            "knowledge representation",
            "semantic graph",
            "vector-space model",
            "semantic similarity",
            "artist biographies",
            "Last.fm",
            "MIREX Audio and Music Similarity evaluation dataset",
            "Last.fm API"
        ],
        "content": "A SEMANTIC-BASED APPROACH FOR ARTIST SIMILARITYSergio Oramas1, Mohamed Sordo2, Luis Espinosa-Anke3, Xavier Serra11Music Technology Group, Universitat Pompeu Fabra2Center for Computational Science, University of Miami3TALN Group, Universitat Pompeu Fabra{sergio.oramas, luis.espinosa, xavier.serra}@upf.edu, msordo@miami.eduABSTRACTThis paper describes and evaluates a method for comput-ing artist similarity from a set of artist biographies. Theproposed method aims at leveraging semantic informationpresent in these biographies, and can be divided in threemain steps, namely: (1) entity linking, i.e. detecting men-tions to named entities in the text and linking them to anexternal knowledge base; (2) deriving a knowledge rep-resentation from these mentions in the form of a seman-tic graph or a mapping to a vector-space model; and (3)computing semantic similarity between documents. Wetest this approach on a corpus of 188 artist biographiesand a slightly larger dataset of 2,336 artists, both gatheredfrom Last.fm. The former is mapped to the MIREX Audioand Music Similarity evaluation dataset, so that its similar-ity judgments can be used as ground truth. For the latterdataset we use the similarity between artists as providedby the Last.fm API. Our evaluation results show that anapproach that computes similarity over a graph of entitiesand semantic categories clearly outperforms a baseline thatexploits word co-occurrences and latent factors.1. INTRODUCTIONArtist biographies are a big source of musical context in-formation and have been previously used for computingartist similarity. However, only shallow approaches havebeen applied by computing word co–occurrences and thusthe semantics implicit in text have been barely exploited.To do so, semantic technologies, and more speciﬁcally En-tity Linking tools may play a key role to annotate unstruc-tured texts. These are able to identify named entities in textand disambiguate them with their corresponding entry in aknowledge base (e.g. Wikipedia, DBpedia or BabelNet).This paper describes a method for computing semanticsimilarity at document-level, and presents evaluation re-sults in the task of artist similarity. The cornerstone ofthis work is the intuition that semantifying and formaliz-c\u0000Sergio Oramas1, Mohamed Sordo2, Luis Espinosa-Anke3, Xavier Serra1.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Sergio Oramas1, Mohamed Sordo2,Luis Espinosa-Anke3, Xavier Serra1. “A Semantic-based Approach forArtist Similarity”, 16th International Society for Music Information Re-trieval Conference, 2015.ing relations between entity mentions in documents (bothat in-document and cross-document levels) can representthe relatedness of two documents. Speciﬁcally, in the taskof artist similarity, this derives in a measure to quantifythe degree of relatedness between two artists by looking attheir biographies.Our experiments start with a preprocessing step whichinvolve Entity Linking over artist biographical texts. Then,a knowledge representation is derived from the detectedentities in the form of a semantic graph or a mapping to avector-space model. Finally, different similarity measuresare applied to a benchmarking dataset. The evaluation re-sults indicate that some approaches presented in this paperclearly outperform a baseline based on shallow word co-occurrence metrics. Source code and datasets are availableonline1.The remainder of this article is structured as follows:Section 2 reviews prominent work in the ﬁelds and topicrelevant to this paper; Section 3 details the different mod-ules that integrate our approach; Section 4 describes thesettings in which experiments were carried out togetherwith the evaluation metrics used; Section 5 presents theevaluation results and discusses the performance of ourmethod; and ﬁnally Section 6 summarizes the main top-ics covered in this article and suggests potential avenuesfor future work.2. RELATED WORKMusic artist similarity has been studied from the score level,the acoustic level, and the cultural level [9]. This work isfocused on the latter approach, and more speciﬁcally intext-based approaches. Literature on document similarity,and more speciﬁcally on the application of text-based ap-proaches for artist similarity is discussed next.The task of identifying similar text instances, either atsentence or document level, has applications in many ar-eas of Artiﬁcial Intelligence and Natural Language Pro-cessing [17]. In general, document similarity can be com-puted according to the following approaches: surface-levelrepresentation like keywords or n-grams [6]; corpus rep-resentation using counts [28], e.g. word-level correlation,jaccard or cosine models; Latent factor models, such as La-tent Semantic Analysis [8]; or methods exploiting external1http://mtg.upf.edu/downloads/datasets/semantic-similarity100knowledge bases like ontologies or encyclopedias [12].The use of text–based approaches for artist and mu-sic similarity was ﬁrst applied in [7], by computing co–occurrences of artist names in web page texts and build-ing term vector representations. By contrast, in [30] termweights are extracted from search engine’s result counts.In [33] n–grams, part–of–speech tagging and noun phrasesare used to build a term proﬁle for artists, weighted by em-ploying tf-idf. Term proﬁles are then compared and thesum of common terms weights gives the similarity mea-sure. More approaches using term weight vectors havebeen developed over different text sources, such as musicreviews [11], blog posts [4], or microblogs [29]. In [18]Latent Semantic Analysis is used to measure artist simi-larity from song lyrics. Domain speciﬁc ontologies havealso been applied to the problem of music recommenda-tion and similarity, such as in [5]. In [16], paths on an on-tological graph extracted from DBpedia are exploited forrecommending music web pages. However, to the best ofour knowledge, there are scant approaches in the musicdomain that exploit implicit semantics and enhance termproﬁles with external knowledge bases.3. METHODOLOGYThe method proposed in this paper can be divided in threemain steps, as depicted in Fig 1. The ﬁrst step performs en-tity linking, that is the detection of mentions to named en-tities in the text and their linking to an external knowledgebase. The second step derives a semantically motivatedknowledge representation from the named entity mentions.This can be achieved by exploiting natural language text asanchor between entities, or by incorporating semantic in-formation from an external knowledge base. In the lattercase, a document is represented either as a semantic graphor as a set of vectors projected on a vector space, whichallows the use of well known vector similarity metrics. Fi-nally, the third step computes semantic similarity betweendocuments (artist biographies in our case). This step cantake into consideration semantic similarity among entitymentions in document pairs, or only the structure and con-tent of the semantic graph.The following sections provide a more detailed descrip-tion of each one of these steps, along with all the approacheswe have considered in each step.\nFigure 1. Workﬂow of the proposed method.3.1 Entity LinkingEntity linking is the task to associate, for a given candi-date textual fragment, the most suitable entry in a refer-ence Knowledge Base (KB) [23]. It encompasses similarsubtasks such as Named Entity Disambiguation [2], whichis precisely linking mentions to entities to a KB, or Wikiﬁ-cation [21], speciﬁcally using Wikipedia as KB.We considered several state-of-the-art entity linking tools,including Babelfy [23], TagMe [10], Agdistis [32] and DB-Pedia Spotlight [20]. However we opted to use the ﬁrstone for consistency purposes, as in a later step we exploitSensEmbed[13], a vector space representation of conceptsbased on BabelNet [24]. Moreover, the use of a single toolacross approaches guarantees that the evaluation will onlyreﬂect the appropriateness of each one of them, and in caseof error propagation all the approaches will be affected thesame.Babelfy [23] is a state-of-the-art system for entity link-ing and word sense disambiguation based on non-strict iden-tiﬁcation of candidate meanings (i.e. not necessarily exactstring matching), together with a graph based algorithmthat traverses the BabelNet graph and selects the most ap-propriate semantic interpretation for each candidate.3.2 Knowledge representation3.2.1 Relation graphRelation extraction has been deﬁned as the process of iden-tifying and annotating relevant semantic relations betweenentities in text [15]. In order to exploit the semantic re-lations between entities present in artist biographies, weapplied the method deﬁned in [25] for relation extractionin the music domain. The method basically consists ofthree steps. First, entities are identiﬁed in the text by ap-plying entity linking. Second, relations between pairs ofentities occurring in the same sentence are identiﬁed andﬁltered by analyzing the structure of the sentence, whichis obtained by running a syntactic parser based on the for-malism of dependency grammar [1]. Finally, the identiﬁedentities and relations are modeled as a knowledge graph.This kind of extracted knowledge graphs may be useful formusic recommendation [31], as recommendations can beconveyed to users by means of natural language. We ap-ply this methodology to the problem of artist similarity, bycreating a graph that connects the entities detected in ev-ery artist biography. We call this approach RG (relationgraph). Figure 2 shows the output of this process for a sin-gle sentence.\nFigure 2. Relation graph of a single sentenceProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1013.2.2 Semantically enriched graphA second approach is proposed using the same set of linkedentities. However, instead of exploiting natural languagetext, we use semantic information from the referenced knowl-edge base to enrich the semantics of the linked entities. Wefollow a semantic enrichment process similar to the onedescribed in [27]. We use semantic information comingfrom DBpedia2. DBpedia resources are generally clas-siﬁed using the DBpedia Ontology, which is a shallow,cross-domain ontology based on the most common info-boxes of Wikipedia. DBpedia resources are categorizedusing this ontology among others (e.g. Yago, schema.org)through therdfs:typeproperty. In addition, each Wiki-pedia page may be associated with a set of Wikipedia cat-egories, which link articles under a common topic. DBpe-dia resources are related to Wikipedia categories throughthe propertydcterms:subject.We take advantage of these two properties to build oursemantically enriched graph. We consider three types ofnodes for this graph: 1) artist entities obtained by match-ing the artist names to their corresponding DBPedia entry;2) named entities detected by the entity linking step; and3) Wikipedia categories associated to all the previous enti-ties. Edges are then added between artist entities and thenamed entities detected in their biographies, and betweenentities and their corresponding Wikipedia categories. Forthe construction of the graph, we can select all the detectednamed entities, or we can ﬁlter them out according to theinformation related to theirrdfs:typeproperty. A setof six types was selected, includingartist,band,work,al-bum,musicgenre, andperson, which we consider more ap-propriate to semantically deﬁne a musical artist.From the previous description, we deﬁne ﬁve variantsof this approach. The ﬁrst variant, which we call AEC(Artists-Entities-Categories), considers all 3 types of nodesalong with their relations (as depicted in Figure 3). Thesecond variant, named AE (Artists-Entities) ignores thecategories of the entities. The third and fourth variant,named AEC-FT and AE-FT, are similar to the ﬁrst and sec-ond variant, respectively, except that the named entities areﬁltered using the above mentioned list of 6 entity types.Finally, the ﬁfth variant, EC, ignores the artist entities ofnode type 1.3.2.3 Sense embeddingsThe semantic representation used in this approach is basedon SensEmbed [13]. SensEmbed is a vector space seman-tic representation of words similar to word2vec [22], whereeach vector represents a BabelNet synset and its lexicaliza-tion. LetAbe the set of artist biographies in our dataset.Each artist biographya2Ais converted to a set of disam-biguated concepts Bfyaafter running Babelfy over it.2http://dbpedia.org\nFigure 3. Semantically enriched subgraph of the same sen-tence from Figure 2, variant AEC with h=13.3 Similarity approaches3.3.1 SimRankSimRank is a similarity measure based on an simple graph-theoretic model [14]. The intuition is that two nodes aresimilar if they are referenced by similar nodes. In partic-ular we use the deﬁnition of bipartite SimRank [14]. Webuild a bipartite graph with named entities and their corre-sponding Wikipedia categories (the EC variant from Sec-tion 3.2.2). The similarity between two named entities (saypandq) is computed with the following recursive equation:s(p, q)=C|O(p)||O(q)||O(p)|Xi=1|O(q)|Xj=1s(Oi(p),Oj(q))(1)whereOdenotes the out-neighboring nodes of a givennode andCis a constant between 0 and 1. Forp=q,s(p, q)is automatically set up to1. Once the similaritybetween all pairs of entities is obtained, we proceed to cal-culate the similarity between pairs of artists (sayaandb)by aggregating the similarities between the named entitiesidentiﬁed in their biographies, as shown in the followingformula:sim(a, b)=Q(a, b)1NXea2aXeb2bs(ea,eb)ifs(ea,eb)\u00000.1(2)wheresdenotes the SimRank of entitieseaandebandNis the number of (ea,eb) pairs withs(ea,eb)\u00000.1.This is done to ﬁlter out less similar pairs. Finally,Q(a, b)is a normalizing factor that accounts for the pairs of artistswith more similar entity pairs than others.3.3.2 Maximal common subgraphMaximal common subgraph (MCS) is a common distancemeasure on graphs. It is based on the maximal commonsubgraph of two graphs. MCS is a symmetric distance met-ric, thusd(A, B)=d(B,A). It takes structure as well ascontent into account. According to [3], the distance be-tween two non empty graphsG1andG2is deﬁned as102 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015d(G1,G2)=1\u0000|mcs(G1,G2)|max(|G1|,|G2|)(3)It can also be seen as a similarity measures, assum-ing thats=1\u0000d, as applied in [19]. To compute thissimilarity measure we need to have a graph for each artist.This can be achieved by ﬁnding subgraphs in the graphapproaches deﬁned in Section 3.2. A subgraph will in-clude an artist entity node and its neighboring nodes. Fur-thermore, we apply the notion of h-hop item neighborhoodgraph deﬁned in [26] to a semantic graph. LetG=(E,P)be an undirected graph whereErepresent the nodes (en-tities), andPthe set of edges withP✓E⇥E. Foran artist itemainG, its h-hop neighborhood subgraphGh(a)=(Eh(a),Ph(a))is the subgraph ofGformedby the set of entities that are reachable fromain at mosth hops, according to the shortest path. Following this ap-proach, we obtain an h-hop item neighborhood graph foreach artist node of the semantic graph. Then, maximalcommon subgraph is computed between each pair of h-hop item neighborhood graphs. For each artist, the list ofall similar artists ordered from the most similar to the lessone is ﬁnally obtained.3.3.3 Cumulative cosine similarityFor each pair of conceptsc2Bfyaandc02Bfy0a(asdeﬁned in Section 3.2.3), we are interested in obtaining thesimilarity of their closest senses. This is achieved by ﬁrstderiving the set of associated vectorsVcandV0c0for eachpair of conceptsc, c0, and then optimizingmaxvc2Vc,v0c02V0c0✓vc⇥v0c0||vc|| ||v0c0||◆(4)i.e. computing cosine similarity between all possiblesenses (each sense represented as a vector) in an all-against-all fashion and keeping the highest scoring similarity scorefor each pair. Finally, the semantic similarity between twoartist biographies is simply the average among all the co-sine similarities between each concept pair.4. EXPERIMENTAL SETUPTo evaluate the accuracy of the proposed approaches wedesigned an experimental evaluation over two datasets. Theﬁrst dataset contains 2,336 artists and it is evaluated usingthe list of similar artists provided by the Last.fm API as aground truth. The second dataset contains 188 artists, andit is evaluated against user similarity judgements from theMIREX Audio Music Similarity and Retrieval task. Apartfrom the deﬁned approaches, a pure text-based approachfor document similarity is added to act as a reference forthe obtained results.4.1 Datasets4.1.1 Last.fm datasetA dataset of 2,336 artist biographies was gathered fromLast.fm. The artists in this dataset share a set of restric-tions. Their biography has at least 500 characters and iswritten in English. All of the artists have a correspon-dent Wikipedia page, and we have been able to mapped itautomatically, obtaining the DBpedia URI of every artist.For every artist, we queried the getSimilar method of theLast.fm API and obtained an ordered list of similar artists.Every artist in the dataset fulﬁlls the requirement of havingat least 10 similar artists within the dataset. We used theselists of similar artists as the ground truth for our evaluation.4.1.2 MIREX datasetTo build this dataset, the gathered artists from Last.fm weremapped to the MIREX Audio Music Similarity task dataset.The AMS dataset (7,000 songs from 602 unique artists)contains human judgments of song similarity. Accordingto [29], the similarity between two artists can be roughlyestimated as the average similarity between their songs.We used the same approach in [29], that is, two artists wereconsidered similar if the average similarity score betweentheir songs was at least 25 (on a ﬁne scale between 0 and100).After the mapping, we obtained an overlap of 268 artists.As we want to evaluate Top-10 similarity, every artist inthe ground truth dataset should have information of at least10 similar artists. However, not every artist in the MIREXevaluation dataset fulﬁlls this requirement. Therefore, afterremoving the artists with less than 10 similars, we obtaineda ﬁnal dataset of 188 artists, and used it for the evaluation.4.2 BaselineIn order to assess the goodness of our approaches, we needto deﬁne a baseline approach with which to compare to.The baseline used in this paper is a classic vector-basedmodel approach used in many Information Retrieval sys-tems. A text document is represented as a vector of wordfrequencies (after removing English stopwords and wordswith less than 2 characters), and a matrix is formed by ag-gregating all the vectors. The word frequencies in the ma-trix are then re-weighted using TF-IDF, and ﬁnally latentsemantic analysis (LSA) [8] is used to produce a vectorof concepts for each document. The similarity betweentwo documents can be obtained by using a cosine similar-ity over their corresponding vectors.4.3 Evaluated approachesFrom all possible combinations of knowledge representa-tions, similarity measures and parameters, we selected aset of 10 different approach variants. The preﬁxes AEC,RG and AE refer to the graph representations (see Sec-tions 3.2.1 and 3.2.2). SE refers to the sense embeddingsapproach, and LSA to the latent semantic analysis base-line approach. When these preﬁxes are followed by FT, itmeans that the entities in the graph have been ﬁltered bytype. The second term in the name refers to the similaritymeasure. MCS refers to maximal common subgraph, andSimRank and Cosine to SimRank and cumulative cosinesimilarity measures. MCS approaches are further followedby a number indicating the number of h-hops of the neigh-borhood subgraph.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 103GenresApproach variants Blues Country Edance Jazz Metal Rap Rocknroll OverallGround Truth 5.78 5.46 6.88 7.04 7.10 8.68 5.17 6.53LSA 4.43 4.12 3.80 4.64 5.79 5.08 4.74 4.69RG MCS 1-hop 2.63 3.50 1.50 2.95 4.00 2.54 1.70 2.68RG MCS 2-hop 4.14 4.92 1.69 2.80 3.78 3.06 2.77 3.27AE MCS 5.52 5.15 4.36 7.00 4.34 5.36 4.46 5.11AE-FT MCS 5.43 6.12 4.16 6.20 6.32 5.36 3.77 5.26AEC MCS 1-hop7.225.92 5.24 7.12 5.48 6.92 4.86 6.02AEC MCS 2-hop 4.22 3.69 4.56 6.20 4.55 4.64 4.09 4.54AEC-FT MCS 1-hop 6.916.80 6.04 7.60 6.79 7.12 5.37 6.59AEC-FT MCS 2-hop 4.09 4.36 5.56 6.72 4.39 4.16 3.77 4.67EC SimRank 6.74 5.38 3.16 6.40 4.59 4.44 3.80 4.85SE Cosine 3.39 5.50 5.32 5.16 4.31 5.36 4.31 4.75Table 3. Average genre distribution of the top-10 similar artists using the MIREX dataset. In other words, on average, howmany of the top-10 similar artists are from the same genre as the query artist. LSA stands for Latent Semantic Analysis,RG for Relation Graph, SE for Sense Embeddings, and AE, AEC and EC represent the semantically enriched graphs withArtists-Entities, Artist-Entities-Categories, and Entities-Categories nodes, respectively. As for the similarity approaches,MCS stands for Maximum Common Subgraph.Precision@N nDCG@NApproach variants N=5 N=10 N=5 N=10LSA 0.100 0.169 0.496 0.526RG MCS 1-hop 0.059 0.087 0.465 0.476RG MCS 2-hop 0.056 0.101 0.433 0.468AE MCS 0.106 0.178 0.503 0.517AE-FT MCS 0.123 0.183 0.552 0.562AEC MCS 1-hop 0.120 0.209 0.573 0.562AEC MCS 2-hop 0.086 0.160 0.550 0.539AEC-FT MCS 1-hop0.140 0.218 0.588 0.578AEC-FT MCS 2-hop 0.100 0.160 0.527 0.534EC SimRank 0.097 0.171 0.509 0.534SE Cosine 0.095 0.163 0.454 0.484Table 1. Precision and normalized discounted cumulativegain for Top-N artist similarity using the MIREX dataset(N={5, 10})4.4 Evaluation measuresTo measure the accuracy of the artist similarity we adopttwo standard performance metrics such as Precision@N,and nDCG@N (normalized discounted cumulative gain).Precision@N is computed as the number of relevant items(i.e., true positives) among the top-N items divided byN,when compared to a ground truth. Precision considers onlythe relevance of items, whilst nDCG takes into accountboth relevance and rank position. Denoting withsaktherelevance of the item in positionkin the Top-N list for theartista, then nDCG@N foracan be deﬁned as:nDCG@N=1IDCG@NNXk=12sak\u00001log2(1 +k)(5)where IDCG@N indicates the score obtained by an ideal orperfect Top-N ranking and acts as a normalization factor.We run our experiments forN=5andN= 10.Precision@N nDCG@NApproach variants N=5 N=10 N=5 N=10LSA 0.090 0.088 0.233 0.269RG MCS 1-hop 0.055 0.083 0.126 0.149AE MCS 0.124 0.200 0.184 0.216AE-FT MCS 0.136 0.201 0.224 0.260AEC MCS 1-hop 0.152 0.224 0.277 0.297AEC-FT MCS 1-hop0.160 0.242 0.288 0.317Table 2. Precision and normalized discounted cumulativegain for Top-N artist similarity using the Last.fm dataset(N={5, 10})5. RESULTS AND DISCUSSIONWe evaluated all the approach variants described in Sec-tion 4.3 on the MIREX dataset, but only a subset of themon the Last.fm dataset, due to the high computational costof some of the approaches.Table 1 shows the Precision@N and nDCG@N resultsof the evaluated approaches using the MIREX dataset, whileTable 2 shows the same results for the Last.fm dataset.We obtained very similar results in both datasets. The ap-proach that gets best performance for every metric, datasetand value of N is the combination of the Artists-Entities-Categories graph ﬁltered by types, with the maximal com-mon subgraph similarity measure using a value ofh=1for obtaining the h-hop item neighborhood graphs.Furthermore, given that the MIREX AMS dataset alsoprovides genre data, we analyzed the distribution of gen-res in the top-10 similar artists for each artist, and aver-aged them by genres. The idea is that an artist’s mostsimilar artists should be from the same genre as the seedartist. Table 3 presents the results. Again, the best resultsare obtained with the approach that combines the Artists-Entities-Categories graph ﬁltered by types, with the maxi-104 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015mal common subgraph similarity measure using a value ofh=1for the h-hop item neighborhood graphs.We extract some insights from these results. First, se-mantic approaches are able to improve pure text-based ap-proaches. Second, using knowledge from an external knowl-edge base provides better results than exploiting the rela-tions inside the text. Third, using a similarity measure thatexploits the structure and content of a graph, such as max-imal common subgraph, overcomes other similarity mea-sures based on semantic similarity among entity mentionsin document pairs.6. CONCLUSIONIn this paper we presented a methodology that exploits se-mantic technologies for computing artist similarity, whichcan be divided in three main steps: First, named entitymentions are identiﬁed in the text and linked to a knowl-edge base. Then, these entity mentions are used to con-struct a semantically motivated knowledge representation.Finally a similarity function is deﬁned on top of the knowl-edge representation to compute the similarity between artists.For each one of these steps we explored several approaches,and evaluated them against a small dataset of 188 artist bi-ographies, and a larger dataset of 2,336 artists, both ob-tained from Last.fm.Results showed that a combination of the Artists-Entity-Categories graph ﬁltered by types, and a maximal com-mon subgraph similarity measure using a value ofh=1for obtaining the h-hop item neighborhood graphs, clearlyoutperforms a baseline approach that exploits word co–occurrences and latent factors. In the light of these results,the following conclusions can be drawn: First, semanticapproaches may outperform pure text-based approaches.Second, we observe that knowledge leveraged from exter-nal ontologies may improve the accuracy of the similaritymeasure. Third, reducing noise by ﬁltering linked entitiesby type is a rewarding step that contributes to an improvedperformance. Finally, we show that similarity measuresthat take into consideration the structure and content ofa graph representation may achieve much higher perfor-mance.There are still many avenues for future work. We wouldlike to compare our semantic-based approach with acous-tic and collaborative ﬁltering approaches. In addition, theuse of text sources different from artist biographies can bestudied. Finally, in order to improve the results obtainedby our semantic approach, different state-of-the-art entitylinking tools can be applied, or a speciﬁc entity linking toolfor the music domain could be created for this purpose.7. REFERENCES[1]Bernd Bohnet. Very high accuracy and fast dependencyparsing is not a contradiction. InProceedings of the23rd International Conference on Computational Lin-guistics, COLING ’10, pages 89–97, Stroudsburg, PA,USA, 2010. Association for Computational Linguis-tics.[2]Razvan Bunescu and Marius Pasca. Using Encyclope-dic Knowledge for Named Entity Disambiguation. InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), pages 9–16, Trento, Italy, 2006.[3]Horst Bunke and Kim Shearer. A graph distance met-ric based on the maximal common subgraph.PatternRecognition Letters, 19(3-4):255–259, March 1998.[4]`Oscar Celma, Pedro Cano, and Perfecto Herrera.Search Sounds An audio crawler focused on weblogs.In7th International Conference on Music InformationRetrieval (ISMIR), 2006.[5]`Oscar Celma and Xavier Serra. FOAFing the music:Bridging the semantic gap in music recommendation.Web Semantics, 6:250–256, 2008.[6]Hung Chim and Xiaotie Deng. Efﬁcient phrase-baseddocument similarity for clustering.Knowledge andData Engineering, IEEE Transactions on, 20(9):1217–1229, 2008.[7]William W. Cohen and Wei Fan. Web-collaborativeﬁltering: recommending music by crawling the Web.Computer Networks, 33:685–698, 2000.[8]Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.Indexing by latent semantic analysis.JAsIs, 41(6):391–407, 1990.[9]Daniel P. W. Ellis, Brian Whitman, Adam Berenzweig,and Steve Lawrence. The quest for ground truth in mu-sical artist similarity. InProc. International Symposiumon Music Information Retrieval (ISMIR 2002), pages170–177, 2002.[10]Paolo Ferragina and Ugo Scaiella. Tagme: on-the-ﬂyannotation of short text fragments (by wikipedia en-tities). InProceedings of the 19th ACM internationalconference on Information and knowledge manage-ment, pages 1625–1628. ACM, 2010.[11]X Hu, JS Downie, Kris West, and AF Ehmann. Min-ing Music Reviews: Promising Preliminary Results. InISMIR, pages 536–539, 2005.[12]Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park,and Xiaohua Zhou. Exploiting Wikipedia as externalknowledge for document clustering. InProceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 389–396.ACM, 2009.[13]Ignacio Iacobacci, Mohammad Taher Pilehvar, andRoberto Navigli. Sensembed: Enhancing word embed-dings for semantic similarity and relatedness. InPro-ceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), Beijing, China, July 2015. Association for Com-putational Linguistics.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 105[14]Glen Jeh and Jennifer Widom. Simrank: a mea-sure of structural-context similarity. InProceedings ofthe eighth ACM SIGKDD international conference onKnowledge discovery and data mining, pages 538–543.ACM, 2002.[15]J. Jiang and C. Zhai. A systematic exploration of thefeature space for relation extraction. InHLT-NAACL,pages 113–120, 2007.[16]Jos´e Paulo Leal, Vˆania Rodrigues, and RicardoQueir´os. Computing Semantic Relatedness using DB-Pedia.1st Symposium on Languages, Applications andTechnologies, SLATE 2012, 2012.[17]Hongzhe Liu and Pengfei Wang. Assessing Text Se-mantic Similarity Using Ontology.Journal of Soft-ware, 9(2):490–497, 2014.[18]Beth Logan and Daniel P W Ellis. Toward EvaluationTechniques for Music Similarity.SIGIR 2003: Work-shop on the Evaluation of Music Information RetrievalSystems, pages 7–11, 2003.[19]Mathias Lux and Michael Granitzer. A Fast and SimplePath Index Based Retrieval Approach for Graph BasedSemantic Descriptions.In Proceedings of the SecondInternational Workshop on Text-Based Information Re-trieval, 2005.[20]Pablo N Mendes, Max Jakob, Andr´es Garc´ıa-Silva, andChristian Bizer. DBpedia spotlight: shedding light onthe web of documents. InProceedings of the 7th Inter-national Conference on Semantic Systems, pages 1–8.ACM, 2011.[21]Rada Mihalcea and Andras Csomai. Wikify!: linkingdocuments to encyclopedic knowledge. InProceedingsof the sixteenth ACM conference on Conference oninformation and knowledge management, pages 233–242. ACM, 2007.[22]Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.Linguistic Regularities in Continuous Space WordRepresentations. InHLT-NAACL, pages 746–751,2013.[23]Andrea Moro, Francesco Cecconi, and Roberto Nav-igli. Multilingual word sense disambiguation and entitylinking for everybody. InProceedings of the 13th Inter-nation Conference on Semantic Web (P&D), 2014.[24]Roberto Navigli and Simone Paolo Ponzetto. Babel-Net: Building a very large multilingual semantic net-work. InProceedings of the 48th annual meeting of theassociation for computational linguistics, pages 216–225. Association for Computational Linguistics, 2010.[25]Sergio Oramas, Mohamed Sordo, and Luis Espinosa-anke. A Rule-Based Approach to Extracting Relationsfrom Music Tidbits. In2nd Workshop in KnowledgeExtraction from Text, WWW’15, 2015.[26]Vito Claudio Ostuni, Tommaso Di Noia, Roberto Mi-rizzi, and Eugenio Di Sciascio. A Linked Data Rec-ommender System using a Neighborhood-based GraphKernel.15th International Conference on ElectronicCommerce and Web Technologies, pages 1–12, 2014.[27]Vito Claudio Ostuni, Sergio Oramas, Tommaso DiNoia, Xavier Serra, and Eugenio Di Sciascio. A Se-mantic Hybrid Approach for Sound Recommendation.24th International World Wide Web Conference (WWW2015), pages 3–4, 2015.[28]Mark Rorvig. Images of similarity: A visual explo-ration of optimal similarity metrics and scaling prop-erties of TREC topic-document sets.Journal of theAmerican Society for Information Science, 50(8):639–651, 1999.[29]Markus Schedl, David Hauger, and Juli´an Urbano.Harvesting microblogs for contextual music similarityestimation: a co-occurrence-based framework.Multi-media Systems, 20(6):693–705, 2013.[30]Markus Schedl, Peter Knees, and Gerhard Widmer. AWeb-Based Approach to Assessing Artist Similarityusing Co-Occurrences. InProceedings of the 4th Inter-national Workshop on Content-Based Multimedia In-dexing{(CBMI’05)}, 2005.[31]Mohamed Sordo, Sergio Oramas, and Luis Espinosa.Extracting Relations from Unstructured Text Sourcesfor Music Recommendation. In20th InternationalConference on Applications of Natural Language to In-formation Systems, pages 1–14, 2015.[32]Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, S¨orenAuer, Daniel Gerber, and Andreas Both. Agdistis-agnostic disambiguation of named entities using linkedopen data. InInternational Semantic Web Conference,page 2, 2014.[33]B. Whitman and S. Lawrence. Inferring descriptionsand similarity for music from community metadata. InProceedings of the 2002 International Computer MusicConference, pages 591–598, 2002.106 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Combining Features for Cover Song Identification.",
        "author": [
            "Julien Osmalskyj",
            "Peter Foster",
            "Simon Dixon",
            "Jean-Jacques Embrechts"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417295",
        "url": "https://doi.org/10.5281/zenodo.1417295",
        "ee": "https://zenodo.org/records/1417295/files/OsmalskyjFDE15.pdf",
        "abstract": "In this paper, we evaluate a set of methods for combining features for cover song identification. We first create mul- tiple classifiers based on global tempo, duration, loudness, beats and chroma average features, training a random for- est for each feature. Subsequently, we evaluate standard combination rules for merging these single classifiers into a composite classifier based on global features. We further obtain two higher level classifiers based on chroma fea- tures: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classifiers with the composite classi- fier based on global features, we use standard rank aggre- gation methods adapted from the information retrieval lit- erature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multi- ple statistics. We observe that each combination rule out- performs single methods in terms of the total number of identified queries. Experiments with rank aggregation me- thods show an increase of up to 23.5 % of the number of identified queries, compared to single classifiers.",
        "zenodo_id": 1417295,
        "dblp_key": "conf/ismir/OsmalskyjFDE15",
        "keywords": [
            "evaluation",
            "cover song identification",
            "multiple classifiers",
            "random forest",
            "standard combination rules",
            "composite classifier",
            "global features",
            "chroma features",
            "cross-correlations",
            "rank aggregation methods"
        ],
        "content": "COMBINING FEATURES FOR COVER SONG IDENTIFICATIONJulien OsmalskyjUniversity of LiègeBelgiumjosmalsky@ulg.ac.bePeter Foster, Simon DixonQueen Mary University of LondonUnited Kingdom{p.a.foster, s.e.dixon}@qmul.ac.ukJean-Jacques EmbrechtsUniversity of LiègeBelgiumjjembrechts@ulg.ac.beABSTRACTIn this paper, we evaluate a set of methods for combiningfeatures for cover song identiﬁcation. We ﬁrst create mul-tiple classiﬁers based on global tempo, duration, loudness,beats and chroma average features, training a random for-est for each feature. Subsequently, we evaluate standardcombination rules for merging these single classiﬁers intoa composite classiﬁer based on global features. We furtherobtain two higher level classiﬁers based on chroma fea-tures: one based on comparing histograms of quantizedchroma features, and a second one based on computingcross-correlations between sequences of chroma features,to account for temporal information. For combining thelatter chroma-based classiﬁers with the composite classi-ﬁer based on global features, we use standard rank aggre-gation methods adapted from the information retrieval lit-erature. We evaluate performance with the Second HandSong dataset, where we quantify performance using multi-ple statistics. We observe that each combination rule out-performs single methods in terms of the total number ofidentiﬁed queries. Experiments with rank aggregation me-thods show an increase of up to 23.5 % of the number ofidentiﬁed queries, compared to single classiﬁers.1. INTRODUCTIONRecent years have seen an increased interest in cover songrecognition problems in the Music Information Retrieval(MIR) community. Such systems deal with the problemof retrieving different versions of a known audio query,where a version can be described as a new performanceor recording of a previously recorded track [26]. Coversong recognition is a challenging task because the differentrenditions of a song may differ from the original work interms of tempo, pitch, instrumentation or singing style. Itis therefore an ongoing challenge to design features whichare robust to variation in these musical characteristics.Several approaches have been studied for cover songrecognition problems. In existing work, retrieving cov-c\u0000Julien Osmalskyj, Peter Foster, Simon Dixon, Jean-Jacques Embrechts.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Julien Osmalskyj, Peter Foster, Si-mon Dixon, Jean-Jacques Embrechts. “Combining Features for CoverSong Identiﬁcation”, 16th International Society for Music InformationRetrieval Conference, 2015.ers is usually done by performing pairwise comparisonsbetween audio queries and a reference database [10, 13,14, 26], or by using index-based methods [2, 3, 16, 18]. Acomprehensive review of existing methods is given in [24].All these methods are based on single chroma representa-tion, and do not consider using multiple features. Onlyfew authors have considered the combination of featuresand distance measures. In the work of Foster et al. [11],multiple chroma-based distances are computed, then com-bined after ranking distances. Similarly, in an investigationperformed by Ravuri et al. [22], the authors compute mul-tiple chroma-based input features at multiple time scales,and combine them using a linear model. Finally, authorsin Osmalskyj et al. [20] compare a range of methods forcombining multiple spectral features for cover song iden-tiﬁcation.In this paper, we make a distinction between cover songretrieval and cover song identiﬁcation. In the ﬁrst case,given an audio query, the goal is to retrieve as many coversas possible in a database. In the second case, the goal isto extract some information about the query, similarly towhat ﬁngerprinting systems do [27]. In that case, it is suf-ﬁcient to retrieve only one version of the requested song asa human listener will act as the ﬁnal expert by conﬁrminga match in the returned set of results. Cover song identiﬁ-cation covers a different set of applications, such as identi-ﬁcation of live music, query by example, or retrieving anyinformation related to an unknown version.To take into account multiple sources of musical infor-mation, we propose to process an audio query using severalmethods based on different features. First, supervised ma-chine learning is used to build classiﬁers that return prob-ability estimates of similarity based on global features, in-cluding the tempo, the duration, the loudness, the num-ber of beats and the average chroma features. We thenmerge these classiﬁers using standard probabilistic fusionrules to build a composite classiﬁer. Then, we combinethe latter with two methods based on chroma features. Theﬁrst one is based on comparing histograms of quantizedchroma features, to take into account the harmonic contentof the songs. The second one is based on the cross-correla-tion of chroma sequences and further accounts for tempo-ral information. As the scores returned by all these meth-ods have different scales, we propose to combine them atthe rank level using standard rank aggregation techniquesinspired by the information retrieval literature, especiallytechniques used in web search engines [9, 21, 23]. We462demonstrate that combining global features with chromabased features for cover identiﬁcation improves the resultsover methods based on single features.The remaining of this paper is organized as follows.Section 2 gives an overview of our approach and describesour methodology. Section 3 details the combination rulesevaluated throughout this research. In Section 4, we de-scribe our experimental setup as well as the evaluation pro-cedure. Section 5 presents the realized experiments and theresults obtained. Finally, Section 6 concludes the paper.2. APPROACH OVERVIEWCover songs are different versions of underlying originalworks. The notion of cover therefore closely relates to mu-sical similarity between two songs. A cover song identiﬁ-cation system may therefore be conceived as measuring thesimilarity between two songs to classify them into asimilaror adissimilarclass. We consider a binary notion of coversong identity. Our approach is based on several pairwisecomparison functions calledrejectors, as used in [19]. Arejector is a functionRthat takes two audio tracks as an in-put and returns a score ranking the similarity between twotracks. In a cover song identiﬁcation scenario, one track isthe query while the other one is any track of the database.Rejectors aim to ﬁlter out result candidates, while retain-ing a subset of the database containing at least one matchwith respect to the query.We design several rejectors based on different featuresand combine them such that the global output takes theinformation brought by each rejector into account. Wemake the assumption that the outputs of rejectors basedon different features are independent, and therefore con-tribute to improving the performance of the system. Weﬁrst design multiple probabilistic rejectors based on sev-eral global features using random forests [5]. We next de-sign a rejector based on the quantization of chroma fea-tures. Finally, to take into account temporal information,we implement a rejector that computes cross-correlationsbetween sequences of beat synchronous chroma features.This technique was ﬁrst proposed by Ellis et al. [10] and isused as a baseline in our research.2.1 Probabilistic RejectorsPrevious work, performed by Osmalskyj et al. [19,20], de-monstrates that features such as tempo, duration, or spec-tral features perform better than random. However, as suchfeatures are global and low-dimensional, they do not bringmuch information when taken individually. Based on thatobservation, we select several of these global features andcombine them in order to build a composite classiﬁer thattakes advantage of each single feature. For each feature,we build a probabilistic rejector using supervised machinelearning. To determine the similarity of candidates withrespect to a query, we perform pairwise comparisons usingthe rejectors. Features are extracted from the tracks andused as an input for the learned model to predict a probabil-ity. The probabilistic rejectors are furthermore combinedusing several rules to build a composite rejector.2.2 Codebook RejectorTo take into account the harmonic content of the songs, webuild a rejector based on the quantization of chroma fea-tures. Similar features have been used in [19] and [11].For each track, chroma features are mapped to speciﬁccodewords. A track is then represented by a histogramof the frequency of each codeword, known as abag-of-featuresrepresentation [12]. Codewords are determinedusing an unsupervised K-Means clustering of 200,000 beatsynchronous and unit-normalized chroma vectors. We eval-uated the number of codewords in the range 25 to 100. Bestperformance was achieved with a clustering of 100 code-words. To account for key transpositions, we make useof theOptimal Transposition Index(OTI) [25] as it is astraightforward approach that has been used in many otherinvestigations [1,11,19,24].The similarity between two bag-of-features representa-tions is computed as thecosine similaritybetween bothhistograms. We evaluated the cosine similarity against Eu-clidean and Bhattacharyya distances, as well as a super-vised learning based distance. However, best results wereachieved with the cosine similarity. Furthermore, the co-sine similarity is fast to compute, especially when the inputvectors are normalized to unit norm, as it can be computedas a simple dot product.2.3 Cross Correlation RejectorTo take into account temporal information, we implement abaseline algorithm, initially proposed by Ellis et al. in [10].In that method, songs are represented by beat-synchronouschroma matrices. Beat-tracking is used to align chromason detected beats. Comparing songs is then performed bycross-correlating entire chroma-by-beat matrices. Sharppeaks in the resulting signal indicate a good alignment be-tween the tracks. The input chroma matrices are high-passﬁltered along time. We re-implemented existing work us-ing a high-pass ﬁlter with thealphacoefﬁcient set to 0.99.To compute the cross-correlation, we used a 2-dimensionalFFT. This, on one hand, allows to ﬁnd the optimal lag inthe time dimension, and on the other hand, to ﬁnd the besttransposition shift along the chroma pitches. To emphasizesharp local maxima, the resulting cross-correlation signalis high-pass ﬁltered. The ﬁnal distance between two songsis taken as the reciprocal of the peak value of the cross-correlated signal.3. COMBINING REJECTORSThe core of our method lies in the combination of rejectors.We ﬁrst build probabilistic rejectors based on global fea-tures and combine them to produce a composite rejector.We evaluate several probabilistic fusion rules. Then, wecombine that composite rejector with two other rejectorsbased on chroma features, using rank aggregation meth-ods. This section details both kinds of combinations.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4633.1 Score-based CombinationAs stated in Section 2.1, previous work shows that rejec-tors based on global features such as the tempo or the du-ration of the songs do not produce satisfying results, whentaken individually. It makes therefore sense to investigatetheir combination so that more information is taken into ac-count when comparing two songs. As the global rejectorsestimate probabilities of cover identities, we evaluate sev-eral combination rules to take advantage of each feature.Multiple rules have been proposed as a mean of combiningprobability estimates for classiﬁcation [7, 8,15]. We selectin particular theproduct,thesumand themedianrules [15]and evaluate the combination of our probabilistic rejectorswith them.3.1.1 Product RuleThe probabilistic product decision rule combines the a pos-teriori probabilities generated by the individual rejectorsby a product rule. ForNrejectors, the rule is given byp=1CsN\u00001QNj=1Rj,s1CsN\u00001QNj=1Rj,s+1CdN\u00001QNj=1Rj,d(1)whereCsis the a priori probability of the similar class,Cdis the a priori probability of the dissimilar class, andRj,s(respectivelyRj,d) is the probability that the rejectorRjconsiders the input tracks similar (respectively dissim-ilar). According to [15], it is a severe rule as it is sufﬁcientfor one rejector to inhibit a particular interpretation by out-putting a close to zero probability for it.3.1.2 Sum RuleThe sum probabilistic rule computes the ﬁnal probabilityby computing the sum of each probability and averaging itby the number of rejectors. It is expressed asp=1NNXi=1Rj(2)whereNis the number of rejectors andRjis the probabil-ity returned by rejectorj. For a set of classiﬁers that showindependent noise behavior (e.g. based on different sets offeatures), the errors in the probability estimates are aver-aged by the summation [7]. In particular, the sum rule canbe useful in reducing the noise for large sets of classiﬁers.3.1.3 Median RuleThe median probabilistic rule is computed by taking themedian of the individual probabilities. It is well estab-lished that the median is a robust estimate of the mean.The probabilistic sum in Equation 2 computes the averageof the a posteriori probabilities. Therefore, if one rejectoroutputs an outlier probability, it will affect the ﬁnal prob-ability and it could lead to an incorrect decision. In thatcase, it might be more appropriate to use the median rulerather than the sum rule [15].3.2 Rank AggregationWhile the composite global rejectors built by probabilis-tic fusion rules output probability estimates, two remain-ing rejectors, based on chroma features, return scores ondifferent scales. Consequently, the rules described in Sec-tion 3.1 do not apply for fusing all rejectors together. Aseach rejector returns a list of ordered tracks, we proposeto fuse all rejectors based on rank aggregation techniques,adapted from the information retrieval literature. Rank ag-gregation methods have been particularly studied in theweb literature [9, 21, 23]. Compared to score-based com-bination, rank-aggregation is more suited as it is naturallycalibrated and scale insensitive [21]. Indeed, using the re-turned scores requires to rescale the score values to thesame range (e.g. between 0 and 1) so that different scalesdo not inﬂuence the aggregation results. Another advan-tage of rank aggregation is that the methods are usuallycomputationally cheap as they usually consist in arithmeticoperations on integer ranks. Furthermore, they require noneor few parameters to set up.In the case of cover song identiﬁcation, each rejectorcompares queries to the entire search database and returnsa full permutation of the database. Rank aggregation meth-ods look at the position of each track in each list, and com-pute an aggregated rank to be associated to each track inthe ﬁnal list. A new list of results is then built by set-ting each track at the new rank position.We evaluated threerank aggregation rules:minimum rank, mean rank, medianrank. For each track, we retrieve its rank in each input list,which allows us to aggregate ranks by respectively com-puting the minimum, the mean and the median of the ranksfor each track. The ﬁnal aggregated list is then sorted ac-cording to the new rank. Details of the experiments andthe results are given in Section 5.4. EXPERIMENTAL SETUP4.1 Evaluation DatabaseFor evaluation, we use the Second Hand Song dataset1(SHS), which is a subset of the Million Song Dataset [4](MSD). The SHS is organized into 5,854cliques,whichcorrespond to groups of cover songs of original works.It contains on average 3.097 versions for 5,854 originalsongs. The SHS does not provide audio ﬁles, but con-tains pre-computed features such as the tempo, the dura-tion, the beats, the loudness and the chroma features for18,196 tracks, which makes it suitable for our research.Furthermore, it has been used in several research papers [3,13,14,18], which allows us to compare our results to othermethods.The SHS proposes a pre-deﬁned learning set (LS) andtest set (TS), respectively containing 70% (12,960 tracks)and 30% (5,236 tracks) of the samples. However, to evalu-ate our method with variable LS and TS sizes, we mergedboth provided sets into one large set of 18,196 songs so thatwe can split it to different LS and TS sizes. Typically, since1http://labrosa.ee.columbia.edu/millionsong/secondhand464 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Cliques sizes0 10 20 30 40 50Number of occurences1101001000\nFigure 1: Distribution of the size of the cliques in the SHSdataset. Most of the cliques have a constant size of 2 or 3.However, some cliques contain more elements. The evalu-ation is therefore speciﬁc to that dataset as songs contain-ing more versions will be more likely to be identiﬁed.supervised learning algorithms such as the random forestsrequire a decent amount of training samples, we set the LSto 70% and the TS to 30% of the SHS. However, to inves-tigate how the system behaves on a larger scale, we alsoexperimented with a larger TS containing 10,870 tracks.As the SHS provides a list of known duplicate tracks, weremoved them from the dataset. Note that due to the re-moval of the duplicates, the number of cliques is reducedto 5,828, losing 26 cliques in the process.It should be noted that the cliques in the SHS do nothave a constant size, as can be seen in Figure 1. Althoughmost of the cliques contain two elements, some cliquescontain a lot more cover versions. Such songs containingmany cover versions will be more likely to be identiﬁed inthat evaluation set. The interpretation of the evaluated met-rics remains therefore limited to the SHS dataset, as theycharacterize not only the identiﬁcation algorithm, but alsothe dataset used to assess them.4.2 RejectorsEach rejector described in Section 2 makes use of the fea-tures pre-computed in the SHS. We speciﬁcally make useof the tempo, the duration, the loudness, the beats as wellas the chroma features. The chroma features provided inthe SHS are aligned on onsets rather than on the beats.As our chroma rejectors make use of beat-synchronouschroma features, we aligned the provided chromas on theprovided beats, therefore approaching the beat-aligned rep-resentation proposed in Ellis et al. [10]. Note that in thework of Khadkevich et al. [14], the authors computed theirown chroma features and compared them to the ones pro-vided in the SHS. They report an improvement of 9.87% interms of mean average precision against the chromas pro-vided in the SHS with their chroma extraction algorithm.We therefore expect our method to perform better using adifferent chroma implementation (compared to the resultspresented in Section 5).To account for differences in key for our probabilisticrejector based on average chroma features, we compute theOTI [25] between average chromas and shift one chromaaccordingly, similarly to what is done in Section 2.2 withthe codebook rejector.For the random forest algorithm, we use both a LS con-taining 70% of the cliques (selected at random) of the SHS,and a LS containing 40% of the cliques to study how thesystem behaves on a larger scale. A model is learned foreach feature by processing the samples of the learning set.Note that to avoid overﬁtting during the learning phase,the depth of the trees is limited and the optimal depth isfound by maximizing the area under theReceiver Oper-ating Characteristic(ROC). The models are learned with100 trees and with a maximal depth of 11.4.3 Evaluation Algorithm and MetricsFor evaluation, each track of the TS is taken as a queryand compared to the remaining tracks of the TS using ourrejectors. As the results are provided for each query as alist of tracks ordered by descending order of similarity, wecompute scores such as the Mean Rank (MR) of the ﬁrstidentiﬁed cover, the Mean Reciprocal Rank (MRR) andthe Mean Average Precision (MAP) [17]. The MR cor-responds to the mean position of the ﬁrst identiﬁed query(lower is better). The MRR is computed as the averageof the reciprocal of the rank of the ﬁrst identiﬁed query(higher is better). The MAP for a set of queries corre-sponds to the mean of the average precision scores for eachquery (higher is better). Note that since we are interested incover song identiﬁcation rather than retrieval, we are onlyinterested in retrieving at least one match for each query.Therefore, MR and MRR are more suited than the MAP asthe latter takes into account the position of all matches inthe list of results and is therefore only given as indicator.We also report the results in terms of the number of queriesidentiﬁed intop-kposition, withkset to 10, 100 and 1000.This metric is also used in the MIREX evaluation [6].5. RESULTS5.1 Combining global rejectorsTo investigate the behavior of probabilistic combinationrules, as presented in Section 3.1, we combined our proba-bilistic rejectors based on global features using the productrule, the sum rule and the median rule. We ﬁrst analyzedhow each single rejector behaves on an evaluation databasecontaining 5,464 tracks, compared to random classiﬁca-tion. For the latter case, we simply built a rejector thatoutputs a probability sampled at random from a uniformdistribution. Figure 2 shows curves corresponding to eachrejector. Examination of the curves of the single rejectorsshows that the rejector based on average chroma featuresperforms better than the others (+92.5 % for top-10 and+18.5 % for top-100 compared to tempo). The tempo,beats and duration rejectors have similar curve shapes andperform similarly when taken individually. The compositemedian rule (in dark bold), obtained by fusing all single re-jectors using the rule described in Section 3.1.3, performsbetter than the individual rejectors. In terms of the numberProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 465Cutoff0 0.2 0.4 0.6 0.8 1Lost queries00.10.20.30.40.50.60.70.80.91\nTop 100Top 1000Top 2000RandomTempoDurationBeatsLoudnessChroma avgMedian rule\nFigure 2: Single rejectors based on global features andcomposite rejector resulting from the probabilistic mediancombination rule, with an evaluation set of 5,464 tracks.The composite rejector outperforms any single rejector.of tracks identiﬁed in the top-10, 100 and 1000, there is animprovement of respectively 62.5 %, 43.7 % and 16.4 %,compared to the average chroma rejector. In terms of MRand MRR, the composite rejector improves the scores by24.9 % and 63.2 % respectively. To establish how all com-bination rules behave, Figure 3 displays the curves corre-sponding to each rule. Overall, all rules behave similarly.Zooming in the lower left corner (higher cutoff), the sumrule outperforms the product and the median. Compared tothe median, the number of tracks identiﬁed in the top-5000(lower-left area) is increased by 0.39% (5,419 tracks over5,398). Similarly, the product rule outperforms other rulesin the upper right corner (lower cutoff), with an increase of24 % and 4 % for the top-10 and top-100 over the medianrule. Our ﬁnal choice is the median rule, as it produces aMR of 979.6 compared to 1127 and 1090 with the sum andproduct rules respectively.5.2 Rank aggregation resultsWe combined the composite rejector based on global fea-tures with chroma based rejectors based on the quantiza-tion of chroma features and based on the cross correlationof chroma sequences. The three rank aggregation methodsdescribed in Section 3.2 are evaluated. We ﬁrst report theresults on a TS containing 30% of the SHS samples con-taining 1,745 cliques and 5,464 queries. Table 1 shows thenumber of queries identiﬁed in the top 10, 100 and 1000for each single rejector and for each aggregation rule. Ex-amining the results, we observe that each aggregation ruleoutperforms each single rejector. Best results for the top-10 returned tracks are achieved with the minimum aggre-gation rule. The number of identiﬁed tracks in the top-10goes from 871 with the cross correlation rejector to 1004with the minimum rule, which corresponds to an improve-ment of15.2%.Best results for the top-100 and top-1000returned tracks are both achieved with the mean rule, withimprovements of respectively23.5%and7.19 %. Figure 4shows the performance of the minimum rank aggregationrule against each single rejector. The zooms in the lowerleft and upper right corners indicate that the aggregatedTopProbaClusterXCorrMinMeanMedian1016956087110049729161001064173115232042213921131000373239313386417742144129Table 1: Results for a TS of 1745 cliques and 5,464 tracks.Rank aggregation combinations increase the number ofidentiﬁed queries for each rule.ProbaClusterXCorrMinMeanMedianMR979.6861.41166718.3704.3749.5MRR0.0160.0590.1220.1070.1120.104MAP0.0080.0270.0670.0550.0590.054Table 2: Results for a TS of 1745 cliques and 5,464. Eachrank aggregation combination outperforms single rejectorsin terms of the Mean Rank (MR).rejector performs better across the whole range of cutoffvalues. We also report the standard metrics (described inSection 4.3) in Table 2. Surprisingly, the MRR and MAPvalues are slightly decreased when compared to the bestperforming single rejector (cross-correlation, XCorr in thetable). This might be due to the fact that when we aggre-gate the lists of results (Section 3.2), several tracks can beranked at the same position. This might therefore affectthe metrics. Note however that in terms of the Mean Rank,each combination outperforms each single rejector.To establish how the aggregated rejectors scale on alarger dataset, we evaluated it on a TS containing 60% ofthe samples of the SHS. The LS used for learning the prob-abilistic rejectors is therefore smaller (40%) and producesdecreased performance for the machine learning modelsbuilt with random forests. That new TS contains 10,870tracks, and is chosen to approach the size of the originalSHS training set (12,960 tracks), to compare our results toresults proposed in existing research papers [2,13,14]. Wefurther increased the size of the TS by decreasing the sizeof the LS to 30% and 20% of the SHS. However, the pro-duced results with the probabilistic rejectors showed worseperformance, due to the lack of enough learning samplesfor the random forest algorithm. Table 3 shows the resultsof our method against existing work. Note that care shouldbe taken while reading these results as our probabilisticmodels do not perform as well as with a larger LS, and asthe sizes and the contents of both evaluation databases dif-fer. In terms of the MR, our method is ranked at the secondposition.6. CONCLUSIONIn this paper, we evaluated multiple techniques for com-bining distances and features for cover song identiﬁcation.We ﬁrst made use of random forests to design probabilisticrejectors based on global features. We evaluated severalstandard combination rules such as the sum, the productand the median rules to build a composite rejector. Resultsshow that combining single rejectors based on global fea-466 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Cutoff0 0.2 0.4 0.6 0.8 1Lost queries00.10.20.30.40.50.60.70.80.91\nTop 100Top 1000RandomMedian ruleProduct ruleSum rule\nCutoff0 0.05 0.1 0.15 0.2 0.25 0.3Lost queries00.010.020.030.040.050.06\nRandomMedian ruleProduct ruleSum ruleCutoff0.97 0.975 0.98 0.985 0.99 0.995 1Lost queries0.750.80.850.90.951\nRandomMedian ruleProduct ruleSum ruleFigure 3: Performance of the probabilistic sum, product and median combination rules to build a composite rejector basedon multiple global features. The second ﬁgure is a zoom of the left lower part (high cutoff). The sum rule performs slightlybetter in that area. The third ﬁgure is a zoom of the upper right area. The product rule performs slightly better there.\nCutoff0 0.2 0.4 0.6 0.8 1Lost queries00.10.20.30.40.50.60.70.80.91\nTop 100Top 1000Random rejectorComposite rejectorQuantizationXCorrMin aggregation\nCutoff0 0.05 0.1 0.15 0.2 0.25 0.3 0.35Lost queries00.010.020.030.040.050.06\nRandom rejectorComposite rejectorQuantizationXCorrMin aggregationCutoff0.99 0.992 0.994 0.996 0.998 1Lost queries0.70.750.80.850.90.951\nRandom rejectorComposite rejectorQuantizationXCorrMin aggregationFigure 4: Performance of the minimum aggregation rule against rejectors based on global features (composite), quanti-zation of chroma features and cross-correlation of chroma sequences (XCorr) on a database containing 5,464 tracks. Thesecond ﬁgure is a zoom of the lower left corner (high cutoff) and the third ﬁgure is a zoom of the upper right corner (lowcutoff). In each case, the aggregation increases the number of identiﬁed tracks.MethodMRMAPKhadkevich et al. [14]958.20.10Rank Aggregation(10,870 tracks)1,455.60.048Bertin-Mahieux et al. 2D-FTM (200 pcs) [3]3,0050.09Humphrey et al. [13]1,8440.28Table 3: Comparison of the rank aggregation methodagainst existing methods evaluated on the SHS originaltraining set. Care should be taken when reading the resultsas the original SHS training set contains 12,960 songs, andour subset contains 10,870 tracks sampled from the SHS.tures improves the performance compared to single clas-siﬁers. We proposed to combine the composite rejectorbased on global features with rejectors based on chromafeatures. To take into account the harmonic content ofthe songs, we introduced a rejector based on comparinghistograms of quantized chroma features. To account fortemporal information, we further implemented a baselinerejector performing cross-correlations between sequencesof chroma features. As all these rejectors return valueson different scales, we proposed to combine them at therank level. We evaluated several rank aggregation meth-ods such as the mean, the median and the minimum ag-gregation rules. We conducted experiments on the SecondHand Song dataset and observed that aggregation meth-ods outperform methods in isolation for cover song identi-ﬁcation. Results are provided in terms of standard metricssuch as the mean rank of the ﬁrst match, the mean recip-rocal rank and the mean average precision, as well as interms of the total number of queries identiﬁed in the top-kresults. Compared to single rejectors, the minimum ag-gregation rule shows an improvement of up to 23.5 % ofthe number of queries identiﬁed in the top-100 returnedtracks. Comparing our results to existing work, we observethat our method does not perform as well as other methodsin terms of mean average precision. However, in termsof mean rank of the ﬁrst identiﬁed query, the results arecomparable to related methods and rank our method at thesecond position. Although our method does not producestate-of-the-art results, we showed that aggregating multi-ple features and distance measures does increase the num-ber of identiﬁed queries. These results suggest that com-bining many other features as well as multiple comparisonalgorithms could lead to signiﬁcant improvements in anycover song identiﬁcation system. Future work therefore in-cludes more experiments with features taking into accounte.g. the melodic line of the songs, or structural informa-tion. In any case, many combining experiments should stillbe performed to improve state-of-the art results.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 4677. REFERENCES[1]T. Ahonen. Compression-based clustering of chroma-gram data: New method and representations. InInter-national Symposium on Computer Music Modeling andRetrieval, pages 474–481, 2012.[2]T. Bertin-Mahieux and D. Ellis. Large-scale cover songrecognition using hashed chroma landmarks. InPro-ceedings of IEEE Workshop on Applications of SignalProcessing to Audio and Acoustics, 2011.[3]T. Bertin-Mahieux and D. Ellis. Large-scale cover songrecognition using the 2D Fourier transform magnitude.InProceedings of the 13th International Conference onMusic Information Retrieval (ISMIR 2012), 2012.[4]T. Bertin-Mahieux, D. Ellis, B. Whitman, andP. Lamere. The million song dataset. InInt. Symp. Mu-sic Inform. Retrieval (ISMIR), 2011.[5]L. Breiman. Random forests.Machine Learning,45(1):5–32, Jan. 2001.[6]J. Downie, A. Ehmann, M. Bay, and M. Jones. The mu-sic information retrieval evaluation exchange: Someobservations and insights. InAdvances in music infor-mation retrieval, pages 93–115. Springer, 2010.[7]R. Duin. The combining classiﬁer: to train or not totrain? InIEEE Int. Conf. Pattern Recognition (ICPR),volume 2, pages 765–770, Quebec City, Canada, Aug.2002.[8]R. Duin and D. Tax. Experiments with classiﬁer com-bining rules. InMultiple Classiﬁer Systems, volume1857 ofLecture Notes in Comp. Science, pages 16–29.Springer, 2000.[9]C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.Rank aggregation methods for the web. InProceedingsof the 10th international conference on World WideWeb, pages 613–622. ACM, 2001.[10]D. Ellis and G. Poliner. Identifying cover songs withchroma features and dynamic programming beat track-ing. InInt. Conf. Acoustics, Speech and Signal Process.(ICASSP), volume 4, 2007.[11]P. Foster, S. Dixon, and A. Klapuri. Identifyingcover songs using information-theoretic measures ofsimilarity.Audio, Speech, and Language Processing,IEEE/ACM Transactions on, 23(6):993–1005, June2015.[12]Z. Fu, G. Lu, K. Ming Ting, and D. Zhang. Musicclassiﬁcation via the bag-of-features approach.PatternRecognition Letters, 32(14):1768 – 1777, 2011.[13]E. Humphrey, O. Nieto, and J. Bello. Data driven anddiscriminative projections for large-scale cover songidentiﬁcation. InInt. Symp. Music Inform. Retrieval(ISMIR), 2013.[14]M. Khadkevich and M. Omologo. Large-scale coversong identiﬁcation using chord proﬁles. InInt. Symp.Music Inform. Retrieval (ISMIR), pages 233–238,2013.[15]J. Kittler, M. Hatef, R. Duin, and J. Matas. On combin-ing classiﬁers.IEEE Trans. Pattern Anal. Mach. Intell.,20(3):226–239, Mar. 1998.[16]F. Kurth and M. Muller. Efﬁcient index-based audiomatching.Audio, Speech, and Language Processing,IEEE Transactions on, 16(2):382–395, 2008.[17]C. Manning, P. Raghavan, and H. Schütze.Introductionto information retrieval, volume 1. Cambridge univer-sity press Cambridge, 2008.[18]B. Martin, D. Brown, P. Hanna, and P. Ferraro. Blastfor audio sequences alignment: A fast scalable coveridentiﬁcation tool. InISMIR, pages 529–534, 2012.[19]J. Osmalskyj, S. Piérard, M. Van Droogenbroeck, andJ.-J. Embrechts. Efﬁcient database pruning for large-scale cover song recognition. InInt. Conf. Acoustics,Speech and Signal Process. (ICASSP), pages 714–718,Vancouver, Canada, May 2013.[20]J. Osmalskyj, M. Van Droogenbroeck, and J.-J. Em-brechts. Performances of low-level audio classiﬁers forlarge-scale music similarity. InInternational Confer-ence on Systems, Signals and Image Processing (IWS-SIP), pages 91–94, Dubrovnik, Croatia, May 2014.[21]R. Prati. Combining feature ranking algorithmsthrough rank aggregation. InNeural Networks(IJCNN), The 2012 International Joint Conference on,pages 1–8. IEEE, 2012.[22]S. Ravuri and D. Ellis. Cover song detection: from highscores to general classiﬁcation. InAcoustics Speechand Signal Processing (ICASSP), 2010 IEEE Interna-tional Conference on, pages 65–68. IEEE, 2010.[23]D. Sculley. Rank aggregation for similar items. InSDM, pages 587–592. SIAM, 2006.[24]J. Serra.Identiﬁcation of versions of the same musi-cal composition by processing audio descriptions. PhDthesis, Universitat Pompeu Fabra, Barcelona, 2011.[25]J. Serra and E. Gómez. Audio cover song identiﬁ-cation based on tonal sequence alignment. InAcous-tics, Speech and Signal Processing, 2008. ICASSP2008. IEEE International Conference on, pages 61–64.IEEE, 2008.[26]J. Serra, E. Gomez, P. Herrera, and X. Serra. Chromabinary similarity and local alignment applied to coversong identiﬁcation.IEEE Trans. Audio, Speech andLanguage Process., 16(6):1138–1152, 2008.[27]A. Wang. An industrial-strength audio search algo-rithm. InInt. Symp. Music Inform. Retrieval (ISMIR),pages 7–13, 2003.468 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Improving MIDI Guitar&apos;s Accuracy with NMF and Neural Net.",
        "author": [
            "Masaki Otsuka",
            "Tetsuro Kitahara"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417531",
        "url": "https://doi.org/10.5281/zenodo.1417531",
        "ee": "https://zenodo.org/records/1417531/files/OtsukaK15.pdf",
        "abstract": "In this paper, we propose a method for improving the accu- racy of MIDI guitars. MIDI guitars are useful tools for var- ious purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have suf- ficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF’s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for inte- grating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two out- puts. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively.",
        "zenodo_id": 1417531,
        "dblp_key": "conf/ismir/OtsukaK15",
        "keywords": [
            "MIDI guitars",
            "accuracy improvement",
            "audio transcription",
            "non-negative matrix factorization (NMF)",
            "neural network",
            "F-score",
            "policy of outputting common parts",
            "experimental results",
            "various purposes",
            "jam session system"
        ],
        "content": "IMPROVING MIDI GUITAR’S ACCURACY WITH NMF AND NEURAL NETMasaki OtsukaandTetsuro KitaharaGraduate School of Integrated Basic Sciences, Nihon University{masaki,kitahara}@kthrlab.jpABSTRACTIn this paper, we propose a method for improving the accu-racy of MIDI guitars. MIDI guitars are useful tools for var-ious purposes from inputting MIDI data to enjoying a jamsession system, but existing MIDI guitars do not have suf-ﬁcient accuracy in converting the performance to an MIDIform. In this paper, we make an attempt on improvingthe accuracy of a MIDI guitar by integrating it with anaudio transcription method based on non-negative matrixfactorization (NMF). First, we investigate an NMF-basedalgorithm for transcribing guitar performances. Althoughthe NMF is a promising method, an effective post-process(i.e., converting the NMF’s output to an MIDI form) is anon-trivial problem. We propose use of a neural networkfor this conversion. Next, we investigate a method for inte-grating the outputs of the MIDI guitar and NMF. Becausethey have different tendencies in wrong outputs, we takean policy of outputting only common parts in the two out-puts. Experimental results showed that the F-score of ourmethod was 0.626 whereas those of the MIDI-guitar-onlyand NMF-and-neural-network-only methods were 0.347and 0.526, respectively.1. INTRODUCTIONA MIDI guitar, which outputs the user’s performance datainto the MIDI format in real time, is useful for guitarists toengage in various music activities such as inputting MIDIdata into a computer and enjoying the use of a jam sessionsystem. However, the accuracy of MIDI guitars is not ashigh as a MIDI keyboard because the MIDI guitar detectsthe strings’ vibration by analyzing the temporal changes inthe magnetic ﬁeld around the strings.There have been many attempts made to transcribe gui-tar performances [1–3, 5, 8–10, 12]. Arimoto et al. remadethe PreFEst method, originally developed by Goto [4],for a guitar based on physical constraints on ﬁngeringforms [1]. Yazawa et al. also focused on latent harmonicallocation for a guitar based on physical constraints in ﬁn-gering form [12]. Barbancho et al. furthermore investi-\nc\u0000Masaki Otsuka and Tetsuro Kitahara.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Masaki Otsuka and Tetsuro Kita-hara. “Improving MIDI Guitar’s Accuracy with NMF and Neural Net”,16th International Society for Music Information Retrieval Conference,2015.gated these physical constraints [2]. Fiss et al. constructeda system that transcribes a guitar performance as a tabla-ture [3]. This system estimates not only the notes that areplayed, but it also estimates how the notes are played (i.e.,string number and fret number) through audio signal pro-cessing. O’Grady et al. considered both the use of non-negative matrix factorization (NMF) [7] and a hardwareimprovement of a MIDI guitar for accurate guitar perfor-mance transcription [8]. Harquist also proposed a real-timeguitar transcription method using NMF [5]. In addition,there have been attempts to improve guitar performancetranscription by integrating audio signal processing withcomputer vision [9, 10].In this paper, we focus on improving the accuracy ofMIDI guitars by integrating them with audio signal pro-cessing technologies (especially NMF). Almost all MIDIguitars have an audio output jack for connecting to a guitarampliﬁer as well as a MIDI output. By connecting this au-dio output jack to a PC’s audio input jack, the guitar’s au-dio signal can be analyzed. By inputting the guitar’s MIDIoutput to that PC, the audio result and the guitar’s MIDIoutput can be integrated. Thus, introducing audio signalprocessing to a MIDI guitar does not require any specialequipment or hardware improvements. O’Grady et al. fo-cused on a similar technique as we employ here but theirmethod involved hardware improvements [8]; our method-ology requires no hardware improvement. Using computervision [9, 10] is an interesting approach, but it requires in-stalling a camera and it may restrict the player’s motions.Using physical constraints in ﬁngering forms [1, 2, 12] isa common and promising approach, but we dare to adoptthe approach of exploring how much we can improve theaccuracy without using physical constraints. Physical con-straints can be applied to our method for further improve-ments in the future.The rest of this paper is organized as follows: In Sec-tion 2 we propose a method for transcribing guitar perfor-mance using NMF. In Section 3 we describe a method forintegrating NMF-based transcription outputs and the MIDIguitar’s outputs. In Section 4 we report our experimentalresults. Finally, we conclude the paper in Section 5.2. AUDIO-TO-MIDI CONVERSION WITH NMFNMF is a technique for decomposing a matrixVinto theproduct of two matricesWandH, that is,V⇠=WH,whereWis a basis matrix andHis a gain matrix. A typical413usage of NMF in automatic music transcription is to applyNMF to a spectrogram. Then, the basis matrixWis anarray ofNcolumn vectorswnthat represent the spectrumof each noten; the gain matrix is an array ofNrow vectorshnthat represent a temporal sequence of the gain for thebasis vectorwn. Because the gain vectorhnrepresents anapproximation of a temporal sequence of the amplitude forthe noten, the onset and offset times for a notencan beidentiﬁed by thresholdinghn; steep rises in the time series{hn,t}trepresent onsets and steep drops represent offsets.However, there are two problems with this technique.The ﬁrst one is that standard NMF is applicable only af-ter the entire spectrogram is obtained. This fact meansthat standard NMF cannot be used for real-time process-ing. The second problem is that it is difﬁcult to determine auniversally appropriate threshold because the actual gainsvary according to playing style, strings, and other factors.Thus, the issues to be resolved here can be summarized asfollows:Issue 1How to apply NMF to real-time processingIssue 2How to determine an appropriate threshold de-pending on the playing style, strings, etc.In this paper, we resolve these issues as follows:Solution 1We ask the user to play a chromatic scale (fromthe lowest note to the highest note) for each string inadvance and apply NMF to thispreliminary perfor-mance. We assume that the spectrum of each noteis similar enough between the preliminary and tar-get performances1if the same person plays the sameinstrument in the same way. Under this assumption,the basis matrix calculated from the preliminary per-formance is then used to obtain the gain vectors forthe target performance.Solution 2We introduce one more preliminary perfor-mance and adaptively determine the threshold. Thispreliminary performance has a similar musical fea-ture to the target performance, and we ask the userto play the phrase speciﬁed by the system accurately(thus, the system knows the ground truth). Adap-tation of the threshold using these data is approxi-mately equivalent to learning a neural network. Wetherefore learn how high the gain is and how steeplythe gain rises at onsets with a neural network and usethis neural network for detecting onsets.In the rest of this section, we ﬁrst describe a method inwhich only Solution 1 is introduced (we call this methodthebaseline method). Next, we introduce Solution 2 to thisbaseline method.1Thetarget performancerefers to the performance to be converted tothe MIDI format.2.1 Baseline method — Introducing Solution 1 onlyStage 1: Estimating basis matrix from preliminaryperformanceAfter the user plays all chromatic notes successively foreach stringk(called the1st preliminary performance), thespectrogramVkis calculated using the short-term Fouriertransform with a 4096-point Hamming window shifted by10 ms (we suppose 44.1-kHz sampling). Then, the spectro-gramVkis decomposed into the basis matrixWkand thegain matrixHkusing NMF. To avoid that spectral peaksfor different notes are mixed into a single basis vector, weprepare 35 basis vectors for each string even though eachstring has 23 notes. We then obtain 23 basis vectors bymerging pairs of basis vectors that have a high cosine sim-ilarity.Stage 2-1: Estimating gain vectors for targetperformanceThe user plays the target performance (i.e., the perfor-mance to be converted to the MIDI format). As the userplays, the power spectrumvt(wheretis time) is obtainedvia the Fourier transform, and then the gain vectorht,kforeach stringkis calculated. The gain vectorht,kis deﬁnedasht,k=W\u00001kvt, whereWkis the basis matrix for thestringk, obtained in Stage 1. BecauseWkis not a squarematrix in general, its inverse matrix cannot be calculated.We therefore use a pseudo-inverse matrix [6] instead.Stage 2-2: Generating MIDI messages by thresholdinggain vectorsAfter the gain vectorht,kis calculated, MIDI messagesare generated. When then-th element ofht,khas a highervalue than the thresholdh0but that ofht\u00001,kdoes not(that is,ht\u00001,k,nh0<ht,k,n), a MIDI Note On mes-sage for the note number corresponding to fretnof stringkis generated. Whenht\u00001,k,n>h0\u0000ht,k,n, a MIDINote Off message is generated. Whenht\u00002,k,n>ht\u00001,k,nandht\u00001,k,n<ht,k,n, even if bothht\u00001,k,nandht,k,narehigher thanh0, we can consider that a note is played againbefore the previous note is decayed enough. If this is thecase, a Note Off message is generated at timet\u00001and aNote On message at timet.2.2 Introducing Solution 2The method discussed above involves thresholding but theappropriate threshold depends on various factors includingthe individual instrument, the strength of picking, and thecharacteristics of the player. In practice, dynamic adjust-ment of the threshold is not straightforward. We thereforeadd one more preliminary performance (called the2nd pre-liminary performance) and adjust the threshold using this2nd preliminary performance under the assumption that acorrect transcription of the 2nd preliminary performancehas been given. Letht,k,nbe the gain in the 2nd prelimi-nary performance at timet, stringk, and fretn. Whethertis an onset time at fretnof stringkin this peformance414 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1. Neural network that we employcan be identiﬁed from the correct transcription, then it isrepresented as follows:st,k,n=⇢1(tis an onset time at fretnof stringk)0(else)What to be solved here is to ﬁndh0such thatht,k,n>h0iffst,k,n=1. Thish0can be estimated by minimizingE(h0)=Pt,k,n{\u0000(\u0000h0+ht,k,n)\u0000st,k,n}2, where\u0000(x)is the sigmoid function, that is,\u0000(x)=1/(1 +e\u0000x).Itis equivalent to training a neural network. The temporaldifferential ofht,k,nis also considered important for onsetdetection, so we obtain a neural network shown in Figure1 by adding such features.Stage 1: Estimating basis matrix from 1st preliminaryperformanceIn the same way as Stage 1 of the baseline method, the 1stpreliminary performance is played by the user, and thenthe basis matrixWkfor each stringkis calculated.Stage 2-1: Estimating gain vectors for 2nd preliminaryperformanceThe user plays the 2nd preliminary performance. As he/sheplays, the power spectrumvtand the gain vectorht,k=W\u00001kvtfor each stringkare calculated every 10 ms in thesame way as in Stage 2-1 of the baseline method.Stage 2-2: Learning neural networkFor each elementht,k,nofht,k, the following steps areperformed ifht,k,nis a peak:1.Features are extracted fromht,k,nand are set to thevectorxt,k,n. In the current implementation, the fol-lowing feature vector is used:xt,k,n=(ht,k,n,ht,k,n\u0000ht\u00002,k,n,ht,k,n\u0000ht0,k,n),wheret0is the time of the last valley beforetin{h⌧,k,n}⌧=0,···,t, in other words, the maximumvalue of⌧(<t) such thath⌧,k,n<h⌧\u00001,k,nandh⌧,k,n<h⌧+1,k,n.2.The supervisionst,k,nis deﬁned as described above.3.The neural networks shown in Figure 1 are trainedusing backpropagation such that the difference be-tween the value of the output nodeyt,k,nand the su-pervisonst,k,nis minimized. We prepare and traindifferent neural networks for different string, but weuse the same neural network for different frets of thesame string due to a limited number of training data.Each neural network has a single hidden layer con-sisting of three to ten nodes (we try all cases andpresent the best result).In the training, the number of data with supervisions of1 and 0 are balanced.Stage 3-1: Estimating gain vectors for targetperformanceDuring the target performance, the spectrum and the gainvector are calculated every 10 ms in the same way as inStage 2-1.Stage 3-2: Generating MIDI messages based on neuralnetworkThe feature vectorxt,k,nis calculated in the same way asin Stage 2-2. Then, the value of the output nodeyt,k,ninthe trained neural network is calculated for each time, eachstring, and each fret. When this value is higher than 0.5, aMIDI Note On message for the corresponding note numberis generated.Theoretically, offsets can also be learned and estimatedwith a neural network. However, for simplicity, offsets aredetected in the same way as in the baseline method.3. INTEGRATION OF MIDI GUITAR AND NMFIn this section, we describe a method for integrating theoutputs of a MIDI guitar and the method discussed in Sec-tion 2.2. When we discuss how to integrate two differ-ent outputs, we should consider a tradeoff between recallrates and precision rates. We believe that precision is moreimportant in our task because false positives (MIDI mes-sages generated but actually not played) directly result indissonant sound; false negatives (MIDI messages not gen-erated but actually played) do not. We therefore adopt anapproach of outputting the common part of the two out-puts.Stages 1 to 3-2We perform the same process as in Section 2.2 is per-formed until Stage 3-2. Although in the method in Section2.2 the value of the output nodeyt,k,nis thresholded, it isnot thresholded here becauseyt,k,nis used in Stage 4.Stage 4: Integration with MIDI guitar outputsFrom the output of the MIDI guitar, we obatin the follow-ing value:Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 415mt,k,n=8<:1\u0000↵(the note corresponding to fretnof stringkis being played.)↵(else)“(A note is) being played” means the state in which theMIDI guitar had output a MIDI Note On message for thisnote number but has not yet output a MIDI Note Off mes-sage. Note that it represents the MIDI guitar’s estima-tion, so it may not agree with whether that note is actuallyplayed. In the equation above,↵is a parameter and is setto 0.3 in the current implementation.Then,zt,k,n=mt,k,nyt,k,nis calculated. The guitarcan play only one note at each string at the same time. Asa result,ˆnt,k= argmaxnzt,k,nis calculated and the fretˆnt,kof stringkis considered tobe played at timet. Then, a MIDI Note On message isgenerated for the corresponding note number. However, nofret is considered to be played on stringkat timetwhenevery element of{zt,k,n}nis lower than a certain threshold(0.3 in the current implementation).4. EXPERIMENTS4.1 Experiment 1 — Use of neural networkExperimental conditionsTo conﬁrm the effect of the use of the neural network de-scribed in Section 2.2, we conducted an experiment aboutconverting guitar performances to the MIDI format. Weused a Roland GK-3 installed into a Stratocaster as a MIDIguitar with a guitar synthesizer (Roland GR-55). The ﬁrstauthor of this paper played 79 four-measure funk rhythmicphrases taken from a guitar phrase book [11]. Of these 79phrases, those shown in Figures 2 and 3 were used for the2nd preliminary performance. The remaining phrases wereused for test data. We attempted the following three cases:Case 1Using only Figure 2,Case 2Using only Figure 3, andCase 3Using both Figures 2 and 3for the 2nd preliminary performance. We used neural net-works that had three to ten nodes in the hidden layer andwill present only the best result.Experimental resultsThe results are listed in Table 1. The number of hiddennodes was three. For brevity, we list the accuracy for eachchapter instead of each phrase in [11]. In [11], phrases aredivided into 16 chapters according to their playing styles,and each chapter includes several phrases. Whereas theF-score for the baseline method was 0.516 on average, theF-score for the proposed method was 0.526 in Case 3. Thisdifference is not very large but one must consider that theproposed method acquired the best threshold because the\nFigure 2. Phrase 1 for the 2nd preliminary performance\nFigure 3. Phrase 2 for the 2nd preliminary performancelisted result for the baseline method was the best one ingiven various thresholds.Figure 4 shows an example of the experimental results.While the baseline method generated many false positives,especially in the ﬁrst measure, most of these false positiveswere eliminated by the proposed method. Thus, the preci-sion rate was improved from 0.659 to 0.692. However,some positives were eliminated so the recall rate decreasedslightly (from 0.562 to 0.556).Figure 5 shows another example. Whereas the base-line method generated many false negatives from the be-ginning to the end, such false negatives were eliminatedby the proposed method. The precision rate was improvedfrom 0.465 to 0.661.4.2 Experiment 2 — IntegrationExperimental conditionsTo conﬁrm the effect of the integration described in Sec-tion 3, we conducted on audio-to-MIDI conversion of gui-tar performances using the MIDI guitar only (MGT), theNMF and neural network only (NMF+NN; Section 2.2),and their integration (INT; Section 3). We used the samedata as Experiment 1. For the 2nd preliminary perfor-mance, we used both Figures 2 and 3 (Case 3). Also inthis experiment, we used neural networks that had three toten nodes in the hidden layer, and will present only the bestresult for each condition.Experimental resultsThe results are listed in Table 2. The numbers of hiddennodes were three for NMF+NN and nine for INT. Whereasthe precision rates for MGT and NMF+NN were 0.258 and0.513, respectively, the precision rate improved to 0.660416 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Table 1. Result of Experiment 1 (R: recall rates,P: precision rates,F: F-score)Baeline methodProposed method(Simple thresholding)Case 1Case 2Case 3ChaptersRPFRPFRPFRPF10.640 0.509 0.5520.636 0.422 0.5030.642 0.376 0.4730.611 0.458 0.52120.647 0.489 0.5550.624 0.483 0.5380.661 0.490 0.5630.635 0.574 0.59530.579 0.496 0.5310.468 0.483 0.4720.603 0.502 0.5410.525 0.560 0.53940.536 0.510 0.5190.554 0.509 0.5290.576 0.479 0.5210.562 0.524 0.54150.731 0.557 0.5850.759 0.399 0.5030.809 0.431 0.5500.838 0.437 0.56160.538 0.410 0.4650.531 0.459 0.4910.581 0.427 0.4910.539 0.460 0.49670.580 0.601 0.5640.562 0.624 0.5690.640 0.569 0.5900.636 0.668 0.63580.528 0.577 0.5400.499 0.481 0.4880.544 0.520 0.5280.518 0.536 0.52590.401 0.489 0.4310.415 0.418 0.4130.425 0.423 0.4220.416 0.451 0.430100.464 0.403 0.4290.476 0.346 0.3970.472 0.326 0.3760.442 0.350 0.382110.432 0.621 0.5050.445 0.621 0.5100.466 0.589 0.5160.470 0.643 0.535120.313 0.600 0.4110.343 0.502 0.4070.373 0.429 0.3990.399 0.567 0.468130.621 0.527 0.5410.568 0.505 0.5040.652 0.522 0.5590.611 0.541 0.543140.412 0.442 0.4220.395 0.425 0.4070.456 0.425 0.4360.441 0.473 0.451150.576 0.407 0.4740.460 0.362 0.3840.558 0.521 0.4630.520 0.554 0.418Final0.475 0.413 0.4220.480 0.400 0.4240.485 0.377 0.4150.484 0.416 0.437Average0.530 0.503 0.5160.513 0.465 0.4880.559 0.463 0.5060.540 0.513 0.526\nFigure 4. Example of Experiment 1 (Track 47-2)via the integration. This fact arises because many falsepositives were eliminated in INT. The recall rate for INTwas 0.595; that for MGT was 0.528. The recall rate im-proved because sequential short notes were fused in MGT,as will be illustrated below, but such errors rarely appearedin INT. Accordingly, the F-score for INT was 0.626; theF-scores for MGT and NMF+NN were 0.347 and 0.526,respectively.Focusing on the results for each chapter, we can see thatthe recall rates for 11 chapters (Chapters 1, 2, 3, 4, 6, 7, 8,10, 12, 13, and ﬁnal) was improved compared with MGT.However, for other chapters (Chapters 5, 9, 11, 14, and15) the recall rates decreased. Chapter 5 in [11] featuresmonophonic phrases but the 2nd preliminary performancedid not include monophonic phrases. This mismatch iswhy the recall rate decreased in Chapter 5. The phrasesin Chapters 9, 14, and 15 also included monophonic notes.On the other hand, the precision rate improved for everychapter compared with MGT. In particular, the precisionrate improved by more than 0.5 for Chapters 5, 11, and 13.\nFigure 5. Example of Experiment 1 (Track 09-1)Figure 6 shows an example of the results. While MGTgenerated false positives in the whole phrase, such falsepositives were eliminated in INT as described above. Thus,the precision rate signiﬁcantly improved from 0.48 (MGT)to 0.83 (INT). At the same time, however, some true pos-itives were also eliminated. On the other hand, MGTcaused errors in the fusion of sequential short notes; INTreduced such errors. Eventually, the recall rate increasedfrom 0.60 (MGT) to 0.67 (INT).Figure 7 shows another example. Similar to the datashown in Figure 6, MGT resulted in errors due to the fu-sion of sequential short notes at the ﬁrst beat of every mea-sure. In INT, such errors were corrected. Thus, the recallrate was improved from 0.61 (MGT) to 0.80 (INT). In ad-dition, MGT generated false positives from the second halfof the ﬁrst measure to the last measure; these false positiveswere eliminated in INT. Thus, the precision rate improvedfrom 0.25 (MGT) to 0.78 (INT). Accordingly, the F-scoreimproved from 0.36 (MGT) to 0.79 (INT).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 417Table 2. Result of Experiment 2 (R: recall rates,P: precision rates,F: F-score)MIDI guitar (MGT)NMF+NNIntegration (INT)ChaptersRPFRPFRPF10.668 0.445 0.5130.611 0.458 0.5210.758 0.589 0.66020.630 0.230 0.3380.635 0.574 0.5950.717 0.648 0.67930.380 0.310 0.3270.525 0.560 0.5390.763 0.613 0.67940.505 0.233 0.3130.562 0.524 0.5410.508 0.670 0.57550.805 0.110 0.1950.838 0.437 0.5610.731 0.643 0.67560.583 0.187 0.2870.539 0.460 0.4960.649 0.660 0.64170.493 0.205 0.2780.636 0.668 0.6350.597 0.660 0.60680.503 0.215 0.2950.518 0.536 0.5250.593 0.644 0.60290.533 0.345 0.4080.416 0.451 0.4300.398 0.603 0.469100.463 0.190 0.2670.442 0.350 0.3820.732 0.545 0.623110.425 0.345 0.3750.470 0.643 0.5350.378 0.825 0.493120.310 0.260 0.2850.399 0.567 0.4680.488 0.708 0.574130.438 0.183 0.2500.611 0.541 0.5430.640 0.674 0.644140.555 0.280 0.3600.441 0.473 0.4510.475 0.776 0.550150.652 0.354 0.4340.520 0.554 0.4180.532 0.695 0.517Final0.505 0.238 0.3130.484 0.416 0.4370.555 0.607 0.542Average0.528 0.258 0.3470.540 0.513 0.5260.595 0.660 0.626\nFigure 6. Example of Experiment 2 (Track 47-2)5. CONCLUSIONA MIDI guitar is a promising tool for guitarists and there-fore is being sold by electronic musical instrument man-ufacturers. However, the audio-to-MIDI conversion accu-racy of MIDI guitars is still insufﬁcient. In particular, theaccuracy is very low for phrases including many brushingnotes like those used in our experiments. To improve thisaccuracy, we attempted to integrate the output of the MIDIguitar and the signal processing result of the guitar’s audiooutput. Our experimental results showed a signiﬁcant im-provement in accuracy: the F-score was 0.626 comparedwith 0.347 for the MIDI guitar only.Although this improvement is signiﬁcant, we need toimprove the accuracy even more to ensure practical use ofMIDI guitars. An idea for further improvement may beincreasing quantity of training data for the neural network(i.e., the 2nd preliminary performance). However, increas-ing these data will result in an increase in the user’s la-bor and the time required for learning the neural network.\nFigure 7. Example of Experiment 2 (Track 09-2)We will therefore investigate a reasonable tradeoff betweenthese time investments and the outcome. In addition, wewill assess the latency in outputting MIDI messages be-cause this latency is an important factor in the use of MIDIguitars as musical instruments.Acknowledgment:This work was supported by JSPS KAK-ENHI Grant Number 26240025. Also, we thank the members ofthis JSPS KAKENHI project including Prof. Shigeki Sagayamaand Prof. Gen Hori for their fruitful suggestions.6. REFERENCES[1]K. Arimoto, T. Fujishima, and M. Goto. A multiple F0 es-timation method using speciﬁc harmonic structure modelsfor guitar performances (in Japanese). InProc. 2006 AutumnMeeting of Acoustic Society of Japan, pages 585–586. 2006.[2]A. M. Barbancho and A. Klapuri. Automatic transcription ofguitar chords and ﬁngering from audio. InIEEE Transactionson Audio, Speech, and Language Processing, volume 20,pages 915–921. 2012.[3]X. Fiss and A. Kwasinski. Automatic real-time electric gui-418 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015tar audio transcription. InProc. IEEE-ICASSP 20111, pages373–376. 2011.[4]M. Goto. A real-time music-scene-description system:Predominant-F0 estimation for detecting melody and basslines in real-world audio signals.Speech Comm., 43(4):311–329, 2004.[5]J. Hartquist. Real-time musical analysis of polyphonic guitaraudio. Master’s thesis, The Faculty of California PolytechnicState University, 2012.[6]A. B. Israel and T. N. E. Greville.Generalized Inverses: The-ory and Applications. Springer, 2003.[7]D. D. Lee and H. S. Seung. Learning the parts of objectswith nonnegative matrix factorization.Nature, 401:788–791,1999.[8]P. D. O’Grady and S. T. Rickard. Automatic hexaphonicguitar transcription using non-negative constraints. InProc.IEEE-ISSC 2009. 2009.[9]M. Paleari, B. Huet, A. Schutz, and D. Slock. A multimodalapproach to music transcription. InProc. IEEE-ICIP 2008,pages 93–96, 2008.[10]T. Yamagami and K. Itou. A bimodal music dictation methodfor composition support by using guitar performance video(in Japanese). InProc. of IPSJ National Convention 2014,volume 2, pages 365–366, 2014.[11]K. Yamaguchi.16 beat ga minitsuku! Funk de oboeru otonano cutting (in Japanese). Rittor Music, 2013.[12]K. Yazawa, K. Itoyama, and H. G. Okuno. Automatic tran-scription of guitar tablature from audio signals in accordancewith player’s proﬁciency. InProc. IEEE-ICASSP 2014, pages3146–3150. 2014.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 419"
    },
    {
        "title": "Improving Optical Music Recognition by Combining Outputs from Multiple Sources.",
        "author": [
            "Víctor Padilla Martín-Caro",
            "Alex McLean",
            "Alan Marsden",
            "Kia Ng"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416258",
        "url": "https://doi.org/10.5281/zenodo.1416258",
        "ee": "https://zenodo.org/records/1416258/files/PadillaMMN15.pdf",
        "abstract": "Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commer- cial OMR programs when applied to images of different scores of the same piece of music. As a result of this pro- cedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate move- ments and sections and removes ossia staves which con- fuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrat- ing on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%.",
        "zenodo_id": 1416258,
        "dblp_key": "conf/ismir/PadillaMMN15",
        "keywords": [
            "image pre-processing",
            "commercial OMR programs",
            "combined output",
            "around 50% fewer errors",
            "split scores",
            "remove ossia staves",
            "align outputs",
            "reject outputs",
            "majority voting",
            "MusicXML format"
        ],
        "content": "IMPROVING OPTICAL MUSIC RECOGNITION BY COMBINING OUTPUTS FROM MULTIPLE SOURCES Victor Padilla Alex McLean Alan Marsden Kia Ng Lancaster University victor.padilla. mc@gmail.com University of Leeds a.mclean@ leeds.ac.uk Lancaster University a.marsden@ lancaster.ac.uk University of Leeds k.c.ng@ leeds.ac.uk ABSTRACT Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commer-cial OMR programs when applied to images of different scores of the same piece of music. As a result of this pro-cedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate move-ments and sections and removes ossia staves which con-fuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrat-ing on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%. 1. INTRODUCTION Musical research increasingly depends on large quantities of data amenable to computational processing. In com-parison to audio and images, the quantities of symbolic data that are easily available are relatively small. Millions of audio recordings are available from various sources (often at a price) and images of tens of thousands of scores are freely available (subject to differences in copy-right laws) in the on-line Petrucci Music Library (also known as IMSLP). In the case of data in formats such as MEI, MusicXML, Lilypond, Humdrum kern, Musedata, and even MIDI, which give explicit information about the notes that make up a piece of music, the available quanti-ties are relatively small. The KernScores archive [21] claims to contain 108,703 files, but many of these are not complete pieces of music. Mutopia, an archive of scores in the Lilypond format, claims to contain 1904 pieces though some of these also are not full pieces. The Musescore collection of scores in MusicXML gives no figures of its contents, but it is not clearly organised and cursory browsing shows that a significant proportion of the material is not useful for musical scholarship. MIDI data is available in larger quantities but usually of uncer-tain provenance and reliability.  The creation of accurate files in symbolic formats such as MusicXML [11] is time-consuming (though we have not been able to find any firm data on how time-consuming). One potential solution to this is to use Opti-cal Music Recognition (OMR) software to generate sym-bolic data such as MusicXML from score images. Indeed, Sapp [21] reports that this technique was used to generate some of the data in the KernScores dataset, and others have also reported on the use of OMR in generating large datasets [2, 3, 6, 23]. However, the error rate in OMR is still too high. Although for some MIR tasks the error rate may be sufficiently low to produce usable data [2, 3, 23], the degree of accuracy is unreliable. This paper reports the results of a project to investigate improving OMR by (i) image pre-processing of scanned scores, and (ii) using multiple sources of information. We use both multiple recognisers (i.e., different OMR pro-grams) and multiple scores of the same piece of music. Preliminary results from an earlier stage of this project were reported in [16]. Since then we have added image pre-processing steps, further developments of the output-combination processes, and mechanisms for handling pi-ano and multi-part music. We also report here the results of much more extensive testing. The basic idea of com-bining output from different OMR programs has been proposed before [4, 5, 13] but this paper presents the first extensive testing of the idea, and adds to that the combi-nation of outputs from different sources for the same piece of music (different editions, parts and scores, etc.).  In view of our objective of facilitating the production of large collections of symbolic music data, our system batch processes the inputted scores without intervention from the user. The basic workflow is illustrated in Figure 1. Each step of the process is described in subsequent sec-tions of this paper, followed by the results of a study that tested the accuracy of the process.  Music notation contains many different kinds of in-formation, ranging from tempo indications to expression  © Victor Padilla, Alex McLean, Alan Marsden & Kia Ng. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Victor Padilla, Alex McLean, Alan Marsden & Kia Ng. “Improving Optical Music Recognition by Combining Outputs from Multiple Sources”, 16th International Society for Music Information Retrieval Conference, 2015. \n517   markings, and to individual notes. The representation var-ies in both score and symbolic music data formats. In this study we assume the most important information in a score to be the pitch and duration of the notes. Therefore, we have concentrated on improving the accuracy of recognition of these features alone. Grace notes, dynam-ics, articulation, the arrangement of notes in voices, and other expression markings, are all ignored. However, when a piece of music has distinct parts for different in-struments (e.g., a piece of chamber music) we do pay at-tention to those distinct parts. For our purposes, a piece of music therefore consists of a collection of “parts”, each of which is a “bag of notes”, with each note having a “pitch”, “onset time” and “duration”. Broadly speaking, we have been able to halve the number of errors made by OMR software in recognition of pitches and rhythms. However, the error rate remains relatively high, and is strongly dependent on the nature of the music being recognised and the features of the in-putted score image. We have not tested how much time is required to manually correct the remaining errors. 2. BACKGROUND 2.1 Available Score Images In the past it was common for research projects to scan scores directly. For example, in [2, 3], pages were scanned from the well-known jazz ‘Fake Book’. Now, however, many collections of scans are available on-line. The largest collection is the Petrucci Music Library (also called IMSLP),1 which in April 2015 claimed to contain 313,229 scores of 92,019 works. Some libraries are plac-ing scans of some of their collections on-line and some scholarly editions, such as the Neue Mozart Ausgabe (NMA),2 are also available on-line. Scores available on-line are usually of music which is no longer in copyright, and date from before the early twentieth century. Most of these scans are in PDF format and many are in binary images (one-bit pixels). Resolution and qualities of the scans varies.  2.2 OMR Software Eight systems are listed in a recent survey as ‘the most relevant OMR software and programs’ [18]. Of these we found four to be usable for our purpose: Capella-Scan 8.0, SharpEye 2.68, SmartScore X2 Pro and PhotoScore Ultimate 7.3 All four pieces of software produce output in                                                            1 www.imslp.org   2 dme.mozarteum.at 3 www.capella.de, www.visiv.co.uk, www.musitek.com, www.sibelius. com/products/photoscore The other four listed by Rebelo et al. are ScoreMaker (cmusic.kawai.jp/products/sm), which we found to be available only in Japanese, Vivaldi Scan, which appears to have been withdrawn from sale, Audiveris (audiveris.kenai.com), open-source software which we found to be insufficiently robust (version 5 was un-der development at the time of this project), and Gamera (gamera.informatik.hsnr.de), which is not actually an OMR system but instead a toolkit for image processing and recognition. MusicXML format [11]. They differ in the image formats which they take as input, and also in whether they can take multiple pages as input. The lowest common denom-inator for input is single-page images in TIFF format.  Although our objective was not to evaluate the differ-ent OMR programs, we did find that the programs dif-fered considerably in their accuracy when applied to dif-ferent music. No one OMR program was consistently bet-ter than the rest. An indication of the differences between them is given in the results section below. 3. OUR MULTIPLE-OMR SYSTEM FOR IMPROVED ACCURACY             Figure 1. Basic workflow of the proposed system. 3.1 Image Pre-Processing As stated above, the common required input for the OMR programs is single-page TIFF images. The first steps in the image pre-processing are therefore to split multiple-page scores into individual pages, and to convert from PDF, which is the most common format used for down-loadable score images, including those from IMSLP.  The other pre-processing steps depend on the detection of staves in the image. In general we use the Miyao [14] staff finding method as implemented in the Gamera soft-ware,4 which locates equally spaced candidate points and links them using dynamic programming. This method did not perform well at detecting short ossia staves. For this we applied the Dalitz method (in class StaffFinder_dalitz) from the same library [9].5  Further processing is required to recognise systems in the images. Contours are detected using the findContours function [22] of the OpenCV library for computer vi-sion,6 with those containing staves marked as systems. Each of the remaining contours are then assigned to the nearest system, looking for the largest bounding box overlap, or simply the nearest system on the y-axis. None of the OMR programs used handled divisions between movements properly: the music in an image was always assumed by the software to be a single continuous                                                            4 gamera.informatik.hsnr.de   5 music-staves.sf.net 6 opencv.org Image pre-processing of scans Multiple OMR recognizers Selection of outputs and correction   MusicXML output 518 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   piece of music. This is not problematic in a process which depends on the intervention of a human operator. Howev-er, this is inefficient and not scalable for large-scale batch processing. Therefore, we implemented a process which recognises the beginnings of movements, or new sec-tions, from the indentation of the first system of staves. Where indented staves are detected, the output is two or more TIFF files containing images of those staves which belong to the same movement or section. This procedure correctly separated all cases in our test dataset. A second common source of error was found to be ‘os-sia’ segments. An ossia is a small staff in a score, gener-ally placed above the main staff, that offers an alternative way of playing a segment of music, for example, giving a possible way of realising ornaments. The OMR programs tended to treat these as regular staves, leading to signifi-cant propagation errors. Since, as indicated above, our aim was to improve the recognition accuracy of pitches and rhythms only, ossia staves would not contain useful information consistent with this aim. The best course of action was to simply remove them from the images. Therefore, the minimum bounding rectangle which in-cluded any staff that was both shorter than the main staff and smaller in vertical size, and any symbols attached to that staff (in the sense of there being some line of black pixels connected to that staff), was removed from the im-age. We did not separately test the ossia-removal step, but found that a few cases were not removed properly. 3.2 Post-Processing: Comparison and Selection The images resulting from the pre-processing steps de-scribed above are then given as input to each of the four OMR programs. Output in MusicXML from each OMR program for every separate page for each score is com-bined to create a single MusicXML file for that score and that OMR program. As mentioned above, we chose to concentrate on the most important aspects of musical in-formation, and so the post-processing steps described be-low ignored all grace notes and all elements of the Mu-sicXML which did not simply describe pitch and rhythm. Most of the processing is done using music21 [8], using its data structures rather than directly processing the Mu-sicXML. The most common errors in the output are incorrect rhythms and missing notes. Occasionally there are errors of pitch, most often resulting from a failure to correctly recognise an accidental. Rests are also often incorrectly recognised, leading to erroneous rhythms. Sometimes larger errors occur, such as the failure to recognise an en-tire staff (occasionally a result of curvature in the scan), and the failure to correctly recognise a clef, can lead to a large-scale propagation of a single error. The aim of our post-processing of MusicXML outputs was to arrive at a single combined MusicXML output which contained a  minimum number of errors. However, this aim was challenging to fulfil because it was not pos-sible to determine, based on the MusicXML alone which details are correct and which are incorrect. Our general approach, following Bugge et al. [4] is to align the out-puts and to use majority voting as a basis for deciding which details are correct and which are incorrect. The first post-processing steps apply to music with more than one voice on a single staff (as is common in keyboard music). Different OMR programs organise their output in different ways and some reorganisation is nec-essary to ensure that proper matching can take place. The steps in this part of the process are: a) Filling gaps with rests. In many cases, rests in voices are not written explicitly in the score, and the OMR soft-ware recognises rests poorly. Furthermore, while music21 correctly records the timing offsets for notes in voices with implied rests, MusicXML output produced from music21 in such cases can contain errors where the offset is ignored. To avoid these problems, we fill all gaps or implied rests with explicit rests so that all voices in the MusicXML contain symbols to fill the duration from the preceding barline. b) Converting voices to chords. The same music can be written using chords in some editions but separate voices in others. (See Figure 2 for an example.) To allow proper comparison between OMR outputs, we convert represen-tations using separate voices into representations that use chords.               Figure 2. Extracts from the NMA and Peters editions of Mozart piano sonata K. 282, showing chords in the NMA where the Peters edition has separate voices. c) Triplets. In many piano scores, triplets are common, but not always specified. Some OMR programs correctly recognise the notes, but not the rhythm. Our application detects whether the length of a bar (measure) matches with the time signature in order to determine whether tri-plets need to be inserted where notes beamed in threes are detected. A grossly inaccurate output can lead to poor alignment and poor results when combining outputs. Therefore, it is better to exclude outputs which contain a lot of errors from subsequent alignment and voting, but again it is not possible to determine whether an output is grossly inac-curate on the basis of its contents alone. We once again \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 519   employed the idea of majority voting: an output which is unlike the others is unlikely to be correct. \n Figure 3. Example of phylogenetic tree with pruning of distant branches. To determine how much outputs are like each other, we adopted the mechanism of ‘phylogenetic trees’ by UPGMA [24]. This mechanism is designed to cluster DNA sequences according to their similarity.  Instead of a DNA string, each staff can be converted into a pitch-rhythm sequence and compared in pairs using the Needleman-Wunsch algorithm [15]. This process leads to the establishment of a similarity matrix and phylogenetic tree. Once the tree is configured, distant branches can be removed by a straightforward operation. So far, our best results have been obtained by using the three or four closest OMR outputs and discarding the rest. An evalua-tion of a more complex algorithm for pruning trees is left for future research. 3.3 Post-Processing: Alignment and Error-Correction In order to determine the correct pitch or duration of a note, on the basis of majority voting, we need to know that the possible values in the different outputs refer to the same note. Therefore, it is necessary to align the out-puts so that: the appropriate details in each output can be compared; a majority vote can be made; the presumably correct details can be selected; and a composite output can be constructed using these details.  There are two methods to align outputs, which we re-fer to as ‘top-down’ and ‘bottom-up’. The first method aims to align the bars (measures) of the outputs so that similar bars are aligned. The second method aims to align the individual notes of the outputs irrespective of bar-lines. The first works better in cases where most barlines have been correctly recognised by the OMR software but the rhythms within the bars might be incorrectly recog-nised due to missing notes or erroneous durations. The second works better in cases where most note durations have been correctly recognised but the output is missing or contains extra barlines. In the following section, we explain the bottom-up process and how we combine the outputs from parts in order to obtain a full score. An ex-planation of our top-down approach used in earlier work can be found in [16]. 3.3.1 Bottom-Up Alignment and Correction, Single Parts Bottom-up alignment is applied to sequences of symbols from a single staff or a sequence of staves that correspond to a single part in the music. This might come from Mu-sicXML output of OMR applied to an image of a single part in chamber music, such as a string quartet, full score, or keyboard music in which the staves for the right and left hands are separated. Each non-ignored symbol repre-sented in the MusicXML (time signature, key signature, note, rest, barline, etc.) is converted to an array of values to give the essential information about the symbol. The first value gives the type of symbol plus, where appropri-ate, its pitch and/or duration class (i.e., according to the note or rest value, not the actual duration taking into ac-count triplet or other tuplet indications). Alignment of OMR outputs, once again using the Needleman-Wunsch algorithm, is done using these first values only. In this way we are able to avoid alignment problems which might otherwise have occurred from one edition of a piece of music indicating triplets explicitly and another implicitly implying triplets, or from one OMR recognis-ing the triplet symbol and another not recongising the symbol. All outputs are aligned using the neighbor-joining algorithm [20], starting with the most similar pair. A composite output is generated which consists of the sequence of symbols which are found to be present at each point in at least half of the outputs. \n Figure 4. Example of removal of voices for alignment and subsequent reconstruction. In the case of music where a staff contains more than one voice, such as in piano music, we adopt a procedure which creates a single sequence of symbols (see Figure 4). To achieve this, notes and rests are ordered first by their onset time. Next, rests are listed before notes and higher notes listed before lower notes. The result is a ca-nonical ordering of symbols that make up a bar of multi-voice music. This means that identical bars will always align perfectly. Voice information (i.e., which voice a note belongs to) is recorded as one of the values of a note’s array. However, this information is not taken into account when the correct values are determined by major-ity voting. Instead, voices are reconstructed when the aligned outputs are combined. Although, this means that the allocation of notes to voices in the combined output \n520 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   might not match any of the inputs (and indeed might not be deemed correct by a human expert), we found consid-erable variability in this aspect of OMR output and there-fore could not rely upon it. At the same time, as in the pre-processing of MusicXML outputs from the OMR programs, additional rests are inserted into the combined output in order to ensure that every voice is complete from the beginning of each bar. 3.3.2 Top-Down Combined Output, Full Score To generate a MusicXML representation of the full score, the results of alignment of the separate parts/staves need to be combined. Often the results for the constituent parts do not contain the same number of bars (measures), usu-ally because of poor recognition of ‘multi-rests’ (i.e., sev-eral bars of rest that look like a single bar), or because of missing barlines. Sometimes OMR software inserts bar-lines where there are none. To achieve parts with the same number of bars and the best construction of the full score, the following steps are implemented: a) Finding the best full score OMR. As a result of er-rors in recognising systems and staves, OMR is more likely to increase than reduce the number of bars. In the case of chamber music (e.g., string quartets), it is com-mon to find OMR putting bars from one part in another part, adding extra rest bars and increasing the global number of bars. A simple algorithm is therefore used to select the OMR output that contains the smallest number of bars and the correct number of parts. We have found this to work correctly in most cases. As an example, the 724 bars in Mozart’s string quartet K. 387 can be con-verted into 747, 843, 730 and 764 bars by different OMR programs. Figure 5 shows the result of OMR errors in in-terpreting the arrangement of staves, in this case causing two bars to be converted into four. One system of staves is mis-interpreted as two staves, and what is actually the second violin part is read as a continuation of the first vi-olin part. In this case there is also the very common error of misreading the alto clef.  \n  Figure 5. Displacement of parts in a string quartet. b) Aligning parts against the best full score. Every bar of each part is converted into a sequence of hash values on the basis of the pitch and rhythm of the contents of the bar. The parts are aligned bar-by-bar (top-down ap-proach) using the Needleman-Wunsch algorithm, and empty bars are introduced where needed. To determine the similarity of each pair of bars, the Needleman-Wunsch algorithm is used to align the contents of the two bars, and the aggregate cost of the best alignment taken as a measure of the similarity of the bars. For further detail, see [16]. This procedure results in correct vertical align-ment of most of the bars and adds multi-rests that were not properly recognised in single parts. 3.4 Implementation Our research was conducted using the Microsoft Win-dows operating system. This was because SharpEye only operates with this system. (The other three have versions for Windows and Macintosh systems.) Software was written in Python and made use of the Gamera and mu-sic21 libraries. The items of OMR software used were designed for interactive use, so it was not a simple matter to integrate them into a single workflow. For this purpose we used the Sikuli scripting language.1 A cluster of six virtual machines running Windows Server 2012, controlled by a remote desktop connection, was setup to run the OMR software and our own pre- and post-processing software. To generate MusicXML from a set of scans of a piece of music, the required scans need to be stored in a particular directory shared between the different machines. The setup also provides different lev-els of Excel files for evaluating results.  With the exception of the commercial OMR programs, the software and documentation are available at https://code.soundsoftware.ac.uk/projects/multiomr and at http://github.com/MultiOMR.  4. EVALUATION 4.1 Materials To test our system, we chose to use string quartets and piano sonatas by Mozart, because both scans and pre-existing symbolic music data for these pieces are availa-ble. The pieces tested were the string quartets dedicated to Joseph Haydn (K. 387, 421, 428, 458, 464 and 465) and the first six piano sonatas (K. 279, 280, 281, 282, 283 and 284). The sources have been taken from IMLSP (Pe-ters edition, full scores and parts) and NMA (full scores). Ground truth files in Humdrum Kern format or Mu-sicXML (when available) were downloaded from KernScores.2 Two movements, (K. 428, mov. 4 and K. 464, mov. 1) are not yet complete on KernScores and so have not been evaluated. The string-quartet dataset in-cluded a total of 459 pages of music notation and the pi-ano-sonata set 165 pages. 4.2 Results For each piece, the output resulting from our system was compared with the data derived from KernScores and the                                                            1 www.sikuli.org 2 kern.ccarh.org \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 521   recognition rate for notes was calculated (i.e., the per-centage of notes in the original which were correctly rep-resented in the output). Errors found in the output of our system compared to the ground truth were recorded in Excel and MusicXML files. These provided information about each incorrect note and its position in the score, the accuracy of the overall result, and the accuracy of each OMR program, plus colour-coding in the MusicXML file to indicate omissions, errors and insertions.    Table 1. Overall recognition rates. OUT = our system; CP = Capella-Scan; PS = PhotoScore; SE = SharpEye; SS = SmartScore.  \n Figure 8. Overall recognition rates. For codes see the caption to Table 1. Summary results are shown in Figure 8 and Table 1. As can be seen, overall, the output of our system was found to be better than all the OMR programs. The dif-ference in recognition rates is higher for the string quar-tets due to very poor recognition in some cases for the Peters full score edition. One of the main strengths of our system rests with having consistent recognition rates of  around 95% in most cases. This can be attributed to the automatic removal of scores poorly recognised at the phylogenetic-tree stage. This pruning removes a large amount of noise introduced by some OMR systems. A second strength is the lack of hard-coded rules. For in-stance, some OMR programs are better at recognising tri-plets and others at detecting pitch. Furthermore, the situa-tion can change with new versions of OMR software and the introduction of new OMR programs. For our system to incorporate new versions and new OMR programs, all that is required is to add the necessary scripts in Sikuli to use those programs to generate MusicXML output from a given set of score images. The entire process (six complete string quartets using parts and full scores), employing six virtualised machines in parallel, with an Intel Xeon processor of 3.30GHz, takes around 3 hours to complete.  5. FUTURE PROSPECTS We have shown that by using pre-processing techniques and combining results from different OMR programs and different sources, we can significantly reduce the number of errors in optical music recognition. For those compil-ing corpuses of musical data in symbolic format, this would increase efficiency and save considerable effort. Furthermore, the reduction in error rate means that re-search which relies on large quantities of musical data which was previously impossible because it would be too costly to compile the data might now become possible. Large quantities of data can now be easily derived from scans available from IMSLP and elsewhere. Although errors remain in the output, the reduced error rate com-pared with raw OMR programs output will enable more valid results to be derived from statistical studies which previously lacked validity. Nevertheless, it should be not-ed that the number of errors which remain will still be too high for musicological research which assumes near 100% accuracy in the data.  Our research has made clear the limitations of current commercial OMR software. For example, in [16] we pro-posed the image pre-processing to extract separate bars in order to arrive at a better alignment of outputs that come from different OMR programs. However, we discovered that the OMR programs were generally incapable of pro-ducing any useful output from a single bar of music so we had to abandon this idea. It is our judgement, based on the project reported here, that further improvement of our system will require the limitations associated with current OMR software to be overcome. This may require a fun-damentally different approach to currently available OMR programs.  Some possible directions have been proposed in the literature. For instance, Fahmy & Blostein [10] proposed a method based on rewriting graphs arising from raw symbol-recognition to produce graphs that conform more closely to musical constraints. Bainbridge & Bell [1] have also proposed making greater use of the semantics of mu-sical notation. Raphael and co-workers [12, 17] have pro-posed a Bayesian approach based on likelihood, again taking account of musical constraints. Church & Cuthbert [7] proposed correcting errors by using information from similar musical sequences in the same piece of music. Finally, Rossant & Bloch [19] proposed the use of fuzzy logic. Since all of these approaches aim to take an image of a score and output the symbols which are represented in that score, they could be incorporated into the work-flow described here, and its output combined with other outputs to further improve recognition.  6. ACKNOWLDGEMENTS This work was supported with funding from the Arts and Humanities Research Council (AHRC; AH/L009870/1). OUTCPPSSESSPiano95,11     74,26     86,13     91,86     85,40    String Quartet96,12     47,84     81,47     86,65     82,40    Average95,61     61,05     83,80     89,25     83,90    \n522 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   7. REFERENCES [1] D. Bainbridge and T. Bell: “A music notation construction engine for optical music recognition,” Software—Practice and Experience, Vol. 33, pp. 173–200, 2003. [2] D. Bainbridge, C. G. Nevill-Manning, I. H. Witten, L. A. Smith, and R. J. McNab: “Towards a digital library of popular music,” Proc. of the 4th ACM conference on Digital libraries (DL '99), pp. 161–169, 1999. [3] D. Bainbridge and K. Wijaya: “Bulk Processing of Optically Scanned Music”, Proc. of 7th Int. Conf. on Image Processing And Its Applications, Vol. 1, pp. 474–478, 1999. [4] E. P. Bugge, K. L. Juncher, B. S. Mathiasen, and J. G. Simonsen: “Using sequence alignment and vot-ing to improve optical music recognition from mul-tiple recognisers,” Proc. of the 12th Int. Soc. for Mu-sic Information Retrieval Conf., pp. 405–410, 2011. [5] D. Byrd and M. Schindele: “Prospects for Improv-ing OMR with Multiple Recognisers,” Proc. of the 7th Int. Conf. on Music Information Retrieval (ISMIR), pp. 41–46, 2006. Revised and expanded version (2007) retrieved February 20, 2013, http://www.informatics.indiana.edu/donbyrd/ MROMRPap. [6] G. S. Choudhury, T. DiLauro, M. Droettboom, I. Fujinaga, B. Harrington, and K. MacMillan: “Optical Music Recognition System within a Large-Scale Digitization Project,” Proc. of the Int. Symp. on Music Information Retrieval, 2000. [7] M. Church and M. S. Cuthbert: “Improving Rhyth-mic Transcriptions via Probability Models Applied Post-OMR,” Proc. of the 15th Conf. of the Int. Soc. for Music Information Retrieval, pp. 643–647, 2014. [8] M. S. Cuthbert and C. Ariza: “music21: A Toolkit for Computer-Aided Musicology and Symbolic Mu-sic Data,” Proc. of the 11th Int. Soc. for Music In-formation Retrieval Conf., pp. 637–42, 2010. [9] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujina-ga: “A Comparative Study of Staff Removal Algo-rithms,” IEEE Transactions on Pattern Analysis and Machine Intelligence,  30(5), 753–766, 2008. [10] H. Fahmy and D. Blostein: “A Graph-Rewriting Paradigm for Discrete Relaxation: Application to Sheet-Music Recognition,” International Journal of Pattern Recognition and Artificial Intelligence, Vol. 12, No.6, pp. 763–799, 1998. [11] M. Good: “MusicXML for notation and analysis,”, in The Virtual Score: Representation, Retrieval, Restoration (Computing in Musicology 12), W. B. Hewlett, and E. Selfridge-Field, Eds. MIT Press, 2001, pp. 113–124. [12] R. Jin and C. Raphael: “Interpreting Rhythm in Op-tical Musical Recognition,” Proc. of the 12th Int. Soc. for Music Information Retrieval Conf., pp. 151–156, 2012. [13] I. Knopke and D. Byrd: “Towards Musicdiff: A Foundation for Improved Optical Music Recognition Using Muliple Recognizers,” Proc. Of the 8th Int. Conf. on Music Information Retrieval, pp. 123–126, 2007. [14] H. Miyao and M. Okamoto: “Stave Extraction for Printed Music Scores Using DP Matching,” Journal of Advanced Computational Intelligence and Intelligent Informatics, Vol. 8, No. 2, pp. 208–215, 2004. [15] S. B. Needleman and C. D. Wunsch: “A general method applicable to the search for similarities in the amino acid sequence of two proteins,” Journal of molecular biology, Vol. 48, No. 3, pp. 443–453, 1970. [16] V. Padilla, A. Marsden, A. McLean, and K. Ng: “Improving OMR for Digital Music Libraries with Multiple Recognisers and Multiple Sources,” Proc. of the 1st Int. Workshop on Digital Libraries for Musicology, pp. 1–8, 2014. [17] C. Raphael and J. Wang: “New Approaches to Opti-cal Music Recognition,” Proc. of the 12th Int. Soc. for Music Information Retrieval Conf., pp. 305–310, 2011. [18] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Mar-cal, C. Guedes, and J. S. Cardoso: “Optical music recognition: state-of-the-art and open issues,” Inter-national Journal of Multimedia Information Retriev-al, Vol. 1, No. 3, pp. 173–190, 2012. [19] F. Rossant and I. Bloch: “Robust and Adaptive OMR System Including FuzzyModeling, Fusion of Musical Rules, and Possible Error Detection,” EURASIP Journal on Advances in Signal Processing, Vol. 2007, Article ID 81541, 25 pp. [20] N. Saitou and M. Nei: “The neighbor-joining method: a new method for reconstructing phylogenetic trees,” Molecular biology and evolution, 4(4), 406-425, 1987. [21] C. S. Sapp: “Online Database of Scores in the Humdrum File Format,” Proc. Of the 6th Int. Conf. on Music Information Retrieval, pp. 664–665, 2005. [22] S. Suzuki and K. Abe: “Topological Structural Analysis of Digitized Binary Images by Border Following,” Computer Vision, Graphics and Image Processing, 30(1), 32–46, 1985. [23] V. Viro: “Peachnote: Music Score Search and Anal-ysis Platform,” Proc. of the 12th Int. Soc. for Music Information Retrieval Conf., pp. 359–362, 2011. [24] M. Zvelebil and J. Baum: Understanding bioinformatics. Garland Science, Abingdon, 2007. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 523"
    },
    {
        "title": "A Toolkit for Live Annotation of Opera Performance: Experiences Capturing Wagner&apos;s Ring Cycle.",
        "author": [
            "Kevin R. Page",
            "Terhi Nurmikko-Fuller",
            "Carolin Rindfleisch",
            "David M. Weigl",
            "Richard Lewis 0001",
            "Laurence Dreyfus",
            "David De Roure"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415582",
        "url": "https://doi.org/10.5281/zenodo.1415582",
        "ee": "https://zenodo.org/records/1415582/files/PageNRWLDR15.pdf",
        "abstract": "Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists’ study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufficiently capture the ephem- eral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musi- cologist throughout a complete multi-day production of Richard Wagner’s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the perfor- mance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web tech- nologies, and of the practical challenges and compromises of fieldwork within a working theatre. Finally we con- sider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered.",
        "zenodo_id": 1415582,
        "dblp_key": "conf/ismir/PageNRWLDR15",
        "keywords": [
            "multimedia material",
            "musicologists study",
            "computational methods",
            "ephemeral phenomena",
            "stagings",
            "toolkit development",
            "tablet-based score interface",
            "scenic setting",
            "real-time use",
            "semantic web technologies"
        ],
        "content": "A TOOLKIT FOR LIVE ANNOTATION OF OPERA PERFORMANCE:EXPERIENCES CAPTURING WAGNER’SRINGCYCLEKevin R. Page†, Terhi Nurmikko-Fuller†, Carolin Rindﬂeisch‡, David M. Weigl†Richard Lewis#, Laurence Dreyfus‡, David De Roure††Oxford e-Research CentreUniversity of OxfordUnited Kingdomfirst.last@oerc.ox.ac.uk‡Faculty of MusicUniversity of OxfordUnited Kingdomfirst.last@music.ox.ac.uk#Department of ComputingGoldsmiths, University of LondonUnited Kingdomfirst.last@gold.ac.ukABSTRACTPerformance of a musical work potentially provides a richsource of multimedia material for future investigation, bothfor musicologists’ study of reception and perception, andin improvement of computational methods applied to itsanalysis. This is particularly true of music theatre, where atraditional recording cannot sufﬁciently capture the ephem-eral phenomena unique to each staging. In this paper weintroduce a toolkit developed with, and used by, a musi-cologist throughout a complete multi-day production ofRichard Wagner’sDer Ring des Nibelungen. The toolkitis centred on a tablet-based score interface through whichthe scholar makes notes on the scenic setting of the perfor-mance as it unfolds, supplemented by a variety of digitaldata gathered to structure and index the annotations. Wereport on our experience developing a system suitable forreal-time use by the musicologist, structuring the data forreuse and further investigation using semantic web tech-nologies, and of the practical challenges and compromisesof ﬁeldwork within a working theatre. Finally we con-sider the utility of our tooling from both a user perspectiveand through an initial quantitative investigation of the datagathered.1. INTRODUCTION AND MOTIVATIONThe performance of a fully staged opera is perhaps therichest form of production when considering the potentialfor a wide diversity of music information united arounda single body of work. Its study provides both opportu-nity and challenges for gathering, organising, retrieving,and analysing data and artefacts from and about the event.Thanks to a willing partnership with the Birmingham Hip-podrome and the Mariinsky Opera under the baton of Val-ery Gergiev, their performance of all four operas compris-c\u0000K. R. Page, T. Nurmikko-Fuller, C. Rindﬂeisch, D. M.Weigl, R. Lewis, L. Dreyfus, D. De Roure.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:K. R. Page, T. Nurmikko-Fuller,C. Rindﬂeisch, D. M. Weigl, R. Lewis, L. Dreyfus, D. De Roure. “Atoolkit for live annotation of opera performance: Experiences capturingWagner’sRingCycle”, 16th International Society for Music InformationRetrieval Conference, 2015.ing Richard Wagner’sDer Ring des Nibelungen(hence-forthRing) over ﬁve days in November 2014 presented aunique opportunity to develop and trial a musical perfor-mance annotation kit providing a structured frame of ref-erence for interpreting collections of multimedia data.In this paper we report on the design and implementa-tion of the annotation software and supporting tools, whichwere co-designed with a musicologist to provide maximalutility when deployed for ﬁeldwork in a working theatre.We begin by considering motivations from the ﬁelds ofmusicology and Music Information Retrieval (MIR).1.1 Musicological motivationIn recent decades, methodological shifts such as a ‘perfor-mative turn’, widely affecting research in the Arts, Human-ities and Social Sciences, and reception theory questionedmusicology’s traditional focus on the work as an idealisedconcept and on the written score. Instead, music is consid-ered as a continuous cultural practice, couched within therespective contexts in which it is perceived, which attachesan increased value both to performance as a general con-cept or ritual as well as to speciﬁc performance events [6].The individual realisation of a work in performance, espe-cially in music theatre, differs signiﬁcantly from the ab-stract aesthetic concept captured in the score: while themusical dimension may be treated with a high degree of‘faithfulness’, scenic interpretation is created afresh in ev-ery new staging. Even in cases such as Wagner’s musicdramas, in which music and scenic events are coordinateddown to the smallest detail, the degree to which his scenicinstructions are followed varies considerably, and the re-ality of individual stagings goes far beyond the concept inthe score. This raises the question of how a music-dramaticperformance, as an ephemeral phenomenon, can be cap-tured [14]. Analyses of recorded performances are almostas old as the respective technologies themselves [9]; butas the recording often assumes the status of an aesthetictext in the process, ephemeral phenomena are again over-looked [5]. As an audiovisual recording is neither an ob-jective nor an exhaustive documentation, the investigationof new ways of capturing different kinds of performancedata is a worthwhile undertaking. Live annotation of a per-formance helps to overcome the ‘recording bias’ by en-211abling researchers to document events and gather informa-tion which cannot be reiﬁed in audiovisual recordings. Themethod of documentation and the resulting record is more-over of a signiﬁcantly different quality: live-annotation al-lows for a selective, focused and structured record-keep-ing, where different annotation schemes can be tailored to aspeciﬁc research question, thus integrating documentationwith on-the-ﬂy analysis. While digital technologies easegathering of this information, this comes at a scale greaterthan could be recorded ‘by hand’. The ability to semanti-cally structure gathered data for publishing and reuse, andto undertake computationally assisted analysis, providesfurther breadth to the study of performances.1.2 MIR motivationA second motivation is the utility of a well-described andstructured multimedia dataset, annotated by an expert mu-sicologist, and tooling to create such corpora, to inform,reﬁne, and test MIR algorithms. A comprehensive datasource could act as an authoritative ground truth for a va-riety of MIR tasks including: automated identiﬁcation ofmusicological facets and melodic phrase recognition (e.g.leitmotif detection); tempo prediction and score following(based on page turns and annotations). It holds prospectsfor hypothesis-driven exploration of bio-sensed data mea-sured from audience members, and for calibration of auto-mated prediction of listener arousal from scores, and po-tentially that of musical expectation and mood.2. RELATED WORKThe complementary nature ofperformance studiesandem-pirical musicology(§6) has been noted by Cook [6]. Ker-shaw [14] discusses performances as “site-speciﬁc spec-tacles”, reporting research that largely conﬁrms theatre re-ception as extensively inﬂuenced by idiosyncratic observerperspectives. The recording as the most accurate captur-ing of the live performance has been commented on byTrezise [24], while Do˘gantan Dack [8] used video record-ings for a performer-centred study of chamber music, al-though not extending to theatrical aspects and staging.In Section 3.3 we capture the scenic elements of per-formance through annotation. The extensive ethnographicstudy ofmusical annotationscarried out by Winget [25] il-lustrates existing precedent for this approach, though fromthe perspective of musicians marking a score rather than amusicologist annotating a live operatic performance. Ourtechnique is strongly guided by established rehearsal prac-tice for opera, where the scenic aspects and stage directionsthat constitute a new staging are captured in an annotatedscore. These ‘scripts’ are not usually published, and noother works capturing timings of speciﬁc features of a liveperformance are known to the authors.An overview ofdigital technologiesin performance stud-ies by Marsden [15] contends that research successfullybridging musicology with the digital is found within thedomain of music information retrieval, rather than musicalor performance analysis. For exploratory analysis, Dolanet al. refer to Sonic Visualizer [3, 11], but are exclusiveof staging, theatrics, and actor dynamics, the digital anno-tation of which has little prior work. Okumuraet al.[18]modelled ways to capture deviations from strict interpreta-tions of the score during a performance – a potential usecase for our dataset. A model for acquiring content, de-scription of data, and subsequent evaluation that comple-ments our work is been outlined by Repetto and Serra [21].Reﬂecting on corpora containing live performance andannotations, Bainbridgeet al.[1] list The Hathitrust Digi-tal Library [4], and the International Music Score LibraryProject (IMSLP) [16], as examples of large-scale digitallibraries for or including music and music-related data;Doerret al.comment on the role of metadata for digitallibrary resource retrieval [10]; cross-cultural approachesand models for resource discovery in music digital librarieshave been examined by Huet al.[13] and Porteret al.[20]respectively; and Smithet al.[23] designed and imple-mented a large database for structural annotation. Theseinform our ontological structures (Section 3.2).3. DESIGN AND IMPLEMENTATIONThe Musical Score Annotation Kit – ‘MuSAK’1– was as-sembled from off-the-shelf hardware and applications com-bined with additional bespoke software, for recording theephemera of live performance, as motivated in Section 1.1.It was designed to three primary requirements:(i)an inter-face sufﬁciently intuitive and fast enough to operate so thatthe musicologist could annotate under the pressure of a liveperformance, including turning pages to match activity onstage;(ii)for reconﬁgurability to incorporate changing an-notation techniques and structures developed in the courseof preparatory study prior to the performance events; and(iii)to be adaptable to the uncertainties of ﬁeldwork in aworking theatre environment, including potential changesto locations, power supply, access, etc. and extremely lim-ited ‘dress rehearsals’ with a touring production.3.1 Toolkit components3.1.1 Annotation server and tablet interfaceAt the heart of MuSAK is an annotation system used bythe musicologist during the performance. Initial designscalled for a taxonomic palette of symbols that could be se-lected on an iPad tablet touchscreen and placed as annota-tions onto a digital copy of the score. This quickly raisedthree problems:(i)all proposed user interface sketches forselecting one of many annotations were complex and in-trusive enough to interrupt score following and the per-formance observation;(ii)the operational cognitive loadwas judged high and different enough from traditional ‘pa-per and pencil’ marking to require a signiﬁcant period oflearning and training before use at a live event;(iii)pre-determining an adequate set of music and scenic symbolsrequired several weeks’ precursory study, leaving limitedtime to add symbols to the system; furthermore, symbolsmight be created ad-hoc during use.1http://www.transforming-musicology.org/tools/metaMuSAK212 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015A pragmatic compromise was reached: short piano scorepages from the IMSLP music library2were shown on atablet, allowing freehand digital annotations according to apictogram key of the musicologist’s design. Desirable ‘incontent’ semantics were lost, but a user experience stronglymatching the traditional and familiar pattern of score mark-ing was gained. It retained digital advantages includingtimestamped annotations and ease of saving, replacing,modifying, and deleting content. Image layers were ‘ﬂat-tened’ to combine scores and existing annotations into newimages, which could be redeployed for re-annotation.A Union Platform3server with custom room modulewas run on a laptop deployed in the theatre, handling stor-age and communication of the annotation events. To sim-plify distribution and quick modiﬁcation, each score pagewas served to the tablet as a JPEG resource from an HTTPdaemon, alongside client HTML and Javascript communi-cating with Union from the regular Safari web browser.The web client implements buttons to turn pages andundo annotations; all annotations were recorded using Java-script event handlers to millisecond accuracy and storedboth by the browser and by Union which logged to ﬁle4.The tablet and server were networked using a small batterypowered wireless router with a private IP address space.3.1.2 Digital penThe tablet tool is, by necessity and design, reductionist. In-evitably some elements of the live performance are worthyof note, but either not preconceived within the symbolickey, or so unique as to require a longer form description.To accommodate this the kit includes a Livescribe Echo5digital ‘smart pen’. It has a standard ballpoint pen tip, butwhen used in conjunction with ‘Anoto’ paper, captures adigital copy of all writing – the paper is printed with a faintnon-repeating pattern, which is read by a small infra-redcamera in the pen that ascertains nib position within andbetween pages. While ‘in content’ semantics are not auto-matically decoded, the use of the pen is similar to standardnote taking, and thus minimises intrusiveness. The digitaltranscription is downloaded from the pen using USB.A second feature is a microphone for timestamped au-dio recordings. When polite to speak aloud, short audiocomments were taken in lieu of written notes; when silencewas required, the microphone captured background noisefor the duration of the performance. While low quality, thelatter is sufﬁcient to calibrate temporal synchronicity.3.1.3 Score following and replay toolWe developed a second, simple, Web application for fol-lowing and recording the page-turns during the performanceby a second operator, independent to the annotator (whomight skip forward and backwards between pages to add2http://imslp.org/3http://www.unionplatform.com/41 CSV ﬁle per score page with co-ordinate deﬁned paths tracing theannotations, 1 row per straight line. Each annotation path may be de-scribed by multiple lines; paths within the same ‘pen down’-‘pen up’event are given the same timestamp. Pages without annotations are emptybut still timestamped to record page turn times.5http://www.livescribe.com/uk/smartpen/echo/notes during quieter spells). The score-following page turnscapture timings for the realization of the music containedon each page for this speciﬁc performance. Pages of thescore are rendered one at a time, timestamped in a Post-greSQL database when the user advances to the next page.An extension of this interface displays tablet annota-tions (§3.1.1) in real-time using an HTML canvas superim-posed over the score. Data is converted from CSV to JSONto ease JavaScript working and a custom renderer calcu-lates the appropriate time delta before drawing a stroke.JSON page-turn timestamps were also combined with thescore, turning pages at the correct moment.3.1.4 Audio and videoAs is typical in commercial theatres, audio and video feedsof the performance were available within the venue (§4)but, also typically, limited distribution rights preclude theirinclusion in public archives. It is desirable, and for somecalculations essential, to reference their implicit existence,particularly when synchronising captured annotations forreplay (§3.1.3 &§4) and structured data dissemination(§3.2) – or rather, to explicitly reference the timeline again-st which the notional recording was made6. For replayof annotations it is possible to include asubstituteaudiorecording of an alternate performance (§4).A second distinct video use was recording the annota-tion actions of the musicologist, providing a contextual ref-erence for toolkit evaluation and, should the Union serverfail, potential for reconstruction of annotation times.3.2 Data publicationThe use of semantic technologies to publish performancemetadata from the Internet Archive Live Music Archive7is described by Bechhoferet al.[2], and in the context ofdiversifying and enriching music information retrieval byPageet al.[19]. Crawfordet al.[7] examines the potentialof Linked Data for early music corpora, and Bainbridgeetal.[1] comments on the effect of musical content analy-sis and Linked Data in the context of digital libraries. Se-bastienet al.[22] report on ontology creation for musicalperfomance, forms and structures.Adopting these motivations, and to provide a strongfoundation for the further investigation and reuse for musi-cology and MIR, we have structured our data as RDF. Thisentails complex ontological structures to fully and explic-itly represent the items and their relationships, illustratedin Fig.1 by the timeline patterns required to encode theapparently simple relationship between the annotation ofscore pages and their performance on stage8.A second beneﬁt of web technology is ﬁdelity of accessat the resource level. For example, we might publish theoverall structure and formal annotations, but restrict accessto the video to individually registered ethnographers.6While the recording is not technically required in addition to thetime-lineof the recording, its conceptual, if not actual, inclusion can simplifythe metadata encoding structures and increase their comprehension.7https://archive.org/details/etree8See [17] for a detailed description of Linked Data generation.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 213Figure 1. Simpliﬁed data modelling.3.3 Score preparation and musicological annotationsCentral to the annotation workﬂow, and used in severaltools, the score page images required several iterations ofprocessing9and annotation before distribution in the kit.Piano and vocal arrangements were chosen to reduce thefrequency of page turns, converted from IMSLP PDFs toimages10. Screen use was reduced by a semi-automatedprocess: whitespace detection identiﬁed edges, and mark-ers indicating proportionately smaller margins were accept-ed or adjusted in a simple editing view; saved geometriesenacted the crop and enable later scaling of annotationsfor overlay on original scores. Artefacts from the pre-IMSLP scanning process were cleaned and score imagessharpened. Scripts applied a consistent naming scheme forimages, used later for inter- and intra-opera page ordering.A two stage annotation process reduced the note-takingrequired during the live performance to a minimum. An-notations extracting certain musical points of interest in thescore (such as leitmotifs and marked changes of tempo ordynamics) were made by hand, using symbols designedprior to the performance. Each opera score was markedover an average of three days11and scanned to images,creating the ﬁrst derivative layer.The musicologist used the same symbolic key to anno-tate this layer using the tablet during the live performance,making further notes where musical aspects differed fromthose expected and previously marked, and of ‘stage direc-tions’ (such as lighting, use of props, actions and move-ments of the characters) which were not directly marked inthe original score yet are critical for any interpretation ofthe performance. These live annotations were ‘ﬂattened’into a second derivative image layer of the score.4. TOOLKIT DEPLOYMENTSThe kit was deployed by a musicologist and two techni-cal assistants as part of a larger project team for the Mari-insky Opera’s production of theRingat the BirminghamHippodrome. Installation in a working theatre hosting alarge touring troupe12necessitated quick adaptations tothe limitations of the available spaces and ad-hoc solutionsas events unfolded – the majority beyond the control ofthe annotation team. Earlier design decisions to reduce9The digital processing scripts described here were implemented us-ing Image Magick and the Perl Image::Magick module.10An image ﬁle per page; served by respective web servers in the kit.11For context, the score forDas Rheingold(the shortest) is over 250pages;G¨otterd¨ammerung(the longest) comprises 365 pages.12Whose predominant language was Russian, compared to English forthe annotation and Hippodrome teams!the technical complexity of the components proved worth-while – it is not an understatement to report that a more so-phisticated version would have been insufﬁciently resilientto the challenges of this ﬁeldwork.For the ﬁrst night’s opera (Das Rheingold) the musi-cologist was located in a dressing room backstage with anaudio and video feed from the stage; while the quality ofthis viewing was far from ideal, it enabled spoken Live-scribe annotations13. On subsequent nights (Die Walk¨ure,Siegfried,&G¨otterd¨ammerung) the ‘audio describing’room was used, adjoining a lighting gallery rear of the cir-cle and with an unobstructed view of the stage. In thisimproved location lights were dimmed and silence main-tained; notes were written, not spoken. The annotationserver and router were co-located with the musicologist ev-ery night and a video camera recorded the annotation pro-cess. The score-following annotation system was run froma laptop in a theatre ofﬁce with an audio feed provided forthe operator.While the simpliﬁed design generally paid dividends,there were some malfunctions: we had not expected nortested for the hour longsecondinterval inDie Walk¨ureand the connection between tablet and annotation servertimed out. The most practicable solution was to restartboth tablet and server, losing annotations for the ﬁrst sceneof the third act14. A second issue occurred when pag-ing through tablet annotations after a performance, causingtime-stamps to be rewritten – original times were reconstr-cuted from page turn logs and intra-page timings.The capturedRingtotalled 15 hours, consisting fournights’ performance over ﬁve days, with corresponding tab-let activity of over 100,000 strokes making 8,216 annota-tions and almost 1,300 performance based page turns. Thekit deployment and data capture generated 1,316 digitalimages, 104 pages of writing producing nearly 13 hoursof digital pen replay, and 15 hours of video footage. WhileNetwork Time Protocol (NTP) clients were used to syn-chronise equipment clocks some drift was observed, due todifferences in Operating Systems and many devices lack-ing a live connection to an NTP server; these offsets arecrucial for data replay and thus explicitly recorded for datapublication (§3.2).A second deployment of the kit demonstrated its ﬂexi-bility in reconﬁguration: at a public engagement event, au-dience members used their mobile devices to provide anno-tations while listening to a live audio replay, either by an-notating musical score, or “annotating” by placing markson a simple image with zones for e.g. fast/slow, loud/soft.Both versions of the interface were provided using simulta-neous client connections. Comparative visualisations wereplayed to a substitute audio track, derived from a commer-cial recording using the MATCH Vamp plugin15and therubberband audio time warping tool16.13In German, the musicologist’s native tongue.14Which includes the section popularly known as theRide of theValkyries. The cause of this problem was not indicated in logs; rebootingmay have destroyed debugging evidence.15https://code.soundsoftware.ac.uk/projects/match-vamp16http://breakfastquay.com/rubberband/214 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015OperaShapes/pageS.D.Dur-ationS.D.Over-headS.D.Das Rheingold5.464.634.9215.78-0.0285.83Die Walk¨ure6.956.4244.3126.590.0388.62Siegfried5.765.1939.2319.060.04412.97G¨otterd¨ammerung7.225.5243.425.45-0.4714.98Table 1. Mean annotation shapes per page, page perfor-mance durations and annotation overhead (both seconds).5. USER EVALUATIONPost-deployment interviews with the musicologist evalu-ated the usablity of MuSAK in this small trial according tolearnability, efﬁciency, memorability, and satisfaction [12].Deﬁned as the degree of ease with which functionalitycan be learnt and task proﬁciency gained,learnabilitywasevaluated through the experience of acquiring the skillsnecessary to complete the annotation process. The mu-sicologist found the system non-invasive and in-line withexisting annotation pragmatics, minimising training time:“[Annotation is] very similar to the process that I asa musicologist used to do regularly...I think it workedvery well because [it] ﬁt in with actions I was very welladapted to...the tools were very non-invasive. ”Efﬁciency of usewas measured in annotation shapes perscore page and analysed through mean and standard devi-ation (Table 1). The average number of annotation shapesper page was between ﬁve and seven across all operas, cor-responding to an average of 9.7 shapes per minute.Memorabilityof the kit – the musicologist’s recall ofset-up and annotation after a ﬁve month period of non-use– was assessed using a think-aloud protocol. The evalua-tion concluded she remembered both to a very high extent.A qualitative evaluation assessed whether functionalityand performance weresatisfactory: the musicologist de-scribed the experience as follows, believing time needed tomake additional freehand annotations and cognitively pro-cess observations made page turn annotations inaccurate.“I was quite well able to keep up with the pace... animportant realisation is that making these scenic anno-tations [...]requires a lot of time to think and... processeven if it is only like 10 seconds or 5 seconds.Page turn analysis (§6) indicates that, on average, the an-notator could keep pace with the performance.The musicologist reported an ability tocapture the id-iosyncratic proﬁle of each speciﬁc performance, includ-ing deviations from the score or expectations based on thescore, as well as staging, lighting, and the behaviour of theactors. The kit was described assupportive of traditionalannotation paradigms, not necessitating new skills for ef-fective use, and the touchpad screen and stylus were:“intuitive [...] similar to using pen and paper which ev-eryone [...] analysing music is very used to. ”The additional affordances of a digital system were noted,including the automaticcapture of the temporal proﬁle ofthe performanceand thebeneﬁt of being able to easily cre-ate corrections, and undo mistakes.\nFigure 2. Musicologist’s page viewing durations and per-formance durations for those pages (SiegfriedAct I).6. DATA INVESTIGATIONA preliminary analysis considers four research questionsto improve our understanding of the data captured. Thesecome with important interpretive contexts: potentially gen-eralisable ﬁndings are limited by the scope of data collec-tion to a single performance of theRing; annotations arerecorded as continuous shapes from pen touching to leav-ing the screen, thus symbols comprising several distinctshapes are identiﬁed as multiple annotations; and someperformance sections are excluded: the start ofDie Walk¨ureAct III due to kit malfunction, the ﬁrst page of each opera,and those either side of the intermissions17.6.1 Overhead of annotation taskTo determine whether the overhead of annotation inter-fered with music following, we compared the musicolo-gist’s page view durations with the score-page performanceevents. The corresponding plots reveal strong tracking ofthe two timelines (Fig.2); Table 1 displays the mean pageperformance durations, and the time difference comparedto the annotator’s mean page view durations (the annota-tion overhead). While performance durations are variabledue to changes in tempo and in musical information den-sity on a given page, the magnitude of the mean annotationoverhead is below half a second in all four performances.Standard deviations indicate there were periods when an-notation acts were delayed, but overall, the musicologistwas able to keep up with the music. The value is negativein two performances, indicating a tendency to read ahead.6.2 Variability of annotation rateWe tested the variability of annotation rates18for eachnight (Table 2; Figure 3). Results demonstrate signiﬁcantcorrelations in each performance, accounting for between18% (G¨otterd¨ammerung) and 43% (Walk¨ure) of the varia-tion in rank between page performance duration and num-ber of annotation shapes per page. The ﬁnding of a largely17Pages were left open during the interval so durations are artefactual.18A hypothetical uniform rate would exhibit strong correlation betweenthe duration of a score page performance event and the number of anno-tation shapes produced for that page.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 215OperaRheingoldWalk¨ureSiegfriedG¨otterd¨ammerungrs.46.65.57.42r2s.21.43.32.18Table 2. Spearman correlation (p<.001) between pageperformance duration and annotation shapes per page.\nFigure 3. Correlation between score page performance du-ration and annotation shapes per page.consistent annotation rate reﬂects the suitability of the an-notation key for the task, suggesting the musicologist couldadopt symbols with different granularities of meaning ac-cording to the time available. It suggests that even seem-ingly small ‘events’ such as gestures were not overlooked.6.3 Annotation density; Performance correspondenceFinally, we investigated whether periods of high annota-tion density reﬂect consistent types of stage events, or cor-responded to scenes of high activity or intensity. For thisanalysis, periods inSiegfriedwith a number of annotationsper minute exceeding a threshold of the mean plus twostandard deviations, as well as three major peaks in anno-tation activity for the ﬁnal act ofG¨otterd¨ammerung, weremapped to page sequences in the annotated score. The mu-sicologist reinspected the corresponding pages, determin-ing that the symbols occurring at these periods predomi-nantly indicate changes in performers’ posture or position.These symbols largely consist of four or more shapes; thus,the high rate of annotation during these relative to otherperiods may be partly artefactual. These peak periods mayrefer to key dramatic moments, e.g. when Siegfried kissesand awakens Br¨unnhilde in Act III, or parts of larger di-alogic scenes, where every utterance is interpolated by apostural change. Certain passages with greater staged ac-tivity (for instance when Siegfried kills Fafner) were notobserved within this subset of the data. A possible explana-tion is that these scenes were staged with a high reliance onlighting effects, annotated with simple symbols; they werealso generally drawn out for longer periods, and thus po-tentially overlooked by our per-minute-metric. The threepeaks in the third act ofG¨otterd¨ammerungeach reﬂectedessential moments: the Rhinemaidens telling Siegfried ab-out the curse; Hagen killing Gunther; and Hagen strug-gling with the Rhinemaidens for the Ring. One other ex-pected scene, Siegfried’s death, took up over two minutes,and was thus represented by two rate observations that bothcame close to the threshold without quite meeting it. A re-ﬁned measure of annotation density accounting for varia-tions in granularity by considering the immediate temporalcontext could accommodate this issue.7. CONCLUSIONS AND FUTURE WORKWe have described the software developed to, in combina-tion with off-the-shelf hardware, form a kit used to capturedata informing performance studies and the MIR analy-ses that may be applied to them. We have reported itsuse to annotate a complete production of Wagner’sRingand evaluation of the kit’s performance after the deploy-ment. An initial data driven investigation of the annota-tions has shown it can support and enrich analysis of theperformance, and that the corpus could be developed as a‘ground truth’ for MIR research.Investigations to date have focused on temporal analysisof acts of annotation, whereas our next step will examinesemantics within the symbols, realizing further beneﬁts forindexing and searching within performance data. We willtrial computer vision techniques to categorise pictogramsin the annotation layers, and revisit options for encodingstronger symbol semantics during the annotation. Whilethe desirable affordances of the current interface precludefull taxonomic symbol selection, our data analysis suggestseven a very coarse grained categorisation (e.g. complex vs.simple events) would yield a much improved musicologi-cal understanding of the data. Our work informs futuredesign of symbols used within the kit: ensuring greateruniformity of semantic complexity which would simplifyanalysis, as would the ability to more clearly delimit writ-ing events, either by the reduction of all symbols to single(rather than compound) drawing, or through a metric com-bining of temporal and geometric distance. Future deploy-ments of the kit will also record instances of ‘undo’.Our data indicates events with complex layering of typeand meaning throughout the performances, cautioning ag-ainst formulation of naively phrased MIR tasks such asidentifying “musicologically interesting parts in this anno-tated score”. Reﬂecting how tools can be utilised for musi-cology, our preliminary study makes clear there is unlikelyto be a ‘perfect’ feature to automatically complete a study;instead the method is iterative, with computational anal-ysis informed by musicology research questions and viceversa – through this iteration a fuller understanding of thequestion, investigation, and its limitations can be found.8. ACKNOWLEDGEMENTSThis work was supported by the UK Arts and Human-ities Research CouncilTransforming Musicologyproject(AH/L006820/1), part ofDigital Transformations. The au-thors gratefully acknowledge assistance during theRingandHearing Wagnerevents: the Birmingham Hippodrometeam, especially Zara Harris and Paul Kaynes; MariinskyOpera Company; and the greater project team. We thankJeff Fuller for invaluable Union Platform help and advice.216 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20159. REFERENCES[1]D. Bainbridge, X. Hu, and J. S. Downie. A musical pro-gression with Greenstone: How music content analysisand linked data is helping redeﬁne the boundaries to amusic digital library. InProceedings of the 1st Interna-tional Workshop on Digital Libraries for Musicologyin conjunction with the ACM/IEEE Digital Librariesconference 2014, pages 1–8, 2014.[2]S. Bechhofer, K. R. Page, and D. De Roure. HelloCleveland! Linked data publication of live musicarchives. In14th International Workshop on Image andAudio Analysis for Multimedia Interactive Services,pages 1–4, 2013.[3]C. Cannam, C. Landone, M. Sandler, and J. Bello. TheSonic Visualiser: A visualisation platform for seman-tic descriptors from musical signals. InProceedings ofthe International Symposium on Music Information Re-trieval, pages 324–327, 2006.[4]H. Christenson. HathiTrust.Library Resources & Tech-nical Services, 55(2):93–102, 2011.[5]N. Cook. Methods for analysing recordings. InTheCambridge Companion to Recorded Music, pages221–245, 2009.[6]N. Cook.Beyond the Score: Music as Performance.Oxford University Press, Oxford, 2014.[7]T. Crawford, B. Fields, D. Lewis, and K. R. Page. Ex-plorations in linked data practice for early music cor-pora. InProceedings of the IEEE/ACM Joint Confer-ence on Digital Libraries, pages 309–312, 2014.[8]M. Do˘gantan Dack. The art of research in live musicperformance.Music Performance Research, 5:34–48,2012.[9]H. Danuser. Urteil und Vorurteil im Interpretationsver-gleich.Zeitschrift f¨ur Musiktheorie, 6(2):76–88, 1975.[10]M. Doerr, C. Bekiari, P. LeBoeuf, and Biblio-theque Nationale de France. FRBRoo, a conceptualmodel for performing arts. In2008 Annual Conferenceof CIDOC, Athens, pages 06–18, 2008.[11]D. Dolan, J. Sloboda, H. J. Jensen, B. Cr¨uts, andE. Feygelson. The improvisatory approach to classi-cal music performance: An empirical investigation intoits characteristics and impact.Music Performance Re-search, 6, 2013.[12]X. Ferr´e, N. Juristo, H.Windl, and L. Constantine. Us-ability basics for software developers.IEEE Software,18(1):22–29, 2001.[13]X. Hu and Y . Yang. Cross-cultural mood regres-sion for music digital libraries. InProceedings ofthe IEEE/ACM Joint Conference on Digital Libraries,pages 471–472, 2014.[14]B. Kershaw. Performance as research: Live events anddocuments. InThe Cambridge Companion to Perfor-mance Studies, pages 23–45, 2008.[15]A. Marsden. ‘What was the question?’: Music analysisand the computer. InModern Methods for Musicology,pages 137–147, 2009.[16]C. A. Mullin. International music score libraryproject/Petrucci music library (review).Notes,67(2):376–381, 2010.[17]T. Nurmikko-Fuller, D. M. Weigl, and K. R. Page. Onorganising multimedia performance corpora for musi-cological study using linked data. InProceedings of the2nd International Workshop on Digital Libraries forMusicology, pages 25–28, 2015.[18]K. Okumura, S. Sako, and T. Kitamura. Stochasticmodeling of a musical performance with expressiverepresentations from the musical score. InProceedingsof the International Symposium on Music InformationRetrieval, pages 531–536, 2011.[19]K. R. Page, B. Fields, B. J. Nagel, G. O’Neill, D. DeRoure, and T. Crawford. Semantics for music analysisthrough linked data: How country is my country? Ine-Science (e-Science), 2010 IEEE Sixth InternationalConference on, pages 41–48, 2010.[20]A. Porter, M. Sordo, and X. Serra. Dunya: a system tobrowse audio music collections exploiting cultural con-text. InProceedings of the International Symposium onMusic Information Retrieval, pages 101–106, 2013.[21]R. C. Repetto and X. Serra. Creating a corpus of Jingju(Beijing opera) music and possibilities for melodicanalysis. InProceedings of the International Sympo-sium on Music Information Retrieval, pages 313–318,2014.[22]V . Sebastien, D. Sebastien, and N. Conruyt. Anno-tating works for music education: Propositions for amusical forms and structures ontology and a musi-cal performance ontology. InProceedings of the Inter-national Symposium on Music Information Retrieval,pages 451–456, 2013.[23]J. B. Smith, J. A. Burgoyne, I. Fujinaga, D. De Roure,and J. S. Downie. Design and creation of a large-scaledatabase of structural annotations. InProceedings ofthe International Symposium on Music Information Re-trieval, pages 555–560, 2011.[24]S. Tresize, N. Cook, E. Clarke, D. Leech-Wilkinson,and J. Rink. The recorded document: Interpretationand discography. InThe Cambridge Companion toRecorded Music, pages 186–209, 2009.[25]M. Winget. Annotations on musical scores by perform-ing musicians: Collaborative models, interactive meth-ods, and music digital library tool development.Jour-nal of the American Society for Information Scienceand Technology, 59(12):1878–1897, 2008.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 217"
    },
    {
        "title": "Harmonic-Percussive Source Separation Using Harmonicity and Sparsity Constraints.",
        "author": [
            "Jeongsoo Park 0001",
            "Kyogu Lee"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417323",
        "url": "https://doi.org/10.5281/zenodo.1417323",
        "ee": "https://zenodo.org/records/1417323/files/ParkL15.pdf",
        "abstract": "In this paper, we propose a novel approach to harmonic- percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying har- monic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observa- tion that the spectral distributions of harmonic and percus- sive signals differ – i.e., harmonic components have har- monic and sparse structure while percussive components are broadband – we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the pro- posed method outperforms the conventional methods.",
        "zenodo_id": 1417323,
        "dblp_key": "conf/ismir/ParkL15",
        "keywords": [
            "harmonic-percussive sound separation",
            "Non-negative Matrix Factorization",
            "sparsity",
            "harmonicity constraints",
            "conventional HPSS methods",
            "temporal continuity of harmonic components",
            "spectral continuity of percussive components",
            "time-varying harmonic signals",
            "percussive ones",
            "imposing different constraints"
        ],
        "content": "HARMONIC-PERCUSSIVE SOURCE SEPARATION USINGHARMONICITY AND SPARSITY CONSTRAINTSJeongsoo Park, Kyogu LeeMusic and Audio Research GroupSeoul National University, Seoul, Republic of Korea{psprink, kglee}@snu.ac.krABSTRACTIn this paper, we propose a novel approach to harmonic-percussive sound separation (HPSS) using Non-negativeMatrix Factorization (NMF) with sparsity and harmonicityconstraints. Conventional HPSS methods have focused ontemporal continuity of harmonic components and spectralcontinuity of percussive components. However, it may notbe appropriate to use them to separate time-varying har-monic signals such as vocals, vibratos, and glissandos, asthey lack in temporal continuity. Based on the observa-tion that the spectral distributions of harmonic and percus-sive signals differ –i.e., harmonic components have har-monic and sparse structure while percussive componentsare broadband – we propose an algorithm that successfullyseparates the rapidly time-varying harmonic signals fromthe percussive ones by imposing different constraints onthe two groups of spectral bases. Experiments with realrecordings as well as synthesized sounds show that the pro-posed method outperforms the conventional methods.1. INTRODUCTIONRecently, musical signal processing has received a greatdeal of attention especially with the rapid growth of digi-tal music sales. Automatic musical feature extraction andanalysis for a large amount of digital music data has beenenabled with the support of computational power. The ma-jor purposes of such tasks include extracting musical infor-mation such as melody extraction, chord estimation, onsetdetection, and tempo estimation.Because most music signals often consist of both har-monic and percussive signals, the extraction of tonal at-tributes is often severely degraded by the presence of per-cussive interference. On the other hand, when we analyzerhythmic attributes such as tempo estimation, the harmonicsignals act as interference that may prevent accurate anal-ysis. Consequently, the separation of harmonic and per-cussive components in music signals will function as anc\u0000Jeongsoo Park, Kyogu Lee.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Jeongsoo Park, Kyogu Lee.“Harmonic-Percussive Source Separation Using Harmonicity and Spar-sity Constraints”, 16th International Society for Music Information Re-trieval Conference, 2015.important pre-processing step that allows efﬁcient and pre-cise analysis.For these reasons, many researchers have focused oninvestigating HPSS using various approaches. Uhleetal. performed singular value decomposition (SVD) fol-lowed by independent component analysis (ICA) to sep-arate drum sounds from the mixture [1]. Gilletet al. pre-sented a drum-transcription algorithm based on band-wisedecomposition using sub-band analysis [2].Other researchers have employed matrix factoriza-tion techniques such as non-negative matric factoriza-tion (NMF). Helenet al. proposed a two-stage pro-cess composed of a matrix-factorization step and a basis-classiﬁcation step [3]. Kimet al. employed the matrix co-factorization technique, where spectrograms of the mix-ture sound and drum-only sound are jointly decomposed[4]. NMF with smoothness and sparseness constraintswas utilized by Canadas-Quesadaet al. [5]. The algo-rithm was developed based on assumptions regarding theanisotropic characteristics of the harmonic and percussivecomponents; harmonic components have temporal con-tinuity and spectral sparsity, whereas percussive compo-nents have spectral continuity and temporal sparsity.Most HPSS algorithms have employed the same as-sumption. Onoet al. presented a simple technique to rep-resent a mixture sound spectrogram as a sum of harmonicand percussive spectrograms based on the Euclidean dis-tance [6]. Their technique aims to minimize the tempo-ral dynamics of harmonic components and the spectral dy-namics of percussive components. They further extendedtheir work to use an alternative cost function based onthe Kullback-Leibler (KL) divergence [7]. More recently,FitzGerald presented a median ﬁltering-based algorithm[8], where a median ﬁlter is applied to the spectrogram ina row-wise and column-wise manner for the extraction ofharmonic and percussive sounds, respectively. Gkiokasetal. also proposed a non-linear ﬁlter-based HPSS algorithm[9].However, the assumption regarding the temporal con-tinuity, which is considered to be crucial for conven-tional harmonic-percussive studies, does not account forthe rapidly time-varying harmonic signals often present invocal sounds and musical expressions such as slides, vi-bratos, or glissandos. This is because their spectrogramsoften ﬂuctuate over short periods of time. Thus, it may de-grade the performance of the algorithms, particularly when148loud vocal components or such musical expressions aremixed.In this paper, we propose a HPSS algorithm that is clas-siﬁed as a spectrogram decomposition-based method. Weconsider the spectrum of harmonic components to have aharmonic and sparse structure in the frequency domain,whereas the spectrum of percussive components to havean unsparse structure. To realize the successful separationof harmonic/percussive sounds, we apply constraints thatimpose a particular structure of the spectral bases. Thenovelty of the proposed method resides in the harmonic-ity constraint, which is an extension of the sparsity con-straint presented in previous works [10]. The constraint isclosely related to the Dirichlet prior, which is frequentlyused in probabilistic analysis. Because the proposed al-gorithm does not assume temporal continuity for the sep-aration of harmonic signals, we can successfully separateharmonic signals from the mixture sound, even when thereare signiﬁcant ﬂuctuations over time.The rest of this paper is organized as follows. Section2 explains in detail how the proposed method works. InSection 3, we present experimental results, and in Section4, we conclude the paper.2. PROPOSED METHODIn this section, we present a detailed explanation of theproposed HPSS method. The proposed algorithm uses thespectrogram-decomposition technique, NMF, with the har-monicity and sparsity constraints based on the Dirichletprior. For the efﬁcient description of the proposed method,we ﬁrst introduce the conventional NMF. Then, the algo-rithm description for the proposed method is presented. Fi-nally, the theoretical relations of the proposed method tothe Dirichlet prior are described.2.1 Conventional NMFLee and Seung introduced the multiplicative update rule ofNMF for KL divergence [11]. As we iteratively update theparameters, we can represent a non-negative matrix, whichmay correspond to a magnitude spectrogram, as a multi-plication of two non-negative matrices that may containspectral bases and temporal bases. The update rule can berepresented as:Hk,n Hk,nPmnWm,kFm,n.˜Fm,noPm0Wm0,k(1)Wm,k Wm,kPnnHk,nFm,n.˜Fm,noPn0Hk,n0(2)whereFand˜Fdenote theM⇥Nmagnitude spectrogramof an audio mixture, and its estimation, respectively,WandHdenote theM⇥Kmatrix of the spectral bases andtheK⇥Nmatrix of their activations.2.2 Formulation of Harmonic-Percussive SeparationWe present a modiﬁed NMF algorithm to impose the char-acteristics of harmonic/percussive sounds. The update ruleis separately represented for the harmonic source basis andpercussive source basis as follows:Hk,n Hk,nPmnWm,kFm,n.˜Fm,noPm0Wm0,k(3)Wm,k Wm,kPnnHk,nFm,n.˜Fm,noPn0Hk,n0(4)wk \u00001\u0000\u0000HH\u0000wk+\u0000HHifft({fft(wk)}p),k2\u0000H(5)wk max (wk,0),k2\u0000H(6)⇢wk \u00001\u0000\u0000HS\u0000wk+\u0000HS(wk)q,k2\u0000Hwk \u00001\u0000\u0000PS\u0000wk+\u0000PS(wk)r,k2\u0000P(7)where\u0000Hand\u0000Pdenote a set of harmonic bases and per-cussive bases, respectively,fft(·)andifft(·)denote thefunctions of the fast Fourier transform (FFT) and the in-verse FFT (IFFT), respectively,wkdenotes thekth col-umn ofW,\u0000HHdenotes the harmonicity weight parameterfor the harmonic signal, and\u0000HSand\u0000PSdenote the sparsityweight parameters for harmonic and percussive signals, re-spectively. Note that Eqns (3) and (4) are identical to Eqns(1) and (2), respectively. Eqns (5)-(7) contribute to shapingthe spectral bases as desired as the iteration proceeds.Mixing weights that have values between 0 and 1 rep-resent the importance of each constraint imposition, andindicate the degree to which we need to impose the charac-teristic. To enable the harmonic bases to have a harmonicand sparse structure while preserving the original ﬁguresof spectral bases,\u0000HHand\u0000HSare set to have small positivenumbers, as the effect of the constraint is accumulated overthe iteration.The exponentsp,q, andrhave to be determined consid-ering the range of each parameter,0r1p, q. Here,pandqrespectively reﬂect the degree of harmonicity andsparsity of the destination, and they have to be controlledconsidering the spectral characteristics of the original har-monic sources. Likewise,rreﬂects the degree of “unspar-sity” of the percussive sources.Among the update equations shown above, the functionof the conventional NMF update equations in Eqns (3) and(4) is to minimize the error betweenFand its estimation˜F. On the other hand, the remainders of the equationsaim to shape the spectral bases. The sparsity constraintin Eqn (7) has been similarly adopted for the matrix de-composition [10], and it is based on the fact that the squareoperation increases the differences among the vector com-ponents. If the square root operation is used instead, asProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 149in the percussive case of Eqn (7), unsparsity can be im-posed to the basis. Similarly, we can extend this conceptto the harmonicity. The second term in Eqn (5) denotesthe harmonics-emphasized basis, which is due to the factthat thespectrum of the spectrumis sparse. To prevent ele-ments from being negative, themax (·,·)operation in Eqn(6) has to be jointly involved.The harmonic and percussive sounds are reconstructedusing the corresponding bases as follows:F(Harmonic)=Xk2\u0000Hwkhk(8)F(P ercussive)=Xk2\u0000Pwkhk(9)wherehkdenotes thekth row ofH.2.3 Relation to Dirichlet PriorThe proposed update equations can be intuitively compre-hended. However, the equations are based on a ﬁrm theo-retical background, not heuristically induced. In this sub-section, we employ Dirichlet prior from the probabilitytheory, and investigate its relations to the proposed method.Priors were primarily adopted for the Bayesian proba-bility theory, including the probabilistic latent componentanalysis (PLCA) or probabilistic latent semantic analysis(PLSA). Such spectrogram decomposition techniques of-ten regard spectrogram components as histogram elementsof multinomial distributions. Because the Dirichlet distri-bution is a conjugate prior of a multinomial distribution,it can be adopted as a prior knowledge of a multinomialdistribution. By adopting the prior, we can modify ourgoal to be the maximizing posterior from the maximizinglikelihood. For this reason, the Dirichlet prior has beenadopted for the matrix factorization in the previous works[10], [12]. Our method employs one of the extensions ofthe Dirichlet prior for harmonicity imposition.Because PLCA is a special case of NMF, where itscost function is KL divergence [13], we can generalize theDirichlet prior of the PLCA [12] by applying it to the NMFalgorithm as follows:Hk,n (1\u0000\u00001)Hk,nPmnWm,kFm,n.˜Fm,noPm0Wm0,k+\u00001Ak,n(10)Wm,k (1\u0000\u00002)Wm,kPnnHk,nFm,n.˜Fm,noPn0Hk,n0+\u00002Bm,k(11)whereAandBdenote the matrices of hyper parameterswith respect toHandW, respectively, and\u00001and\u00002de-note the mixing weights. In our research, we focus only onthe spectral bases, and thus Eqn (10) is discarded. As canbe observed, the proposed update equations, Eqns (3)-(7),have the same form as Eqn (11), and the way in which weshape the spectral bases depends on the form ofBmatrix.Frequency-domain sparsity imposition can be easilyachieved by setting the hyper parameterBas [10]bk=(wk)u(12)wherebkdenotes thekth column ofB, andudenotes anexponent that controls the degree of sparsity ofbk.On the other hand, harmonicity imposition can beachieved when the hyper parameter is represented asbk=ifft({fft(wk)}v)(13)wherevdenotes the exponent that controls the degree ofharmonicity ofbk. This is because a periodic signal canbe represented as a sum of sinusoids, and the spectrumof the periodic signal is sparse. Conversely, if a spec-trum is sparse, we can assume that the original signal hasa strongly periodic characteristic. Thus, we aim to makethespectrum of the spectrumto be sparse in order to shapea signal such that it has a harmonic structure. Note that inorder to prevent destructive interference caused by phasedistortion, we have to manipulate only the magnitudeswithin the IFFT function, preserving the original phasesoffft(wk).3. PERFORMANCE EVALUATION3.1 Sample ProblemIn this section, we apply the proposed method and the con-ventional methods to simple sample examples, which issuitable for showing the novelty and validity of the pro-posed method. Spectrograms of synthesized sounds thatconsist of horizontal and vertical lines are presented inFigure 1(a) and Figure 2(a). Figure 1(a) models the casewhere a pitched harmonic sound is sustained for a certainperiod. The sounds of harmonic instruments such as gui-tars, pianos, ﬂutes, and violins fall within this scenario.On the other hand, Figure 2(a) illustrates the case wherea harmonic signal alters its frequency over time. In thiscase, vibratos, glissandos, and vocal signals correspond tothe harmonic components. We compare the performanceof the proposed method to the separation results obtainedusing three conventional methods: Onoet al.’s Euclideandistance-based method [6], Onoet al.’s KL divergence-based method [7], and FitzGerald’s method [8].As shown in Figure 1(b), both the conventional methodsand the proposed method are able to successfully separatethe sounds. This is because the horizontal lines in this ex-ample have horizontally continuous characteristics, whichare assumed by the conventional methods to be present.However, when the harmonic sound vibrates and the hor-izontal lines ﬂuctuate, as shown in Figure 2(a), conven-tional methods cannot distinguish the horizontal lines fromvertical lines. As we can see in Figure 2(b), the estimatedpercussive components of conventional methods containharmonic partials, and only the proposed method can suc-cessfully separate them. Thus, we can claim that the pro-150 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Original\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\n(a) Original ﬁgure\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\n(b) Separation results of Ono’s Euclidean distance-based method,Ono’s KL divergence-based method, FitzGerald’s method, and theproposed method (from top to bottom)Figure 1. Sample example of separating horizontal linesand vertical lines.posed method is not affected by variations in the pitch be-cause it relies on the harmonic structure of the vertical axis,and not the degree of horizontal transition.3.2 Qualitative AnalysisWe evaluated the performance of the proposed method us-ing a real recording example. Figure 3 shows a log-scaleplot of the spectrogram of an excerpt from “Billie Jean,”byMichael Jackson. The signal was sampled at 22,050 Hz,and the frame size and overlap size were set to 1,024 and512, respectively. We can observe from the spectrogram\nOriginal\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\n(a) Original ﬁgure\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nHarmonic\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\nPercussive\nTime (s)Frequency (Hz)0.20.40.60.8100.511.52x 104\n(b) Separation results of Ono’s Euclidean distance-based method,Ono’s KL divergence-based method, FitzGerald’s method, and theproposed method (from top to bottom)Figure 2. Sample example of separating ﬂuctuating hori-zontal lines and vertical lines.that the excerpt contains both harmonic and percussivecomponents. The harmonic components can be seen ashorizontally connected lines, whereas the percussive com-ponents are seen as vertical lines as in the sample exam-ples.Figure 4(a) and (b) show the separation results of theharmonic sound (up) and percussive sound (down), whichwere obtained using Onoet al.’s Euclidean distance-basedmethod and KL divergence-based method, respectively.Here, we set the parameters to the values recommended inthe references. We observe that the estimated percussiveProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 151Original\nTime (s)Frequency (Hz)24681012140500100015002000\nFigure 3. Spectrogram of a real audio recording example(“Billie Jean” by Michael Jackson).components still contain harmonic components that maycorrespond to the vocal components. This is because Onoet al.’s algorithms aim to minimize the temporal transitionof the harmonic spectrogram. However, vocal componentsin the original spectrogram do not match well with the un-derlying assumption.Figure 4(c) shows the result of FitzGerald’s methodwith a median ﬁlter length of 17 and when the exponentfor the Wiener ﬁlter-based soft mask is two, as recom-mended by FitzGerald [8]. We also observe that the sep-arated percussive components still contain harmonic com-ponents, as in the previous case. This is because of the useof a one-dimensional median ﬁlter, which assumes that theharmonic components are sustained for several periods.Figure 4(d) shows the performance of the proposedmethod. We observe that the harmonic and percussivecomponents are clearly separated, and the percussive com-ponents do not have any vocal components in these results.This is because unlike conventional methods, the proposedalgorithm does not rely on the horizontal continuity prin-ciple. Rather, the proposed algorithm tries to account forthe harmonic components using the harmonic and sparsespectral bases.3.3 Quantitative AnalysisWe performed a quantitative analysis to verify the va-lidity of the proposed algorithm. First, we compileda dataset that consists of 10 audio samples, which isa subset of the MASS database [14], but two sets ofdata, namelytamy-quepenatantofaz6-19andtamy-quepenatantofaz46-57, were excluded in this exper-iment because they lack percussive signals. Then, weobtained a spectrogram for each audio sample with theframe size and hop size set to 2,048 samples and 1,024samples, respectively. Note that the sampling rate ofthe songs in the MASS dataset is 44,100 Hz. Fi-nally, we measured the signal-to-distortion ratio (SDR),signal-to-interference ratio (SIR), and signal-to-artifactratio (SAR) using the BSSEV AL toolbox (http://bass-db.gforce.inria.fr /bsseval/) supported by [15]. Table 1shows the parameter values of the proposed method used inthis experiment. The parameters of the conventional meth-ods are set to the recommended values, as in the previousexperiment.The evaluation results are summarized in Figure 5. Wecan see that the proposed method guarantees a better av-\nHarmonic\nTime (s)Frequency (Hz)24681012140500100015002000\nPercussive\nTime (s)Frequency (Hz)24681012140500100015002000(a) Ono’s Euclidean distance-based method\nHarmonic\nTime (s)Frequency (Hz)24681012140500100015002000\nPercussive\nTime (s)Frequency (Hz)24681012140500100015002000(b) Ono’s KL divergence-based method\nHarmonic\nTime (s)Frequency (Hz)24681012140500100015002000\nPercussive\nTime (s)Frequency (Hz)24681012140500100015002000(c) FitzGerald’s method\nHarmonic\nTime (s)Frequency (Hz)24681012140500100015002000\nPercussive\nTime (s)Frequency (Hz)24681012140500100015002000(d) Proposed methodFigure 4. Qualitative performance comparison of conven-tional and proposed methods.152 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015erage SDR result compared to conventional methods, eventhough the proposed method has a lower SIR performancethan Onoet al.’s Euclidean distance-based method. This isbecause the proposed method far outperforms other meth-ods with respect to the SAR, which has a trade-off relationwith the SIR [16].ParameterValuep1.1q1.1r0.5\u0000HH0.001\u0000HS0.001\u0000PS0.1Number of bases (H,P)(300,200)Table 1. Experimental parameters.\n00.511.522.533.54Average SDR (dB scale)  Ono (Euclidean dist.)Ono (KL div.)FitzGeraldProposed\n(a) SDR\n012345678910Average SIR (dB scale)  \nOno (Euclidean dist.)Ono (KL div.)FitzGeraldProposed(b) SIR\n024681012Average SAR (dB scale)  Ono (Euclidean dist.)Ono (KL div.)FitzGeraldProposed\n(c) SARFigure 5. Quantitative performance comparison of con-ventional and proposed methods.4. CONCLUSIONIn this paper, we proposed a novel HPSS algorithm basedon NMF with harmonicity and sparsity constraints. Con-ventional methods assumed that the harmonic componentswere represented as horizontal lines with temporal conti-nuity. However, such an assumption could not be appliedto the vocal components or various musical expressionsof harmonic instruments. To overcome this problem, wepresented a harmonicity constraint, which is a generalizedDirichlet prior. By letting the spectrum of the spectrum beharmonic and sparse, we could reﬁne the harmonic compo-nents and eliminate inharmonic components. The experi-mental results showed the validity of the proposed methodby comparing it with conventional methods.5. ACKNOWLEDGEMENTSThis research was partly supported by the MSIP(Ministryof Science, ICT and Future Planning), Korea, under theITRC(Information Technology Research Center) supportprogram (IITP-2015-H8501-15-1016) supervised by theIITP(Institute for Information & communications Technol-ogy Promotion). Also, this research was supported in partby the A3 Foresight Program through the National Re-search Foundation of Korea (NRF) funded by the Ministryof Education, Science and Technology.6. REFERENCES[1]C. Uhle, C. Dittmar, and T. Sporer, “Extraction of drumtracks from polyphonic music using independent sub-space analysis,”Proceedings of the ICA, pp. 843–847,April, 2003.[2]O. Gillet and G. Richard, “Drum track transcriptionof polyphonic music using noise subspace projec-tion,”Proceedings of the ISMIR, pp. 92–99, Septem-ber, 2005.[3]M. Helen and T. Virtanen, “Separation of drums frompolyphonic music using non-negative matrix factoriza-tion and support vector machine,”Proceedings of theEUSIPCO, September 2005.[4]M. Kim, J. Yoo, K. Kang, and S. Choi, “Nonnegativematrix partial co-factorization for spectral and tempo-ral drum source separation,”IEEE Journal of SelectedTopics in Signal Processing, V ol.5, No.6, pp. 1192–1204, 2011.[5]F. J. Canadas-Quesada, P. Vera-Candeas, N. Ruiz-Reyes, J. Carabias-Orti, and P. Cabanas-Molero, “Per-cussive/harmonic sound separation by non-negativematrix factorization with smoothness/sparseness con-straints,”EURASIP Journal on Audio, Speech, and Mu-sic Processing, V ol.2014, No.1, pp. 1–17, 2014.[6]N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka, andS. Sagayama, “Separation of a monaural audio signalProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 153into harmonic/percussive components by complemen-tary diffusion on spectrogram,”Proceedings of the EU-SIPCO, August, 2008.[7]N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama,“A real-time equalizer of harmonic and percussivecomponents in music signals,”Proceedings of the IS-MIR, pp. 139–144, September, 2008.[8]D. FitzGerald, “Harmonic/percussive separation usingmedian ﬁltering,”Proceedings of the 13th Interna-tional Conference on Digital Audio Effects (DAFx-10),2010.[9]A. Gkiokas, V . Papavassiliou, V . Katsouros, and G.Carayannis, “Deploying nonlinear image ﬁlters tospectrogram for harmonic/percussive separation,”Pro-ceedings of the 15th International Conference on Dig-ital Audio Effects (DAFx-12), 2012.[10]M. Kim and P. Smaragdis, “Manifold preserving hi-erarchical topic models for quantization and approxi-mation,”Proceedings of the 30th International Confer-ence on Machine Learning (ICML-13), pp. 1373–1381,2013.[11]D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix factorization,”Proceedings of the Ad-vances in Neural Information Processing Systems,November, 2000.[12]P. Smaragdis and G. J. Mysore, “Separation by hum-ming: User-guided sound extraction from monophonicmixtures,”IEEE Workshop on Applications of SignalProcessing to Audio and Acoustics (WASPAA), Octo-ber, 2009.[13]C. Ding, T. Li, and W. Peng, “On the equiva-lence between non-negative matrix factorization andprobabilistic latent semantic indexing,”ComputationalStatistics & Data Analysis, V ol.52, No.8, pp. 3913–3927, 2008.[14]M. Vinyes, “MTG MASS database,” http://www.mtg.upf.edu/static/mass/resources, 2008.[15]E. Vincent, R. Gribonval, and C. Fevotte, “Perfor-mance measurement in blind audio source separation,”IEEE Transactions on Audio, Speech, and LanguageProcessing, V ol.14, No.4, pp. 1462–1469, 2006.[16]D. L. Sun, and G. J. Mysore, “Universal speech modelsfor speaker independent single channel source separa-tion,”Proceedings of the ICASSP, 2013.154 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Song2Quartet: A System for Generating String Quartet Cover Songs from Polyphonic Audio of Popular Music.",
        "author": [
            "Graham Percival",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415014",
        "url": "https://doi.org/10.5281/zenodo.1415014",
        "ee": "https://zenodo.org/records/1415014/files/PercivalFG15.pdf",
        "abstract": "We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilis- tic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Pre- vious work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time- frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet pro- duces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet mu- sic while retaining some partial phrases and chord voicings from the target audio.",
        "zenodo_id": 1415014,
        "dblp_key": "conf/ismir/PercivalFG15",
        "keywords": [
            "Song2Quartet",
            "system for generating string quartet versions",
            "popular songs",
            "combining probabilistic models",
            "corpus of symbolic classical music",
            "target audio file",
            "add novelty to listening experience",
            "familiarity with string quartets",
            "audio features of the target song",
            "previously work in automatic arrangement"
        ],
        "content": "SONG2QUARTET: A SYSTEM FOR GENERATING STRING QUARTETCOVER SONGS FROM POLYPHONIC AUDIO OF POPULAR MUSICGraham Percival, Satoru Fukayama, Masataka GotoNational Institute of Advanced Industrial Science and Technology (AIST), Japangraham@percival-music.ca,s.fukayama@aist.go.jp,m.goto@aist.go.jpABSTRACTWe present Song2Quartet, a system for generating stringquartet versions of popular songs by combining probabilis-tic models estimated from a corpus of symbolic classicalmusic with the target audio ﬁle of any song. Song2Quartetallows users to add novelty to listening experience of theirfavorite songs and gain familiarity with string quartets. Pre-vious work in automatic arrangement of music only usedsymbolic scores to achieve a particular musical style; ourchallenge is to also consider audio features of the targetpopular song. In addition to typical audio music contentanalysis such as beat and chord estimation, we also use time-frequency spectral analysis in order to better reﬂect partialphrases of the song in its cover version. Song2Quartet pro-duces a probabilistic network of possible musical notes atevery sixteenth note for each accompanying instrument ofthe quartet by combining beats, chords, and spectrogramfrom the target song with Markov chains estimated fromour corpora of quartet music. As a result, the musical scoreof the cover version can be generated by ﬁnding the optimalpaths through these networks. We show that the generatedresults follow the conventions of classical string quartet mu-sic while retaining some partial phrases and chord voicingsfrom the target audio.1. INTRODUCTIONCover songs are arrangements of an original song with cer-tain variations which add novelty. Changing the instrumentsused is one such variation, but a complete switch of instru-mentation may result in very unusual parts. For example,completely replacing a chord-heavy guitar part with a violinmay result in unplayable (or very difﬁcult) chords. Arrang-ing music for different instruments requires considerationabout the music those instruments normally perform.Previous approaches in automated arrangement are mostlyperformed in the symbolic domain of music. Melody har-monization and re-harmonization of chord sequences takesymbols of chords or pitches as inputs [1, 7, 10, 16]. Gui-tar arrangements of piano music can be generated from ac\u0000Graham Percival, Satoru Fukayama, Masataka Goto.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Graham Percival, Satoru Fukayama, Masa-taka Goto. “ Song2Quartet: A System for Generating String Quartet CoverSongs from Polyphonic Audio of Popular Music ”, 16th InternationalSociety for Music Information Retrieval Conference, 2015.Target audio ﬁlePop songCorpus of symbolic musicMozart string quartetsAudio analysisSpectral magnitude,onsets, chords, etc.Score analysisProbabilistic modelsof pitch and rhythmProbabilisticgenerationPaths throughnetworksScoreMozart quartet-stylecover of pop songFigure 1: Generating a cover song with a speciﬁc style.Sample results are available at:https://staff.aist.go.jp/m.goto/Song2Quartet/MusicXML score [14]. Statistical modelling of a corpushas also been used to generate electronic dance music [6].Furthermore, automatically generating music in a speciﬁcinstrumental style is not well explored. In a great dealof work on computer-assisted composition [8], some auto-matic composition systems attempted to generate resultswith a particular composer’s musical style [4] or the user’smusical style [15]. However, those systems cannot be usedto generate cover songs in a particular instrument style bypreserving the recognizable parts of the original songs.We present Song2Quartet to address this issue. Anoverview of our system is shown in Figure 1. Two novelaspects of this work, the audio analysis for generating coversongs and generating music in a speciﬁc instrumental style,are addressed in the audio analysis and score analysis mod-ules, respectively.To ensure that the generated cover songs include featuresthat are also recognizable in the original audio, the audioanalysis module estimates notable rhythms, chord voicings,and contrary motions between melody and bass by extract-ing the audio spectrum. In parallel, to generate music tobe playable and recognizably following the classical stringquartet style, the score analysis module captures charac-teristics of the string quartet from the corpus of symbolicmusic such as the typical note onsets in a measure and thepitch transitions of each instrument in the quartet.These two aspects are balanced by means of a probabilis-tic formulation, where the corpus style and audio analysisare combined by weighted multiplication. The audio analy-sis provides probabilities for observing note events at every16thnote, and the score analysis mainly provides the tran-sition probabilities of notes. We formalize our generationof cover songs as ﬁnding the sequence of notes which max-imizes the probabilities obtained from the modules usingdynamic programming, with techniques to compress thesearch space to make our problem tractable.114Wav ﬁleHPSSHarmonic portionVQ TransformBeatestimationChord estimation(including bass)MelodyestimationCalculatesliced meansOnsetestimationAM\nAO\nCB\nC\nM\ntimetimetimetimeMIDIMIDI\nMIDIMIDIMIDIFigure 2: Audio analysis (4 measures shown in examples).2. ANALYSIS2.1 Features needed from audioKnowing which pitches are in the polyphonic music isuseful in creating cover songs. Since multi-pitch analysismethods often suffer from pitch and onset detection errorswhen handling polyphonic music with a drum track, wecannot simply apply the analysis beforehand and use theanalysis results as constraints. However, we can use theaudio feature extraction portions of multi-pitch analysis toaid in generating cover songs. Concretely, after performingHarmonic/Percussive source separation, the magnitudes andonsets of each note are obtained by applying a variable-Qspectral transform and calculating the salience function ofthe onset events.The melody, chords, bass, and beats of a song providemusical facets which should be observed in the cover ver-sion of a song. These facets are extracted from the audio us-ing Songle, a music understanding engine [13]. The melodyand bass pitches, as well as the chord labels, are segmentedaccording to the time grid provided by the analyzed beats.Later, these will be combined with the beat-aligned audiospectral analysis to form probabilistic constraints.2.2 Audio analysisFigure 2 shows an overview of the audio analysis. We per-form Harmonic/Percussive source separation with medianﬁltering [9], then use a variable-Q transform (VQT) [18]with a Blackman-Harris window and the variable-Q pa-rameter\u0000set to use a constant fraction of the equivalentrectangular bandwidths [11], giving us spectral analysisS. The frequency range was set to 65 Hz–2500 Hz (MIDIpitches 36–99).We then perform beat estimation on the original audiowith Songle and divide each beat into 4, giving 16thnotes.The means (over time) of VQT bins that fall within therange of each 16thnote are calculated, producing the slicedspectrogramAM.AMis normalized to the range[0,1].To estimate onset probabilities in the target song, weuse two methods: ﬂux ofAMand ﬁrst-derivative Savitzky-Golay ﬁltering [17] onS. The ﬂux ofAMis simply thehalf-wave rectiﬁed difference between successive 16thnotesofAM. For the latter method, we calculate the smoothedﬁrst derivative ofSalong the time axis using Savitzky-Golay ﬁltering with a window size of 21 samples to ﬁnd\nSymbolic musicGet goodmeasures16th-noterhythm grid1st-order Markovrhythms1st-order Markovabsolute pitches1st-order Markovrelative pitchesGR\nTR\nTA\nTE\ntimeR. eventR. eventR. eventMIDIMIDIrel. MIDIØFigure 3: Score analysis (Mozart cello in examples).1/rests.40/noteheads.s20/noteheads.s231/rests.321/rests.4/dots.dot1/rests.3222044/noteheads.s2eventmusic/flags.u30/noteheads.s2/flags.u40/noteheads.s233Figure 4: Rhythmic events detected in the score.the peaks ofS. To quantize the onset estimation to the16th-note level, we ﬁnd the maximum peak within a timewindow equal to a 16thnote duration, but shifted backwardsin time by 25% to accommodate slight inaccuracies in thebeat detection. Both methods operate on each MIDI pitchindependently. We setAOto be the sum of the two methods,and normalize it to the range[0,1].Finally, we extract two more pieces of information using[13]: the melodyM, and the chords in each song, includingboth the overall chord nameCand the bass pitchCB.2.3 Features needed from the scoreFeatures obtained from the score analysis contribute tomaintaining the musical style. Classical string quartet mu-sic rarely includes complex rhythms and very large pitchintervals, so we obtain these tendencies as probabilities ofrhythm and pitch intervals from the corpus of scores.2.4 Score analysisFigure 3 shows an overview of the score analysis. Weused the Music21 [5] toolkit and corpus to analyze stringquartets by Haydn, Mozart, and Beethoven. Our analysiscomprised of pitches and rhythms, and only used music in4/4 time which ﬁt into a 16th-note grid. If the time signaturechanged in the middle of a movement, we only consideredthe portion(s) in 4/4.We calculated the probabilities of rhythmic events ina 16thnote grid. Rhythmic events were deﬁned as one offour possible values: 0 indicated a new note, 1 indicateda new rest, 2 indicated a continued note, and 3 indicateda continued rest; an example is shown in Figure 4. Thisresulted in a 4x16 matrix of probabilitiesGR, with eachprobability being the number of occurrences divided by thenumber of measures.We extracted 1st-order Markovian [2] rhythm transitions.This is simply the probability of each [previous event, nextevent] pair occurring, and produced a 4x4 matrixTR.We calculated 1st-order Markovian pitch transitions forboth absolute pitches and relative pitches. We considered achord-note or pair of chords to include every pitch transi-tion between the notes in successive chords. For simplicity,we recorded these transitions in two 100x100 matricesTAProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 115Song informationViolin 1melodyCelloDPViolin 2 & Violashared DPavoidnotesoutput notes\navoidnotesoutput notesoutput notesFigure 5: Overview of creating parts.andTE, even though a classical string quartet will not haveany notes below MIDI pitch 36. For the absolute pitches,we added a10\u00003chance of any transition between validpitches; this is necessary to allow some modern pop songswith non-classical chord progressions to be generated, par-ticularly in the cello which is limited to the bass notesCB.3. PROBABILISTIC GENERATIONFigure 5 gives an overview of generating the quartet parts.First, the violin 1 part is set to the melody. Second, thecello part is generated with a probabilistic method and dy-namic programming. Third, the violin 2 and viola parts aregenerated together via the same probabilistic method anddynamic programming.To prepare for the dynamic programming, we need todeﬁne theemissionandtransitionmatrices, denoted byEandT, respectively. Our time unit is 16thnotes, and weconsider 200 possible events for each time-slice: 0 is a rest,1–99 are note onsets of the same MIDI pitches, 100 is aheld rest, and 101–199 are held notes (of MIDI pitch+100).We deﬁneNas the number of 16thnotes in the target song.An overview of calculatingEandTis shown in Figure 6.3.1 Constructing probability matrices3.1.1 Construction emission probabilitiesEThe emission probabilitiesEis a matrix of sizeN⇥200,representing every possible event at every 16thnote. Theyare generated by calculatingEO(onsets) andEH(heldnotes), each of sizeN⇥100,EO=A0O⌦C0⌦G0R⌦IR⌦V(1)EH=A0M⌦C0⌦G0R⌦IR⌦V(2)where⌦is the element-wise product. The intuition behindthis multiplication is that we consider each variable to bean independent probability distribution, so we are calcu-lating the joint distribution.EOandEHare then stackedvertically to formE. The variables are:•A0O,A0M—Audio onsets and magnitudes: Audio onsetsAOand magnitudesAMfor MIDI pitches 1–99 aretaken directly from the audio analysis. The “silence”event (0) is set to a constant value of10\u00005.•C0—Chord tones: We construct a matrix of all MIDIpitches for every 16thnote in the song; each cell is 1 ifthat pitch is in the given chord,10\u00002otherwise. For thecello, we use the bass note of each chordCB; for otherinstruments, we use any chord tone included inC.\nWav ﬁleSheet music corpusOtherinstrumentsAudio analysis⌦⌦Corpus analysisCreate transitionIRHMVAO,CAM,CGRTR,TA,TEEH\nEO\nE\nT\nstacktimeeventold eventnew eventFigure 6: Calculating emission and transition probabilitiesEandT.⌦indicates element-wise multiplication.•G0R—Rhythm grids: We take the overall probabilityof a rhythmic event in the corpus at each 16thnoteGR,and repeat it for every 16 time-slices inN.•IR—Extreme instrument ranges: We specify maxi-mum bounds for instrument ranges: MIDI pitches 36–69for cello, 48–81 for viola, and 55–99 for violin. Whena corpus of symbolic music is used, the pitch transitionprobabilities narrow these ranges;IRis only relevant ifthe user chooses not to use any corpus.•V—Avoid previously-used notes: We reduce the prob-ability of using the same notes as other instruments bysetting them to10\u00002inV; other events are set to 1. Wealso reduce the probability of playing a note one octavehigher than an existing note (as those are likely octaveerrors in the audio analysis) by likewise setting thosevalues to10\u00002.We eliminate any non-rest values less than10\u00003to re-duce the computational load for music generation.3.1.2 Construction transition probabilitiesTThe transition probabilitiesTare a matrix of size200⇥200,representing every possible event-to-event pair.T=T0R⌦T0A⌦T0E⌦H(3)The variables are:•T0R—Rhythm transitions: We useTR, the probabilityof each rhythm event following a previous rhythm event.The note onset and held note probabilities are copied tovectors 1–99 and 101-199 respectively, while the restonset and held rest probabilities are copied into vectors0 and 100.•T0A,T0E—Pitch transitions: We use the probabilitiesof each pitch following a previous pitch consideringabsolute or relative pitches,TAandTErespectively.These matrices are originally100⇥100; we simply copythe matrices four times to create200⇥200matrices (thatis to say, allowing these relative transitions to apply toonset-onset, onset-held, held-onset, held-held pairs).•H—Hold-events only after onset-events: Each “hold”event (events 100 and up) can only occur after its re-spective “onset” event. We formalize this constraintas a matrixHwhere rows 0–99 are all 1, while rows100–199 contain two identity matrices (in columns 0–99and 100-199).116 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 201501,0002,0000100200t\ntime (16thnotes)events\u0000ln(E)E[t]01002000100200\n!Extend\u0000ln(E[t])+01002000100200\nprevious eventsnew events\u0000ln(T)\nLL[0,·,·]L[t,·,·]L[2400,·,·]Figure 7: Combining emission and transition probabilitiesEandTinto overall log-probabilitiesL.3.2 Generation of a cover song under constraintsWe combineEandTto form the log-probabilityLof thearranged score given the observed audio and corpus data,which has dimensionsN⇥200⇥200. For eacht2N,L[t,·,·]=\u0000ln(E[t,\u0000,:])\u0000ln(T)(4)whereE[t,\u0000,:]indicates that the 1x200 column vectorE[t]is extended to form a 200x200 matrix. This is illustrated inFigure 7. SinceEandTcontain very small numbers, weadd their negative log-values instead of multiplying them.Lcan be visualized by considering it to be a networkof time-events (Figure 8). The maximum probability ofa given score occurs when the negative log-probability isminimized; i.e. by ﬁnding the shortest path throughLwitha standard dynamic programming algorithm [3].3.2.1 Local and Global Shortest PathsAs shown in Figure 5, we calculate the cello accompanimentpart ﬁrst. After that, we could solve the viola and thenviolin 2 parts separately, but we found that this occasionallyproduced very high violin 2 music. Instead we solve theviolin 2 and viola parts together, with the constraint thatthey cannot play the same pitch at the same time.In order to ﬁnd two shortest paths simultaneously, weconstruct a large network with every possible combinationof nodes from each time-slice of the individual violin 2 andviola networks. For example, if at timetthe violin 2 couldhave 4 possible events and the viola could have 5 possibleevents, then the combined network will have 20 possibleevents for timet. The edge weights are simply the sum ofthe existing edges from the individual networks.3.2.2 Compacting MatricesTo lower memory usage and improve processing time, wereduce the size of the matrices. We construct a mappingfor each time-slicetbetween the non-inﬁnite weights inLand a smaller matrix. This takes approximately 1 second,and results in a matrix which is roughly 1% of the originalsize (e.g., 96 million entries reduced to 1.2 million entries).Note that this compression is lossless and does not affectthe shortest-path calculation, as an edge with weight1willnot appear in the shortest path.\nFigure 8: Network of possible pitchesL; shortest pathcolored red. Node labels are in the form “time-event”,with eventxbeing a MIDI pitch onset (x<100) or hold(x\u0000100). For legibility, edges with a weight of inﬁnity andnodes with no non-inﬁnite-weight edges are not displayed.“Compacting”Lin this way speeds up the computationof the single cello part, but its true value is found when com-bining the violin 2 and viola parts. Without any compacting,a normal pop song (150 measures) produces a network fora single part with2400⇥200⇥200 = 9.6·107entries.However, naively combining the violin 2 and viola partsproduces a network with2400⇥2002⇥2002=3.8·1012entries (15 TB of memory). We therefore perform tworounds of compacting; before and after combining the parts.After compacting the individual violin 2 and viola parts,we are left with networks of size approximately 1.6 millionand 2.3 million. After performing the second round of com-pacting (this time on the combined matrix), the memoryrequirement is reduced from 5.8 GB to 0.25 GB.3.2.3 Weighted probabilitiesWe found that the initial system produced music which wastoo heavily biased towards one “prototypical” measure ofrhythms for each composer. We therefore multiplied eachmatrix by its own weighting factor, and allowed the user tospecify and experiment with their own desired weights.4. EXAMPLES AND DISCUSSIONTo illustrate aspects of the generated music, we created afew cover versions of “PROLOGUE” (RWC-MDB-P-2001No.7) from the RWC Music Database [12], with a varietyof weights to the probability distributions. Short excerptsof the beginning of “PROLOGUE” are shown in Figure 10with four variations: no corpus analysis, no audio spectralanalysis, equal weights, and a set of custom weights.Figures 10a and 10b clearly demonstrate the usefulnessof combining audio with score analysis. Figure 10a doesnot use any corpus information (the weights ofGR,TR,TA, andTEare set to 0), and produces music which isnot idiomatic and is extremely difﬁcult to perform. In theother extreme, Figure 10b uses the full Haydn string quartetcorpus analysis, but does not use any spectral informationProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 117(the weights ofAOandAMare set to 0), and producesmusic which is playable but very repetitive and “boring”:Other than measure 10 (the transition from the introductionto the melody), each instrument in the accompaniment playsthe same rhythm in every measure (with the exception ofthe cello in measure 5), and 76% of measures contain asingle pitch while 24% of measures contains two pitches.Figure 10c uses all available data with weights of 1,and the music is both quite playable and more interestingthan Figure 10b. There is more variation in the rhythms,and most notes are typical classical-style durations suchas whole notes, half notes, or quarter notes. There area few non-chord pitch changes (e.g., violin 2 measure 3,viola measure 13), but not many. This version contains onemistake: the viola in measure 13 begins with a C\\16thnotewhich quickly changes to aC]chord-tone. This could beavoided by decreasing the probability of non-chord tones,but doing so would also decrease the chance of a non-chordtone in the original song from being reproduced. This is anillustration of the choices available to the user.Figures 10d (Haydn), 10e (Mozart), and 10f (Beethoven)demonstrate a custom set of weights. After some exper-imentation, we (subjectively) chose to set the onsetAOweight to 0.9, the corpus rhythmsGRandTRweights to0.5, and the corpus pitch transitionTAandTEweights to0.25. These three cover versions produce noticeably distinctmusic, arising solely due to the corpus used. The overalldistribution of rhythmic durations seems natural: the cellohas longer notes than the inner two voices. The distributionof pitches is reasonable, with all instruments playing in acomfortable range; the corpus clearly helps in avoiding theextreme pitches that were present in Figure 10a.A few parts of the cover versions are the same in allcompositions. Measure 10 always ends with a G]-C\\(alter-natively “spelled” as B]) in the cello and violin 2, with theviola ﬁlling in a transition from D]to C\\(or B]); this makesa nice V–I chord sequence (G]major to C]major) leadinginto measure 11. In addition, the V–I resolution in mea-sures 10–11 always includes contrary motion in the celloand violin 2. Our probabilistic generation does not takerelative motion of multiple voices into account, so this nicevoice leading must arise from the strength of its presence inthe audio spectral analysis.A few problems exist in the voice leading. For example,Figure 10d shows a number of parallel ﬁfths (e.g., violaand cello, measures4!5!6,8!9). These likely arisedue to the 2ndand 3rdharmonics of bass guitar notes in theoriginal recording. A similar problem occurs with suddenjumps of an octave after one 16thor 8thnote appears in a fewplaces (e.g., viola measure 4 and cello measure 12). Thesealso likely arise due to inaccuracies in the spectral analysis:the energy in upper partials of a single note can vary, somultiple onsets are detected in close succession. Moreadvanced signal processing in terms of onset estimation orpitch salience calculation could mitigate this issue. Anotherﬁx for the parallel ﬁfths would be to use a more advancedmathematical model; a ﬁrst-order Markov model does nottrack the inter-dependence between quartet parts.00.5100.020.040.060.08weight ofRG,TRx-corr.Violin 2 Rhythms00.5100.020.040.060.08weight ofRG,TRx-corr.Viola Rhythms00.5100.020.040.060.08weight ofRG,TRx-corr.Cello Rhythms\n00.5100.10.20.30.4weight ofTA,TEx-corr.Violin 2 Pitches00.5100.10.20.30.4weight ofTA,TEx-corr.Viola Pitches00.5100.10.20.30.4weight ofTA,TEx-corr.Cello Pitches\nHaydnMozartBeethovenFigure 9: Objective analysis of weights; unless otherwisespeciﬁed, our custom weights are used.4.1 Objective analysisFigure 9 shows the effect of changing the rhythmic orpitch corpus weights. The “pitch” plots show the cross-correlation between the corpus relative pitch distributionTEand the relative pitches calculated from the generatedscores. The “rhythm” plots show the cross-correlation be-tween corpus and generated scores, based on the types ofmeasures appearing in the output. Concretely, we con-struct a dictionary of full-measure rhythmic events (suchas 0222133002130011 from Figure 4) along with their fre-quency of appearance, for both the corpus and the gener-ated music. We then calculate the cross-correlation betweenthose dictionaries for the corpus and each cover version.Increasing the weight generally increases the correla-tion between corpus and generated music for both pitchesand rhythms. One counter-example is violin 2 and violain Mozart quartets. We theorize that this arises because in-creasing the rhythmic weight reduces the number of “com-pletely eighth note” measures in the generated music, how-ever such measures are very common in the original corpus.5. CONCLUSION AND FUTURE WORKWe presented Song2Quartet, a system for generating stringquartet cover versions of popular music using audio andsymbolic corpus analysis. Both the target pop song audioﬁle and the corpus of classical music contribute to the out-put; using only one or the other produces clearly inferiorresults. In order to avoid awkward second violin parts, weperformed a semi-global optimization whereby we createdthe second violin and viola parts at the same time.The current system makes a number of ad hoc assump-tions, such as the melody always being played by the ﬁrstviolin and all rhythms ﬁtting into 16th-note rhythms. Ourevaluation was primarily based on informal listening, whichshowed promise despite some voice leading errors.We plan to extend the data-driven corpus analysis so thatusers may generate cover versions for other groups of clas-sical instruments. We also plan to add a GUI so that userscan place the melody in different instruments at any pointin the song. Finally, we would like to include evaluationsof the generated scores’ “playability” by musicians.Acknowledgments:This work was supported in part byCREST, JST.118 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 \n/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up\n/brackettips.downvlcvlavln-2vln-1/dots.dot/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s0/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/flags.u3/flags.d4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/flags.u3/dots.dot/dots.dot/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/dots.dot/noteheads.s1/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G8/brackettips.up/brackettips.down/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/accidentals.rightparen /accidentals.leftparen/accidentals.sharp/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/flags.u4/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/dots.dot(a) “PROLOGUE” with audio analysis butno string quartet corpora. /noteheads.s2 /accidentals.rightparen /accidentals.leftparen/accidentals.sharp/noteheads.s2/rests.2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/accidentals.natural/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/accidentals.natural/dots.dot/noteheads.s2/accidentals.natural/dots.dot/flags.u3/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/noteheads.s2/dots.dot/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.0/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/noteheads.s1/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up/brackettips.downvlcvlavln-2vln-1/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s1/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s2/timesig.C44/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2(b) “PROLOGUE” withHaydnstring quartets butno audio spectral analysis. /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/flags.u3/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/flags.u3/dots.dot/dots.dot/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/flags.u3/noteheads.s1/noteheads.s2/noteheads.s1/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/flags.u3/rests.2/noteheads.s1/noteheads.s2/noteheads.s1/flags.u3/noteheads.s1/dots.dot/flags.u3/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.1/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/flags.u3/noteheads.s2/noteheads.s2/accidentals.sharp/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/flags.u4/accidentals.natural/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/flags.u3/noteheads.s2/noteheads.s0/noteheads.s0/noteheads.s1/rests.0/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s0/noteheads.s1/rests.0/noteheads.s1/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up/brackettips.downvlcvlavln-2vln-1/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s0/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s0/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/dots.dot/dots.dot/noteheads.s1/noteheads.s1/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/rests.0/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/rests.0/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.0/noteheads.s1/noteheads.s1/noteheads.s1/dots.dot/rests.0/noteheads.s0/noteheads.s2/noteheads.s1(c) “PROLOGUE” withHaydnstring quartets andall weights set to 1.0. /noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/flags.u3/rests.0/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/flags.d3/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/accidentals.natural/flags.u3/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/flags.u3/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s1/noteheads.s2/rests.1/noteheads.s2/dots.dot/rests.2/noteheads.s1/flags.u3/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/dots.dot/rests.3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/flags.u3/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/flags.d4/noteheads.s2/noteheads.s1/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s1/flags.u3/noteheads.s0/noteheads.s0/noteheads.s2/rests.0/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s1/rests.0/clefs.G/noteheads.s1/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up/brackettips.downvlcvlavln-2vln-1/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s0/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s0/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/flags.d3/noteheads.s1/noteheads.s2/noteheads.s2/rests.0/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/dots.dot/flags.u3/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2(d) “PROLOGUE” withHaydnstring quartets andcustom weights. /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/rests.2/flags.u3/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/flags.u3/dots.dot/noteheads.s1/dots.dot/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.natural/accidentals.natural/flags.d3/noteheads.s1/noteheads.s2/noteheads.s2/flags.d3/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/rests.1/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/flags.u3/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/rests.2/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/flags.u3/flags.u3/flags.u3/noteheads.s2/noteheads.s2/dots.dot/rests.3/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s1/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/flags.u4/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2/flags.d3/noteheads.s2/dots.dot/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up/brackettips.downvlcvlavln-2vln-1/dots.dot/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/rests.0/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/flags.u3/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/flags.u3/noteheads.s2/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/noteheads.s2(e) “PROLOGUE” withMozartstring quartets andcustom weights. /noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/rests.2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/rests.0/noteheads.s2/dots.dot/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/flags.d3/accidentals.natural/noteheads.s2/accidentals.natural/accidentals.natural/noteheads.s2/noteheads.s2/accidentals.natural/dots.dot/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3/dots.dot/noteheads.s2/rests.1/dots.dot/rests.2/noteheads.s1/noteheads.s1/noteheads.s2/flags.u3/noteheads.s1/noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/dots.dot/rests.3/dots.dot/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/flags.u3/flags.d4/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/flags.u3/noteheads.s0/noteheads.s2/noteheads.s2/rests.0/dots.dot/noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/flags.u3/noteheads.s0/noteheads.s0/noteheads.s1/rests.0/flags.d4/dots.dot/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/rests.0/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/rests.0/brackettips.up/brackettips.downvlcvlavln-2vln-1/noteheads.s2 = 121/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.F/noteheads.s0/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.C/noteheads.s2/timesig.C44/accidentals.sharp/accidentals.sharp/accidentals.sharp/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2/flags.d3/noteheads.s1/noteheads.s2/noteheads.s2/rests.0/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/rests.0/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/dots.dot/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/rests.0/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s0/noteheads.s2/noteheads.s2/rests.0/noteheads.s2(f) “PROLOGUE” withBeethovenstring quartets andcustom weights.Figure 10: Sample output; full versions and synthesized audio available at:https://staff.aist.go.jp/m.goto/Song2Quartet/Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1196. REFERENCES[1]Moray Allan and Christopher K. I. Williams. Harmonis-ing chorales by probabilistic inference. InNIPS, pages25–32, 2005.[2]Charles Ames. The markov process as a compositionalmodel: a survey and tutorial.Leonardo, pages 175–187,1989.[3]Richard Bellman.Dynamic Programming. PrincetonUniversity Press, Princeton, NJ, USA, 1 edition, 1957.[4]David Cope. Computer modeling of musical intelli-gence in EMI.Computer Music Journal, pages 69–83,1992.[5]Michael Scott Cuthbert, Christopher Ariza, and LisaFriedland. Feature extraction and machine learning onsymbolic music using the music21 toolkit. InISMIR,pages 387–392, 2011.[6]Arne Eigenfeldt and Philippe Pasquier. Considering ver-tical and horizontal context in corpus-based generativeelectronic dance music. InProceedings of the FourthInternational Conference on Computational Creativity,volume 72, 2013.[7]Benjamin Evans, Satoru Fukayama, Masataka Goto,Nagisa Munekata, and Tetsuo Ono. AutoChorusCreator:Four-Part Chorus Generator with Musical Feature Con-trol, Using Search Spaces Constructed from Rules ofMusic Theory. InInternational Computer Music Con-ference, 2014.[8]Jose D Fern´andez and Francisco Vico. Ai methods in al-gorithmic composition: A comprehensive survey.Jour-nal of Artiﬁcial Intelligence Research, pages 513–582,2013.[9]Derry Fitzgerald. Harmonic/Percussive Separation us-ing Median Filtering. InInternational Conference onDigital Audio Effects, 2010.[10]Satoru Fukayama and Masataka Goto. Chord-sequence-factory: A chord arrangement system modifying fac-torized chord sequence probabilities. InISMIR, pages457–462, 2013.[11]Brian R Glasberg and Brian CJ Moore. Derivation ofauditory ﬁlter shapes from notched-noise data.Hearingresearch, 47(1):103–138, 1990.[12]Masataka Goto, Hiroki Hashiguchi, TakuichiNishimura, and Ryuichi Oka. RWC Music Database:Popular, Classical and Jazz Music Databases. InISMIR,volume 2, pages 287–288, 2002.[13]Masataka Goto, Kazuyoshi Yoshii, Hiromasa Fujihara,Matthias Mauch, and Tomoyasu Nakano. Songle: Aweb service for active music listening improved by usercontributions. InISMIR, pages 311–316, 2011.[14]Gen Hori, Hirokazu Kameoka, and Shigeki Sagayama.Input-Output HMM Applied to Automatic Arrange-ment for Guitars.Journal of Information Processing,21(2):264–271, 2013.[15]Francois Pachet. The continuator: Musical interactionwith style.Journal of New Music Research, 32(3):333–341, 2003.[16]Francois Pachet and Pierre Roy. Musical harmonizationwith constraints: A survey.Constraints, 01/2001(6):7–19, 2001.[17]Abraham. Savitzky and M. J. E. Golay. Smoothing anddifferentiation of data by simpliﬁed least squares proce-dures.Analytical Chemistry, 36(8):1627–1639, 1964.[18]Christian Sch¨orkhuber, Anssi Klapuri, Nicki Holighaus,and Monika D¨orﬂer. A matlab toolbox for efﬁcientperfect reconstruction time-frequency transforms withlog-frequency resolution. InAudio Engineering SocietyConference: 53rd International Conference: SemanticAudio. Audio Engineering Society, 2014.120 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "AcousticBrainz: A Community Platform for Gathering Music Information Obtained from Audio.",
        "author": [
            "Alastair Porter",
            "Dmitry Bogdanov",
            "Robert Kaye",
            "Roman Tsukanov",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414938",
        "url": "https://doi.org/10.5281/zenodo.1414938",
        "ee": "https://zenodo.org/records/1414938/files/PorterBKTS15.pdf",
        "abstract": "We introduce the AcousticBrainz project, an open plat- form for gathering music information. At its core, Acous- ticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Mu- sic Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis re- sults to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to var- ious sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classi- fiers aimed at adding musically relevant semantic informa- tion. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be ac- cessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this commu- nity that will define the actual uses and applications of its data.",
        "zenodo_id": 1414938,
        "dblp_key": "conf/ismir/PorterBKTS15",
        "keywords": [
            "AcousticBrainz",
            "open platform",
            "music information retrieval",
            "audio recordings",
            "MusicBrainz identifier",
            "MusicBrainz",
            "feature extractor",
            "Essentia audio analysis library",
            "classifiers",
            "community contribution"
        ],
        "content": "ACOUSTICBRAINZ: A COMMUNITY PLATFORM FOR GATHERINGMUSIC INFORMATION OBTAINED FROM AUDIOAlastair Porter†‡, Dmitry Bogdanov†, Robert Kaye‡, Roman Tsukanov‡, Xavier Serra††Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain‡MetaBrainz Foundationalastair.porter,dmitry.bogdanov,xavier.serra@upf.edurob,roman@metabrainz.orgABSTRACTWe introduce the AcousticBrainz project, an open plat-form for gathering music information. At its core, Acous-ticBrainz is a database of music descriptors computed fromaudio recordings using a number of state-of-the-art Mu-sic Information Retrieval algorithms. Users run a suppliedfeature extractor on audio ﬁles and upload the analysis re-sults to the AcousticBrainz server. All submissions includea MusicBrainz identiﬁer allowing them to be linked to var-ious sources of editorial information. The feature extractoris based on the open source Essentia audio analysis library.From the data submitted by the community, we run classi-ﬁers aimed at adding musically relevant semantic informa-tion. These classiﬁers can be developed by the communityusing tools available on the AcousticBrainz website. Alldata in AcousticBrainz is freely available and can be ac-cessed through the website or API. For AcousticBrainz tobe successful we need to have an active community thatcontributes to and uses this platform, and it is this commu-nity that will deﬁne the actual uses and applications of itsdata.1. INTRODUCTIONOne of the biggest bottlenecks in many Music InformationRetrieval (MIR) tasks is the access to large amounts ofmusic data, in particular to audio features extracted fromcommercial music recordings. Most approaches to taskssuch as music classiﬁcation, auto-tagging, music similar-ity and music recommendation, are based on using audiofeatures obtained from well-established audio signal pro-cessing algorithms. This is a time consuming process thatis beyond the possibilities of any individual researcher. Itmay not be possible for researchers to gather this muchinformation, annotate it according to their needs, or com-pute the required features at the scale required for the task.c\u0000Alastair Porter, Dmitry Bogdanov, Robert Kaye, RomanTsukanov, Xavier Serra.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Alastair Porter, Dmitry Bogdanov,Robert Kaye, Roman Tsukanov, Xavier Serra. “AcousticBrainz: a com-munity platform for gathering music information obtained from audio”,16th International Society for Music Information Retrieval Conference,2015.For example, existing datasets for genre classiﬁcation areof insufﬁcient size with respect to both the number of in-stances per class and the ability of these instances to ac-curately represent the entire musical genre space [4]. Alist of datasets commonly used in MIR is provided in [1].Half of them have fewer than 10,000 instances, althoughin recent years there have been attempts to create largerdatasets. Building such datasets would allow research atthe scale of the requirements of commercial applications.In general however, the creation of datasets may be dif-ﬁcult for researchers due to a number of reasons:•Gathering and sharing datasets require legal considera-tions with regard to the distribution of copyrighted ma-terial [7].•Collections which are hand-created may be biased intheir contents and annotations, especially if they arecreated by only one person, or if they are created forthe evaluation of a speciﬁc task or algorithm (such asthe GZTAN dataset, commonly used to evaluate audiofeature-based genre classiﬁcation algorithms [10]).One project in recent years to address some of these is-sues is the Million Song Dataset (MSD) [1]. At the timeof its release, this was the largest dataset of music descrip-tors in the MIR community and has gained a lot of atten-tion for its size and breadth of music content, as well asthe simplicity of accessing its data. The MSD relies onthe EchoNest API1to compute its descriptors, a commer-cial product which is closed to academic inspection. Somedownsides of this approach include:•Implementation details of the algorithms used to com-pute the descriptors are unknown and it is impossible toreview the quality of their implementation.•The dataset is ﬁxed in time, and does not appear to havebeen updated with new features, or music released sinceit was created.The MSD has been further expanded with features com-puted using open source algorithms, on audio samplesfrom 7digital.com [9]. As this dataset reﬂects the MSD,it is also ﬁxed in time, and features do not represent thewhole recording, but only the sample.1http://developer.echonest.com786Based on these considerations, we believe that there isstill space for a large dynamic dataset consisting of musicfeatures calculated with open algorithms.2. ACOUSTICBRAINZWe are introducing a new platform, AcousticBrainz,2toassist with the gathering of musical data from the mu-sic enthusiast and research community, and to provide re-searchers with large datasets of recordings to work with.All of the source code in AcousticBrainz is open,3en-couraging sharing of algorithms between contributors andproviding the ability for people to improve on the work ofothers. All submitted and generated data is freely availableunder a Creative Commons CC-0 license (public domain).The platform is split into three categories: feature ex-traction, data storage, and the creation of musical semanticinformation. A feature extractor, based on algorithms inthe Essentia audio analysis library [2], can be downloadedby anyone who wishes to contribute data to the project.They run this extractor on their personal computer, giv-ing audio ﬁles as input. The output of this extractor is aJSON ﬁle for each audio track containing descriptors (seeSection 2.3.3). A submission tool provided with the ex-tractor automatically uploads the JSON ﬁles to the Acous-ticBrainz server.A database stores submissions and makes the data avail-able via an API. AcousticBrainz only stores descriptorsof audio, and never the actual audio itself. Submissionsare identiﬁed by the MusicBrainz identiﬁer (MBID) of theinput audio ﬁle. These stable identiﬁers let us uniquelyand unambiguously refer to a music recording, and canalso let us obtain additional editorial information fromMusicBrainz and from other services that also understandMBIDs.To encourage experimentation with the data, the Acous-ticBrainz website lets anybody create, annotate, and sharetheir own datasets consisting of recordings present in thedatabase. A search interface lets users query for recordingsbased on editorial data from MusicBrainz or extracted fea-tures and add the results to the dataset. From these datasetsusers can build classiﬁer models which can be used to es-timate characteristics of any recording present in Acous-ticBrainz.2.1 MusicBrainzMusicBrainz4is a community-maintained open encyclo-pedia of music information. It contains editorial metadatafor many musical concepts, including Artists (individuals,groups, and other people associated with musical events),Releases, Recordings, and Works. It also contains relation-ships between items, and to other external databases. Datais entered manually by a large community of volunteers(editors), who also vote on changes made by other editorsto ensure its quality. It is used by a number of commercial2http://acousticbrainz.org3https://github.com/metabrainz/acousticbrainz-server4https://musicbrainz.orgcompanies.5Every item in the database is uniquely iden-tiﬁed by an MBID and many companies and organizationsrely on these IDs as identiﬁers for music-related concepts.MBIDs can be used to retrieve data from external serviceswhich understand them (e.g., Last.fm, WikiData), and arealso a part of the Music Ontology.2.2 Current submission statistics\nFigure 1: Release years of submissions from ﬁle metadata.Format Countmp3 1,784,778ﬂac 777,826vorbis 83,867aac 64,733alac 29,481wmav2 4,019other 1,320Table 1: Number of submissions per audio codec.At this time,6the AcousticBrainz database has audiofeatures submitted for 1,671,701 unique recording MBIDs.We keep duplicate submissions from different sources, re-sulting in a total of 2,747,094 submissions. For thesesubmissions we also have metadata information availablefrom MusicBrainz, including 99,159 artists and 165,394releases. The duplicates consist of analyzed features ofvarious source audio ﬁles, with differing codecs, encoders,and bit rates. These duplicates let us see real-world ex-amples of the effect of different codecs and encoding pa-rameters on our descriptors. We have collected submissionfor 538,614 unique MBIDs (807,307 including duplicates)for audio ﬁles encoded using a lossless codec (FLAC andALAC), which is in itself is a large database. The mostcommon audio format for submitted ﬁles is MP3, withmore submissions than for all other formats combined (Ta-ble 1). 94% of submitted ﬁles contain year metadata. Weshow a histogram of the year that submissions were re-leased in Figure 1. The majority of tracks are from the5https://metabrainz.org/customers6July 7 2015Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7871990s and ﬁrst decade of the 2000s. As tracks submit-ted to AcousticBrainz require a MBID this distributionmay also be reﬂective of the content in the MusicBrainzdatabase. The current size of the database (containing allJSON ﬁle submissions) is approximately 118GB, split be-tween 102GB of low-level data (average ﬁle size 40kB),and 12GB of high-level (ﬁle size 4kB).Tag CountRock 195,837Pop 103,486Classical 90,231Jazz 88,702Soundtrack 79,056Electronic 71,758Metal 44,961Other 42,706Country 40,078Alternative 35,900Alternative Rock 35,525Folk 32,108Unknown 29,413Punk 27,977Hip-Hop 24,083Blues 23,276Indie 21,709Classic Rock 18,417Ambient 18,074Industrial 17,816(a) Genre as reported in ﬁlemetadata.Genre %Rock 41.15Electronic 19.65Pop 7.73Jazz 6.80Country 4.42Folk 3.83Rhythm 3.61& bluesBlues 2.86Hip Hop 2.23Classical 1.81Asian 1.69Caribbean 1.63& LatinSka 0.89Avant-Garde 0.47Easy Listening 0.45Comedy 0.44African 0.28Other 0.09(b) Percentages of broadgenre categories.Table 2: Genre statistics.We ﬁnd genre metadata present for 1,908,251 submis-sions. The top 20 genre annotations account for 52.9% ofthe tags used in this subset. We show the list of these gen-res and their counts in Table 2 (a). We also compute per-centages over 691,431 recordings (41.4% of total record-ings in AcousticBrainz) annotated by genre using Last.fmtags and shown in Table 2 (b). To ﬁnd these broad genrelabels we look up a recording by its MBID and if this fails,by the artist and track title. Last.fm tags are ranked by themost commonly applied tag. We match highly ranking tagsto popular music genres found in beets, a tool for identify-ing, tagging, and renaming audio ﬁles,7. If a match occursas a more speciﬁc subgenre, we report it as this subgenre’sparent genre. While this process is lossy (we don’t matchtags which are misspelled) and subjective (not everyoneagrees on genres or subgenres), we believe it nonethelessgives a good overview of the contents of the database.2.3 ArchitectureThe architecture of AcousticBrainz is presented in Fig-ure 2. The community uses the feature extractor and sub-mission tools to send music features extracted from audioto the server. The server stores this data (which we call“low-level” data) in a database and makes it available tothe rest of the community. The community can also pro-vide classiﬁer models (designed using the tools we pro-vide), for inferring information from this data (which we7https://github.com/sampsyo/beets/blob/0c7823/beetsplug/lastgenre/genres-tree.yamlcall “high-level” data). The high-level data is computedon the server without needing to access audio ﬁles. Thecommunity can moderate the models and the good onesare used to compute high-level data for all AcousticBrainzsubmissions.2.3.1 Feature extractor and submission toolWe have created a music feature extractor using the Essen-tia library.8We use this library for computing features be-cause it has been successfully used in a number of similaraudio analysis applications, such as Freesound, and othercommercial systems. We describe the features computedby the extractor in more detail in Section 2.3.3. We dis-tribute this extractor, written in C++, through our website9as a static binary for Windows, OSX, and Linux. We use astatic binary because it lets us include the same version ofall of our dependencies across all platforms.The feature extractor runs at about 20⇥real time, thatis, a ﬁle with length 3 minutes takes 9–10 seconds to run(on an Intel i5 3.30GHz machine).We have two clients to help the community computefeatures on their audio ﬁles and submit them to the Acous-ticBrainz server—A command-line tool written in Python,and a graphical interface written in C++ with QT. Theseclients automatically search for all audio ﬁles in a direc-tory, compute their features, and send JSON ﬁles contain-ing the features to the server using its API.The submission tool only submits data which have beenpreviously tagged with MBIDs. Software exists to matchaudio ﬁles on disk to Releases on MusicBrainz based ontrack lengths, ﬁle names, existing tags, and audio ﬁnger-printing. It is possible that audio ﬁles will be tagged in-correctly, either due to user error or incorrect ﬁngerprintmatching, however we believe this to account for only asmall amount of data submitted.Each JSON ﬁle contains metadata identifying the ver-sion of the feature extractor used, including informationabout the exact version the source code (git commit hash)and also an increasing version number which we willchange as we make incompatible changes to features in thefuture.2.3.2 ServerThe features of submitted tracks are stored as JSON in aPostgreSQL database. The server interface is written inPython using the Flask web application framework. AnAPI accepts requests from clients, ﬁlters exact duplicates(where the feature extractor outputs exactly the same con-tent for two concurrent runs on the same ﬁle), and storesthe results in the database. For privacy reasons, the serverstores no identifying information about submitters.Every 30 seconds the server starts a process to searchfor recent submissions. For these documents the serverruns a feature extractor to obtain high-level descriptors forthese ﬁles (Section 2.3.4). Once the high level computa-8http://essentia.upf.edu9http://acousticbrainz.org/download788 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2: AcousticBrainz architecture.tion process is complete, all of the data about the submittedtrack is made available to the community.Once extracted features are made available we store allof the computed data and metadata collected from Mu-sicBrainz in an ElasticSearch search server. This searchsystem lets users perform queries such as ﬁnding allrecordings with a particular attribute or attribute range(e.g., with a BPM between 110 and 120, or an estimatedgenre of jazz), or by ﬁltering by some known metadata(such as all recordings by a particular artist).All of the submitted and computed information is madeavailable via the AcousticBrainz website and API. Thewebsite has a page for each submitted recording, outlin-ing metadata, providing an overview of the low-level andhigh-level data, and linking to external sources, includinga player to listen to the song if it is available on a publicstreaming service. The API gives access to the JSON doc-uments that make up the low-level and high-level data, andaccess to the search interface. Documents are identiﬁed bytheir MBID. Groups of documents, for example all record-ings in an album, can be downloaded by ﬁrst getting thelist of MBIDs from MusicBrainz.2.3.3 Low-level music dataOur feature extractor computes spectral, time-domain,rhythm, and tonal descriptors. They include features char-acterizing overall loudness, dynamics, and spectral shapeof the signal, rhythm descriptors (including beat positionsand BPM value), and tonal information (including chromafeatures, keys and scales). All descriptors are analyzedon a signal resampled to 44.1kHz sampling rate, summedto mono and normalized using replay gain. Many of thedescriptors are computed across frames and are thereforesummarized by their statistical distribution (we currentlydo not provide per-frame information). More detailed in-formation about the low-level data, including references tothe employed MIR and audio analysis algorithms, is pro-vided in the ofﬁcial documentation for Essentia,10or byreviewing the code.11An example of the output of the fea-ture extractor can be seen on AcousticBrainz website.12We provide a list of music descriptors computed by thefeature extractor and currently present in AcousticBrainzin Table 3.2.3.4 High-level music dataLow-level data submitted by the users opens possibili-ties to apply data mining and machine learning techniquesacross the whole AcousticBrainz collection, or subsets,without needing access to audio ﬁles. In particular thesetechniques may allow us to infer semantic annotation ofmusic in terms of concepts used by people when describingmusic (e.g., genres, styles, moods, uses of music, instru-mentation, etc.) Currently, AcousticBrainz provides toolsfor creating datasets to represent these types of conceptsand train classiﬁer models (see Section 3). The trainingprocess is done automatically using SVM classiﬁers (C-SVC with polynomial or RBF kernels). A training scriptﬁnds optimal data preprocessing and SVM parameteriza-tion given a ground-truth dataset of low-level data in agrid search using 5-fold cross-validation. The details onthe considered parameters can be found in the classiﬁca-tion project template in the source code.13After modera-tion the resulting high-level data can be computed from thelow-level data in the AcousticBrainz database.Our current high-level data includes estimations doneby classiﬁers pre-trained using a number of annotated col-10http://essentia.upf.edu/documentation/streaming_extractor_music.html11https://github.com/MTG/essentia/tree/master/src/examples12http://acousticbrainz.org/data13https://github.com/MTG/gaia/tree/master/src/bindings/pygaia/scripts/classificationProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 789low-level.* rhythm.* tonal.*average loudness, dynamic complexity,silence rate 20dB / 30dB / 60dB,spectral centroid / kurtosis / spread / skewness / rolloff / decrease,hfc, spectral strongpeak, zerocrossingrate,spectral rms, spectral ﬂux, spectral energy,spectral energyband low / middle low / middle high / high,barkbands, melbands, erbbands, mfcc, gfcc,barkbands crest / ﬂatness db / kurtosis / skewness / spread,melbands crest / ﬂatness db / kurtosis / skewness / spread,erbbands crest / ﬂatness db / kurtosis / skewness / spread,dissonance, spectral entropy, pitch salience, spectral complexity,spectral contrast coeffs / valleysbeats position, beats count,bpm,bpm histogram ﬁrst peak bpm /spread / weight,bpm histogram second peak bpm /spread / weight,beats loudness,beats loudness band ratio,onset rate,danceabilitytuning frequency,hpcp, thpcp,hpcp entropy,key key, key scale, key strength,chords strength,chords histogram,chords changes rate, chords numberrate,chords key, chords scale,tuning diatonic strength,tuning equal tempered deviation,tuning nontempered energy ratioTable 3: Descriptors extracted by Essentia’s music extractor v1.0 currently present in AcousticBrainz. The descriptors aregrouped according to the namespaces within the music extractor’s output.Name Source Type Sizegenre dortmund Music Audio Benchmark Data Set [5] Genre 1886 track excerpts, 46-490 per genregenre rosamerica In-house [4] Genre 400 tracks, 50 per genregenre tzanetakis GTZAN Genre Collection [11] Genre 1000 track excerpts, 100 per genregenre electronic In-house Electronic music subgenres 250 track excerpts, 50 per genremood acoustic In-house [8] Sound (acoustic, non-acoustic) 321 full tracks + excerpts, 193/128 per classmood electronic In-house [8] Sound (electronic, non-electronic) 332 full tracks + excerpts, 164/168 per classtimbre In-house Timbre colour (dark, bright) 3000 track excerpts, 1500 per classtonal atonal In-house Tonality (tonal/atonal) 345 track excerpts, 200/145danceability In-house Danceability 306 full tracks, 124/182 per classismir04 rhythm ISMIR2004 Rhythm Classiﬁcation Dataset [3] Ballroom music dance styles 683 track excerpts, 60-110 per classvoice instrumental In-house Voice/instrumental music 1000 track excerpts, 500 per classgender In-house Gender in vocal music (male/female) 3311 full tracks, 1508/1803 per classmood happy In-house [8] Mood (happy, non-happy) 302 full tracks + excerpts, 139/163 per classmood sad In-house [8] Mood (sad, non-sad) 230 full tracks + excerpts, 96/134 per classmood aggressive In-house [8] Mood (aggressive, non-aggressive) 280 full tracks + excerpts, 133/147 per classmood relaxed In-house [8] Mood (relaxed, non-relaxed) 446 full tracks + excerpts, 145/301 per classmood party In-house [8] Mood (party, non-party) 349 full tracks + excerpts, 198/151 per classmoods mirex MIREX Audio Mood Classiﬁcation Dataset [6] Mood (5 clusters) 269 track excerpts, 60-110 per classTable 4: Music collections used for training high-level classiﬁer models currently included in AcousticBrainz.lections, some of which are commonly used in MIR (Ta-ble 4). These datasets pre-date the AcousticBrainz plat-form and so some of them are not yet open to inspection.We anticipate that the community can help to build bet-ter classiﬁers using the low-level data already submitted toAcousticBrainz.The evaluation metrics obtained from training our cur-rent models14show promising results. However, the accu-racy and reliability of our current high-level data is underdoubt, as little research on the portability of such mod-els to the large scale has been done within MIR. We seethe design of new classiﬁer models using AcousticBrainzdata as an attractive challenge for MIR researchers and weanticipate AcousticBrainz to become a platform for build-ing classiﬁers on larger collections created and annotatedby the community using the tools we provide (see Sec-tion 3). The high-level data within AcousticBrainz willbe constantly updated using the improved classiﬁer mod-els proposed by the community.3. BUILDING ANNOTATED DATASETSWe have developed an interface which lets users createdatasets, comprised of a name, a list of classes, and a list14http://acousticbrainz.org/dataof instances for each class. These instances refer to record-ings in the AcousticBrainz database, and so are referredto by MBID. MBIDs can be chosen manually, or addedas the result of a search query for all recordings matchinggiven criteria. To assist in the inspection of datasets, meta-data of these recordings from MusicBrainz is also shown.Users can create these datasets individually or collaboratetogether to suggest classes, class boundaries, and content.We currently limit our interest to classiﬁcation problems,though we see future value in allowing users to create otherkinds of annotated datasets such as collections of singulartypes of data (e.g., music from a speciﬁc culture), user-deﬁned lists of recordings, or sets of recordings with afreeform annotations including tags.Once a dataset has been created, a user can choose togenerate a model representing the dataset. This model istrained using the same training script used to generate ourexisting models (Section 2.3.4). We report to the user theaccuracy of the model, giving them the chance to share theresults with the wider community, or continue improvingthe model (Figure 3).Once a model has been created and approved by thecommunity we can choose to process all existing low-leveldata with this model in order to make these new estima-tions available for the community. We are able to computehigh-level data at a rate of about 1000/minute using a sin-790 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 3: Results of a classiﬁcation. The user can choose to continue working on improving the classiﬁer accuracy, orsubmit it to the community.gle core, and so anticipate that recomputing the dataset atits current size will only take a few days. As the datasetgrows the task can be parallelized over many machines.4. CHALLENGES AND FUTURE WORKTo keep our low-level data at the level of the state of the artin MIR, we will continue to release updates to the featureextractor, and we also encourage participation in this pro-cess. Because we rely on the good will of the community torun this extractor on their audio collections we face a trade-off between the frequency of updates and their willingnessto run the extractor. We anticipate that we could releasean update once or twice a year, increasing the number andquality of the features. Our high-level data will also be un-der constant improvement. We hope that the system thatwe have developed will foster collaboration to build betterannotations of musically useful concepts.Other datasets, such as the MSD contain more detailedfeatures than those which we compute for our low-leveldata. The continual testing and improvement and integra-tion of new algorithms will allow us to close this gap offeature content. Since we rely on contributions by the com-munity, we may be missing some popular music. Continu-ing to solicit requests will ensure we have as broad a cover-age as possible. While soliciting audio features we have toensure that incorrect submissions are not made, either ma-liciously or due to incorrect metadata. We are developinga technique to determine if two submissions are identicalbased on their features.Updating the feature extractor and classiﬁer models im-plies compatibility problems with our data. As our submit-ted data includes information about the version of the ex-tractor used to compute it, we can determine if two piecesof data computed by different versions of the feature ex-tractor are compatible. We are compiling a dedicated audiocollection to perform tests with different extractor versionsand estimate the differences in feature values. These testscan also help us to assess the robustness of music featurespresent in low-level data, the identiﬁcation of which is achallenging task [12]. To take advantage of as much dataas possible, we will not discard old submissions from ourdatabase when a new extractors are released. High-leveldata will be updated with respect to low-level data whenpossible. Improvements to the collection creation interfaceon the AcousticBrainz website will let us build datasets touse with other machine learning techniques.We expect that the data provided by AcousticBrainz willbe useful to both the MIR community and others inter-ested in this type of data. In exchange we need the Acous-ticBrainz community to help in expanding the dataset andimproving its quality. The interest in our platform becameapparent directly after its launch when we were able to ob-tain features for 500,000 ﬁles in less than 3 weeks, buildingup to over 2.7 million submissions. The continued supportof providing features and collaborating on data collectionprojects will ensure the success of this project.5. ACKNOWLEDGEMENTSThis research has been partially funded by the CompMusic(ERC 267583) and SIGMUS (TIN2012-36650) projectsThe authors would like to thank the entire MusicBrainzcommunity and the members of the AcousticBrainz com-munity who helped to test the feature extractor and con-tribute data to the project.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 7916. REFERENCES[1]T. Bertin-Mahieux, D. PW Ellis, B. Whitman, andP. Lamere. The million song dataset. InProceedingsof the International Society for Music Information Re-trieval Conference, pages 591–6, 2011.[2]D. Bogdanov, N. Wack, E. G´omez, S. Gulati, P. Her-rera, O. Mayor, G. Roma, J. Salamon, J.R. Zapata, andX. Serra. Essentia: An audio analysis library for mu-sic information retrieval. InProceedings of the Inter-national Society for Music Information Retrieval Con-ference, pages 493–498, 2013.[3]P. Cano, E. G´omez, F. Gouyon, P. Herrera, M. Koppen-berger, B. Ong, X. Serra, S. Streich, and N. Wack. IS-MIR 2004 audio description contest. Technical report,2006. Available online: http://mtg.upf.edu/node/461.[4]E. Guaus.Audio content processing for automatic mu-sic genre classiﬁcation: descriptors, databases, andclassiﬁers. PhD thesis, Universitat Pompeu Fabra,2009.[5]H. Homburg, I. Mierswa, B. M¨oller, K. Morik, andM. Wurst. A benchmark dataset for audio classiﬁ-cation and clustering. InProceedings of the Inter-national Conference on Music Information Retrieval,pages 528–31, 2005.[6]X. Hu and J. S. Downie. Exploring mood metadata:Relationships with genre, artist and usage metadata. InProceedings of the International Conference on MusicInformation Retrieval, pages 67–72. Citeseer, 2007.[7]D. Karydi, I. Karydis, and I. Deliyannis. Legal issuesin using musical content from iTunes and YouTube formusic information retrieval. InInternational Confer-ence on Information Law, 2012.[8]C. Laurier, O. Meyers, J. Serr`a, M. Blech, and P. Her-rera. Music Mood Annotator Design and Integration. InInternational Workshop on Content-Based MultimediaIndexing, 2009.[9]A. Schindler, R. Mayer, and A. Rauber. Facilitatingcomprehensive benchmarking experiments on the mil-lion song dataset. InProceedings of the InternationalSociety for Music Information Retrieval Conference,pages 469–74, 2012.[10]B. L. Sturm. An analysis of the gtzan music genredataset. InProceedings of the second internationalACM workshop on Music information retrieval withuser-centered and multimodal strategies, pages 7–12.ACM, 2012.[11]G. Tzanetakis and P. Cook. Musical genre classiﬁca-tion of audio signals.IEEE transactions on Speech andAudio Processing, 10(5):293–302, 2002.[12]J. Urbano, D. Bogdanov, P. Herrera, E. G´omez, andX. Serra. What is the effect of audio quality on the ro-bustness of mfccs and chroma features? InProceed-ings of the International Society for Music InformationRetrieval Conference, pages 573–8, 2014.792 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Modeling Genre with the Music Genome Project: Comparing Human-Labeled Attributes and Audio Features.",
        "author": [
            "Matthew Prockup",
            "Andreas F. Ehmann",
            "Fabien Gouyon",
            "Erik M. Schmidt",
            "Òscar Celma",
            "Youngmoo E. Kim"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417523",
        "url": "https://doi.org/10.5281/zenodo.1417523",
        "ee": "https://zenodo.org/records/1417523/files/ProckupEGSCK15.pdf",
        "abstract": "Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly defined or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large ex- tent be objectively modeled via a combination of musi- cal attributes. We employ a data-driven approach utiliz- ing a subset of 48 hand-labeled musical attributes com- prising instrumentation, timbre, and rhythm across more than one million examples from Pandorar Internet Ra- dio’s Music Genome Projectr. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven mod- els derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that com- bine audio features and learned musical attributes perform better than those derived from audio features alone.",
        "zenodo_id": 1417523,
        "dblp_key": "conf/ismir/ProckupEGSCK15",
        "keywords": [
            "music",
            "genres",
            "objectively",
            "modeling",
            "musical",
            "attributes",
            "data-driven",
            "Pandora",
            "Internet",
            "Radio"
        ],
        "content": "MODELING GENRE WITH THE MUSIC GENOME PROJECT:COMPARING HUMAN-LABELED ATTRIBUTES AND AUDIO FEATURESMatthew Prockup+⇤, Andreas F. Ehmann⇤, Fabien Gouyon⇤Erik M. Schmidt⇤, Oscar Celma⇤, and Youngmoo E. Kim+Electrical and Computer Engineering, Drexel University+and Pandora Media Inc.⇤{mprockup,ykim}@drexel.edu{aehmann,fgouyon,eschmidt,ocelma}@pandora.comABSTRACTGenre provides one of the most convenient categorizationsof music, but it is often regarded as a poorly deﬁned orlargely subjective musical construct. In this work, weprovide evidence that musical genres can to a large ex-tent be objectively modeled via a combination of musi-cal attributes. We employ a data-driven approach utiliz-ing a subset of 48 hand-labeled musical attributes com-prising instrumentation, timbre, and rhythm across morethan one million examples from PandorarInternet Ra-dio’sMusic Genome Projectr. A set of audio featuresmotivated by timbre and rhythm are then implemented tomodel genre both directly and through audio-driven mod-els derived from the hand-labeled musical attributes. Inmost cases, machine learning models built directly fromhand-labeled attributes outperform models based on audiofeatures. Among the audio-based models, those that com-bine audio features and learned musical attributes performbetter than those derived from audio features alone.1. INTRODUCTIONMusicalgenreis a high-level label given to a piece of mu-sic (e.g., Rock, Jazz) to both associate it with similar musicpieces and distinguish it from others. Genre is a very pop-ular way to organize music as it is being used by virtuallyall actors in the music industry, from record labels and mu-sic retailers, to music consumers and musicians via radioand music streaming services on the internet.Just because genres are widely used does not necessar-ily mean that they are easy to categorize, or easy to rec-ognize. In fact, previous research shows that the music in-dustry uses inconsistent genre taxonomies [21], and thereis debate over whether genre is the product of objective orsubjective categorizations [28]. Furthermore, it is debatedwhether individual musical properties (e.g. tempo, rhythm,instrumentation), which are not always exclusive to a sin-c\u0000Matthew Prockup, Andreas F. Ehmann, Fabien GouyonErik M. Schmidt, Oscar Celma, and Youngmoo E. Kim.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Matthew Prockup, Andreas F.Ehmann, Fabien Gouyon Erik M. Schmidt, Oscar Celma, and YoungmooE. Kim. “Modeling Genre with the Music Genome Project: ComparingHuman-Labeled Attributes and Audio Features”, 16th International Soci-ety for Music Information Retrieval Conference, 2015.gle genre, represent deﬁning components [1, 10]. For ex-ample, an Afro-Latin clave pattern occurs many places,both in Antonio Carlos Jobim’sThe Girl from Ipanema(Jazz) and in The Beatles’And I Love Her(Rock). It caneven be heard in the recently popular song,All About thatBass, by Meghan Trainor. However, when discriminatingthe more speciﬁc subgenres of ‘Bebop’ Jazz (fast swing)and ‘Brazilian’ Jazz (Afro-Latin rhythms), this clave prop-erty becomes much more salient. Despite these intriguingrelationships, a large-scale analysis of the association ofmusical properties to genre, to the knowledge of the au-thors, has yet to be performed.If it were possible to deﬁne a categorization of musicgenres that is useful, meaningful, consensual and consis-tentat some level, then an automated categorization of mu-sic pieces into genres would be both achievable and highlydesirable. Since early research in Music Information Re-trieval (MIR), and still to date, the automatic genre recog-nition from music pieces has precisely been an importanttopic [1, 28, 30].In this work, we explore the intriguing relationshipof genre and musical attributes. In Section 3, we willoverview the expertly-curated data used. In Section 4,we detail an applied musicology experiment that usesexpertly-labeled musical attributes to model genre. Wethen report in Section 5 on a series of experiments regard-ing automated categorization of music pieces into genresusing audio signal analysis. In the following section, wewill brieﬂy outline each of these approaches.2. APPROACHIn this work we explore four approaches to modeling mu-sical genre, investigating both expert human annotationsas well as audio representations (Figure 1). We explorea subset of 12 ‘Basic’ musical genres (e.g. Jazz) as wellas a selected subset of 47 subgenres (e.g. Bebop). Inthe ﬁrst approach, we address via data-driven experimentswhether objective musical attributes of music pieces carrysufﬁcient information to categorize their genre. The nextset of approaches uses audio features to model genre auto-matically. In the second approach, we use audio featuresdirectly. The third approach uses audio features to modeleach of the musical attributes individually, which are thenused to model genre. In the fourth approach, the estimatedattributes are used in conjunction with raw audio features.31RT+RTRT+RTHuman AttributesAudio FeaturesTimbreAudio Feat.RhythmAudio Feat.\nRhythmAudio Feat.RhythmAttrib.Instr., Vocals, Sonority Attrib.T+REstimated AttributesRhythmAudio Feat.Estimated Attrib. & Audio Feat.\nT+RTimbreAudio Feat.TimbreAudio Feat.TimbreFeat.Instr.VocalsSonorityRhythmAttrib.RhythmFeat.RhythmAttrib.Instr., Vocals, Sonority Attrib.BA\nDCLearn GenreLearn Genre\nLearn GenreLearn GenreLearn RhythmLearn Instr.,Vocals, SonorityLearn RhythmLearn Instr.,Vocals, SonorityFigure 1. An overview of the experiments performed.By injecting human-inspired context, we hope to automat-ically capture elements of genre in a manner similar to thatof models derived from attributes labeled by music experts.3. DATA - THE MUSIC GENOME PROJECTrBoth the musical attribute and genre labels used were de-ﬁned and collected by musical experts on a corpus of overone million music pieces from PandorarInternet Radio’sMusic Genome Projectr(MGP)1. The labels were col-lected over a period of nearly 15 years and great care wasplaced in deﬁning them and analyzing each song with thatconsistent set of criteria.3.1 Musical AttributesThe musical attributes refer to speciﬁc musical compo-nents comprising elements of the vocals, instrumentation,sonority, and rhythm. They are designed to have a gen-eralized meaning across all genres (in western music) andmap to speciﬁc and deterministic musical qualities. In thiswork, we choose subset of 48 attributes (10 rhythm, 38timbre). An overview of the attributes is shown in Table 1.Meterattributes denote musical meters separate from simpleduple (e.g, cut-time, compound-duple, odd)Rhythmic Feelattributes denote rhythmic interpretation(e.g., swing, shufﬂe, back-beat strength) and elements ofrhythmic perception (e.g., syncopation, danceability)Vocalattributes denote the presence of vocals and timbralcharacteristics of voice (e.g., male, female, vocal grittiness).Instrumentationattributes denote the presence of instru-ments (e.g., piano) and their timbre (e.g., guitar distortion)Sonorityattributes describe production techniques (e.g., stu-dio, live) and the overall sound (e.g., acoustic, synthesized)Table 1. Explanations of rhythm and timbre attributes.1“Pandora” and “Music Genome Project” are registered trademarks ofPandora Media, Inc.http://www.pandora.com/about/mgpEach of the attributes is rated on continuous scale from0-1. In some contexts, it is helpful to convert them to bi-nary labels if they show only low (absence) or high (pres-ence) ratings with little in between [25].3.2 Genre and SubgenreIn this work we will explore a selected subset of 12 ‘Ba-sic’ genres and 47 additional sub-genres. ‘Basic’ genre isassembled as a mix of very expansive genres (e.g., Rock,Jazz) as well as some more focused ones (e.g., Disco andBluegrass), serving as an analog to many previous genreexperiments in MIR. The presence of a genre is notated in-dependently for each song by a binary label. A selectionof genre labels and a simplistic high-level organization fordiscussion purposes is shown in Table 2.Basic Genre: Rock, Jazz, Rap, Latin, Disco, Bluegrass, etc.Jazz Subgenre: Cool, Fusion, Hard Bop, Afro-Cuban, etc.Rock Subgenre: Light, Hard, Punk, etc.Rap Subgenre: Party, Old School, Hardcore, etc.Dance Subgenre: Trance, House, etc.World Subgenre: Cajun, North African, Indian, Celtic, etc.Table 2. Some of the musical genres and subgenres used.4. MUSICAL ATTRIBUTE MODELS OF GENREIn order to see the extent to which genre can be modeled bymusical attributes, we ﬁrst perform an applied musicologyexperiment using the set of expertly-labeled attributes fromSection 3.1 and relate them to labels of genre. A model foreach induvidual genre is trained on each of the musicalattributes alone and in rhythm- and timbre-based aggrega-tions. This will show the role that each attribute or collec-tion of attributes plays and how they interact with one an-other in order to create joint representations of genre. Eachmodel employs logistic regression trained using stochasticgradient decent (SGD) [25]. The training data was sep-arated on a randomly shufﬂed 70%:30% (train:test) splitwith no shared artists between training and testing. Dueto the size of the dataset, a single trial for each attributeis both tractable and sufﬁcient. The learning rate for eachgenre model is tuned adaptively.4.1 Evaluating the Role of Musical AttributesIn order to evaluate each of the models, the area underthe receiver operating characteristic (ROC) curve will beused. Each genre has large and varying class imbalance,so this is ﬁrst corrected for by weighting training exam-ples appropriately in the cost function. However, accuracyalone still does not tell the whole story. High accuracy canbe achieved by predicting only the negative class (genreabsence). Area under the ROC curve allows for a morecomparable difference between each of the models thanraw accuracy alone. It gives insight into the trade-off be-tween true positive and false positive rates. Alternatively32 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015we could have used precision and recall (PR) curves forevaluation, but it is shown that if one model dominates inthe ROC domain, it will also dominate in the PR domainand vice-versa [5]. In this work, the area under the ROCcurve will be referred to as AUC.The results for each of the attribute-based genre modelsare shown in Tables 3 and 4. The tables outline the AUCvalues for classifying genre using timbre attributes, rhythmattributes, and their combination. Table 3 summarizes allresults, showing the mean of all AUC values for each genremodel contained in the subgroups deﬁned in Section 3.2.Using attributes of rhythm and timbre together show bet-ter performance than using each alone. Secondly, timbretends to perform better than rhythm. This suggests thatthe timbre attributes in this context are better descriptors.However in some cases, the rhythm attributes, even thoughthere are less of them (10 rhythm, 38 timbre), are not thatfar behind. They are especially important in deﬁning Jazzand Rap, where rhythms such as swing in Jazz or synco-pated vocal cadences over back-beat heavy drums in Rapplay deﬁning roles.Genre Group Timbre Rhythm BothBasic 0.905 0.841 0.918Rock Sub 0.910 0.819 0.919Jazz Sub 0.925 0.856 0.945Rap Sub 0.901 0.891 0.940Dance Sub 0.961 0.881 0.965World Sub 0.885 0.833 0.904Mean0.913 0.848 0.931Table 3. An overview of all models using musical at-tributes.In Table 4 we show the individual AUC results forthe set of ‘Basic’ genres and subgenres of Jazz. Withinthese individual groups, rhythm and timbre attributes to-gether are once again able to better represent genre thanwhen used individually. Each of the ‘Basic’ genres can berepresented reasonably well with just timbre, as each hasslightly differing instrumentation. However, we again seethe importance of rhythm, describing what instrumentationand timbre cannot capture alone. Genres heavily reliant onspeciﬁc rhythms (e.g., Funk, Rap, Latin, Disco, Jazz) areall able to be represented rather well with only rhythm at-tributes. In the Jazz subgenre this emphasis on rhythm incertain cases is even more clear. In the next subsection, wewill dive deeper into the attributes that best describe theJazz subgenres.4.2 The Inﬂuence of Rhythm and Timbre in JazzIn order to more deeply explore the deﬁning relationshipsof rhythm and instrumentation within a subgenre, we willlook further into Jazz. Table 5 shows a subset of the im-portant musical attributes for the Jazz subgenres. The AUCaccuracy of classifying each subgenre based on individualmusical attributes is shown.The presence of solo brass (e.g, trumpet), piano, reeds(e.g., saxophone) and auxiliary percussion (e.g., congas)are important deﬁning characteristics of instrumentation.BasicJazzGenre Timbre Rhythm BothSubgenre Timbre Rhythm BothRock 0.843 0.759 0.856New Orleans 0.970 0.957 0.989Blues 0.913 0.783 0.915Boogie 0.943 0.893 0.978Gospel 0.810 0.664 0.843Swing 0.970 0.933 0.984Soul 0.869 0.793 0.887Bebop 0.976 0.965 0.988Funk 0.937 0.862 0.937Cool 0.964 0.928 0.975Rap 0.926 0.890 0.951Hard Bop 0.944 0.905 0.967Folk 0.943 0.760 0.952Fusion 0.843 0.750 0.886Country 0.952 0.794 0.955Free 0.906 0.855 0.936Reggae 0.893 0.819 0.905Afro-Cuban 0.961 0.910 0.972Latin 0.940 0.904 0.945Brazilian 0.871 0.847 0.905Disco 0.899 0.891 0.902Acid 0.886 0.660 0.891Jazz 0.937 0.850 0.963Smooth 0.862 0.667 0.871Mean0.905 0.814 0.918Mean0.925 0.856 0.945Table 4. Experimental results for ‘Basic’ genre and Jazzsubgenre models using musical attributes.JazzTimbreAux.RhythmSubgenreSolo Brass Piano Reeds Perc.BackBeat Dance Swing Shufﬂe Syncop.New Orleans0.808 0.786 0.7900.6800.652 0.5640.936*0.513 0.515Boogie0.5100.924*0.5440.7140.5920.712 0.7370.505 0.676Swing0.721 0.784 0.7480.6790.624 0.5780.923*0.511 0.508Bebop0.725 0.850 0.862 0.7030.662 0.5250.946*0.509 0.602Cool0.6390.750 0.836 0.7010.697 0.4240.890*0.504 0.568HardBop0.6060.774 0.7370.6690.7260.5550.808*0.684 0.606Fusion0.604 0.497 0.669 0.5070.574 0.577 0.507 0.500 0.693*Free0.606 0.5380.7840.6150.809* 0.7650.577 0.515 0.558Afro-Cuban0.6960.822 0.706 0.832*0.7820.648 0.512 0.5010.790Brazilian0.5600.7360.568 0.5720.761*0.555 0.532 0.504 0.635Acid0.591 0.513 0.658* 0.5070.585 0.622 0.509 0.515 0.635Smooth0.530 0.5770.748*0.5900.559 0.614 0.513 0.509 0.573Table 5. Attributes important to the Jazz subgenres areshown. AUC values greater than 0.70 are bold. The highestperforming attribute for each genre is denoted with a *.Boogie and Afro-Cuban styles, even though different,place heavy emphasis on the piano, which is shown hereas well. Bebop, Hard-bob, and Afro-Cuban Jazz show em-phasis placed on solo brass, piano, and reeds, as they relyheavily on solo artists of these instruments (e.g., “Dizzy”Gillespie, Miles Davis, Thelonious Monk, John Coltrane).The presence of auxiliary percussion is also a good de-scriptor of Afro-Cuban Jazz, where the use of hand drums(e.g., bongos, congas) is very prevalent.Rhythm is also important in Jazz subgenres. The dance-ability, back-beat, and presence of swing and syncopationare deﬁning characteristics of certain Jazz rhythms. It isimportant to note that a high AUC does not necessarily de-note the presence of that attribute, only its consistent re-lationship. For example, back-beat is a good predictor ofFree Jazz possibly due to its absolute absence. Alterna-tively, one may think that the presence of swing is impor-tant in all Jazz. Bebop, Hard Bop, New Orleans, and SwingJazz do have a heavy dependence on swing being present.However, Afro-Cuban Jazz relies on straight time, clave-based rhythms, so syncopation is actually a better predic-tor. It is also important to note that while the attributesof swing and shufﬂe are musically related, there is a cleardistinction in their application. In this case, swing is veryimportant, while shufﬂe is only slightly useful (e.g., Boo-gie). However, outside of the Jazz genre, the opposite casemay be true, where shufﬂe is the more important attribute(e.g. Blues, Country). This suggests that it is important tomake a clear distinction between swing and shufﬂe.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 335. PREDICTING GENRE FROM AUDIOThere is a large body of work on musical genre recogni-tion from and audio signals [28,30]. However, most knownprior work in this area focuses on discriminating a discreteset of basic genre labels with little emphasis on what de-ﬁnes genre. In response, researchers have tried to developdatasets that focus on style or subgenre labels (e.g., ball-room dance [7, 13, 24], latin [19], electronic dance [23],Indian [17]) that have clear relations to the presence ofspeciﬁc musical attributes. However, because models aredesigned for these speciﬁc sets, the methods used do notadapt to larger more generalized music collections. For ex-ample, tempo alone is a good descriptor for the ballroomdance style dataset, which is not true for more general col-lections [12].Other work in genre recognition avoids the problem ofstrict genre class separations. Audio feature similarity, selforganizing maps, and nearest-neighbor approaches can beused estimate genre of an unknown example [22]. Simi-larly, auto-tagging approaches use audio features to learnthe presence of both musical attributes and genre tags cu-rated by the public [2, 8] or by experts [29].In this work, we compare modeling genre both withaudio features directly and with stacked approaches thatexploit the relationships of audio features and musical at-tributes.5.1 Timbre Related FeaturesIn order to capture timbral components and model vo-cal, instrumentation, and sonority attributes, block-basedMel-Frequency Cepstral Coefﬁcients (MFCC) are imple-mented. Means and covariances of 20 MFCCs are cal-culated across non-overlapping 3-second blocks. Theseblock-covariances are further summarized over the pieceby calculating their means and variances [27]. This yieldsa 460 dimensional timbre based feature set.5.2 Rhythm Related FeaturesIn order to capture aspects of each rhythm attribute, a setof rhythm-speciﬁc features was implemented. All rhythmfeatures described in this section rely on global estimatesof an accent signal [3]Thebeat proﬁlequantizes the accent signal betweenconsecutive beats to 36 subdivisions. The beat proﬁle fea-tures are statistics of those 36 bins over all beats. The fea-ture relies on estimates of both beats [9] and tempo.Thetempogram ratiofeature (TGR) uses the tempo es-timate to remove the tempo dependence in a tempogram.By normalizing the tempo axis of the tempogram by thetempo estimate, a fractional relationship to the tempo isgained. A compact, tempo-invariant feature is created bycapturing the weights of the tempogram at musically re-lated ratios relative to the tempo estimate.TheMellin scale transformis a scale invariant trans-form of a time domain signal. Similar musical patternsat different tempos are scaled relative to the tempo. TheMellin scale transform is invariant to that tempo scaling. Itwas ﬁrst introduced in the context of rhythmic similarity byHolzapfel [16], around which our implementation is based.In order to exploit the natural periodicity in the transform,the discrete cosine transform (DCT) is computed. Medianremoval (by subtracting the local median) and half-waverectifying the DCT creates a new feature that emphasizestransform periodicities.The previous rhythm features are also extended tomultiple-band versions by using accent signals that areconstrained to be within a set of speciﬁc sub-bands. Thisaffords the ability to capture the rhythmic function of in-struments in different frequency ranges. The rhythm fea-ture set used in this work is an aggregation of the medianremoved Mellin Transform DCT and multi-band represen-tations of the beat proﬁle and the tempogram ratio features.This yields a 372 dimensional rhythm based feature set thatwas shown in previous work to be relatively effective atcapturing musical attributes related to rhythm (see [25] formore details).5.3 Genre Recognition ExperimentsIn addition to the experiment from Section 4, we presentthree additional methods for modeling genre, each basedon audio signal analysis. The second method (Figure 1b)performs the task of genre recognition with rhythm andtimbre inspired audio features directly. The third method(Figure 1c) is motivated similar to the ﬁrst experiment,which employs the expertly-labeled musical attributes.However, inspired by work in transfer learning [4], au-dio features are used to develop models for the humanly-deﬁned attributes which in turn are used to model genre.Through this supervised pre-training of musical attributes,models of genre can be learned from attributes’ estimatedpresence. In the fourth approach (Figure 1d), inspiredby [6] and [18], the learned attributes are combined withthe audio features directly in a shared middle layer to trainmodels of genre.Similar to Section 4, genre is modeled with logistic re-gression ﬁt using stochastic gradient decent (SGD). Thedata was separated on the same 70%:30% (train:test) split.Once again, there were no shared artists between trainingand testing. Due to the size of the dataset, a single trial foreach genre, as well as for each learned musical attribute,is both tractable and sufﬁcient. The learning rate for eachmodel is tuned adaptively.5.3.1 Using Audio Features DirectlyOf the four presented approaches, the second uses audiofeatures directly to model genre. The features from Sec-tions 5.1 and 5.2 are used in aggregation and a model istrained and tested for each individual genre. This providesa baseline for what audio features are able to capture with-out any added context. However, this lack of context makesit hard to interpret what about genre they are capturing.5.3.2 Stacked MethodsThe third and fourth approaches are also driven by au-dio features. However instead of targeting genre directly,34 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015models are learned for each of the vocal, instrumentation,sonority, and rhythm attributes. Inspired by approaches intransfer learning [4], and similar in structure to previousmethods in the MIR community [20], the learned attributesare then used to predict genre. This approach is formu-lated similar to a basic neural network with a supervisedpre-trained (and no longer hidden) musical attributes layer.The rhythm-based attributes are modeled with a featureaggregation of the Mellin DCT, multi-band beat proﬁle,and multi-band tempogram ratio features. The vocals, in-strumentation, and sonority attributes are modeled with theblock-based MFCC features. Each attribute is modeled us-ing logistic regression for binary labels (categorical) andlinear regression for continuous labels (scale-based). If anindividual attribute is formulated as a binary classiﬁcationtask (see Section 3.1), the probability of the positive class(its presence) is used as the feature value.The ﬁrst version of the stacked methods (third ap-proach) uses audio features to estimate musical attributesand employs only those estimated attributes to modelgenre. The second version (fourth approach) concatenatesthe audio features and the learned attributes in a sharedmiddle layer to model genre [6, 18].5.4 ResultsIn this section, we will give an overview of all of the re-sults from the audio-based methods, and compare themto the models learned from the expertly-labeled attributes.In order to show the overall performance of each methodin a compact way, only combined rhythm and timbre ap-proaches will be compared. Once again each genre modelwill be evaluated using area under the ROC curve (AUC).In order to better evaluate the stacked models, we will ﬁn-ish with a brief evaluation of the learned attributes.5.4.1 Learning GenreA summary of the results for the audio experiments us-ing rhythm and timbre features is shown in Table 6. Thehuman attribute model results are also included for com-parison. Similar to Table 3, the mean AUC of each genregrouping is shown.Genre Human Audio Learned Audio +Group Attrib. Feat. Attrib. LearnedBasic 0.918 0.892 0.878 0.899Rock Sub 0.919 0.902 0.903 0.911Jazz Sub 0.945 0.910 0.893 0.923Rap Sub 0.940 0.916 0.914 0.927Dance Sub 0.965 0.963 0.955 0.966World Sub 0.904 0.850 0.846 0.865Mean0.931 0.905 0.897 0.915Table 6. An overview of experimental results using audio-based models that utilize timbre and rhythm features.Compared to the human attributes approach, using au-dio features alone to model genre performs relatively well.This is especially true for the ‘Basic’, Rock, and Dancegroups, where the audio feature AUC results are very closeto human attribute performance. Across the other groups,the differences between the audio feature models and themusical attribute models suggest that the audio featureslose some important, genre-deﬁning information. Further-more, performance that was close to musical attributeswhen using only audio features alone is also close whenmusical attributes learned from audio features. This sug-gests that, in these cases, the audio features may be cap-turing similarly salient components related to the musicalattributes that describe these genre groups.Overall, the learned attributes perform just as good asor worse than the audio features alone. This suggests thatthey are at most as powerful as the audio features usedto train them. However, combining audio features andlearned attributes shows signiﬁcant improvement (pairedt-testp<0.01across all genres) over using audio featuresor learned attributes alone. This evidence suggests thataudio features and learned attribute models each containslightly different information. The added human contextof the learned attributes is helpful to achieve results thatapproach those of the expertly-labeled attributes. This alsosuggests that the decisions made from learned labels arepossibly more similar to the decisions made from humanattribute labels, and the errors in estimating the musical at-tributes are possibly to blame for the performance decreasewhen used alone.Basic Human Audio Learned Audio +Jazz Human Audio Learned Audio +Genre Attrib. Feat. Attrib. LearnedSubgenre Attrib. Feat. Attrib. LearnedRock 0.856 0.831 0.835 0.839New Orleans 0.989 0.947 0.951 0.956Blues 0.915 0.892 0.883 0.899Boogie 0.978 0.962 0.939 0.962Gospel 0.843 0.798 0.794 0.805Swing 0.984 0.929 0.929 0.940Soul 0.887 0.833 0.818 0.842Bebop 0.988 0.951 0.943 0.957Funk 0.937 0.911 0.886 0.918Cool 0.975 0.900 0.901 0.916Rap 0.951 0.963 0.951 0.969HardBop 0.967 0.946 0.930 0.952Folk 0.952 0.905 0.903 0.916Fusion 0.886 0.844 0.812 0.867Country 0.955 0.885 0.880 0.897Free 0.936 0.920 0.923 0.931Reggae 0.905 0.926 0.885 0.929AfroCuban 0.972 0.934 0.912 0.946Latin 0.945 0.921 0.905 0.923Brazilian 0.905 0.879 0.858 0.904Disco 0.902 0.936 0.893 0.938Acid 0.891 0.841 0.763 0.846Jazz 0.963 0.907 0.906 0.916Smooth 0.871 0.868 0.853 0.894Mean0.918 0.892 0.878 0.899Mean0.945 0.910 0.893 0.923Table 7. Experimental results for the ‘Basic’ genres andJazz subgenres using audio-based models.The left half of Table 7 shows the results for predict-ing the ‘Basic’ genre labels. Within this set, we see someinteresting patterns start to emerge. In the case of Rap,Reggae, and Disco, audio features are actually able to out-perform the musical attributes. This suggests that our smallselected subset of 48 human attribute labels do not alwaystell the whole story, and that the audio features, which aremuch larger in dimensionality, possibly contain additionaland/or different information. As in previous results, thelearned attribute models perform similarly to methods thatuse audio features directly, but with a few exceptions. Inthe cases that the audio feature models do better than thehuman-labeled musical attribute models, the learned at-tribute models are able to performat mostas good as thehuman-labeled musical attribute models. This once againsuggests that the learned attribute approach may be betterapproximating the decisions the human-labeled attributeapproach is making. When adding audio and learned at-tributes together, the added context is once again beneﬁ-cial, with combined methods outperforming models thatuse audio or learned attributes alone. Audio feature mod-els that perform better than the human attributes modelsProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 35are additionally improved, showing again that the audiofeatures and human attribute labels contain complementaryinformation.The right half of Table 7 shows the results for predict-ing the Jazz subgenre labels. The Jazz genre shows moreexpected relationships between the human attribute, au-dio feature, and learned attribute methods. The combinedmethod outperforms each of the audio feature and learnedattribute methods. The human attribute method performsbetter than all audio-based methods.5.4.2 Learning AttributesIn order to further explore the stacked audio-based mod-els, we performed a small evaluation of how well the audiofeatures are able to learn each of the expertly-labeled mu-sical attributes. Sticking with a common theme, we willexplore the results of modeling attributes that are impor-tant to Jazz (from Table 5). Table 8 shows the ability todirectly predict these attributes from audio features. AUCaccuracies are reported for the binary attributes;R2valuesare reported for continuous attributes. The results of eval-uating the model for the training and testing sets is shown.Musical Audio Training Testing LabelAttributes Features AUC/R2AUC/R2TypeSolo Brass Timbre 0.796 0.798 binaryPiano Timbre 0.721 0.716 binaryReeds Timbre 0.790 0.789 binaryAux Percussion Timbre 0.750 0.750 binaryFeelSwing Rhythm 0.907 0.902 binaryFeelShufﬂe Rhythm 0.919 0.920 binaryFeelSyncopation Rhythm 0.772 0.770 binaryFeelBackBeat Rhythm 0.400 0.393 continuousFeelDance Rhythm 0.527 0.515 continuousTable 8. The results for learning binary (AUC) and contin-uous (R2) attributes important to Jazz are shown.First of all, we see that testing and training AUC isalmost identical. Because of this, a single trial (fold) isappropriate when learning attribute models. The learnedmodels should generalize over all music without over ﬁt-ting. This justiﬁes using the the same 70%:30% (train:test)split for each layer in the stacked models. We see thatMFCC’s do somewhat well for brass and reeds, but thelower AUC overall shows that these timbre features are notdoing enough to capture these attributes, which may be asource of error in genre models that rely heavily on timbre.However, the rhythm results are much better, especially forswing and shufﬂe, which was argued in Section 4 and Ta-ble 5 as an important distinction to make when predictingJazz subgenres.Attribute Type Num Mean Median MaximumContinous Rhythm (R2) 3 0.432±0.077 0.393 0.515Continous Timbre (R2) 12 0.266±0.192 0.194 0.514All Continuous15 0.299±0.186 0.389 0.515Binary Rhythm (AUC) 7 0.889±0.059 0.902 0.946Binary Timbre (AUC) 26 0.794±0.074 0.794 0.925All Binary15 0.814±0.080 0.806 0.946Table 9. Overall summary of learned attributes.Table 9 shows a summary of learning the all of the se-lected 48 attributes from audio features. It shows similartrends to Table 8, with rhythmic attributes better describedby audio features than timbral attributes. Furthermore, thecontinuous timbral attributes, which are sometimes com-plicated perceptually (e.g., vocal grittiness), are not mod-eled very well at all. This suggests that MFCC’s, and pos-sibly other spectral approximations, do not provide the fullpicture into what we perceive as the components of timbre.This is especially true in the context of instrument identi-ﬁcation in mixtures, which is a main utility of the timbrefeatures in this context. While these models as a wholecan be improved, the problems of instrument identiﬁcationand rhythm analysis are separate, large, and active researchareas [14, 15, 25, 26].6. CONCLUSIONIn this work, we demonstrated that there is potential to de-mystify the constructs of musical genre into distinct mu-sicological components. The attributes we selected frommusic experts are able to provide a great deal of genre dis-tinguishing information, but this is only an initial investiga-tion into these questions. We were also able to discover andoutline the importance of certain attributes in speciﬁc con-texts. This strongly suggests that the expression of musicalattributes are necessary additions to deﬁnitions of genre.It was also shown here (and in previous work [25]) thataudio features motivated by timbre and rhythm are, withsome success, able to model musical attributes. Audio fea-tures are also able to describe musical genre directly andthrough stacked approaches that exploit the learned modelsof musical attributes. This is strong evidence suggestingthat audio-based approaches are learning the presence ofthe musical attributes, to some degree, when distinguish-ing genre. In some cases, the audio-based models weremore powerful than the human musical attribute models.This suggests that there is more to genre than our chosensubset of rhythm and orchestration attributes, and it makesus contemplate that there is more about the deﬁnition ofgenre yet to be discovered.In seeking to improve on this work, we next look toinvestigate replacing the feature concatenation with latefusion of context-dependent classiﬁers (e.g., rhythm, tim-bre), which has shown improved results for genre classiﬁ-cation [11]. It may also be helpful to use a greater numberof the available attributes than the chosen 48, as well asadditional attribute types (e.g., melody, harmony). Further-more, perhaps the most interesting direction is to treat eachmusical attribute model as a hidden layer in a neural net-work. In these cases, the models that are trained to predictmusicological attributes will serve as a form of domain-speciﬁc pre-training. These models would perform fullback propagation across an additional layer which con-nects our attributes to genres. This will potentially helpto learn better models of genre as well as adjust the mod-els of musical attributes in order better capture their genrerelationships.36 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]Jean-Julien Aucouturier and Francois Pachet. Representingmusical genre: A state of the art.Journal of New Music Re-search, 32(1):83–93, 2003.[2]Thierry Bertin-Mahieux, Douglas Eck, and Michael Mandel.Automatic tagging of audio: The state-of-the-art.Machineaudition: Principles, algorithms and systems, pages 334–352, 2010.[3]Sebastian B¨ock and Gerhard Widmer. Maximum ﬁlter vibratosuppression for onset detection. InProc. of the 16th Interna-tional Conference on Digital Audio Effects (DAFx-13), 2013.[4]Rich Caruana. Multitask learning.Machine learning,28(1):41–75, 1997.[5]Jesse Davis and Mark Goadrich. The relationship betweenprecision-recall and roc curves. InProc. of the 23rd inter-national conference on Machine learning, pages 233–240.ACM, 2006.[6]Li Deng and Dong Yu. Deep convex net: A scalable architec-ture for speech pattern classiﬁcation. InProc. of Interspeech,2011.[7]Simon Dixon, Elias Pampalk, and Gerhard Widmer. Classiﬁ-cation of dance music by periodicity patterns. InProc. of theInternational Society for Music Information Retrieval Con-ference, 2003.[8]Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, andStephen Green. Automatic generation of social tags for musicrecommendation. InAdvances in neural information process-ing systems, pages 385–392, 2008.[9]Daniel PW Ellis. Beat tracking by dynamic programming.Journal of New Music Research, 36(1):51–60, 2007.[10]Franco Fabbri. A theory of musical genres: Two applications.Popular music perspectives, 1:52–81, 1982.[11]Arthur Flexer, Fabien Gouyon, Simon Dixon, and GerhardWidmer. Probabilistic combination of features for music clas-siﬁcation. InProc. of the International Society for Music In-formation Retrieval Conference, pages 111–114, 2006.[12]Fabien Gouyon and Simon Dixon. Dance music classiﬁca-tion: A tempo-based approach. InProc. of the InternationalSociety for Music Information Retrieval Conference, 2004.[13]Fabien Gouyon, Simon Dixon, Elias Pampalk, and GerhardWidmer. Evaluating rhythmic descriptors for musical genreclassiﬁcation. InProc. of the AES 25th International Confer-ence, pages 196–204, 2004.[14]Philippe Hamel, Sean Wood, and Douglas Eck. Automaticidentiﬁcation of instrument classes in polyphonic and poly-instrument audio. InProc. of the International Society forMusic Information Retrieval Conference, pages 399–404,2009.[15]Perfecto Herrera-Boyer, Geoffroy Peeters, and Shlomo Dub-nov. Automatic classiﬁcation of musical instrument sounds.Journal of New Music Research, 32(1):3–21, 2003.[16]Andr´e Holzapfel and Yannis Stylianou. Scale transform inrhythmic similarity of music.IEEE Trans. on Audio, Speechand Language Processing, 19(1):176–185, 2011.[17]S Jothilakshmi and N Kathiresan. Automatic music genreclassiﬁcation for indian music. InProc. Int. Conf. SoftwareComputer App, 2012.[18]Peter Knees, Tim Pohle, Markus Schedl, and Gerhard Wid-mer. Combining audio-based similarity with web-based datato accelerate automatic music playlist generation. InProc. ofthe 8th ACM international workshop on Multimedia informa-tion retrieval, pages 147–154. ACM, 2006.[19]Miguel Lopes, Fabien Gouyon, Alessandro L Koerich, andLuiz ES Oliveira. Selection of training instances for musicgenre classiﬁcation. InProc. of the International Conferenceon Pattern Recognition, pages 4569–4572. IEEE, 2010.[20]F. Pachet and P. Roy. Improving multilabel analysis of mu-sic titles: A large-scale validation of the correction approach.IEEE Trans. on Audio, Speech and Language Processing,17(2):335 –343, 2009.[21]Franc ¸ois Pachet and Daniel Cazaly. A taxonomy of musi-cal genres. InContent-Based Multimedia Information AccessConference, pages 1238–1245, 2000.[22]Elias Pampalk, Arthur Flexer, and Gerhard Widmer. Improve-ments of audio-based music similarity and genre classiﬁca-ton. InProc. of the International Society for Music Informa-tion Retrieval Conference, volume 5, pages 634–637, 2005.[23]Maria Panteli, Niels Bogaards, and Aline Honingh. Model-ing rhythm similarity for electronic dance music.Proc. of theInternational Society for Music Information Retrieval Con-ference, 2014.[24]Geoffroy Peeters. Rhythm classiﬁcation using spectralrhythm patterns. InProc. of the International Society for Mu-sic Information Retrieval Conference, pages 644–647, 2005.[25]Matthew Prockup, Andreas F. Ehmann, Fabien Gouyon,Erik M. Schmidt, and Youngmoo E. Kim. Modeling musi-cal rhythm at scale using the music genome project.IEEEWorkshop on Applications of Signal Processing to Audio andAcoustics, 2015.[26]Jeffrey Scott and Youngmoo E Kim. Instrument identiﬁcationinformed multi-track mixing. InProc. of the InternationalSociety for Music Information Retrieval Conference, pages305–310, 2013.[27]Klaus Seyerlehner, Markus Schedl, Peter Knees, and Rein-hard Sonnleitner. A reﬁned block-level feature set for classi-ﬁcation, similarity and tag prediction.Extended Abstract toMIREX, 2011.[28]Bob L Sturm. The state of the art ten years after a state of theart: Future research in music information retrieval.Journal ofNew Music Research, 43(2):147–172, 2014.[29]Derek Tingle, Youngmoo E Kim, and Douglas Turnbull.Exploring automatic music annotation with acoustically-objective tags. InProc. of the international conference onMultimedia information retrieval, pages 55–62. ACM, 2010.[30]George Tzanetakis and Perry Cook. Musical genre classiﬁ-cation of audio signals.IEEE Trans. on Audio, Speech andLanguage Processing, 10(5):293–302, 2002.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 37"
    },
    {
        "title": "Large-Scale Content-Based Matching of MIDI and Audio Files.",
        "author": [
            "Colin Raffel",
            "Daniel P. W. Ellis"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417371",
        "url": "https://doi.org/10.5281/zenodo.1417371",
        "ee": "https://zenodo.org/records/1417371/files/RaffelE15.pdf",
        "abstract": "MIDI files, when paired with corresponding audio record- ings, can be used as ground truth for many music infor- mation retrieval tasks. We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content based solely on content, i.e., with- out using any metadata. The core of our approach is a con- volutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset.",
        "zenodo_id": 1417371,
        "dblp_key": "conf/ismir/RaffelE15",
        "keywords": [
            "MIDI files",
            "audio recordings",
            "ground truth",
            "music information retrieval",
            "corpus of audio content",
            "efficient matching",
            "audio content",
            "large-scale dynamic time warping",
            "corpus of MIDI files",
            "Million Song Dataset"
        ],
        "content": "LARGE-SCALE CONTENT-BASED MATCHING OF MIDI AND AUDIO FILESColin Raffel, Daniel P. W. EllisLabROSA, Department of Electrical EngineeringColumbia University, New York, NY{craffel,dpwe}@ee.columbia.eduABSTRACTMIDI ﬁles, when paired with corresponding audio record-ings, can be used as ground truth for many music infor-mation retrieval tasks. We present a system which canefﬁciently match and align MIDI ﬁles to entries in a largecorpus of audio content based solely on content, i.e., with-out using any metadata. The core of our approach is a con-volutional network-based cross-modality hashing schemewhich transforms feature matrices into sequences of vectorsin a common Hamming space. Once represented in thisway, we can efﬁciently perform large-scale dynamic timewarping searches to match MIDI data to audio recordings.We evaluate our approach on the task of matching a hugecorpus of MIDI ﬁles to the Million Song Dataset.1. TRAINING DATA FOR MIRCentral to the task of content-based Music Information Re-trieval (MIR) is the curation of ground-truth data for tasksof interest (e.g. timestamped chord labels for automaticchord estimation, beat positions for beat tracking, promi-nent melody time series for melody extraction, etc.). Thequantity and quality of this ground-truth is often instrumen-tal in the success of MIR systems which utilize it as trainingdata. Creating appropriate labels for a recording of a givensong by hand typically requires person-hours on the orderof the duration of the data, and so training data availabilityis a frequent bottleneck in content-based MIR tasks.MIDI ﬁles that are time-aligned to matching audio canprovide ground-truth information [8,25] and can be utilizedin score-informed source separation systems [9, 10]. AMIDI ﬁle can serve as a timed sequence of note annotations(a “piano roll”). It is much easier to estimate informationsuch as beat locations, chord labels, or predominant melodyfrom these representations than from an audio signal. Anumber of tools have been developed for inferring this kindof information from MIDI ﬁles [6, 7, 17, 19].Halevy et al. [11] argue that some of the biggest suc-cesses in machine learning came about because “...a largetraining set of the input-output behavior that we seek to au-tomate is available to us in the wild.” The motivation behindc\u0000Colin Raffel, Daniel P. W. Ellis.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Colin Raffel, Daniel P. W. Ellis. “Large-Scale Content-Based Matching of MIDI and Audio Files”, 16th InternationalSociety for Music Information Retrieval Conference, 2015.J/Jerseygi.midV/VARIA18O.MIDCarpenters/WeveOnly.mid2009 MIDI/handy_man1-D105.midG/Garotos Modernos - Bailanta De Fronteira.midVarious Artists/REWINDNAS.MIDGoldenEarring/Twilight_Zone.midSure.Polyphone.Midi/Poly 2268.midd/danza3.mid100%sure.polyphone.midi/Fresh.midrogers_kenny/medley.mid2009 MIDI/looking_out_my_backdoor3-Bb192.midFigure 1. Random sampling of 12 MIDI ﬁlenames andtheir parent directories from our corpus of 455,333 MIDIﬁles scraped from the Internet.this project is that MIDI ﬁles ﬁt this description. Througha large-scale web scrape, we obtained 455,333 MIDI ﬁles,140,910 of which were unique – orders of magnitude largerthan any available dataset of aligned transcriptions. Thisproliferation of data is likely due to the fact that MIDI ﬁlesare typically a few kilobytes in size and were therefore apopular format for distributing and storing music recordingswhen hard drives had only megabytes of storage.The mere existence of a large collection of MIDI datais not enough: In order to use MIDI ﬁles as ground truth,they need to be both matched (paired with a correspondingaudio recording) and aligned (adjusted so that the timingof the events in the ﬁle match the audio). Alignment hasbeen studied extensively [8,25], but prior work typically as-sumes that the MIDI and audio have been correctly matched.Given large corpora of audio and MIDI ﬁles, the task ofmatching entries of each type may seem to be a simple mat-ter of fuzzy text matching of the ﬁles’ metadata. However,MIDI ﬁles almost never contain structured metadata, andas a result the best-case scenario is that the artist and songtitle are included in the ﬁle or directory name. While wefound some examples of this in our collection of scrapedMIDI ﬁles, the vast majority of the ﬁles had effectively nometadata information. Figure 1 shows a random samplingof directory and ﬁlenames from our collection.Since the goal of matching MIDI and audio ﬁles is toﬁnd pairs that havecontentin common, we can in principleidentify matches regardless of metadata availability or accu-racy. However, comparing content is more complicated andmore expensive than a fuzzy text match. SinceNMcom-parisons are required to match a MIDI dataset of sizeNtoan audio ﬁle dataset of sizeM, matching large collections234is practical only when the individual comparisons can bemade very fast. Thus, the key aspect of our work is ahighly-efﬁcientscheme to match the content of MIDI and audioﬁles. Our system learns a cross-modality hashing whichconverts both MIDI and audio content vectors to a commonHamming (binary) space in which the “local match” opera-tion at the core of dynamic time warping (DTW) reducesto a very fast table lookup. As described below, this allowsus to match a single MIDI ﬁle to a huge collection of audioﬁles in minutes rather than hours.The idea of using DTW distance to match MIDI ﬁles toaudio recordings is not new. For example, in [13], MIDI-audio matching is done by ﬁnding the minimal DTW dis-tance between all pairs of chromagrams of (synthesized)MIDI and audio ﬁles. Our approach differs in a few keyways: First, instead of using chromagrams (a hand-designedrepresentation), we learn a common representation for MIDIand audio data. Second, our datasets are many orders ofmagnitude larger (hundreds of thousands vs. hundreds ofﬁles), which necessitates a much more efﬁcient approach.Speciﬁcally, by mapping to a Hamming space we greatlyspeed up distance matrix calculation and we receive quad-ratic speed gains by implicitly downsampling the audioand MIDI feature sequences as part of our learned featuremapping.In the following section, we detail the dataset of MIDIﬁles we scraped from the Internet and describe how we pre-pared a subset for training our hasher. Our cross-modalityhashing model is described in Section 3. Finally, in sec-tion 4 we evaluate our system’s performance on the taskof matching ﬁles from our MIDI dataset to entries in theMillion Song Dataset [3].2. PREPARING DATAOur project began with a large-scale scrape of MIDI ﬁlesfrom the Internet. We obtained 455,333 ﬁles, of which140,910 were found to have unique MD5 checksums. Thegreat majority of these ﬁles had little or no metadata in-formation. The goal of the present work is to develop anefﬁcient way to match this corpus against the Million SongDataset (MSD), or, more speciﬁcally, to the short previewaudio recordings provided by 7digital [20].For evaluation, we need a collection of ground-truthMIDI-audio pairs which are correctly matched. Our ap-proach can then be judged based on how accurately it isable to recover these pairings using the content of the au-dio and MIDI ﬁles alone. To develop our cross-modalityhashing scheme, we further require a collection ofalignedMIDI and audio ﬁles, to supply the matching pairs of fea-ture vectors from each domain that will be used to train ourmodel for hashing MIDI and audio features to a commonHamming space (described in Section 3). Given matchedaudio and MIDI ﬁles, existing alignment techniques canbe used to create this training data; however, we must ex-clude incorrect matches and failed alignments. Even at thescale of this reduced set of training data, manual alignmentveriﬁcation is impractical, so we developed an improvedalignment quality score which we describe in Section 2.3.2.1 Metadata matchingTo obtain a collection of MIDI-audio pairs, we ﬁrst sepa-rated a subset of MIDI ﬁles for which the directory namecorresponded to the song’s artist and the ﬁlename gavethe song’s title. The resulting metadata needed additionalcanonicalization; for example, “The Beatles”, “Beatles,The”, “Beatles”, and “The Beatles John Paul Ringo George”all appeared as artists. To normalize these issues, we ap-plied some manual text processing and resolved the artistsand song titles against the Freebase [5] and Echo Nest1databases. This resulted in a collection of 17,243 MIDIﬁles for 10,060 unique songs, which we will refer to as the“clean MIDI subset”.We will leverage the clean MIDI subset in two ways:First, to obtain ground-truth pairings of MSD/MIDI matches,and second, to create training data for our hashing scheme.The training data does not need to be restricted to the MSD,and using other sources to increase the training set sizewill likely improve our hashing performance, so we com-bined the MSD with three benchmark audio collections:CAL500 [26], CAL10k [24], and uspop2002 [2]. To matchthese datasets to the clean MIDI subset, we used the Pythonsearch engine librarywhoosh2to perform a fuzzy match-ing of their metadata. This resulted in 26,311 audio/MIDIﬁle pairs corresponding to 5,243 unique songs.2.2 Aligning audio to synthesized MIDIFuzzy metadata matching is not enough to ensure that wehave MIDI and audio ﬁles with matching content: For in-stance, the metadata could be incorrect, the fuzzy text matchcould have failed, the MIDI could be a poor transcription(e.g., missing instruments or sections), and/or the MIDIand audio data could correspond to different versions of thesong. Since we will use DTW to align the audio contentto an audio resynthesis of the MIDI content [8,13, 25], wecould potentially use the overall match cost – the quantityminimized by DTW – as an indicator of valid matches,since unrelated MIDI and audio pairs will likely result ina high optimal match cost. (An overview of DTW and itsapplication to music can be found in [18].)Unfortunately, the calibration of this raw match cost“conﬁdence score” is typically not comparable between dif-ferent alignments. Our application, however, requires aDTW conﬁdence score that can reliably decide when anaudio/MIDI ﬁle pairing is valid for use as training data forour hashing model. Our best results came from the follow-ing system for aligning a single MIDI/audio ﬁle pair: First,we synthesize the MIDI data usingfluidsynth.3Wethen estimate the MIDI beat locations using the MIDI ﬁle’stempo change information and the method described in [19].To circumvent the common issue where the beat is trackedone-half beat out of phase, we double the BPM until it is atleast 240. We compute4beat locations for the audio signalwith the constraint that the BPM should remain close to the1http://developer.echonest.com/docs/v42https://github.com/dokipen/whoosh3http://www.fluidsynth.org4All audio analysis was accomplished withlibrosa[16].Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 235global MIDI tempo. We then compute log-amplitude beat-synchronous constant-Q transforms (CQTs) of audio andsynthesized MIDI data with semitone frequency spacingand a frequency range from C3 (65.4 Hz) to C7 (1046.5 Hz).The resulting feature matrices are then of dimensionalityN⇥DandM⇥DwhereNandMare the resulting num-ber of beats in the MIDI and audio recordings respectivelyandDis48(the number of semitones between C3 and C7).Example CQTs computed from a 7digital preview clip andfrom a synthesized MIDI ﬁle can be seen in Figure 2(a) and2(b) respectively.We then use DTW to ﬁnd the lowest-cost path througha full pairwise cosine distance matrixS2RN⇥Mof theMIDI and audio CQTs. This path can be represented astwo sequencesp, q2RLof indices from each sequencesuch thatp[i]=n, q[i]=mimplies that thenth MIDI beatshould be aligned to themth audio beat. Traditional DTWconstrains this path to include the start and end of eachsequence, i.e.p[1] =q[1] = 1andp[L]=N;q[L]=M.However, the MSD audio consists of cropped preview clipsfrom 7digital, while MIDI ﬁles are generally transcriptionsof the entire song. We therefore modify this constraint sothat eithergNp[L]NorgMq[L]M;gisa parameter which provides a small amount of additionaltolerance and is normally close to1. We employ an additivepenalty\u0000for “non-diagonal moves” (i.e. path entries whereeitherp[i]=p[i+ 1]orq[i]=q[i+ 1]) which, in oursetting, is set to approximate a typical distance value inS. The combined use ofgand\u0000typically results in pathswhere bothp[1]andq[1]are close to1, so no further pathconstraints are needed. For synthesized MIDI-to-audioalignment, we usedg=.95and set\u0000to the 90th percentileof all the values inS. The cosine distance matrix and thelowest-cost DTW path for the CQTs shown in Figure 2(a)and 2(b) can be seen in Figure 2(e).2.3 DTW cost as conﬁdence scoreThe cost of a DTW pathp, qthroughSis calculated by thesum of the distances between the aligned entries of eachsequence:c=LXi=1S[p[i],q[i]] +T(p[i]\u0000p[i\u00001],q[i]\u0000q[i\u00001])where the transition cost termT(u, v)=0ifuandvare 1,otherwiseT(u, v)=\u0000. As discussed in [13], this cost isnot comparable between different alignments for two mainreasons: Firstly, the path length can vary greatly acrossMIDI/audio ﬁle pairs depending onNandM. We thereforeprefer a per-stepmeandistance, where we dividecbyL.Secondly, various factors irrelevant to alignment such asdifferences in production and instrumentation can effect aglobal shift on the values ofS, even when its local variationsstill reveal the correct alignment. This can be mitigatedby normalizing the DTW cost by the mean value of thesubmatrix ofScontaining the DTW path:B=max(p)Xi=min(p)max(q)Xj=min(q)S[i, j]We combine the above to obtain a modiﬁed DTW costˆc:ˆc=cLBTo estimate the largest value ofˆcfor acceptable align-ments, we manually auditioned 125 alignments and recordedwhether the audio and MIDI were well-synchronized fortheir entire duration, our criterion for acceptance. Thisground-truth supported a receiver operating characteristic(ROC) forˆcwith an AUC score of0.986, indicating a highlyreliable conﬁdence metric. A threshold of0.78allowed zerofalse accepts on this set while only falsely discarding 15well-aligned pairs. Retaining all alignments with costs bet-ter (lower) than this threshold resulted in 10,035 successfulalignments.Recall that these matched and aligned pairs serve twopurposes: They provide training data for our hashing model;and we also use them to evaluate the entire content-basedmatching system. For a fair evaluation, we exclude itemsused in training from the evaluation, thus we split the suc-cessful alignments into three parts: 50% to use as trainingdata,25%as a “development set” to tune the content-basedmatching system, and the remaining25%to use for ﬁnalevaluation of our system. Care was taken to split basedonsongs, rather than by entry (since some songs appearmultiple times).3. CROSS-MODALITY HASHING OF MIDI ANDAUDIO DATAWe now arrive at the central part of our work, the schemefor hashing both audio and MIDI data to a common simplerepresentation to allow very fast computation of the distancematrixSneeded for DTW alignment. In principle, giventhe conﬁdence score of the previous section, to ﬁnd audiocontent that matches a given MIDI ﬁle, all we need todo is perform alignment against every possible candidateaudio ﬁle and choose the audio ﬁle with the lowest score.To maximize the chances of ﬁnding a match, we need touse a large and comprehensive pool of audio ﬁles. Weuse the 994,960 7digital preview clips corresponding tothe Million Song Dataset, which consist of (typically) 30second portions of recordings from the largest standardresearch corpus of popular music [20]. A complete searchfor matches could thus involve 994,960 alignments for eachof our 140,910 MIDI ﬁles.The CQT-to-CQT approach of section 2.2 cannot fea-sibly achieve this. The median number of beats in ourMIDI ﬁles is 1218, and for the 7digital preview clips it is186. Computing the cosine distance matrixSof this size(forD= 48dimension CQT features) using the highlyoptimized C++ code fromscipy[14] takes on average9.82 milliseconds on an Intel Core i7-4930k processor.When implemented using the LLVM just-in-time compilerPython modulenumba,5the DTW cost calculation de-scribed above takes on average 892 microseconds on thesame processor. Matching asingleMIDI ﬁle to the MSDusing this approach would thus take just under three hours;5http://numba.pydata.org/236 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2. Audio and hash-based features and alignment for Billy Idol - “Dancing With Myself” (MSD track IDTRCZQLG128F427296A). (a) Normalized constant-Q transform of 7digital preview clip, with semitones on the ver-tical axis and beats on the horizontal axis. (b) Normalized CQT for synthesized MIDI ﬁle. (c) Hash bitvector sequence for7digital preview clip, with pooled beat indices and Hamming space dimension on the horizontal and vertical axes respectively.(d) Hash sequence for synthesized MIDI. (e) Distance matrix and DTW path (displayed as a white dotted line) for CQTs.Darker cells indicate smaller distances. (f) Distance matrix and DTW path for hash sequences.matching our entire 140,910 MIDI ﬁle collection to theMSD would take years. Clearly, a more efﬁcient approachis necessary.Calculating the distance matrix and the DTW cost arebothO(NM)in complexity; the distance matrix calcula-tion is about 10 times slower presumably because it involvesDmultiply-accumulate operations to compute the innerproduct for each point. Calculating the distance betweenfeature vectors is therefore the bottleneck in our system, soany reduction in the number of feature vectors (i.e., beats)in each sequence will give quadratic speed gains for bothDTW and distance matrix calculations.Motivated by these issues, we propose a system whichlearns a common, reduced representation for the audio andMIDI features in a Hamming space. By replacing constant-Q spectra with bitvectors, we replace the expensive innerproduct computation by an exclusive-or operation followedby simple table lookup: The exclusive-or of two bitvectorsaandbwill yield a bitvector consisting of1s whereaandbdiffer and0s elsewhere, and the number of1s in allbitvectors of lengthDcan be precomputed and stored in atable of size2D. In the course of computing our Hammingspace representation, we also implicitly downsample thesequences over time, which provides speedups for bothdistance matrix and DTW calculation. Our approach has theadditional potential beneﬁt oflearningthe most effectiverepresentation for comparing audio and MIDI constant-Qspectra, rather than assuming the cosine distance of CQTvectors is suitable.3.1 Hashing with convolutional networksOur hashing model is based on the Siamese network archi-tecture proposed in [15]. Given feature vectors{x}and{y}from two modalities, and a set of pairsPsuch that(x, y)2Pindicates thatxandyare considered “similar”,and a second setNconsisting of “dissimilar” pairs, a non-linear mapping is learned from each modality to a commonHamming space such that similar and dissimilar featurevectors are respectively mapped to bitvectors with smalland large Hamming distances. A straightforward objectivefunction which can be minimized to ﬁnd an appropriatemapping isL=1|P|X(x,y)2Pkf(x)\u0000g(y)k22\u0000↵|N|X(x,y)2Nmax(0,m\u0000kf(x)\u0000g(y)k2)2wherefandgare the nonlinear mappings for each modality,↵is a parameter to control the importance of separatingdissimilar items, andmis a target separation of dissimilarpairs.The task is then to optimize the nonlinear mappingsfandgwith respect toL. In [15] the mappings are imple-mented as multilayer nonlinear networks. In the presentwork, we will use convolutional networks due to their abil-ity to exploit invariances in the input feature representation;CQTs contain invariances in both the time and frequencyaxes, so convolutional networks are particularly well-suitedfor our task. Our two feature modalities are CQTs fromsynthesized MIDI ﬁles and audio ﬁles. We assemble theset of “similar” cross-modality pairsPby taking the CQTProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 237frames from individual aligned beats in our training set. Thechoice ofNis less obvious, but randomly choosing CQTspectra from non-aligned beats in our collection achievedsatisfactory results.3.2 System speciﬁcsTraining the hashing model involves presenting trainingexamples and backpropagating the gradient ofLthroughthe model parameters. We held out 10% of the training setdescribed in Section 2 as a validation set, not used in train-ing the networks. We z-scored the remaining 90% acrossfeature dimensions and re-used the means and standarddeviations from this set to z-score the validation set.For efﬁciency, we used minibatches of training exam-ples; each minibatch consisted of 50 sequences obtained bychoosing a random offset for each training sequence pairand cropping out the next 100 beats. ForN, we simplypresented the network with subsequences chosen at randomfrom different songs. Each time the network had iteratedover minibatches from the entire training set (one epoch),we repeated the random sampling process. For optimiza-tion, we used RMSProp, a recently-proposed stochasticoptimization technique [23]. After each 100 minibatches,we computed the lossLon the validation set. If the vali-dation loss was less than99%of the previous lowest, wetrained for 1000 more iterations (minibatches).While the validation loss is a reasonable indicator ofnetwork performance, its scale will vary depending on the↵andmregularization hyperparameters. To obtain a moreconsistent metric, we also computed the distribution ofdistances between the hash vectors produced by the networkfor the pairs inPand those inN. To directly measurenetwork performance, we used the Bhattacharya distance[4] to compute the separation of these distributions.In each modality, the hashing networks have the same ar-chitecture: A series of alternating convolutional and poolinglayers followed by a series of fully-connected layers. Alllayers except the last use rectiﬁer nonlinearities; as in [15],the output layer uses a hyperbolic tangent. This choiceallows us to obtain binary hash vectors by testing whethereach output unit is greater or less than zero. We chose 16bits for our Hamming space, since 16 bit values are efﬁ-ciently manipulated as short unsigned integers. The ﬁrstconvolutional layer has 16 ﬁlters each of size 5 beats by 12semitones, which gives our network some temporal contextand octave invariance. As advocated by [21], all subse-quent convolutional layers had2n+33⇥3 ﬁlters, wherenis the depth of the layer. All pooling layers performed max-pooling, with a pooling size of 2⇥2. Finally, as suggestedin [12], we initialized all weights with normally-distributedrandom variables with mean of zero and a standard devia-tion ofp2/nin, whereninis the number of inputs to eachlayer. Our model was implemented usingtheano[1] andlasagne.6To ensure good performance, we optimized all modelhyperparameters using Whetlab,7a web API which im-6https://github.com/Lasagne/Lasagne7Between submission and acceptance of this paper, Whetlab announced\nFigure 3. Output hash distance distributions for our best-performing network.plements black-box Bayesian optimization [22]. We usedWhetlab to optimize the number of convolutional/poolinglayers, the number and size of the fully-connected layers,the RMSProp learning rate and decay parameters, and the↵andmregularization parameters ofL. As a hyperpa-rameter optimization objective, we used the Bhattacharyyadistance as described above. The best performing networkfound by Whetlab had 2 convolutional layers, 2 “hidden”fully-connected layers with 2048 units in addition to a fully-connected output layer, a learning rate of .001 with anRMSProp decay parameter of .65,↵=.5, andm=4.This hyperparameter conﬁguration yielded the output hashdistance distributions forPandNshown in Figure 3, for aBhattacharyya separation of0.488.4. MATCHING MIDI FILES TO THE MSDAfter training our hashing system as described above, theprocess of matching MIDI collections to the MSD proceedsas follows: First, we precompute hash sequences for every7digital preview clip and every MIDI ﬁle in the clean MIDIsubset. Note that in this setting we are not computing fea-ture sequences for known MIDI/audio pairs, so we cannotforce the audio’s beat tracking tempo to be the same as theMIDI’s; instead, we estimate their tempos independently.Then, we compute the DTW cost as described in Section2.2 between every audio and MIDI hash sequence.We tuned the parameters of the DTW cost calculation tooptimize results over our “development” set of successfullyaligned MIDI/MSD pairs. We found it beneﬁcial to use asmaller value ofg=0.9. Using a ﬁxed value for the non-diagonal move penalty avoids the percentile calculation, sowe chose\u0000=4. Finally, we found that normalizing by theaverage distance valueBdid not help, so we skipped thisstep.4.1 ResultsBitvector sequences for the CQTs shown in Figure 2(a)and 2(b) can be seen in 2(c) and 2(d) respectively. Noteit would be ending its service. For posterity, the results of our hyperparame-ter search are available athttp://bit.ly/hash-param-search.238 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Rank1 10 100 1000 10000Percent15.2 41.6 62.8 82.7 95.9Table 1. Percentage of MIDI-MSD pairs whose hash se-quences had a rank better than each threshold.that because our networks contain two downsample-by-2pooling layers, the number of bitvectors is14of the numberof constant-Q spectra for each sequence. The Hammingdistance matrix and lowest-cost DTW path for the hashsequences are shown in Figure 2(f). In this example, wesee the same structure as in the CQT-based cosine distancematrix of 2(e), and the same DTW path was successfullyobtained.To evaluate our system using the known MIDI-audiopairs of our evaluation set, we rank MSD entries accord-ing to their hash sequence DTW distance to a given MIDIﬁle, and determine the rank of the correct match for eachMIDI ﬁle. The correct item received a mean reciprocalrank of0.241, indicating that the correct matches tended tobe ranked highly. Some intuition about the system perfor-mance is given by reporting the percentage of MIDI ﬁles inthe test set where the correct match ranked below a certainthreshold; this is shown for various thresholds in Table 1.Studying Table 1 reveals that we can’t rely on the correctentry appearing as the top match among all the MSD tracks;the DTW distance for true matches only appears at rank 1about 15.2% of time. Furthermore, for a signiﬁcant portionof our MIDI ﬁles, the correct match did not rank in thetop 1000. This was usually caused by the MIDI ﬁle beingbeat tracked at a different tempo than the audio ﬁle, whichinﬂated the DTW score. For MIDI ﬁles where the truematch ranked highly but not ﬁrst, the top rank was oftena different version (cover, remix, etc.) of the correct entry.Finally, some degree of inaccuracy can be attributed to thefact that our hashing model is not perfect (as shown inFigure 3) and that the MSD is very large, containing manypossible decoys. In a relatively small proportion of cases,the MIDI hash sequence ended up being very similar tomany MSD hash sequences, pushing down the rank of thecorrect entry.Given that we cannot reliably assume the top hit fromhashing is the correct MSD entry, it is more realistic tolook at our system as a pruning technique; that is, it can beused to discard MSD entries which we can be reasonablyconﬁdent do not match a given MIDI ﬁle. For example,Table 1 tells us that we can use our system to compute thehash-based DTW score between a MIDI ﬁle and every entryin the MSD, then discard all but 1% of the MSD and onlyrisk discarding the correct match about 4.1% of the time.We could then perform the more precise DTW on the fullCQT representations to ﬁnd the best match in the remain-ing candidates. Pruning methods are valuable only whenthey are substantially faster than performing the originalcomputation; fortunately, our approach is orders of magni-tude faster: On the same Intel Core i7-4930k processor, forthe median hash sequence lengths, calculating a Hammingdistance matrix between hash sequences is about 400 timesfaster than computing the CQT cosine distance matrix (24.8microseconds vs. 9.82 milliseconds) and computing theDTW score is about 9 times faster (106 microseconds vs.892 microseconds). These speedups can be attributed to thefact that computing a table lookup is much more efﬁcientthan computing the cosine distance between two vectorsand that, thanks to downsampling, our hash-based distancematrices have116of the entries of the CQT-based ones. Insummary, a straightforward way to describe the success ofour system is to observe that we can, with high conﬁdence,discard 99% of the entries of the MSD by performing a cal-culation that takes about as much time as matching against1% of the MSD.5. FUTURE WORKDespite our system’s efﬁciency, we estimate that perform-ing a full match of our 140,910 MIDI ﬁle collection againstthe MSD would still take a few weeks, assuming we areparallelizing the process on the 12-thread Intel i7-4930k.There is therefore room for improving the efﬁciency of ourtechnique. One possibility would be to utilize some of themany pruning techniques which have been proposed forthe general case of large-scale DTW search. Unfortunately,most of these techniques rely on the assumption that thequery sequence is of the same length or shorter than all thesequences in the database and so would need to be modiﬁedbefore being applied to our problem. In terms of accu-racy, as noted above most of our hash-match failures canbe attributed to erroneous beat tracking. With a better beattracking system or with added robustness to this kind oferror, we could improve the pruning ability of our approach.We could also compare the accuracy of our system to aslower approach on a much smaller task to help pinpointfailure modes. Even without these improvements, our pro-posed system will successfully provide orders of magnitudeof speedup for our problem of resolving our huge MIDIcollection against the MSD. All the code used in this projectis available online.86. ACKNOWLEDGEMENTSWe would like to thank Zhengshan Shi and Hilary Mogulfor preliminary work on this project and Eric J. Humphreyand Brian McFee for fruitful discussions.7. REFERENCES[1]Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Bergeron,Nicolas Bouchard, and Yoshua Bengio. Theano: newfeatures and speed improvements. Deep Learning andUnsupervised Feature Learning NIPS Workshop, 2012.[2]Adam Berenzweig, Beth Logan, Daniel P. W. Ellis, andBrian Whitman. A large-scale evaluation of acoustic8http://github.com/craffel/midi-datasetProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 239and subjective music-similarity measures.ComputerMusic Journal, 28(2):63–76, 2004.[3]Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian Whit-man, and Paul Lamere. The million song dataset. InProceedings of the 12th International Society for Mu-sic Information Retrieval Conference, pages 591–596,2011.[4]Anil Kumar Bhattacharyya. On a measure of diver-gence between two statistical populations deﬁned bytheir probability distributions.Bulletin of the CalcuttaMathematical Society, 35:99–109, 1943.[5]Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor. Freebase: a collaborativelycreated graph database for structuring human knowl-edge. InProceedings of the 2008 ACM SIGMOD in-ternational conference on Management of data, pages1247–1250. ACM, 2008.[6]Michael Scott Cuthbert and Christopher Ariza.music21: A toolkit for computer-aided musicologyand symbolic music data. InProceedings of the 11thInternational Society for Music Information RetrievalConference, pages 637–642, 2010.[7]Tuomas Eerola and Petri Toiviainen. MIR in Matlab:The MIDI toolbox. InProceedings of the 5th Interna-tional Society for Music Information Retrieval Confer-ence, pages 22–27, 2004.[8]Sebastian Ewert, Meinard M¨uller, Verena Konz, DanielM¨ullensiefen, and Geraint A. Wiggins. Towards cross-version harmonic analysis of music.IEEE Transactionson Multimedia, 14(3):770–782, 2012.[9]Sebastian Ewert, Bryan Pardo, Mathias Muller, andMark D. Plumbley. Score-informed source separationfor musical audio recordings: An overview.IEEE SignalProcessing Magazine, 31(3):116–124, 2014.[10]Joachim Ganseman, Gautham J. Mysore, Paul Scheun-ders, and Jonathan S. Abel. Source separation by scoresynthesis. InProceedings of the International ComputerMusic Conference, pages 462–465, 2010.[11]Alon Halevy, Peter Norvig, and Fernando Pereira. Theunreasonable effectiveness of data.IEEE IntelligentSystems, 24(2):8–12, 2009.[12]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. Delving deep into rectiﬁers: Surpassing human-level performance on ImageNet classiﬁcation.arXivpreprint arXiv:1502.01852, 2015.[13]Ning Hu, Roger B. Dannenberg, and George Tzanetakis.Polyphonic audio matching and alignment for musicretrieval. InIEEE Workshop on Applications of Sig-nal Processing to Audio and Acoustics, pages 185–188,2003.[14]Eric Jones, Travis Oliphant, and Pearu Peterson. SciPy:Open source scientiﬁc tools for Python. 2014.[15]Jonathan Masci, Michael M. Bronstein, Alexan-der M. Bronstein, and J¨urgen Schmidhuber. Multimodalsimilarity-preserving hashing.IEEE Transactions onPattern Analysis and Machine Intelligence, 36(4):824–830, 2014.[16]Brian McFee, Matt McVicar, Colin Raffel, DawenLiang, and Douglas Repetto. librosa: v0.3.1, November2014.[17]Cory McKay and Ichiro Fujinaga. jSymbolic: A featureextractor for MIDI ﬁles. InProceedings of the Inter-national Computer Music Conference, pages 302–305,2006.[18]Meinard M¨uller.Information retrieval for music andmotion. Springer, 2007.[19]Colin Raffel and Daniel P. W. Ellis. Intuitive anal-ysis, creation and manipulation of MIDI data withpretty_midi. InProceedings of the 15th Interna-tional Society for Music Information Retrieval Confer-ence Late Breaking and Demo Papers, 2014.[20]Alexander Schindler, Rudolf Mayer, and AndreasRauber. Facilitating comprehensive benchmarking ex-periments on the million song dataset. InProceedingsof the 13th International Society for Music InformationRetrieval Conference, pages 469–474, 2012.[21]Karen Simonyan and Andrew Zisserman. Very deepconvolutional networks for large-scale image recogni-tion.arXiv preprint arXiv:1409.1556, 2014.[22]Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.Practical bayesian optimization of machine learningalgorithms. InAdvances in Neural Information Process-ing Systems, pages 2951–2959, 2012.[23]Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average ofits recent magnitude.COURSERA: Neural Networksfor Machine Learning, 4, 2012.[24]Derek Tingle, Youngmoo E. Kim, and DouglasTurnbull. Exploring automatic music annotation with“acoustically-objective” tags. InProceedings of the in-ternational conference on Multimedia information re-trieval, pages 55–62. ACM, 2010.[25]Robert J. Turetsky and Daniel P. W. Ellis. Ground-truthtranscriptions of real music from force-aligned MIDIsyntheses.Proceedings of the 4th International Societyfor Music Information Retrieval Conference, pages 135–141, 2003.[26]Douglas Turnbull, Luke Barrington, David Torres, andGert Lanckriet. Towards musical query-by-semantic-description using the CAL500 data set. InProceedingsof the 30th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 439–446. ACM, 2007.240 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Comparison of the Singing Style of Two Jingju Schools.",
        "author": [
            "Rafael Caro Repetto",
            "Rong Gong",
            "Nadine Kroher",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416692",
        "url": "https://doi.org/10.5281/zenodo.1416692",
        "ee": "https://zenodo.org/records/1416692/files/RepettoGKS15.pdf",
        "abstract": "Performing schools (liupai) in jingju (also known as Pe- king or Beijing opera) are one of the most important ele- ments for the appreciation of this genre among connois- seurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most re- nowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musico- logical literature, we have selected those that can be stud- ied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the de- scriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology.",
        "zenodo_id": 1416692,
        "dblp_key": "conf/ismir/RepettoGKS15",
        "keywords": [
            "performing schools",
            "jingju",
            "musicological descriptions",
            "singing style",
            "dan role-type",
            "Mei school",
            "Cheng school",
            "MIR techniques",
            "audio features",
            "jingju music research corpus"
        ],
        "content": "COMPARISON OF THE SINGING STYLE OF  TWO JINGJU SCHOOLS Rafael Caro Repetto, Rong Gong, Nadine Kroher, Xavier Serra Music Technology Group, Universitat Pompeu Fabra, Barcelona {rafael.caro, rong.gong, nadine.kroher, xavier.serra}@upf.edu  ABSTRACT Performing schools (liupai) in jingju (also known as Pe-king or Beijing opera) are one of the most important ele-ments for the appreciation of this genre among connois-seurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most re-nowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musico-logical literature, we have selected those that can be stud-ied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the de-scriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology. 1. MOTIVATION This paper is a joint work between an ethnomusicologist (the first author) and a team of MIR researchers in the framework of the CompMusic project. In this project we exploit jingju music characteristics (and other music tra-ditions) with the aim of pushing forward the state of the art in MIR. In last ISMIR Conference (Taipei, 2014) jingju music received significant attention, with a specific tutorial and several papers published by members of our team [1-3], as well as the work by Tian et al. [4]. In the present paper though, the motivation has been to test the potential of current MIR methodologies to support and enhance qualitative and descriptive musicological anal-yses of jingju music. To this aim, we have selected one of the more relevant aspects of jingju music appreciation, which is the singing style of different performing schools; specifically, we have focused on two of the more popular ones, Mei school and Cheng school. The paper is structured hence in the following sec-tions. In the introduction we present the concept of jingju schools and the importance of singing style, as well as explain the purpose of the research undertaken for this paper. In the following two sections, we introduce the collection of recordings selected and the methodology proposed, and analyse the obtained results. In the discus-sion section we reflect on the challenges for expanding our research and present the direction of our future work. We conclude by summarising the musicological out-comes of the current research. 2. INTRODUCTION Jingju is one of the genres of Chinese traditional theatre arts, arguably the most widespread and acclaimed one. Originally a folk art form, the actor traditionally was in charge of the whole creative process, from costumes and make-up, to acting, dancing, reciting, arranging the music and sometimes even writing (or improvising) the lyrics. In order to structure their performance, actors drew on a vast repertoire of predefined conventions handed down by tradition and which concerns every single aspect of this art. Characters of jingju plays are classified in acting categories or role-types, which define which set of con-ventions the actor who plays that role-type should master. The high complexity of such conventions requires the ac-tor to specialize in the performance of just one role-type during his life-time career. Along jingju history, there were some outstanding actors that excelled in the mastery of these conventions and pushed forward the artistic standards of their respective role-types or the genre as a  Figure 1. Mei Lanfang (left) and Cheng Yanqiu (right) \n © Rafael Caro Repetto, Rong Gong, Nadine Kroher, Xavier Serra.  Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0). Attribution: Rafael Caro Repetto, Rong Gong, Nadine Kroher, Xavier Serra. “Comparison of the singing style of two jingju schools”, 16th International Society for Music Information Re-trieval Conference, 2015. \n507   whole. Some of these masters would bring their own per-sonalities to their performances and created personal styles. In a tradition that is orally transmitted, this would result in the appearance of liupai, or performing schools.1 The first half of the 20th century was the period of ma-jor development of jingju and when the most renowned schools appeared. It saw the extraordinary development of the dan role-type, that portraying young or mid-aged female characters, but performed by male actors, due to social and political constrictions. Four of them gained the title of “four great dan actors” (si da ming dan),2 and founded their own schools. Their strong personality, the context of market competition, and even their own physi-cal condition caused schools of dan to be the ones with a greater degree of difference within one specific role-type. Among these four schools, those founded by Mei Lan-fang (1894-1961) and Cheng Yanqiu (1904-1958) (Figure 1),3 respectively named Mei and Cheng schools, are the most widespread and followed ones today, currently per-formed in its vast majority by female actresses. These are precisely the ones we chose for our study. Each jingju school is highly associated to a particular repertoire of plays and generally to a predominant per-formance skill. In fact, this repertoire is formed of plays arranged by the school founder to precisely showcase its mastery in that specific skill. In the case of Mei and Cheng schools, singing is the most representative and ac-claimed aspect of their art. This aspect concerns mainly two elements, the arrangement of new tunes4 and the singing style. Among these two, the singing style is the feature that makes a performance instantly recognizable as belonging to any of these schools, and also one of the skills that performers, both professional and amateurs, put more effort to master. At the same time, it is one of the most important criteria for appreciating a performance among connoisseurs. The features that define singing style not only consist in the way voice is used in singing, but also in the very quality of the voice. Both of them should be considered not as natural personal qualities of particular actors, but as conventions that have to be trained and mastered. The resulting voice is to be under-stood hence as “an artificial voice, in the sense of display-ing artifice, or art” [5], and followers of each school aim at mastering this voice quality as well. Descriptions made in jingju musicology about singing style generally focus on its perceptual characteristics and                                                            1 The translation of liupai as school can be subject to misinterpretation. Differently to other traditions, jingju schools do not imply training in specific institutions or affiliation to specific lineages. They consist in the transmission of the performance style of individual great masters, so that the reference is always the founder of the school, and not the teach-er from whom the new actors or actresses learn. 2 Quotes from Chinese sources are given in our translation. 3 Picture from http://zh.wikipedia.org/wiki/程砚秋#/media/File:梅兰芳与程砚秋.JPG (detail). 4 Since jingju music is created by applying pre-existing melodic conven-tions, it is customary to use the term arrangement (bianqu) instead of composition to refer to this process. the psychological profile of the characters conveyed through those characteristics. Wichmann [5] quotes a typ-ical description of Mei school’s singing style from Zhongguo da baike quanshu (China Great Encyclopedia) by Hu Qiaomu, in which timbre is described as “‘sweet, fragile, clear and crisp, round, embellished and liquid.’ This timbre is considered ideal for portraying ‘natural, graceful and poised, dignified, gentle and lovely tradi-tional women. ’” In all the musicological sources consult-ed for this paper [5-10], description of singing style al-ways includes this kind of terminology. However, since the aim of our research is add precision to musicological description, we have selected those characteristics for which audio features can be objectively computed. Table 1 shows the musicological characteristics selected and their corresponding audio features. \nIn the last few years there have been several studies about singing characteristics in different Chinese tradi-tional theatre genres, like jingju [11-12] and kunqu [13-14]. In these studies different role-types have been com-pared in terms of several singing characteristics by ana-lysing monophonic recordings produced by the authors. In our work, we look in depth to one particular role-type, dan, and analyse singing characteristics with explicit ref-erence to its musicological descriptions and using com-mercial recordings. In the following section we describe the collection of recordings and explain the methodology proposed. 3. METHODOLOGY For this study, we have selected a collection of recordings from our jingju music research corpus [1], according to two criteria: representativeness and comparability. In or-der to assure that these recordings are representative of their school, we have considered both the recording artist and the recorded aria. We have looked for artists whose school filiation is explicitly stated in the release’s book-let, and arias belonging to plays for which we have liter-ary evidence (mostly from [8]) that are representative of their school. In order to maximize comparability, we have searched for plays for which musicological literature spe-cifically acknowledges a particular rendition from each of these two schools, as it is the case of Su San qijie accord-ing to [5, 8]. Since these are rare cases, due to the fact Characteristics Audio features Pitch register Pitch histogram (1st degree) Vibrato rate variability Vibrato rate (SD) Volume variability Loudness (SD) Brightness Spectral centroid (mean) LTAS Tristimulus Timbre variability Spectral flux (mean) Table 1. Musicological characteristics and their corre-sponding audio features considered in our study. 508 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   \n Figure 2. Block diagram of the methodology.  \nTristimulus \nLTAS \n1st degree \nRate, extent (mean, SD) \nMean, SD \nMean, SD \nMean, SD \nPitch tracks manual correction \nPitch histogram computation \nVibrato detection \nLoudness \nSpec. centroid \nSpectral flux \nAudio \nManual segmentation \nPredominant melody comp. \nHarmonic model analysis-synthesis that each school has developed its own specific reper-toires, we have also searched for arias with similar music structure. This is the case of fhc and sln, arranged in the same shengqiang and similar banshi.1 The resulting col-lection of recordings is described in Table 2, together with the abbreviations used throughout the paper. We ar-gue that the size of this collection is appropriate for the current research since we are not performing a quantita-tive study. Instead we are using MIR methodologies for supporting qualitative descriptions. Figure 2 describes the methodology proposed for this paper. Each of the recordings is ripped in a lossless com-pressed format with a sampling rate of 44.1 kHz. We manually identify the sections containing singing voice, for which we compute the predominant melody using the vamp plug-in version of Salamon and Gómez’s algorithm [15], setting a pitch range threshold of 100 Hz to 1000 Hz, and the voicing parameter at its maximum level, 3.0. Given that a percentage of error results, pitch tracks are manually corrected (an average of 7.16% from the com-puted frames). In order to measure pitch register, we use the methodology proposed in [16] to compute pitch his-tograms and obtain the pitch of the first degree2 from their peaks values. The algorithm presented in [17] is used to measure vibrato rate and extent, of which we cal-culate mean and standard deviation (SD). To measure the remaining features, we separate the singing voice from the accompaniment by computing a harmonic model analysis and synthesis using the methodology presented                                                            1 Shengqiang is the musical convention in jingju that determines the melodic skeleton; banshi refers to the metrical pattern. For more de-tailed information about these concepts, please refer to [1, 5]. 2 We use “first degree” to translate gongyin. This term refers to the first degree of the sung scale, and in jianpu notation is notated with the num-ber 1 (http://en.wikipedia.org/wiki/Numbered_musical_notation). Alt-hough common functions are shared, we consciously avoid the term tonic, for its implications with tonality, absent in jingju music. in [18], and apply to it standard algorithms for the com-putation of those features as implemented in the Essentia library [19]. For loudness we use the Loudness algorithm, normalizing the resulting mean to a factor of 0.5, so that SD is better comparable. To measure brightness, we compute mean and SD of the spectral centroid using the Centroid algorithm. In order to better understand timbre qualities, we also compute tristimulus using the Tristimu-lus algorithm, and long-term-average spectrum (LTAS) using the implementation presented in [20]. Finally, to measure timbre variability, we compute spectral flux mean and SD using the Flux algorithm from Essentia. In the following section, we analyse the results ob-tained for each school and relate them with their corre-sponding musicological characteristics. School Work: Play. “Aria” (Character) Recording: MusicBrainz ID Length Artist Mei fhc: Feng huan chao. “Ben ying dang sui muqin Haojing bi nan” (Cheng Xue’e) fhc-LYf: a1e4b77b-88b0-4003-b688-66e39f579dc6 7:33 Li Yufu fhc-SYh: 4e3b46b2-9db7-4f52-af95-e43239a6c0e1 6:56 Shi Yihong fhc-LSs: 83d2fc7f-e1c1-4359-b417-ed9e519ecbb7 7:34 Li Shengsu ssqj: Su San qijie. “Yu Tangchun han bei lei mang wang qian jin” (Su San) ssqj-LSs: 067b8f25-888a-4a08-a495-cbc402846b10 7:15 Cheng ssqj-CXqd: 87dbdf41-37ff-4f4a-83d4-7169d674579a  6:20 Chi Xiaoqiu sln: Suo lin nang. “Chunqiu ting wai feng yu bao” (Xue Xiangling) sln-CXq: 11a44af7-e29a-4c50-aa38-6139d37ca306  3:21 sln-LPh: 3dcae41a-795c-4b7d-979b-1b52aa42dd3a  3:06 Li Peihong sln-LGj: 1e705224-0b44-48aa-a0de-6386cda9d517 3:15 Liu Guijuan Table 2. Description of the recordings used in this paper. When applicable, short forms are provided. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 509   \n4. ANALYSIS OF THE RESULTS Table 3 shows a summary with the average measurement values from each of the features computed for each school.1 In this section we analyse how these results re-late to the musicological descriptions of their corre-sponding musical characteristics. According to our musicological references, pitch reg-ister in Mei is higher than in Cheng. Since pitch range of arias for the dan role-type is consistent across plays, ap-proximately an octave and a major third, we take the pitch of the first degree as an indicator of pitch register. However, this degree is rarely sung in arias of this role-type. This is due to one singing convention, according to which female role-types shift their pitch register a fifth higher than male role-types, so that the modal center be-comes the fifth degree. To measure the pitch of the first degree hence, we compute a pitch histogram and assign a modal degree to each peak by listening to the recordings with the aid of scores. Since we observed that the peak corresponding to the sixth degree is usually the cleanest one, we take it as reference by assigning the value of 900 cents. Figure 3 shows the resulting pitch histograms for ssqj-LSs and ssqj-CXq, compared with the equal tempered scale. It has to be noted that in jingju there is no absolute standard pitch for tuning, but it depends on the actor’s or actress’ needs. Notwithstanding this, a pitch is commonly assumed as reference for each role-type; for the dan role-type first degree is expected to be around E4 (329.63 Hz) [21]. Results in Table 3 show that this is the case for both schools, although first degree in Cheng is in average 6.28 Hz (33.30 cents) lower than E4, and Mei 5.78 Hz (30.09 cents) higher. Consequently, first degree in Cheng is in average 63.39 cents lower than Mei, more than a semitone. Results for all the re-cordings show that in every case first degrees from Mei are higher than those for Cheng, although the smallest difference between recordings from each school is 1.31 Hz. These results hence invite us to support the musico-logical description for pitch register. Besides the aforementioned results, Figure 3 suggests that pitch histograms can shed light upon other aspects of singing style. Chen [22] has used histograms to study                                                            1 Detailed results and more plots can be found in https://github.com/jingjuschools/jingjuschoolsISMIR2015  that, as Figure 3 shows, compared with the equal tem-pered scale the fourth degree, although seldom used, is sung at a higher pitch, what is common knowledge in musicological literature. Unexpectedly, the higher octave of the first degree appears in the histograms slightly shifted higher, especially in Mei school, for which we have not found literary evidence. Besides, peak shape differences, cleaner and with lower valleys for Cheng, also suggest differences in singing style, probably re-garding vibrato and ornamentation. These observations invite us to argue that pitch histograms could be further exploited for the characterisation of singing style and ex-plore features unnoticed or not explicitly accounted for in our references.2 According to the sources consulted, Cheng “excels in \n                                                           2 Differences in peaks height indicate different melodic preferences in each school, what concerns tune arrangement, an issue not considered in this paper. School Features  1st deg. (Hz) Vibrato Loudness Spectral centroid (Hz) Spectral flux Rate (Hz) Extent (cents) Mean SD Mean SD SD Mean SD Mean SD Mei 335.41 4.757 0.728 137.325 37.466 0.279 2536.739 366.968 0.121 0.063 Cheng 323.35 6.090 0.963 111.101 41.157 0.387 2136.555 451.642 0.087 0.058 Table 3. Average measurement values from each of the features computed for each school. \n \n Figure 3. Pitch histograms for ssqj-LSs (top) and ssqj-CXq (bottom). Vertical lines show the equal tempered scale, solid red line marks the first degree, and dotted red line marks its higher octave. \n510 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   \nusing slow and fast vibratos” [9]. Consequently, the fea-ture that can better reflect this characteristic is the stand-ard deviation of vibrato rate. As can be seen in Table 3, this value is higher in Cheng than in Mei, and this is also the case for every recording. However, the difference is less than 0.3 Hz, what is barely appreciable by human ear. Variability in vibrato extent, as reflected in our re-sults, is slightly higher in Cheng than in Mei, but the dif-ference in this case is even less significant. Besides, spe-cific results from each recording are less consistent, since some instances from Mei school show higher SD in vi-brato extent than others from Cheng, and vice versa. What our results clearly show though, is that vibrato in Mei is considerably slower and wider than in Cheng, a feature that is consistent across all the recordings. Inter-estingly enough, we have not found such a remark in our musicological sources. Results obtained for loudness variance also support musicological description, which takes volume variabil-ity as a characteristic of Cheng school compared to Mei. Results for each recording are less consistent than for other features, finding one case in Mei with higher SD than the lowest value for a recording in Cheng. These results, however, ought to be taken carefully. Firstly, they might have been affected by possibly different mix-ing levels in the production process. Secondly, being loudness a perceptual feature, the algorithm used is an approximation to it by a simple modification of ampli-tude values, what prevent us to take them as a faithful representation of the characteristic measured. Our musicological references agree that timbre in Mei school is brighter than in Cheng. To measure brightness, we have computed spectral centroid mean and SD. Val-ues for the mean effectively show that Mei has brighter timbre than Cheng, as an average and for every recording in each school. Given the complexity of timbre, we have also looked at LTAS, a feature that has been used in [23] to study and compare vocal tract and formant structure. Figure 4 shows the LTAS for the three performances of fhc and sln. To focus on the region with greater loud-ness, plots show the region between 200 Hz and 5000 Hz, with a logarithmic scale in the x-axis. In these plots it can be observed how frequencies over 1000 Hz are considerably higher in Mei than in Cheng, as well as the peaks with the highest loudness, contributing thus to timbre brightness. Besides this information, LTAS al-lows us to compare vocal tract between performers in each school, so that we can observe that timbre similarity is higher in Cheng than in Mei. This method hence seems promising in order to characterise individual ac-tors’ or actresses’ particularities within the overall re-quirements of the school. Tristimulus has also been used to study and compare timbre qualities [24]. Figure 5 shows that in average both the second and the third of the three output components measured by tristumulus have higher values for Mei than for Cheng, what once more support the higher weight of higher partials in Mei. This figure also suggests a prom-ising tool for classification according to timbre quality.  Finally, results for spectral flux, computed with the aim of measuring timbre variability, show a greater value in Mei than in Cheng, a difference which is consistent across all the recordings, with special homogeneity in Mei. Literature however remarks Cheng’s timbre varia-bility as a defining trait of this school. It is also interest-ing to notice that the SD value for spectral centroid also shows a higher value in Cheng, although is not as con-sistent as the spectral flux values across all the record-ings. Since this descriptor was computed only for the sections of the recordings that contained singing, we re- \n Figure 4. LTAS for the three recordings of fhc (top)  and the three recordings of sln (bottom). \n Figure 5. Scatter plot displaying values for the 2nd and 3rd tristimulus components for each recording and the mean for each school. \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 511   computed it setting different loudness thresholds for the transition frames in order to discard an influence of the segmentation. Results didn’t change any tendency to-wards a higher value in Cheng than in Mei. How to ap-proach timbre variability in jingju singing using audio analysis features remains hence an issue for future re-search, as discussed in the following section. 5. DISCUSSON The analysis of the results presented in the previous sec-tion suggests that the methodology proposed in this paper is promising for the intended task, namely, supporting musicological description using audio analysis features. However, there are also some challenges that have to be addressed when extending this research in future work.  We aim to improve our methodology in the following two senses: automatizing most of the steps for audio analysis and improving existing algorithms to better fit jingju music characteristics. Some researchers in our team are currently working on improving the automatic segmentation method by Chen [22], adapting Ishwars’ methodology [25] to jingju music for better extraction of predominant melody, and developing new algorithms for automatic computation of the first degree. Improvement of the harmonic model analysis and synthesis as present-ed in [18] is also to be undertaken in the near future. Arguably, the bigger challenge for the continuation of this research is gaining the engagement of jingju musi-cologists. In this paper we have aimed to show how the present approach can benefit musicological work. Yet to that aim we have consciously avoided most of the termi-nology that is more commonly used by experts when de-scribing singing style. The authors have not yet agreed on a methodology from an MIR point of view for approach-ing descriptions such as “sweet” (tian), “mellow” (run), “fragile” (cui), “round” (yuan), or “wide” (kuan). Even when considering a characteristic like timbre variability, whose study by means of spectral flux disagrees with musicological descriptions, we wonder how much of this disagreement is due to difficulties in establishing a com-mon terminology between these two disciplines. From the field of MIR there have been recent calls for a better un-derstanding of the musical content of commonly comput-ed descriptors [26]. The authors argue that collaborative research as the one undertaken here would also encourage jingju experts to reflect on their terminology in terms of audio analysis features, and hopefully would gain com-plementary precision for those concepts. Besides supporting musicological research, the use of audio features for qualitative analysis can be exploited for educational purposes. Jingju is a tradition that relies on oral transmission for training young actors and actresses. The key method in this training tradition consists on “teaching by mouth and heart” (kou chuan xin shou), that is, the teacher sings and the student repeats as many times as needed for achieving an acceptable standard. Recently, new technologies are being used as part of this process. Students use their cell phones to record their teachers, and audio and video recordings of performances are easi-ly accessible in the web. Technologies that could auto-matically evaluate the degree of similarity between the teacher’s and the student’s performance, and moreover offer a precise description of dissimilarities, would guide the trainee in better understanding his or her own learning process. The aim of such technologies would be perform-ing qualitative analysis of audio recordings, similar to the ones implemented in this paper. Building upon the results obtained and the methodology tested in the current work, we have started to develop such educational tools. To this goal we will require closer collaboration with jingju ex-perts. The involvement of these experts and the ac-ceptance of the educational tools by jingju trainees will also provide better evaluation methods for our research. 6. CONCLUSIONS The current paper has studied the potential of using audio analysis features for supporting and enhancing the musi-cological description of singing style in two jingju schools for the dan role-type, namely Mei and Cheng. Our results support the description given in musicological literature for most of the characteristics analysed, and add precision to them. Pitch register in Cheng school is in av-erage 63.39 cents lower than Mei. Variability in vibrato rate is slightly higher in Cheng, what agrees with musico-logical description, but less than 0.3 Hz. Volume variabil-ity as a characteristic of Cheng school has been supported by the measure of loudness, whose SD is 38.71% higher in this school than in Mei. That this school is brighter in timbre than Cheng is supported by the mean value of its spectral centroid, 18.73% higher, but also by measure-ments in LTAS and tristimulus. Besides supporting these descriptions, our method also suggests some characteris-tics not explicitly accounted for in the sources consulted: vibrato in Mei is in average 1.33 Hz slower and 26.22 cents wider than in Cheng, pitch histograms suggest new characterisations for tuning and intonation, and LTAS looks promising for comparing vocal tracts of singers within a school. Only in the case of spectral flux, com-puted for studying timbre variability, the results do not support the musicological description, an issue that will be addressed in future research. In the light of these re-sults, we have started to extend this methodology for the development of educational tools, a project in which we hope to gain the engagement of jingju experts, who could benefit from this approach for their own research. 7. ACKNOWLEDGEMENTS This research is funded by the European Research Coun-cil under the European Union’s Seventh Framework Pro-gram, as part of the CompMusic project (ERC grant agreement 267583). 512 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   8. REFERENCES [1] R. Caro Repetto, and X. Serra: “Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis,” ISMIR 2014, pp. 313-318, 2014.  [2] A. Srinivasamurthy, R. Caro Repetto, H. Sundar, and X. Serra: “Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera,” ISMIR 2014, pp. 431-436, 2014.  [3] S. Zhang, R. Caro Repetto, and X. Serra: “Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing,” ISMIR 2014, pp. 343-348, 2014. [4] M. Tian, G. Fazekas, D. Black, and M. Sandler: “Design and Evaluation of Onset Detectors Using Different Fusion Policies,” ISMIR 2014, pp. 631-636.  [5] E. Wichmann: Listening to Theatre: The Aural Dimension of Beijing Opera, University of Hawaii Press, Honolulu, 1991.  [6] H. Li 李海涓: “Jingju qingyi ‘Mei’ ‘Cheng’ er pai zai changfa shang de tong yu yi” 京剧青衣“梅”“程”二派在唱法上的同与异 (Similarities and differences in the singing techniques between the two schools of jingju qingyi Mei and Cheng), Zhejiang yishu zhiye xueyuan xuebao, Vol. 11, No. 1, pp. 50-55, 2013.  [7] R. Wang 汪人立: “Mei pai changqiang yinyue de meixue pinge” 梅派唱腔音乐的美学品格 (Character of music aesthetics in Mei school’s singing), Yishu bai jia, 1996, No. 1, pp. 40-47. [8] T. Wu 吴同宾, and Y. Zhou 周亚勋: Jingju zhishi cidian 京剧知识词典 (Dictionary of jingju knowledge), Tianjin renmin chubanshe, Tianjin, 2006. [9] S. Yu 俞淑华: “Chuyi Chengpai de changqiang yu banzou” 刍议程派的唱腔与伴奏 (My humble opinion about Cheng school’s singing and instrumental accompaniment), Zuojia zazhi, 2012, No. 1, pp. 209-210. [10] S. Yu 俞淑华: “Lun jingju ‘si da ming dan’ de changqiang yinse” 论京剧“四大名旦”的唱腔音色 (Discussing singing timbre in jingju’s ‘four great dan actors’), Ming zuo xinshang, 2011, No. 33, pp. 114-115. [11] J. Sundberg, L. Gu, Q. Huang, and P. Huang: “Acoustical study of classical Peking Opera singing,” Journal of Voice, Vol. 26, No. 2, pp. 137-143, 2012. [12] L. Yang, M. Tian, and E. Chew: “Vibrato characteristics and frequency histogram envelopes in Beijing opera singing,” 5th International Workshop on Folk Music Analysis, pp. 139-140, 2015. [13] L. Dong, J. Sundberg, and J. Kong: “Loudness and Pitch of Kunqu Opera,” Journal of Voice, Vol. 28, No. 1, pp. 14-19, 2014. [14] L. Dong, J. Kong, and J. Sundberg: “Long-term-average spectrum characteristics of Kunqu Opera singers’ speaking, singing and stage speech,” Logopedics Phoniatrics Vocology, Vol. 39, No. 2, pp. 72-80, 2014. [15] J. Salamon, and E. Gómez: “Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics,” IEEE Transactions on Audio, Speech, and Language Processing, Vol. 20, No. 6, pp. 1759-1770, 2012. [16] G. K. Koduri, V. Ishwar, J. Serrà, X. Serra, and H. Murthy: “Intonation analysis of ragas in Carnatic music,” JNMR, Vol. 43, No. 1, pp. 72-93. [17] P. Herrera, and J. Bonada: “Vibrato Extraction and Parametrization in the Spectral Modeling Synthesis Framework,” DAFx, 1998. [18] X. Serra, and J. Smith: “Spectral Modeling Synthesis: A Sound Analysis/Synthesis Based on a Deterministic plus Stochastic Decomposition,” Computer Music Journal, Vol. 14, No. 4, pp. 12-24, 1990. [19] D. Bogdanov, N. Wack, E. Gómez, S. Gulati, P. Herrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and X. Serra: “ESSENTIA: an Audio Analysis Library for Music Information Retrieval,” ISMIR 2013, pp. 423-498, 2013. [20] T. Kinnunen, V. Hautamäki, and P. Fränti: “On the Use of Long-Term Average Spectrum in Automatic Speaker Recognition,” Proceedings of the 5th International Symposium on Chinese Spoken Language Precessing, pp. 559-567, 2006. [21] B. Cao 曹宝荣: Jingju changqiang banshi jiedu: Xia ce 京剧唱腔板式解读·下册 (Deciphering banshi in jingju singing: Second volume), Renmin yinyue chubanshe, Beijing, 2010. [22] K. Chen: Characterization of Pitch Intonation of Beijing Opera, Master thesis, Universitat Pompeu Fabra, Barcelona, 2013. [23] J. Sundberg: The Science of the Singing Voice, Northern Illinois University Press, Dekalb, 1987. [24] M. Campbell, and C. Greated: The Musician’s Guide to Acoustics, Oxford University Press, Oxford, 1987. [25] V. Ishwar: Pitch Estimation of the Predominant Vocal Melody from Heterophonic Music Audio Recordings, Master Thesis, Universitat Pompeu Fabra, Barcelona, 2014. [26] B. Sturm: “A Simple Method to Determine if a Music Information Retrieval System is a ‘Horse’,” IEEE Transactions on Multimedia, Vol. 16, No. 6, pp. 1636-1644, 2014. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 513"
    },
    {
        "title": "Image Quality Estimation for Multi-Score OMR.",
        "author": [
            "Dan Ringwalt",
            "Roger B. Dannenberg"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414890",
        "url": "https://doi.org/10.5281/zenodo.1414890",
        "ee": "https://zenodo.org/records/1414890/files/RingwaltD15.pdf",
        "abstract": "Optical music recognition (OMR) is the recognition of im- ages of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve ac- curacy. As a simpler alternative, we have developed fea- tures which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Fur- thermore, quality may be used to weight each score in an alignment, which should improve existing systems’ robust- ness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but significantly corre- lated with the true accuracy. Improved features should be able to produce highly consistent results.",
        "zenodo_id": 1414890,
        "dblp_key": "conf/ismir/RingwaltD15",
        "keywords": [
            "Optical music recognition",
            "aligning results",
            "multi-score OMR",
            "predicting score quality",
            "selecting highest-quality score",
            "weighting scores in alignment",
            "commercial OMR software",
            "test set of MIDI recordings",
            "correlated with true accuracy",
            "improved robustness"
        ],
        "content": "IMAGE QUALITY ESTIMATION FOR MULTI-SCORE OMRDan Ringwalt, Roger B. DannenbergCarnegie Mellon UniversitySchool of Computer Scienceringwalt@cmu.edu,rbd@cs.cmu.eduABSTRACTOptical music recognition (OMR) is the recognition of im-ages of musical scores. Recent research has suggestedaligning the results of OMR from multiple scores of thesame work (multi-score OMR, MS-OMR) to improve ac-curacy. As a simpler alternative, we have developed fea-tures which predict the quality of a given score, allowingus to select the highest-quality score to use for OMR. Fur-thermore, quality may be used to weight each score in analignment, which should improve existing systems’ robust-ness. Using commercial OMR software on a test set ofMIDI recordings and multiple corresponding scores, ourpredicted OMR accuracy is weakly but signiﬁcantly corre-lated with the true accuracy. Improved features should beable to produce highly consistent results.1. INTRODUCTIONOptical music recognition (OMR) is the problem of con-verting scanned music scores into a symbolic format suchas MIDI. The advantages of OMR for computer music ap-plications are clear, but it has yet to be widely used in manyapplications which use MIDI or MusicXML scores. Al-though OMR has been studied extensively since the 1960s,no OMR system has near-perfect accuracy. Commonly, theoutput of an OMR system must be checked by hand and atleast a few corrections must be made, making the processextremely time-consuming [2]. This limits the amount ofmusic which may be digitized, and in fact, much music isstill digitized completely by hand in sources such as theMutopia Project [1]. Recent research has focused on usingcontextual information beyond what is present on a singlepage to improve OMR results.Recently, the Petrucci Music Library (or InternationalMusic Score Library Project, IMSLP) [17] has become ahigh-quality source of public domain music scores. Thesite allows users to scan and upload scores. Therefore,there may be several scores of the same work, which maybe musically identical, or different editions, arrangements,or parts. There is a large discrepancy between the scan-ning equipment each user has, along with their relative carec\u0000Dan Ringwalt, Roger B. Dannenberg.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Dan Ringwalt, Roger B. Dannenberg.“Image Quality Estimation for Multi-Score OMR”, 16th International So-ciety for Music Information Retrieval Conference, 2015.in scanning, so image quality varies widely. At the timeof writing, IMSLP contains over 90,000 works, for whichthere are over 300,000 uploaded scores.Although many OMR errors are due to notational com-plexity [2], we expect at least some mistakes to be due torandom deformation in the score, independent of the con-tent. Then if multiple scores are available correspondingto one piece, a consensus built from OMR applied to eachscore should be more accurate than any one score. Thepossibility of aligning multiple scores of the same work tobuild a single result (multi-score OMR, MS-OMR) is al-ready being explored [21].However, scores available from IMSLP and othersources vary widely in noise introduced in the scanningprocess. Previous work on multi-recognizer OMR (MR-OMR), where the results are aligned from several OMRsystems on the same score, has noted that a consensus re-sult using simple voting may be worse than the result ofthe best recognizer [4]. Similarly, if there are several poorscores for a work and one good score, a MS-OMR resultmay be worse than the result on the highest-quality scorealone. An MS-OMR system that correctly estimates thequality of each score and acts accordingly should over-come this limitation.Formally, we want to predict some accuracy measure ofOMR, given features extracted from an image. We deﬁnethequalityof an image to be the predicted accuracy givenby our resulting model. Quality should depend on factorssuch as random noise, deformation of the page, and resolu-tion, and is expected to be correlated with OMR accuracy.Our predicted value is mostly useful in comparisons be-tween scores; even if the actual accuracy is on a0to1scale, a quality value learned using linear regression maybe outside this range for some scores, and so it may notbe interpretable as an accuracy value. However, even if weevaluate multiple recognizers using the same methodology,then we can learn a separate quality value for each recog-nizer, and predict the best-performing recognizer for a newscore.Clearly, the quality value gives useful information to aMS-OMR system. We may want to throw out some scoresaltogether if their quality is too low, as they may not con-tribute much of a beneﬁt in addition to the higher-qualityscores. As a simpliﬁcation, we may only take the highest-quality score, and perform normal OMR. If our qualityvalue is accurate, then this is the safest approach, becauseby introducing other scores, we risk lowering the accuracy.17This is clearly less computationally expensive than obtain-ing and aligning multiple OMR results, but should result inmuch higher accuracy than randomly choosing any avail-able score. We consider this approach to MS-OMR in thispaper.2. RELATED WORK2.1 Multi-Recognizer and Multi-Image OMRRecent research has focused on improving OMR accuracyby aligning several OMR results and building a consensusscore. Byrd and Schindele [5] designed a multi-recognizerOMR system which applies multiple OMR systems to thesame score, and resolves conﬂicts between results usingpre-deﬁned rules. This is built on the assumption that eachOMR system will have particular situations in which itoutperforms the other systems. As OMR systems are un-der development and their strengths and weaknesses maychange, a system is proposed which automatically learnsthe performance of each system in different possible situa-tions.More recently, Bugge et al. [4] proposed another multi-recognizer OMR system that resolves conﬂicts betweeneach recognizer by a simple majority vote. Scores are ex-ported as MusicXML from each recognizer, and convertedto a custom subset of MusicXML, “MusicXiMpLe,” whichonly stores the information necessary to decode note pitchand duration.Padilla et al. have suggested extending multiple-recognizer OMR to align the results from multiple imagesof the same score [21]. A method is proposed to proﬁle theresponse of each OMR tool to score quality, by adding ad-ditional noise to existing scores with available ground truthand measuring OMR accuracy.2.2 Image QualityExisting measures have been designed to estimate the levelof degradation present in an image due to the scanning pro-cess. Kanungo et al. developed a local distortion model(referred to asKanungo noise) for binary images which isan extension of simple salt-and-pepper noise, and uses6parameters [15]. The additional parameters capture the in-creased noise near the boundary between black and whitepixels, and correlation in noise between nearby pixels.Kanungo et al. previously estimated the Kanungo noiseparameters of a binary image of a text document [16]. Theestimation requires an ideal set of synthetic text documentswith similar font face and size to the scanned image. Givenan estimated set of parameters, each ideal image is de-graded using the parameters. All 3x3 square patterns arefound in each degraded ideal image and the input image,and a histogram for the count of each of23⇤3= 512pat-terns is made for the degraded ideal images and the input.A Kolmogorov-Smirnov test statistic is measured betweenthe cumulative distribution functions of both histograms.This statistic is minimized using the Nelder-Mead simplexmethod [19].Additionally, prior work in OMR has focused on un-doing global distortions present in the input image. Thelevel of distortion detected by these methods is another fea-ture which should be negatively correlated with OMR ac-curacy. For example, Fujinaga’s staff detection algorithm[12] tries to correct bending of the staves due to page curl.Thisdeskewingprocess translates each column of the im-age to make the staff lines more horizontal. We use themean vertical translation performed by deskewing as onefeature.We may also robustly estimate the resolution of an im-age using the distance between staff lines. Unlike the ac-tual size of the image, this does not depend on the sizeof the original page, and all symbols such as notes willbe directly proportional to the stafﬂine distance. We useCardoso et al.’s robust estimated stafﬂine distance [7] asanother feature.3. METHODS3.1 Data AcquisitionAll available scores of Ludwig van Beethoven’s pianosonatas were obtained from IMSLP. In total, there were32sonatas, with285different scores.MIDI versions of several movements from theBeethoven piano sonatas were obtained from the MutopiaProject [1], and served as ground truth to compare with theOMR results. The MIDI version was automatically gen-erated from a manually transcribed LilyPond [20] sourceﬁle.As the MIDI ﬁles are separated by movement, thescores were also split into each movement. Therefore, eachworkis deﬁned to be a single movement of a sonata.3.2 Score PreprocessingThe scores were preprocessed by a custom system beforeextracting image quality features and performing OMR.Our methods for rotation correction and staff and staff sys-tem detection are described in [25].Many scores had movements which started in the mid-dle of the page. Therefore, the staff systems which formedthe start of each movement were labeled by hand. Our sys-tem was used to automatically segment pages as necessaryto split the score into movements.We kept67original scores from IMSLP which con-tained an entire sonata and were not an arrangement orother version, and had ground truth for at least one move-ment available from the Mutopia Project. We success-fully generated and processed95single-movement scoresfor16works (single movements), belonging to8differentsonatas.3.3 Image Quality FeaturesKanungo parameter estimation was performed on each pre-processed page. A page from a LilyPond-engraved scoreobtained from the Mutopia Project was used as the idealimage. Each image was scaled to a normalized stafﬂine18 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 \n14 The interesting levels of describing CWMN begin with what Bellini e t  a l  (2007) call basic s y m b o l s  (graphic elements: noteheads, flags, the letter “p”, etc.; these have no meaning by themselves) and composite o r  complete s y m b o l s  ( t h i n g s  w i t h  s e m a n t i c s :  e i g h t h  n o t e s  w i t h  f l a g s ,  b e a m e d  e i g h t h  n o t e s ,  chords, dynamic marks like “pp” and “mp”, etc.). There is only a relative handful of basic symbols, but a huge number of composite symbols. Droettboom & Fujinaga comment: In many classification problems the evaluation metric is fairly straightforward. For example at the character level of OCR, it is simply a matter of finding the ratio between correctly identified characters and the total number of characters. In other classification domains, this is not so simple, for example document segmentation, recognition of maps, mathematical equations, graphical drawings, and music scores. In these domains, there are often multiple correct output representations, which makes the problem of c o m p a r i n g  a  given output to high-level ground truth v e r y  d i f f i c u l t .  I n  f a c t ,  i t  c o u l d  b e  a r g u e d  t h a t  a  complete and robust system to evaluate OMR output would be almost as complex and error-prone as an OMR system itself. Symbol-level analysis may not be directly suitable for comparing commercial OMR products, because such systems are usually “black boxes” that take an image as input and produce a score-level representation as output. Note particularly the last statement; their “symbols” are likely identical to the Bellini et al’s “basic symbols”, but their “score-level representation” is clearly a level above Bellini et al’s “composite-symbol-level representation”. In fact, these problems of OMR directly reflect the intricate semantics of music notation. The last marked note in Figure 2(c) i s  a  c o m p o s i t e  s y m b o l  c o n s i s t i n g  o f  n o t e h ead, stem, and flags. Its duration of a 16th note is clear just from the composite symbol. However, seeing that its pitch is (in ISO notation) E3 r e q u i r e s  t a k i n g  i n t o  a c c o u n t  the four factors described under “Context and pitch notation” in Section 4 above. Instead of “basic” and “composite” symbols, many authors have spoken of low-level symbols and high-level symbols, and we prefer the latter terminology. Table 1 lists the levels of description of CWMN that we have mentioned.  Level Bellini et al. term Examples Has semantics? pixel --  no low-level symbol basic symbol noteheads, flags, the letter “p” no high-level symbol composite symbol 8th notes with flags, chords, dynamic marks “p” and “pp” yes score -- Schumann: Fantasiestücke, Henle ed. yes  Table 1. Levels of description of CWMN 5.1.2 Levels of Description and Error Rates Now consider Figure 12 (from Reed, 1995, p. 73).  Figure 12. A minor problem at low level, but a serious problem at high level In this case, the high-level symbols, i.e., symbols with semantics, are the clef, time signature, notes—64ths in Figure 12(a), 32nds in 12(b)—and slur. Each of the clef and the slur is a single low-level symbol. But the time signature and notes are comprised of multiple low-level symbols: for the former, digits; for the latter, noteheads and beams (and, in other cases, flags, accidentals, augmentation dots, etc.). Reed points out that in this example (ignoring the clef and time signature), the only problem in the reconstructed score is a single missing beam, and if low-level symbols are counted, 19 of 20 (95%) are Figure 1. This error only counts as one error under low-level evaluation, but several under high-level evaluation, asthe length of each note is incorrect. Source: [24]distance value of8. Kanungo noise parameters were es-timated using the SciPy [13] implementation of Nelder-Mead optimization [19], as described in Section 2.2.Nelder-Mead was run10times starting from a uniformlyrandom parameter distribution, and was stopped after50function evaluations each time. The resulting Kanungo pa-rameters(⌫, ↵0,↵ ,\u00000,\u0000,k)were used as features to pre-dict OMR performance.We also performed Fujinaga’s staff detection algorithm,which skews the image to correct page curl. This gives usthe amount of page curl in the original image. We use themean vertical translation performed by this deskewing asone feature, which represents the degree of distortion in thepage.Finally, we used Cardoso et al.’s robust stafﬂine distanceestimation method [7]. We used the stafﬂine distance, andthe ratio of stafﬂine thickness to distance, as two more fea-tures. The stafﬂine distance represents the resolution of theimage, while the thickness-to-distance ratio represents therelative thickness of lines on the page.3.4 OMRThe preprocessed movements were processed by theSharpEye 2 OMR system, version 2.68. The result wasexported to MIDI.4. EVALUATION4.1 OMR Evaluation MethodsOMR researchers have yet to adopt any evaluation met-ric as a common standard [6], and specialized evaluationmethods will likely be needed for most systems. We choseas basic of an evaluation method as possible: simply com-paring the start time of each note to the ground truth. Thisstill requires rests, accidentals, and other basic symbols tobe detected correctly in the usual case; it cannot detect atoo-short note followed by a too-long rest, but this partic-ular error should be extremely rare. Although it does nottest other information like dynamic markings, we considerthese to be of secondary importance compared to the actualnotes. As we only consider the start position of each note,and not the duration of notes and rests, our evaluation is afurther simpliﬁcation of previous evaluations, which con-sider both the start and end of notes [4, 14].SharpEye 2 outputs a proprietary .mro format whichcontains information such as the position of some individ-ual symbols. Therefore, it is possible to conduct alow-levelevaluation if the score is labeled with the position ofeach symbol. Although both values should be highly cor-related, high-level accuracy may decrease drastically withonly a small decrease in low-level accuracy, as illustratedin Figure 1.Our evaluation method is considered high-level. Thisallows us to use MIDI recordings from the MutopiaProject, which only contain the actual notes, as our labeleddata. One potential issue with MIDI is that to simulate arealistic performance, staccato notes may have a shortenedlength followed by a rest for their remaining time. Ourevaluation, which only tests the start of each note, accountsfor this.4.2 Accuracy ValueGiven two aligned scores, we need to derive a single valuefor the accuracy. Here, each note is represented as the timein the score, and a pitch, and a note is correctly detectedif there is a note with the exact same values in the originalscore. The OMR output may contain both false positives,where a note is accidentally detected, and false negatives,where a note is missing. We may calculate the precisionp, which is the proportion of true positives to all detectednotes, and the recallr, which is the proportion of true posi-tives to all notes in the original score. The standard methodof combining these values, which we use as our accuracyvalue, is theF1score:F1=2prp+r4.3 MIDI-MIDI AlignmentAll MIDI ﬁles were imported into Python usingmusic21[8]. Next, we aligned each OMR output to theground truth, to correct for missing or extra measures dueto OMR errors. We noticed that LilyPond’s MIDI output(used by Mutopia) pads a pickup measure to the length ofa full measure, while SharpEye 2’s does not. Therefore,we align each beat rather than each measure, so that thepickup will also be correctly aligned.The standard alignment algorithm, used in both bioin-formatics and computer music applications, is Needleman-Wunsch [18, 3]. It minimizes the sum of the distancebetween each aligned element of two sequences, plus apenalty for each inserted gap. In our case, our distancematrix has one row for each beat in the real score, and onecolumn for each beat in the OMR score. The distance entryfor each pair is1\u0000F1for the pair of beats, multiplied bythe maximum of the number of notes in both beats. (This isimplicitly0when both beats only contain rests, and theF1score would normally be undeﬁned.) We use a gap penaltyof10.After Needleman-Wunsch, we simply calculate theF1score for the entire aligned scores, with new positions forthe notes accounting for inserted gaps. This is our OMRaccuracy value.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 19Figure 2. The OMRF1score for each movement com-pared with the predicted accuracy, with the best-ﬁt line.4.4 Quality EstimationWe used a linear model to predict the OMRF1score givenour features. We chose the Scikit-learn [22] implementa-tion of Support Vector Regression with a linear kernel, asit seemed to perform better than ordinary least squares lin-ear regression. The model was validated by leave-one-outtesting on each work: for each work, a model was trainedexcluding its corresponding scores, and the predicted bestscore for the work was compared to the score with thehighest accuracy. Finally, we ﬁt the model to the entiredataset to determine the coefﬁcients.5. RESULTSThe OMRF1score and predicted accuracy were weaklybut signiﬁcantly correlated (R=0.30,p=0.0029). Thedata is shown in Figure 2.For each work, we compared the score with the high-est OMR accuracy and the score with the highest predictedquality using leave-one-out testing (Table 1). Six of the 16works had a correctly predicted best score, whereas usinguniformly random guessing, the expected number of cor-rect scores is only2.82. The full OMR accuracy results arepresented in Table 2.We also noted that the best few scores may all havenearly the same high accuracy. In these cases, it is notnecessary that our top predicted score has the highest ac-curacy, but the accuracy should be close to the highest.For each work, we considered the mean accuracy of allscores, which is the expected accuracy of a score selectedby random choice, the highest accuracy, and the accuracyof the predicted best score. The mean of the expected ac-curacy for random guessing is0.61, and the mean of thebest accuracy (the best possible result) is0.82, while themean accuracy of the best predicted scores is0.74. Thechosen score’s accuracy was higher than expected in 14 of16 cases. This conﬁrms that our method reliably outper-forms random guessing, but there is still room to improveWorkBest Pred. Best # Scores1.1IMSLP66390 IMSLP05524 81.4IMSLP66390 IMSLP05524 75.1IMSLP66394 IMSLP6639455.3IMSLP66394 IMSLP6639456.3IMSLP66395 IMSLP66395519.1IMSLP00019 IMSLP04073 619.2IMSLP05545 IMSLP69581 620.1IMSLP45469 IMSLP66410 720.2IMSLP05546 IMSLP05546723.2IMSLP51795 IMSLP04078 323.3IMSLP66412 IMSLP66412625.1IMSLP66414 IMSLP66414625.2IMSLP66414 IMSLP69588 625.3IMSLP66414 IMSLP69588 627.1IMSLP66416 IMSLP05553 627.2IMSLP69590 IMSLP05553 6Table 1. Accuracy predictions on the Beethoven pianosonata test set. For each work (identiﬁed bysonata num-ber.movement), we compare the score with the highestOMR accuracy (Best) and the highest predicted quality(Pred. Best).in choosing one of the best scores.The coefﬁcients of our linear model (Table 3 in the ap-pendix) are directly interpretable as the effect each pa-rameter has on OMR accuracy. Many results were un-expected. For example,⌫represents the probability ofsalt-and-pepper noise in the Kanungo model, which shouldnegatively affect OMR accuracy, but its coefﬁcient is posi-tive. However, as it is on a small scale (typically0\u00000.05),it has a smaller impact on accuracy. This result may bedue to a few outliers which had poor results for Kanungoestimation.The coefﬁcient for meanskew, which is the deforma-tion undone by Fujinaga’s deskewing, is also unexpect-edly positive. This may indicate a ﬂaw in our implemen-tation, or again, outliers. We did ﬁnd that staffdist is pos-itively correlated with accuracy, as we expect that higher-resolution scores will have better results. The coefﬁcient issmall, but more signiﬁcant as staffdist is on a larger scale(usually at least20).6. CONCLUSIONSWe introduced an estimated OMR accuracy measure, andshowed that its correlation to the true accuracy is statis-tically signiﬁcant. However, the correlation is too low tocorrectly predict the best-quality score a majority of thetime. On the other hand, this validates the use of featuresextracted from the image to select higher-quality scores.By reﬁning our features and adding additional ones, weshould be able to build a practical quality estimation sys-tem which can support multi-score OMR.Since most of our current image features are parame-ters for Kanungo noise, the success of the image qualityestimation is dependent on these parameters being accu-20 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015rate. The Kanungo estimation process is currently verytime-intensive, requiring around2minutes per page. Someinstances of Nelder-Mead will become stuck in a local op-timum, so repeating Nelder-Mead even more times shouldimprove results. However, the time involved made this im-practical in this case.7. FUTURE DIRECTIONS7.1 Image Quality FeaturesWe only found a weak correlation between OMR accuracyand predicted quality, and noted that the Kanungo parame-ter estimates were noisy. Furthermore, the estimation pro-cess is too slow to be practical for a large music librarysuch as IMSLP. Therefore, a better performing, faster Ka-nungo estimation process is needed to make image qualityestimation practical.We may be able to improve Kanungo estimation by us-ing assumptions speciﬁc to music scores, which would al-low us to test a much smaller area of the image. For ex-ample, if we ﬁnd all empty stretches of staff on the page,we can concatenate some of these as the input to Kanungoestimation. We may generate an ideal empty staff usingthe estimated stafﬂine distance and thickness. This uses amuch smaller image, and may even be more robust as dif-ferences in typography between the ideal and input imagewill not affect it.Finally, our features only take into account errors in-troduced in the scanning process. However, differences inthe original score, such as different fonts, should also af-fect the accuracy of a particular OMR recognizer.Adap-tiveOMR systems [11, 23] improve their performance onscores with a certain font and other particularities by learn-ing from their corrected output. If an adaptive system istrained using a homogenous set of scores with a particu-lar font, then we may be able to extract information aboutthe font from its classiﬁcation model. Features which havebeen used for handwritten music writer identiﬁcation [10]may be useful.7.2 OMR EvaluationWe mentioned that a small difference in low-level accu-racy may make a dramatic difference in high-level accu-racy. Therefore, low-level accuracy may be a more stablevalue to use when performing regression. However, ob-taining a real-world test set of a similar size with low-levelground truth would be much more time-consuming.Using scores from the Mutopia Project, it would be pos-sible to modify LilyPond to output the position of eachsymbol, giving us a low-level ground truth. Next, we couldapply deformations such as Kanungo noise to the outputbefore performing OMR. This is similar to Padilla et al.’sproposal to add additional noise to real images from IM-SLP to proﬁle each OMR recognizer. However, if we startfrom ideal computer-engraved images, then the parameterswe use to add noise to the image are exactly the same asour image quality features. Therefore, we may design ourtest set to cover the entire parameter space, and we candirectly learn our image quality function using regressionfrom the input parameters to the OMR accuracy for eachrecognizer.On the other hand, we may be able to improve our re-sults while keeping high-level accuracy. We may obtaina broader range of scores from IMSLP paired with MIDIrecordings from the Mutopia Project, which would provideus with more training data. Using more data, we couldtrain a more sophisticated model than linear regression,which would hopefully better predict accuracy. We notedthat a single error has a proportional effect in low-level ac-curacy but a much bigger effect on high-level accuracy, sohigh-level accuracy likely has a nonlinear relationship withquality. Therefore, methods such as kernel SVR or randomforests may be able to capture this nonlinear relation.We noticed that some MIDI scores were unable to beopened by music21, and they were excluded from the anal-ysis. This is believed to be because some note durationscannot be unambiguously converted from a ﬂoating-pointtime value back to the music-theoretic note values whichmusic21 uses. This should be possible to ﬁx by using theMIDI ﬁles in their original form, which would allow us toinclude more data in our analysis.7.3 Alignment-Based MS-OMRAlthough we presented our method as a simpler alterna-tive to existing MS-OMR systems, our image quality esti-mate may be used in a larger system. An MS-OMR sys-tem which aligns multiple results, as in [21], may be aug-mented by weighting each score by its quality in the vote.Furthermore, alignment-based MS-OMR systems require amultiple sequence alignment, and ﬁnding the globally op-timal such alignment is NP-complete [26]. Approximatemultiple alignment algorithms often use a series of pair-wise alignments [9]. Recent research in aligning multiplemusical recordings or scores used a progressive alignment,where pairwise alignments were performed sequentially onthe inputs [27, 4]. Ordering OMR results from highest tolowest quality may work better than other orders.We have demonstrated the usefulness of image qualityestimation in predicting OMR accuracy. A more robustquality estimate should be useful for any MS-OMR sys-tem. This should have a signiﬁcant impact on OMR accu-racy for large music libraries such as IMSLP.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 21WorkScore Accu. Qual.Score Accu. Qual.Score Accu. Qual.Score Accu. Qual.Score Accu. Qual.1.100001 0.95 0.6203796 0.70 0.7905524 0.95 0.8851707 0.57 Error66279 0.70 0.751.166390 0.96 0.7977993 0.61 0.5090564 0.08 0.18243106 0.84 0.831.400001 0.94 0.5603796 0.79 0.6605524 0.31 0.8251707 0.37 Error66279 0.78 0.851.466390 0.96 0.8677993 0.62 0.70243106 0.67 0.655.100005 0.92 0.8202412 0.08 Error03858 0.81 0.6951714 0.85 Error66394 0.96 0.835.168715 0.69 0.69243114 0.91 0.755.300005 0.17 0.6703858 0.49 0.4651714 0.18 Error66394 0.60 0.8768715 0.47 0.715.3243114 0.17 0.536.300006 0.41 0.7903859 0.34 0.8051715 0.40 Error66395 0.43 0.8568719 0.36 0.686.3243121 0.10 0.8019.100019 0.75 0.7204073 0.15 0.8905545 0.26 0.7645370 0.26 0.6251743 0.20 0.7319.166408 0.28 Error69581 0.72 0.62345618 0.27 Error19.200019 0.91 0.7504073 0.84 0.7405545 0.94 0.8445370 0.94 0.7851743 0.67 0.5719.266408 0.98 Error69581 0.93 0.93345618 0.94 Error20.100020 0.08 0.6604075 0.85 0.7505546 0.96 0.8345469 0.97 0.5251745 0.67 0.5420.166410 0.07 0.8269582 0.90 0.8520.200020 0.95 0.5904075 0.14 0.6405546 0.98 0.8845469 0.95 0.7451745 0.79 0.5020.266410 0.08 0.5069582 0.94 0.7023.203184 0.11 0.5104078 0.46 0.7051795 0.55 0.5923.300023 0.57 0.6703184 0.09 0.5504078 0.38 0.7205549 0.58 0.7851795 0.41 0.5323.366412 0.60 0.8625.100025 0.97 0.5203185 0.43 Error04081 0.80 0.8305551 0.96 0.7651797 0.88 0.6825.166414 0.98 0.8869588 0.26 0.8425.200025 0.84 0.7304081 0.73 0.5705551 0.95 0.8551797 0.74 0.6466414 0.99 0.6825.269588 0.87 0.8925.300025 0.92 0.5604081 0.66 0.6305551 0.96 0.7951797 0.74 0.7066414 0.96 0.7425.369588 0.94 0.8427.100027 0.88 0.7504090 0.79 0.8605553 0.90 0.8351799 0.48 0.7666416 0.91 0.8127.169590 0.70 0.7827.200027 0.27 0.6904090 0.21 0.7005553 0.27 0.7551799 0.18 0.6066416 0.29 0.7427.269590 0.51 0.55Table 2. OMR accuracy (F1) values for each score (by IMSLP ID), and predicted quality values.\nVariable CoefﬁcientVariable Coefﬁcient⌫4.2meanskew19.34↵01.7staffdist0.021↵0.10staffthickratio0.22\u00000\u00000.70\u0000\u00000.077k\u00000.0026Table 3. Coefﬁcients of the linear model for image quality.22 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]The Mutopia Project, 2015.http://mutopiaproject.org/ (accessed July, 2015).[2]D. Bainbridge and T. Bell. The challenge of opticalmusic recognition.Computers and the Humanities,35(2):95-121, 2001.[3]J. J. Bloch and R. B. Dannenberg. Real-time accom-paniment of polyphonic keyboard performance. InProceedings of the 1985 International Computer Mu-sic Conference, pages 279-290, 1985.[4]E. P. Bugge, K. L. Juncher, B. S. Mathiasen Jakob,and J. G. Simonsen. Using sequence alignment andvoting to improve optical music recognition frommultiple recognizers. InProceedings of the 12th In-ternational Conference on Music Information Re-trieval, pages 405-410, 2001.[5]D. Byrd and M. Schindele. Prospects for improvingOMR with multiple recognizers. InProceedings ofthe 7th International Conference on Music Informa-tion Retrieval, pages 41-46, 2006.[6]D. Byrd and J. G. Simonsen. Towards a stan-dard testbed for optical music recognition:deﬁnitions, metrics, and page images, 2015.http://www.informatics.indiana.edu/donbyrd/OMRTestbed/OMRStandardTestbed1Mar2013.pdf(accessed July, 2015).[7]J. S. Cardoso and A. Rebelo. Robust stafﬂine thick-ness and distance estimation in binary and graylevelmusic scores. In20th International Conference onPattern Recognition, pages 1856-1859, 2010.[8]M. S. Cuthbert and C. Ariza. music21: A toolkitfor computer-aided musicology and symbolic musicdata. InProceedings of the 11th International Con-ference on Music Information Retrieval, pages 637-42, 2010.[9]R. C. Edgar. MUSCLE: Multiple sequence align-ment with high accuracy and high throughput.Nu-cleic acids research, 32(5):1792-7, 2004.[10]A. Forn´es, A. Dutta, A. Gordo, and J. Llad´os. TheICDAR 2011 music scores competition: staff re-moval and writer identiﬁcation. In2001 InternationalConference on Document Analysis and Recognition,pages 1511-15, 2011.[11]I. Fujinaga.Adaptive Optical Music Recognition.PhD thesis, McGill University, 1996.[12]I. Fujinaga. Staff detection and removal. InVisualPerception of Music Notation: On-Line and Off-LineRecognition, pages 1-39, 2004.[13]E. Jones, T. Oliphant, P. Peterson,et al.SciPy:Open source scientiﬁc tools for Python, 2001.http://www.scipy.org/ (accessed July, 2015).[14]G. Jones, B. Ong, I. Bruno, and K. Ng. Optical musicimaging: music document digitisation, recognition,evaluation, and restoration. InInteractive MultimediaMusic Technologies, pages 50-79, 2008.[15]T. Kanungo, R. M. Haralick, and I. Phillips. Globaland local document degradation models. InPro-ceedings of the Second International Conference onDocument Analysis and Recognition, pages 730-734,1993.[16]T. Kanungo and Q. Zheng. Estimation of morpho-logical degradation model parameters. In2001 IEEEInternational Conference on Acoustics, Speech, andSignal Processing Proceedings, volume 3, pages1961-1964, 2001.[17]Project Petrucci LLC. IMSLP/Petrucci Music Li-brary, 2015. http://imslp.org/ (accessed July, 2015).[18]S. B. Needleman and C. D. Wunsch. A generalmethod applicable to the search for similarities inthe amino acid sequence of two proteins.Journal ofmolecular biology, 48(3):443-453, 1970.[19]J. A. Nelder and R. Mead. A simplex methodfor function minimization.The computer journal,7(4):308-313, 1965.[20]H.-W. Nienhuys and J. Nieuwenhuizen. LilyPond, asystem for automated music engraving. InProceed-ings of the XIV Colloquium on Musical Informatics(XIV CIM 2003), pages 1-6, 2003.[21]V . Padilla, A. Marsden, A. McLean, and K. Ng. Im-proving OMR for digital music libraries with mul-tiple recognizers and multiple sources. InProceed-ings of the 1st International Workshop on Digital Li-braries for Musicology - DLfM ’14, pages 1-8, 2014.[22]F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D.Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay. Scikit-learn: Machine learning in Python.Jour-nal of Machine Learning Research, 12:2825-2830,2011.[23]L. Pugin, J. A. Burgoyne, and C. Ha. MAP adaptationto improve optical music recognition of early musicdocuments using hidden Markov models. InProceed-ings of the 8th International Conference on Music In-formation Retrieval, pages 513-16, 2007.[24]T. Reed.Optical music recognition. Master’s thesis,University of Calgary, 1995.[25]D. Ringwalt, R. B. Dannenberg, and A. Russell. Op-tical music recognition for live score display. InPro-ceedings of the 2015 Conference on New Interfacesfor Musical Expression, 2015.[26]L. Wang and T. Jiang. On the complexity of multiplesequence alignment.Journal of Computational Biol-ogy, 1(4):337-348, 1994.[27]S. Wang and S. Dixon. Robust joint alignment ofmultiple versions of a piece of music. InProceed-ings of the 15th International Conference on MusicInformation Retrieval, pages 83-88, 2014.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 23"
    },
    {
        "title": "Melodic Similarity in Traditional French-Canadian Instrumental Dance Tunes.",
        "author": [
            "Laura Risk",
            "Lillio Mok",
            "Andrew Hankinson",
            "Julie Cumming"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414858",
        "url": "https://doi.org/10.5281/zenodo.1414858",
        "ee": "https://zenodo.org/records/1414858/files/RiskMHC15.pdf",
        "abstract": "Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits.",
        "zenodo_id": 1414858,
        "dblp_key": "conf/ismir/RiskMHC15",
        "keywords": [
            "French-Canadian",
            "instrumental dance tunes",
            "varied and complex",
            "aural tradition",
            "few notated sources",
            "practitioners",
            "combination of tune segments",
            "personalized settings",
            "extreme amount of variation",
            "same tune or tune segment"
        ],
        "content": "MELODIC SIMILARITY IN TRADITIONAL FRENCH-CANADIANINSTRUMENTAL DANCE TUNESLaura RiskLillio Mok Andrew Hankinson Julie CummingSchulich School of MusicCentre for Interdisciplinary Research in Music Media and Technology (CIRMMT)McGill Universitylaura.risk@mail.mcgill.caABSTRACTCommercial recordings of French-Canadian instrumentaldance tunes represent a varied and complex corpus ofstudy. Thiswasa primarily aural tradition, transmittedfrom performertoperformerwithfew notated sourcesuntil thelate20th century. Practitioners routinelycombined tune segmentstocreate new tunes andpersonalized settings of existing tunes. This has resultedina corpus that exhibitsanextreme amount of variation,even among tuneswiththesamename. In addition, thesametune or tune segmentmayappear under severaldifferent names.Previous attemptsatbuilding systems for automatedretrieval and ranking of instrumental dance tunesperformwellfor near-exact matching of tunes, but donot workaswellinretrieving and ranking,inorder ofmostto leastsimilar, variants of a tune; especially thosewithvariationsasextremeasthis particular corpus. Inthis paperwe willdescribe a new approach capable ofranked retrieval of variant tunes, and demonstrateitseffectiveness on a transcribed corpus of incipits.1. INTRODUCTIONCommercial recordings of French-Canadian instrumentaldance tunes from the 1920s through the 1980s documenta working-class repertoire now celebratedasthetraditional instrumental music of Québec [19]. However,the musical contents of these recordings remain largelyunexamined. The only detailed musicological studyislimited toa subset of metrically irregular tunes [8]. Inthis paperweoutline the challenges associatedwithstudying this repertoire and describe a new systemdevelopedto aid inﬁnding and ranking similaritybetween tunesinthis repertoire. This systemwasbuiltinresponseto twomusicological challenges:todeterminethe degree of shared repertoire among early commercialrecording artistsinMontréal, andtoidentify how asingle tuneisvariedindifferent renditions. Using oursystem we have identiﬁed a number of concordant tunes \n© Laura Risk,LillioMok, Andrew Hankinson, JulieCumming. Licensed under a Creative Commons Attribution 4.0International License (CC BY 4.0).Attribution:Laura Risk,LillioMok, Andrew Hankinson, Julie Cumming. “Melodic SimilarityinTraditional French-Canadian Instrumental Dance Tunes,” 16thInternational Society for Music Information Retrieval Conference,2015.(versions of thesametune) previously unrecognizedasbeing musically related.Broadly speaking, the traditional instrumental musicof Québecissimilartothe instrumental traditions ofIreland, Scotland and the UnitedStates.The repertoireconsists primarily of short, fast-paced dance tunesusually performed on the violin, accordion, orharmonica.Withvery few exceptions,eachtune hasatleast twostrains (sections), commonly labeled“A”and“B.” Many of the tunes have their rootsinBritish Islesand American ﬁddling traditions, though others arederived from popular French songs or early twentieth-century marching-band repertoire [8; 13; 23].2. THE CHALLENGEThe French-Canadian tradition has developed almostexclusivelyas anaural and recorded tradition.Withfewnotated sources, musicians would often learn tunes “onthe ﬂy,” constructing their own versions from memoryand iǌecting their own personal style. From the 1930sthrough the 1960s radio broadcasts played a signiﬁcantroleinaural transmission of this repertoire. Onemusician recalled,“Iwouldlisten tothe radiowithmybrother, and afterwardswewould sing the melodiesinour room.Wespent ourtimeconstantly asking ourselvesif itwasreally correct” [9].Asa result of this mode oftransmission, andinthe absence of a culture of“correctness” [7], many tunes performed and recordedinQuébec existinmultiple, equally valid settings. Tuneswould be modiﬁed by transposingallor part of the tune,reworking melodic ﬁgurations, adding and subtractingbeats, composing new strains, or combining strains fromseveral tunestoform new tunes. This diversity ofinterpretationisclearly documented on commercialrecordings from the era.Tunetitleswere often lost or alteredintransmission.Fiddler Yvon Mimeault, for instance, assigned his owntitles tomost of the tunes that he learned from the radiointhe 1950s [14]. Other musicians renamed tunes quiteintentionally. In the 1920s and 30s, ﬁddler Isidore Soucysometimes recorded a tune for one record label underonetitle,and within months recorded thesametune for adifferent record label under a differenttitle.The tune“Money Musk”is anexception. Between 1920 and 1980itwasrecorded over twentytimes,frequentlywithsigniﬁcant melodic and rhythmic variation and93additional strains, but almost always under thesametitle. Toillustratewithone example, ﬁddlers IsidoreSoucy, Joseph Ovila LaMadeleine, and Joseph Allardallreleased settings of thesamemelodyin1928. Theirrecordings were titled, respectively, “Reel du bon vieuxtemps,” “Reel princesse,” and “Reel deMme.Renault”(note that Soucy reverses the order of the A and BstrainswithrespecttoLaMadeleine and Allard).Although these are recognizably thesamemelody, theyhave a signiﬁcant degree of melodic and metricalvariation (ﬁgure 1). Audioﬁlesforallthree are availablethrough the Virtual Gramophone website of Library andArchives Canada [20].\nFigure 1:Incipits for three variants of a strain recordedin 1928.The earliest recording sessions of traditionalinstrumental musicinQuébec were quick and largelyunrehearsed. A pianist or guitarist would usuallyaccompany a soloistwithminimal rehearsaltimepriortorecording. Some of the recordingsalsoincludeaccompaniment onjawharp (guimbarde) or spoons. Theperformances are unedited and were usually completedon either the ﬁrst or second take. Some contain obviousmusical errors, such as missed entries or wrong notes.These performances were pressedto78 RPMrecords,withone three-minute rendition of a tune perside [25]. These recordings contain a signiﬁcant amountof background noise introducedinthe recording chain,alongwiththe standard problems of the 78 RPM formatsuch as hiss, pops, and clicks.The noisy recording environment and the relativelypoor quality of the recordings resultinrecordings thatare difﬁculttofollow, even for human listeners. Duetothese difﬁcultieswedecided nottoexplore signal–basedapproachestoanalyzing this repertoire. Instead, ourapproachwasto: 1) transcribe the A and B strains of arecording into MusicXML using a notation editor(Finale), and 2) devise a system for analyzing andcomputing the distance betweentwovariants of thesametune.Duval [8]estimatesthat the traditional instrumentalmusic of Québec containsat least5000 distinct tunes,not including variants. This gives a potential corpus ofwellover 10,000 strains.Weare currently using oursystemtoparse a database containing 710 strains.Ofthese, 667 were recorded between 1923 and 1929 and 59strains are from renditions of “Money Musk” (16 strainsof “Money Musk” were recorded between 1923 and1929). This collection contains approximately 85% ofallFrench-Canadian recordings of traditional instrumentalmusic on the violin priorto1930, and approximately50% of those recorded on any instrument priorto1930.This selection of repertoireisclearly not random, butrather reﬂects the imperatives of several musicologicalquestions, as discussed below.3. PREVIOUS WORKScholars of aural traditions have long been fascinated byrepertoire variation, and comparative studies abound.Bayard [2] proposedaninﬂuential theory of “tunefamilies” by which the bulk of British Isles and NorthAmerican folk song melodies could be categorizedasvariants of asmallnumber of distinct prototypicalmelodies. Cowdery [4], drawing examples from Irishtraditional music, pointed out that musicians do not thinkinterms of abstract prototypes but rather create newtunes or variants by reworking and combining segmentsof known repertoire.Heargued that tune families weremore appropriately deﬁned by the presence of recurringmelodic motives, and not by their degree of deviationfrom a “standard” or “ideal” version of the tune.Our query and ranking systemisintendedtohelpscholars study the diversity of melodic variants within agiven corpus. Musically, our approachissimilartotheapproach describedinthree previous studies. ÓSúilleabháin [22] analyzed the melodic variations ofIrish ﬁddler Tommie Potts accordingtoa framework of“setaccented tones.” Goertzen [10] argued thatseemingly disparate variants of Texas contest-styleﬁddle tunes are linkedtoa shared sense ofeachtune’s“essence,”itselfcomposed of tune-speciﬁc musicalmarkers. Duval [8] analyzed temporal variationas aninnovative element of performance practiceinFrench-Canadian tunes. However, none of these studies wereperformed using computational tools.Several existing online resources allow userstosearch ﬁddle tunes by musical incipit. Both the ScottishMusic Index [12] and the Traditional Tune Archive [18]classify tunes accordingtonumerical theme codes thatcontain thescaledegrees of the strong beats of the ﬁrsttwobars. Usersmaysearch on thesesitesfor themecodes that exactlymatcha given string and that beginwith that string, but may not search for tune variants.TunePal [6]isa tool that translates audio intosymbolic notation (ABC) and then compares thatnotationtoa crowd-sourced database of traditional Irishtunes usinganedit-distance function. TunePal looks forexact orsimilarstrings of ABC regardless of metricalplacement andiseffectiveatidentifying tunes and tunesettingswithasmallamount of melodic variation(provided there has been no transposition). However,variantswithsigniﬁcant melodic variationmaynot beconsidered a match.VanKranenburg [26] presents a comprehensivesurvey of computational modelling of similaritytoDutch folk songs.Heconcludes that identifyingcharacteristic motifsisthe most important factor whendetermining similarity between two melodies. As well,\n94 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 2:Interval matrix for the B strain of “Reel deMme.Renault” (Joseph Allard, 1928;seeﬁgure 1). The columnheadings (1to17) indicate the strong beats. The row headings (0.0to16.0) indicate the metric offset from a given strongbeat. The ﬁrst row (w) gives theweakbeats foreachstrong beatasintervals from that strong beat. Interval valuesindicate number of whole steps.\nhe demonstrates that the frequency of global features(interval features and other pitch-based features,duration-ratio features, and other rhythmic features)isnot sufﬁcienttoidentifysimilarmelodies, but thatsearching for ordered sequences of certain featuresatalocal levelmayhelp locatesimilarsong melodies. Thesystem described in this paper builds on these ﬁndings.4. METHODOLOGYThe design of our system exploits some generalcharacteristics of French-Canadian instrumental dancetunes.Wedetermineanoptimal alignment betweentwofour-measure incipits and then compare certain featuresatcorresponding locationsinthe incipits. Transpositioniscommoninthis repertoire and tonal centerissometimes ambiguous,soalltunes are internallyrepresentedasa melodic contour.Eachtune hastwoormore strains that function more orlessindependentlyand must therefore be treated separately. Most strainsmaybe uniquely identiﬁed by a four-bar incipit. Sincemost tunes repeat after four measures, a transcribedincipit contains the primary motivic material for thatstrain.Asinrelated British Isles and North Americanﬁddle repertoires, metrical placement matters: the notesthat fall on the strong beats are more essentialtotheidentity of a strain than those on the weak beats [11].Our system requires asetof strainsinsymbolicformat and operatesin twophases. A construction phaseisusedtobuild matrix representations ofeachstrain’sincipit, after which a comparison phase computes thepairwise similarity between matrices. The constructionphase ﬁrst scans and truncates the prepared strainstoafour-measure incipit by using themusic21toolkit [5].Melodic, or horizontal, intervals between the ﬁrst noteon the ﬁrst strong beat and the notes onallother strongbeats are then identiﬁed using the VIS analysisframework [1]. Horizontal intervals betweeneachnoteon aweakbeat and the note onitspreceding strong beatarealsoindexed and stored. The resulting feature vectorsof these pair-wise indexed intervals createanintervalmatrix.Eachjthcolumn of the matrix represents thehorizontal intervals of the melody between thejthstrongbeat andallsubsequent strong beats. Figure 2 shows theinterval matrix for the ﬁrst strain shown in ﬁgure 1.The comparison phase aligns the strong beats oftwointerval matrices before computing feature similarity.For matrices of thesamelength,wefollow earliermusicological studies of related British Isles traditions[11; 22]and align the strong beats of the incipitsbĳectively (beat-to-beat). In other words, the idiomaticalignment fortwoequal-length incipitsAandBistheithbeatinAtotheithbeatinB. For matrices of differentlengths, a standard longest-common-subsequence (LCS)dynamic programming algorithm [17; 21]isusedtoﬁndthe best alignment betweentwoincipits considering thenote on each strong beat.After alignment this phase iteratively matches thenotes ofeachaligned strong andweakbeatinthetwomatrices. Non-matching notes are checked for threepossible musical variation techniques: displacement,reversal and contour similarity. The aligned pair ofstrong beats (i,j)isnon-matching when the note on beatiinAisnot thesameasthe note on beatjinB. Thus,giventwoincipitsAandBandeachkthpair (i,j) ofaligned but non-matching strong beats,displacementofnotes on strong beatstoa correspondingweakbeatoccurs when the note on theithbeatisamongst the notesintheweakbeats after thejthbeat, or vice versa. Areversalof notes on strong beats occurs when the noteon theithbeatisthesameasthat on beatj+1, and viceversa. Finally,contour similarityisdetected when thehorizontal interval between the notes on beatsiandi+1are the same as the interval between beats j and j + 1.Toaccount forcaseswhere the incipitsmayotherwisematchbut have different notes on the ﬁrstaligned strong beat, the algorithm movestothe nextcolumn of both interval matrices and repeats thecomparison phase. This recursive strategyisonly usedon uptohalf the interval columns of the incipits. Foreachrepetition of the comparison phase, a similaritymatrixisconstructed from the results of analyzing thematching, displacement, reversal, and contour similarityof the aligned strong beatsinthe corresponding intervalcolumns between incipit pairs.Each row of a similarity matrix corresponds to the Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 95Figure 3:The similarity matrix comparing the B strain of “Reel deMme.Renault” (Joseph Allard, 1928) and the Bstrain of “Reel princesse” (Joseph Ovila LaMadeleine, 1928).Seeﬁgure 1 for incipits. This ﬁgure shows the featurevalues for the ﬁrst six matched strong beats in the two incipits.result of a particular feature analysis, and the columnsrepresent thekthpair of aligned strong beats fromeachincipit’s interval matrix.Each cell inthe similaritymatrixisthus the result of a feature analysis for a givenstrong beat pair between incipitsAandB. Entriesin eachsimilarity matrix are then combinedwitha weightingfactortoyield a single similarity measure foreachsimilarity matrix (ﬁgure 3). The “Strong BeatComparison” matrices indicate the value of matchingstrong beats and identify aligned strong beats. The“WeakBeats Comparison” matrices indicate thefraction, and relative order, of matching weak beats.Our weighting schemeisdetermined by trial anderror.Upto85% of the weight valueisassignedtomatching strong beats, displaced or reversed strongbeats, and matching contour,withthe remainder used forweighting matchingweakbeats. This weighting schemehas the effect of selecting incipitswitha high percentageof matching strong beats and then ranking thoseselectedaccording to the number of matching weak beats.5. RESULTSTwooutputs from this systemmaybe usefultomusicians, musicologists, and other researchers: thesimilarity matrices, which allow userstodirectlycomparetwoincipits, and a rankedlistof similaritymeasures between one strain andallother strainsinthedatabase.Weattempted a traditional precision and recallanalysis but foundit tobeanunsuitable measure ofeffectiveness. It returned unnaturally high resultsbecause the weightingwasdeterminedtomaximizeprecision and recall for known concordant strains.Asnotedinthe discussion below, precision and recall for 25variants of the A strain of “Money Musk” were either100% (n=10) or 96% (n=15), given 24 relevantitems,24retrieved items, and a database of 59 items.Figure 4 gives incipits for the top 5 strainsintherankedlistfor the B strain of Joseph Allard’s “Reel deMme.Renault,”ascomparedto666 other strainsrecorded between 1923 and 1929. A human-providedmusical analysis identiﬁed strains 1 and 3 (“Reel du bonvieux temps,” “Reel princesse”)asthe onlyconcordances in the database.Figure 4:Similarity results for a query of the B strain of“Reel de Mme. Renault” (Joseph Allard, 1928)To testour system,werandomlyselectedatestsetof100 strains from thefull database of 710 strains.Wethenselectedquery strainsatrandom from within thistestset.Weconﬁrmed via a human-supplied musicalanalysis thateachquery strain hadat leastoneconcordanceinthetestset.Those that did not werediscarded and new strains were randomly selected, untilwereached a total of 10. Approximately 50% of thestrainsinthe full database are not concordantwithanyother strains in the database.Wecomparedeachof these query strainstothetestsetusing four different ranking approaches (ﬁgure 5).The Levenshtein and Geometric Distance measures weredrawn from the similarity evaluation system describedin[16]. TheMATT2system [6]isdesigned for a repertoireof Irish tunes andisthe search algorithm underlying theTunePal app described earlier. It presumes thattranscriptions of tunes on “unusually pitched”instruments (instrumentswithrepertoire-speciﬁc andnon-standard tunings) have been normalizedtoa singlefundamental pitch (“transposition invariance”).\n96 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 5:Query results for 10 strains out of atestsetof100 strains. Asexpected,allof these analytical systemsperformedwellwhen identifying exact or near-exactmatches. However, our systemwasabletoidentify moreextreme melodic and metrical variants andtosupply aranking of those variants. Our systemalsoidentiﬁedconcordances in transposed keys.6.DISCUSSIONWebuilt this systeminresponseto twomusicologicalchallenges. First,wewantedtoidentify concordancesinthe earliest commercial recordings of French-Canadiantunesinordertodetermine the degree of sharedrepertoire among these recording artists. Second,wewantedtoanalyze variation techniqueina single tune,“Money Musk,” dueto itspopularityinrecordings of theera.By applying our approachtoa database of 667strains recorded between 1923 and 1929,wewere abletoidentify nearly 150 concordant strains. Most of theseconcordances were previously unrecognizedasrelatedtunes.Using these resultsincombinationwitharchivalresearch,wehave been abletoidentify patterns ofmusical borrowing for certain musicians.Ofthe 16 sidesthat ﬁddler Isidore Soucy recorded for ColumbiaRecordsinNewYork Cityin1927–1929 [24], forinstance, eight were tunes that he had recorded for theStarr labelinMontréal only a few months earlier, mostunder differenttitles.In contrast, when Soucy borrowedfrom his Starr releases for other Starr recordings, heusually re-recorded only single strains (combinedwithnew material or with strains borrowed from other tunes).Wehave been abletodocument the musical linksbetween asmallgroup of ﬁddlers living and workinginthe Montréal regioninthelate1920s. These musiciansoften re-recorded thesametunes and strains withinweeks ofeachother. Joseph Allard and Isidore Soucy,for example, recorded thesametunein late1928 underthetitles“Quadrille Acadien” and “Gigue Indienne,”respectively (Victor 263543–A, Starr 15517–A). In thesummer of 1927,WillieRinguette and Isidore Soucyadded thesameC strainto twodifferent tunes (Starr15347–A, Starr 15363–B). Finally,wewere abletoidentify French-Canadianvariants of many common North American tunes suchas“Soldier’s Joy,”“Haste tothe Wedding,” “Fisher’sHornpipe,” “Bristol Hornpipe,” “Rickett’s Hornpipe,”“Chicken Reel,” “Irish Washerwoman,”“KeelRow,”“Lord McDonald” and “Home Sweet Home.”Weapplied our systemtoa database of 59 strainsdrawn from 13 renditions of “Money Musk.”All“Money Musk” settings include some version oftwoparticular strains, usually labeled A and B, though notallperformers playthe A strain ﬁrst and the B strainsecond. These strainsmaybeeasilyrecognized: bothbeginwitha down-and-back motion that outlines a tonicchord, though the A strain begins on the ﬁfthscaledegree and the B on the ﬁrstscaledegree. (For threeearly and quite varied renditions of “Money Musk,”listen torecordings on the Virtual Gramophone byIsidore Soucy [Starr 15302–B, 1927], Joseph Allard[Victor 263527–B, 1928] and Alfred Montmarquette[Starr 15475–A, 1928]).A human-supplied musical analysis of these 59strains identiﬁed 25 variants of the A strain, 15 variantsof the B strain, 10 strains that were neither A nor Bvariants, and 9 strains that could be conceived ofasdistant variants of A (4 strains) or B (5 strains). Inaddition, this analysis revealedtwotypes of B strains:thosewithanascending melodic contourinthe secondbar (7 strains), and thosewitha descending ﬁgure (8strains).Weused our systemtogenerate rankedlistsforeachof the 59 strains. For the 25 A strains, the top 24resultsinthe rankedlistcontained either 24 (n=10) or 23(n=15) of the remaining A strains. The B-strain resultswere more complex and are summarized in ﬁgure 6.These results suggest that the A-strain variants of“Money Musk” are moresimilarto eachother than arethe B-strain variants, and that B-ascending variants aremore diverse than B-descending. This analysisalsoallows ustoidentify certain variantsasmusical outliers.Asnoted above, 15 of the A strains recalled 23 of 24other A strains. In 12 of these instances, the missingstrainwasthesame.This suggests that this strain wouldbe a good candidate for further study.The resultsinﬁgure 6alsopointtoa splitinthe B-ascending strains, between those that are mostsimilarto\nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 97the other B-ascending strains and those that are equallysimilartoB-ascending and B-descending strains. Inparticular, the strain recorded by Arthur-Joseph Boulaystands out foritsdissimilaritytoother B-ascendingstrains. These B-strain results suggests that the originalmusicological analysis thatclassiﬁedthe B strainsaseither ascending or descendingmayneedtobe reﬁnedwith reference to additional signiﬁcant features.\nFigure 6:Results for 15 B strains of “Money Musk” outof a database of 59 strains.Examples suchasthese suggest that our approachmayhelp scholars of instrumental dance music achieve amore nuanced study of musical similarity. Speciﬁcally,our systemmayhelptoidentify concordances, parsedegrees of melodic variation, and pinpoint instances thatrequire further examination. The systemalsoprovidestools—the similarity matrices and the ranked lists—tofacilitate such examination.Certain instances of comparison remain problematic,however. The system does not currently recognizechangesinmeter, occasionally resultinginincipits thatare slightly shorter or longer than four measures. Thisisthecasefor both “Reel princesse” and “Reel du bonvieux temps” (ﬁgure 1). The rankedlistresults for suchcasesarestillreasonably accurate (ﬁgure 4). Notealsothat the placement of the barlinesinmetrically irregularrenditions of tunes is at the discretion of the transcriber. In addition, the French-Canadian repertoire containssome tuneswithvariantsinboth compoundmeter(9/8or 6/8) and simplemeter(3/2, 4/4, 2/2, or 2/4). In suchcases,the system does not always ﬁnd a satisfactoryalignment between strong beats. The systemmay alsogenerate incorrect results whenthetwoincipits are of different lengths. Thisislargelybecause variationsinlength of musicallysimilarstrainsare dueto anexpansion of the shortertothe longer.While a naïve dynamic programming approachtoalignmentisinsensitivetoexpansion, this issuemaybesolved by reducing the weighting on alignments thatcompress the shorter incipit.7. FUTURE WORKAlthough our systemiscurrently designed for thespeciﬁc attributes of French-Canadian ﬁddle tunes, thecomparison functions and weighting calculationmaybeadapted for other repertoires. Our systemmaybeparticularly useful for repertoiresinwhich new melodiesare constructed using modiﬁed segments of extantmelodies. Such repertoires are primarily aural, butmayalsoinclude notated repertoires suchasRenaissanceMassesbased on pre-existent material. This wouldrequire modifying the system for polyphonic sources.More immediately,wewould liketoinvestigate theapplication of our systemtoBritish Isles and NorthAmerica ﬁddling traditions.Wedo not anticipateneedingtorevise the comparison functions andweighting calculation for this repertoire, and thousandsof these tunes are already availableinsymbolic notationvia online databases [3; 15; 18].Our system identiﬁes nuances betweentwostrainsandisparticularly useful for identifying strainswithahigh degree of variation. However,werecognize that ourapproachmaybelessversatile than a more generalizedcomparison function suchas anedit distance or EarthMover’s Distance function. Eventuallywemayseektocombine our systemwitha “ﬁrst pass” edit distance orEarth Mover’s Distance function.8. CONCLUSIONMusical repertoires that circulate primarilyinauraltradition often contain signiﬁcant variance betweendifferent instances of thesametune. Analyzing variationand transformationinsuch repertoires has beenanimportant part of ethnomusicology, musicology andfolklore scholarship for decades. This paper haspresented a novel toolto aidresearchersinvarianceanalysis in instrumental dance tunes.The source code for our system has been publishedunderanopen source license, available on GitHubathttp://github.com/ELVIS-Project/ﬁddle-tunes.Webelieve that our systemmaybe of practical usefor musicologists and musicians specializinginthetraditional instrumental musics of the British Isles andNorth America. Itmay alsoprove a useful model whenbuilding analytical tools for other repertoires containinga large number of variants.ACKNOWLEDGEMENTSThe authors would liketothank the following people fortheir help and assistance: Ichiro Fujinaga, DavidBrackett, Bryan Duggan,MattKelly,PetervanKranenburg, Dorothea Blostein, Marc Bolduc, andJeanDuval. This workwassupported by the Social Sciencesand Humanities Research Council of Canadaaspart ofthe Single Interface for Music Score Searching andAnalysis (SIMSSA) Project, and by a Doctoral Awardfrom Bibliothèque et Archives nationales du Québec.\n98 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20159. REFERENCES[1] Antilla, C., and J. Cumming: “The VIS Framework: Analyzing counterpoint in large datasets.” In Proc. Conf.Int’l Society for Music Information Retrieval. Taipei, 71–6. 2014.[2] S. Bayard: “Prolegomena to a study of the principal melodic families of British-American folk song.” The Journal of American Folklore 63 (247) 1–44, 1950. [3] J. Chambers: “JC’s ABC tune ﬁnder.” 2015. http://trillian.mit.edu/~jc/cgi/abc/tuneﬁnd (accessed 3 May 2015).[4] J. Cowdery: “A fresh look at the concept of tune family.” Ethnomusicology 28 (3) 495–504, 1984. [5] Cuthbert, M., and C. Ariza: “music21: A toolkit for computer-aided musicology and symbolic music data.” In Proc. Conf. of the Int’l Society for Music Information Retrieval. Utrecht, The Netherlands, 637–42. 2010.[6] B. Duggan: “Machine annotation of traditional Irish dance music.” PhD diss., Dublin Institute of Technology, 2009.[7] K. Dunlay: “‘Correctness’ in Cape Breton ﬁddle music.” MacKinnon’s Brook: Traditional Fiddle Music of Cape Breton, Volume 4. Rounder Records 7040 Liner Notes, 2008. [8] J. Duval: “Porteurs de pays à l’air libre: jeu et eǌeuxdes pièces asymétriques dans la musique traditionnelle du Québec.” PhD diss., Université de Montréal, 2013.[9] E. Favreau: “La transmission de la musique traditionnelle par la radio.” Bulletin Mnémo 2 (1) 1997. http://www.mnemo.qc.ca/html2/97(11)a.html (accessed 3 May 2015).[10] Goertzen, C.: “Texas contest ﬁddling: What modernvariation technique tells us.” In Routes & roots: Fiddle and dance studies from around the North Atlantic 4, edited by I. Russell, and C. Goertzen, 98–111. Aberdeen: The Elphinstone Institute. 2012.[11] C. Gore: The Scottish ﬁddle music index: The 18th & 19th century printed collections. Musselburgh, Scotland: Amaising. 1994.[12] ———: “The Scottish music index.” 2015. http://www.scottishmusicindex.org (accessed 3 May 2015).[13] L. Hart, and G. Sandell: Danse ce soir: Fiddle and accordion music of Québec. Paciﬁc, MO: Mel Bay. 2001.[14] IREPI: “Yvon Mimeault, violoneux.” L’Inventaire des ressources ethnologiques du patrimoine immatériel 2015. http://www.irepi.ulaval.ca/ﬁche-yvon-mimeault-86.html [15] J. Keith: “The session.” 2015. https://thesession.org (accessed 3 May 2015).[16] M. Kelly: “Evaluation of melody similarity measures.” MSc diss., Queen’s University (Ontario), 2012.[17] Kleinberg, J., and É. Tardos: “Dynamic programming.” In Algorithm design, 251–335. Boston: Pearson/Addison-Wesley. 2006.[18] A. Kuntz, and V. Pelliccioni: “Traditional tune archive.” 2015. http://tunearch.org/wiki/TTA (accessed 3 May 2015).[19] G. Labbé: Musiciens traditionnels du Québec (1920–1993). Montréal: VLB. 1995.[20] Library and Archives Canada: “The virtual gramophone.” 2015. http://www.collectionscanada.gc.ca/gramophone/index-e.html(accessed 3 May 2015).[21] M. Mongeau, and D. Sankoff: “Comparison of musical sequences.” Computers and the Humanities 24161–75, 1990. [22] M. Ó Súilleabháin: “Innovation and tradition in the music of Tommie Potts.” PhD diss., Queen’s University (Belfast), 1987.[23] L Ornstein: “A life of music: History and repertoire of Louis Boudreault, traditional ﬁddler from Chicoutimi,Quebec.” MA diss., Université Laval, 1985.[24] R. Spottswood: Ethnic music on records: A discography of ethnic recordings produced in the UnitedStates, 1893–1942. Vol. 1: Western Europe. Urbana: University of Illinois. 1991.[25] R. Thérien: L’Histoire de l’enregistrement sonore au Québec et dans le monde 1878–1950. Québec: Presses de l’Université Laval. 2003.[26] P. Van Kranenburg: “A computational approach to content-based retrieval of folk song melodies.” PhD diss., Utrecht University, 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 99"
    },
    {
        "title": "Selective Acquisition Techniques for Enculturation-Based Melodic Phrase Segmentation.",
        "author": [
            "Marcelo E. Rodríguez-López",
            "Anja Volk"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415064",
        "url": "https://doi.org/10.5281/zenodo.1415064",
        "ee": "https://zenodo.org/records/1415064/files/Rodriguez-Lopez15.pdf",
        "abstract": "Automatic melody segmentation is an important yet un- solved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous lis- tening experience plays a considerable role in the percep- tion of melodic segment structure. At present automatic melody segmenters that model listening experience com- monly do so using unsupervised statistical learning with ‘non-selective’ information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory. In this paper we investigate techniques for ‘selective’ information acquisition, i.e. our learning model uses a goal- oriented approach to select what to store in memory. We test the usefulness of the segmentations produced using se- lective acquisition learning in a melody classification ex- periment involving melodies of different cultures. Our re- sults show that the segments produced by our selective learner segmenters substantially improve classification ac- curacy when compared to segments produced by a non- selective learner segmenter, two local segmentation meth- ods, and two na¨ıve baselines.",
        "zenodo_id": 1415064,
        "dblp_key": "conf/ismir/Rodriguez-Lopez15",
        "keywords": [
            "Automatic melody segmentation",
            "Music Information Retrieval",
            "Listening experience",
            "Melodic segment structure",
            "Unsupervised statistical learning",
            "Non-selective information acquisition",
            "Goal-oriented approach",
            "Selective information acquisition",
            "Melody classification experiment",
            "Segments produced by selective learner"
        ],
        "content": "SELECTIVE ACQUISITION TECHNIQUES FORENCULTURATION-BASED MELODIC PHRASE SEGMENTATIONMarcelo E. Rodr´ıguez-L´opezUtrecht Universitym.e.rodriguezlopez@uu.nlAnja VolkUtrecht Universitya.volk@uu.nlABSTRACTAutomatic melody segmentation is an important yet un-solved problem in Music Information Retrieval. Researchin the ﬁeld of Music Cognition suggests that previous lis-tening experience plays a considerable role in the percep-tion of melodic segment structure. At present automaticmelody segmenters that model listening experience com-monly do so using unsupervised statistical learning with‘non-selective’ information acquisition techniques, i.e. thelearners gather and store information indiscriminately intomemory.In this paper we investigate techniques for ‘selective’information acquisition, i.e. our learning model uses a goal-oriented approach to select what to store in memory. Wetest the usefulness of the segmentations produced using se-lective acquisition learning in a melody classiﬁcation ex-periment involving melodies of different cultures. Our re-sults show that the segments produced by our selectivelearner segmenters substantially improve classiﬁcation ac-curacy when compared to segments produced by a non-selective learner segmenter, two local segmentation meth-ods, and two na¨ıve baselines.1. INTRODUCTIONMotivation: In Music Information Retrieval (MIR),melody segmentation refers to the task of dividing amelody into smaller units, such as ﬁgures, phrases, or sec-tions. Given that melody is an aspect of music shared by al-most all cultures in the world, and that melodies are knownto be memorable, many MIR systems base their functional-ity in melody processing. Automatic melody segmentationis hence an important preprocessing step for MIR tasks in-volving searching, browsing, visualising, and summarisingmusic collections.Scope: Research in automatic melody segmentation hasbeen conducted by subdividing the segmentation probleminto a number of subtasks, the most traditional one beingsegment boundary detection, i.e. automatically locating thetime instants separating contiguous segments. In this paperc\u0000Marcelo E. Rodr´ıguez-L´opez, Anja V olk.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Marcelo E. Rodr´ıguez-L´opez, AnjaV olk. “Selective Acquisition Techniques for Enculturation-Based Melo-dic Phrase Segmentation”, 16th International Society for Music Informa-tion Retrieval Conference, 2015.we focus on detecting the boundaries of segments resem-bling the musicological concept ofsubphrase. The musi-cal factors inﬂuencing the perception of melodic segmentboundaries are diverse [4, 8]. In this paper we focus onmodelling factors related to previous listening experienceand melodic expectation [1, 9, 23, 25, 27].Terminology: We use the term ‘phrase’ to refer to a se-quence of notes lasting roughly from 6 notes to 8 bars. Weuse the term ‘ﬁgure’ to refer to a relatively short sequenceof notes, lasting roughly from 2-6 notes. We use the term‘subphrase’ to refer to melodic ﬁgures in the context ofphrases, i.e. as the constituent parts of a melodic phrase.Assumptions: Our main assumption is that human lis-teners exposed to melodies of a given culture acquire avocabulary of melodic ﬁgures through ‘incidental’ learn-ing,1and that this acquired melodic vocabulary aids thesegmentation of phrases into subphrases.2We refer tosuch a listener as ‘enculturated’.Problem statement: At present, automatic melody seg-menters that model previous listening experience usuallydo so by storing information indiscriminately into mem-ory. We argue that selective (rather than indiscriminate)information acquisition is necessary to simulate encultura-tion. We hence propose and investigate two techniques forselective acquisition in the context of phrase segmentation:one in which an artiﬁcial learner selects the subphrases thatgive it the ‘clearest’ possible ‘understanding’ of a phrase,and another in which the learner attempts to use subphrasesit ‘knows well’ to expand its melodic vocabulary. To com-pare the segmentations produced by enculturated segmen-ters using selective and non-selective acquisition techni-ques, we perform a melody classiﬁcation experiment in-volving melodies of different cultures, where the segmentsare used as classiﬁcation features.Paper contributions:We have three main contribu-tions. First, the proposed techniques for selective acquisi-tion are, to the best of our knowledge, novel in the contextof melody segmentation. Second, we focus on subphraselevel segmentation, which is a neglected area in music seg-mentation research. Third, our results show that the seg-ments produced by our selective learning segmenters sub-stantially improve classiﬁcation accuracy when comparedto segments produced by using a non-selective learning1We use the term incidental to mean that the listener does not have anexplicit learning intention.2Refer to [15, 16, 24] for experimental work in music cognition andcognitive neuroscience that supports our assumption.218segmenter, two local segmentation methods [5,6], and twona¨ıve baselines.Paper summary:The remainder of this paper is or-ganised as follows:§2 reviews related work,§3 describesour selective acquisition learning model ,§4 describes ourproposed enculturated segmenter,§5 describes the classi-ﬁcation experiment and presents results,§6 discusses theevaluation results, and ﬁnally,§7 summarises our conclu-sions and outlines possibilities of future work.2. RELATED WORKPreviously proposed melody segmenters that model listen-ing experience have mostly used non-selective learners.For instance, [23] presents a segmentation model with along-term memory (LTM) component. To train the LTMmodel the prediction by partial match (PPM) algorithm [21]is used, which gathers and stores ngrams and ngram statis-tics indiscriminately into LTM. Much of the work carriedout by the authors of [23] in melodic learning has focusedmostly on dealing with melodic multidimensionality [19]and on the combination of short-term and long-term mem-ory models [20], but not much attention has been paid tothe construction of the LTM itself.We base our approach on [11], where selective acquisi-tion learning is used for motivic pattern extraction from acorpus of melodies. Our approach extends their work byproposing and testing different selective acquisition tech-niques, and by combining the learning approach proposedin [11] with characteristics of the ‘feature selection’ learn-ing approach proposed in [3] for natural language process-ing. Moreover, we focus on using selective learning to cre-ate more powerful LTM models for melody segmentation.In the following section we describe our approach in detail.3. ENCULTURATION VIA SELECTIVEACQUISITION LEARNINGThe goal of selective acquisition learning is to construct anenculturated LTM model. In this paper we model encul-turation as a reﬁnement process. That is, our learner takestwo inputs: (1) a LTM model, which is simply a collectionof melodic ﬁgures acquired during prior listening experi-ence, and (2) a corpus of melodies of a given culture towhich the learner is to be exposed. The output is a LTMmodel in which, ideally, only melodic ﬁgures characteris-tic of the culture to which the learner has been exposed arepreserved. Our learning approach is summarised as pseudocode in Algorithm 1.As shown in Algorithm 1, our learner ‘listens’ to eachmelody one phrase at a time, and decides which ﬁgures tostore in LTM by evaluating different segmentations. Thatis, the learner stores in LTM only the ﬁgures that allow itto segment the phrase in an optimal way. This process iscontinued until the learner has acquired the melodic vo-cabulary that allows it to perform optimal segmentations.In the following sections we describe each part of the ap-proach in more detail.Input:LTM model, Phrase-segmented MelodicCorpus,Output:LTM modelwhiletermination condition not metdoread melody from corpus;foreach phrase in melodydoCompute possible segmentations;Select the optimal segmentation;Store suphrases in LTM;Check termination condition;Algorithm 1:Selective Acquisition Learning3.1 Input/Output3.1.1 Input: Melody RepresentationOur learner takes as input melodies represented as a se-quence of chromatic pitches, constrained to a range of twooctaves.3Formally, we takep=p1...pNto be a se-quence of pitch intervals, where each intervalpi2A={\u000012,...,0,...,+12}. InAeach numerical value en-codes the distance in semitones between two contiguouspitches, and the±symbol encodes its orientation (ascend-ing, descending).3.1.2 Input: phrase segmented corpusWe assume input melodies are annotated with phrase bound-aries, so that our learner can process melodies on a phraseby phrase basis, ﬁnding for each an optimal segmenta-tion. We choose to process phrases based on cognitiveconstraints, as exhaustively evaluating multiple segmenta-tions for a whole melody would break known limitationsof human memory.3.1.3 Input/Output: long term memory (LTM) modelWe model LTM probabilistically using a Markov modellingstrategy. Essentially this boils down to constructing a datastructure to hold the number of times melodic ﬁgures up to5 intervals appear in a corpus, and then use those counts toestimate probabilities (we go into more detail in§3.3).43In this paper our learner and segmenters take as input symbolic en-codings of melodies, i.e. computer readable representations of scorestranscribed by experts (see§5.1 for more details). Symbolically encodedmelodies can be represented in a variety of ways, e.g. chromatic pitch,step-leap pitch intevals, inter onset intervals, and so on. In statisticallearning this multi-dimensional attribute representation of melodic eventscan be tackled usingmultiple viewpoint systems[7, 19]. However, usingmultiple viewpoints comes at expense of a considerable increase in thecomplexity of the statistical model architecture, resulting in an increase inprocessing time and space requirements, as well as lower interpretabilityof the model. In this paper we favour using a single melodic represen-tation to simplify the evaluation our of segmenters, which is importantconsidering that we evaluate our segmenters indirectly, by means of aclassiﬁcation experiment (see§5).4The input LTM model can also be computed by sampling fromknown parametric distributions, e.g. in [2] the LTM model is constructedsampling from a Dirichlet distribution. However, by using corpus statis-tics we can assess how different (and perhaps more suitable) are the seg-mentations produced by one of the learners in respect to the others whenexposed to the same melodies, which is a better way to try to prove ordisprove our hypothesis.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2193.2 Computing Possible SegmentationsIdeally, our learner should evaluate all possible segmenta-tions of a phrase. However, processing time is exponentialon the number of notes in the phrase, so in practice eval-uating all segmentations is unfeasible. Thus, we use thealgorithm proposed in [17] to efﬁciently compute a con-strained space of possible segmentations. The algorithmtakes as input the minimum and maximum length of sub-phrases, as well as the minimum and maximum number ofsubphrases. As we mentioned previously we have limitedsubphrases to be sequences of 1-5 intervals in length. Wealso limit phrases to be composed of at most 6 subphrases(by doing so we are able to cope with phrases of a maxi-mum length of 30 intervals).3.3 Select the optimal segmentationBelow we present two techniques to select an optimal seg-mentation. One in which the learner selects subphrases thatgive it the ‘clearest’ possible understanding of a phrase,and another in which the learner uses subphrases it ‘knowswell’ to increase its vocabulary.3.3.1 Common and Complete FiguresMelodic ﬁgures that aid segmentation should be ‘charac-teristic’ of a melodic culture. One way to measure howcharacteristic ﬁgures are is by searching for ‘common’ ﬁg-ures in a corpus representative of a melodic culture. How-ever, common ﬁgures are mainly of short duration, andnormally less speciﬁc and informative than ﬁgures of largerduration (see [28]). There is hence a trade-off betweenhow common a ﬁgure is and how speciﬁc to a given tra-dition it can be.5Thus, we need a way to automaticallydetermine how long do the ﬁgures we are after need to be,so that we search for the longest possible common ﬁguresinstead of only the most common ones. One way to doso is by attempting to determine if a given ﬁgure is some-how ‘complete’ on its own, or if its part of a larger ﬁgure.Our search then would be for ﬁgures that are common, yetlarge enough so as to be perceptually complete. Accord-ing to melodic expectation theory [14, 27], the perceptualcompleteness of a melodic ﬁgure is inversely proportionalto the degree by which it stimulates expectation. In otherwords, melodic ﬁgures for which is hard to predict whatcomes next are perceived as more complete than those forwhich is easy to predict what comes next.Using information theory we can attempt to jointly quan-tify the commonness and completeness of a ﬁgure. If fromwithin a phrase of lengthTwe take a ﬁgurew=pi...pj,withi, j2[1 :T], we can compute its conditional entropyhash(x|w)=P(w)Xx2AP(x|w)log(P(x|w))(1)5In natural language this is also a commonly found problem, ‘content’or informative words (e.g. nouns) tend to be of greater length that ‘non-content’ words (e.g. determinants).wherexis used to symbolise melodic events that canfolloww, andPdenotes probability. In Eq. 1 the ﬁrst termP(·)will be high for common ﬁgures in a corpus, and thesecond termPP(·)log(P(·))will be high if it is hard topredict what comes afterw. Hence,hwill be high for ﬁg-ures that are common and complete in an information the-oretic sense.The values of probabilitiesP(·)can be estimated fromthe counts ofwand the concatenationwxin a given melo-dic corpus:P(w)⇠N(w)/NTandP(x|w)⇠N(wx)/N(w), whereN(·)denotes counts, andNTdenotes the to-tal number of counts for ﬁgures of length equal towin thecorpus.3.3.2 Monitoring LTMUsing conditional entropy we can monitor the state of ourLTM before and after a new melodic ﬁgure is listened to.So, ﬁrst, the total entropy for ﬁgureswof the same size isHo=\u0000Xw2A⇤P(w)Xx2AP(x|w)logP(x|w)(2)where we useA⇤to denote the space of all ﬁgures ofsizeowith attribute spaceA. In our LTMo={1,...,5}and hence its total entropy isH=H1+···+H5(3)and then we can deﬁne\u0000Has\u0000H=Hafter listening tow\u0000Hbefore listening tow(4)which allows us to monitor the evolution of our LTM.3.3.3 Selection Technique 1We have now the necessary information to formulate ourﬁrst selection technique. Since common and complete ﬁg-ures are expected to have high entropy, a ‘good’ phrasesegmentation among a group of possible segmentations isthat segmentation with the highest average\u0000H. That is, ifwe have a space of possible segmentationsS, the average\u0000Hof a candidate segmentations=w1,...,wmis\u0000(s)=\u0000H(w1)+···+\u0000H(wm)m(5)and hence our ﬁrst selection technique iss⇤= argmaxs2S\u0000(s)(6)Wheres⇤denotes the segmentation with maximal score.Note that, to ensure convergence, the leaner stores in LTMonly the subphrases ins⇤for which\u0000His positive.One problem with our ﬁrst technique is that it makesour learner very conservative. The melodic ﬁgures storedare characteristic of the corpus as a whole. Hence, thetechnique operates under the assumption that the corpus isstylistically homogeneous. For most cultural traditions theassumption of complete stylistic homogeneity is too strong(it is likely that certain ﬁgures are important but only char-acteristic of subsets of the corpus).220 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Collection Subset Cultural Encoding Number of Average Melody Number of Average PhraseName Abbreviation Name Origin of Sample Melodies Size in Notes Phrases Size in NotesMTCFS Dutch⇤⇤kern 4120 52.3 (22.5) 19935 9.1 (2.5)EFSCCHINA Chinese⇤⇤kern 2201 62.8 (41.2) 11046 12.5 (4.7)OHFT- Hungarian EsAC 2323 38.6 (12.0) 9308 9.6 (3.2)Table 1. Melodic Corpora. Numbers in parenthesis correspond to standard deviation.3.3.4 Selection Technique 2Our second technique aims to relax the assumption of ho-mogeneity and stimulate the learner to expand its vocabu-lary. More importantly, it aims to reveal segmentations inwhich one or more subphrases are common and complete,and others are representative of the melody, yet relativelyrare in the corpus. For a ﬁgurewthe latter idea can bequantiﬁed as⇢(w)=\u0000Pmelody(w)⇤log(Pcorpus(w))(7)withPmelody(w)⇠M(w)/MTandPcorpus(w)⇠N(w)/NT, whereMdenotes counts ofwin the melody,MTis used to indicate the total number of counts of ﬁguresof size equal towin the melody/corpus, andNdenotescounts ofwin the corpus.For a complete segmentation we take the average of⇢⇢(s)=⇢(w1)+···+⇢(wm)m(8)Finally, we combine the⇢and\u0000using a geometric mean:6\u0000(s)=p\u0000(s)·⇢(s)(9)and compute our second technique ass⇤= argmaxs2S\u0000(s)(10)Wheres⇤denotes the segmentation with maximal score.Our leaner stores all subphrases ofs⇤in LTM.3.4 Termination ConditionWe keep track of the scores ofs⇤when processing thecorpus, expecting that, as the learner reaches convergence,the score difference between subsequent instancess⇤getssmaller and smaller. We hence assume convergence hasbeen reached if\u0000s⇤<✏.Since Eq. 10 encourages learning new vocabulary, con-vergence is slow and not guaranteed. Thus, in addition to\u0000s⇤<✏, we also set a maximum number of learning iter-ations as a second termination condition.4. ENCULTURATED SEGMENTATIONOnce the LTM model has been trained (via either selec-tive or non-selective learning), our segmenter proceeds ina way similar to Algorithm 1. That is, it processes eachmelody a phrase at a time, for each phrase it computes a6Since\u0000(s)can in principle be negative, to compute\u0000we considernegative\u0000H(w)values to be zero when computing\u0000(s)to avoid thepossibility of negativity.space of possible segmentations, and selects the best one.However, this time the selection of the best segmentationis made by computingh⇤= argmaxs2Sh(s)(11)whereh(s)=h(w1)+···+h(wm)mandh(w)is computedusing Eq. 1.5. EVALUATING SUBPHRASE SEGMENTATIONSAt present, freely available corpora annotated with sub-phrase boundaries do not exist. This implies we are un-able to evaluate our segmenters in a traditional scenario(i.e. by comparing automatic segmentations to human-an-notated segmentations). Hence, we opt for a ‘use-case’evaluation scenario: test the output of our segmenters in amelody classiﬁcation experiment.The classiﬁcation task consists in predicting the culturalorigin of each melody in a dataset of melodies, using sub-phrases as classiﬁcation features. In this scenario ‘good’segmentations should facilitate classiﬁcation and thus re-sult in high classiﬁcation performance.In the following subsections we describe the melodiccorpora used for our classiﬁcation experiment, the com-pared segmenters, the classiﬁers employed, and ﬁnally welist evaluation metrics and present results.All segmenters and baselines were coded in Matlab. Allsource ﬁles as well as the train/test data listings are avail-able athttp://www.projects.science.uu.nl/music/.5.1 Phrase Annotated Melodic CorporaThe melodic corpora used in our experiments is summa-rised in Table 1. TheMeertens Tune Collection7(MTC) isa collection of Dutch folk songs. TheEssen Folk Song Col-lection8(EFSC) is a collection of vocal folk songs fromEurasia. TheOld Hungarian Folksong Typescollection9(OHFT) is a collection of vocal folk songs from Hungary.All corpora summarised in Table 1 have been annotatedwith phrase boundaries by expert Ethnomusicologists.10We cleaned the collections by removing all melodies withoverly short and overly long phrases. We considered a7http://www.liederenbank.nl8http://www.esac-data.org9We obtained theOHFTdata directly from the author of [11].10In the case of theEFSC-CHINAthe origin of the phrase markingsis uncertain. However, it is often assumed it corresponds to notatedbreath marks and/or to the phrase boundaries of lyrics. In the case oftheMTC-FSphrase boundary markings where produced by two experts(which agreed on a single segmentation). The annotation process is de-tailed in [26]. In the case of theOHFTthe phrase boundary markingprocess is detailed in [10, 12].Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 221SegmenterParameter SettingSegmentation Results (for the best parametric setting)Mean Number of Mean Number of Total Number of UniqueSubphrases per Phrase Subphrases per Melody Subphrases per CorpusCHD C H D C H DNSLTM training: PPM-C, with exclusion,5.0 4.8 4.8 27.7 16.7 23.4 1300 1091 11971000 melodies of each culture.ST1LTM training: convergence 10E-8 or 8000 phrases,3.8 3.7 3.7 21.7 13.7 19.6 3437 2562 22041000 melodies of each culture.ST2LTM training: convergence 10E-8 or 8000 phrases,3.6 3.5 3.5 23.2 15.4 21.3 3566 2743 23111000 melodies of each culture.LBDMdetection threshold{0.2,0.4,0.6}, others: suggested setting in [5].3.0 2.9 2.9 15.5 10.4 15.4 4497 3841 2999PATdetection threshold{0.2,0.4,0.6}, others: suggested setting in [6].3.6 3.4 3.4 19.3 11.4 16.7 3603 3139 2810FIXLENconstant sizeCS=3intervals.4.0 3.8 3.8 22.0 13.5 18.9 1371 1179 1474RANDconstant sizeRS2[2\u00004]intervals.4.1 3.9 3.9 22.2 13.5 19.2 2827 2413 2551Table 2. Parameter settings and segmentation results. C - Chinese, H - Hungarian, D - Dutch. Text in bold indicates bestperforming parametric settings.phrase to be overly short if it contains only one note orone interval. We considered a phrase to be overly long if itis longer than 30 notes in length.5.2 Enculturated SegmentersWe evaluate three enculturated segmenters: NS, ST1, ST2.The NS segmenter uses a LTM model trained with non-selective acquisition (using the PPM-C algorithm [22]).The ST1 segmenter uses a LTM model trained with the se-lective acquisition technique 1, Eq. 6. The ST2 segmenteruses a LTM model trained with the selective acquisitiontechnique 2, Eq. 10. A sample of 1000 melodies from eachcollection is used to train the LTM models. The paramet-ric settings for each enculturated segmenter are speciﬁedin Table 2.5.3 Reference Segmenters and BaselinesWe compared the performance of the enculturated segmen-ters to two local boundary detection segmenters (LBDMand PAT), and two na¨ıve baseline segmenters (FIXLENandRAND). The LBDMand PATsegmenters were selectedfor comparison because they have been used for subphraselevel segmentation in the past [6, 18]. The LBDMseg-menter [5] computes subphrase boundaries by detectinglarge pitch intervals and inter-onset-intervals. Intervals si-zes are given a score by comparing them to immediatelysurrounding intervals (the larger the difference the higherthe score). High scoring intervals are taken as subphraseends. The PATsegmenter [6] computes subphrase bound-aries by detecting and scoring repetitions of pitch inter-val sequences within each phrase. The starting points ofhigh scoring repetitions are taken as subphrase starts. TheFIXLENbaseline segments a phrase into subphrases of con-stant size. The RANDbaseline segments a phrase into sub-phrases of randomly chosen sizes. The parametric settingsfor each of the reference and baseline segmenters are spec-iﬁed in Table 2.5.4 Features and ClassiﬁersAs mentioned above, in our experiment we are interestedin evaluating the effectiveness of subphrases as classiﬁca-tion features. To use subphrases in the most transparentway, we represent melodies as a ‘bag-of-subphrases’. Thatis, we use a vector space model representation,11whereeach vector element is weighted using the common termfrequency - inverse document frequency (tf⇤id f) heuris-tic [13]. We then use two simple and well known classiﬁersfor the cultural origin prediction task:k-meansandk near-est neighbours(kNN).Segmenterk-means (k=3)kNN (k optimised)RPARPANS0.94 0.930.710.93 0.870.83ST10.90 0.950.740.93 0.940.87⇤ST20.92 0.930.710.92 0.960.88LBDM0.47 0.500.470.75 0.840.76PAT0.74 0.760.580.83 0.870.79FIXLEN0.88 0.890.670.86 0.900.83RAND0.84 0.840.630.88 0.850.78Table 3. Clasiﬁcation results: recall (R), precision (P),and accuracy (A) averaged over 10-folds. Text in boldhighlights the highest performances. Asterisks indicateperformances that are not signiﬁcantly different from thehighest performances.5.5 Test set, Performance Measures, and ResultsWe constructed a dataset of 3000 melodies by randomlysampling 1000 melodies from each corpus. (All melodiesused to train the enculturated segmenters were excludedfrom the sample.) For each of the 3000 melodies, theclassiﬁers are required to predict whether the melody isof Hungarian, Chinese, or Dutch origin.Validation tech-nique: We used 10-fold cross validation to iteratively sep-arate the melodic dataset into training and test sets.Eval-uation measures: Given aNtotalof melodies per foldto be classiﬁed, we usetpto indicate the number of truepositives,fpthe false positives, andfnthe false nega-tives. With these statistics we measure measure classiﬁca-tion performance using accuracyA=NcorrectNtotal, precisionP=tptp+fpand recallR=tptp+fn.Statistical testing:W e11in a vector space model, melodies are represented as a vector of size|V|, where|V|is the number of unique ﬁgures occurring in the corpus. Ifa ﬁgure occurs in the melody, its value in the vector is equal to the numberof times it appears in the melody. The frequency of occurrence of eachﬁgure is then used as a feature for classiﬁcation.222 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015used an an ANOV A test (↵=0.01) with Bonferroni cor-rection to test the statistical signiﬁcance of the differencesin accuracy for each segmenter.Setting and optimisingclassiﬁer parameters: The training sets were used to opti-mise the permutation labels of the k-means classiﬁer andselect the optimal number of nearest neighbours for thekNN classiﬁer. The optimal number of nearest neighbours(selected fromk2[1,15]) was set by optimizing cross-validated accuracy on the training data.The results of our experiment are presented in Table 3.We discuss our results below.6. DISCUSSION6.1 Selective vs. Non-Selective Learning SegmentersTable 2 shows the NS segmenter produces relatively shortsegments, resulting in an average of⇠4.9 subphrases perphrase, and an average of⇠1196 unique subphrases overall three corpora. Conversely, the ST1-2 segmenters pro-duce larger segments, resulting in an average of⇠3.6 sub-phrases per phrase, and an average of⇠2767 unique sub-phrases over all three corpora. Using the k-means classiﬁerwith subphrases computed using ST1 we obtain a (statis-tically signiﬁcant) 3%Aimprovement over the NS seg-menter, which seems to be driven by a 2% improvementinP. Using the k-NN classiﬁer with subphrases computedusing both ST1 and ST2 we obtain (statistically signiﬁ-cant) 3-4%Aimprovements over the NS segmenter, whichare again in pair with 7-9% increases inP. These resultsshow the larger segments produced by the ST1-2 segmen-ters allow better discrimination between melodies of differ-ent cultural origin, suggesting that selective learning leadsto better models of prior listening experience than non-se-lective learning.6.2 Selective Learning Segmenters vs. LocalSegmentersSegmentation results in Table 2 show that local segmentersprefer larger segments than the ST1-2 segmenters. Also,the local segmenters produce an average of⇠3481 uniquesubphrases over all three corpora, which is 741 subphraseslarger than the average of unique subphrases produces bythe ST1-2 segmenters. Table 3 shows thatAresults usingthe segments produced by ST1-2 are>8% better thanAresults using the segments produced by LBDMand PAT.TheAperformance improvements are in line with rela-tively large improvements in bothPandR. These resultsshow that the larger segments produced by the local seg-menters leads to an increase in unique subphrases, and thatthese unique subphrases are not discriminative of culturalorigin. The relatively large improvements inAof the ST1-2 segmenters over the local segmenters supports the hy-pothesis that enculturated listening might be of importancefor the segmentation of melodic phrases.6.3 Selective Learning Segmenters vs. BaselinesTable 2 shows the baseline segmenters produce relativelyshort segments (of 2 or 3 intervals), resulting in an av-erage of⇠3.9 subphrases per phrase, and an average of⇠1969 unique subphrases over all three corpora. Whenusing the k-means classiﬁer we can observe signiﬁcant andrelatively large differences (>5%) between theAobtainedusing ST1-2 and those obtained using the baseline seg-menters. These results show the larger segments producedby the ST1-2 segmenters allow better discrimination be-tween melodies of different cultural origin than the shortersegments produced by the baseline segmenters, indicatingonce more the ST1-2 segmenters might be capturing im-portant aspects of subphrase structure.6.4 ScepticismAny conclusions from our use case evaluation results arelimited to classiﬁcation schemes using ‘bag-of-subphrases’representations of melodies. This representation limits thesimilarity assesment between any two subphrases to exactmatches, which might be introducing an unwanted bias onthe evaluation. To draw more deﬁnitive conclusions ourexperiment needs to be complemented with other use casestudies.7. CONCLUSIONSIn this paper we introduce techniques for selective acquisi-tion learning in the context of melodic segmentation, specif-ically the segmentation of melodic phrases into subphrases.Our aim is to show that enculturated listening is importantfor the segmentation of melodic phrases, and that selectiverather than indiscriminative acquisition techniques are bet-ter to model an enculturated segmenter. We present twoselective acquisition techniques: one in which an artiﬁciallearner selects the subphrases that give it the ‘clearest’ pos-sible understanding of a phrase, and another in which thelearner attempts to use subphrases it ‘knows well’ to ex-pand its melodic vocabulary.To test the segmentations produced by enculturated seg-menters using selective and non-selective acquisition tech-niques, we perform a melody classiﬁcation experiment in-volving melodies of different cultures. Our results showthat the segments produced by our selective learning seg-menters substantially improve classiﬁcation accuracy whencompared to segments produced by using a non-selectivelearning segmenter, two local segmentation methods, andtwo na¨ıve baselines.In future work we plan to conduct experiments to testthe sensitivity of our selection techniques to cross-learn-ing. That is, cases in which the learners have prior knowl-edge of one melodic tradition and are required to adapttheir knowledge to the particularities of a different melo-dic tradition. We also plan to extend the current approachso that it can process multiple attribute representations ofa melody. To this end an integration between our approachand the multipleviewpoint formalism of [7, 19] is planned.Acknowledgments:We thank Z. Juh´asz for sharing with ustheOHFTdataset, and also to the anonymous reviewers for theuseful comments. This work is supported by the Netherlands Or-ganization for Scientiﬁc Research, NWO-VIDI grant 276-35-001to A. V olk.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2238. REFERENCES[1]S. Abdallah, H. Ekeus, P. Foster, A. Robertson, andM. Plumbley. Cognitive music modelling: An informationdynamics approach. In3rd International Workshop on Cog-nitive Information Processing (CIP), pages 1–8. IEEE, 2012.[2]S. Abdallah and M. Plumbley. Information dynamics: pat-terns of expectation and surprise in the perception of music.Connection Science, 21(2-3):89–117, 2009.[3]A. Berger, V . Della Pietra, and S. Della Pietra. A maximumentropy approach to natural language processing.Computa-tional linguistics, 22(1):39–71, 1996.[4]M. Bruderer, M. Mckinney, and A. Kohlrausch. The percep-tion of structural boundaries in melody lines of western pop-ular music.Musicae Scientiae, 13(2):273–313, 2009.[5]E. Cambouropoulos. The local boundary detection model(LBDM) and its application in the study of expressive timing.InProceedings of the International Computer Music Confer-ence (ICMC01), pages 232–235, 2001.[6]E. Cambouropoulos. Musical parallelism and melodic seg-mentation.Music Perception, 23(3):249–268, 2006.[7]D. Conklin and I. H. Witten. Multiple viewpoint systems formusic prediction.Journal of New Music Research, 24(1):51–73, 1995.[8]I. Deli`ege. Wagner alte weise: Une approche percepitve.Mu-sicae Scientiae, 2(1 suppl):63–89, 1998.[9]D. Huron.Sweet anticipation: Music and the psychology ofexpectation. MIT press, 2006.[10]PJ´ard´anyi. Experiences and results in systematizing hungar-ian folk-songs.Studia Musicologica, pages 287–291, 1965.[11]Z. Juh´asz. Segmentation of hungarian folk songs using anentropy-based learning system.Journal of New Music Re-search, 33(1):5–15, 2004.[12]Z. Kod´aly and L. Vargyas.Folk music of Hungary. Da CapoPress, 1982.[13]C. D. Manning, P. Raghavan, and H. Sch¨utze. Scoring, termweighting and the vector space model.Introduction to Infor-mation Retrieval, 100, 2008.[14]L. B. Meyer. Meaning in music and information theory.Jour-nal of Aesthetics and Art Criticism, pages 412–424, 1957.[15]S. J. Morrison, S. M. Demorest, E. H. Aylward, S. C . Cramer,and K. R. Maravilla. FMRI investigation of cross-culturalmusic comprehension.Neuroimage, 20(1):378–384, 2003.[16]Y . Nan, T. R. Kn¨osche, and A. D. Friederici. The perceptionof musical phrase structure: a cross-cultural erp study.Brainresearch, 1094(1):179–191, 2006.[17]J.D. Opdyke. A uniﬁed approach to algorithms generating un-restricted and restricted integer compositions and integer par-titions.Journal of Mathematical Modelling and Algorithms,9(1):53–97, 2010.[18]N. Orio and G. Neve. Experiments on segmentation techni-ques for music documents indexing. InProceedings of theInternational Conference on Music Information Retrieval (IS-MIR), pages 104–107, 2005.[19]M. Pearce.The construction and evaluation of statisticalmodels of melodic structure in music perception and compo-sition. PhD thesis, Department of Computing, City Univer-sity, 2005.[20]M. Pearce, D. Conklin, and G. Wiggins. Methods for combin-ing statistical models of music. InComputer Music Modelingand Retrieval, pages 295–312. Springer, 2005.[21]M. Pearce and G. Wiggins. An empirical comparison of theperformance of ppm variants on a prediction task with mono-phonic music. InArtiﬁcial Intelligence and Creativity in Artsand Science Symposium, 2003.[22]M. Pearce and G. Wiggins. Improved methods for statisticalmodelling of monophonic music.Journal of New Music Re-search, 33(4):367–385, 2004.[23]M. Pearce and G. Wiggins. The information dynamics of me-lodic boundary detection. InProceedings of the Ninth In-ternational Conference on Music Perception and Cognition,pages 860–865, 2006.[24]M. Rohrmeier, P. Rebuschat, and I. Cross. Incidental and on-line learning of melodic structure.Consciousness and cogni-tion, 20(2):214–222, 2011.[25]C. Thornton. Generation of folk song melodies using bayestransforms.Journal of New Music Research, 40(4):293–312,2011.[26]P. van Kranenburg, M. de Bruin, L. Grijp, and F. Wiering.The meertens tune collections. 2014.[27]G. Wiggins and J. Forth. IDyOT: A computational theory ofcreativity as everyday reasoning from learned information. InComputational Creativity Research: Towards Creative Ma-chines, pages 127–148. Springer, 2015.[28]J. Wołkowicz, Z. Kulka, and V . Keˇselj. N-gram-basedapproach to composer recognition.Archives of Acoustics,33(1):43–55, 2008.224 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Automatic Tune Family Identification by Musical Sequence Alignment.",
        "author": [
            "Patrick E. Savage",
            "Quentin D. Atkinson"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417135",
        "url": "https://doi.org/10.5281/zenodo.1417135",
        "ee": "https://zenodo.org/records/1417135/files/SavageA15.pdf",
        "abstract": "Musics, like languages and genes, evolve through a pro- cess of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been ham- pered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein fam- ilies. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combina- tions of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combina- tion that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our ap- proach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale.",
        "zenodo_id": 1417135,
        "dblp_key": "conf/ismir/SavageA15",
        "keywords": [
            "Musical evolution",
            "Transmission",
            "Variation",
            "Selection",
            "Tune families",
            "Protein sequence alignment",
            "Automatic method",
            "Ground-truth dataset",
            "Expert musicologists",
            "Cross-cultural"
        ],
        "content": "AUTOMATIC TUNE FAMILY IDENTIFICATION BY MUSICAL SEQUENCE ALIGNMENT Patrick E. Savage Quentin D. Atkinson Tokyo University of the Arts, Dept. of Musicology patsavagenz@gmail.com Auckland University, Dept. of Psychology q.atkinson@auckland.ac.nz ABSTRACT Musics, like languages and genes, evolve through a pro-cess of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been ham-pered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein fam-ilies. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combina-tions of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combina-tion that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our ap-proach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale.  1. INTRODUCTION Darwin’s theory of evolution is a broad one that applies not only to biology but also to cultural forms such as lan-guage and music [21], [27]. Musicologists have long been interested in understanding how and why music evolves, particularly the three key mechanisms of 1) transmission between generations, 2) generation of musical variation, and 3) selection of certain variants over others [10], [21]. In some cases, historical notations, audio recordings, or other musical “fossils” allow us to document music’s cul-tural evolution through the accumulation of minute varia-tions over time [5], [14], [28]. More often, the process of oral transmission results in contemporaneous groups of related melodies known as “tune families” [2], careful comparison of which can be used to partially reconstruct the process of musical evolution [4]. This situation is analogous to the evolution of language families and bio-logical species [1].  Traditionally, analysis of tune family evolution has been done by manually identifying and aligning small groups of related melodies (see Fig. 1a) and then qualita-tively comparing the similarities and differences. This led to two major challenges that limited the scale of tune family research: 1) the need for an automated method of comparing large numbers of melodies; and 2) the need for an objective means of determining tune family member-ship.  Thanks to the rise of music information retrieval (MIR), the first challenge has been largely overcome by automated sequence alignment algorithms for identifying melodic similarity [9], [16], [23], some of which have been specifically designed for studying tune families [24-26]. However, the second challenge remains unsolved, with tune family identification considered “currently too ambitious to perform automatically” [24].  Here we propose a novel method of tune family iden-tification inspired by molecular genetics [8]. In particular, the problem of protein family identification shares many analogies with tune family identification. Proteins are bi-ological molecules that are constructed by joining se-quences of amino acids into 3-dimensional structures that function to catalyze biochemical reactions. Meanwhile, tunes are constructed by joining sequences of notes into multidimensional melodies that function to carry song lyrics, accompany dance, etc. When attempting to identi-fy both protein families and tune families, a major chal-lenge is to determine whether any observed similarities are due to chance or common ancestry. We sought to develop automated methods for identi-fying and aligning tune families that could be used in fu-ture large-scale studies of musical evolution throughout the world. To do this, we adapted methods designed for identifying and aligning protein families and tested their effectiveness on a cross-cultural ground-truth set of well-established tune families that had already been manually identified and aligned by expert musicologists. We then tested out different model parameters to determine which parameters are most effective at capturing the known ground-truth patterns. 2. DATA Our ground-truth dataset consisted of 26 melodies from four contrasting tune families that had previously been  \n © Patrick E. Savage, Quentin D. Atkinson. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Patrick E. Savage, Quentin D. Atkinson. “Automatic tune family identification by musical sequence alignment”, 16th International Society for Music Information Retrieval Conference, 2015. 162   \n Figure 1. A sample portion of a manually aligned tune family. a) The opening phrase of three tunes manually aligned by Bayard [3] and identified as part of the tune family he labeled “Brave Donnelly”. b) The same infor-mation encoded as aligned pitch-class sequences using our proposed method (see Methods and Fig. 2). Note that keys are transposed so that the tonic (originally F) is always represented as C. identified and aligned manually by expert musicologists1. Two of these tune families were British-American tune families that had been chosen by Samuel Bayard (who coined the term “tune family”) in order to capture \"...all the problems attending a comparative tune study, and all the important features of traditional development that we constantly encounter when we try to elucidate the really extensive families of tunes.\" [3]. The other two were Jap-anese tune families chosen for similar reasons by the Jap-anese folksong scholars MACHIDA Kashō a n d  TAKEUCHI Tsutomu [12]. We chose this dataset be-cause we needed a known baseline against which to com-pare the effectiveness of our methods, and because we wanted our method to have cross-cultural validity that is not limited to idiosyncracies of the types of European-American folk tunes that have traditionally been studied. In addition, the first author has first-hand experience singing English and Japanese folksongs, and this dataset is also comparable to similar but larger collections of British-American and Japanese folk songs (approximately 5,000 each in [5], [18]) to which we aim to eventually apply these automated methods. Music is much more than notes transcribed in a score. However, in order to understand tune family evolution, we need a standardized method of comparing tunes across time and space. To allow for analysis of tunes                                                               1 Full metadata and aligned sequences are available at http://dx.doi.org/10.6084/m9.figshare.1468015  Figure 2. The most widely used “alphabet” for describ-ing musical pitches divides an octave into 12 equally spaced semitones. Here these are visualized using the standard piano keyboard representation, with C repre-senting the tonic. documented before the advent of audio recording tech-nology, this requires the use of transcriptions, although this comes at the cost of losing details about performance style (e.g., timbre, ornamentation, microtuning, microtim-ing). Furthermore, to allow evolutionary analysis using state-of-the-art methods from evolutionary biology, we need to further reduce the information in the score into aligned sequences. This approach was already implicit in the melodic alignment approach developed by tune fami-ly scholars, in which tunes were transposed into a com-mon key and time signatures, phrases, and rhythms were stretched and compressed as necessary to align notes sharing similar pitches (see Fig. 1a).  Just as DNA can be modeled as a sequence construct-ed from an “alphabet” of 4 nucleic acids (C, G, A, or T) or a protein can be modeled as a sequence constructed from an alphabet of 20 amino acids, a melody can be modeled as a sequence constructed from an alphabet of 12 pitch classes representing the 12 notes of the chro-matic scale (Fig. 2). By aligning sequences known to share common ancestry (as done manually in [3] and [12]), we can identify points on the alignment that are conserved, where a different pitch has been substituted, or where a pitch has been inserted/deleted (“indel”, repre-sented using dashes). Fig. 1b shows how this method is used to encode the manual alignment shown in Fig. 1a. This information can then be analyzed quantitatively to reconstruct a phylogenetic tree, network, or other repre-sentation of the evolutionary history of the tune family. The intuition of early tune family scholars to empha-size alignment of pitches, rather than rhythms or global stylistic features, is supported by recent research that has demonstrated quantitatively that pitch is greatly superior to rhythm and to global stylistic features both for the pur-poses of tune family identification in particular and for melodic similarity in general [23], [25]. However, judi-cious use of rhythm and other non-pitch features may im-prove tune family identification [25], and we explore this using several modeling techniques. 3. METHODS 3.1 Sequence alignment parameters  \n14 MIDWEST FOLKLORE, IV : I and American tradition; the Brave Donnelly, so far as I know, only in British. In the tune tables given below, all airs have been put into the same register, to facilitate comparison. Original keys or signatures are given in the notes, however, and of course the modes, melodic intervals and note values have been preserved. Having taken into account all the versions and related airs known to me, I believe that these tables represent the tunes adequately. Table I illustrates the Brave Donnelly tune.' TABLE I Pt. 1 A35 B t J v _--m ,,- t -6 Xt W 1,Ht~~~~~~4 5 Ilrf . li- r 7 ii- 1 - ' - ~ 5 12 4 56 ~' 1 2 3 57 2. 3 1 .i ~~~~~5 7 This Brave Donnelly tune shows a not unusual case of the preservation of a melody in several rather close variants among the Irish, Welsh and English. The rhythmic differences between variants (D, F in 4/4 time, the others in 6/8; one bar of F cor- responding to two in the other sets) are features of common re- currence in our tune families. Comparing B with A (or any set) we may see that often the melodic lines go momentarily in precisely opposite directions, without obscuring the cognateness of the sets. This occurs, e.g., in B, bars 1, 9, 11, 14, 15. Such features remind us 1 Tune Table I: A is \"Well done, cries she, brave Donnelly,\" in C. V. Stanford ed., The Complete Petrie Collection of Ancient Irish Music (Lon- don: Boosey & Co., 1902-hereinafter called \"Petrie\" ), No. 316. Tune given in the original key. Evidently the air had a persistent association in Ireland with the prize-ring ballad \"Donnelly and Cooper\"; see a close variant, to that piece, in Colm 0 Lochlainn, Irish Street Ballads (Dublin & Lon- don: Constable & Co., 1939), p. 52. A third close variant of the air is in Sabine Baring-Gould, Sonas of the West (revised ed., London: Methuen & Co., 1905), p. 38, to \"The Seasons of the Year.\" B is \"Y' Deryn Du Pigfelyn\" (The Golden-beaked blackbird), Maria Jane Williams, Ancient National Airs of Gwent and Morganwg (Llandovery, 1844), pp. 12, 13. In original key. Note, p. 78, begins \"Very commonly sung in South Wales.\" \nThis content downloaded from 106.168.234.226 on Sat, 30 Aug 2014 01:27:51 AMAll use subject to JSTOR Terms and ConditionsD: CCCCGC-GAb-bGGAE: FEECCCCGAbCbAGFF: GCCCCCCGAb-GAGF a)#b)#\nC D E F G A B     \nProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 163   Automated sequence alignment requires a number of pa-rameters to be defined. The choice of values for these pa-rameters depends on the nature of the data and the goals of classification. Because automated tune family identifi-cation remains largely unexplored, we don’t yet know which values are most appropriate for this goal. There-fore, we tested several values for each parameter to allow for empirical comparison of which parameter values per-formed best. When possible, we tested values that have worked well in similar work on protein family identifica-tion and automated melodic similarity algorithms.  3.1.1 Gap penalties The functional mechanisms of protein structure result in substitutions being much more common than indels (in-sertions/deletions). Thus, most amino acid alignment al-gorithms set a gap opening penalty (GOP) parameter to be quite high to penalize the creation of gaps in a se-quence. However, when indels do occur, they often en-compass not only one amino acid residue, but rather can include fairly long sections. Thus gap extension penalties (GEP) are usually set to be substantially smaller than gap opening penalties (the default values for the popular ClustalW algorithm are for GOP and GEP values of 15 and 6.66, respectively [22]). The mechanisms of musical sequence evolution are less well known, but previous tune family research sug-gests that insertion/deletion (e.g., of ornamentation) is quite common and may even be more common than sub-stitution of different pitches. Thus, it seemed desirable to examine the effect of using a range of GOP and GEP val-ues, ranging from the combination of GOP=0.8, GEP=0.2 used to align tunes in [25], to the amino acid alignment values given above. To do this, we chose GOP values of .8, 4, 8, 12, and 16, for each of which we tested GOP:GEP ratios of both 2 and 4. Thus, the gap penalty parameters ranged from minimums of GOP=0.8, GEP=0.2 (GOP:GEP ratio=4) to maximums of GOP=16, GEP=8 (GOP:GEP ratio =2). For all gap penalty parame-ters we followed previous tune family research [25] in using the Needleman-Wunsch alignment algorithm [17], as implemented in the Biostrings package in R V3.1.1 [19]. 3.1.2 Pitch There are various possibilities for weighting pitches to accommodate different degrees of similarity beyond sim-ple match and mismatch. Previous weighting schemes using interval consonance or interval size have shown minimal improvement over a simple match/mismatch model [25]. Here we instead explore a novel weighting scheme based on qualitative tune family research that has found that tunes will sometimes change mode (i.e., some or all scale degrees may become flattened or sharped to shift from major to minor or vice-versa [3]). To do this, we simply treated an alignment of major and minor ver-sions of each scale degree as a match (i.e., treating lower-case letters in Fig. 2 as capitals). 3.1.3 Rhythm/text Previous tune family research has suggested that some notes are likely to be more evolutionarily stable than oth-ers. In particular, notes that are rhythmically accented [6] or that carry text [11] are proposed to be more reliable in identifying tune families than rhythmically unaccented or non-text-carrying notes, respectively. To examine these possibilities, we contrasted the results using the full se-quences with those using shorter sequences created by excluding rhythmically unaccented notes (i.e., notes not falling on the first beat of a measure) or non-text-carrying notes (e.g., notes where the vowel is held over from a previous note) from the full sequences. 3.1.4 Summary In sum, we tested all possible combinations of the follow-ing parameters: 1) Gap opening penalty: i) .8, ii) 4, iii) 8, iv) 12 or v) 16 2) Gap opening penalty : Gap extension penalty (GOP:GEP) ratio: i) 2 or ii) 4 3) Pitch: i) including or ii) ignoring mode 4) Rhythm: i) including or ii) ignoring rhythmically unaccented notes 5) Text: i) including or ii) ignoring non-text-carrying notes This gave a total of 5x2x2x2x2=80 parameter combina-tions to explore, the average values of which are reported in Table 1. 3.2 Evaluation In order to achieve our goal of automated identification and alignment for the purpose of reconstructing tune fam-ily evolution, we need a method of quantifying how well a given alignment captures the manual judgments of ex-perts. The goal is to maximize both the degree of match in the alignment within tune families and the degree of accuracy in separating between tune families.  3.2.1 Sequence alignment To evaluate alignment within tune families, we need a measure of the degree to which the similarities between sequences captured by the automated alignment matched similarities captured by the manual alignments. For this, we adopted the Mantel distance matrix correlation test [13]. The Mantel r-value is identical to a standard Pear-son correlation r-value, but the Mantel significance test controls for the fact that pairwise distance values in a dis-tance matrix are not independent of one another. We adopted the simplest method for comparing pairs of sequences, which is by calculating their percent identi-ty (PID).  This is calculated based on the number of aligned pitches that are identical (ID) divided by the se-quence length (L) according to the following equation:  PID=100IDL1+L22!\"###$%&&&                                   (1) 164 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   This equation uses the average length of both sequences as the denominator, as this appears to be the most con-sistent measure of percent identity when dealing with cases where the sequences have unequal lengths due to the insertion/deletion of large segments [15] (as occurs in our dataset).   3.2.2 Tune family identification To evaluate separation between tune families, we need a measure of the degree to which our automated clustering into tune families matches the manual tune family classi-fications. This needs to take into account both true posi-tives (tunes correctly grouped into a given tune family) and false positives (tunes incorrectly grouped into a given tune family).  A method used previously by van Kranenburg et al. [25], used the true positive rate (tpr) and false positive rate (fpr) to calculate a score J as follows:  J=tpr1+fpr                                   (2) Because van Kranenburg et al. did not have a method for automatically identifying boundaries between tune fami-lies, they used a “nearest neighbor” criterion to define true positives. Thus, J represents the proportion of tunes whose nearest neighbor (tune with highest automatically measured similarity) is also in the same (manually identi-fied) tune family. Here we calculate this J score, as well as a second J score that more directly tests our goal of identifying boundaries between tune families.  For this second J score, the criterion used to define true positives is of significant sequence similarity for each pair of tunes. Significance is assessed by a random permutation test, in which the PID value for a given pair of sequence is compared against the distribution of 100 random PID values given the same sequence lengths and compositions, as calculated by randomly reordering one of the sequences [8]. Thus, when calculating this second J score, bold values within the boxes in Table 2 (i.e., sig-nificant sequence similarity between pairs of tunes manu-ally identified as belonging to the same tune family) are counted as true positives, while bold values outside of the boxes (i.e., significant sequence similarity between pairs of tunes not manually identified as belonging to the same tune family) are counted as false positives.  4. RESULTS The average scores under the different alignment parame-ters are shown in Table 1, with the best-performing pa-rameter values highlighted in bold. 4.1 Sequence alignment (within-family) The degree to which similarities within tune families cap-tured by the automated alignment match those captured by the manual alignments of experts are indexed by the Mantel correlation r-values, reported in Table 1. On aver-age, all of the alignment parameter combinations gave similarly strong correlations ranging from r=.82-.85.    Within-family Between-family Automated alignment parameter Parameter value r J (nearest neighbor) J (signif-icance) GOP .8 0.850 0.875 0.408 4 0.843 0.870 0.421 8 0.823 0.849 0.479 12 0.833 0.877 0.497 16 0.829 0.844 0.474 GOP:GEP ratio 2 0.834 0.862 0.462 4 0.837 0.864 0.450 Mode Included 0.839 0.841 0.445 Ignored 0.832 0.885 0.467 Rhythmically unaccented notes Included 0.841 0.964 0.587 Ignored 0.830 0.762 0.325 Non-text notes Included 0.838 0.873 0.460 Ignored 0.833 0.853 0.452 Table 1. Mean values comparing different automated alignment parameters against manual ground-truth align-ments. Best-performing values are highlighted in bold. See Methods for details.  4.2 Tune family identification (between-family) The degree to which the automated algorithms were able to separate between tune families is indexed by the J scores, reported in the right-hand columns of Table 1. Us-ing gap opening penalties of 12, ignoring mode, including non-text notes, and especially including rhythmically un-accented notes all improved tune-family identification. GOP:GEP ratios of 4 gave slightly higher J scores using the nearest neighbor criterion, but a ratio of 2 gave higher J scores using the more crucial criterion of significant pairwise sequence similarity. The specific parameter combination combining the best-performing parameter values - GOP=12, GOP:GEP ratio=2, ignoring mode, in-cluding rhythmically unaccented notes and including non-text notes - resulted in a Mantel correlation of r=.83 and J scores of J=1 and J=.64 for the nearest neighbor and significance criteria, respectively.   It was not possible to directly compare all parameters using the approach presented in [25], in part because the approach in [25] is based on sequences of pairwise me-lodic intervals, whereas the manual alignments that formed our ground-truth dataset were based on sequences of individual notes in relation to the tonic (i.e., tonic in-tervals). However, it was possible to directly compare between-family identification J scores using the best-performing parameter combination listed above, but using sequences of melodic intervals rather than tonic intervals. This melodic interval approach resulted in J scores of J=.88 and J=.33 for the nearest neighbor and significance criteria, respectively. These values were somewhat lower than the respective values using our tonic interval ap-proach (J=1 and J=.64). However, further analyses are required to determine the degree to which incorporating  Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 165   \nTable 2. Pairwise percent identity scores among the 26 tunes. Tunes are labeled based on manual classifications by musicologists [3], [12]. Numbers correspond to the four tune families (1=”Brave Donnelly”, 2=”Job of Jour-neywork”, 3=”Oiwake”, 4= “Okesa”), letters correspond to the different variant tunes within each family. The val-ues in the lower triangle are based on automated align-ments using the best-performing parameters (GOP=12, GOP:GEP ratio=2, ignoring mode, including rhythmically unaccented notes and including non-text notes). The val-ues in the upper triangle are based on manual alignments. Inter-tune family manual values are not shown because manual alignments were only done within tune families. Solid borders indicate automatically identified tune fami-lies in which at least three tunes are all significantly simi-lar to one another. When these did not capture all tunes in a manually identified tune family, the manually identified boundaries are shown using dashed borders. Bold values indicates pairs whose similarities are significant at P<.05.  more fine-grained weighting of intervals, rhythmic in-formation, etc. of the type used in [25] affects tune family identification using both melodic interval and tonic inter-val approaches.     4.3 Overall reconstruction of tune family evolution The results of the top-performing parameter combination listed above are compared against manual classifications   in Table 2 and Fig. 3. The lower triangle in Table 2 gives the raw pairwise sequence identity values, using bold text to indicate pairs of sequences whose similarities were sta-tistically significant, while the upper diagonal gives with-in-family sequence identity values for the manual align-ments. The mean percent identity values were somewhat higher for the automated alignments than the manual alignments within each family (45.7% vs. 33.7%, respec-tively). This presumably reflects the automated alignment identifying more false links, although in some cases it may also be identifying better alignments than the manual ones. Comparison with manual alignments conducted by different musicologists may help to clarify this issue in the future. Fig. 3 summarizes the information in Table 2 visually using a NeighborNet diagram. NeighborNet is a type of phylogenetic network that is similar to a neighbor-joining tree, but allows visualization of conflicting non-tree like structure (“reticulation”). 100% of the tunes (26/26) were correctly grouped such that their nearest neighbor was a member of the same tune family, and the sub-grouping of tune family 2 also corresponded to Bayard’s sub-grouping into a “long” and “short” version. However, on-ly 85% (22/26) of these tunes were automatically grouped into a tune family using the criterion that all pairs within a family must be significantly similar to one another. Us-ing this criterion also mis-identified the “long” and “short” versions of tune family 2 as two distinct tune  1A 1B 1C 1D 1E 1F 2A 2B 2C 2D 2E 2F 2G 2H 2I 2J 2K 3A 3B 3C 3D 4A 4B 4C 4D 4E 1A  33 45 42 52 38                     1B 51  29 37 31 28                     1C 59 47  34 28 38                     1D 47 47 48  40 32                     1E 62 54 43 45  48                     1F 53 43 50 48 61                      2A 41 36 34 37 34 36  49 32 23 27 19 19 18 13 15 16          2B 35 38 44 37 45 38 54  51 50 31 25 23 26 20 18 21          2C 40 49 41 39 40 41 47 57  44 41 23 34 28 28 21 16          2D 33 41 42 34 33 35 45 54 61  29 19 19 26 22 18 12          2E 31 34 43 39 36 42 45 48 57 44  32 27 21 22 21 23          2F 43 37 41 36 46 34 39 48 41 45 35  28 16 22 22 29          2G 38 34 41 34 39 31 34 36 42 40 41 55  34 33 43 29          2H 31 33 30 31 38 30 37 45 41 43 33 36 47  36 62 37          2I 44 35 34 34 45 36 28 28 42 35 39 30 35 46  44 24          2J 40 38 28 29 38 35 26 35 39 34 31 39 55 62 49  41          2K 36 34 35 30 28 31 31 41 46 45 34 43 45 48 30 39           3A 32 51 36 37 29 33 31 40 42 34 35 35 42 38 31 38 43  64 44 47      3B 40 40 35 36 32 30 36 43 46 40 38 39 40 36 33 41 32 61  57 55      3C 42 42 38 35 40 45 30 38 34 33 38 41 39 25 29 35 39 51 62  73      3D 38 45 37 44 37 38 25 36 30 43 37 44 29 28 23 36 31 56 60 67       4A 40 40 28 31 39 40 26 29 34 27 31 28 27 31 40 32 27 27 29 29 23  32 39 35 33 4B 32 29 33 38 39 36 27 29 35 28 39 30 27 24 30 30 22 35 32 28 38 40  43 45 44 4C 31 23 36 33 31 40 26 31 28 27 38 34 18 24 21 19 25 23 29 31 30 37 52  67 61 4D 35 26 27 30 33 36 28 26 36 28 35 31 26 25 27 22 21 26 30 21 24 41 55 65  78 4E 32 32 35 28 39 32 27 30 40 32 41 33 26 27 32 29 23 31 33 36 28 42 62 56 62  166 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   \n Figure 3. A NeighborNet visualization of the phyloge-netic relationships among the 26 tunes automatically identified by the best-performing alignment algorithm. See Table 2 for explanation of tune labels 1A-4E and  solid/dashed lines.  families. Joining families into “superfamilies” when only one or a few members have significant similarities to members of other families [8] would join the “long” and “short” versions into a superfamily, but would also join all the tune families into this superfamily.  5. DISCUSSION AND FUTURE WORK Although previous research suggested that tune family identification was “too ambitious to perform automatical-ly” [24], we have presented an automated approach that successfully recovers most of the key relationships within and between tune families identified manually by musi-cologists. Our approach adapts sequence alignment algo-rithms for protein family identification to successfully delineate the boundaries separating groups of melodies that share similar sequences of pitches due to descent from a common ancestor.  Our approach correctly identified three out of the four manually identified tune families, as well as both the “long version” and “short version” sub-groups of the fourth “Job of Journeywork” tune family. However, our automated approach failed to unite these sub-groups into a single tune family, instead splitting them into two tune families. The “Job of Journeywork” tune family was spe-cifically chosen by Bayard [3] to present one of the most complicated examples of tune family evolution, including several measures that were deleted from the beginning of the “long version” and added to the end of the “short ver-sion”. Hence, this type of complex evolution may require more complex algorithms and/or the incorporation of ex-pert knowledge beyond the basic pitch sequence infor-mation encoded in the simplified model used here. How-ever, the fact that our approach captured the relationships among the four tunes from the “Oiwake” tune family, de-spite the fact that this family contained both internal and terminal insertion/deletion events of substantial length, suggests that our approach is still able to capture fairly complicated patterns of musical evolution.  One area for improvement of our method is that the false positive rate is somewhat high (see Table 2). We believe that this may be due to the fact that our method is designed primarily to distinguish between chance and common ancestry, and does not do a very good job of dis-tinguishing between common ancestry and convergent evolution. Hence, it appears likely that many of the false positives are due to stylistic similarities shared between unrelated tunes that share similar scales and motivic pat-terns (e.g., 1A and 2A, both Irish tunes in a diatonic ma-jor scale). Horizontal transmission and/or convergent evolution of such traits among phylogenetically unrelated groups have long been known to complicate analysis of tune family evolution [3], [7]. Horizontal transmission and convergent evolution are challenges shared with lan-guage evolution and genetic evolution, and may benefit from methods developed in these fields [1].   In the future we hope to extend our approach to larger datasets, and to incorporate more-sophisticated models of cultural evolution and sequence alignment [1], more-nuanced weighting of musical information (e.g., beyond simple match/mismatch models of pitch, rhythm, and text [24-26]), and higher-level units of musical structure and meaning. In music, as in genetics, the individual notes that make up the sequences have little meaning in them-selves. The phylogenetic analysis of sequences is thus merely the starting point from which to understand how and why these sequences combine to form higher-level functional units (e.g., motives, phrases) that co-evolve with their song texts and cultural contexts of music-making as they are passed down from singer to singer through centuries of oral tradition. Using such infor-mation, we hope to not only identify previously unknown tune family relationships on a wide scale, but also to care-fully reconstruct the histories and mechanisms of tune family evolution to identify general processes governing the cultural evolution of music. The general nature of our approach means that it should be applicable not only to folk music, but also to art music (e.g., European classical music [28], Japanese gagaku [14]) and popular music (e.g., copyright disputes [20]). Understanding the cultural evolution of music should help to identify the mecha-nisms that govern stability and creativity of aesthetic forms, as well as to use this knowledge to help musicians and musical cultures struggling to adapt their intangible cultural heritage to today’s globalized world.  Acknowledgments: We thank H. Oota and H. Matsumae for advice on adapting genetic sequence alignment algo-rithms to music, and S. Brown, T. Currie, and four anon-ymous reviewers for comments on previous drafts of this paper. Funding support for this work was provided by a Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT) scholarship to P.E.S and a Rutherford Discovery Fellowship to Q.D.A.  1A1E1F1B3A3B3C3D2I2J2H2K2G2F2C2D2B2A2E4B4E4D4C4A1D1C0.1(long&version)&(short&version)&\n“Brave&Donnelly”&“Job&of&Journeywork”&“Oiwake”&“Okesa”&Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 167   6. REFERENCES [1] Q. D. Atkinson and R. D. Gray, “Curious parallels and curious connections: Phylogenetic thinking in biology and historical linguistics,” Systematic Biology, vol. 54, no. 4, pp. 513–526, 2005. [2] S. P. Bayard, “Prolegomena to a study of the principal melodic families of British-American folk song,” Journal of American Folklore, vol. 63, no. 247, pp. 1–44, 1950. [3] S. P. Bayard, “Two representative tune families of British tradition,” Midwest Folklore, vol. 4, no. 1, pp. 13–33, 1954.  [4] C. Boiles, “Reconstruction of proto-melody,” Anuario Interamericano de Investigacion Musical, vol. 9, pp. 45–63, 1973. [5] B. H. Bronson, The traditional tunes of the Child ballads: With their texts, according to the extant records of Great Britain and America [4 volumes]. Princeton, NJ: Princeton University Press, 1959-1972. [6] B. H. Bronson, “Toward the comparative analysis of British-American folk tunes,” Journal of American Folklore, vol. 72, no. 284, pp. 165–191, 1959. [7] J. R. Cowdery, “A fresh look at the concept of tune family,” Ethnomusicology, vol. 28, no. 3, pp. 495–504, 1984. [8] R. F. Doolittle, “Similar amino acid sequences: Chance or common ancestry?,” Science, vol. 214, no. 4517, pp. 149–159, 1981. [9] P. Ferraro and P. Hanna, “Optimizations of local edition for evaluating similarity between monophonic musical sequences,” Proceedings of the International Conference on Computer-Assisted Information Retrieval, pp. 64-69, 2007. [10] International Folk Music Council, “Resolutions: Definition of folk music,” Journal of the International Folk Music Council, vol. 7, p. 23, 1955. [11] A. Kaneshiro, “Kashi onretsuhou ni yoru Oiwakebushi no hikaku [Comparison of Oiwake melodies through lyric-note alignment],” Minzoku Ongaku, vol. 5, no. 1, pp. 30–36, 1990. [12] K. Machida and T. Takeuchi, Eds., Esashi Oiwake to Sado Okesa: Min’yo genryuukou [Folk song geneologies: Esashi Oiwake and Sado Okesa] [4 LPs]. Kawasaki: Columbia. AL-5047/50, 1965. [13] N. Mantel, “The detection of disease clustering and a generalized regression approach,” Cancer Research, vol. 27, no. 2, pp. 209–220, 1967. [14] A. Marett, “Togaku: Where have the Tang melodies gone, and where have the new melodies come from?,” Ethnomusicology, vol. 29, no. 3, pp. 409–431, 1985. [15] A. C. W. May, “Percent sequence identity: The need to be explicit,” Structure, vol. 12, pp. 737–738, May 2004. [16] M. Mongeau and D. Sankoff, “Comparison of musical sequences,” Computers and the Humanities, vol. 24, pp. 161–175, 1990. [17] S. B. Needleman and C. D. Wunsch, “A general method applicable to the search for similarities in the amino acid sequence of two proteins,” Jour-nal of Molecular Biology, vol. 48, pp. 443–453, 1970.  [18] NHK (Nippon Hōsō Kyōkai), Ed., Nihon min’yō taikan [Japanese folk song anthology] [13 volumes]. Tokyo: NHK, 1944-1994. [19] R Development Core Team, R: A language and environment for statistical computing. Vienna: R Foundation for Statistical Computing, 2011. [20] M. Robine, P. Hanna, P. Ferraro, and J. Allali, “Adaptation of string matching algorithms for identificaton of near-duplicate music documents,” Proceedings of the Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection, pp. 37–43, 2007. [21] P. E. Savage and S. Brown, “Toward a new comparative musicology,” Analytical Approaches to World Music, vol. 2, no. 2, pp. 148–197, 2013. [22] J. D. Thompson, D. G. Higgins, and T. J. Gibson, “CLUSTAL W: Improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice,” Nucleic Acids Research, vol. 22, no. 22, pp. 4673–4680, 1994. [23] J. Urbano, J. Lloréns, J. Morato, and S. Sánchez-cuadrado, “Melodic similarity through shape similarity,” Proceedings of the International Symposium on Computer Music Modeling and Retrieval, pp. 338–355,  2011. [24] P. van Kranenburg, J. Garbers, A. Volk, F. Wiering, L. Grijp, and R. C. Veltkamp, “Towards integration of MIR and folk song research,” Pro-ceedings of the International Symposium on Mu-sic Information Retrieval, pp. 505–508, 2007. [25] P. van Kranenburg, A. Volk, and F. Wiering, “A comparison between global and local features for computational classification of folk song melodies,” Journal of New Music Research, vol. 42, no. 1, pp. 1–18, 2013. [26] P. van Kranenburg, A. Volk, F. Wiering, and R. C. Veltkamp, “Musical models for folk-song melody alignment,” Proceedings of the Interna-tional Symposium on Music Information Retrieval, pp. 507–512, 2009. [27] A. Whiten, R. A. Hinde, C. B. Stringer, and K. N. Laland, Culture evolves. Oxford: Oxford University Press, 2012. [28] H. F. Windram, T. Charlston, and C. J. Howe, “A phylogenetic analysis of Orlando Gibbons’s Prelude in G,” Early Music, vol. 42, no. 4, pp. 515–528, 2014. 168 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.",
        "author": [
            "Jan Schlüter",
            "Thomas Grill"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417745",
        "url": "https://doi.org/10.5281/zenodo.1417745",
        "ee": "https://zenodo.org/records/1417745/files/SchluterG15.pdf",
        "abstract": "In computer vision, state-of-the-art object recognition sys- tems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically ex- plored for music signals. Using the problem of singing voice detection with neural networks as an example, we ap- ply a range of label-preserving audio transformations to as- sess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shift- ing to be the most helpful augmentation method. Com- bined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public data- sets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval.",
        "zenodo_id": 1417745,
        "dblp_key": "conf/ismir/SchluterG15",
        "keywords": [
            "label-preserving image transformations",
            "augmenting training datasets",
            "learn invariances",
            "improve generalization",
            "singing voice detection",
            "neural networks",
            "audio transformations",
            "pitch shifting",
            "time stretching",
            "random frequency filtering"
        ],
        "content": "EXPLORING DATA AUGMENTATION FOR IMPROVEDSINGING VOICE DETECTION WITH NEURAL NETWORKSJan Schlüter and Thomas GrillAustrian Research Institute for Artiﬁcial Intelligence, Viennajan.schlueter@ofai.at thomas.grill@ofai.atABSTRACTIn computer vision, state-of-the-art object recognition sys-tems rely on label-preserving image transformations suchas scaling and rotation to augment the training datasets.The additional training examples help the system to learninvariances that are difﬁcult to build into the model, andimprove generalization to unseen data. To the best of ourknowledge, this approach has not been systematically ex-plored for music signals. Using the problem of singingvoice detection with neural networks as an example, we ap-ply a range of label-preserving audio transformations to as-sess their utility for music data augmentation. In line withrecent research in speech recognition, we ﬁnd pitch shift-ing to be the most helpful augmentation method. Com-bined with time stretching and random frequency ﬁltering,we achieve a reduction in classiﬁcation error between 10and 30%, reaching the state of the art on two public data-sets. We expect that audio data augmentation would yieldsigniﬁcant gains for several other sequence labelling andevent detection tasks in music information retrieval.1. INTRODUCTIONModern approaches for object recognition in images areclosing the gap to human performance [5]. Besides us-ing an architecture tailored towards images (ConvolutionalNeural Networks, CNNs), large datasets and a lot of com-puting power, a key ingredient in building these systems isdata augmentation, the technique of training and/or testingon systematically transformed examples. The transforma-tions are typically chosen to be label-preserving, such thatthey can be trivially used to extend the training set and en-courage the system to become invariant to these transfor-mations. As a complementary measure, at test time, aggre-gating predictions of a system over transformed inputs in-creases robustness against transformations the system hasnot learned to (or not been trained to) be fully invariant to.While even earliest work on CNNs [13] successfullyemploys data augmentation, and research on speech recog-nition – an inspiration for many of the techniques used inc\u0000Jan Schlüter and Thomas Grill.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Jan Schlüter and Thomas Grill. “Ex-ploring Data Augmentation for Improved Singing Voice Detection withNeural Networks”, 16th International Society for Music Information Re-trieval Conference, 2015.music information retrieval (MIR) – has picked it up aswell [9], we could only ﬁnd anecdotal references to it inthe MIR literature [8,18], but no systematic treatment.In this work, we devise a range of label-preserving au-dio transformations and compare their utility for music sig-nals on a benchmark problem. Speciﬁcally, we chose thesequence labelling task of singing voice detection: It iswell-covered, but best reported accuracies on public data-sets are around 90%, suggesting some leeway. Further-more, it does not require profound musical knowledge tosolve, making it an ideal candidate for training a classiﬁeron low-level inputs. This allows observing the effect ofdata augmentation unaffected by engineered features, andunhindered by doubtable ground truth. For the classiﬁer,we chose CNNs, proven powerful enough to pick up in-variances taught by data augmentation in other ﬁelds.The following section will review related work on dataaugmentation in computer vision, speech recognition andmusic information retrieval, as well as the state of the artin singing voice detection. Section 3 describes the methodwe used as our starting point, Section 4 details the aug-mentation methods we applied on top of it, and Section5 presents our ﬁndings. Finally, Section 6 rounds up anddiscusses implications of our work.2. RELATED WORKFor computer vision, a wealth of transformations has beentried and tested: As an early example (1998), Le et al. [13]applied translation, scaling (proportional and dispropor-tional) and horizontal shearing to training images of hand-written digits, improving test error from 0.95% to 0.8%.Krizhevsky et al. [12], in an inﬂuential work on large-scaleobject recognition from natural images, employed trans-lation, horizontal reﬂection, and color variation. They donot provide a detailed comparison, but note that it allowedto train larger networks and the color variations alone im-prove accuracy by 1 percent point. Crucially, most meth-ods also apply speciﬁc transformations at test time [5].In 2013, Jaitly and Hinton [9] pioneered the use of label-preserving audio transformations for speech recognition.They ﬁnd pitch shifting of spectrograms prior to mel ﬁlter-ing at training and test time to reduce phone error rate from21.6% to 20.5%, and report that scaling mel spectra eitherin time or frequency dimensions or constructing examplesfrom perturbated LPC coefﬁcients did not help. Concur-rently, Kanda et al. [10] showed that combining pitch shift-121ing with time stretching and random frequency distortionsreduces word errors by 10%, with pitch shifting provingmost beneﬁcial and effects of the three distortion meth-ods adding up almost linearly. Cui et al. [3] combinedpitch shifting with a method transforming speech to an-other speaker’s voice in feature space and Ragni et al. [20]combined it with unsupervised training, both targetting un-common languages with small datasets. To the best of ourknowledge, this comprises the full body of work on dataaugmentation in speech recognition.In MIR, literature is even more scarce. Li and Chan [18]observed that Mel-Frequency Cepstral Coefﬁcients are sen-sitive to changes in tempo and key, and show that augment-ing the training and/or test data with pitch and tempo trans-forms slightly improves genre recognition accuracy on theGTZAN dataset. While this is a promising ﬁrst step, genreclassiﬁcation is a highly ambiguous task with no clear up-per bound to compare results to. Humphrey et al. [8] ap-plied pitch shifting to generate additional training exam-ples for chord recognition learned by a CNN. For this task,pitch shifting is not label-preserving, but changes the labelin a known way. While test accuracy slightly drops whentrained with augmented data, they do observe increased ro-bustness against transposed input.Current state-of-the-art approaches for singing voice de-tection build on Recurrent Neural Networks (RNNs). Le-glaive et al. [15] trained a bidirectional RNN on mel spec-tra preprocessed with a highly tuned harmonic/percussiveseparation stage. They set the state of the art on the publicJamendo dataset [21], albeit using a “shotgun approach”of training 20 variants and picking the one performing beston the test set. Lehner et al. [16] trained an RNN on a setof ﬁve high-level features, some of which were designedspeciﬁcally for the task. They achieve the second best re-sult on Jamendo and also report results on RWC [4, 19], asecond public dataset. For perspective, we will compareour results to both of these approaches.3. BASE METHODAs a starting point for our experiments, we design a straight-forward system applying CNNs on mel spectrograms.3.1 Feature ExtractionWe subsample and downmix the input signal to 22.05 kHzmono and perform a Short-Time Fourier Transform (STFT)with Hann windows, a frame length of 1024 and hop size of315 samples (yielding 70 frames per second). We discardthe phases and apply a mel ﬁlterbank with 80 triangularﬁlters from 27.5 Hz to 8 kHz, then logarithmize the magni-tudes (after clipping values below10\u00007). Finally, we nor-malize each mel band to zero mean and unit variance overthe training set.3.2 Network architectureAs is customary, our CNN employs three types of feedfor-ward neural network layers: Convolutional layers convolv-ing a stack of 2D inputs with a set of learned 2D kernels,pooling layers subsampling a stack of 2D inputs by takingthe maximum over small groups of neighboring pixels, anddense layers ﬂattening the input to a vector and applying adot product with a learned weight matrix.Speciﬁcally, we apply two3⇥3convolutions of 64 and32 kernels, respectively, followed by3⇥3non-overlappingmax-pooling, two more3⇥3convolutions of 128 and 64kernels, respectively, another3⇥3pooling stage, two denselayers of 256 and 64 units, respectively, and a ﬁnal denselayer of a single sigmoidal output unit. Each hidden layeris followed by ay(x) = max(x/100,x)nonlinearity [1].The architecture is loosely copied from [11], but scaleddown as our datasets are orders of magnitude smaller. Itwas ﬁxed in advance and not optimized further, as the fo-cus of this work lies on data augmentation.3.3 TrainingOur networks are trained on mel spectrogram excerpts of115 frames (~1.6 sec) paired with a label denoting the pres-ence of voice in the central frame.Excerpts are formed with a hop size of 1 frame, result-ing in a huge number of training examples. However, theseare highly redundant: Many excerpts overlap, and excerptsfrom different positions in the same music piece often fea-ture the same instruments and vocalists in the same key.Thus, instead of iterating over a full dataset, we train thenetworks for a ﬁxed number of 40,000 weight updates.While some excerpts are only seen once, this visits eachsong often enough to learn the variation present in the data.Updates are computed with stochastic gradient descent oncross-entropy error using mini-batches of 32 randomly cho-sen examples, Nesterov momentum of 0.95, and a learningrate of 0.01 scaled by 0.85 every 2000 updates. Weightsare initialized from random orthogonal matrices [22].For regularization, we set the target values to0.02and0.98instead of0and1. This avoids driving the output layerweights to larger and larger magnitudes while the networkattempts to have the sigmoid output reach its asymptotesfor training examples it already got correct [14]. We foundthis to be a more effective measure against overﬁtting thanL2 weight regularization. As a complementary measure,we apply 50% dropout [7] to the inputs of all dense layers.All parameters were determined in initial experimentsby monitoring classiﬁcation accuracy at optimal thresholdon validation data, which proved much more reliable thancross-entropy loss or accuracy at a ﬁxed threshold of 0.5.4. DATA AUGMENTATIONWe devised a range of augmentation methods that can beefﬁciently implemented to work on spectrograms or melspectrograms: Two are data-independent, four are speciﬁcto audio data and one is speciﬁc to binary sequence la-belling. All of them can be cheaply applied on-the-ﬂyduring training (some before, some after the mel-scalingstage) while collecting excerpts for the next mini-batch,and all of them have a single parameter modifying the ef-fect strength we will vary in our experiments.122 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015(a) Linear-frequency spectrogram ex-cerpt of 4 sec. The framed part will bemel-scaled and serve as network input.\n(b) Corresponding mel spectrogram.\n(c) Dropout and Gaussian noise.\n(d) Pitch shift of +/-20%.\n(e) Time stretch of +/-20%.\n(f) Loudness of +/-10 dB.\n(g) Random frequency ﬁlters.\n(h) Random ﬁlter responses of up to 10 dB.\n(i) Same ﬁlter responses mapped to mel scale.Figure 1: Illustration of data augmentation methods on spectrograms (0:23–0:27 of “Bucle Paranoideal” by LaBarcaDeSua)4.1 Data-independent MethodsAn obvious way to increase a model’s robustness is to cor-rupt training examples with random noise. We considerdropout– setting inputs to zero with a given probability –and additiveGaussian noisewith a given standard devia-tion. This is fully independent of the kind of data we have,and we apply it directly to the mel spectrograms fed intothe network. Figure 1c shows an example spectrogram ex-cerpt corrupted with 20% dropout and Gaussian noise of\u0000=0.2, respectively.4.2 Audio-speciﬁc MethodsJust like in speech recognition,pitch shiftingandtimestretchingthe audio data by moderate amounts does notchange the label for a lot of MIR tasks. We implementedthis by scaling linear-frequency spectrogram excerpts ver-tically (for pitch shifting) or horizontally (for time stretch-ing), then retaining the (ﬁxed-size) bottom central part,so the bottom is always aligned with 0 Hz, and the cen-ter is always aligned with the label. Finally, the warpedand cropped spectrogram excerpt is mel-scaled, normal-ized and fed to the network. Figure 1a shows a linearspectrogram excerpt along with the cropping borders, andFigures 1d–e show the resulting mel spectrogram excerptwith different amounts of shifting or stretching. Duringtraining, the factor for each example is chosen uniformlyat random1in a given range such as 80% to 120%, and thewidth of the range deﬁnes the effect strength we can vary.1Choosing factors on a logarithmic scale did not improve results.A much simpler idea focuses on invariance toloudness:We scale linear spectrograms by a random factor in a givendecibel range, or, equivalently, add a random offset to log-magnitude mel spectrograms (Figure 1f). Effect strength iscontrolled by the allowed factor (or offset) range.As a fourth method, we apply randomfrequency ﬁltersto the linear spectrogram. Speciﬁcally, we create a ﬁlter re-sponse as a Gaussian functionf(x)=s·exp(0.5·(x\u0000µ)2/\u00002), withµrandomly chosen on a logarithmic scalefrom 150 Hz to 8 kHz,\u0000randomly chosen between 5 and 7semitones, andsrandomly chosen in a given range such as\u000010dB to10dB, the width of the range being varied in ourexperiments. Figure 1h displays 50 of such ﬁlter responses,Figure 1g shows two resulting excerpts. When using thismethod alone, we map responses to the mel scale, loga-rithmize them (Figure 1i) and add them to the mel spectro-grams to avoid the need for mel-scaling on the ﬂy.4.3 Task-speciﬁc MethodFor the detection task considered here, we can easily createadditional training examples with known labels bymixingtwo music excerpts together. For simplicity, we only re-gard the case of blending a given training exampleAwitha randomly chosen negative exampleB, such that the re-sulting mix will inheritA’s label. Mixes are created fromlinear spectrograms asC=( 1\u0000f)·A+f·B, withfchosen uniformly at random between0and0.5, prior tomel-scaling and normalization, but after any other augmen-tations. We control the effect strength via the probabilityof the augmentation being applied to any given example.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 123Figure 2: Classiﬁcation error for different augmentation methods on internal datasets (left:In-House A, right:In-House B)Bars and whiskers indicate the mean and its 95% conﬁdence interval computed from ﬁve repetitions of each experiment.5. EXPERIMENTAL RESULTSWe ﬁrst compare the different augmentation methods inisolation at different augmentation strengths on two inter-nal development datasets, to determine how helpful theyare and how to parameterize them, and then combine thebest methods. In a second set of experiments, we assess theuse of augmentation at test time, both for networks trainedwithout and with data augmentation. Finally, we evaluatethe best system on two public datasets, comparing againstour base system and the state of the art.5.1 DatasetsIn total, we work with four datasets, two of them public:–In-House A:188 30-second preview snippets from an on-line music store, covering a very wide range of genres andorigins. We use 100 ﬁles for training, the remaining onesfor evaluation.–In-House B:149 full-length rock songs. While being farless diverse, this dataset features a lot of electric guitarsthat share characteristics with singing voice. We use 65ﬁles for training, 10 for validation and 74 for testing.–Jamendo:93 full-length Creative Commons songs col-lected and annotated by Ramona et al. [21]. For compar-ison to existing results, we follow the ofﬁcial split of 61ﬁles for training and only 16 ﬁles each for validation andtesting.–RWC:The RWC-Pop collection by Goto et al. [4] con-tains 100 pop songs, with singing voice annotations byMauch et al. [19]. To compare results to Lehner et al.[16], we use the same 5-fold cross-validation split (per-sonal communication).Each dataset includes annotations indicating the pres-ence of vocals with sub-second granularity. Except forRWC, datasets do not contain duplicate artists.5.2 EvaluationAt test time, for each spectrogram excerpt, the networkoutputs a value between 0 and 1 indicating the probabilityof voice being present at the center of the excerpt. Feedingmaximally overlapping excerpts, we obtain a sequence of70 predictions per second. Following Lehner et al. [17],we apply a sliding median ﬁlter of 800 ms to smoothen theoutput, then apply a threshold to obtain binary predictions.We compare these predictions to the ground truth labels toobtain the number of true and false positives and negatives,accumulated over all songs in the test set.While several authors use the F-Score to summarize re-sults, we follow Mauch et al.’s [19] argument that a taskwith over 50% positive examples is not well-suited for adocument retrieval evaluation measure. Instead, we focuson classiﬁcation error, and also report recall and speciﬁty(recall of the negative class).5.3 Results on Internal DatasetsIn our ﬁrst set of experiments, we train our network witheach of the seven different augmentation methods on eachof our two internal datasets, and evaluate it on the (unmod-iﬁed) test sets. We compare classiﬁcation errors at the op-timal binarization threshold to enable a fair comparison ofaugmentation methods unaffected by threshold estimation.Figure 2 depicts our results. The ﬁrst line gives the re-sult of the base system without any data augmentation. Allother lines except for the last three show results with a sin-gle data augmentation method at a particular strength.Corrupting the inputs even with small amounts of noiseclearly just diminishes accuracy. Possibly, its regularizingeffects [2] only apply to simpler models, as it is not used inrecent object recognition systems either [5, 11, 12]. Pitchshifting in a range of±20%or±30%gives a signiﬁcantreduction in classiﬁcation error of up to 25% relative. Itseems to appropriately ﬁll in some gaps in vocal range un-covered by our small training sets. Time stretching doesnot have a strong effect, indicating that the cues the net-work picked up are not sensitive to tempo. Similarly, ran-dom loudness change does not affect performance. Ran-dom frequency ﬁlters give a modest improvement, with the124 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015MethodErrorRecallSpec.Lehner et al. [16]10.6%90.6%–Leglaive et al. [15]8.5%92.6%–Ours w/o augmentation9.4%90.8%90.5%train augmentation8.0%91.4%92.5%test augmentation9.0%92.0%90.1%train/test augmentation7.7%90.3%94.1%Table 1: Results on JamendoMethodErrorRecallSpec.Lehner et al. [16]7.7%93.4%–Ours w/o augmentation8.2%92.4%90.8%train augmentation7.4%93.6%91.0%test augmentation8.2%93.4%89.4%train/test augmentation7.3%93.5%91.6%Table 2: Results on RWCbest setting at a maximum strength of 10 dB. Mixing innegative examples clearly hurts, but a lot less severely onthe second dataset. Presumably this is because the seconddataset is a lot more homogeneous, and two rock songsmixed together still form a somewhat realistic example,while excerpts randomly mixed from the ﬁrst dataset arefar from anything in the test set. We hoped this would drivethe network to recognize voice irrespectively of the back-ground, but apparently this is too hard or besides the task.The third from last row in Figure 2 shows performancefor combining pitch shifting of±30%, time stretching of±30%and ﬁltering of±10dB. While error reductions donot add up linearly as in [10], we do observe an additional~6% relative improvement over pitch shifting alone.5.4 Test-time AugmentationIn object recognition systems, it is customary to also applya set of augmentations at test time and aggregate predic-tions over the different variants [5, 11, 12]. Here, we av-erage network predictions (before temporal smoothing andthresholding) over the original input and pitch-shifted in-put of\u000020%,\u000010%,+10%and+20%. Unsurprisingly,other augmentations were not helpful at test time: Tempoand loudness changes hardly affected training either, andall remaining methods corrupt data.The last two rows in Figure 2 show results with thismeasure when training without data augmentation and ourchosen combination, respectively. Test-time augmentationis beneﬁcial independently of train-time augmentation, butincreases computational costs of doing predictions.5.5 Final Results on Public DatasetsTo set our results in perspective, we evaluate the base sys-tem on the two public datasets, adding our combined train-time augmentation, test-time pitch-shifting, or both. ForJamendo, we optimize the classiﬁcation threshold on thevalidation set. For RWC, we simply use the optimal thresh-old determined on the ﬁrst internal dataset.As can be seen in Tables 1–2, on both datasets we slight-ly improve upon the state of the art. This shows that aug-mentation did not only help because our base system was aweak starting point, but actually managed to raise the bar.We assume that the methods we compared to would alsobeneﬁt from data augmentation, possibly surpassing ours.6. DISCUSSIONWe evaluated seven label-preserving audio transformationsfor their utility as data augmentation methods on musicdata, using singing voice detection as the benchmark task.Results were mixed: Pitch shifting and random frequencyﬁlters brought a considerable improvement, time stretchingdid not change a lot, but did not seem harmful either, loud-ness changes were ineffective and the remaining methodseven reduced accuracy.The strong inﬂuence of augmentation by pitch shifting,both in training and at test-time, indicates that it would beworthwhile to design the classiﬁer to be more robust topitch shifting in the ﬁrst place. For example, this could beachieved by using log-frequency spectrograms and insert-ing a convolutional layer in the end that spans most of thefrequency dimension, but still allows ﬁlters to be shifted ina limited range.Frequency ﬁltering as the second best method deservescloser attention. The scheme we devised is just one ofmany possibilities, and probably far from optimal. A closerinvestigation of why it helped might lead to more effectiveschemes. An open question relating to this is whether aug-mentation methods should generate (a) realistic examplesakin to the test data, (b) variations that are missing from thetraining and test set, but easy to classify by humans, or (c)corrupted versions that rule out inrobust solutions. For ex-ample, it is imaginable that narrow-band ﬁlters removingfrequency components at random would force a classiﬁerto always take all harmonics into account.Regarding the task of singing voice detection, better so-lutions would be reached by training larger CNNs or bag-ging multiple networks, and faster solutions by extract-ing the knowledge into smaller models [6]. In addition,adding recurrent connections to the hidden layers mighthelp the network to take into account more context in alight-weight way, allowing to reduce the input (and thus,the dense layer) size by a large margin.Finally, we expect that data augmentation would provebeneﬁcial for a range of other MIR tasks, especially thoseoperating on a low level.7. ACKNOWLEDGMENTSThis research is funded by the Federal Ministry for Trans-port, Innovation & Technology (BMVIT) and the AustrianScience Fund (FWF): TRP 307-N23, and the Vienna Sci-ence and Technology Fund (WWTF): MA14-018. We alsogratefully acknowledge the support of NVIDIA Corpora-tion with the donation of a Tesla K40 GPU used for thisresearch. Last but not least, we thank Bernhard Lehner forfruitful discussions on singing voice detection.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1258. REFERENCES[1]A. Jannun A. Maas and A. Ng. Rectiﬁer nonlineari-ties improve neural network acoustic models. InInt.Conf. on Machine Learning (ICML) Workshop on DeepLearning for Audio, Speech, and Language Processing,2013.[2]Guozhong An. The effects of adding noise duringbackpropagation training on a generalization perfor-mance.Neural Comput., 8(3):643–674, April 1996.[3]Xiaodong Cui, Vaibhava Goel, and Brian Kingsbury.Data Augmentation for Deep Neural Network Acous-tic Modeling. InProc. of the 2014 IEEE Int. Conf.on Acoustics, Speech and Signal Processing (ICASSP),Florence, Italy, 2014.[4]Masataka Goto, Hiroki Hashiguchi, TakuichiNishimura, and Ryuichi Oka. RWC music database:Popular, classical, and jazz music databases. InProc.of the 3rd Int. Conf. on Music Information Retrieval(ISMIR), pages 287–288, October 2002.[5]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. Delving deep into rectiﬁers: Surpassing human-level performance on ImageNet classiﬁcation.CoRR,abs/1502.01852, February 2015.[6]G. Hinton, O. Vinyals, and J. Dean. Distilling theKnowledge in a Neural Network. ArXiv:1503.02531,March 2015.[7]G.E. Hinton, N. Srivastava, A. Krizhevsky,I. Sutskever, and R.R. Salakhutdinov. Improvingneural networks by preventing co-adaptation of featuredetectors. arXiv:1207.0580, July 2012.[8]Eric J. Humphrey and Juan P. Bello. Rethinking au-tomatic chord recognition with convolutional neuralnetworks. InProc. of the 11th Int. Conf. on MachineLearning and Applications (ICMLA), 2012.[9]Navdeep Jaitly and Geoffrey E. Hinton. Vocal tractlength perturbation (VTLP) improves speech recogni-tion. InInt. Conf. on Machine Learning (ICML) Work-shop on Deep Learning for Audio, Speech, and Lan-guage Processing, 2013.[10]Naoyuki Kanda, Ryu Takeda, and Yasunari Obuchi.Elastic spectral distortion for low resource speechrecognition with deep neural networks. InAutomaticSpeech Recognition and Understanding Workshop(ASRU), Olomouc, Czech Republic, 2013.[11]Karen Simonyan, Andrew Zisserman. Very deep con-volutional networks for large-scale image recognition.InProc. of the 3rd Int. Conf. on Learning Representa-tions (ICLR), May 2015.[12]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-ton. Imagenet classiﬁcation with deep convolutionalneural networks. In F. Pereira, C.J.C. Burges, L. Bot-tou, and K.Q. Weinberger, editors,Advances in NeuralInformation Processing Systems 25, pages 1097–1105.Curran Associates, Inc., 2012.[13]Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.Gradient-based learning applied to document recogni-tion.Proc. of the IEEE, 86(11):2278–2324, November1998.[14]Y. LeCun, L. Bottou, G. Orr, and K. Müller. EfﬁcientBackProp. In G. Orr and Müller K., editors,NeuralNetworks: Tricks of the trade. Springer, 1998.[15]Simon Leglaive, Romain Hennequin, and RolandBadeau. Singing voice detection with deep recurrentneural networks. InProc. of the 2015 IEEE Int. Conf.on Acoustics, Speech and Signal Processing (ICASSP),pages 121–125, Brisbane, Australia, April 2015.[16]Bernhard Lehner, Gerhard Widmer, and SebastianBöck. A low-latency, real-time-capable singing voicedetection method with LSTM recurrent neural net-works. InProc. of the 23th European Signal ProcessingConf. (EUSIPCO), Nice, France, 2015.[17]Bernhard Lehner, Gerhard Widmer, and ReinhardSonnleitner. On the reduction of false positives insinging voice detection. InProc. of the 2014 IEEEInt. Conf. on Acoustics, Speech, and Signal Processing(ICASSP), pages 7530–7534, 2014.[18]Tom LH. Li and Antoni B. Chan. Genre classiﬁcationand the invariance of MFCC features to key and tempo.InProc. of the 17th Int. Conf. on MultiMedia Modeling(MMM), Taipei, Taiwan, 2011.[19]Matthias Mauch, Hiromasa Fujihara, KazuyoshiYoshii, and Masataka Goto. Timbre and melody fea-tures for the recognition of vocal activity and instru-mental solos in polyphonic music. InProc. of the 12thInt. Society for Music Information Retrieval Conf. (IS-MIR), 2011.[20]Anton Ragni, Kate M. Knill, Shakti P. Rath, and MarkJ. F. Gales. Data augmentation for low resource lan-guages. In Haizhou Li, Helen M. Meng, Bin Ma, En-gsiong Chng, and Lei Xie, editors,Proc. of the 15thAnnual Conf. of the Int. Speech Communication Asso-ciation (INTERSPEECH), pages 810–814, Singapore,2014. ISCA.[21]Mathieu Ramona, Gaël Richard, and Bertrand David.Vocal detection in music with support vector machines.InProc. of the 2008 IEEE Int. Conf. on Acoustics,Speech, and Signal Processing (ICASSP), pages 1885–1888, 2008.[22]Andrew M. Saxe, James L. McClelland, and SuryaGanguli. Exact solutions to the nonlinear dynamics oflearning in deep linear neural networks. InInt. Conf.on Learning Representations (ICLR), 2014.126 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Automatic Solfège Assessment.",
        "author": [
            "Rodrigo Schramm",
            "Helena de Souza Nunes",
            "Cláudio Rosito Jung"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414720",
        "url": "https://doi.org/10.5281/zenodo.1414720",
        "ee": "https://zenodo.org/records/1414720/files/SchrammNJ15.pdf",
        "abstract": "This paper presents a note-by-note approach for auto- matic solfège assessment. The proposed system uses me- lodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The tech- nique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individ- ual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student.",
        "zenodo_id": 1414720,
        "dblp_key": "conf/ismir/SchrammNJ15",
        "keywords": [
            "melodic transcription",
            "solfège assessment",
            "audio signal",
            "melodic segments",
            "Bayesian classifier",
            "ground truth",
            "pitch accuracy",
            "onset accuracy",
            "offset accuracy",
            "expert listeners"
        ],
        "content": "AUTOMATIC SOLFÈGE ASSESSMENTRodrigo Schramm1Helena de Souza Nunes2Cláudio Rosito Jung11Institute of Informatics, Federal University of Rio Grande do Sul, Brazil2Department of Music, Federal University of Rio Grande do Sul of Music, Brazilrodrigos@caef.ufrgs.br, helena@caef.ufrgs.br, crjung@inf.ufrgs.brABSTRACTThis paper presents a note-by-note approach for auto-matic solfège assessment. The proposed system uses me-lodic transcription techniques to extract the sung notes fromthe audio signal, and the sequence of melodic segments issubsequently processed by a two stage algorithm. On theﬁrst stage, an aggregation process is introduced to performthe temporal alignment between the transcribed melodyand the music score (ground truth). This stage implicitlyaggregates and links the best combination of the extractedmelodic segments with the expected note in the groundtruth. On the second stage, a statistical method is used toevaluate the accuracy of each detected sung note. The tech-nique is implemented using a Bayesian classiﬁer, which istrained using an audio dataset containing individual scoresprovided by a committee of expert listeners. These individ-ual scores were measured at each musical note, regardingthe pitch, onset, and offset accuracy. Experimental resultsindicate that the classiﬁcation scheme is suitable to be usedas an assessment tool, providing useful feedback to thestudent.1. INTRODUCTIONThe practice of solfège is used by beginner musicians tolearn and improve the ability of the musical reading throughthe repeated singing of musical notes from a music score. Infact, this kind of exercise is a fundamental part of the musiclearning process. It guides the student to build its own mu-sical perceptions by creating an internal image of the soundalong the vocal emission of a note (or sequences of notes asintervals, scales and melodies). The ability to read the noteson a music score and at the same time to hear internallyand to sing thema prima vistais here generically calledsolfège, and it is considered a prerequisite for performanceand effective musical knowledge [13]. During the solfège,is crucial to have a constant feedback by an external expert,who should be responsible for detecting eventual mistakesand pinpoint the best way to ﬁx them. Traditionally, theevaluation process of the solfège is conducted by a musicteacher, inside of a classroom. Nowadays, with the spreadc\u0000Rodrigo Schramm, Helena de Souza Nunes, Cláudio Ros-ito Jung. Licensed under a Creative Commons Attribution 4.0 InternationalLicense (CC BY 4.0).Attribution:Rodrigo Schramm, Helena de SouzaNunes, Cláudio Rosito Jung. “Automatic Solfège Assessment ”, 16thInternational Society for Music Information Retrieval Conference, 2015.of the internet, new educational methods bring up new pos-sibilities to music education using the e-learning paradigm.In the case of large number of evaluations, which is a typi-cal situation in distance learning courses, the labor of theteacher becomes exhaustive and tedious. Even in cases oftraditional and presential music lessons, the judgment by anexpert musician is not a trivial task, specially because thehuman discernment may be affected by subjective factorsand fatigue [14, 20]. Thus, an automatic solfège assessmenttool can be very helpful in this context.Usually, solfège is evaluated by comparing the singingperformance with the target music score (ground truth).In this case, the ﬁrst part of this task have some similari-ties with automatic melodic transcription algorithms [14].However, a set of similarity measures to correlate the userperformance with the expert’s (human) judgment is stillneeded. Although there are some papers that provide anoverall score for a given solfège [10], it is not to our knowl-edge the existence of systems that perform a note-by-noteanalysis, which is very important in music teaching. In thispaper we introduce a new note-by-note evaluation methodbased on the individual scores provided by human evalu-ators. More precisely, we introduce a Bayesian classiﬁerwhich is applied to each musical note detection, working asan alternative to the correlation method based on the globaljudgment score used in [10]. The main difference here isthe fact that the performance can be evaluated with a smallgranularity, at each musical note, but keeping the assess-ment correlated with the human judgment. Additionally, theBayesian approach allows the mapping of the performanceerrors into a conﬁdence measure. We also introduced anew temporal alignment method between the transcribedmelody and the music score (ground truth) by using a clus-tering process. The grouping process was chosen in placeof dynamic time warping (DTW) [12] approach because itis less sensible to error propagation and it does not have anymonotonicity condition.This paper is organized as follows: Section 2 presentsan overview about the related techniques. Section 3 showsa detailed description of the audio database generation andthe corresponding annotation process by the musicians ex-perts. Section 4 describes the proposed method to automaticsolfège assessment. Section 5 presents the results of ourexperiments and Section 6 draws the conclusion of thiswork.1832. RELATED WORKSAs far as we know, there is no method for solfège evaluationin a note-by-note scale. Therefore, this section will revisesome papers that tackle related problems. For example, Jhaand Rao [4] focused on the vowel quality of the singingvoice. The authors use low-level features, including thespectrum envelope and pitch contour for singing evalua-tion. Their algorithm detects the onset of each vowel bysearching for rapid changes in speciﬁc frequency bandsthat characterize the vowel formants, and then correlateseach vowel with an articulatory space by a linear regres-sion scheme. Miryala et al. [8] do not perform assessmentdirectly, but their approach automatically identiﬁes vocalexpressions as voice glides and vibratos, which could bealso used as a kind of singing evaluation.The related problem of melodic transcription has beenstudied by several researchers. The common pipeline on themelody transcription techniques splits the process in low-level feature estimation, note segmentation and labeling,and post processing [3,11]. For example, [19] implementeda melodic transcription algorithm by detecting a sequenceof fundamental frequencies in a frame-wise fashion, whichare subsequently converted into observation probabilitiesand used in a Hidden Markov Model (HMM). Ryynanenand Klapuri [17] implemented a similar approach, but ex-tending the number of low-level features. Thus, besidesthe fundamental frequency estimates, they also mappedinto probabilities distributions the features regarding voice/unvoice, accent, and meter estimation. Frequently, a mu-sicological model is also included in music transcriptionalgorithms to improve the system accuracy, acting as a priorprobability. The authors of [19] also incorporate a durationmodel, which maps probability density functions with thesubdivisions and multiples unities of the beat time. Musico-logical models might be used to detect the tonality and therhythmic structure of the musical performance, constrainingthe output options and consequently improving the accu-racy [5]. Unfortunately, the musicological model cannot bedirectly used asa prioriinformation on assessment toolssince it is not possible to have any expectation about thestudent singing performance.The work by Molina et al. [10], which explores thesinging assessment regarding note-based melodic similari-ties, as well as the temporal alignment between the studentperformance and the target melody, has similar goals toours. Despite the use of note-level similarity measures, theﬁnal evaluation is built using the global assessment scoresfrom the human experts, who had placed a global score (be-tween 1 and 10) for each singing performance on a previoustraining stage. The estimated correlations in [10] seem tobe advantageous to extract a measure of quality in a globalcontext. However, as a drawback, that approach discardlocal (note level) information from the experts’ evaluation.In other words, it is not possible to precisely locate andquantify the note(s) responsible for a bad or good scorefrom the singing performance. A small extension of thisapproach was presented in [6], including new audio spectralfeatures. A recent work [9] shows a taxonomy of evaluationmeasures used in several automatic singing transcriptionsalgorithms. Most of the tabulated approaches have usedevaluation measures for singing transcription algorithmsbased on note/frame-level error. There are also some strate-gies that use time warping alignment information betweenthe ground truth and the transcribed melody [10]. Despitethe variety and effort to build robust and comprehensiveevaluations measures, these previous ideas cannot be di-rectly used in the context of solfège assessment. In fact, theused deﬁnition of correct pitch/ onset/offset in [9] appliesranges of tolerance with ﬁxed values, that may be a reli-able procedure to compare distinct algorithms of melodictranscription. However, it may not agree with the humanjudgment perception in a solfège assessment context. Someauthors [6,10] tried to solve this issue connecting the expertanalysis with the evaluation measures, but the ﬁnal humanevaluation carries out only a global interpretation, lackingin details at individual sung notes. In the next sections wepresent our dataset and proposed model for solfège assess-ment. This model aims to evaluate individual sung notes,giving a note-based feedback that makes a meaningful linkwith the human judgment by musician experts.3. PROPOSED DATASET AND ANNOTATIONSThe proposed dataset consists of sequences of musical inter-vals in the chromatic scale. The audio recordings were doneusing seven adults, including trained (three) and untrained(four) singers ranging from 17 to 61 years old. These me-lodic sequences were recorded during four months, in monoformat with a sample rate of44100Hzand16bits quantiza-tion.It was decided to support the singing process by a refer-ence piano audio track, since a part of the group of singerswas unable to read music scores. In this reference audiotrack, the intervals were played in sequence, but with gapsbetween them. Each singer ﬁlled these gaps repeating theprevious heard melodic interval at the next beat time, andall recording sessions were synchronized by a metronome.The singers were asked to choose and, if possible, todiversify the used phonemes. They were also asked tosing freely, but respecting the pitch, attack and durationof the previously indicated sounds, aiming to capture realexamples of spontaneous everyday singing. Intentionallyaiming to capture a higher variability of natural situations,the recordings were conducted in two distinct environments:a part of the examples was recorded in a studio, where theresulting audio records are clean; another part of the audiorecords was done in informal conditions, presenting somebackground noise and reverberation.A total of 21 sessions were recorded, containing (twelveascending intervals and twelve descendants intervals of thechromatic scale). Each singer performed the melodic in-tervals in three distinct tempos: Adagio, 60 bpm; AndanteModerato, 90 bpm; and Allegro, 120 bpm. Along with therecordings, an annotation process was conducted by a com-mittee of experts (ﬁve graduated musicians with more thanten years of experience in solfège assessment auditions) inorder to label each sung note from the recorded dataset into184 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015two possible categories (correct and incorrect) regardingthe pitch, onset and offset accuracy. Before each annotationsection, the committee was advised to hear some randomsamples from the dataset. This warmup procedure was im-portant because it helped to create an agreement among theexperts, who shared some important characteristics and as-pects of the recorded melodic intervals. As the dataset wasbroken in parts, this process was repeated in several days,until the whole set of audio records had been evaluated (infact, the whole process for building the annotated datasettook several months).For each sung note in the audio dataset, all the ﬁveevaluators casted a vote (correct or incorrect) regardingeach analyzed parameter (pitch, onset and offset). As it willbe explained in the next section, disagreement among theevaluators were kept and used to model our probabilisticclassiﬁer. Also, each note is assigned to a single label(correct or incorrect) regarding to each parameter, based onthe majority of votes cast by the experts (i.e., at least 3 votesfor the same label). Hence, some labels can be consideredmore reliable than others, based on the number of votes.For example, regarding the pitch,15.38%of the samplesreceived 3 votes in agreement, which means an expressivedegree of doubt among the experts. The same analysis wasmade for the onset and offset parameters, and the percentageof notes with 3 votes (doubt) was10.71%and12.09%,respectively. The ﬁnal annotated dataset contains3276labeled samples.4. OUR MODELThe proposed computational model for automatic solfègeevaluation is structured in two main stages. The ﬁrst stageperforms the melodic transcription, using the pYIN algo-rithm [7] to extract the fundamental frequency from theaudio signal. The pYIN algorithm is a modiﬁcation of thesmoothing procedure of the YIN technique [1], introducinga probabilistic variant that outputs multiple pitch candidatesalong with the associated probabilities. It also employs aHidden Markov Model (HMM) based on [16] to perform thepitch tracking, providing an improvement in the accuracyof the standard YIN. The extracted frame-wise sequencef0is then segmented and labeled into segments of musicnotes using the hysteresis approach of [11]. After, in thisstage, we introduced a new alignment procedure, where thetranscribed sung segments are aligned with the music score.This procedure converts group of melodic segments intoatomic unities (music notes) and allows a direct comparison(note against note) between the transcribed melody and theground truth. In the second stage, a probabilistic classiﬁerperforms the note-based evaluation. The algorithm takesthe generated sequence of notes from the previous stageand applies a Bayesian classiﬁer to evaluate the accuracyof the parameters pitch, onset and offset. At this stage, arejection procedure is also introduced to map the doubtfrom the categorization (correct or incorrect) given by theexpert listeners. These stages are described next.4.1 Melodic alignmentTo evaluate the solfège performance, a comparison of thesinging performance with the target music score (groundtruth) is required. Thus, after obtaining the automatic me-lodic transcription (using [7] and [11]), it is still necessaryto connect each transcribed note with its corresponding notein the music score.The ﬁrst challenge is the fact that the melodic transcrip-tion often generates groups of fragmented notes (segments),which should be mapped to only one element of the groundtruth. Each melodic fragment is represented byfil, whereiis the segment index andlis the relative index of eachframe within this segment. Additionally, as in [10], there isno assumption of synchronization by a metronome in ourapproach, so that the transcribed notes might be misaligned.In [10], an integrated dynamic time warping procedure(DTW) was employed to perform the time alignment in aframe-wise fashion. However, in some cases, the bound-ary condition of the DTW algorithm might propagate theaccumulated matching error, which causes an undesirablealignment between the transcribed sequence and the groundtruth.Here, we propose a new alignment process that, at thesame time, groups note fragments and also maps the result-ing block with the correspondent music note in the groundtruth. Despite being similar to the DTW approach, it doesnot propagate the cumulative error since it does not need toobey the boundary condition of the DTW algorithm. Thejoint grouping/alignment process was designed as a bruteforce algorithm that is implemented using a cost matrixC.For each notekin the ground truth, the algorithm computesthe cumulative distance measure considering all possibili-ties of grouping of adjacent segments, starting at segmentindexiand stopping at segment indexj. This algorithm isefﬁciently built with the support of a 3D data structure, asdepicted in Figure 1a. Thus, for each possible combination(k,i,j), a dissimilarity measure is computed asC(k,i,j)=a1Df(k,i,j)+a2Dd(k,i,j)+a3Ds(k,i,j)+a4De(k,i,j),(1)whereDf=|fgtk\u0000median(fi,1...fj,lmax)|(2)is the pitch distance between the ground truth notekandthe median values off0belonging to the range starting atﬁrst frame of the segmentiand ﬁnishing at the last framelmaxof the segmentj,Dd=|Dgtk\u0000jÂm=iDm|(3)measures the duration difference (in seconds) between thenotekin the ground truth (Dgtk) and the group formed fromsegmentitojin the transcribed melody (Diis the durationof segmenti),Ds=|Sgtk\u0000Si|(4)accounts for the delay or advance (in seconds) of the onsetof the ﬁrst segment of the selected group and the groundProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 185k=2i=\nj=Figure 1: (a) 3D structure used to compute the similarities between the transcribed melodic segments and the music score.(b) Grouping process of several segments (gray) into one music note (blue). (c) The best grouping for the notekin theground truth is found by the indexesi(ﬁrst element) andj(last element), which minimize the functionC(k,i,j).truth notek, and analogouslyDe=|Egtk\u0000Ej|(5)accounts for the delay or advance of the offset (in seconds).The coefﬁcientsaiare weights to balance the individualcontribution of each measure, and our experiments showthata1=1.0,a2=2.0,a3=2.0,a4=2.0is a good com-bination.The grouping process and its mapping to the groundtruth sequence is achieved by a functionu(k)=(ik,jk)=argmini,jC(k,i,j),(6)so that each notekis mapped to the group of segments fromindicesi(ﬁrst segment) toj(last segment), obtaining theﬁnal and consolidated transcribed note.The computational complexity of the alignment processin the worst case isO(MN2), whereMis the number ofmusic notes in the ground truth andNis the number of me-lodic segments. However, the inclusion of componentsDsandDein Eq. (1) makes the magnitude of the dissimilaritymeasure to grow fast when the group of segments is farfrom the expected time position. As a consequence, it ispossible to interrupt the brute force search loop in a few it-erations by limiting the value ofC(k,i,j). Furthermore, thewindow of evaluation containing the melodic segments canbe restricted to begin closer to the target note. This processwill also decrease the computational cost and also avoideventual local minimum issues in Eq. (6). Figure 1b illus-trates one example of the grouping and alignment process,in which six segments are mapped into three notes.4.2 Note-based evaluationAfter the alignment achieved by the melodic transcription,the system performs the note-based assessment. Distinctprobability density functions are modeled to represent thecorrect and incorrect sung notes, regarding individually tothe pitch (Df, in midi scale), onset (Ds, in seconds) andoffset (De, in seconds) deviations. For each sung note, aBayesian classiﬁer assigns the parameters pitch, onset andoffset into correctjor incorrectjcategories. Next, theBayesian classiﬁcation process will be explained, focusingon theDf(pitch) parameter. However, it is worth notingthat the classiﬁcation process is also individually applied toDsandDein an analogous way.Figure 2a shows the histograms of the pitch deviationsfor correct and incorrect categories based on the expert’sevaluation, denoted byjDfandjDf, respectively. As it canbe observed, the histogram ofjDfpresents a sharp peakclose the origin (related to low pitch errors), as expected.Nevertheless, the two categories present considerable over-lap, corroborating the discrepancies in the accuracy evalua-tion by experts when for intermediate errors in the pitch. Infact, since we had used the individual ratings of each notefrom all evaluators to build de histograms, the pitch devi-ationDfrelated to a note that received conﬂicting labelsamong the evaluators contributes both for the histograms ofjDfandjDf.A conditional probability density function is then es-timated from the distributions ofDffor each classr2{jDf,jDf}, so that a posterior probability (that can be con-sidered a measure of conﬁdence) can be easily obtained.Among several existing parametric probability density func-tions (PDFs) for modeling positive random variables, theGamma distribution was chosen because it has been suc-cessfully used to model similar problems [18], which havesimilar characteristics to our data, such as single mode andfrequently skewed shape. ThegammaPDF, parameterizedby the two positive parameters shapearand scaleqr, isgiven by:p(Df|r)⇠Ga(Df;ar,qr)=Dfar\u00001e\u0000drqrG(ar)qarr,(7)whereGis thegammafunction.The shape (ar) and scale (qr) parameters for each classr2{j,j}were estimated using a maximum likelihoodapproach [15]. Given the PDFsp(Df|j)andp(Df|j),we can estimate theposteriorprobability of the pitch of acorrect/incorrect sung note by using the Bayes rule [2]:p(r|Df)=p(Df|r)P(r)p(Df),(8)wherep(Df)=p(Df|j)P(j)+p(Df|j)P(j)is the over-all distribution ofDf, and thepriorprobabilitiesP(j)andP(j)are deﬁned as equiprobable.Figure 2b illustrates the decision boundary forjDfandjDfas a red vertical dashed line, and it can be observed186 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 201500.20.40.60.800.10.20.30.40.5\n\u0000fp(\u0000f|')\n  \u0000f(samples)p(\u0000f|')⇠Gamma\n00.20.40.60.800.10.20.30.40.5\n\u0000fp(\u0000f|')\n  \u0000f(samples)p(\u0000f|')⇠Gamma\n(a)\n00.10.20.30.40.50.60.70.80.9100.20.40.60.81\nDfP(r|Df)P(j|Df)P(j|Df)DfB1\u0000TDf\n(b)Figure 2: (a) Histogram ofDffor classesjandjalongwith ﬁtted Gamma PDFs. (b) Posterior probabilities, alongwith acceptance and rejection regions.that there is a “fuzzy” decision boundary around it. In thisregion, there is considerable overlap betweenp(Df|j)andp(Df|j), causing the winning posterior probability to bejust a little above 0.5. Since this overlap region is caused inpart by conﬂicting labels from the evaluators, an appropriateoption is to reject samples that fall inside this fuzzy region.As in [18], the errors (or misclassiﬁcations) are convertedinto rejects using the Bayes rejection rule for the minimumerror [21]. The rejection rule splits the sample space into anacceptance regionAand a rejection regionR, that is givenby:R(TDf)={Df|1\u0000maxrp(r|Df)>TDf},(9)A(TDf)={Df|1\u0000maxrp(r|Df)TDf},(10)where the thresholdTDfbalances the tradeoff between thenumber of rejected samples and the error ratee(TDf), givenby:e(TDf)=ÂDf2A(TDf)⇣1\u0000maxrp(r|Df)⌘p(Df).(11)The choice of the thresholdTDf=0.33was determinedfrom a set of experiments where the classiﬁcation accuracyand the number of rejections were taken into account (moredetails about this choice are presented in section 5). Theposterior probabilities and the boundaries between regionsAandRgenerated by this threshold are shown in Figure 2b.Thus, regarding the pitch accuracy and using theBayesian classiﬁer given by Eq. (8) in combination withthe rejection procedure provided by Eqs. (9) and (10), eachsung note is classiﬁed into three possible classes: correct,incorrect, or undetermined (reject). When a classiﬁcation isdone (correct or incorrect), the corresponding probabilitymeasure is also used to provide a meaningful feedback ofconﬁdence to the user. The whole note-based evaluationprocess is also done independently for the onset and offsetnote accuracy. This means that, for each sung note, thesystem output gives individual class labels and conﬁdencemeasures for pitch, onset and offset.5. EXPERIMENTAL RESULTSAiming to extract an objective evaluation of the proposedsolfège assessment system, a set of experiments were con-ducted using the annotated audio dataset described in Sec-tion 3. From the audio recordings, we extracted the melodictranscriptions, which were subsequently aligned with theground truth, as described in Section 4.1. The pitch, onsetand offset deviations (Df,DsandDe) were computed fromthe comparison between the ground truth and the alignedmelodies, and a subset of the samples was used to estimatethe parameters required in the corresponding Gamma PDFs.The remaining samples were reserved to test the model.For the validation scheme, we used a10-fold cross-validation scheme, in which the dataset is split randomlyinto ten equal parts. For each round of the cross-validation,9folds are used to train the probabilistic model and theremaining fold is used to validate the Bayesian classiﬁerdescribed in Section 4.2. In our experiments, we used theBayesian classiﬁer with and without the rejection rule. Inboth situations, the system classiﬁes each parameter (pitch,onset, offset) of each sung note in two possibles categories:correct or incorrect (when the rejection rule was applied,some notes were kept unclassiﬁed).Table 1 shows the confusion matrices generated by theBayesian classiﬁers for the pitch, onset and offset withoutthe rejection rule, and the accuracy is over 90% for thethree analyzed parameters. Also, the system tends to pro-duce more false negatives (i.e., mark as incorrect a correctlysung note) then false positives, particularly for the offsetparameters, being a “rigid” evaluator. The misclassiﬁcationerrors are caused by two main reasons: ﬁrst, a possible badmelodic transcription and/or bad alignment between thesung fragments and the ground truth can introduce errorson the similarities measures; second, the disagreement be-tween the human evaluators generated an inherently fuzzyregion near to the decision boundary. In fact, as noted inSection 3, 10 to 15% of the notes presented strong disagree-ment among the evaluators, so that the ground truth labelmay not be reliable.The rejection rule provided by Eq. 9 avoids the classi-ﬁcation of samples that potentially fall inside this fuzzyregion. The effect of varying the rejection thresholds in thepercentage of accepted samples and also the accuracy forthe pitch, onset and offset analysis is shown in Figure 3.As expected, lower thresholds decrease the number of ac-cepted samples and increases the accuracy rate. Althoughthe deﬁnition of an optimal value for the threshold is difﬁ-cult, the accuracy should be as maximum as possible whilethe number of rejected samples should be minimal. As thefocus of this work is on music education, we believe it isProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 187Target ClassOutput ClassjDfjDfjDf88.99%11.01%jDf7.27%92.73%90.86%(a) Pitch evaluation\nTarget ClassOutput ClassjDSjDSjDS89.17%10.83%jDS8.74%91.26%90.22%(b) Onset evaluation\nTarget ClassOutput ClassjDEjDEjDE84.71%15.29%jDE2.54%97.46%91.08%(c) Offset evaluationTable 1: Evaluation of the proposed approach using 10-Folds cross validation without the Bayesian rejection rule.Target ClassOutput ClassjDfjDfjDf94.45%5.55%jDf2.54%97.46%95.96%(a) Pitch evaluation:TDf=0.33Target ClassOutput ClassjDSjDSjDS94.17%5.83%jDS7.34%92.66%93.42%(b) Onset evaluation:TDS=0.31Target ClassOutput ClassjDEjDEjDE91.64%8.36%jDE2.54%97.46%94.55%(c) Offset evaluation:TDE=0.39Table 2: Evaluation of the proposed approach using10-Folds cross validation with the Bayesian rejection rule. The systemcan answer in 90% of the times, increasing the ﬁnal accuracy in almost 4%.\n0.30.350.40.450.50.50.60.70.80.9\nThreshold TPercentageaccuracy∆saccepted∆saccuracy∆eaccepted∆eaccuracy∆faccepted∆fFigure 3: Comparative of the accuracy versus the numberof non-rejected samples. Solid lines show the accuracyevolution, which are affected by the thresholdsTDf(pitch),TDs(onset), andTDe(offset).preferred to not have an answer than to provide an incorrectfeedback. Based on this assumption, and also consideringthat the percentage of samples with doubt from the expertevaluation is over10%, we decided to set all thresholds toreject 15% of the samples in average.Table 2 shows the accuracy evaluation for the 10-foldexperiment using the Bayesian classiﬁer with the rejectionrule, in which the rejection thresholdsTDf(pitch),TDS(on-set) andTDE(offset) were set so that 15% of the samples arerejected, matching approximately the percentage of sampleswith dubious labels. As it can be observed, the overall accu-racies for all analyzed parameters increased in 3-5% whencompared to the option without rejection, reaching up toalmost 96% accuracy. Also, the number of false negativeswas greatly reduced, particularly for the offset evaluation.This fact indicates that when in doubt, the evaluators tendto label a note as correct rather than incorrect. Furthermore,32–35%of the rejected samples received3agreeing votesby the experts, which means that our system is removingmore than twice of the samples related to the experts’ doubtwhen compared with the whole dataset.6. CONCLUSIONThis paper presented a note-by-note approach for automaticsolfège assessment focused on musical education, in whicheach sung note is evaluated considering the human evalu-ation perception in small scale, focused on the parametersof pitch, onset and offset at a speciﬁc part of the solfegepractice. The proposed system uses melodic transcriptiontechniques to extract the sung notes from the audio sig-nal, and the sequence of melodic segments is subsequentlyprocessed by a two stage algorithm. In the ﬁrst stage, anaggregation process was introduced to perform the tem-poral alignment between the transcribed melody and themusic score (ground truth). This stage implicitly aggre-gates and links the best combination of the extracted me-lodic segments with the expected notes in the ground truth.The proposed alignment process does not impose the DTWboundary condition between the two sequences, avoidingthe propagation of the accumulated matching error. In thesecond stage, a Bayesian classiﬁer is used to evaluate theaccuracy of each detected sung note. This statistical modelwas trained using a combination of the extracted measures(Df,Ds, andDe) with the individual scores provided by acommittee of expert listeners.Experimental results indicate that the classiﬁcationscheme achieved accuracy rates in the range90–91%with-out using the rejection rule (i.e., feedback for all evaluatednotes), and93–96%using the Bayesian rejection procedure(for the chosen thresholds, our tool is able to give feedbackin85%of the trials in average). Besides the classiﬁca-tion label (correct, incorrect or undeﬁned), the system alsoprovides probability measure, which helps to indicate howlikely correct or incorrect was the performance of the sungnote. As future work, new research is planned to integratenew audio features, as well as the usage of lyrics analysis,to improve the segmentation and alignment on the ﬁrst stageof this approach.Acknowledgements:Thanks to CAPES Foundation, Min-istry of Education of Brazil, scholarship BEX-2106/13-2.188 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]Alain de Cheveigné and Hideki Kawahara. YIN, afundamental frequency estimator for speech and mu-sic.The Journal of the Acoustical Society of America,111(4):1917–1930, 2002.[2]Richard O. Duda, Peter E. Hart, and David G. Stork.Pattern Classiﬁcation (2nd Edition). Wiley-Interscience,pages 24–27, 2001.[3]Emilia Gómez and J Bonada. Towards computer-assisted ﬂamenco transcription: An experimental com-parison of automatic transcription algorithms as appliedto a cappella singing.Computer Music Journal, 37:73–90, 2013.[4]Mayank Vibhuti Jha and Preeti Rao. Assessing vowelquality for singing evaluation. InProceedings of theNational Conference on Communications (NCC), pages1–5, Kharagpur, India, Feb 2012.[5]Anssi Klapuri and Manuel Davy.Signal ProcessingMethods for Music Transcription. Springer-Verlag NewYork, Inc., Secaucus, NJ, USA, pages 361–390, 2006.[6]Chang-Hung Lin, Yuan-Shan Lee, Ming-Yen Chen, andJia-Ching Wang. Automatic singing evaluating systembased on acoustic features and rhythm. InProceedingsof the IEEE International Conference on Orange Tech-nologies (ICOT), 2014, pages 165–168, Sept 2014.[7]Matthias Mauch and Simon Dixon. pyin: A fundamen-tal frequency estimator using probabilistic thresholddistributions. InProceedings of the IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP 2014), pages 659–663, May 2014.[8]Sai Sumanth Miryala, Ranjita Bhagwan, MonojitChoudhury, and Kalika Bali. Automatically identifyingvocal expressions for music transcription. InProceed-ings of the 14th International Society of Music Informa-tion Retrieval, ISMIR 2013, Curitiba, Brazil, November4-8, pages 239–244, 2013.[9]Emilio Molina, Ana M. Barbancho, Lorenzo J. Tardón,and Isabel Barbancho. Evaluation framework for auto-matic singing transcription. InProceedings of the 15thInternational Society for Music Information RetrievalConference, ISMIR 2014, Taipei, Taiwan, October 27-31, pages 567–572, 2014.[10]Emilio Molina, Isabel Barbancho, Emilia Gómez,Ana Maria Barbancho, and Lorenzo J. Tardón. Fun-damental frequency alignment vs. note-based melodicsimilarity for singing voice assessment. InProceed-ings of the IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP , 2013), pages744–748, May 2013.[11]Emilio Molina, Lorenzo J. Tardón, Ana M. Barban-cho, and Isabel Barbancho. Sipth: Singing transcriptionbased on hysteresis deﬁned on the pitch-time curve.IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, 23(2):252–263, Feb 2015.[12]Meinard Müller.Information Retrieval for Music andMotion. Springer Verlag, pages 69–74, 2007.[13]Dorothy Payne. Essential skills, part 1 of 4: Essentialskills for promoting a lifelong love of music and mu-sic making.American Music Teacher, February–March2005.[14]Graham E. Poliner, Daniel P. W. Ellis, A. F. Ehmann,Emilia Gómez, S. Streich, and Beesuan Ong. Melodytranscription from music audio: Approaches and eval-uation.IEEE Transactions on Audio, Speech and Lan-guage Processing, 15(4):1247–1256, 2007.[15]Kandethody M. Ramachandran and Cris P. Tsokos.Mathematical Statistics with Applications. Elsevier Aca-demic Press, 2009.[16]Matti Ryynänen. Probabilistic modelling of note eventsin the transcription of monophonic melodies. Master’sthesis, Tampere University of Technology, March 2004.[17]Matti Ryynänen and Anssi Klapuri. Modelling of noteevents for singing transcription. InProc. ISCA Tutorialand Research Workshop on Statistical and PerceptualAudio, Jeju, Korea, October 2004.[18]Rodrigo Schramm, Cláudio Rosito Jung, and EduardoReck Miranda. Dynamic time warping for music con-ducting gestures evaluation.Multimedia, IEEE Transac-tions on, 17(2):243–255, Feb 2015.[19]Timo Viitaniemi, Anssi Klapuri, and Antti Eronen. Aprobabilistic model for the transcription of single-voicemelodies. InTampere University of Technology, pages59–63, 2003.[20]Joel Wapnick and Elizabeth Ekholm. Expert consensusin solo voice performance evaluation.Journal of Voice,11(4):429 – 436, 1997.[21]Andrew R. Webb.Statistical Pattern Recognition. Wiley,Chichester,UK, 3 edition, pages 8–17, 2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 189"
    },
    {
        "title": "Improving Genre Annotations for the Million Song Dataset.",
        "author": [
            "Hendrik Schreiber 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414760",
        "url": "https://doi.org/10.5281/zenodo.1414760",
        "ee": "https://zenodo.org/records/1414760/files/Schreiber15.pdf",
        "abstract": "Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this pur- pose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multi- ple attempts have been made to add song-level genre anno- tations, which are required for supervised machine learn- ing tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating ad- ditional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top- level genres. These are most often used in MGR systems. We then combine multiple datasets using majority vot- ing. This both promises a more reliable ground truth and allows the evaluation of the newly generated and pre- existing datasets. To facilitate further research, all derived genre annotations are publicly available on our website.",
        "zenodo_id": 1414760,
        "dblp_key": "conf/ismir/Schreiber15",
        "keywords": [
            "Automatic music genre recognition",
            "Ground truth dataset",
            "Public dataset",
            "Mislabeling",
            "Duplications",
            "Million Song Dataset",
            "Feature and metadata",
            "Crowd-sourced genre labels",
            "Taxonomies",
            "Majority voting"
        ],
        "content": "IMPROVING GENRE ANNOTATIONS FOR THE MILLION SONGDATASETHendrik Schreibertagtraum industries incorporatedhs@tagtraum.comABSTRACTAny automatic music genre recognition (MGR) systemmust show its value in tests against a ground truth dataset.Recently, the public dataset most often used for this pur-pose has been proven problematic, because of mislabeling,duplications, and its relatively small size. Another dataset,the Million Song Dataset (MSD), a collection of featuresand metadata for one million tracks, unfortunately does notcontain readily accessible genre labels. Therefore, multi-ple attempts have been made to add song-level genre anno-tations, which are required for supervised machine learn-ing tasks. Thus far, the quality of these annotations has notbeen evaluated.In this paper we present a method for creating ad-ditional genre annotations for the MSD from databases,which contain multiple, crowd-sourced genre labels persong (Last.fm, beaTunes). Based on label co-occurrencerates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems.We then combine multiple datasets using majority vot-ing. This both promises a more reliable ground truthand allows the evaluation of the newly generated and pre-existing datasets. To facilitate further research, all derivedgenre annotations are publicly available on our website.1. INTRODUCTIONAutomatic music genre recognition (MGR) is among themost popular Music Information Retrieval (MIR) tasks [5].Until 2012, the majority of datasets used for MGR re-search was private and the most popular public dataset wasGTZAN [13, 14]. Unfortunately, GTZAN has some doc-umented deﬁciencies [12]. Additionally, with 1,000 ex-cerpts from ten different genres, GTZAN is relatively smallby today’s standards. Desirable as dataset for MGR, interms of size and available features, is the Million SongDataset (MSD) [2]. But by 2012, when it was still verynew, only three of the 345 publications (0.7%) surveyedin [13] had used it. This may be explained by the factthat the MSD does not contain explicit genre annotations.The authors of all three publications ﬁrst had to derivec\u0000Hendrik Schreiber.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Hendrik Schreiber. “Improving genreannotations for the million song dataset”, 16th International Society forMusic Information Retrieval Conference, 2015.song-level genre labels for a subset of the MSD as groundtruth. For this purpose, Hu [7] and Schindler [10] bothused album-level genre labels scraped from the All Mu-sic Guide website1. Dieleman et al. [3] selected 20 com-monly used genres from the MusicBrainz artist tags con-tained in the MSD—an approach similar to what the MSDauthor suggested for the MSD Genre Dataset, a “simpliﬁedgenre dataset from the Million Song Dataset for teachingpurposes”2. With the exception of [10], the used groundtruths aren’t re-usable or well documented. And in the caseof [10], they have not been evaluated and don’t allow formultiple genre annotations per song.In the spirit of [9], what is required to help facilitateMGR research using the MSD, is a song-level ground truthwith a documented level of accuracy that also allows forambiguity. In the following sections we will ﬁrst derive(where necessary) and then compare four different genredatasets for the MSD. In Section 2 we describe how wecreated the beaTunes Genre Dataset (BDG). In Section 3,we apply a similar approach to the Last.fm Dataset3, cre-ating a Last.fm Genre Dataset (LFMGD). In Section 4we explore to which degree the BDG, LFMGD and thedatasets created by Hu (HO)4and Schindler (Top-MAGD)agree, and derive two new datasets, by combining multi-ple sources. Finally, in Section 5 and Section 6, we deﬁnebenchmark partitions to promote repeatability of experi-ments using the new datasets and point to additional rawdata.2. BEATUNES GENRE DATASETbeaTunes5is a consumer application that encourages itsusers to correct song metadata using multiple heuristics.It also supports sending anonymized metadata to a cen-tral database, which matches it to metadata sent by otherusers. Much like tags on Last.fm, this allows keeping trackof multiple user-submitted genres per song. For example,one song may have been associated with the labelRockby ﬁve users, while three users regarded the same song aPopsong. The database currently contains more than 8701http://allmusic.com/2http://labrosa.ee.columbia.edu/millionsong/blog/11-2-28-deriving-genre-dataset3http://labrosa.ee.columbia.edu/millionsong/lastfm4http://web.cs.miami.edu/home/yajiehu/resource/genre/5http://www.beatunes.com/241million user song submissions of which 772 million are la-beled with a genre and mapped to more than 85 millionsongs. Furthermore, the database stores each user’s sys-tem language. In the remainder of this section we describe,how we used the existing genre labels to assign top-levelgenres (seeds) to each song and matched them to songs inthe MSD.2.1 Genre Label NormalizationIn the beaTunes database, more than one million dif-ferent, user-submitted genre labels are stored. Someof these are slight spelling variations of popular genrenames likeHip-Hopor composites of multiple genreslikeHip-Hop/Rap. Others describe custom categoriza-tion schemes, ratings, or are simply noise. In order to ex-tract the most-used and thus most important genre labelsfrom the database, we ﬁrst normalized their names andthen ranked them by usage count. The following normal-ization procedure was employed (building on [6]):1.Convert to lowercase2.Remove whitespace3.Convert’n’,and, and&in different spellings ofR&B,D&B, andRock’n’Rollton4.Replacealt.withalternativeandtrad.withtraditional5.Tokenize with+&/,;:!\\[]()as delimiters6.From each token, remove all characters that aren’tletters or digits7.Sort tokens alphanumerically8.Concatenate tokens with/as delimiterThis effectively treats composite labels likeHip-Hop/Rapas their own genre, but makes sure thatHip-Hop/Rapis equal toRap/Hip-Hop. The specialtreatment in step 3 forR&B,D&B, andRock’n’Rollis necessary, as the&character is also used as delimiterin composite labels (e.g.Christian & Gospel).After normalization, almost 700,000 different genre labelsremain. However, 50% of all user-submitted songs arecovered by the 16 most-used genres, 80% by the top 131genres, and 90% by the top 750 genres.2.2 Language-Speciﬁc CountsSince genre labels reﬂect how listeners with a speciﬁc cul-tural background perceive music and what it means tothem [1, 4, 8], we investigated how the collection’s topgenre rankings differ when taking the user’s system lan-guage into account. Not surprisingly, by and large theyare quite similar—withRock,Pop,Hip-HopandJazzoccurring in most top tens (Table 1). But there are a fewnotable exceptions. English speaking listeners are the onlyones withCountry(ranked 9th) in their top ten genres,French speakers rankReggae(5th) higher than others,Spanish speakers rankLatin(5th),House(7th), andOtros(8th) high, and Japanese speakers rankJ-Pop(3rd) near the top. Clearly, these differences are indica-tive of cultural preferences and should be taken into ac-count when creating genre taxonomies. Therefore, in theremainder of this paper, we have only used the beaTuneslabel-submissions of English-speaking users.2.3 Inferring Genre TaxonomiesAs the beaTunes database contains on average about nineuser submissions (i.e. genre labels) per song, we can recordco-occurrences of labels on a per-song basis and thus in-fer relationships between them. Latent Semantic Analysis(LSA) with cosine similarity has been used for this pur-pose before [11]. But because we did not plan on usingthe cosine distance as metric, we did not deem it necessaryto use Singular Value Decomposition (SVD) to keep thedimensionality low. Instead, we opted for a much simplermethod. We ﬁltered out rarely used labels and restrictedourselves to the top 1,000 genres covering over 93% of alluser submissions with genre information.Formally, we deﬁneG:={Rock,Pop,. . .}with|G|=1000 =nas the set of thentop genres, which arestored as distinct values in the vectorg2Gnwithg:=(Rock,Pop,. . .). Each user submission is deﬁned as asparse vectoru2Nnwithui=⇢1,ifgi=user-submitted genre0,otherwise.(1)To establish the connection between a songsand itsuser labelsu, we simply add up allu’s belonging to thesong and divide by the number ofu’s. Thus each song isrepresented by a vectors2Rnwith0si1andPn\u00001i=0si=1, denoting each genre’s relative strength. Tocompute the co-occurrences for a given genregiwith allother genresg, we element-wise average allsfor whichsi6=0is true. I.e.:Cgi:= ¯s,for allswithsi6= 0;C2Rn⇥n(2)The result is the matrixCthat allows us to see howoften a given genre co-occurs with another genre. NotethatCis not symmetric as it would have been, hadwe used SVD with cosine similarity. So just becauseAlternativeco-occurs withRockfairly strongly(CAlternative,Rock=0.156), the opposite is not necessar-ily true (CRock,Alternative=0.026, see Table 2). FromtheCvalues for the beaTunes database, it is also obvious,thatRockandPopcan be distinguished very well—bothlabels co-occur much more with themselves than with theother (Rock:0.609/0.057,Pop:0.593/0.077).We exploit the asymmetry ofCto construct a taxonomyby deﬁning the following two rules:(1) If a genreaco-occurs with another genrebmorethan a minimum threshold⌧, andaco-occurs withbmorethan the other way around, then we assume thatais a sub-genre ofb. More formally:ais a sub-genre ofb,iffa6=b^Ca,b>⌧^Ca,b>Cb,afor alla, b2G(3)242 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015AllEnglish German French Spanish Japanese(N=7 7 2.1)(N=5 2 1.1)(N=9 7.9)(N=4 3.3)(N=2 7.1)(N=1 1.0)1.RockRock Pop Rock Rock Rock2.PopPop Rock Pop Pop Pop3.AlternativeAlternative Electronic Jazz JazzJ-Pop4.JazzHip-Hop/Rap Hip-Hop Hip-Hop Soundtrack R&B5.Hip-HopHip-Hop JazzReggae LatinSoundtrack6.Hip-Hop/RapR&B Alternative R&B Dance Jazz7.SoundtrackSoundtrack Dance SoundtrackHouseElectronica/Dance8.R&BJazz R&B BluesOtrosM#✏(Rock)9.ElectronicCountryRock/Pop Electronic Blues Altern. & Punk10.CountryAltern. & Punk Soundtrack Rap Electronica Hip-Hop/RapTable 1. Top ten genres used by beaTunes users with different languages.Ndenotes the number of submissions in millions.Co-Occurrence Rank1. 2. 3. 4.RockRock (0.609) Pop (0.057) Alternative (0.026) Rock/Pop (0.016)PopPop (0.593) Rock (0.077) Rock/Pop (0.014) R&B (0.013)AlternativeAlternative (0.394) Rock (0.156) Pop (0.052) Alternative/Punk (0.036)R&BR&B (0.566) Pop (0.061) Soul (0.036) R&B/Soul (0.033)SoundtrackSoundtrack (0.754) Rock (0.024) Pop (0.022) Game (0.011)...... ... ... ...Table 2. Genre labels in the beaTunes database and their top four co-occurring labels ordered by relative strength given inparenthesis. The underlying values from the co-occurrence matrixCwere computed taking only submissions by Englishspeakers and the 1,000 most-used labels into account.(2) Because this rule allows a genre to be a sub-genre ofmultiple genres, we add:ais adirectsub-genre ofb,iffais a sub-genre ofb^Ca,b>Ca,cwithc6=a^c6=b;a, b, c2G(4)By ﬁnding all direct sub-genres and their parents, wecan now create a set of trees. The number of created treesdepends on the threshold⌧. We found, that to properly dis-tinguish between genres likePop,Rock,Dance,R&B,Folk, andOther,⌧:= 0.085proved to be useful, re-sulting in 141 trees. The roots of these trees are typi-cally the names of seed-genres likeJazz,Pop,Rock,etc. (see Figure 1).Not all generated trees have children. For example, thetree with the seed-genreGrooveconsists of just the root.AlthoughGrooveco-occurs withR&B,Rock,Funk, andSoul, the co-occurrence rates with genres other than itselfare all below⌧. Even the co-occurrence with itself is low(0.157). This suggests, thatGrooveis not really a genre,but more a property of a genre. Another example for a root-only tree isCalypso. Here the co-occurrence with itselfis much higher (0.606) and indeedCalypsoqualiﬁes asstand-alone genre that simply does not have any sub-genresin this database.Naturally, the generated taxonomies are only simpliﬁedmappings of the more complex relationship graph repre-sented byC. In reality, genres aren’t necessarily exclusivemembers of one tree or another (e.g. fusion genres). Anontology is the much better construct. But, as we will see,for the purpose of mapping most sub-genres to their seed-genre, trees are useful.RockMetalAlternativePunk...PopFolk PopAcoustic PopTop 40...Hip-HopEast CoastRapTurntablism...RnBMotownFunkSoulUrban...Figure 1. Partial, generated trees for the seed-genresRock,Pop,Hip-Hop, andR&B.2.4 Matching with Million Song DatasetTo create song-level genre annotations for the MSD, wequeried the beaTunes database for songs with artist/titlepairs contained in the MSD and were able to match677,038 songs. In order to ease the comparison with theHO and Top-MAGD datasets, we associated each matchedsong with the seed-genre of its most often occurringgenre label, taking advantage of the taxonomies created inSection 2.3.Motown, for example, is represented by itsseed-genreRnB. In many cases, the found seed-genres areProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 243Co-Occurrence Rank1. 2. 3. 4.rockrock (0.128) alternative (0.023) pop (0.021) indie (0.021)poppop (0.107) rock (0.037) femalevocalists (0.024) 80s (0.018)alternativealternative (0.076) rock (0.062) indie (0.037) alternativerock (0.023)indieindie (0.108) rock (0.045) alternative (0.034) indierock (0.026)electronicelectronic (0.119) dance (0.026) trance (0.021) electronica (0.019)...... ... ... ...Table 3. Tags in the Last.fm dataset and their top four co-occurring labels ordered by relative strength given in parenthesis,based on the co-occurrence matrixC, computed taking the 1,000 most-used labels into account.equal to the All Music Guide labels used by Top-MAGD(Blues,Country,Electronic,International,Jazz,Latin,PopRock,Rap,Reggae,RnB,NewAge,Folk,Vocal). With a few exceptions: In our gen-erated taxonomy for English users,BluesandVocalare not seed-genres, but rather sub-genres ofRockandJazz, respectively. Therefore, in these cases we usedthe label itself instead. We also translatedWorldtoInternational, andPop,Rock, andPop/RocktoPopRock, andHip-HoptoRap. All songs we could notmap to a Top-MAGD label were dropped, leaving us with609,865 songs—90% of the originally matched songs. Wecall this dataset the beaTunes Genre Dataset (BGD).3. LAST.FM GENRE DATASETThe Last.fm dataset is similar to the beaTunes database,in that it also contains multiple user-submitted labels persong which are each associated with a weight. Thereforewe can use the same method to build a co-occurrence ma-trix and construct genre trees. The main difference lies inthe kind of labels used. While the beaTunes labels are al-most exclusively genre names, Last.fm tags vary a lot incontent. Many are also genre labels, but others describe amood, situation, location, time, or something completelydifferent. As the dataset contains 522,366 different tags,it is not feasible to manually extract only the genre relatedones. Therefore we again chose to incorporate the 1,000most-used tags into computed genre trees. Because a sin-gle Last.fm song is often associated with many more tagsthan a beaTunes song with genre labels, we had to choosea different⌧. Just like for BGD, we wanted to be able tosee genres likeElectronic,Jazz,PopandRockasseed-genres and therefore chose⌧:= 0.040, which allowsfor this (see Table 3 for sample co-occurence values).To create the Last.fm Genre Dataset (LFMGD), we as-sociated each song with the seed-genre of the strongesttag that has a seed-genre corresponding to a Top-MAGDlabel or already corresponds to one of the Top-MAGDlabels itself. In either case, we adjusted the spellingsuitably. We also translatedhiphoptoRap, andpop,rock,poprocktoPopRock, andworldtoInternational. Again, all songs not easily mappableto a Top-MAGD label were removed from the set. This leftus with 340,323 (67.4%) of the 505,216 tracks originallylabeled with at least one tag.Top-MAGD LFMGD BGDHO56.6%52.7%54.9%Top-MAGD- 75.8%84.1%LFMGD- - 81.0%Table 5. Pairwise agreement rates for all four datasets for136,639 MSD tracks occurring in all sets. The highestagreement is set inbold, the lowest initalic.DatasetTop-MAGD LFMGD BGDAgreement Rate90.4% 87.2%95.8%Table 6. Agreement rates for genre labels in Top-MAGD,LFMGD, and BGD when compared with the 133,676tracks in CD1, found by majority voting.4. CONSTRUCTING GROUND TRUTHTo construct a reliable ground truth, we evaluated agree-ment rates between the existing and constructed datasetsusing the genre labels from Top-MAGD. We then com-bined the more promising sets (Section 4.1). BecauseTop-MAGD labels as the lowest common denominator aresomewhat unsatisfying, we then used just LFMGD andBGD to construct an additional dataset with ﬁner genregranularity (Section 4.2).4.1 Truth by MajorityAfter removal of duplicates6, we found 136,639 tracks oc-curring in all four datasets Top-MAGD, LFMGD, BGD,and HO, all labeled with Top-MAGD genres. As a rela-tive measure of trustworthiness, we calculated their pair-wise agreement rate (Table 5). While the rates betweenTop-MAGD, LFMGD, and BGD are above 75%, those in-volving HO are below 57%. Unlike the other sets, HO wascreated with a combined classiﬁer and is not the result ofcrowd-sourcing or any kind of expert annotation. There-fore a lower agreement rate was to be expected. The al-most 20 percentage points difference illustrates that HO isnot suitable as ground truth.Since the other datasets were in relatively high agree-ment and we did not have a strong reason to believe, that6http://labrosa.ee.columbia.edu/millionsong/blog/11-3-15-921810-song-dataset-duplicates244 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Top-MAGDBluesCountryElectronicFolkInternationalJazzLatinNew AgePopRockRapReggaeRnBVocalBlues78.1%0.1%0.0%0.4%0.1%1.5%0.0%0.0%17.9%0.0%0.0%1.7%0.0%Country0.0%86.1%0.0%2.3%0.1%0.2%0.0%0.0%11.4%0.0%0.0%0.0%0.0%Electronic0.0%0.0%82.4%0.0%0.4%0.2%0.1%0.1%15.7%0.9%0.0%0.2%0.0%Folk0.0%0.8%0.1%49.2%14.9%0.0%0.2%0.1%34.3%0.0%0.0%0.0%0.3%International0.1%0.0%7.8%0.3%83.6%0.7%0.8%1.2%4.5%0.0%0.0%0.4%0.7%Jazz0.1%0.0%2.2%0.0%0.5%76.2%1.2%0.8%6.5%0.2%0.0%1.5%10.8%Latin0.0%0.0%0.3%0.0%0.7%0.3%95.6%0.0%2.5%0.2%0.0%0.0%0.3%New Age0.0%0.0%2.7%0.0%1.4%0.9%0.0%93.5%1.5%0.0%0.0%0.0%0.0%PopRock0.0%0.1%1.0%0.2%0.6%0.1%0.9%0.0%96.0%0.1%0.0%0.8%0.2%Rap0.0%0.0%3.0%0.0%0.7%0.0%0.2%0.0%4.5%91.0%0.0%0.4%0.0%Reggae0.0%0.0%2.2%0.0%1.7%0.1%1.0%0.0%21.5%1.2%72.2%0.1%0.0%RnB0.0%0.0%0.5%0.0%0.0%0.0%0.0%0.0%3.3%0.4%0.0%95.8%0.0%Vocal0.0%0.0%0.0%0.0%0.0%0.0%2.9%0.0%1.3%0.0%0.0%0.0%95.8%BGDBluesCountryElectronicFolkInternationalJazzLatinNew AgePopRockRapReggaeRnBVocalBlues97.6%0.0%0.0%0.2%0.0%0.1%0.0%0.0%1.4%0.0%0.0%0.6%0.0%Country0.1%97.8%0.0%0.4%0.0%0.0%0.0%0.0%1.6%0.0%0.0%0.1%0.0%Electronic0.2%0.0%91.2%0.0%0.4%0.3%0.0%0.6%6.5%0.6%0.1%0.2%0.0%Folk0.4%1.8%0.0%93.9%0.2%0.0%0.0%0.1%3.6%0.0%0.0%0.0%0.0%International0.0%0.2%0.7%0.9%93.8%0.5%0.7%0.5%2.2%0.0%0.5%0.0%0.0%Jazz0.1%0.0%0.4%0.0%0.1%97.5%0.2%0.1%1.1%0.1%0.0%0.4%0.0%Latin0.1%0.0%0.5%0.3%1.6%0.7%91.3%0.0%4.9%0.2%0.3%0.1%0.0%New Age0.1%0.0%0.6%0.1%0.9%0.4%0.0%97.4%0.6%0.0%0.0%0.0%0.0%PopRock0.3%0.3%1.3%0.6%0.1%0.1%0.2%0.0%96.4%0.1%0.1%0.3%0.0%Rap0.1%0.0%0.9%0.0%0.0%0.1%0.0%0.0%1.4%96.5%0.2%0.8%0.0%Reggae0.2%0.0%0.3%0.0%0.2%0.0%0.0%0.0%0.4%0.5%98.3%0.1%0.0%RnB0.1%0.0%0.2%0.0%0.1%0.2%0.0%0.0%3.8%0.6%0.0%94.9%0.0%Vocal1.3%0.0%0.0%0.0%0.4%16.3%0.4%0.0%16.3%0.0%0.0%0.4%64.9%LFMGDBluesCountryElectronicFolkInternationalJazzLatinNew AgePopRockRapReggaeRnBVocalBlues92.3%0.4%0.2%0.2%0.0%1.2%0.0%0.0%5.3%0.1%0.2%0.0%0.0%Country0.2%91.8%0.0%1.2%0.0%0.2%0.0%0.0%6.4%0.1%0.1%0.0%0.0%Electronic0.0%0.0%85.0%0.1%0.2%2.7%0.1%0.2%9.8%1.1%0.7%0.0%0.1%Folk1.3%3.7%0.0%87.6%0.1%0.6%0.0%0.0%6.3%0.0%0.1%0.0%0.2%International0.1%0.2%2.4%17.2%64.7%3.0%1.4%1.2%7.9%0.3%1.4%0.0%0.3%Jazz0.4%0.1%0.5%0.0%0.3%95.1%0.1%0.1%2.8%0.1%0.1%0.0%0.3%Latin0.2%0.2%1.6%1.2%2.4%3.8%59.0%0.1%29.6%0.6%0.9%0.0%0.3%New Age0.1%0.1%12.1%2.9%1.4%17.6%0.9%54.8%9.8%0.1%0.0%0.0%0.1%PopRock1.2%1.1%3.4%2.8%0.1%1.0%0.1%0.1%88.9%0.4%0.7%0.1%0.2%Rap0.0%0.1%1.4%0.0%0.0%1.2%0.3%0.0%4.0%92.2%0.3%0.4%0.0%Reggae0.0%0.0%0.2%0.0%0.1%0.2%0.1%0.0%2.2%0.3%96.6%0.2%0.0%RnB3.2%0.2%0.5%0.1%0.0%9.1%0.1%0.0%20.5%3.9%0.4%61.2%0.7%Vocal0.4%1.7%0.4%0.8%2.1%16.7%1.3%1.3%25.9%2.1%0.0%0.0%47.3%Table 4. Confusion matrices between CD1 and Top-MAGD, BGD, and LFMGD. Values greater 10% are set inbold.one of them is better than the other, we constructed a Com-bined Dataset 1 (CD1) from them using unweighted major-ity voting. CD1 contains only those tracks, that are labeledexclusively with the Top-MAGD genre set and for whichthe majority of labels from Top-MAGD, LFMGD, andBGD are identical. MSD duplicates were removed. Outof 136,991 tracks we found a majority genre for 133,676(97.6% of all), of which 98,149 were found by unanimousconsent (73.4% of majorities). To document ambiguity,we recorded both the majority decision and the minorityvote, if there was one. This may be used in the evaluationof MGR systems, e.g. for fractional scores, or as indicatorfor uncertainty. The majority genre distribution of CD1 isshown in Figure 2.RockPopis with 59.8% by far themost dominant genre,Vocalwith 0.2% the most under-represented one.When comparing Top-MAGD, LFMGD, and BGD tothe majority labels from CD1, we found that BGD matchesbest with 95.8%, followed by Top-MAGD with 90.4%,and LFMGD with 87.2% (Table 6). We believe that therelatively low agreement rate for LFMGD indicates roomfor improvement in the used mapping procedure fromtags to genres, rather then problems with the originalLast.fm dataset. Even though Top-MAGD was derivedfrom album-level genre labels, it agrees with CD1 remark-ably well, which attests to the quality of the set. BGDmight be seen as the best of both worlds: its data sourceis song-level like LFMGD and at the same time somewhatlimited to a genre vocabulary—more like Top-MAGD thanLFMGD. This means the problematic mapping from free-form tags to genres is much easier. Overall, one might in-terpret these numbers as estimates for an upper boundaryof MGR systems that test against a ground truth with onlyone genre label per song.To provide more detail regarding the individual weak-nesses of the datasets relative to CD1, we created confu-sion matrices (Table 4). In Top-MAGD the largest mis-classiﬁcations occur forFolk(34.3%),Reggae(21.5%),Blues(17.9%),Electronic(15.7%), andCountry(11.4%), which are all categorized asPopRock. BGDclassiﬁesVocalrelatively poorly: 16.3% are misclassi-ﬁed asJazzand 16.3% asPopRock. LFMGD tendsto misclassifyLatin,RnB, andVocalasPopRock(29.6%, 20.5%, 25.9%), andVocalasJazz(16.7%). Insummary, most errors occur with songs falsely identiﬁedasRockPop. Additionally,Vocaltends to be misclas-siﬁed asJazz. We suspect this happens mainly, becauseVocalis not seen as a genre, but rather as a style.4.2 Truth by ConsensusSimilar to Top-MAGD, almost 60% of all songs in CD1 arelabeledPopRock, Obviously, this rather coarse labelingis unsatisfying. Therefore we decided to create anotherProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 2450204060BluesCountryElectronicFolkIntern.JazzLatinNew AgePop RockRapReggaeRnBVocal2.23.911.42.21.15.82.1159.84.62.72.90.2Tracks per Genre[%]Figure 2. Majority genre distribution of tracks in CD1.\n0204060BluesCountryElectronicFolkJazzLatinMetalNew AgePopPunkRapReggaeRnBRockWorld3.24.711.42.27.71.64.80.66.81.75.74.25.139.21Tracks per Genre[%]Figure 3. Genre distribution of tracks in CD2C.dataset, Combined Dataset 2 (CD2), which differentiatesbetween these two genres and adds two additional ones thatare popular among users of beaTunes and Last.fm (MetalandPunk). BecauseInternationalis hardly usedin user-submitted tags and thus seems artiﬁcial, we usedWorldinstead. We also translatedSoultoR&Bin orderto group them together, and removedVocal, because it isthe genre BGD and LFMGD confused most in CD1.As sources for CD2 we used suitably modiﬁed versionsof LFMGD and BGD and found 280,831 songs that bothﬁt our genre-set, occur in both datasets, and aren’t dupli-cates. 191,401 (68.2%) of the songs in CD2 have only onegenre label, found by consensus. For convenience, we cre-ated another dataset called Combined Dataset 2 Consensus(CD2C) containing just those songs. As shown in Figure 3,the genre distribution for CD2C is a little more even thanCD1—Rockbeing represented with a 39.2% share,Popwith 6.8%, andNew Agewith 0.6%.5. BENCHMARK PARTITIONSInspired by [10] we provide three kinds of benchmark par-titions for CD1, CD2, and CD2C in order to promote re-peatability of experiments beyond x-fold cross validation.These partitions are:•“Traditional” splits into training and test sets, withsizes 90%, 80%, 66%, and 50%; no stratiﬁcation.•Splits into training and test sets, with sizes 90%,80%, 66%, and 50% and genre stratiﬁcation.•Splits with a ﬁxed number of training samples pergenre (1,000/2,000/3,000). Genres with fewer songsthan the training size were dropped.As CD2 songs are not always labeled with a majoritygenre, we used the ﬁrst listed genre for stratiﬁcation.6. ADDITIONAL DATABGD and LFMGD represent simpliﬁed views on reality,suitable for comparisons with other, similar datasets likeTop-MAGD. They both assign only one genre per song andthe genre labels themselves are very limited. Both simpli-ﬁcations are problematic [9], which is why the combineddatasets presented in this paper contain multiple genre la-bels where feasible. But for both BGD and LFMGD thereis actually much more information available on a per-songbasis. We are publishing it on our website in the hope thatit proves useful for further research. Speciﬁcally, this in-cludes:•Multiple genre annotations/tags per song along withrelative strength, and number of user-submissions tojudge reliability.•Co-occurrence matrices computed as describedin Section 2.3.•Derived genre taxonomies.All data can be found athttp://www.tagtraum.com/msd_genre_datasets.html.7. CONCLUSION AND FUTURE WORKReliable and accessible annotations for large datasets arean important precondition for the development of success-ful music genre recognition (MGR) systems. Some often-used reference datasets are either relatively small or sufferfrom other deﬁciencies. To promote the adoption of theMillion Song Dataset (MSD) for MGR research, we bothevaluated existing and created two new genre annotationdatasets for subsets of the MSD. Given that the large sizesof the datasets render manual validation almost impossi-ble, we used either majority voting or consensus to vali-date existing data, and allowed for ambiguity in the cre-ated ground truths. In direct comparison with the generatedground truth CD1, 90.4% of the compared Top-MAGD la-bels were in agreement. To further promote experimen-tation and comparability, we also provided traditional andstratiﬁed benchmark partitions, as well as most of the datathe combined datasets were derived from. In the processof creating the new datasets, we used simpliﬁcations likeEnglish-only labels and trees instead of graphs. Futurework is needed to overcome these simpliﬁcations and bet-ter model the real world.We hope the provided datasets prove useful for futurepublications in order to create better MGR systems.246 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Jean-Julien Aucouturier and Francois Pachet. Repre-senting musical genre: A state of the art.Journal ofNew Music Research, 32(1):83–93, 2003.[2]Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-man, and Paul Lamere. The million song dataset. InProceedings of the 12th International Conference onMusic Information Retrieval (ISMIR), pages 591–596,2011.[3]Sander Dieleman, Phil´emon Brakel, and BenjaminSchrauwen. Audio-based music classiﬁcation with apretrained convolutional network. InProceedings ofthe 12th International Conference on Music Informa-tion Retrieval (ISMIR), pages 669–674, 2011.[4]Franco Fabbri. A theory of musical genres: Two ap-plications.Popular Music Perspectives, pages 52–81,1981.[5]Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Deng-sheng Zhang. A survey of audio-based music classiﬁ-cation and annotation.Multimedia, IEEE Transactionson, 13(2):303–319, 2011.[6]Gijs Geleijnse, Markus Schedl, and Peter Knees. Thequest for ground truth in musical artist tagging in thesocial web era. InProceedings of the 8th InternationalConference on Music Information Retrieval (ISMIR),pages 525–530, 2007.[7]Yajie Hu and Mitsunori Ogihara. Genre classiﬁcationfor million song dataset using conﬁdence-based clas-siﬁers combination. InProceedings of the 35th inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, pages 1083–1084.ACM, 2012.[8]Jin Ha Lee, Kahyun Choi, Xiao Hu, and J StephenDownie. K-pop genres: A cross-cultural exploration.InProceedings of the 14th International Conference onMusic Information Retrieval (ISMIR), pages 529–534,2013.[9]Cory McKay and Ichiro Fujinaga. Musical genre clas-siﬁcation: Is it worth pursuing and how can it be im-proved? InProceedings of the 7th International Con-ference on Music Information Retrieval (ISMIR), pages101–106, 2006.[10]Alexander Schindler, Rudolf Mayer, and AndreasRauber. Facilitating comprehensive benchmarking ex-periments on the million song dataset. InProceedingsof the 13th International Conference on Music Infor-mation Retrieval (ISMIR), pages 469–474, 2012.[11]Mohamed Sordo, Oscar Celma, Martin Blech, and En-ric Guaus. The quest for musical genres: Do the ex-perts and the wisdom of crowds agree? InProceedingsof the 9th International Conference on Music Informa-tion Retrieval (ISMIR), pages 255–260, 2008.[12]Bob L Sturm. An analysis of the gtzan music genredataset. InProceedings of the second internationalACM workshop on Music information retrieval withuser-centered and multimodal strategies, pages 7–12.ACM, 2012.[13]Bob L Sturm. A survey of evaluation in music genrerecognition. InAdaptive Multimedia Retrieval: Seman-tics, Context, and Adaptation, pages 29–66. Springer,2014.[14]George Tzanetakis and Perry Cook. Musical genreclassiﬁcation of audio signals.IEEE Transactions onSpeech and Audio Processing, 10(5):293–302, 2002.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 247"
    },
    {
        "title": "Schematizing the Treatment of Dissonance in 16th-Century Counterpoint.",
        "author": [
            "Andie Sigler",
            "Jon Wild",
            "Eliot Handelman"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417369",
        "url": "https://doi.org/10.5281/zenodo.1417369",
        "ee": "https://zenodo.org/records/1417369/files/SiglerWH15.pdf",
        "abstract": "We describe a computational project concerning labeling of dissonance treatments – schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large cor- pus of 16th-century polyphonic music. We then apply struc- tural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possi- bilities.",
        "zenodo_id": 1417369,
        "dblp_key": "conf/ismir/SiglerWH15",
        "keywords": [
            "dissonance treatments",
            "schematic descriptions",
            "automatic score annotation",
            "database methods",
            "large corpus",
            "16th-century polyphonic music",
            "structural techniques",
            "coincidence of schemata",
            "extrapolate from found structures",
            "unused possibilities"
        ],
        "content": "SCHEMATIZING THE TREATMENT OF DISSONANCE IN16TH-CENTURY COUNTERPOINTAndie SiglerSchool of Computer Science,McGill University;Computing MusicJon WildSchulich School of Music,McGill Universitywild@music.mcgill.caEliot HandelmanComputing MusicABSTRACTWe describe a computational project concerning labelingofdissonance treatments– schematic descriptions of theuses of dissonances. We use automatic score annotationand database methods to develop schemata for a large cor-pus of 16th-century polyphonic music. We then apply struc-tural techniques to investigate coincidence of schemata,and to extrapolate from found structures to unused possi-bilities.1. INTRODUCTIONWe develop a set of schematic dissonance treatments (i.e.schemata under which the uses of dissonance are classi-ﬁable) using a large corpus of mass movements (almost1000) of Palestrina and Victoria, dating from the 16th cen-tury. Palestrina in particular has a resonance through thehistory of music as one whose style was raised to the sta-tus of a didactic norm.1As a result, Palestrina’s practice(or a simpliﬁcation of it) has been well known and imitatedfor centuries among academics and music students.2As afoil for Palestrina, we compare masses by Victoria, roughlycontemporaneous and with a similar dissonance treatment.The wealth of available literature on the dissonance prac-tice of this style gives us a departure point for developinga computational platform for its investigation, with a viewto generalization.1As pointed out in Alfred Mann’s 1991 forward to Jeppesen’sCoun-terpoint[2], one of several classic texts on the Palestrina style – as thetitle shows, the name Palestrina is all but synonymous with certain as-pects of basic musical organization – in particular the way a “point” (i.e.a note – or perhaps a musical “idea”) sounds and moves “counter” to (i.e.in relation to) another point or set of points.2Including e.g. Haydn, Mozart, Beethoven, Schubert, Rossini,Chopin, Berlioz, Liszt, Brahms, Bruckner, R. Strauss, and Hindemith,who all are known to have used Fux’sGradus ad Parnassum, based onthe Palestrina style ([1], Mann’s introduction).c\u0000Andie Sigler, Jon Wild, Eliot Handelman.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Andie Sigler, Jon Wild, Eliot Han-delman. “Schematizing the Treatment of Dissonance in 16th-CenturyCounterpoint”, 16th International Society for Music Information Re-trieval Conference, 2015.2. METHODOLOGY2.1 Automatic Score AnnotationPart of our methodology for investigating dissonances is tolook atautomatically annotated scores. Seeing annotatedscores helps us evaluate the correspondence of our speci-ﬁcation to our intention, and develop to new schemata. Itallows us to identify musical factors that would likely nothave been apparent otherwise (i.e. in a situation where datawas displayed in a musically non-intuitive way, or wherescores had to be painstakingly scrutinized to locate scarceoccurrences).Using a web-based music analysis system producedby Computing Music, we generate annotated scoreson-demand(at load time). It’s possible to load and analyseany score (including ones outside the corpus under inves-tigation), to load a random score from the corpus, or to“spin” through a corpus with a search for instances of aparticular conﬁguration, such that one keystroke displaysa new annotated score focussed on the relevant measure,bringing together similar occurrences from disparate loca-tions.2.2 Saving FeaturesOn a ﬁrst pass through the score, we save a set offea-turesfor each dissonance, including duration, surround-ing melodic intervals, metric weight, and type of attack,as needed to deﬁne our schemata ( – as we developedand added new schemata, an initial set of features was ex-panded). Any features not used in a given schema are opento any value.Saving a set of features for each dissonance rather thanjust applying a set of schematic ﬁlters on the ﬁrst passthrough the score has certain advantages. Suppose we runall dissonances through a set of ﬁlters, and several of themare labelled P for passing. Now if we want to ask questionsabout the set of passing notes (in fact matched by severaldifferent but related schemas) – e.g. how many are goingup or down, or how many are half-notes – we have tore-asksome of the same questions we already asked in orderto label them in the ﬁrst place. As well, if we have a setof remainingunlabelleddissonances, we will have no ideahowthey failed the tests for the different labels, or whatsubsets of unlabelled dissonances might have in common.We save feature-sets (and schema labels) in a database,645so that we can query them in different ways; databasesearching also helps us develop schemata based on feat-ural similarities of unschematized dissonances.3. DEFINITIONS AND SCHEMATA3.1 Dissonance and MeterAs in standard practice, we deﬁne thedissonant intervalsas the minor and major second, perfect fourth, tritone, andminor and major seventh, and their compounds (i.e. withadditional octaves). Adissonanceoccurs when two notescoincide or overlap in time and form a dissonant interval.From the initial set of dissonances in a score, we re-move certain fourths and tritones that participate in sonori-ties considered consonant.3If a perfect fourth is accom-panied by an additional voice sounding a third or ﬁfth (ortheir octave compound) below its lower note, it is consid-ered consonant. Likewise a diminished ﬁfth accompaniedby the pitch a major sixth below its lower note, or an aug-mented fourth accompanied by the pitch a minor third be-low its lower note.Metercan be thought of as a temporal grid. We general-ize to metricweight, where different places in the measureare said to be equally “strong” or “weak.” The downbeatis the strongest, followed by the divisions into halves, thendivisions into quarters, then eighths.The meters under consideration areduple, using whole-note divisions (e.g. 4/2; 2/2), ortriple, using dotted-wholedivisions (e.g. 3/2, 6/2, 9/2). We don’t differentiate be-tween whole notes in a duple meter or or dotted wholes ina triple meter; each one represents an equal beat.3.2 One- and Two-voice SchemataAlthough a dissonance is deﬁned as a relationship betweentwo notes in two different voices, commonly only one noteneeds to be “explained.”4Typically, when one note isstruck and then sustained (or reattacked) while the secondnote is struck (anobliquemotion), and the second note ison a “weak” metric position, only thesecondnote needsexplanation, since the dissonance only occurs once the sec-ond note enters – we call the second note the dissonance(with respect to the ﬁrst note). In these kinds of cases, wecan schematize a dissonance treatment with respect to fea-tures of the voice containing the dissonance, and not thevoice against which it dissonates. For example, apass-ingnote is schematized by either of two different melodicshapes: (step up, step up) or (step down, step down).5It issimultaneously schematized by one of four different metricshapes: a half note on a weak half preceded by a durationof at least a half, a quarter note on a weak quarter, an eighthon a weak quarter, or an eighth on a weak eighth. We3These correspond to major and minor triads in root position or ﬁrstinversion, and diminished triads in ﬁrst inversion, though these designa-tions are anachronistic for the 16th century.4Informally,explainingmeans locating a theorized schematic disso-nance treatment to which a dissonance corresponds.5I.e. (step up, step up) gives a ﬁgure ofthree notes, including a stepuptothe dissonating note called the “passing note”, and another step upfromthe passing note.have deﬁned several other “single-voice” schemata; theseare summarized in Table 1.Asuspensionis atwo-voiceschema, involving the sus-pended note as well as its counterpart, the “agent.” Theagent, or active voice, is an obliquely struck dissonanceon a strong beat, after which theothervoice (the suspen-sion) is constrained to resolve downward by step. Sincewe’ve already set up machinery to ﬁndattackeddisso-nances, rather than to ﬁnd notes that are dissonated againstat a particular place in their duration, it’s convenient to startthe schematization of suspensions with the agent, ratherthan with the suspension note itself. When we ﬁnd anoblique dissonance on a strong beat, we can pull in a fea-ture set for the note against which it dissonates, and checkwhether the combination constitutes a suspension. Furtherdescription of suspensions can be found in Table 2.3.3 Extending the pairwise modelWe originally deﬁned dissonances as occurring betweentwovoices. One exception to this model that we havealready addressed is the consideration as consonant offourths and tritones that are covered by certain notes ina lower-sounding voice – these are “vertical” orharmonicschemata. Apart from these, we have so far used a pair-wise model to schematize dissonances between any twovoices. But we found we had to extend the pairwise modelto account for some dissonances. These are summarized inTable 3.1. We ﬁnd that if a note is consonant with an agent of asuspension, it can be dissonant with the suspension withoutfurther constraint; as well, we ﬁnd situations where a noteis dissonant with an agent, but explicable as consonant withthe suspension.2. On a weak quarter, two quarter notes or eighths (orone of each) may be dissonant with respect to each other, ifthere is a third voice such that each is explained as conso-nant, passing, neighboring, a cambiata, or an anticipationwith respect to the (same) third voice. (See Figure 1.)3. A notemthat is dissonant within a given pair ofvoices is in condition M if it has the same pitch class asa note in a third voice that was already sounding whenmentered, and is sustained at least until the end ofm. Notesin condition M are often approached and left by leap. Anote in condition M may be attacked simultaneously witha dissonance; in this case the note not labelled M will beexplained (e.g. as a passing or neighbor note) with respectto the third voice.64. DISCUSSION: EXCEPTIONS AND INDUCTIONAt the time of this writing, there are still⇠360 dissonancesin the Palestrina-Victoria corpus that are not explained by6In fact, if we look at half notes that are dissonant counterparts to con-dition M, we ﬁnd that they areallpassing notes, with six exceptions thatare upper neighbors – and these six are all in the same mass of Victoria.This is an example of a unique dissonance treatment, used motivically,that is clearly related to the more common passing version.646 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015SymbolNameMelodic schemaMetric schemaAttackPPassing(step up, step up)(step dn, step dn)weak quartereighth on weak quarterweak eighthweak half after\u0000halfobliqueNNeighbor(step up, step dn)(step dn, step up)(same as for P)obliqueC5,C4,C3(5/4/3-note)Cambiata(step dn, third dn,step up, step up)– or ﬁrstnnotes of thisweak quarterobliqueAAnticipation(step, repeat)weak quarterweak eighthobliqueEEchapp´ee(step up, leap dn)(step dn, leap up)weak quarterobliqueF“Fake”suspension(step or repeat,step dn)syncopated wholesyncopated halfsyncopated dotted-halfobliqueQ,QxThird quarter(step dn, step dn)quarter on weak half;Q if after\u0000half, otherwiseQxobliquesimultaneousLLeap of third(third dn, step up)weak quarterobliqueTable 1: “Single-voice” SchemataSymbolNameDescriptionS,GSuspension,AgentSuspension S is sustained or reattacked on the same note; agent G strikesoblique dissonance; S moves down (to itsresolution) by step on a weaker beatthan G.T,T2,GSuspension withthird-skip, AgentAs S, but with resolution (third dn, step up) in quarter notes; the note skippeddown to can be dissonant (called T2).Table 2: SuspensionsSymbolNameDescriptionGcConsonantwith AgentDissonant with a suspension S or T but consonant with its agent. Or dissonantwith a “Fake” suspension F and consonant with its “agent.”ScCons. withSuspensionDissonant with an agent but consonant with its suspension.M/M2,MxMatchHas the same pitch / pitch class as a note in athirdvoice already sounding whenM entered, and is sustained at least until the end of M. M’s dissonant counterpartMx is attacked simultaneously with M; explained as P or N with thethirdvoice.WWeak-quarterclashOn a weak quarter, a dissonance between two quarter notes or eighths (or oneof each), such that each is consonant, passing, neighboring, a cambiata, or ananticipation with respect to some (same) third voice.Table 3: Schemata: Extending the Pairwise Modelany of our schemata (versus⇠194100 thatare– we’ve suc-cessfully schematized>99.8% of dissonances in the cor-pus). Unschematized dissonances are marked with an X inannotated scores. There are quite a few errors (e.g. wrongnotes or durations) in our corpus, and it looks like a con-siderable proportion of Xs are due to these. The ability toquickly navigate to problematic dissonances allows us tomake corrections where they are necessary (i.e. by com-parison with another edition) – correction of the corpus iscurrently underway. This method doesn’t locateallerrorsin the corpus, but it does point out especially “bad” onesfrom the point of view of dissonance.Examining Xs is also part of our development method-ology for formulating new dissonance categories. For in-stance, by doing some ﬁltering on a database of unmatchedfeature-sets, we noticed that there were 54 unschematizeddissonances that are on weak quarters and are approachedby a third down and left by a step up. We wrote this schemainto our speciﬁcation (“L” in Table 1), and then were ableto “spin” through the instances in the corpus to see whetherthe schema met our expectations on the annotated scores.7In another database exploration case, we began by ob-serving that there were quite a few unschematized disso-nant half- and quarter-notes on beat one, which were ap-proached and left by a step. This preliminary schemati-7After ﬁnding this dissonance in the database, we observed that it ismentioned (as possibly an “archaism”) in [3], p. 220. Jeppesen’s studyproves to be a tour de force of detail – for example, on p. 268 he shows asuspension that jumps down a ﬁfth before leaping back up a fourth to itsresolution, saying that as far as he’s observed, “this occurs but once in thewhole collection of Palestrina’s compositions,” despite being a standardpractice in [1]. We don’t ﬁnd a second occurrence in Palestrina, and itoccurs once in our Victoria corpus.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 647Figure 1: W (weak-quarter clash) are “explained” disso-nances (or consonances) with respect to other voices (inthis case P and C5), but simultaneous, unclassiﬁed disso-nances with respect to each other.\nFigure 2: The tied note in the third voice marked “F S” isa fake suspension (F) with respect to the bottom voice, anda “real” suspension (S) with respect to the agent (markedG) in the voice above.zation was obviously too general to keep as a ﬁnal label-ing, since we don’t wish to allow passing and neighbor-ing notes on strong beats indiscriminately. But lookingthrough these instances showed us that (along with a smallnumber of less explicable occurrences), there were a cou-ple of schematic situations. One such situation occurredwhen the dissonance in question had anagentas its dis-sonant counterpart, while beingconsonantwith the corre-spondingsuspension(shown as label Sc in Table 3). Wealso were able to reﬁne our deﬁnition ofsuspensionsbylooking at these unschematized strong-beat dissonances.Our original deﬁnition stipulated that the agent must beconsonantwith suspension’s note ofresolution(whetheror not the agent is still sounding at the time of the reso-lution). In fact, we ﬁnd there is one situation where thisrule doesn’t hold: when the suspended interval is a dimin-ished ﬁfth, resolution forms a fourth (– dissonant) with the\nFigure 3: A unique structure of simultaneous dissonances:two passing notes, a neighbor, and a cambiata. (Palestrina:Laudata Dominum, Gloria)agent. When this happens, the agent always moves up astep to meet the resolution in a (consonant)third.When exploring for new schemata, we sometimes comeagainst occurrences that are interestingly rare. For in-stance, we ﬁnd that there are six third-quarter passing notesgoingupwardin the combined Palestrina-Victoria corpus.Of these, four are in one mass of Victoria, and are essen-tially repetitions of the same single situation. The remain-ing two are separate instances in Palestrina. These kind ofinstances open musicological questions as to the interpre-tation of these scare occurrences:whywas this possibilityused just here, and practically nowhere else.Database exploration works not only for inductionof new schemas, but for deeper exploration of deﬁnedschemas. For instance, if we look at the feature set forthe relatively rare half-note lower-neighbors, we ﬁnd thatmostof them (in Palestrina 119/150, or 79%) are a perfectfourth above the note they dissonate with. A few (13, or9%) are a tritone below, and on closer inspection, these allseem to take part in very similar cadential ﬁgures. Victoriauses the tritone/cadential lower neighbor somewhat moreoften – 20/83 or 24%, and the perfect fourth above 43/83or 52%: a similar but less dramatic tendency.Likewise, we ﬁnd that our category for “fake” suspen-sion (F) (which Jeppesen calls a “consonant fourth”) neveroccurs with a tritone, and in factalwaysoccurs with eithera fourth, or (less frequently) a fourthandseventh or second(i.e. with respect to two different voices) . Furthermore,the F which isonlya fourth at its onset is almost alwaysaccompanied by a suspension (S) of a seventh or secondon the next strong beat (Figure 2) – the fake suspension of648 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015a fourth withnoseventh or second at all is found only 16times in the Palestrina corpus and never in Victoria. Wecould continue this line of musicological investigation bysurveying for further details, ﬁnding e.g. those fake sus-pensions which are a half note in duration, or those intro-duced by leap, or those which include a dissonance of ami-norsecond ormajorseventh (a rare occurrence), or thosewhich have a resolution of amajorsecond (relatively rare).We wonder: would it be feasible toautomaticallyin-duce dissonance treatments over a corpus (i.e. start fromscratch and have a search deliver a set of schemata that areused a minimum number of times in a corpus). Althoughthis would be computationally expensive, it seems possi-ble.The strategy for doing so, however, is not com-pletely transparent. If we address the subset of one-voiceschemata, we can imagine trying to cover the set of dis-sonances with minimal explanatory schemata (with theheuristic that more proximate intervals have to be part ofa schema before more distant intervals can be included).For conjunctions, this is straightforward enough (e.g. mustbe on a weak quarterandresolve down). For disjunctions,we would have to infer whether a reduced set of featuresshould be speciﬁed, or whether to use a wild card. Wewould have to be careful not to overﬁt schemata, whichwould result in a large number of highly speciﬁc schematainstead of a smaller number of more general ones (e.g. apassing note ﬁgure, once completed can be followed by astep up, or a leap up, or a third down, etc.). There’s alsono obvious way of joining multiple discovered schemataunder one descriptive tag. For example, eight differentschemata emerge for what we call “passing notes” (de-pending on their position, duration, and orientation) – andthis is not including third-quarter passing notes, which wehave chosen to name differently.The schemata found would be constrained to be de-scribed by the feature set we’re examining. We’ve usedshorthand features such as “weak quarter,” generalizingsecond and fourth quarters, and “leap up,” generalizingseveral intervals. If we started off an automatic schemainduction with these generalized features, it would be pow-erless to differentiate them ( – generalizingreducesourpower as human experts to differentiate them, but we stillstand a chance of doing so by looking at scores). On theother hand, if we start with alargerfeature set, we increasethe search space exponentially, but add an interesting layeroffeatureinduction. Even if we start with a larger featureset, we’re still constrained by pre-process feature selection,whereas humans are free to add features midstream.We won’t discuss here the added problem of tryingto induce two-voice schemata such as suspensions fromscratch, nor the various three-voice schemata. We wouldalso need to considerharmonictreatment: dissonancesmay be treated differently when they’re a part of a chord(aside from the chords we have already discussed, for somecorpora seventh chords, root-position diminished triads, orsecond-inversion triads have special status). Having errorsin the corpus also complicates the picture.While automatic schema induction is an interesting con-cept, for the time being it seems that using database queriesand automatic score annotation to facilitate deep interac-tion of human intelligence with a musical corpus is stillthe most effective procedure.5. STRUCTURING DISSONANCESSo far, what we’ve described are speciﬁc ﬁlters deﬁned onfeature vectors. These ﬁlters assign tags to notes, label-ing the dissonance treatment of the note. Now we have theopportunity to see how these dissonance treatments inter-act. For instance, it’s quite common to have two or morepassing notes in different voices at the same time. Whatother combinations of dissonance might occur? For thisanalysis, we don’t have to develop new schemata and ﬁl-ter for them, we merely have tobuild structuresout of thedissonances we already have.The procedure is this: we take a set of labeled disso-nances, and build graphs of temporal relations betweenthem. For the purpose of this example, we keep the spacesmall by only examining a subspace of temporal relationsbetween dissonances. We use three types of temporal re-lation: monophony (i.e. one or more notes beginning andending at the same time), inclusion (i.e. a note’s durationbeingwithinthe duration of another note), and overlap.We also use a subset of dissonances: passing and neigh-boring tones, third-quarters, anticipations,´echapp´ees,cambiatas, dissonant leaps of a third, “real” and “fake”suspensions, and weak-quarter clashes. The experimentreduces each score tojustthe notes marked with these la-bels, and then constructs polyphonic structures out of theremaining subscore. That is, we will connect tagged disso-nances that are in temporal contact with one another, thenexamine sets of connected dissonances in the corpus. Inwhat follows, we are counting notnotes, butstructures,which can contain one or more notes.We obtain 297 different structures by this method – 243in Palestrina and 175 in Victoria, with 121 in their intersec-tion, and therefore 122 in Palestrina but not Victoria and54 in Victoria but not Palestrina.8Of the 297, 113 occuronlyoncein the combined corpus, while another 80 occurfewer than ﬁve times. In general, we see a relatively smallnumber of structures occurring very frequently, and a largenumber of structures occurring rarely.The most common structures in Palestrina and Victoriadiffer only slightly. The most frequent for both composersis the lone passing note, followed by the suspension, thedouble-passing note (i.e. two simultaneous passing notes),and then the (lone) neighbor. The next most common forPalestrina is the third-quarter passing note, then simultane-ous passing and neighbor notes, and simultaneous neigh-bor notes. Victoria would be the same, except the third8The absolute numbers themselves are not of great interest, and wedon’t offer a proper statistical analysis, we only mean to give a generalorientation as to the structural variety available from the point of view ofthis experiment. The numbers are, furthermore, provisional since we’restill correcting the corpus, but the great majority of rare structures arenotdue to corpus errors.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 649quarter dissonance is slightly more rare in Victoria, appear-ing after the latter two.Other structures show a greater difference in practicebetween the composers. For instance, just looking at struc-tures with double suspensions, we ﬁnd that Palestrina re-solves these at different times (i.e. one resolution coming aquarter note before the other) over half of the time, whereasVictoria only resolves them at different times about 10 per-cent of the time.9We ﬁnd also that simultaneous “fake”suspensions don’t occur in Palestrina, while there are 26instances in the Victoria corpus. A ﬁgure in which a noteis a dissonant third-quarter with respect to one voice at thesame time as being interpreted as the agent of a diminishedsuspension10in another voice is found 23 times in Palest-rina and once in Victoria. The cambiata occurring withinthe duration of a suspension, and the double third-quarterdissonance are also much more frequent in Palestrina thanin Victoria. Everything found in Victoria more than threetimes is also found in Palestrina at least once – it’s not ob-vious if this is an artifact of the difference in the sizes ofthe corpora,11or whether it reﬂects on the practice of thecomposers.6. EXTRAPOLATION AND NEW STRUCTURESThe distribution of structures of labelled dissonances, withmany structures used only once or a handful of times,shows us that we are not dealing with a closed set ofreusable possibilities, but acomposablespace. This sug-gests that it’s possible to build structures that are not inthe corpus, but that are within the matrix of possibilitiesoutlined by the corpus. In efforts to build style-copyingautomata, a trend has been tore-useandre-combineele-ments found in a corpus. But since it is the responsibilityof the artist to offersomething newin each work, reasoningaboutunusedstructures is essential for deeper explorationof corpus extension.What we present in this section was not constructed au-tomatically; we simply show that the structures we ob-tained from labelled dissonances seem to constitute a setwith missing elements whichmighthave been used in thecorpus. It is our opinion that it would be possible to con-struct these automatically, and that in any case, the set ofunused possibilities (and the set of once-used possibilities)are an avenue of insight into the nature of composition.Our ability to schematize the treatment of a great major-ity of dissonances in the corpus points to a constrained andrule-bound composition practice. How does this relate tothe obligation to create new and different works? And is itpossible toreasonabout newness and difference? In thissection we suggest an approach.We can proceed rather conservatively: instead of try-ing to invent complex and exotic new combinations thatmight be realizable, we can start by looking for unﬁlled9We can see this because these two instances have different poly-phonic proﬁles: if they both resolve at the same time they’re in rhythmicmonophony with one another, whereas if one resolves ﬁrst, one suspen-sion is durationally contained within the other.10I.e. a suspension with duration of a quarter.11261 movements of Victoria vs. 705 of Palestrinaniches that are relatively simple. For instance, if we takethe subset of structures that consist of more than twosimul-taneousdissonances including at least one cambiata andone neighbor, we ﬁnda single structurethat occurs once:a cambiata, neighbor, and two passing tones at the sametime. This means that thesimplercambiata, neighbor, andonepassing tone never occurs! We also see other obviouscombinations including a cambiata and two neighbors, twocambiatas and a neighbor, and a cambiata, two neighborsand one passing note. It is simple to enumerate all of thepossibilities in this small combinatorial space.12For a given constructed dissonance structure, it’s notguaranteed that it is realizable. We can try to realize itsystematically by generating and testing candidates. Thespace of candidates is small enough to be tractably enu-merable, especially if we proceed in stages, leaving theissue of voicing (order of voices from low to high) untillater. Candidates can be rejected if they cause unschema-tized dissonances (Xs), or break some other constraint –e.g. we might reject parallel ﬁfths, octaves, and unisons, toconform with the style. It turns out that we can constructviable fragments in which a cambiata, a neighbor, and apassing tone occur simultaneously, or in which a cambi-ata and two neighbors occur simultaneously (left as an ex-ercise for the reader!). As far as we can tell, there’s no“reason” that these don’t occur in the corpus.We can extend this game of ﬁnding unused potentials bytaking the interval combinations of a structure as anotherparameter. For instance, four simultaneous passing notesoccur about 40 times in the combined corpus, but most ofthe time the passing note “chord” is just a minor or ma-jor third, with pairs of passing notes up and down througheach note of the third. There is one instance where a mi-nor triad is constituted (one passing note is preceded bya dissonant third quarter). The major triad occurs severaltimes in the triple-passing-tone structure; it appears to bean unused possibility in the quadruple.The possibility for combinatorial explorations are vast.For instance, there are more than 70 differentsonorities(pitch-class sets sounding at some moment) in Palestrina,while only 7 of them need not involve dissonance. The restare constructed precisely in the manner we have just beendescribing, with combinations of dissonance treatments.Equally great are the opportunities for musicologists tostudy speciﬁc usages in their musical, textual, and histori-cal contexts; the computational means to ﬁnd and annotatesets of occurrences will surely facilitate this process.The general methodology used here can be extended toother corpora, and to other aspects of musical practice. Thecomputational study of musical corpora through schemati-zation, structure-building, automatic annotation, and gen-erative extrapolation will bring a new scope and precisionto our understanding of musical practice and potential.12In fact thewholespace of dissonance structures under this modelmay be small enough to be feasibly enumerable. If so, how does this factrelate to our surmise thatfor Palestrina and Victoria, the space seems tobe “composed” rather than enumerated? This is a question for the practiceand philosophy of the nascent discipline of constructive musicology, orthe study of corpora through computational extension.650 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]Fux, J.J.; Mann A. (trans. & ed.):The Study of Coun-terpoint from J.J. Fux’s Gradus Ad Parnassum.Norton& Co. (1971/1965/1725)[2]Jeppesen, K.:Counterpoint: The Polyphonic Vo-cal Style of the Sixteenth Century.Dover Publications(1992/1939/1931)[3]Jeppesen, K.:The Style of Palestrina and the Disso-nanceDover Publications (1970/1946)[Sch.1999]Schubert, P.:Modal Counterpoint, Renais-sance StyleOxford University Press (1999)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 651"
    },
    {
        "title": "Audio Chord Recognition with a Hybrid Recurrent Neural Network.",
        "author": [
            "Siddharth Sigtia",
            "Nicolas Boulanger-Lewandowski",
            "Simon Dixon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416594",
        "url": "https://doi.org/10.5281/zenodo.1416594",
        "ee": "https://zenodo.org/records/1416594/files/SigtiaBD15.pdf",
        "abstract": "In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language mod- els for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic sig- nal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a mod- ification to beam search using a hash table which yields im- proved results while reducing memory requirements by an order of magnitude, thus making the proposed model suit- able for real-time applications. We evaluate our model's performance on a dataset with publicly available annota- tions and demonstrate that the performance is comparable to existing state of the art approaches for chord recogni- tion.",
        "zenodo_id": 1416594,
        "dblp_key": "conf/ismir/SigtiaBD15",
        "keywords": [
            "audio chord estimation",
            "novel architecture",
            "recurrent neural network",
            "temporal dependencies",
            "feed forward deep neural networks",
            "discriminative features",
            "time-frequency representation",
            "beam search",
            "hash table",
            "real-time applications"
        ],
        "content": "AUDIO CHORD RECOGNITION WITH A HYBRID RECURRENTNEURAL NETWORKSiddharth Sigtia⇤Nicolas Boulanger-Lewandowski†Simon Dixon⇤⇤Centre for Digital Music, Queen Mary University of London, London, UK†Dept. IRO, Universit´e de Montr´eal, Montr´eal (QC), H3C 3J7, Canada⇤{s.s.sigtia,s.e.dixon}@qmul.ac.ukABSTRACTIn this paper, we present a novel architecture for audiochord estimation using a hybrid recurrent neural network.The architecture replaces hidden Markov models (HMMs)with recurrent neural network (RNN) based language mod-els for modelling temporal dependencies between chords.We demonstrate the ability of feed forward deep neuralnetworks (DNNs) to learn discriminative features directlyfrom a time-frequency representation of the acoustic sig-nal, eliminating the need for a complex feature extractionstage. For the hybrid RNN architecture, inference overthe output variables of interest is performed using beamsearch. In addition to the hybrid model, we propose a mod-iﬁcation to beam search using a hash table which yields im-proved results while reducing memory requirements by anorder of magnitude, thus making the proposed model suit-able for real-time applications. We evaluate our model'sperformance on a dataset with publicly available annota-tions and demonstrate that the performance is comparableto existing state of the art approaches for chord recogni-tion.1. INTRODUCTIONThe ideas presented in this paper are motivated by the re-cent progress in end-to-end machine learning and neuralnetworks. In the last decade, it has been shown that givena large dataset, deep neural networks (DNNs) are capableof learning useful features for discriminative tasks. Thishas led complex feature extraction methods to be replacedwith neural nets that act directly on raw data or low levelfeatures. Current state-of-the-art methods in speech recog-nition and computer vision employ DNNs for feature ex-traction [12]. In addition to feature learning, recurrent neu-ral networks (RNNs) have been shown to be very power-ful models for temporal sequences [9, 12]. In the ﬁeld ofMusic Information Retrieval (MIR), various studies have†NB is currently working at Google Inc., Mountain View, USA© Siddharth Sigtia, Nicolas Boulanger-Lewandowski, Si-mon Dixon. Licensed under a Creative Commons Attribution 4.0 Inter-national License (CC BY 4.0).Attribution:Siddharth Sigtia, NicolasBoulanger-Lewandowski, Simon Dixon. “Audio Chord Recognition witha Hybrid Recurrent Neural Network”, 16th International Society for Mu-sic Information Retrieval Conference, 2015.applied neural network based models to different tasks [3,11, 15]. These experiments have been motivated by thefact that hand-crafting features to extract musically rele-vant information from audio is a difﬁcult task. Existingapproaches in MIR would beneﬁt greatly if feature extrac-tion could be automated.Audio chord recognition is a fundamental problem inMIR (see [13] for a review). At a high level, popular chordrecognition algorithms follow a pipeline similar to the onefollowed in speech. Most systems are comprised of anacoustic modelwhich is used to process the acoustic in-formation present in the audio signal. The estimates of theacoustic model are further reﬁned by alanguage modelthatmodels the temporal relationships and structure present insequences of chord symbols. Our proposed approach de-viates from existing approaches in two fundamental ways.We use DNNs to learn discriminative features from a time-frequency representation of the audio. This is contrary tothe common approach of extracting chroma features (andtheir many variants) as a preprocessing step. Secondly, wegeneralise the popular method of using a Hidden MarkovModel (HMM) language model with a more powerful RNNbased language model. Finally, we combine the acousticand language models using a hybrid RNN architecture pre-viously used for phoneme recognition and music transcrip-tion [5, 14].In the past, RNNs have been applied to chord recog-nition and music transcription in a sequence transductionframework [3, 4]. However, these models suffer from anissue known asteacher forcing, which occurs due to thediscrepancy between the training objective and the waythe RNN is used at test time. During training, RNNs aretrained to predict the output at any time step, given the cor-rect outputs at all preceding steps. This is in contrast tohow they are used at test time, where the RNN is fed pre-dictions from previous time steps as inputs to the model.This can lead to an unsuitable weighting of the acousticand symbolic information, which can quickly cause errorsto accumulate at test time. The hybrid RNN architectureresolves this issue by offering a principled way for explic-itly combining acoustic and symbolic predictions [14].The hybrid RNN model outputs a sequence of condi-tional probability distributions over the output variables(Section 3). The structure of the graphical model makesthe problem of exactly estimating the most likely sequenceof outputs intractable. Beam search is a popular heuris-127tic graph search algorithm which is used to decode condi-tional distributions of this form. Beam search when usedfor decoding temporal sequences is fundamentally limitedby the fact that sequences that are quasi-identical (differat only few time steps) can occupy most of the positionswithin the beam, thus narrowing the range of possibilitiesexplored by the search algorithm. We propose a modiﬁ-cation to the beam search algorithm which we callhashedbeam searchin order to encouragediversityin the exploredsolutions and reduce computational cost.The rest of the paper is organised as follows: Section 2describes the feature learning pipeline. Section 3 brieﬂy in-troduces the hybrid RNN architecture. Section 4 describesthe proposed modiﬁcation to the beam search algorithm.Experimental details are provided in Section 5, results areoutlined in Section 6 and the paper is concluded in Section7.2. FEATURE LEARNINGWe follow a pipeline similar to the one adopted in [3, 15]for feature extraction. We transform the raw audio signalinto a time-frequency representation with the constant-Qtransform [6]. We ﬁrst down-sample the audio to 11.025kHz and compute the CQT with a hop-size of 1024 sam-ples. The CQT is computed over 7 octaves with 24 bins peroctave yielding a 168 dimensional vector of real values.One of the advantages of using the CQT is that the rep-resentation is low dimensional and linear in pitch. Com-puting the short-time Fourier transform over long analysiswindows would lead to a much higher dimensional rep-resentation. Lower dimensional representations are usefulwhen using DNNs since we can train models with fewerparameters, which makes the parameter estimation prob-lem easier.After extracting CQT frames for each track, we use aDNN to classify each frame to its corresponding chord la-bel. As mentioned earlier, DNNs have been shown to bevery powerful classiﬁers. DNNs learn complex non-lineartransformations of the input data through their hidden lay-ers. In our experiments we used DNNs with 3 hidden lay-ers. We constrained all the layers to have the same numberof hidden units to simplify the task of searching for goodDNN architectures. The DNNs have a softmax output layerand the model parameters are obtained using maximumlikelihood estimation.Once the DNNs are trained, we use the activations of theﬁnal hidden layer of the DNN as features. In our experi-ments we observed that the acoustic model performancewas improved (⇠3%absolute improvement in frame-levelaccuracy) if we provided each frame of features with con-text information. Context information was provided byperforming mean and variance pooling over a context win-dow around the central frame of interest [3]. A contextwindow of length2k+1is comprised of the central frameof interest, along withkframes before and after the centralframe. In our experiments we found that a context windowof7frames provided the best results.We trained the network with mini-batch stochastic gra-\nFigure 1. Feature Extraction Pipelinedient descent. Instead of using learning rate update sched-ules, we use ADADELTA which adapts the learning rateover iterations [18]. In our experiments we found Dropoutwas essential to improve generalisation [16]. We found aDropout rate of0.3applied to all layers of the DNN to beoptimal for controlling overﬁtting. Once the models aretrained, we use the model that performs best on the vali-dation set to extract features. In our experiments, the bestperforming model had 100 hidden units in each layer. Fig-ure 1 is a graphical representation of the feature extractionpipeline. In section 6, we compare DNN acoustic modelswith different feature inputs.3. HYBRID RECURRENT NEURAL NETWORKSSimilar to language, chord sequences are highly correlatedin time. We propose exploiting this structure for audiochord estimation using hybrid RNNs. The hybrid RNNis a generative graphical model that combines the predic-tions of an arbitrary frame level classiﬁer with the predic-tions of an RNN language model. For temporal problems,the predictions of the system can be greatly improved bymodelling the relationships between outputs, analogous tolanguage modelling in speech. Typically, HMMs are em-ployed in order to model and exploit this structure. HybridRNNs generalise the HMM architecture by using powerfulRNN language models.3.1 RNN Language ModelRNNs can be used to deﬁne a probability distribution overa sequencez={z⌧|0⌧T}in the following manner:P(z)=TYt=1P(zt|At)(1)whereAt⌘{z⌧|⌧<t}is the sequence history at timet.The above factorisation is achieved by allowing the RNNat timet\u00001to predict the outputs at the next time step,yielding the conditional distributionP(zt|At).The RNN is able to model temporal relationships via itshidden state which at any timethas recurrent connections128 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015to the hidden state att\u00001. The hidden state is updatedaccording to the following equation:ht=\u0000(Wzhzt\u00001+Whhht\u00001+bh)(2)whereWzhare the weights from the inputs att\u00001to thehidden units att,Whhare the recurrent weights betweenhidden units att\u00001andtandbhare the hidden biases.The form of the hidden state (Equation 2) implies that thepredictions at timetare explicitly conditioned on the en-tire sequence historyAt. This is contrary to HMMs whichare constrained by the Markov property. Therefore, the-oretically RNNs can model complex and long-term tem-poral dependencies between outputs. The parameters ofthe RNN are estimated by using stochastic gradient basedmethods. Although theoretically very powerful, RNNs arelimited by the effectiveness of the optimisation method [2].The hidden units described above can be replaced by Long-Short Term Memory (LSTM) units in order to improve theparameter estimation and generalisation capabilities of theRNN (see [10] for a review). In our model we use RNNswith LSTM memory units to model the symbolic structureof chord sequences.3.2 Hybrid ArchitectureGiven a sequence of acoustic framesxand a sequence ofcorresponding chord outputsz, the Hybrid RNN modelfactorises the joint probability ofxandzaccording to thefollowing equation:P(z,x)=P(z1...zT,x1...xT)(3)=P(z1)P(x1|z1)TYt=2P(zt|At)P(xt|zt)/P(z1)P(z1|x1)P(z1)TYt=2P(zt|At)P(zt|xt)P(zt).By restricting the acoustic model to operate on an acous-tic framextindependent of previous inputs and outputs,the distributionsP(zt|At)andP(zt|xt)can be indepen-dently modelled by an RNN and an arbitrary frame-levelclassiﬁer, respectively. The form of the joint probabilitydistribution makes maximum likelihood estimation of themodel parameters using gradient based optimisers easy.The acoustic and language model terms separate out whenoptimising the log-likelihood and the model parameterscan be trained using gradient based methods according tothe following equations where⇥a,⇥lare parameters ofthe acoustic and language models, respectively:@logP(z,x)@⇥a=@@⇥aTXt=1logP(zt|xt)(4)@logP(z,x)@⇥l=@@⇥lTXt=2logP(zt|At).(5)Although the hybrid RNN has a similar structure (seper-ate acoustic and language models) to the sequence trans-duction model in [9], the hybrid RNN explicitly combinesthe acoustic and language model distributions. The trans-duction model in [9], modelsunalignedsequences with animplicit exponential duration.The property that the acoustic and language models canbe trained independently has some useful implications. InMIR, it is easier to obtain chord and note transcriptionsfrom the web as compared to audio data due to copyrightissues. We can use the abundance of transcribed data totrain powerful language models for various tasks, withoutthe need for annotated, aligned audio data.4. INFERENCEThe hybrid RNN generalises the HMM graph by condi-tioningzton the entire sequence historyAt, as comparedto the HMM graph whereztis only conditioned onzt\u00001(Equation 3). This conditioning allows musicological struc-ture learnt by the language model to inﬂuence successivepredictions. One consequence of the more general graph-ical structure is that at test time, inference over the outputvariables attrequires knowledge of all predictions madetill timet. At anyt, the historyAtis still uncertain. Mak-ing estimates in a greedy chronological manner does notnecessary yield good solutions. Good solutions correspondto sequences that maximise the likelihood globally.Beam search is a standard search algorithm used to de-code the outputs of an RNN [5, 9, 14]. Beam search is abreadth-ﬁrst graph search algorithm which maintains onlythe topwsolutions at any given time. At timet, the algo-rithm generates candidate solutions and their likelihoods att+1, for all the sub-sequences present in the beam. Thecandidate solutions are then sorted by log-likelihood andthe topwsolutions are kept for further search. A beam ca-pacity of1is equivalent to greedy search and a beam widthofNTis equivalent to an exhaustive search, whereNisthe number of output symbols andTis the total number oftime steps.Beam search suffers from a pathological condition whenused for decoding sequences. Quasi-identical sequenceswith high likelihoods can saturate the beam. This limitsthe range of solutions evaluated by the algorithm. This isespecially true when decoding long sequences. The per-formance of beam search can be improved by pruning so-lutions that are unlikely. The dynamic programming (DP)based pruned beam search algorithm makes better use ofthe available beam capacityw[3, 5]. The strategy em-ployed for pruning is that at any timet, the most likelysequence with output symbolzt2Cis considered andother sequences are discarded, whereCis the set of outputsymbols.Although the DP beam search algorithm performs wellin practice [3,5], pruning based on the last emitted symbolis a strict constraint. In the next section we propose a mod-iﬁcation to the beam search algorithm that is more generaland allows ﬂexible design to enforcediversityin the setof solutions that are explored and to reduce computationalcost.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1294.1 Hashed Beam SearchAs discussed before, beam search can lead to poor esti-mates of the optimal solution due to saturation of the beamwith similar sequences. The efﬁciency of the search algo-rithm can be improved by pruning solutions that are suf-ﬁciently similar to a sequence with higher likelihood. Wepropose a more general variant of the pruned beam searchalgorithm where the metric for similarity of sequences canbe chosen according to the given problem. We encode thesimilarity metric in the form of ahash functionthat deter-mines the similarity of 2 sequences. Given 2 solutions withthe same hash value, the solution with the higher likelihoodis retained.The proposed algorithm is more general and ﬂexiblesince it allows the similarity metric to be chosen based onthe particular instance of the decoding problem. We de-scribe the algorithm for decoding chord sequences. Welet the hash function be the lastnemitted symbols. Withthis hash function, if there are two candidate solutions withthe same sequence ofnsymbols at the end, then the hashfunction produces the same key and we retain the solu-tion with the higher likelihood. Whenn=1, the algo-rithm is equivalent to the DP beam search algorithm. Whenn=len(sequence), then the algorithm is equivalent to reg-ular beam search. Therefore, by increasing the value ofn,we can linearly relax the constraint used for pruning in theDP-like beam search algorithm.Another generalisation that can be achieved with thehash table is that for each hash key, we can maintain a listofksolutions using a process called chaining [17]. Thisis more general than the DP beam search algorithm whereonly the top solution is kept for each output symbol. Algo-rithm 1 describes the proposed hashed beam search algo-rithm, while Algorithm 2 describes the beam objects. Thetime complexity of Algorithm 1 isO(NTwlogw). Eventhough the time complexity of the proposed algorithm isthe same as regular beam search, the algorithm is able tosigniﬁcantly improve performance by pruning unlikely so-lutions (see Section 6). In Algorithms1and2,sis a sub-sequence,lis the log-likelihood ofsandfhis the hashfunction.Algorithm 1Hashed Beam SearchFind the most likely sequencezgivenxwith a beamwidthw.beam new beam objectbeam.insert(0,{})fort=1toTdonewbeam new beam objectfor(l, s)inbeamdoforzinCdol0= logPlm(z|s)Pam(z|xt)\u0000logP(z)newbeam.insert(l+l0,{s, z})beam newbeamreturnbeam.pop()Although the description of the proposed algorithm hasbeen within the context of decoding chord sequences, var-ious other measures of similarity can be constructed de-pending upon the problem. For example, for chord andspeech recognition, we can use the lastnunaligned sym-bols as the hash function (results with chords were uninter-esting). For problems where the predictions are obtainedfrom an RNN and frame-based similarity measures are in-sufﬁcient, we can use a vector quantised version of the ﬁnalhidden state as the key for the hash table entry.Algorithm 2Description of beam objects givenw,k,fhInitialise beam objectbeam.hashQ = dictionary of priority queues⇤beam.queue = indexed priority queue of lengthw⇤⇤Insertl, sinto beamkey=fh(s)queue = beam.queuehashQ = beam.hashQ[key]ﬁtsinqueue =notqueue.full()orl\u0000queue.min()ﬁtsinhashQ =nothashQ.full()orl\u0000hashQ.min()ifﬁtsinqueueandﬁtsinhashQthenhashQ.insert(l, s)ifhashQ.overfull()thenitem = hashQ.delmin()queue.remove(item)queue.insert(l, s)ifqueue.overfull()thenitem = queue.delmin()beam.hashQ[fh(item.s)].remove(item)⇤The dictionary maps hash keys to priority queues oflengthkwhich maintain (at most) the topkentries at alltimes.⇤⇤Anindexedpriority queue allows efﬁcient random ac-cess and deletion [1].5. EXPERIMENTS5.1 DatasetUnlike other approaches to chord estimation, our proposedapproach aims to learn the audio features, the acoustic modeland the language model from the training data. Therefore,maximum likelihood training of the acoustic and languagemodels requires sufﬁcient training data, depending on thecomplexity of the chosen models. Additionally, we requirethe raw audio for all the examples in the dataset in orderto train the acoustic model which operates on CQTs ex-tracted from the audio. In order to satisfy these constraints,we use the dataset used for the MIREX Audio Chord Es-timation Challenge. The MIREX data is comprised of twodatasets. The ﬁrst dataset is the collected Beatles, Queenand Zweieck datasets1. The second dataset is an abridgedversion of the Billboard dataset [7].The Beatles, Queen and Zweieck dataset contains anno-tations for217tracks and the Billboard dataset contains an-notations for740unique tracks. The corresponding audiofor the provided annotations are not publicly available and1http://www.isophonics.net/130 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015we had to acquire the audio independently. We were ableto collect all the audio for the Beatles, Queen and Zweieckdataset and650out of the740unique tracks for the Bill-board dataset (see footnote2for details), leading to a totalof867tracks for training and testing3. Although we arenot able to directly compare results with MIREX evalua-tions due to the missing tracks, we show that the trainingdata is sufﬁcient for estimating models of sufﬁcient accu-racy and the results are comparable to the top performingentries submitted to MIREX 2014. Keeping in mind thelimited number of examples in the dataset, all the groundtruth chord annotations were mapped to the major/minorchord dictionary which is comprises of 12 major chords,12 minor chords and oneno chordclass. All results are re-ported on 4-fold cross-validation experiments on the entiredataset. For training the acoustic and language models, thetrainingdata was further divided into a training (80%) andvalidation split (20%).5.2 Acoustic Model TrainingThe features obtained from the DNN feature extractionstage (Section 2) are input to an acoustic model which pro-vides a posterior probability over chord labels,P(zt|xt)given an input feature vector. Similar to the feature extrac-tion, we use DNNs with a softmax output layer to modelthe probabilities of output chord classes. We train modelswith 3 hidden layers with varying number of hidden units.The acoustic models are trained on a frame-wise basis, in-dependently of the language models. We use stochasticmini-batch gradient descent with ADADELTA for estimat-ing the DNN parameters. We use a constant Dropout rateof0.3on all the DNN layers to reduce overﬁtting. Dropoutwas found to be essential for good generalisation perfor-mance, yielding an absolute performance improvement ofup to4%on the test set. We used a mini-batch size of 100and early stopping for training. Training was stopped ifthe log-likelihood of the validation set did not increase for20 iterations over the entire training set. Unlike the featureextraction stage, we do not discard any of the trained mod-els. Instead of using only the best performing model on thevalidation set, we average the predictions of all the trainedmodels to form anensemble of DNNs[8] as the acousticmodel. We found that simply averaging the predictions ofthe acoustic classiﬁers led to an absolute improvement ofup to3%on frame classiﬁcation accuracies.5.3 Language Model TrainingAs outlined in Section 3, we use RNNs with LSTM unitsfor language modelling. The training data for the languagemodels is obtained by sampling the ground truth chordtranscriptions at the same frame-rate at which CQTs areextracted from the audio waveforms. We use RNNs with 2layers of hidden recurrent units (100LSTM units each) andan output softmax layer. Each training sequence was fur-ther divided into sub-sequences of length100. The RNNs2www.eecs.qmul.ac.uk/⇠sss313AUDFPRINT was used to align the audio to the corresponding an-notations: http://labrosa.ee.columbia.edu/matlab/audfprint/Language ModelNoneLSTM RNNAcoustic ModelOR WAOROR WAORDNN-CQT57.0% 56.5%62.8% 62.0%DNN-DNN Feats69.8% 69.1%73.4% 73.0%DNN-CW DNN Feats72.9% 72.5%75.5% 75.0%Table 1. 4-fold cross-validation results on the MIREXdataset for the major/minor prediction task. DNN-CQTrefers to CQT inputs to a DNN acoustic model. DNN-DNN Feats refers to DNN feature inputs to the DNNacoustic model. DNN-CW DNN Feats refers to DNN fea-tures with a context window as input to the acoustic model.were trained with stochastic gradient descent on individ-ual sub-sequences, without any mini-batching. Unlike theacoustic models, we observed that ADADELTA did notperform very well for RNN training. Instead, we used aninitial learning rate of0.001that was linearly decreasedto0over1000training iterations. We also found that aconstant momentum rate of0.9helped training convergefaster and yielded better results on the test set. We usedearly stopping and training was stopped if validation log-likelihood did not increase after 20 epochs. We used gra-dient clipping when the norm of the gradients was greaterthan50to avoid gradient explosion in the early stages oftraining.6. RESULTSIn Table 1, we present 4-fold cross validation results on thecombined MIREX dataset at the major/minor chord level.The metrics used for evaluation are the overlap ratio (OR)and the weighted average overlap ratio (WAOR) which arecommonly used for evaluating chord recognition systems(including MIREX). The test data is sampled every10mssimilar to the MIREX evaluations. The outputs of the hy-brid model were decoded with the proposed hashed beamsearch algorithm. A grid search was performed over thedecoding parameters and the presented results correspondto the parameters that were determined to be optimal overthe training set.From Table 1, it is clear that the hybrid model improvesperformance over the acoustic-only models. The resultsshow that the performance of the acoustic model is greatlyimproved when the input features to the model are learnt bya DNN as opposed to CQT inputs. The performance of theacoustic model is further improved (3%absolute improve-ment) when mean and variance pooling is performed overa context window of DNN features. It is interesting to notethat the relative improvement in performance is highest forthe DNN-CQT and DNN-DNN Feats conﬁgurations. Thisis due to the fact that the hybrid model is derived with theexplicit assumption that given a statezt, the acoustic framextis conditionally independent of all state and acousticvectors occurring at all other times. Applying a contextwindow to the features violates this independence assump-tion and therefore the relative improvement is diminished.However, the improved performance of the acoustic modelProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 131Figure 2. Effect of varying beam width on OR on MIREXdata.n=2,k=1due to context windowing offsets this loss. Although for-mal comparisons cannot be made, the accuracies achievedwith the hybrid model are similar to the best performingmodel submitted to the 2014 MIREX evaluation4on theBillboard dataset (OR =75.57%).To investigate the advantage of the proposed hashed beamsearch algorithm, we plot the overlap ratio against the beamwidth. Figure 2 illustrates that the proposed algorithm canachieve marginally better decoding performance at a sig-niﬁcant reduction in beam size. As an example, the hashedbeam search yields an OR of75.1%with a beam width of5, while regular beam search yields74.7%accuracy with abeam width of1000. The time taken to run the hash beamsearch (w=5) over the test set was5minutes, as com-pared to the regular beam algorithm (w= 1000) whichtook17hours to decode the test set. The algorithm’s abil-ity to yield good performance at signiﬁcantly smaller beamwidths indicates that it performs efﬁcient pruning of simi-lar paths, thus utilising the available beam width more ef-ﬁciently. The run-times of the algorithm show that it canbe used for real-time applications without compromisingrecognition accuracy.In addition to the beam width, the hash beam search al-gorithm allows the user to specify the similarity metric andthe number of solutions for each hash table entry. We in-vestigate the effect of these parameters on the OR and plotthe results in Figure 3. We let the similarity metric be thepreviousnframes and observe performance asnis linearlyincreased for a ﬁxed beam width of25. From Figure3weobserve that the performance is quite robust to changes inthe number of past frames for small values ofn. One pos-sible explanation for the graph is that since the test data issampled at a frame rate of 10ms, all occurrences of chordslast for several frames. Therefore counting the previousnframes, effectively leads to the same metric each time. Weexperimented with using the previousnuniqueframes asa metric but found that the results deteriorated quite dras-tically asnwas increased. This might reﬂect the limited4www.music-ir.org/mirex/wiki/2014:AudioChordEstimationResults\nFigure 3. Effect of varying hashed beam search parame-tersfh,kon OR on MIREX dataset.w= 25.memory of RNN language models and the issues causeddue to lack of explicit duration modelling. The blue linein Figure 3 illustrates the effect of varying the number ofsolutions per hash table entry. From this graph we see thatperformance deteriorates signiﬁcantly once the number ofentries per bin crosses a certain threshold (⇠5). This isdue to the fact that maintaining many solutions of the samekind saturates the beam capacity with very similar solu-tions, limiting the breadth of search.7. CONCLUSION AND FUTURE WORKWe present a chord estimation system based on a hybridrecurrent neural network and the results are competitivewith existing state-of-the-art approaches. We show thatDNNs are powerful acoustic models. By learning features,they eliminate the need for complex feature engineering.The hybrid RNN model allows us to superimpose an RNNlanguage model on the acoustic model predictions. Ad-ditionally, language models can be trained on chord datafrom the web without the corresponding audio. The re-sults clearly indicate that the language model helps im-prove model performance by modelling the temporal re-lationships between output chord symbols and reﬁning thepredictions of the acoustic model. The proposed variant ofthe beam search algorithm signiﬁcantly reduces memoryusage and run times, making the model suitable for real-time applications.In the future, we would like to conduct chord recog-nition experiments on larger datasets. This is because themodelling and generalisation capabilities of neural networksimprove with more available data for training. An impor-tant issue that remains with respect to RNN language mod-els is the problem of duration modelling. Although RNNsare very good at modelling the transition probabilities be-tween events, durations of each event are not modeled ex-plicitly. For musical applications like chord recognitionand music transcription, accurate estimates for durationsof note occurrences can further help improve the effective-ness of RNN based language models.132 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]https://pypi.python.org/pypi/pqdict/.[2]Yoshua Bengio, Patrice Simard, and Paolo Frasconi.Learning long-term dependencies with gradient de-scent is difﬁcult.IEEE Transactions on Neural Net-works, 5(2):157–166, 1994.[3]Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent. Audio Chord Recognition with Re-current Neural Networks. InThe International Societyfor Music Information Retrieval Conference (ISMIR),pages 335–340, 2013.[4]Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent. High-dimensional sequence transduc-tion. InIEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3178–3182. IEEE, 2013.[5]Nicolas Boulanger-Lewandowski, Jasha Droppo, MikeSeltzer, and Dong Yu. Phone sequence modeling withrecurrent neural networks. InIEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), pages 5417–5421. IEEE, 2014.[6]Judith C Brown. Calculation of a constant Q spectraltransform.The Journal of the Acoustical Society ofAmerica, 89(1):425–434, 1991.[7]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-jinaga. An Expert Ground Truth Set for Audio ChordRecognition and Music Analysis. InThe InternationalSociety for Music Information Retrieval Conference(ISMIR), pages 633–638, 2011.[8]Thomas G Dietterich. Ensemble methods in machinelearning. InMultiple Classiﬁer Systems, pages 1–15.Springer, 2000.[9]Alex Graves. Sequence transduction with recurrentneural networks. InRepresentation Learning Work-shop, Internation Conference on Machine Learning(ICML), 2012.[10]Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,Bas R Steunebrink, and J¨urgen Schmidhuber.LSTM: A Search Space Odyssey.arXiv preprintarXiv:1503.04069, 2015.[11]Eric J Humphrey and Juan Pablo Bello. Rethinkingautomatic chord recognition with convolutional neu-ral networks. In11th International Conference on Ma-chine Learning and Applications (ICMLA), volume 2,pages 357–362. IEEE, 2012.[12]Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.Deep learning.Nature, 521(7553):436–444, 2015.[13]Matt McVicar, Ra´ul Santos-Rodr´ıguez, Yizhao Ni, andTijl De Bie. Automatic chord estimation from audio:A review of the state of the art.IEEE/ACM Trans-actions on Audio, Speech, and Language Processing,22(2):556–575, 2014.[14]S. Sigtia, E. Benetos, N. Boulanger-Lewandowski,T. Weyde, Artur S. d’Avila Garcez, and S. Dixon.A Hybrid Recurrent Neural Network for Music Tran-scription. InIEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP), pages2061–2065, Brisbane, Australia, April 2015.[15]Siddharth Sigtia and Simon Dixon. Improved musicfeature learning with deep neural networks. InIEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 6959–6963. IEEE,2014.[16]Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:A simple way to prevent neural networks from over-ﬁtting.The Journal of Machine Learning Research,15(1):1929–1958, 2014.[17]Clifford Stein, T Cormen, R Rivest, and C Leiserson.Introduction to algorithms, volume 3. MIT Press Cam-bridge, MA, 2001.[18]Matthew D Zeiler. ADADELTA: An adaptive learningrate method.arXiv preprint arXiv:1212.5701, 2012.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 133"
    },
    {
        "title": "Music Shapelets for Fast Cover Song Recognition.",
        "author": [
            "Diego Furtado Silva",
            "Vinícius M. A. de Souza",
            "Gustavo E. A. P. A. Batista"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416236",
        "url": "https://doi.org/10.5281/zenodo.1416236",
        "ee": "https://zenodo.org/records/1416236/files/SilvaSB15.pdf",
        "abstract": "A cover song is a new performance or recording of a pre- viously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new ver- sions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, tim- bre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve po- tential cover versions for a query recording. In this pa- per, we adapt the idea of time series shapelets for content- based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best de- scribe each song. We demonstrate that we can use such small segments to identify cover songs with higher identi- fication rates and more than one order of magnitude faster than methods that use features to describe the whole music.",
        "zenodo_id": 1416236,
        "dblp_key": "conf/ismir/SilvaSB15",
        "keywords": [
            "cover song",
            "automatic identification",
            "content-based music retrieval",
            "time series shapelets",
            "similarity search",
            "key differences",
            "tempo",
            "structure",
            "language",
            "identification rates"
        ],
        "content": "MUSIC SHAPELETS FOR FAST COVER SONG RECOGNITIONDiego F. Silva Vin´ıcius M. A. Souza Gustavo E. A. P. A. BatistaInstituto de Ciˆencias Matem´aticas e de Computac ¸˜ao – Universidade de S˜ao Paulo{diegofsilva,vsouza,gbatista}@icmc.usp.brABSTRACTA cover song is a new performance or recording of a pre-viously recorded music by an artist other than the originalone. The automatic identiﬁcation of cover songs is usefulfor a wide range of tasks, from fans looking for new ver-sions of their favorite songs to organizations involved inlicensing copyrighted songs. This is a difﬁcult task giventhat a cover may differ from the original song in key, tim-bre, tempo, structure, arrangement and even language ofthe vocals. Cover song identiﬁcation has attracted someattention recently. However, most of the state-of-the-artapproaches are based on similarity search, which involvesa large number of similarity computations to retrieve po-tential cover versions for a query recording. In this pa-per, we adapt the idea of time series shapelets for content-based music retrieval. Our proposal adds a training phasethat ﬁnds small excerpts of feature vectors that best de-scribe each song. We demonstrate that we can use suchsmall segments to identify cover songs with higher identi-ﬁcation rates and more than one order of magnitude fasterthan methods that use features to describe the whole music.1. INTRODUCTIONRecording or live performing songs previously recorded byother composers are typical ways found by several early-career and independent musicians to publicize their work.Established artists also play versions composed by othermusicians as a way to honor their idols or friends, amongother reasons. These versions of an original compositionare popularly calledcover songs.The identiﬁcation of cover songs has different uses. Forinstance, it can be used for estimating the popularity of anartist or composition, since a highly covered song or artistis an indicative of the popularity/quality of the compositionor the author’s prestige in the musical world. In a differentscenario, a search engine for cover songs can help musicconsumers to identify different versions of their favoritesongs played by other artists in different music styles orlanguage.c\u0000Diego F. Silva, Vin´ıcius M. A. Souza, Gustavo E. A. P.A. Batista. Licensed under a Creative Commons Attribution 4.0 Interna-tional License (CC BY 4.0).Attribution:Diego F. Silva, Vin´ıcius M. A.Souza, Gustavo E. A. P. A. Batista. “Music Shapelets for Fast Cover SongRecognition”, 16th International Society for Music Information RetrievalConference, 2015.Musicians that upload cover versions to websites suchasYouTube, Last.fmorSoundCloudfrequently neglect thatthe original songs may be copyright-protected. Copyrightis a legal right created by the law that grants the creator ofan original work (temporary) exclusive rights to its use anddistribution. Legally speaking, when an interpreter doesnot possess a license to distribute his/her recording, thisversion is considered illegal.For these reasons, cover song recognition algorithmsare essential in different practical applications. However,as noted by [12], the automatic identiﬁcation of coversongs is a difﬁcult task given that a cover may differ fromthe original song in key, timbre, tempo, structure, arrange-ment and language of the vocals.Another difﬁculty faced by automatic cover song iden-tiﬁcation systems, particularly those based on expensivesimilarity comparisons, is the time spent to retrieve record-ings that are potential covers. For instance, websites suchasYouTubehave 300 hours of video (and audio) uploadedevery minute1. A signiﬁcant amount of these videos is re-lated to music content. Therefore, cover song identiﬁcationalgorithms have to be efﬁcient in terms of query processingtime in order to handle such massive amounts of data.This paper proposes a novel algorithm to efﬁciently re-trieve cover songs based on small but representative ex-cerpts of music. Our main hypothesis is that we can char-acterize a speciﬁc music with small segments and use suchinformation to search for cover songs without the need tocheck the whole songs.Our hypothesis is supported by the success of a sim-ilar technique used in time series classiﬁcation, namedshapelets[16]. Informally, shapelets are time series sub-sequences, which are in some sense maximally represen-tative of a class. For time series, shapelets provide in-terpretable and accurate results and are signiﬁcantly fasterthan existing approaches.In this paper, we adapt the general idea of shapelets forcontent-based music retrieval. For this, we evaluate severaldifferent ways to adapt the original idea to music signals.In summary, the main contributions of our proposal are:•Our method adds a training phase to the task ofcontent-based music information retrieval, whichseeks to ﬁnd small excerpts of feature vectors thatbest describe each signal. In this way, we make thesimilarity search faster;1www.youtube.com/yt/press/statistics.html.441•Even with small segments, we demonstrate thatwe can improve the identiﬁcation rates obtained bymethods that use features to describe the whole mu-sic;•We show how to use our proposal along with a spe-ciﬁc retrieval system. However, we note that ourmethod can be added to any algorithm based on asimilar sequence of steps, even methods to furtherspeed-up the query. To do this, we simply need toapply such an algorithm on the shapelets, instead ofthe complete features vectors.2. BACKGROUND AND RELATED WORKThe task of cover song recognition can be described as thefollowing: given a set,S, of music recordings and a querymusic,q, we aim to identify ifqis a version of one of thesongs inS. Thus, a cover song recognition system can beconsidered a querying and retrieval system.The state-of-the-art querying and retrieval systems canbe divided into ﬁve main blocks [12]:i)feature extraction;ii)key invariance;iii)tempo invariance;iv)structure in-variance; andv)distance calculation. Figure 1 illustratesthese steps. This general framework leaves open whichmethod will be applied in each step.\nFeature extractionTe m p o  invarianceDistance calculationKey invarianceStructure invariance\nFigure 1. General retrieval system blocks. The feature ex-traction and distance calculation are required and shouldappear in this order. The other ones may provide best re-sults, but are optionalFeature extraction is a change of representation fromthe high-dimensional raw signal to a more informativeand lower-dimensional set of features. Chroma-features orpitch class proﬁles (PCP) are among the most used featuresfor computing music similarity. These features are a rep-resentation of the spectral energy in the frequency range ofeach one of the twelve semitones. A good review of PCP,as well as other chroma-based features, can be found in [7].Transpose a music for another key or main tonality is acommonly used practice to adapt the song to a singer or tomake it heavier or lighter. Key invariance tries to reducethe effects of these changes in music retrieval systems thatuse tonal information. A simple and effective method toprovide robustness to key changes is the optimal transposi-tion index (OTI) [11]. As a ﬁrst step, this method computesa vector of harmonic pitch class proﬁles (HPCP) for eachsong, which is the normalized mean value of the energyin each semitone [5]. When comparing two songsAandB, the method ﬁxes the HPCP ofA. For each shift of theHPCP ofB, it measures the inner product between the twovectors. The shift that maximizes this product is chosenand the songBis transposed using such a shift value.Tempo invariance is the robustness to changes betweendifferent versions caused by faster or slower performances.One way of achieving tempo invariance is by modifyingthe feature extraction phase to extract one or more featurevectors per beat [4], instead of a time-based window. An-other possibility is the use of speciﬁc feature sets, such aschroma energy normalized statistics (CENS) [8]. Thesefeatures use a second stage in the chroma vector estimationthat provides a higher robustness to local tempo variations.Structure invariance is the robustness to deviations inlong-term structure, such as repeated chorus or skippedverses. This invariance may be achieved by several dif-ferent approaches, such as dynamic programming-basedalgorithms [3], sequential windowing [13] or by summa-rizing the music pieces into their most repeated parts [9].The last step of a querying and retrieval system is thesimilarity computation between the query and referencedata by means of a distance calculation. The most com-mon approaches for this task are dynamic programmingbased algorithms that try to ﬁnd an optimal alignment offeature vectors. A well-known example of this approach isthe Dynamic Time Warping (DTW) distance function.In this paper, we present an approach that adds a train-ing phase to this process. This step seeks to ﬁnd the mostsigniﬁcant excerpt of each song in the setS(training set).These small segments are used in a comparison with thequery songq. Our method is inspired by the idea of timeseries shapelets, presented next.3. SHAPELETSTime series shapelets is a well-known approach for timeseries classiﬁcation [16]. In classiﬁcation, there exists atraining set of labeled instances,S. A typical learningsystem uses the information inSto create a classiﬁcationmodel, in a step known as training phase. When a new in-stance is available, the classiﬁcation algorithm associatesit to one of the classes inS.A time series shapelet may be informally deﬁned as thesubsequence that is the most representative of a class. Theoriginal algorithm of [16] ﬁnds a set of shapelets and usethem to construct a decision tree classiﬁcation model. Thetraining phase of such learning system consists of three ba-sic steps:•Generate candidates:this step consists in extract-ing all subsequences from each training time series;•Candidates’ quality assessment:this step assessesthe quality of each subsequence candidate consider-ing its class separability;•Classiﬁcation model generation:this step inducesa decision tree. The decision in each node is basedon the distance between the query time series and ashapelet associated to that node.442 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015In the ﬁrst step, the length of the candidates is an in-trinsic parameter of the candidates generation. The origi-nal algorithm limits the search to a range between a min-imum (minlen) and maximum (maxlen) length. All thesubsequences with length betweenminlenandmaxlenarestored as candidates.Given a candidates, we need to measure the distancebetweensand a whole time seriesx. Notice that a directcomparison between them is not always possible sincesandxcan have very different lengths. Considerlas thecandidate’s length. Thedistance(s, x)is deﬁned as thesmallest Euclidean distance between the candidatesandeach subsequence ofxwithlobservations.The next steps of the shapelet algorithm are directly re-lated to the classiﬁcation task. Since this is not our focus,we suppress details of the algorithm from this point.The general idea of classifying time series by shapeletsis to use the distances between candidates and training timeseries to construct a classiﬁcation model. First, the algo-rithm estimates the best information gain (IG) that can beobtained by each candidate. This is made by grouping thetraining examples that are closer – according a distancethreshold – from the training examples that are more dis-tant from the candidate. The best value for the threshold –calledbest split point– is deﬁned by assessing the separa-tion obtained by different values.Finally, the algorithm uses the IG to create a decisiontree. A decision node uses the information of the bestshapelet candidate. In order to decide the class of a testexample, we measure the distance between the query andthe shapelet. If the distance is smaller or equal to the splitpoint, its class is the one associated with the shapelet. Oth-erwise, the query is labeled as belonging to the other class.For details on how to ﬁnd the optimal split point and thedecision tree’s construction, we refer the reader to [16].4. OUR PROPOSAL: MUSIC SHAPELETSIn this paper, we propose to adapt the idea of shapelets fora fast content-based music retrieval, more speciﬁcally forcover songs identiﬁcation. Our adaptations are detailed inthe next sections.4.1 WindowingThe original approach to ﬁnding subsequence candidatesuses sliding windows with different lengths. These lengthsare the enumeration of all values in a range provided bythe user. The sliding window swipes across the entire timeseries and such a process is performed for each examplein the training set. We found this process to be very timeconsuming, accounting for most of the time spent in thetraining phase.We note that music datasets are typically higher-dimensional than most time series benchmark datasets, inboth number of objects as well as number of observations.Thus, we use a reduced set of speciﬁc values as windowlength instead of an interval of values. We empiricallynoted that it is possible to ﬁnd good candidates withoutenumerating all the lengths in a given range.In addition, the original approach uses a sliding win-dow that starts at every single observation of a time series.We slightly modiﬁed it so that the sliding windows skip acertain amount of observations proportional to the windowlength. This windowing technique with partial overlappingis common in audio analysis.4.2 Dynamic Time WarpingShapelets use Euclidean distance (ED) as the similaritymeasure to compare a shapelet and a whole time series.However, ED is sensitive to local distortions in the timeaxis, calledwarping. Warping invariance is usually bene-ﬁcial for music similarity due to the differences in tempoor rhythm that can occur when a song is played live or bydifferent artists.In order to investigate this assumption, we evaluate theuse of ED and Dynamic Time Warping (DTW) to com-pare shapelets extracted from music data. There is an ob-vious problem with the use of DTW, related to its com-plexity. While ED is linear on the number of observations,DTW has a quadratic complexity. Nevertheless, there is aplethora of methods that can be used so that we may ac-celerate the calculation of the distance between a shapeletand a whole music [10].4.3 Distance–based Shapelet QualityShapelets were originally proposed for time series classi-ﬁcation. In cover song identiﬁcation we are interested inproviding a ranking of recordings considering the similar-ity to a query. Therefore, IG is not the best choice to mea-sure the candidates’ quality.IG in shapelet context ﬁnds the best split points and can-didates according to class separability. However, music re-trieval problems typically have a large number of classes(each class representing a single song) with few examples(different recordings of a certain song), hindering the anal-ysis of class separability.For this reason, we propose and evaluate the substitu-tion of the IG by a distance-based criterion. We considerthat a good candidate has a small distance value to all theversions of the related song and a high distance value toany recording of another song. Thus, we propose the crite-rion DistDiff, deﬁned in Equation 1.DistDiff(s)= m i ni=1..n(distance(s, OtherClass(i)))\u00001mmXi=1distance(s, SameClass(i))(1)wheresis a candidate for shapelet,SameClassis the setofmversions of the song from were the candidate comefrom,OtherClassis the set ofnrecordings that does notrepresent a version of the same composition than the originofsanddistance(s, Set(i))is the distance between theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 443candidate and thei-th recording inSet(SameClassorOtherClass).Clearly, we are interested in candidates that provide ahigh value to the ﬁrst term and a small value to the second.So, as higher the value of DistDiff, higher the quality of thecandidate. In case of draw, we use the minimum averagerank of the versions of the song related tosas tie breaking.In other words, if two candidates have the same value ofDistDiff, the best candidate is the one that provides thebest average ranking positions for the versions of the songfrom wherescomes from.4.4 SimilaritySince the technique of time series shapelets is interested inclass separability, it stores at most one shapelet per class.On the other hand, in our problem we are interested in allexamples of each “class label”. So, we store one shapeletper recording in the training set, instead one for each com-position.The ﬁnal step, the querying and retrieval itself, is madein two simple steps. First, our method measures the dis-tance between the query music and each of the shapeletsfound in the training phase. Finally, the ranking is givenby sorting these distances in ascending order.4.5 TripletsIn a real scenario where the task of music retrieval will beperformed, it is highly probable that a speciﬁc song has oneto three authorized versions such as the original recordingin a studio, an acoustic and a live version. Obviously, thereare exceptions such as remix and many versions of liveperformances. Thus, when we extract shapelets from thesesongs in a conventional way, we have only a few instancesfor each class in the training set. This may hamper thecandidate’s quality calculation.In addition, only a small segment of a song can be unin-formative. This fact has been observed in other applicationdomains. For instance, [14] uses features from the begin-ning, the middle and the end of each recording to performthe genre recognition task.For these reasons, we also evaluated the idea of repre-senting each recording as three shapelets. Figure 2 illus-trate this procedure. The ﬁrst step of this procedure di-vides the feature vector into three parts of the same length.After that, we ﬁnd the most representative subsequence ofeach segment. Finally, during the retrieval phase, we usethe mean distance from a query recording to each of thethree shapelets. We will refer to these triple of shapelets astriplets.5. EXPERIMENTAL EVALUATIONIn this section, we present the datasets used in our evalua-tion and the experimental results. We conclude this sectiondiscussing the advantages of our method in terms of timecomplexity in the retrieval phase.\n......\nMusicsignalsChromavectorsCandidatesgenerationQualityassessment\nSets ofcandidatesTripletsFigure 2. General procedure to generate triplets5.1 DatasetsWe evaluate our proposal in two datasets with different mu-sic styles. The ﬁrst dataset is composed by classical musicwhile the second contains popular songs.The dataset123 Classicalwas originally used in [1].This dataset has 123 different recordings concerning 19compositions from Classical (between 1730 and 1820) andRomantic (between 1780 and 1910) ages. From the 123recordings, 67 were performed by orchestras and the re-maining 56 were played in piano.We also collected popular songs from videos ofYouTubeand built a dataset namedYouTube Covers.W emade theYouTube Coversdataset freely available in ourwebsite [15] for interested researchers. This dataset wasbuilt with the goal of evaluating our proposal in a morediverse data since the covers songs in the123 Classicaldataset in general faithfully resembling their original ver-sions.TheYouTube Coversdataset has 50 original songs fromdifferent music genres such asreggae, jazz, rockandpopmusicaccompanied of cover versions. In our experiments,we divide this dataset in training and test data. The train-ing data have the original recording in studio and a liveversion for each music. In the test data, each music has 5different cover versions that include versions of differentmusic styles, acoustic versions, live performances of es-tablished artists, fan videos, etc. Thus, this dataset have atotal of 350 songs (100 examples for training and 250 fortest). A complete description ofYouTube Coversdataset isavailable in our website.As the123 Classicaldataset doesn’t have a natural divi-sion in training/test sets and has a reduced amount of data,we conducted our experimental evaluation in this datasetusing stratiﬁed random sampling with1/3of data to train-ing and the remaining for test. With this procedure, thenumber of examples per class in the training phase variesfrom 1 to 5.5.2 Evaluation ScenariosIn this paper, we consider two different scenarios to evalu-ate our method:i)test set as queryandii)training set as444 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015query. In both, the ﬁrst stage ﬁnds shapelets in the trainingpartition.In the ﬁrst scenario, we perform a query when a newsong arrives. This setting simulates the scenario in whichwe would like to know if the (test) query is a cover of somepreviously labeled song. In other words, we use the unla-beled recordings to ﬁnd similar labeled ones.In the second scenario, we simulate the scenario inwhich the author of one of the training songs wants toknow if there are uncertiﬁed versions of his/her music inthe repository. Thus, we should use his/her original record-ing as query. Therefore, the training instances are used asqueries and we use the shapelets to return unlabeled songsthat are potentially covers.5.3 Experimental SetupIn order to evaluate the proposed method, we compare itsresults against two competitors. The ﬁrst one is the DTWalignment of the feature vector representing the whole mu-sic. The second one uses a music summarization algorithmto ﬁnd signiﬁcant segments of the recordings. For this, weuse a method that considers that the most signiﬁcant ex-cerpts of music are those that are most repeated [2]. Afterﬁnding such excerpts, the similarity search occurs as pro-posed in this paper.As feature sets, we used the chroma energy normalizedstatistics (CENS), as well as chroma extracted together thebeat estimating. In general, CENS results are slightly bet-ter. Thus, we focus our evaluation using this feature. Toextract the CENS, we used the Matlab implementation pro-vided by the Chroma Toolbox [7] with the default parame-ters settings.We used the optimal transposition index (OTI) tech-nique to improve robustness for key variances. Shapeletsare not used to decide the shift to provide such an invari-ance. This is done by using the harmonic pitch class pro-ﬁles (HPCP) of the complete chroma vector.Our proposal have two parameters related to the win-dowing:i)window length andii)overlapping proportionof consecutive windows. For the ﬁrst parameter, we use thevalues25,50and75for shapelets and25for triplets. Forthe second parameter, we use2/3of the window length asoverlapping proportionTo provide an intuition to the reader about the ﬁrst pa-rameter. The mean length of the chroma feature vectorsin the datasets123 ClassicalandYouTube Coversare215and527, respectively. Therefore, a window length of25represents approximately11%and5%, respectively, of theaverage length of the recordings in these datasets.5.4 Evaluation MeasuresIn order to assess the quality of our proposal, we used threeevaluation measures adopted by MIREX2for the coversong identiﬁcation task. Such measures take into accountthe position of the relevant songs in the estimated rankingof similarity.2http://www.music-ir.org/mirex/wiki/2015:Audio_Cover_Song_IdentificationGiven a set ofnquery songs, a retrieval method returnsa rankri(i=1,2,...,n) for each of them. The func-tion⌦(ri,j)returns the value1if thej\u0000th-ranked songobtained for thei\u0000thquery is a relevant song or0other-wise. In the context of this work, a relevant song is a coverversion of the query recording.The ﬁrst evaluation measure represents the mean num-ber of relevant songs retrieved among the top ten positionsof the ranking (MNTop10). Formally, the MNTop10 is de-ﬁned according to Equation 2.MNTop10 =1nnXi=110Xj=1⌦(ri,j)(2)The mean average precision (MAP) is the mean valueof the average precision (AP) for each query song. The APis deﬁned in Equation 3.AP(ri)=1nnXj=1\"⌦(ri,j) 1jjXk=1⌦(ri,k)!#(3)Finally, we also use the mean rank of ﬁrst correctlyidentiﬁed cover (MFRank). In other words, this measureestimates, on average, the number of songs we need to ex-amine in order to ﬁnd a relevant one. The MFRank is de-ﬁned by Equation 4.MFRank=1nnXi=1fp(ri)(4)wherefp(ri)is a function that returns the ﬁrst occurrenceof a relevant object in the rankingri.For the ﬁrst two measures, larger values represent bet-ter performance. For the last one the smaller values areindicative of superiority.5.5 ResultsIn the Section 4, we proposed several adaptations to theoriginal shapelets approach to the music retrieval setting.Unfortunately, due to lack of space, we are not able to showdetailed results for all combinations of these techniques. Intotal, we have 16 different combinations of techniques. Allthose results are available on the website created for thiswork [15].In this section, we present a subset of the results accord-ing to the following criteria:•OTI. We show all results with OTI as key invariancemethod. For the datasetYouTube Covers, the use ofOTI led to signiﬁcant improvements. For the123Classicaldataset, OTI performed quite similarly tothe same method without OTI. This may occur be-cause the problem of key variations is more evidentin the pop music. We notice we used the simplestversion of OTI, that assesses just one tonal shift.•Shapelet evaluation. We evaluate all results withDistDiff. In most cases, information gain performedProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 445worst than DistDiff. Even more, there are caseswhere the use of IG causes a signiﬁcant perfor-mance deterioration. For example, when using a sin-gle shapelet per recording onYouTube Covers, themethod using information gain achieved MNTop10=0.75, MAP =25.29%and MFRank =17.52. Bychanging this measure by the DistDiff criterion, pro-posed in this paper, the results become MNTop10 =1.22, MAP =47.14%and MFRank =9.72.•Triplet. We show the results using triplets. In gen-eral the use of a single shapelet to describe the train-ing songs did not outperform the use of triplets. Al-though obtain an improvement in isolated cases, thedifferences are small in these cases.Therefore, we will ﬁx our analysis to the methods thatuse OTI and triplets evaluated by DistDiff criterion. Thelast remaining decision concerns the use of Euclidean orDTW distances. We show the results obtained with both.Table 1 shows the results obtained on123 Classicaldataset and Table 2 shows the results obtained onYouTubeCoversdataset.Table 1. Results achieved on the dataset123 ClassicalScenario 1 - Test set as queryMNTop10 MAP (%) MFRankDTW 2.34 97.24 1.12Summarization 2.27 93.461.00Triplets-DTW2.3997.24 1.02Triplets-ED 2.3898.05 1.00Scenario 2 - Training set as queryMNTop10 MAP (%) MFRankDTW 4.73 98.921.00Summarization 4.44 91.52 1.02Triplets-DTW4.78 99.41 1.00Triplets-ED 4.71 97.921.00Table 2. Results achieved on the datasetYouTube CoversScenario 1 - Test set as queryMNTop10 MAP (%) MFRankDTW 1.14 42.49 11.69Summarization 0.85 32.11 13.82Triplets-DTW1.2945.558.45Triplets-ED 1.2647.808.49Scenario 2 - Training set as queryMNTop10 MAP (%) MFRankDTW 2.11 39.19 6.58Summarization 1.66 29.20 14.46Triplets-DTW 2.82 52.874.65Triplets-ED2.87 54.955.185.6 DiscussionThe results show that triplets outperformed similarity esti-mation by using music summarization and achieved equalor better results than the DTW matching of the whole fea-ture vector.More importantly, we notice that the querying usingshapelets is signiﬁcantly more efﬁcient than the matchingbetween the whole songs. Although our method requires atraining phase that is absent in similarity search with DTW,such a phase is performed only once.Letlandmbe the length of feature vectors of the queryand the labeled songs. The complexity to ﬁnd an alignmentbased on dynamic programming, such as DTW, isO(lm).Now, letsbe the size of each shapelet of the training song.The complexity to calculate the shapelet-based Euclideandistance between the query and the original song isO(ls),withs⌧m.Table 3 shows the time in seconds to perform the re-trieval step using Triplets-ED and DTW matching the en-tire feature vectors.Table 3. Total time (in seconds) to calculate the distancebetween all the queries (test set) and the training set byusing DTW and Triplets-EDDataset123 Classical YouTube CoversDTW 2,294 14,124Triplets-ED 148 928The result of this experiment shows that our method isabout15times faster to retrieve music by similarity. Weargue that our method may be further faster with the useof techniques to speed-up the similarity search – to ﬁndthe best match between the shapelet and the whole featurevector.The identiﬁcation rates were similar for both triplets ap-proaches, alternating the best results between them. Al-though the time spent to calculate Triplets-DTW is poten-tially lower than the obtained by a straightforward imple-mentation of Euclidean distance [10], the time spent by oursimple implementation is similar to the DTW alignment ofthe whole feature vector.6. CONCLUSIONIn this paper, we propose a novel technique to content-based music retrieval. Our method is naturally invariantto structure and open to aggregate invariance to key andtempo by the choice of appropriate methods, such as OTIand CENS as feature vector.We evaluated our method in a cover song recognitionscenario. We achieved better results than the widely ap-plied approach of DTW alignment and a similar approachbased on a well-known summarization algorithm. Ourmethod is also more than one order of magnitude fasterthan these methods.There are several possible extensions for this work.For instance, we can extend our idea to a shapelet-transform [6]. The evaluated scenario also suggests re-search on incremental learning of shapelets, the retrievalconsidering that novel songs may arrive, among othertasks. Finally, we intend to investigate how to improve thetime cost of DTW similarity search in order to make thetime of Triplets-DTW be competitive with Triplets-ED.7. ACKNOWLEDGMENTSThe authors would like to thank FAPESP by the grants#2011/17698-5, #2013/26151-5, and 2015/07628-0 andCNPq by grants 446330/2014-0 and 303083/2013-1.446 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]Juan Pablo Bello. Measuring structural similarity inmusic.IEEE Transactions on Audio, Speech, and Lan-guage Processing, 19(7):2013–2025, 2011.[2]Matthew L. Cooper and Jonathan Foote. Automaticmusic summarization via similarity analysis. InInter-national Society for Music Information Retrieval Con-ference, 2002.[3]Emanuele Di Buccio, Nicola Montecchio, and NicolaOrio. A scalable cover identiﬁcation engine. InInter-national Conference on Multimedia, pages 1143–1146,2010.[4]Daniel P. W. Ellis and Graham E. Poliner. Identifyingcover songs with chroma features and dynamic pro-gramming beat tracking. InIEEE International Con-ference on Acoustics, Speech and Signal Processing,volume 4, pages 1429–1432, 2007.[5]Emilia G´omez and Perfecto Herrera. Estimating thetonality of polyphonic audio ﬁles: Cognitive versusmachine learning modelling strategies. InInternationalSociety for Music Information Retrieval Conference,pages 92–95, 2004.[6]Jason Lines, Luke M. Davis, Jon Hills, and AnthonyBagnall. A shapelet transform for time series classiﬁ-cation. InACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 289–297, 2012.[7]Meinard M¨uller and Sebastian Ewert. Chroma Tool-box: MATLAB implementations for extracting vari-ants of chroma-based audio features. InInternationalSociety for Music Information Retrieval Conference,pages 1–6.[8]Meinard M¨uller, Frank Kurth, and Michael Clausen.Audio matching via chroma-based statistical features.InInternational Society for Music Information Re-trieval Conference, pages 288–295, 2005.[9]Bee Suan Ong.Structural Analysis and Segmentationof Music Signals. PhD thesis, 2007.[10]Thanawin Rakthanmanon, Bilson Campana, Abdul-lah Mueen, Gustavo Batista, Brandon Westover, QiangZhu, Jesin Zakaria, and Eamonn Keogh. Searching andmining trillions of time series subsequences under dy-namic time warping. InACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 262–270, 2012.[11]Joan Serra, Emilia G´omez, and Perfecto Herrera.Transposing chroma representations to a common key.InIEEE CS Conference on The Use of Symbols to Rep-resent Music and Multimedia Objects, pages 45–48,2008.[12]Joan Serr`a, Emilia G´omez, and Perfecto Herrera. Au-dio cover song identiﬁcation and similarity: back-ground, approaches, evaluation, and beyond. InAd-vances in Music Information Retrieval, pages 307–332.Springer, 2010.[13]Joan Serr`a, Xavier Serr`a, and Ralph G. Andrzejak.Cross recurrence quantiﬁcation for cover song identi-ﬁcation.New Journal of Physics, 11(9):093017, 2009.[14]Carlos Nascimento Silla Jr, Alessandro Lameiras Ko-erich, and Celso A. A. Kaestner. The latin musicdatabase. InInternational Society for Music Informa-tion Retrieval Conference, pages 451–456, 2008.[15]Diego F. Silva, Vin´ıcius M. A. Souza, and Gus-tavo E. A. P. A. Batista. Website for this work–https://sites.google.com/site/ismir2015shapelets/.[16]Lexiang Ye and Eamonn Keogh. Time series shapelets:a new primitive for data mining. InACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining, pages 947–956, 2009.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 447"
    },
    {
        "title": "Analysis of the Evolution of Research Groups and Topics in the ISMIR Conference.",
        "author": [
            "Mohamed Sordo",
            "Mitsunori Ogihara",
            "Stefan Wuchty"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416354",
        "url": "https://doi.org/10.5281/zenodo.1416354",
        "ee": "https://zenodo.org/records/1416354/files/SordoOW15.pdf",
        "abstract": "We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we first investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of re- searchers, allowing us to investigate their evolution and topic dependence. Notably, we find that large groups last longer if they actively alter their membership. Further- more, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their mem- bership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage.",
        "zenodo_id": 1416354,
        "dblp_key": "conf/ismir/SordoOW15",
        "keywords": [
            "co-authorship network",
            "topological changes",
            "topics over time",
            "research groups",
            "evolution",
            "topic dependence",
            "active alteration",
            "membership",
            "research impact",
            "citation advantage"
        ],
        "content": "ANALYSIS OF THE EVOLUTION OF RESEARCH GROUPS AND TOPICSIN THE ISMIR CONFERENCEMohamed SordoCenter for Computational ScienceUniversity of Miamimsordo@miami.eduMitsunori OgiharaDept. of Computer ScienceUniversity of Miamiogihara@cs.miami.eduStefan WuchtyDept. of Computer ScienceUniversity of Miamiwuchtys@cs.miami.eduABSTRACTWe present an analysis of the topics and research groupsthat participated in the ISMIR conference over the last 15years, based on its proceedings. While we ﬁrst investigatethe topological changes of the co-authorship network aswell as topics over time, we also identify groups of re-searchers, allowing us to investigate their evolution andtopic dependence. Notably, we ﬁnd that large groups lastlonger if they actively alter their membership. Further-more, such groups tend to cover a wider selection of topics,suggesting that a change of members as well as of researchtopics increases their adaptability. In turn, smaller groupsshow the opposite behavior, persisting longer if their mem-bership is altered minimally and focus on a smaller setof topics. Finally, by analyzing the effect of group sizeand lifespan on research impact, we observed that paperspenned by medium sized and long lasting groups tend tohave a citation advantage.1. INTRODUCTIONMusic Information Retrieval (MIR) is an interdisciplinaryresearch ﬁeld that integrates a wide variety of research ar-eas, including audio signal processing, musicology, mu-sic psychology and cognition, information retrieval, andhuman-computer interfaces. The collection of papers pub-lished in the annual proceedings of the ISMIR conferenceprovides a wealth of information enabling us to mine forknowledge such as the networks of researchers that con-tribute papers and corresponding topics. Speciﬁcally, suchabundant data allows us to explore two main research ques-tions. First we focus on topics in the ﬁeld. Given thebreadth of the expertise of the ﬁeld and the high speedat which the digital technologies are developing, we in-vestigate if popular topics can be transient. Second, westudy the stability of research groups that emerge from theco-authorships of manuscripts, focusing on their sizes, di-versity of topics and competitiveness. While various ap-proaches for the exploration of knowledge in the ISMIRc\u0000Mohamed Sordo, Mitsunori Ogihara, Stefan Wuchty.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Mohamed Sordo, Mitsunori Ogi-hara, Stefan Wuchty. “Analysis of the Evolution of Research Groups andTopics in the ISMIR Conference”, 16th International Society for MusicInformation Retrieval Conference, 2015.paper collection exist, we considered a combination of net-work and text analysis.Recently, the analysis of scientiﬁc endeavors by investi-gating author relationships and their manuscripts providedinsights into innovation and idea creation processes [3],inter-dependencies between disciplines [2], or potential high-impact discoveries [5]. Utilizing proceedings of the main-stream conference we provide such a map of the status andtemporal evolution of the MIR ﬁeld. To the best of ourknowledge, only two studies on the nature of the ISMIRproceedings [7,10] have been presented recently. Grachtenet al. [7] applied text mining techniques and non-negativematrix factorization to identify topics and study their evo-lution over time. Lee et al. [10] applied simple text statis-tics to detect topics in paper titles and abstracts. In ourcase, we present a much broader analysis of research top-ics, that we map to categories that were deﬁned by the IS-MIR community. While Lee et al. [10] also presenteda few statistics to identify patterns of co-authorship wemodel the co-authorship as a complex network and studyits topology. Yet, the main contribution of this paper is theidentiﬁcation of research groups and their evolution overtime, and especially their time and topic dependencies. Inthe context of this paper, we do not use the deﬁnition of aresearch group in its traditional sense (e.g., a research in-stitution). Rather, we deﬁne it as a topological group in aco-authorship network.As for the organization of the paper we ﬁrst describe thenetwork of collaborations among authors in section 2. Insection 3, we provide an analysis of the manuscripts textcontents with a generative mixture model, allowing us toﬁnd temporal trends in the popularity of topics over theyears. In section 4 we identify research groups in the co-authorship network and analyze their evolution throughoutthe lifetime of the conference, investigating their time andtopic dependencies. Finally, we discuss our ﬁndings in sec-tion 5.2. CO-AUTHORSHIP ANALYSISUtilizing all manuscripts in the proceedings of the ISMIRconference from 2000-2014 we observed that the meannumber of authors per manuscript is growing over time(Table 1), conﬁrming previous results [18]. Starting withthe proceedings of the 2000 conference, we added newmanuscripts that were published in a given year to a grow-204Year Papers Authors Authors/Paper2000 35 63 1.942001 37 82 2.542002 53 113 2.362003 47 108 2.742004 104 213 2.412005 114 232 2.732006 87 185 2.562007 127 267 2.842008 105 262 2.932009 123 292 3.052010 110 262 3.012011 133 320 2.972012 101 264 3.212013 98 232 3.022014 106 273 3.24Table 1. For each year, we show the total number of papersand authors that published a manuscript in the proceedingsof the ISMIR conference.ing pool of papers. Based on such cumulative sets of manu-scripts, we constructed undirected unweighted networksG, where nodes represent authors, while edges indicatetheir co-authorships up to a given year.Table 2 suggests that the cumulative networks drasti-cally increased in size over time, a statistics that coincideswith an increasing number of collaboration partners (i.e.mean degreehki).Another important measure of social networks is theclustering coefﬁcient, reﬂecting the transitivity of a net-work. In particular, this network parameter determines thefraction of edges that appear between the neighbors of agiven author over all such possible links [17]. Table 2 indi-cates that the co-authorship networks appear increasinglyclustered, resembling a well known feature of other socialnetworks from different domains [9,12]. Such a high levelof clustering may be rooted in the assumption that manyauthors work in the same research ﬁeld, and as a conse-quence, are aware of each others work [13]. Another pos-sible explanation may be that authors tend to write paperswith colleagues from the same institution. Furthermore,we stress that our way of constructing a network of collab-orations between authors emphasizes manuscripts with alarge number of authors. Speciﬁcally, a set of authors thatpenned a manuscript together is represented as a clique, agraph that has a clustering coefﬁcient of 1. Consequently,manuscripts with many authors potentially introduce a biastoward strongly clustered networks.Another network parameter that well reﬂects the under-lying topology of an emerging network over time is theStrong Giant Component,SGC, deﬁned as the greatestconnected subset of nodes in a network. In particular, ahigh value ofSGCpoints to the observation that the vastmajority of scientists are connected through mutual collab-orations. During the ﬁrst years of the conference (up until2007), Table 2 indicates that the size ofSGCs was small,YearNhkiChdiSGC D2000 63 1.81 0.47 1.00 9.52% 12001 129 2.51 0.55 1.00 6.20% 12002 202 2.62 0.55 3.20 10.40% 62003 268 2.86 0.55 3.22 8.21% 62004 400 2.92 0.58 4.14 10.75% 102005 522 3.18 0.59 3.96 14.75% 92006 625 3.18 0.60 4.34 14.72% 102007 756 3.34 0.62 4.85 20.24% 112008 884 3.44 0.64 7.72 41.18% 172009 1041 3.58 0.65 8.13 46.11% 182010 1170 3.70 0.66 6.60 48.55% 152011 1339 3.76 0.67 6.47 53.70% 152012 1442 3.94 0.68 5.82 58.46% 142013 1548 4.03 0.69 5.74 61.18% 132014 1683 4.14 0.70 5.52 60.90% 13Table 2. We show properties of the cumulative authors’collaboration networks, combining manuscripts up to agiven year. In particular,Nis the number of nodes,hkiis the mean degree,Cis the clustering coefﬁcient. Fur-thermore,hdiis the avg. shortest path of theSGC, whichstands for the size (percentage of nodes) of the strong giantcomponent, whileDis the diameter of theSGC.suggesting that collaborations between authors appearedrather scattered. However, the size of theSGCdoubledin 2008, indicating an increased convergence where previ-ously present authors increasingly published a manuscripttogether. On the other hand, the observed increase in sizealso points to a gradual increase in the mean shortest pathhdibetween all pairs of nodes in theSGC. A closer lookat our data conﬁrmed that the increase in size of theSGCwas the consequence of a merger of the two largest com-ponents from the previous year. Notably, this topologicalchange was caused by a small set of nodes that bridgedthe previously disconnected components in the underlyingnetwork. As a consequence, the topological mean shortestpath lengths between nodes increased substantially sinceshortest paths between nodes that were placed in previ-ously disjoint components run through the small set of con-necting nodes. Such an assumption is further conﬁrmed bythe increasing diameter of the underlying networks deﬁnedas the maximum of shortest paths through a given network(Table 2).3. RESEARCH TOPICSThe analysis of the time evolution of research topics is avaluable asset for a research community to solve initialproblems and to adapt to challenging areas of research.In this section, we automatically extract underlying top-ics from the text content of proceeding papers, allowing usto map the evolution of these topics since the inception ofthe MIR ﬁeld.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 205topic 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14MIR Data & fundamentalsmus. signal processing 17.1 - - - - - 8.0 - -19.5- - 10.9 10.2 12.3metadata, & semantic web 11.4 5.6 -17.012.5 11.516.112.7 10.5 9.8 11.8 9.8 - - -social tags & user gen. data - - - - - - -13.510.5 12.2 10.912.811.912.2-lyrics & genres & moods - - - - - - - - 11.4 11.4 - 10.5 9.9 - 11.3Domain Knowledgecomp. music. & ethnomus. - 8.3 - - - - - - - - - - - - -mus. notation - 8.3 - - - - - - - - - - - - -mir & cultures - - - - - - - - - - - 9.8 - 10.2 -Mus. Features & Propertiesmelody & motives 11.4 - 11.3 8.5 8.7 - 9.2 11.9 - - - 11.3 12.9 - -harmony, chords & tonality - 13.9 - -13.58.8 9.2 10.3 9.5 13.0 10.9 10.5 11.9 10.2 -rhythm, beat, tempo -19.4- 12.813.512.4 - -13.38.9 11.8 - -12.212.3mus. affect, emot. & mood - - - 10.6 - - - - - - - - - 10.2 -structure, segment. & form - - 11.3 - - - - - 10.5 12.2 10.0 12.0 8.9 10.2 13.2Music Processingsound source separation - - - - - - 8.0 10.3 - -13.6-14.9 12.211.3mus. transcrip. & annot. 5.7 8.3 - - - 11.5 - - - - - - -12.2-optical mus. recognition - - - - - - 6.9 10.3 - - - - 9.9 - -align., synch. & score foll. - - - 10.6 - 12.4 - - - - - - - - -mus. summarization - - 7.5 - - - - - - - - - - - -ﬁngerprinting - - 11.3 - - - - - - - -12.8---automatic classiﬁcation 8.6 11.1 11.3 12.813.5 14.213.8 12.7 12.4 13.0 11.8 - - -14.2indexing & querying22.913.9 9.4 10.6 7.7 9.7 9.2 - - - 10.9 - - - -pattern match. & detection - 11.1 - 8.5 10.6 9.7 - - 11.4 - - - - - 5.7similarity metrics - - - 8.5 9.6 - 11.5 8.7 - - 8.2 10.5 - - -Applicationuser behavior & modeling - - - - - - - - - - - - 8.9 - -digital libraries & archives 11.4 - - - 10.6 - - - - - - - - - -mus. retrieval systems - -22.6- - - - - 10.5 - - - - - 8.5mus. rec. & playlist gen. - - 15.1 - - 9.7 8.0 - - - - - - - 11.3mus. & gaming - - - - - - - 9.5 - - - - - - -mus. software 11.4 - - - - - - - - - - - - - -Table 3. Utilizing a LDA model, we determined topic evolution over time, where topics are grouped according to the topicclassiﬁcation in the call for papers. Values in bold correspond to the most salient topics in each ISMIR conference edition.3.1 Topic extractionWe automatically extract the main topics by using LatentDirichlet Allocation (LDA) [4], a generative probabilisticmodel in which documents are represented as random mix-tures over latent topics. Each topic is characterized by amultinomial distribution over words that form those doc-uments [4]. As a main characteristic LDA assumes thatthe topic distribution has a Dirichlet prior, which not onlyresults in a smooth distribution but also simpliﬁes the prob-lem of topic inference [16].In particular, we used the MALLET implementation ofLDA, a java-based package for statistical natural languageprocessing, document classiﬁcation, clustering, topic mod-eling and machine learning applications of text [11]. MAL-LET’s implementation takes a text corpus and the numberof topics (k) to generate as input, and produces a list of themost relevant topics for that corpus, along with the topics’most salient terms. Furthermore, MALLET also providesa distribution of the topics among the documents that formthe corpus and includes a text pre-processing step prior togenerate the topic models.Here, we build a corpus for each set of manuscripts inthe ISMIR proceedings in a given year and setk= 10,resembling the number of oral sessions deﬁned by the pro-gram chairs, which typically group paper presentations bytheir topic afﬁnity. For the text pre-processing step, weremoved English stopwords, considered words that werelonger than 2 characters and used a combination of wordunigrams and bigrams. Since topics produced by an LDAmodel are only described by their word distribution, wemanually assigned “titles” after an inspection of the mostprobable terms. In particular, we used the list of topicsdescribed in the conference call for papers1as our ba-sis to assign and disambiguate topic titles2. We also ob-served that this LDA implementation was systematicallyproducing a topic containing most of the common words inany MIR publication (such asmusic,system,information,query,retrieval). Since such topics were almost never themost salient topic of a document in the corpus we removedthem from our analysis.3.2 Topic evolutionTable 3 shows the most salient topics that appeared in theISMIR proceedings over time, as well as a visualization oftheir evolution, pointing to their presence in each confer-ence edition. Each value in Table 3 represents the percent-age of papers per year whose most probable topic in the1http://ismir2015.uma.es/callforpapers.html2due to lack of space we made the topic distribution available online:https://goo.gl/6OmGl5206 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 1. Fission/fusion patterns in social networks from [14]. Considering social networks over time, groups are governedby dynamic events such as mergers (i.e .fusion) and splits of groups (ﬁssion).document–topic distribution corresponds to the topic in agiven row. For instance, the topicindexing and queryingwas the most salient topic of 22.9% of the papers in the IS-MIR 2000 edition. We stress that the lack of a value for atopic in a speciﬁc conference edition does not necessarilypoint to its absence in the underlying edition. In fact, suchan observation rather indicates that the topic in questionwas not among thek= 10most salient topics that year.For a better interpretation, we grouped topics accord-ing to the topic classiﬁcation in the call for papers. No-tably, we observed that the most salient topics over timebelonged to the categories “MIR Data and Fundamentals”,“Musical Features and Properties” and “Music Process-ing”, respectively, categories that can be regarded as thecore categories in the MIR ﬁeld. Some topics have beenlargely present over time, such as automatic classiﬁcation,harmony, melody, etc. Other topics appeared or becamemore popular halfway through the life-span of the confer-ence (e.g. tags, lyrics, moods, structure, etc.) or in thelast few years (e.g. source separation and music cultures).Such observations may be the consequence of introduc-ing emerging research topics or approaches from “neigh-boring” communities or from a shift in research fundingby national or international agencies. Finally, some top-ics emerged that have only been present in a short timeperiod (e.g. digital libraries or music and gaming). In par-ticular, we highlight thedigital librariestopic, which wasmore present during the ﬁrst editions of the conference, butdisappeared from the most salient topics over time. Suchan observation may be explained by the increased focus onmusic content- and context-based analysis (groups 1, 3 and4).4. GROUP DETECTION AND EVOLUTIONIn the past few years, considerable attention has been paidto uncovering topological groups in social networks. Thesegroups are expected to fundamentally impact the network’sdynamical properties as well: nodes that belong to the sametightly connected module are expected to display highlycorrelated dynamical activity, compared to nodes belong-ing to different groups. Previous studies found that largegroups are more stable and have a longer lifetime if theyare capable of dynamically altering their membership, sug-gesting that an ability to change the group composition re-sults in better adaptability [14]. Small groups display theopposite trend, suggesting that their condition for stabilityis an unchanged group composition. These discoveries areexpected to play a fundamental role in our understandingof human dynamics, with particular impact on our abilityto detect persuasion campaigns in a changing network en-vironment. Notably, dynamics of group composition havebeen noted by Dunbar and co-workers as a key mecha-nism to understand underlying human behavior across do-mains [1, 6]. In particular, we not only expect that suchpatterns will occur in the co-authorship networks based onconference proceedings of the ISMIR conference but alsoassume that the (in)stability of groups is a function of theirunderlying topics.4.1 MethodOur method is a modiﬁcation of the method presented in[14]. In particular, we deﬁne a co-authorship network foreach edition of the conference, where each edge representsa manuscript that a pair of authors penned in a given year.Furthermore, we extract groups using the clique percola-tion method (CPM), an algorithm for the detection of over-lapping network communities [15]. Groups in CPM, calledk-clique percolation clusters, are built up from adjacentk-cliques3.T w ok-cliques are considered adjacent if theysharek\u00001nodes. Such a deﬁnition allows nodes to ap-pear in severalk-clique percolation clusters, a suitable as-sumption, given that authors may participate in more thanone group. Speciﬁcally, we setk=3, since papers inthe ISMIR proceedings are co-authored on average by 3scientists. As a consequence, this restriction implies thatauthors who collaborate with less than 2 other authors willnever be part of a group.After groups have been determined in a given year, weneed to ﬁnd their possible matches in subsequent years. Inparticular, we construct a joint network by merging nodesand edges of networks at consecutive time stepstandt+1[14], considering different ﬁssion/fusion patterns (Fig. 1).We label the set of groups in timetasD, the set of groupsin timet+1asE, and the set of groups in the joint network3subgraphs of size k in which each node is connected to every othernodesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 207life span12 3 4 5 6 8 9num. groups327 65 24 7 2 5 1 1Table 4. Distribution of groups by their life timeasV. The deﬁnition of CPM implies that each group inD(E) is contained in exactly one group inV, although notall groups inVwill contain a group inD(E). If a groupVk2Vcontains a groupEj2Ebut no group inD, thengroupEjis considered born. Similarly, if a groupVk2Vcontains a groupDi2Dbut no group inE, then groupDiis considered dead. Furthermore, if a groupVk2Vcontains one or more groups inDand one or more groupsinE, then the relative overlap between all different pairs(Dki,Ekj)is obtained as:Cki,j=Dki\\EkjDki[Ekj(1)The pair (Dki,Ekj) of groups that maximizes this for-mula is considered a match of the same group in consecu-tive time stepstandt+1. The remaining groups are eithermarked as dead (Dk) or born (Ek).Contrary to the approach in [14], we considered theoverlap of nodes (instead of edges) to check whether groupsinDandEare contained inV. Since our networks arebuilt at discrete times two networks at time stepstandt+1do not necessarily have a high overlap as suggestedin [14]. Although the overlap of nodes might incur morenoisy matchings [14], we observed that our approach isless sensitive to the noise since the overlap of networks islimited.In some cases, we may consider a group dead in timestept+1but observe it re-born in time stept+2as aconsequence of members that did not publish a paper in theproceedings of yeart+1. Even though the time interval islarger than one year we consider them as the same group.Speciﬁcally, we matched such dead groups at timetwithborn groups att+nwithn=2, as larger values ofnonlymerged a very small number of groups.4.2 Experimental resultsApplying our method to our set of ISMIR proceedings weobtained a list of 432 groups, distributed as shown in ta-ble 4. Notably, we observed that only 40 groups persistedfor 3 or more years, representing less than 10% of the total.In particular, we split the groups in two categories: groupswith short (<3) and a long life spans (\u00003). Analyzinggroup sizes we determined the average size of each group(in terms of group members in each time step) over timeand split the groups in three size categories: small (avg.size<4members), medium (4avg. size<5) and large(avg. size>5). The three size categories contained 234,120 and 78 groups, respectively.Group size\u0000avgAvg. cumul. authorssmall0.58 5.17±1.47medium1.18 9.06±2.54large2.35 13.55±3.47Table 5. Group member variability in groups with long lifetime.Group sizeµ(\u0000) mediansmall3.17±1.073medium3.88±1.603large6.0±2.045Table 6. Topic variability in groups with long life time.4.2.1 Group member variabilityWe analyzed the variability of group members when groupspersisted for a longer period of time. In particular, wecalculated the variance of group size for each group ineach group category, and averaged them using\u0000avg=pPtvar(st), wherestis the size of a group at a partic-ular timet. Moreover, we computed the average numberof distinct authors that participated in the group at a giventime. Table 5 indicates that larger groups tend to have ahigher variability of members to persist for a longer pe-riod of time. Notably, we observed the opposite when weconsidered small groups, conﬁrming results in [14].4.2.2 Topic variabilityA higher topic variability means that groups change top-ics constantly throughout their life time. In particular, wecalculated the average number of topics covered by small,medium and large groups (Table 6). Similar to the pre-vious experiment, we only considered groups with a lifetime\u00003. To persist longer, large groups tend to covermore topics as exempliﬁed by a higher topic variability, asopposed to medium or small groups. Such observationssuggest that the persistence of groups does not only de-pend on their member dynamics, but also on the variabilityof research topics.4.2.3 Group characteristics and scientiﬁc impactFocusing on the relation between group characteristics andscientiﬁc impact we considered the number of citations ofeach paper, as of Google Scholar, representing an indica-tor of scientiﬁc impact. Speciﬁcally, we group papers bytheir most salient topic and only select the top 10 mostcited papers in each topic, providing a total of 243 papersfrom 28 different topics4. Out of this set of 243 papers, weobserved that only 137 were published by groups while theremainder was penned by one or two authors. As presentedin Table 7 we observed that papers written by mediumsized groups tend to get signiﬁcantly more citations than4some topics are present in less than 10 papers.208 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Group size/lifespan avg. paper citations # paperssmall62.54±54.7454medium113.67±107.5637large64.65±40.7146short73.49±67.3795long85.12±83.7742Table 7. Relation between group characteristics (size andlife span) and scientiﬁc impact. We only consider the top10 most cited papers per topic.other group categories. As for aspects of a group’s lifetime, papers by groups that last longer tend to get more ci-tations than short living groups. Such an observation maybe rooted in the assumption that persisting research groupswith stable members may have a higher chance of gettingnoticed by their peers, positively affecting their researchimpact. Furthermore, we stress that the distribution of ci-tations has heavy tails [18]. As a consequence the numberof citations of highly cited papers varies widely, explainingthe large margin of error in our analysis.5. CONCLUSIONSIn this paper, we analyzed the evolution of the MIR ﬁeldrepresented by the proceedings of its most prestigious con-ference ISMIR over the last 15 years. Notably, we foundthat the co-authorship network indicated a converging ﬁeldof authors as indicated by the emergence of large con-nected and clustered network components as well as a trendtoward larger research teams. While such a trend maybe rooted in the way we constructed the network of co-authorships, our results also suggest that authors that havepreviously published conference papers separately increas-ingly collaborate. Therefore, present conference contribu-tions may be viewed as ’seeds’ for future collaborationsbetween researchers that have not yet worked together. As-suming that increasing levels of collaboration govern inno-vation and the development of a research ﬁeld, our resultsindicate that the ISMIR conference is a potential driver ofthe Music Information Retrieval ﬁeld.Furthermore, a topic analysis revealed persistent as wellas ’rising’ and ’falling’ research topics over the years, pro-viding a simple assessment of ISMIR’s evolution. Suchan analysis allowed us to investigate the longevity as wellas the salience of certain topics. Our results also indicatethe emergence of novel topics that potentially may dom-inate the focus of conference contributions in the future.Moreover, we assumed that the evolution of topics may bea function of the underlying groups of co-authors, prompt-ing us to analyze their composition. Notably, we foundthat large groups persist through higher variability of teammembers while small groups show the opposite behavior.Furthermore, large groups show more variability of topicsas opposed to medium or small groups. While not neces-sarily a function of group size, such results suggest that thevariability of group composition may be the driving factorof topic variability. In particular, such results support thenotion that groups composed of incumbents and newcom-ers have a heightened chance of success [8]. As a conse-quence, our results suggest that large transient groups maybe the drivers for innovation given that such groups pro-vide topic variability. In turn, the arrival of new membersof a group may be accompanied by the introduction of newtopics. As such, our observations also suggest that grouppersistence is not only a question of the variability of teammembers but also of research topics, ultimately providinga competitive edge.6. ACKNOWLEDGMENTSMohamed Sordo acknowledges`Oscar Celma, Am´elie An-glade, Perfecto Herrera and Simon Dixon, with whom hecollaborated a few years ago on an unpublished manuscriptthat inﬂuenced the ﬁrst part of this paper.7. REFERENCES[1]Filippo Aureli, Colleen M. Schaffner, ChristopheBoesch, Simon K. Bearder, Josep Call, Colin A. Chap-man, Richard Connor, Anthony Di Fiore, Robin I. M.Dunbar, and S. Peter et al. Henzi. Fission-fusion dy-namics.Current Anthropology, 49(4):627–654, 2008.[2]Leana Bellanca. Measuring interdisciplinary research:analysis of co-authorship for research staff at the Uni-versity of York.Bioscience Horizons, 2(2):99–112,2009.[3]Luis M. A. Bettencourt, David I. Kaiser, and JasleenKaur. Scientiﬁc discovery and topological transitionsin collaboration networks.Journal of Biometrics,3(3):210–221, 2009.[4]David M. Blei, Andrew Y. Ng, and Michael I. Jor-dan. Latent dirichlet allocation.The Journal of Ma-chine Learning Research, 3:993–1022, 2003.[5]Chaomei Chen, Yue Chen, Mark Horowitz, HaiyanHou, Zeyuan Liu, and Don Pellegrino. Towards an ex-planatory and computational theory of scientiﬁc dis-covery.Journal of Informetrics, 3(3):191–209, 2009.[6]Robin I. M. Dunbar. Social cognition on the internet:testing constraints on social network size.Philosophi-cal Transactions of the Royal Society B: Biological Sci-ences, 367(1599):2192–2201, 2012.[7]Maarten Grachten, Markus Schedl, Tim Pohle, andGerhard Widmer. The ismir cloud: A decade of ismirconferences at your ﬁngertips. In10th InternationalSociety for Music Information Retrieval Conference,pages 63–68, 2009.[8]Roger Guimera, Brian Uzzi, Jarret Spiro, and LuisAmaral. Team assembly mechanisms determine col-laboration network structure and team performance.Science, 308(5722):697–702, 2005.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 209[9]Jon M. Kleinberg. Navigation in a small world.Nature,406:845, 2000.[10]Jin Ha Lee, M. Cameron Jones, and J. Stephen Downie.An analysis of ismir proceedings: Patterns of author-ship, topic, and citation. In10th International Societyfor Music Information Retrieval Conference, pages 57–62, 2009.[11]Andrew K. McCallum. Mallet: A machine learning forlanguage toolkit. http://mallet.cs.umass.edu, 2002.[12]Mark E. J. Newman. The structure and functionof complex networks.SIAM Review, 45(2):167–256,2003.[13]Mark E. J. Newman. Who is the best connected scien-tist? A study of scientiﬁc coauthorship networks.Lec-ture Note in Physics-New York then Berlin-, 650:337–370, 2004.[14]Gergely Palla, Albert-L´aszl´o Barab´asi, and Tam´asVicsek. Quantifying social group evolution.Nature,446(7136):664–667, 2007.[15]Gergely Palla, Imre Der´enyi, Ill´es Farkas, and Tam´asVicsek. Uncovering the overlapping community struc-ture of complex networks in nature and society.Nature,435(7043):814–818, 2005.[16]Mark Steyvers and Tom Grifﬁths. Probabilistictopic models.Handbook of latent semantic analysis,427(7):424–440, 2007.[17]Duncan J. Watts and Steven H. Strogatz. Collec-tive dynamics of ’small-world’ networks.Nature,393(6684):440–442, June 1998.[18]Stefan Wuchty, Benjamin F. Jones, and Brian Uzzi.The increasing dominance of teams in production ofknowledge.Science, 316(5827):1036–1039, 2007.210 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Particle Filters for Efficient Meter Tracking with Dynamic Bayesian Networks.",
        "author": [
            "Ajay Srinivasamurthy",
            "Andre Holzapfel",
            "Ali Taylan Cemgil",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416736",
        "url": "https://doi.org/10.5281/zenodo.1416736",
        "ee": "https://zenodo.org/records/1416736/files/Srinivasamurthy15.pdf",
        "abstract": "Recent approaches in meter tracking have successfully ap- plied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the appli- cation of exact inference is computationally demanding. More efficient approximate inference algorithms using par- ticle filters (PF) can be developed to overcome this limita- tion. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an exist- ing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Fil- ter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter track- ing accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We docu- ment that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music.",
        "zenodo_id": 1416736,
        "dblp_key": "conf/ismir/Srinivasamurthy15",
        "keywords": [
            "Bayesian models",
            "exact inference",
            "particle filters",
            "computational demands",
            "Carnatic music",
            "Ballroom dances",
            "meter tracking",
            "observation model",
            "corpora",
            "inference schemes"
        ],
        "content": "PARTICLE FILTERS FOR EFFICIENT METER TRACKING WITHDYNAMIC BAYESIAN NETWORKSAjay Srinivasamurthy⇤\u001cD\u001cvbXKm`i?v!mT7X2/mAndreHolzapfel†\u001cM/`2!`?vi?KQbXQ`;AliTaylan Cemgil†i\u001cvH\u001cMX+2K;BH!#QmMX2/mXi`XavierSerra⇤t\u001cpB2`Xb2``\u001c!mT7X2/m⇤Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain†Dept. of Computer Engineering, Boğaziçi University, Istanbul, TurkeyABSTRACTRecentapproachesinmetertrackinghavesuccessfullyap-plied Bayesian models. While the proposed models canbe adapted to different musical styles, the applicability ofthese flexible methods so far is limited because the appli-cation of exact inference is computationally demanding.Moreefficientapproximateinferencealgorithmsusingpar-ticlefilters(PF)canbedevelopedtoovercomethislimita-tion. In this paper, we assume that the type of meter of apieceisknown,andusethisknowledgetosimplifyanexist-ing Bayesian model with the goal of incorporating a morediverse observation model. We then propose Particle Fil-terbasedinferenceschemesforboththeoriginalmodelandthe simplification. We compare the results obtained fromexact and approximate inference in terms of meter track-ingaccuracyaswellasintermsofcomputationaldemands.EvaluationsareperformedusingcorporaofCarnaticmusicfromIndiaandacollectionofBallroomdances. Wedocu-mentthattheapproximatemethodsperformsimilartoexactinference, at a lower computational cost. Furthermore, weshow that the inference schemes remain accurate for longand full length recordings in Carnatic music.1. INTRODUCTIONRhythm analysis of musical audio signals plays an impor-tant role in Music Information Retrieval (MIR) research.Many of the works in MIR related to rhythm attempt toestablish a relation between the audio signal and the un-derlying musical meter. For instance, in the task of beattracking, the goal is to obtain an alignment of the metri-cal level referred to as thetactus[15] to an audio signal,see [8] for a list of references to recent beat tracking algo-rithms. Tracking meter at a higher metrical level is a taskpursuedunderthetitleofdownbeatdetection. Approacheswerepresentedthateitherattempttoidentifythedownbeatseparately from the tactus [7], or that pursue beat trackingand downbeat detection as a combined task [11,17]. Thecombined task of beat and downbeat detection is what werefer to as meter tracking, since it aims at aligning several© Ajay Srinivasamurthy, Andre Holzapfel, Ali TaylanCemgil, Xavier Serra.LicensedunderaCreativeCommonsAttribution4.0InternationalLicense(CC BY 4.0).Attribution:Ajay Srinivasamurthy, Andre Holzapfel,Ali Taylan Cemgil, Xavier Serra. “Particle Filters for Efficient MeterTracking with Dynamic Bayesian Networks”, 16th International Societyfor Music Information Retrieval Conference, 2015.levels of a known meter to an audio recording of a musicperformance.Manyapplicationscanprofitfromaccuratemeterorbeattracking. Somesynchronizationtasks,suchastheonepre-sentedin[6],trackingthebeatissufficient. However,otherapplications, such as musical structure analysis [16] canprofit from a more detailed understanding of the temporalstructure of a performance. Approaches that can achievesuchananalysisforawidervarietyofmusicusuallyincor-porate machine learning strategies to adapt to new styles.For instance, Böck et al. [1] presented a method for beattrackinginvariousstylesthatachieveshighaccuracyusingrecurrentneuralnetworksthatwereadaptedtotheindivid-ual styles. The task of downbeat tracking was addressedin[4]usingasetofdeepbeliefnetworkstrainedonvariousfeatures, and the regularity of the outputs was enforced byincorporatingasimplehiddenMarkovmodel(HMM).Thetask of meter tracking was combined with the determina-tion of the type of meter in [9], using a Dynamic BayesianNetwork (DBN) similar to the one applied in [1].Asignificantshortcomingofthementionedtrackingap-proaches is that their flexibility in terms of musical stylecomesatanincreasedcomputationalcost,eitherintermsoftimespentforthetrainingofnetworks[1,4],orintermsoflonginferencetimes[9]. Inthepresentpaper,weapproachfasterinferenceinaDBNintwoways. Firstly,weproposea change to the model structure as presented in [9,14] thatenables faster inference by simplifying the independenceassumptions between the variables of the model. The pro-posed simplification also addresses one of the main limit-ing factors in most of the approaches so far: a simplisticobservation model that cannot effectively handle diversityin rhythmic patterns. Secondly, one reason for long infer-encetimesofthemodelproposedin[9]istheutilizationofexact inference in an HMM, which discretizes the hiddenvariablesofthestatespacetocomputethemostlikelypathin the exact posterior distribution using the Viterbi algo-rithm. Here, we avoid the discretization of the state spaceby approximating the posterior using particle filter meth-ods [3]. The biggest challenge in applying such approx-imate methods to meter tracking is the multi-modality ofthe underlying posterior distribution [22] due to the ambi-guity inherent to musical meter. Recently, methods wereproposed that overcome these challenges [14]. We outlinetheexisting[9,14]andtheproposedsimplifiedmodel,andcompare the performance of exact and approximate infer-enceschemesforboththemodels,intermsofmetertrack-197(a)model-A(b) model-BFigure 1: The DBNs used in this paper: circles andsquares denote continuous and discrete variables, respec-tively. Graynodesandwhitenodesrepresentobservedandlatent variables, respectively. Model-A is from [14] andmodel-B is the proposed simplification.ing accuracy and computational demands.Carnaticmusic,theartmusictraditionfromSouthIndiais a representative case to study in this context. Meter inCarnaticmusicisdefinedbythetāḷa,whicharetimecycleswith three metrical levels: thesama(downbeat, the firstpulse of the cycle), beat, and the subdivision level (a com-prehensiveaccountonCarnaticmusicisprovidedin[19]).InperformancesofCarnaticmusic,however,largedegreesoffreedomaretakenbythemusicianstoconcealtheunder-lying meter and to add metrical ambiguity, for instance bychanging the beat structure during a metrical cycle. Thisplayful rhythmic character of Carnatic music leads to ourhypothesisthatmetertrackingshouldbeabletoprofitfroma diverse observation model. Most of the rhythmic struc-tures, melodic phrases, and strutural elements are tightlyassociatedwiththecyclesofthetāḷa[20]andhencetrack-ing the sama (downbeat) is an important MIR task in Car-naticmusic,whichisthemainfocusofthispaper. WewillalsoevaluateifmetertrackinginCarnaticmusiccanprofitfrom including a richer observation model that can incor-porate information from multiple patterns.Inordertofurtherillustratetheabilityoftheapproachtogeneralize, it will be additionally evaluated on a corpus ofBallroom dances [5]. Furthermore, reproducibility will beensured by providing free access for research purposes toall code repositories and datasets1. We begin by describ-ingthemodelsandinferenceschemesthatweuseformetertracking.2. MODEL STRUCTUREWe compare two different Bayesian models for the taskof meter tracking. The first model (model-A), depicted inFigure 1a, is identical to the model used in [9,14] and wasinitially proposed in [24]. We propose and discuss a sim-plificationtomodel-Aforthetaskofmetertracking,shownasmodel-BinFigure 1b. Model-Busesadiverseobserva-tionmodelandcanbeappliedifthetypeofmeterisknowninadvance. Itistobenotedthatmodel-Acanalsobeusedfor inferring the type of meter, though we apply it in thispaper only for meter tracking.1Please see the companion webpage for more details:?iiT,ff+QKTKmbB+XmT7X2/mfBbKB`@kyR8@T7InaDBN,anobservedsequenceoffeaturesderivedfromanaudiosignaly1:K={y1,...,yK}isgeneratedbyase-quenceofhidden(unknown)variablesx1:K={x1,...,xK},whereKis the length of the sequence (number of audioframesinanaudioexcerpt). Thejointprobabilitydistribu-tion of hidden and observed variables factorizes as,P(y1:K,x0:K)=P(x0)·K\u0000k=1P(xk|xk\u00001)P(yk|xk)(1)where,P(x0)is the initial state distribution,P(xk|xk\u00001)is the transition model, andP(yk|xk)is the observationmodel.2.1 Hidden VariablesAt each audio framek, the hidden variables describe thestate of a hypothetical bar pointerxk=[\u0000k˙\u0000krk], repre-senting the bar position, instantaneous tempo and a rhyth-micpatternindicator,respectively(seeFigure1of[23]foran illustration).•Bar position: The bar position\u00002[0,M), whereMisthe length of the bar (cycle). The maximum value ofMdepends on the longest bar (cycle) that is tracked. Weset the length of a full note to 1600, and scale other bar(cycle)lengths accordingly.•Rhythmic pattern: The rhythmic pattern variabler2{1,...,R}is an indicator variable to select one of theRobservation models corresponding to each bar (cycle)length rhythmic pattern learned from data. Each patternhas a bar lengthMand a number of beatsB, which areassumed to be known in advance, i.e. the goal is thetrackingof a known metrical structure.•Instantaneous tempo: Instantaneous tempo˙\u0000is the rateatwhichthebarpositionvariableprogressesthroughthecycle at each time frame, measured in bar positions pertime frame. The range of the variable˙\u0000k2[˙\u0000min,˙\u0000max]depends on the length of the cycleMand the hop size(\u0000=0.02sused in this paper), and can be preset orlearned from data. A tempo value of˙\u0000kcorresponds toa bar (cycle) length of (\u0000·M/˙\u0000k) seconds and (60·B·˙\u0000k/(M·\u0000))beatsper minute.Theconditionaldependencerelationsbetweenthevariablesfor both the models are shown in Figure 1.2.2 Initial state distributionWe can useP(x0)to incorporate prior information aboutthe metrical structure of the music into the model. In thispaper, we assume uniform priors on all variables, withinthe allowed ranges of tempo.2.3 Model-A: Transition and Observation modelDue to the conditional dependence relations in Figure 1a,the transition model factorizes as,P(xk|xk\u00001)=P(\u0000k|\u0000k\u00001,˙\u0000k\u00001,rk\u00001)P(˙\u0000k|˙\u0000k\u00001)⇥P(rk|rk\u00001,\u0000k,\u0000k\u00001)(2)Each of the terms in Eqn (2) are defined in Eqns (3)–(5).P(\u0000k|\u0000k\u00001,˙\u0000k\u00001,rk\u00001)=\n\u0000(3)198 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015where\n\u0000is an indicator function that takes a value of oneif\u0000k=(\u0000k\u00001+˙\u0000k\u00001)mod(M(rk))and zero otherwise(in our case,M(rk)=M), meaning that the bar positionadvances at the rate of the instantaneous tempo variable,and folds back when it crosses the maximum value that isdefined by the lengthMofthe metrical cycle.P(˙\u0000k|˙\u0000k\u00001)/N(˙\u0000k\u00001,\u00002˙\u0000)⇥\n˙\u0000(4)where\n˙\u0000is an indicator function that equals one if˙\u0000k2[˙\u0000min,˙\u0000max]andzerootherwise.N(µ, \u0000)denotesanormaldistribution with meanµandstandard deviation\u0000.P(rk|rk\u00001,\u0000k,\u0000k\u00001)=(A(rk\u00001,rk)if\u0000k<\u0000k\u00001\nrelse(5)where,A(i, j)is the time-homogeneous transition proba-bility fromritorj, and\nris an indicator function thatequals one whenrk=rk\u00001and zero otherwise. Since therhythmicpatternsareonebar(cycle)inlength,patterntran-sitions are allowed only at the end of the bar (cycle). Thepattern transition probabilities are learned from data.The observation model is identical to the one used in[14], and depends only on the bar position and rhythmicpattern variables. We use a two dimensional spectral fluxfeature in two frequency bands (Low:250 Hz, High:>250 Hz). Using beat and downbeat annotated trainingdata, a k-means clustering algorithm clusters and assignseach bar of the dataset (represented by a point in a 128-dimensional space) to one of theRrhythmic patterns. Wethendiscretizethebarinto64thnotecells(correspondingto25barpositionswithMmax= 1600),collectallthefeatureswithinthecellforeachpattern,andcomputethemaximumlikelihoodestimatesoftheparametersofatwocomponentGaussian Mixture Model (GMM). The observation proba-bility hence is computed as,P(y|x)=P(y|\u0000, r)=2Xi=1w\u0000,r,iN(y;µ\u0000,r,i,⌃\u0000,r,i)(6)where,N(y;µ,⌃)denotes a normal distribution and forthe mixture componenti,w\u0000,r,i,µ\u0000,r,iand⌃\u0000,r,iare thecomponent weight, mean (2-dimensional) and the covari-ance matrix (2⇥2), respectively.2.4 Model-B: Transition and Observation modelWe propose a simpler model-B (Figure 1b) that uses a di-versemixtureobservationmodelincorporatingobservationsfrom multiple rhythmic patterns. Since all the rhythmicpatterns belong to the same type of meter (tāḷa), we cansimplifymodel-Atotrackonlythe\u0000and˙\u0000variableswhileusinganobservationmodelthatcomputesthelikelihoodofan observation by marginalizing over all the patterns. Themotivationforthissimplificationistwo-fold: theinferenceis simplified, and we can increase the influence of diversepatternsthatoccurthroughoutametricalcycleintheinfer-ence.Formodel-B,wefirstdefinexk=[\u0000k,rk],where\u0000k=[\u0000k,˙\u0000k]. BasedontheconditionaldependencerelationsinFigure 1b, the transition model now is,P(xk|xk\u00001)=P(\u0000k|\u0000k\u00001)=P(\u0000k|\u0000k\u00001,˙\u0000k\u00001)P(˙\u0000k|˙\u0000k\u00001)(7)Eqns. (3) and (4) remain identical apart from the removalof the dependence onrk\u00001in Eqn (3). The observationmodel is a pre-computed mixture observation model com-puted from Eqn (6) by marginalizing over the patterns, as-suming equal priors.P(y|\u0000)/RXj=1P(y|\u0000, r=j)(8)3. INFERENCE METHODSThegoalofinferenceistofindahiddenvariablesequencethatmaximizestheposteriorprobabilityofthehiddenstatesgivenanobservedsequenceoffeatures: amaximumapos-teriori(MAP)sequencex⇤1:KthatmaximizesP(x1:K|y1:K).The inferred hidden variable sequencex⇤1:Kcan then betranslatedintoasequencedownbeat(sama)instants(\u0000⇤k=0), beat instants (\u0000⇤k=i·M/B,i=1,...,B), and thelocalinstantaneoustempo(˙\u0000⇤k). Wedescribetwodifferentinference schemes, an exact inference using an HMM in adiscretizedstatespace,andanapproximateinferenceusingparticle filters using the continuous values of\u0000and˙\u0000.3.1 Hidden Markov model (HMM)By discretizing the continuous variables bar position andtempo, we can perform an exact inference using HMM.We use the discretization proposed in [14], by replacingthecontinuousvariables\u0000and˙\u0000bytheirdiscretizedcoun-terparts,m2{1,2,...,dMe}andn2{nmin,nmin+1,···,nmax},withthediscretetempolimitsasnmin=b˙\u0000mincandN=nmax=d˙\u0000maxe,whered·eandb·cdenotetheceiland floor operations, respectively. Eqns (2), (3) and (5)remain valid. We define the tempo transition probabilitywithin the allowed tempo range as,P(nk|nk\u00001)=8><>:1\u0000pnifnk=nk\u00001pn2ifnk=nk\u00001±10otherwise(9)wherepnistheprobabilityoftempochange. WeuseViterbialgorithm[18]toobtainaMAPsequenceofstateswiththeHMM.WerefertotheHMMsforinferencefrommodel-Aand model-B as HMMaandHMMb,respectively.Thedrawbackofthisapproachisthatthediscretizationhastobeonaveryfinegridinordertoguaranteegoodper-formance, which leads to a prohibitively large state spaceand,asaconsequence,toacomputationallydemandingin-ference. The size of the state space isS=M·N·Randneeds anS⇥Ssized transition matrix. As an example, di-viding a bar intoM= 1600position states, withN= 15tempostatesandR=4patterns,thesizeofthestatespaceisS= 96000states. The computational complexity oftheViterbialgorithmisO(K·|S|2). EventhoughthestatetransitionmatrixissparseduetolessernumberofallowedtransitionsleadingtoacomplexityofO(K·M·R),theinfer-ence with HMM can become computationally prohibitiveand does not scale well with increasing number of states.This problem can be overcome, for instance, by using ap-proximate inference methods such as particle filters.3.2 Particle Filter (PF)Particle filters (or Sequential Monte Carlo methods) are aclass of approximate inference algorithms to estimate theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 199posteriordensityofastatespace. TheyovercometwomainproblemsoftheHMM:discretizationofthestatespaceandthequadraticscalingupofthesizeofstatespacewithmorenumberofvariables. Inaddition,theycanincorporatelongterm relationships between hidden variables.TheexactcomputationoftheposteriorP(x1:K|y1:K)isoftenintractable,butitcanbeevaluatedpointwise. Inpar-ticlefilters,theposteriorisapproximatedusingaweightedset of points (known as particles) in the state space as,P(x1:K|y1:K)⇡NpXi=1w(i)K\u0000(x1:K\u0000x(i)1:K)(10)Here,{x(i)1:K}is a set of points (particles) with associatedweights{w(i)K},i=1,...,Np, andx1:Kis the set of allstate trajectories until frameK, while\u0000(x)is the Diracdelta function,\u0000(x)=1ifx=0and0otherwise.Npis the number of particles.Toapproximatetheposteriorpointwise,weneedasuit-ablemethodtodrawsamplesx(i)kandcomputeappropriateweightsw(i)krecursively at each time step. A simple ap-proachisSequentialImportanceSampling(SIS)[3],wherewesamplefromaproposaldistributionQ(x1:K|y1:K)thathas the same support and is as similar to the true (target)distributionP(x1:K|y1:K)as possible. To account for thefact that we sampled from a proposal and not the target,weattachanimportanceweightw(i)Ktoeachparticle,com-puted as,w(i)K=P(x1:K|y1:K)Q(x1:K|y1:K)(11)Withasuitableproposaldensity,theseweightscanbecom-puted recursively as,w(i)k/w(i)k\u00001P(yk|x(i)k)P(x(i)k|x(i)k\u00001)Q(x(i)k|x(i)k\u00001,yk)(12)Following [14], we choose to sample from the transitionprobabilityQ(x(i)k|x(i)k\u00001,yk)=P(x(i)k|x(i)k\u00001), which re-duces Eqn (12) tow(i)k/w(i)k\u00001P(yk|x(i)k)(13)The SIS algorithm derives samples by first sampling fromproposal, in this case the transition probability and thencomputes weights according to Eqn (13). Once we deter-minetheparticletrajectories{x(i)1:K},wethenselectthetra-jectoryx(i⇤)1:Kwiththehighestweightw(i⇤)KastheMAPstatesequence.Many extensions have been proposed to the basic SISfilter (see [3] for a comprehensive overview) to addressseveral problems with it. We briefly mention some of therelevantextensions,emphasizingtheirkeyaspects. Amoredetailed description of the algorithms has been presentedin [14]. The most challenging problem in particle filter-ing is the degeneracy problem, where within a short time,mostoftheparticleshaveaweightclosetozero,represent-ing unlikely regions of state space. This is contrary to theideal case when we want the proposal to match well withthe target distribution leading to a uniform weight distri-bution with low variance. To reduce the variance of theparticleweights,resamplingstepsarenecessary,whichre-placeslowweightparticleswithhigherweightparticlesbyselecting particles with a probability proportional to theirweights. Severalresamplingmethodshavebeenproposed,but we use systematic resampling in this paper as recom-mendedin[3]. Withresamplingastheessentialdifference,theSISfilterwithresamplingiscalledasSequentialImpor-tance Sampling/Resampling (SISR) filter.Inmetertracking,duetometricalambiguities,theposte-rior distributionP(xk|y1:k)is highly multimodal. Resam-pling tends to lead to a concentration of particles in onemode of the posterior, while the remaining modes are notcovered. One way to alleviate this problem is to compressthe weightswk=w(i)k,i=1,...,Npby a monotoni-cally increasing function to increase the weights of parti-cles in low probability regions so that they can survive re-sampling. Afterresampling,theweightshavetobeuncom-pressedtogiveavalidprobabilitydistribution. ThiscanbeformulatedasanAuxiliaryParticleFilter(APF)[10]. Fur-ther,asystemthatiscapableofhandlingmetricalambigu-ities must maintain this multimodality and be able to trackseveral hypotheses together, which SISR and APF cannotdo explicitly. A system called the Mixture Particle Filter(MPF) was proposed to track multiple hypotheses in [22],and was adapted to meter inference in [14].In an MPF, each particle is assigned to a cluster that(ideally) represents a mode of the posterior. During re-sampling,theparticlesofaclusterinteractonlywithparti-clesofthesamecluster. Resamplingisdoneindependentlyin each cluster, while maintaining the probability distribu-tion intact. This way, all the modes of the posterior canbetrackedthroughthewholeaudiopiece,andthebesthy-pothesiscanbechosenattheend. Weuseanidenticalclus-teringschemeusingacyclicdistancemeasureasdescribedin[14]totrackseveraldifferentpossiblemetricalpositionsat a given time. In the MPF, after an initial cluster assign-ment, we perform a re-clustering before every resamplingstep,mergingorsplittingclustersbasedontheaveragedis-tance between cluster centroids. The clustering, mergingand splitting of clusters is necessary to control the numberof clusters, which ideally represents the number of modesin the posterior. The mixture particle filter can be com-bined with the Auxiliary resampling to give the AuxiliaryMixture Particle Filter (AMPF). As recommended in [14],we resample at a fixed intervalTs. It was shown in [14]that AMPF can be effectively used for the task of meterinference and tracking.With model-A, we setup an AMPF (AMPFa) to com-pute the pointwise estimates of the posterior ofx1:K, rep-resentedbynw(i)x,K,x(i)1:K,i=1,···,Npo,whereNpisthenumber of particles andw(i)x,Kare the weights correspond-ing to the particle trajectoriesx(i)1:K. The weights are up-datedasinEqn (13),usingtheobservationmodelinEqn (6).This particle filter is identical to the AMPF described in[14],however,inthispaperitisevaluatedforthefirsttimeassuming several patterns with transitions allowed.For the simplified model-B, we setup AMPFbsimilarlyfor\u00001:K, represented bynw(i)\u0000,K,\u0000(i)1:K,i=1,···,Npo,wherew(i)\u0000,Kare the weights corresponding to the particletrajectories\u0000(i)1:K. Similar to Eqn (13), the weight updates200 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Algorithm 1Outlineof the AMPFbalgorithm1:fori=1toNpdo2:Sample(\u0000(i)0)⇠P(\u00000)P(˙\u00000),setw(i)\u0000,0=1/Np3:Cluster{\u0000(i)0}and obtain cluster assignments{c(i)0}4:fork=1toKdo5:fori=1toNpdo6:Sample\u0000(i)k⇠P(\u0000(i)k|\u0000(i)k\u00001), Setc(i)k=c(i)k\u000017:˜w(i)\u0000,k=w(i)\u0000,k⇥RPj=1P(yk|\u0000(i)k,r=j)8:fori=1toNpdo\u0000Normalizeweights9:w(i)\u0000,k=˜w(i)\u0000,kPNpi=1˜w(i)\u0000,k10:ifmod(k,Ts)=0then11:Recluster and Resample{\u0000k,w\u0000,k}and obtain{ˆ\u0000k,ˆw\u0000,k},update{c(i)k}12:fori=1toNpdo13:Set\u0000(i)k=ˆ\u0000(i)k,w\u0000,k=ˆw\u0000,k14:Sample˙\u0000(i)k⇠P(˙\u0000(i)k|˙\u0000(i)k\u00001)for AMPFbare,w(i)\u0000,k/w(i)\u0000,k\u00001P(yk|\u0000(i)k)(14)whereP(yk|\u0000(i)k)is computed as in Eqn (8) by marginal-izingP(yk|x(i)k)overr(i)k. The AMPFbenables thereforetoincorporatethefullexpressivityoftheobservedpatternsintotheinference. AnoutlineofAMPFbisprovidedinAl-gorithm 1.The complexity of the PF schemes scale linearly withNpirrespective of the size of state space, leading to an ef-ficientinferenceinlargestatespaces. Further,comparedtotheHMMusingViterbidecodingthathasaspacecomplex-ityofO(K·|S|),thePFneedstostorejustNpstatetrajec-tories and weights, significantly reducing the memory re-quirements. An additional advantage is that the number ofparticles can be chosen based on the computational powerwecanafford,andwecanmakethestatespacelargerwithno or only a marginal increase in the computational re-quirements. Sincetheobservationlikehoodcanbeprecom-puted, inference with model-B requires much lower com-putational resources, with only a marginal increase in costduring inference with increase in number of patterns.4. EXPERIMENTSThe experiments aim to compare the performance of theparticle filter and the HMM inference schemes for metertrackingwithbothmodel-Aandmodel-B.Further,wewishtoseeifusingalargernumberofpatternsperrhythmclass(tāḷa) improves meter tracking performance. Meter track-ingisdoneforeachtypeofmeter(tāḷa)separately,inatwofold cross validation experiment.4.1 Music CorporaThe primary dataset we evaluate on is the Carnatic musicdataset(CMD)usedin[9]. Itincludes118twominutelongexcerpts spanning four commonly used tāḷas as shown inTable 1,withatotaldurationof236minutesandover5500TāḷaMB#ExcerptsCMD#PiecesCMDfĀdi (8/8)1600830 (60)50(252.8)Rūpaka(3/4)1200330 (60)50(267.4)Miśra chāpu (7/8)1400730 (60)48(342.1)Khaṇḍa chāpu (5/8)1000528 (56)28(134.6)Table 1: The Carnatic music datasets, showing the cyclelengthMused in the paper and the number of beatsBfor each tāḷa. The analogous time signature is also shown.CMDisasubsetofCMDf,withtwominuteexcerptsfromfullpieces. Thenumberofpieces/excerptsinbothdatasetsisalsoshown,thenumbersinparenthesesindicatethetotalduration of audio in minutes.sama instances. To test if the results extend to full pieces,we use the super set of CMD consisting of longer and fulllength pieces (called CMDf) as used in [21]. CMDfcom-prises about 16.6 hours of audio with over 22600 sama in-stances. For comparability, we also present results on theBallroom dataset [5], using the annotations from [12].4.2 Parameter Selection and LearningThetemporangesweremanuallysetforCarnaticmusicas˙\u00002[4,15](cycle lengths between 1.33s and 8s) and˙\u00002[6,32](barlengthsbetween0.75sto5.3s)fortheBallroomdataset. WithMmax= 1600(corresponds to ādi tāḷa with8 beats/cycle), the length of cycleMand the number ofbeatsBfor each tāḷa is shown in Table 1. For Ballroomdataset, we usedM= 1600andM= 1200for trackingtime signatures 4/4 and 3/4, respectively. For the HMM,we usepn=0.02as in [12], and for the AMPF, we use\u0000˙\u0000= 10\u00004·M. We explore the performance withR={1,2,4},withthenumberofparticlessettoNp= 1500·R.TheotherAMPFparametersareidenticaltothevaluesusedin [14].4.3 Evaluation MeasuresA variety of measures for evaluating beat and downbeattracking performance are available (see [2] for a detailedoverview and descriptions of the metrics listed below2).Wechosetwometricsthatarecharacterizedbyasetofdi-verse properties and are widely used in beat tracking eval-uation. We describe it for beats, but the definitions extendtodownbeats/samasaswell,withthesametolerances. Weuse the prefix ‘s-’ and ‘b-’ to distinguish between the per-formancemeasuresofsamaandbeattracking,respectively.Fmeas(F-measure): TheF-measure(anumberbetween0and1) is computed from correctly detected beats withina window of±70ms as the harmonic mean of the preci-sion (the ratio between the number of correctly detectedbeats and all detected beats) and recall (the ratio betweenthe number of correctly detected beats and the total anno-tated beats).AMLt(Allowed Metrical Levels with no continuity re-quired): In the AMLt measure (a number between0and1),beatsequencesareconsideredascorrectifthebeatsoc-cur on the off-beat, or are double or half of the annotatedtempo,allowingformetricalambiguities. Thevalueofthis2Weusedthecodeavailableat?iiT,ff+Q/2XbQmM/bQ7ir\u001c`2X\u001c+XmFfT`QD2+ibf#2\u001ci@2p\u001cHm\u001ciBQMfwith default settingsProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 201Sama trackingBeattrackingMeasures-Fmeass-AMLtb-Fmeasb-AMLtR124124124124HMMa0.733 0.736 0.7130.837 0.837 0.8040.85 0.847 0.8500.868 0.874 0.852AMPFa0.708 0.697 0.7040.827 0.809 0.8220.846 0.833 0.8430.872 0.874 0.862HMMb0.726 0.735 0.7360.830 0.862 0.8670.844 0.849 0.8370.864 0.893 0.900AMPFb0.690 0.712 0.7350.832 0.842 0.8530.833 0.838 0.8460.869 0.888 0.890Klapuri [11]0.1750.1810.6570.650Table2: MetertrackingperformanceonCMD.Inaddition,theperformanceofmetertrackingwiththealgorithmproposedin [11] is also shown for reference.DatasetCMDfBallroomMeasures-Fmeas b-Fmeass-Fmeas b-FmeasHMMa0.727 0.8340.806 0.929AMPFb0.728 0.8340.793 0.930Table 3: F-measure for meter tracking on CMDfand theBallroom dataset, withR=4. Values in each column arenot statistically significantly different.measure is then the ratio between the number of correctlyestimatedbeatsdividedbythenumberofannotatedbeats.4.4 Results and DiscussionWe report the average Fmeas and AMLt values for all ex-cerpts over all the tāḷas for the HMM and AMPF schemesinTable 2. TheresultsforAMPFarethemeanvaluesoverthree experiments. We conducted evaluations using sev-eralothermeasuresaswellwithoutanyqualitativechangeinresults. Therefore,experimentalresultsaredocumentedusing these two measures. We use a three-way ANOVAwithtāḷa,inferencescheme,andRasfactorstoassesssta-tisticallysignificantdifferences(at5%significancelevels).In general, we see that the beat tracking performanceis similar across all the inference schemes and values ofR, with the b-Fmeas and b-AMLt values being compara-ble. This shows that adding a diverse observation modeland additional patterns does not add a significant change,showing that handling pattern diversity is not needed forbeat tracking.For sama tracking, we see that the AMPFs show sta-tistically equivalent performance to the HMMs. The sim-pler AMPFbperforms as good or better than AMPFa, witha lower computational complexity. Higher number of pat-terns (R>1) do not show significant improvement intracking performance, despite a richer observation model.This observation needs further exploration to verify if in-corporating more patterns with the currently used featureshelpstoimprovesamatracking. Further,s-AMLtissignif-icantlylargerthans-Fmeasandshowsthatthereisapoten-tial for improvement in tracking the correct metrical level.Though we report only consolidated set of results aver-aged over all the tāḷas, the tracking performance is signif-icantly poorer for ādi tāḷa (e.g. s-Fmeas = 0.4, b-Fmeas =0.632withAMPFbandR=4),withsuperior(andstatisti-callyequivalent)resultswithotherthreetāḷas(e.g. s-Fmeas= 0.849, b-Fmeas = 0.92 with AMPFbandR=4). Thisis attributed to the long cycle durations and a large vari-etyofpatternsināditāḷa,whichshowsadefinitescopeforimprovement using higher number of patterns and betterobservation models.We extend the evaluation and report the performanceof HMMaand the proposed AMPFbon CMDfand Ball-roomdatasets(inanidenticalsetting,assumingthattheme-tertypeisknown)inTable 3. Weseethattheobservationsfrom CMD extend to these datasets too. We further see asimilarperformancebetweenCMDandCMDf,thatshowsthattheAMPFgeneralizestolongerandfulllengthpieces.Oneofthemainadvantagesofmodel-Bovermodel-Aisthelowercomputationalcost. Formetertrackingundertheconditionsdescribed,alltheinferenceschemeshavefasterthan real time execution. Inference in model-B is fasterthan that in model-A: model-B speeds up inference by afactor of about 5 for HMM and 2.5 for AMPF (forR=4and ādi tāḷa). Even in the smaller state space with model-B,HMMbhasahighermemoryrequirementthanAMPFb,which shows the utility of PF inference schemes.5. CONCLUSIONSFor the task of meter tracking, we presented a simplifiedBayesianmodelthatincorporatesaricherobservationmodel.We compared the performance of an exact inference us-ing an HMM using a discrete approximation of the mod-els, with an approximate inference using an AMPF on theexact model. The simplified model leads to faster infer-ence and a similar performance as the full model, with theperformanceextendingtofulllengthpiecesandgeneraliz-ing to different music styles. However, the proposed wayto enrich the observation model did not lead to significantdifferences in performance. This might be caused by thesimplistic audio features, and improving signal represen-tations appears as a necessary next step. In the future, weplantoexploreapproximateinferenceinimprovedmodels(such as [13] using an improved state space discretizationand tempo transition model) that also use better observa-tion models and can effectively utilize multiple rhythmicpatterns. WealsoplantoextendmetertrackingtoHindus-tani music, where long cycles (longer than a minute) existand hence present additional challenges.AcknowledgmentsThis work is supported by the European Research Council(grant number 267583) and a Marie Curie Intra-EuropeanFellowship(grantnumber328379). TheauthorsalsothankFlorianKrebsandSebastianBöckatJohannesKeplerUni-versity, Linz, Austria for providing access to their coderepositories.202 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20156. REFERENCES[1]S. Böck, F. Krebs, and G. Widmer. A multi-model ap-proachtobeattrackingconsideringheterogeneousmu-sic styles. InProc. of the 15th International Societyfor Music Information Retrieval Conference (ISMIR),pages602–607, Taipei, Taiwan, 2014.[2]M.Davies,N.Degara,andM.D.Plumbley.Evaluationmethods for musical audio beat tracking algorithms.Queen Mary University of London, Technical ReportC4DM-09-06,2009.[3]A. Doucet and A. M. Johansen. A tutorial on particlefilteringandsmoothing: Fifteenyearslater.HandbookofNonlinear Filtering, 2009.[4]S. Durand, J. P. Bello, B. David, and G. Richard.Downbeat tracking with multiple features and deepneural networks. InProc. of the 40th IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing(ICASSP), Brisbane, Australia, 2015.[5]F.Gouyon,A.Klapuri,S.Dixon,M.Alonso,G.Tzane-takis,C.Uhle,andP.Cano.Anexperimentalcompari-sonofaudiotempoinductionalgorithms.IEEETrans-actions on Audio, Speech and Language Processing,14(5):1832–1844,2006.[6]D.K.Grunberg,A.M.Batula,andY.E.Kim.Towardsthe development of robot musical audition. InProc. oftheMusic,Mind,andInventionWorkshop(MMI),NewJersey,USA, 2012.[7]J. A. Hockman, M. E. P. Davies, and I. Fujinaga. Onein the Jungle: Downbeat Detection in Hardcore, Jun-gle, and Drum and Bass. InProc. of the 13th Interna-tionalSocietyforMusicInformationRetrievalConfer-ence(ISMIR),pages169–174,Porto,Portugal,October2012.[8]A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. L.Oliveira, and F. Gouyon. Selective Sampling for BeatTracking Evaluation.IEEE Transactions on Audio,Speech, and Language Processing, 20(9):2539–2548,November2012.[9]A.Holzapfel,F.Krebs,andA.Srinivasamurthy.Track-ing the “odd”: Meter inference in a culturally diversemusiccorpus.InProc.ofthe15thInternationalSocietyfor Music Information Retrieval Conference (ISMIR),pages425–430, Taipei, Taiwan, 2014.[10]A.JohansenandA.Doucet.Anoteonauxiliaryparticlefilters.StatisticsandProbabilityLetters,78(12):1498–1504,2008.[11]A. P. Klapuri, A. J. Eronen, and J. T. Astola. AnalysisoftheMeterofAcousticMusicalSignals.IEEETrans-actions on Audio, Speech, and Language Processing,14(1):342–355,2006.[12]F. Krebs, S. Böck, and G. Widmer. Rhythmic patternmodeling for beat- and downbeat tracking in musicalaudio.InProc.ofthe14thInternationalSocietyforMu-sic Information Retrieval Conference (ISMIR), pages227–232, 2013.[13]F. Krebs, S. Böck, and G. Widmer. An efficient state-space model for joint tempo and meter tracking. InProc.ofthe16thInternationalSocietyforMusicInfor-mation Retrieval Conference (ISMIR), Malaga, Spain,October 2015.[14]F. Krebs, A. Holzapfel, A.T. Cemgil, and G. Widmer.Inferring metrical structure in music using particle fil-ters.IEEE/ACM Transactions on Audio, Speech, andLanguage Processing,23(5):817–827, May 2015.[15]J. London.Hearing in Time: Psychological Aspects ofMusicalMeter.OxfordUniversityPress,Oxford,2004.[16]J.Paulus,M.Müller,andA.Klapuri.Audio-basedmu-sicstructureanalysis.InProc.ofthe11thInternationalSociety for Music Information Retrieval Conference(ISMIR),pages625–636,Utrecht,Netherlands,August2010.[17]G. Peeters and H. Papadopoulos. Simultaneous beatand downbeat-tracking using a probabilistic frame-work: Theoryandlarge-scaleevaluation.IEEETrans-actions on Audio, Speech and Language Processing,19(6):1754–1769, 2011.[18]L. Rabiner. A tutorial on hidden Markov models andselectedapplicationsinspeechrecognition.Proc.oftheIEEE,77(2):257–286, February 1989.[19]P. Sambamoorthy.South Indian Music Vol. I-VI. TheIndian Music Publishing House, 1998.[20]A. Srinivasamurthy, A. Holzapfel, and X. Serra. InSearch of Automatic Rhythm Analysis Methods forTurkish and Indian Art Music.Journal of New MusicResearch, 43(1):97–117, 2014.[21]A. Srinivasamurthy and X. Serra. A supervised ap-proachtohierarchicalmetricalcycletrackingfromau-dio music recordings. InProc. of the 39th IEEE In-ternational Conference on Acoustics, Speech and Sig-nalProcessing(ICASSP),pages5237–5241,Florence,Italy, May 2014.[22]J.Vermaak,A.Doucet,andP.Pérez.Maintainingmul-timodalitythroughmixturetracking.InProc.ofthe9thIEEE International Conference on Computer Vision,pages 1110–1116, Nice, France, October 2003.[23]N. Whiteley, A. T. Cemgil, and S. Godsill. Bayesianmodelling of temporal structure in musical audio. InProc. of the 7th International Society for Music Infor-mation Retrieval Conference (ISMIR), Victoria, 2006.[24]N. Whiteley, A. T. Cemgil, and S. Godsill. Sequen-tial inference of rhythmic structure in musical audio.InProc. of the 33rd IEEE International ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),volume 4, pages 1321–1325, Honolulu, USA, April2007.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 203"
    },
    {
        "title": "Towards Music Imagery Information Retrieval: Introducing the OpenMIIR Dataset of EEG Recordings from Music Perception and Imagination.",
        "author": [
            "Sebastian Stober",
            "Avital Sternin",
            "Adrian M. Owen",
            "Jessica A. Grahn"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416270",
        "url": "https://doi.org/10.5281/zenodo.1416270",
        "ee": "https://zenodo.org/records/1416270/files/StoberSOG15.pdf",
        "abstract": "Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments – each 7–16s long – taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data.",
        "zenodo_id": 1416270,
        "dblp_key": "conf/ismir/StoberSOG15",
        "keywords": [
            "music imagery information retrieval",
            "electroencephalography",
            "EEG recordings",
            "music perception",
            "music imagination",
            "public domain dataset",
            "music perception and imagination",
            "music fragments",
            "music genres",
            "musical dimensions"
        ],
        "content": "TOWARDS MUSIC IMAGERY INFORMATION RETRIEVAL:INTRODUCING THE OPENMIIR DATASETOF EEG RECORDINGS FROM MUSIC PERCEPTION AND IMAGINATIONSebastian Stober, Avital Sternin, Adrian M. Owen and Jessica A. GrahnBrain and Mind Institute, Department of Psychology, Western University, London, ON, Canada{sstober,asternin,adrian.owen,jgrahn}@uwo.caABSTRACTMusic imagery information retrieval (MIIR) systems may oneday be able to recognize a song from only our thoughts. Asas t e pt o w a r d ss u c ht e c h n o l o g y ,w ea r ep r e s e n t i n gap u b l i cdomain dataset of electroencephalography (EEG) recordingstaken during music perception and imagination. We acquiredthis data during an ongoing study that so far comprises 10subjects listening to and imagining 12 short music fragments–e a c h7 – 1 6 sl o n g–t a k e nf r o mw e l l - k n o w np i e c e s . T h e s estimuli were selected from different genres and systematicallyvary along musical dimensions such as meter, tempo and thepresence of lyrics. This way, various retrieval scenarios canbe addressed and the success of classifying based on specificdimensions can be tested. The dataset is aimed to enable musicinformation retrieval researchers interested in these new MIIRchallenges to easily test and adapt their existing approachesfor music analysis like fingerprinting, beat tracking, or tempoestimation on EEG data.1. INTRODUCTIONWe all imagine music in our everyday lives. Individuals canimagine themselves producing music, imagine listening to oth-ers produce music, or simply “hear” the music in their heads.Music imagination is used by musicians to memorize musicpieces and anyone who has ever had an “ear-worm” – a tunestuck in their head – has experienced imagining music. Recentresearch also suggests that it might one day be possible toretrieve a music piece from a database by just thinking of it.As already motivated in [29], music imagery information re-trieval (MIIR)–i . e . ,r e t r i e v i n gm u s i cb yi m a g i n a t i o n–h a st h epotential to overcome the query expressivity bottleneck of cur-rent music information retrieval (MIR)s y s t e m s ,w h i c hr e q u i r etheir users to somehow imitate the desired song through singing,humming, or beat-boxing [31] or to describe it using tags, meta-data, or lyrics fragments. Furthermore, music imagery appearsto be a very promising means for driving brain-computer in-c\u0000Sebastian Stober, Avital Sternin, Adrian M. Owen andJessica A. Grahn.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Sebastian Stober, Avital Sternin, Adrian M. Owenand Jessica A. Grahn. “Towards Music Imagery Information Retrieval:Introducing the OpenMIIR Dataset of EEG Recordings from Music Perceptionand Imagination”, 16th International Society for Music Information RetrievalConference, 2015.terfaces (BCIs) that use electroencephalography (EEG)–apopular non-invasive neuroimaging technique that relies onelectrodes placed on the scalp to measure the electrical activityof the brain. For instance, Schaefer et al. [23] argue that“musicis especially suitable to use here as (externally or internallygenerated) stimulus material, since it unfolds over time, andEEGis especially precise in measuring the timing of a response. ”This allows us to exploit temporal characteristics of the signalsuch as rhythmic information.Still, EEG data is generally very noisy and thus extractingrelevant information can be challenging. This calls for sophisti-cated signal processing techniques as they have emerged in thefield of MIR within the last decade. However, MIR researcherswith the potential expertise to analyze music imagery data usu-ally do not have access to the required equipment to acquirethe necessary data for MIIR experiments in the first place.1In order to remove this substantial hurdle and encourage theMIR community to try their methods in this emerging interdis-ciplinary field, we are introducing theOpenMIIRdataset.In the following sections, we will review closely relatedwork inSection 2,d e s c r i b eo u ra p p r o a c hf o rd a t aa c q u i s i t i o n(Section 3)a n db a s i cp r o c e s s i n g(Section 4), and outline furthersteps inSection 5.2. RELATED WORKRetrieval based on brain wave recordings is still a very youngand largely unexplored domain. A recent review of neuroimag-ing methods forMIRthat also covers techniques different fromEEG is given in [14].EEGsignals have been used to measureemotions induced by music perception [1,16] and to distinguishperceived rhythmic stimuli [28]. It has been shown that oscilla-tory neural activity in the gamma frequency band (20-60 Hz) issensitive to accented tones in a rhythmic sequence [27]. Oscilla-tions in the beta band (20-30 Hz) entrain to rhythmic sequences[2, 17] and increase in anticipation of strong tones in a non-isochronous, rhythmic sequence [5,6,13]. The magnitude ofsteady state evoked potentials (SSEPs), which reflect neural os-cillations entrained to the stimulus, changes when subjects hearrhythmic sequences for frequencies related to the metrical struc-ture of the rhythm. This is a sign of entrainment to beat and me-ter [19, 20].EEGstudies have further shown that perturbations1For instance, the Biosemi EEG system used here costs several ten-thousand dollars. Consumer-level EEG devices with a much lower price havebecome available recently but it is still open whether their measuring precisionand resolution is sufficient for MIIR research.763of the rhythmic pattern lead to distinguishable event-relatedpotentials (ERPs)2[7]. This effect appears to be independentof the listener’s level of musical proficiency. Furthermore, Vleket al. [32] showed that imagined auditory accents imposed ontop of a steady metronome click can be recognized from EEG.EEG has also been successfully used to distinguish per-ceived melodies. In a study by Schaefer et al. [26], 10 partic-ipants listened to 7 short melody clips with a length between3.26s and 4.36s. For single-trial classification, each stimuluswas presented 140 times in randomized back-to-back sequencesof all stimuli. Using a quadratically regularized linear logistic-regression classifier with 10-fold cross-validation, they wereable to successfully classify theERPso fs i n g l et r i a l s .W i t h i nsubjects, the accuracy varied between 25% and 70%. Apply-ing the same classification scheme across participants, theyobtained between 35% and 53% accuracy. In a further analysis,they combined all trials from all subjects and stimuli into agrand averageERP.U s i n gs i n g u l a r - v a l u ed e c o m p o s i t i o n ,t h e yobtained a fronto-central component that explained 23% of thetotal signal variance. The time courses corresponding to thiscomponent showed significant differences between stimuli thatwere strong enough to allow cross-participant classification.Furthermore, a correlation with the stimulus envelopes of upto 0.48 was observed with the highest value over all stimuli atat i m el a go f7 0 – 1 0 0 m s .FMRI studies [10,11] have shown that similar brain struc-tures and processes are involved during music perception andimagination. As Hubbard concludes in his recent review of theliterature on auditory imagery,“auditory imagery preservesmany structural and temporal properties of auditory stimuli”and“involves many of the same brain areas as auditory per-ception”[12]. This is also underlined by Schaefer [23, p. 142]whose“most important conclusion is that there is a substantialamount of overlap between the two tasks[music perceptionand imagination],a n dt h a t‘ i n t e r n a l l y ’c r e a t i n gap e r c e p t u a lexperience uses functionalities of ‘normal’ perception.”Thus,brain signals recorded while listening to a music piece couldserve as reference data. The data could be used in a retrievalsystem to detect salient elements expected during imagination.Ar e c e n tm e t a - a n a l y s i s[ 2 5 ]s u m m a r i z e de v i d e n c et h a tEEGis capable of detecting brain activity during the imaginationof music. Most notably, encouraging preliminary results forrecognizing imagined music fragments fromEEGrecordingswere reported in [24] in which 4 out of 8 participants producedimagery that was classifiable (in a binary comparison) with anaccuracy between 70% and 90% after 11 trials.Another closely related field of research is the reconstruc-tion of auditory stimuli from EEG recordings. Deng et al. [3]observed thatEEGrecorded during listening to natural speechcontains traces of the speech amplitude envelope. They usedindependent component analysis (ICA)a n das o u r c el o c a l -ization technique to enhance the strength of this signal andsuccessfully identify heard sentences. Applying their techniqueto imagined speech, they reported statistically significant single-sentence classification performance for 2 of 8 subjects withbetter performance when several sentences were combined for2Ad e s c r i p t i o no fh o we v e n t - r e l a t e dp o t e n t i a l s(ERPs) are computed andsome examples are provided inSection 4.al o n g e rt r i a ld u r a t i o n .Recently, O’Sullivan et al. [21] proposed a method for de-coding attentional selection in a cocktail party environmentfrom single-trial EEG recordings approximately one minutelong. In their experiment, 40 subjects were presented with 2classic works of fiction at the same time – each one to a differ-ent ear – for 30 trials. To determine which of the 2 stimuli asubject attended to, they reconstructed both stimulus envelopesfrom the recorded EEG. To this end, they trained two differentdecoders per trial using a linear regression approach – one toreconstruct the attended stimulus and the other to reconstructthe unattended one. This resulted in 60 decoders per subject.These decoders where then averaged in a leave-one-out cross-validation scheme. During testing, each decoder would predictthe stimulus with the best reconstruction from the EEG usingthe Pearson correlation of the envelopes as measure of qual-ity. Using subject-specific decoders averaged from 29 trainingtrials, the prediction of the attended stimulus decoder was cor-rect for 89% of the trials whereas the mean accuracy of theunattended stimulus decoder was 78.9%. Alternatively, usingag r a n d - a v e r a g ed e c o d i n gm e t h o dt h a tc o m b i n e dt h ed e c o d e r sfrom every other subject and every other trial, they obtained amean accuracy of 82% and 75% respectively.3. STUDY DESCRIPTIONThis section provides details about the study that was conductedto collect the data released in the OpenMIIR dataset. The studyconsisted of two portions. We first collected information aboutthe participants using questionnaires and behavioral testing(Section 3.1)a n dt h e nr a nt h ea c t u a lE E Ge x p e r i m e n t(Sec-tion 3.2)w i t ht h o s ep a r t i c i p a n t sm a t c h i n go u ri n c l u s i o nc r i t e r i a .The 12 music stimuli used in this experiment are described inSection 3.3.3.1 Questionnaires and Behavioral Testing14 participants were recruited using approved posters at theUniversity of Western Ontario. We collected information aboutthe participants’ previous music experience, their ability toimagine sounds, and information about musical sophisticationusing an adapted version of the widely used Goldsmith’s Mu-sical Sophistication Index (G-MSI) [18] combined with anadapted clarity of auditory imagination scale [33]. Questionsfrom the perceptual abilities and musical training subscales ofthe G-MSI were used to identify individual differences in theseareas. For the clarity of auditory imagery scale, participantshad to self-report their ability to clearly hear sounds in theirhead. Our version of this scale added five music-related itemsto five items from the original scale.We also had participants complete a beat tapping and a stim-uli familiarity task. Participants listened to each stimulus andwere asked to tap along with the music on the table top. Theexperimenter then rated their tapping ability on a scale from 1(difficult to assess) to 3 (tapping done properly). After listeningto each stimulus participants rated their familiarity with thestimuli on a scale from 1 (unfamiliar) to 3 (very familiar). Toparticipate in theEEGportion of the study, the participantshad to receive a score of at least 90% on our beat tapping task.764 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015s\"mtracker*presenta\"on*system*screen*&*speakers*feedback*keyboard*presenta(on*system*feedback*video*audio*events*\n(op\"cal)*receiver*\nmarkers*\nrecording*system*sound*booth*\nBiosemi*Ac\"veTwo*64*EEG*+*4*EOG*channels*@*512*Hz*\nEEG*amp*on*baJery*Figure 1.S e t u pf o rt h eE E Ge x p e r i m e n t .T h ep r e s e n t a t i o na n drecording systems were placed outside to reduce the impactof electrical line noise that could be picked up by the EEGamplifier.Participants received scores from 75%–100% with an averagescore of 96%. Furthermore, they needed to receive a scoreof at least 80% on our stimuli familiarity task. Participantsreceived scores from 71%–100% with an average score 87%.These requirements resulted in rejecting 4 participants. Thisleft 10 participants (3 male), aged 19–36, with normal hear-ing and no history of brain injury. These 10 participants hadan average tapping score of 98% and an average familiarityscore of 92%. Eight participants had formal musical training(1–10 years), and four of those participants played instrumentsregularly at the time of data collection. After the experiment,we asked participants the method they used to imagine music.The participants were split evenly between imagining them-selves producing the music (singing or humming) and simply“hearing the music in [their] head.”3.2 EEG RecordingFor the EEG portion of the study, the 10 participants wereseated in an audiometric room (Eckel model CL-13) and con-nected to a BioSemi Active-Two system recording 64+2 EEGchannels at 512 Hz as shown inFigure 1.H o r i z o n t a l a n dvertical EOG channels were used to record eye movements.We also recorded the left and right mastoid channel as EEGreference signals. Due to an oversight, the mastoid data wasnot collected for the first 5 subjects. The presented audio wasrouted through a Cedrus StimTracker connected to the EEG re-ceiver, which allowed a high-precision synchronization (<0.05ms) of the stimulus onsets with theEEGdata. The experimentwas programmed and presented using PsychToolbox run inMatlab 2014a. A computer monitor displayed the instructionsand fixation cross for the participants to focus on during thetrials to reduce eye movements. The stimuli and cue clickswere played through speakers at a comfortable volume that waskept constant across participants. Headphones were not usedbecause pilot participants reported headphones caused themto hear their heartbeat which interfered with the imaginationportion of the experiment.The EEG experiment was divided into 2 parts with 5 blockseach as illustrated inFigure 2.As i n g l eb l o c kc o m p r i s e do fa l lTable 1.I n f o r m a t i o na b o u tt h et e m p o ,m e t e ra n dl e n g t ho ft h estimuli (without cue clicks) used in this study.ID Name Meter Length Tempo1C h i m C h i m C h e r e e ( l y r i c s ) 3 / 4 1 3 . 3 s 2 1 2 B P M2T a k e M e O u t t o t h e B a l l g a m e ( l y r i c s ) 3 / 4 7 . 7 s 1 8 9 B P M3J i n g l e B e l l s ( l y r i c s ) 4 / 4 9 . 7 s 2 0 0 B P M4M a r y H a d a L i t t l e L a m b ( l y r i c s ) 4 / 4 1 1 . 6 s 1 6 0 B P M11 Chim Chim Cheree 3/4 13.5s 212 BPM12 Take Me Out to the Ballgame 3/4 7.7s 189 BPM13 Jingle Bells 4/4 9.0s 200 BPM14 Mary Had a Little Lamb 4/4 12.2s 160 BPM21 Emperor Waltz 3/4 8.3s 178 BPM22 Hedwig’s Theme (Harry Potter) 3/4 16.0s 166 BPM23 Imperial March (Star Wars Theme) 4/4 9.2s 104 BPM24 Eine Kleine Nachtmusik 4/4 6.9s 140 BPMmean 10.4s 176 BPM12 stimuli in randomized order. Between blocks, participantscould take breaks at their own pace. We recorded EEG in 4conditions:1.Stimulus perception preceded by cue clicks2.Stimulus imagination preceded by cue clicks3.Stimulus imagination without cue clicks4.Stimulus imagination without cue clicks, with feedbackThe goal was to use the cue to align trials of the same stimuluscollected under conditions 1 and 2. Lining up the trials allowsus to directly compare the perception and imagination of musicand to identify overlapping features in the data. Conditions 3and 4 simulate a more realistic query scenario during whichthe system does not have prior information about the tempoand meter of the imagined stimulus. These two conditionswere identical except for the trial context. While the condition1–3 trials were recorded directly back-to-back within the firstpart of the experiment, all condition 4 trials were recordedseparately in the second part, without any cue clicks or tempopriming by prior presentation of the stimulus. After each con-dition 4 trial, participants provided feedback by pressing oneof two buttons indicating on whether or not they felt they hadimagined the stimulus correctly. In total, 240 trials (12 stimulix4c o n d i t i o n sx5b l o c k s )w e r er e c o r d e dp e rs u b j e c t .T h ee v e n tmarkers recorded in the raw EEG comprise:•Trial labels (as a concatenation of stimulus ID and condition)at the beginning of each trial•Exact audio onsets for the first cue click of each trial inconditions 1 and 2 (detected by the Stimtracker)•Subject feedback for the condition 4 trials (separate eventIDs for positive and negative feedback)3.3 StimuliTable 1shows an overview of the stimuli used in the study.This selection represents a tradeoff between exploration andexploitation of the stimulus space. As music has many facets,there are naturally many possible dimensions in which musicpieces may vary. Obviously, only a limited subspace could beexplored with any given set of stimuli. This had to be balancedagainst the number of trials that could be recorded for eachstimulus (exploitation) within a given time limit of 2 hours for asingle recording session (including fitting the EEG equipment).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 765X\"Stimulus X time \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \nCondition 1 Cued Perception \u0002 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 Condition 4 Imagination \n\u0001\u0001\u0001\u0001\u0001\u0001\u0001 Feedback \n\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \nCondition 2 Cued Imagination \n\u0001\u0001\u0001\u0001\u0001\u0001\u0001 Condition 3 Imagination \nPart I \nY\"Stimulus Y Part II time all 12 stimuli in random order 5 blocks 5 blocks all 12 stimuli in random order 5x12x3 trials 5x12x1 trials Figure 2.I l l u s t r a t i o no ft h ed e s i g nf o rt h eE E Gp o r t i o no ft h es t u d y .Based on the findings from related studies (c.f.Section 2),we primarily focused on the rhythm/meter and tempo dimen-sions. Consequently, the set of stimuli was evenly divided intopieces with 3/4 and 4/4 meter, i.e. two very distinct rhythmic“feels.” The tempo spanned a range between 104 and 212 beatsper minute (BPM). Furthermore, we were also interested inwhether the presence of lyrics would improve the recognizabil-ity of the stimuli. Hence, we divided the stimulus set into 3equally sized groups:•4r e c o r d i n g so fs o n g sw i t hl y r i c s( 1 – 4 ) ,•4r e c o r d i n g so ft h es a m es o n g sw i t h o u tl y r i c s( 1 1 – 1 4 ) ,a n d•4i n s t r u m e n t a lp i e c e s( 2 1 – 2 4 ) .The pairs of recordings for the same song with and withoutlyrics were tempo-matched by pre-selection and subsequentfine adjustment using the time-stretching function of Audac-ity.3Due to minor differences in tempo between pairs ofstimuli with and without lyrics, the tempo of the stimuli hadto be slightly modified after the first five participants.All stimuli were considered to be well-known pieces in theNorth-American cultural context. They were normalized involume and kept as similar in length as possible with care takento ensure that they all contained complete musical phrases start-ing from the beginning of the piece. Each stimulus startedwith approximately two seconds of clicks (1 or 2 bars) as anauditory cue to the tempo and onset of the music. The clicksbegan to fade out at the 1s-mark within the cue and stopped atthe onset of the music.3.4 Data and Code SharingWith the explicit consent of all participants and the approval ofthe ethics board at the University of Western Ontario, the datacollected in this study are released as OpenMIIR dataset4un-der theOpen Data Commons Public Domain Dedication andLicense (PDDL).5This comprises the anonymized answersfrom the questionnaires, the behavioral scores, the subjects’feedback for the trials in condition 4 and the raw EEG andEOG data of all trials at the original sample rate of 512 Hz.This amounts to approximately 700 MB of data per subject.3http://web.audacityteam.org/4https://github.com/sstober/openmiir5http://opendatacommons.org/licenses/pddlRaw data are shared in the FIF format used by MNE [9], whichcan easily be converted to the MAT format of Matlab.Additionally, the Matlab code and the stimuli for runningthe study are made available as well as the python code forcleaning and processing the raw EEG data as described inSec-tion 4.T h e p y t h o n c o d e u s e s t h e l i b r a r i e s M N E - P y t h o n [ 8 ]and deepthought6,w h i c ha r eb o t hp u b l i s h e da so p e n - s o u r c eunder the 3-clause BSD license.7This approach ensures accessibility and reproducibility. Re-searchers have the possibility to just apply their methods onthe already pre-processed data or change any step in the pre-processing pipeline according to their needs. No proprietarysoftware is required for working with the data. The wiki onthe dataset website can be used to share code, ideas and resultsrelated to the dataset.4. BASIC EEG PROCESSINGThis section describes basic EEG processing techniques thatmay serve as a basis for the application of more sophisticatedanalysis methods. More examples are linked in the wiki on thedataset website.4.1 EEG Data CleaningEEGrecordings are usually very noisy. They contain artifactscaused by muscle activity such as eye blinking as well as pos-sible drifts in the impedance of the individual electrodes overthe course of a recording. Furthermore, the recording equip-ment is very sensitive and easily picks up interferences suchas electrical line noise from the surroundings. The followingcommon-practice pre-processing steps were applied to removeunwanted artifacts.The raw EEG and EOG data were processed using theMNE-Python toolbox. The data was first visually inspected forartifacts. For one subject (P05), we identified several episodesof strong movement artifacts during trials. Hence, these partic-ular data need to be treated with care when used for analysis–p o s s i b l yp i c k i n go n l ys p e c i f i ct r i a l sw i t h o u ta r t i f a c t s .T h eb a d6https://github.com/sstober/deepthought7http://opensource.org/licenses/BSD-3-Clause766 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015trials might however still be used for testing the robustness ofanalysis techniques.For recordings with additional mastoid channels, the EEGdata was re-referenced by subtracting the mean mastoid sig-nal [30]. We then removed and interpolated bad EEG channelsidentified by manual visual inspection. For interpolation, thespherical splines method described in [22] was applied. Thenumber of bad channels in a single recording session varied be-tween 0 and 3. The data were then filtered with an fft-bandpass,keeping a frequency range between 0.5 and 30 Hz. This alsoremoved any slow signal drift in the EEG. Afterwards, wedown-sampled to a sampling rate of 64 Hz. To remove artifactscaused by eye blinks, we computed independent componentsusing extended InfomaxICA[15] and semi-automatically re-moved components that had a high correlation with the EOGchannels. Finally, the 64 EEG channels were reconstructedfrom the remaining independent components without reducingdimensionality.4.2 Grand Average Trial ERPsAc o m m o na p p r o a c ht oE E Ga n a l y s i si st h r o u g ht h eu s eo fevent-related potentials (ERPs). An ERP is an electrophysio-logical response that occurs as a direct result of a stimulus. RawEEG data is full of unwanted signals. In order to extract thesignal of interest from the noise, participants are presented withthe same stimulus many times. The brain’s response to the stim-ulus remains constant while the noise changes. The consistentbrain response becomes apparent when the signals from themultiple stimulus presentations are averaged together and therandom noise is averaged to zero. In order to identify commonbrain response patterns across subjects, grand average ERPsare computed by averaging the ERPs of different subjects.The size and the timing of peaks in the ERP waveformprovide information about the brain processes that occur inresponse to the presented stimulus. By performing a principlecomponent analysis (PCA), information regarding the spatialfeatures of these processes can be obtained.As proposed in [26], we computed grand averageERPsb yaggregating over all trials (excluding the cue clicks) of the samestimulus from all subjects except P05 (due to the movementartifacts). In their experiment, Schaefer et al. [26] used veryshort stimuli allowing each stimulus to be repeated many times.They averaged across hundreds of short (3.26s) trials, concate-nated the obtained grand average ERPs and then appliedPCA,which resulted in clearly defined spatial components. We hadfewer repetitions of our stimuli. Therefore, to preserve as muchdata as possible, we used the full length of the trials as opposedto the first 3.26 seconds. We then concatenated the grand av-erageERPsa n da p p l i e daPCA,w h i c hr e s u l t e di np r i n c i p a lcomponents with poorly defined spatial features as shown inFigure 3(A and B). As an alternative, we performed aPCAonthe concatenated raw trials without first calculating an averageacross trials. This approach produced clearly defined spatialcomponents shown inFigure 3(C and D). Components 2 to4a r es i m i l a rt ot h o s ed e s c r i b e di n[ 2 6 ] . E x c e p tf o rt h e i r( a r -bitrary) polarity, the components are very similar across thetwo conditions, which may be indicative of similar processesbeing involved in both perception and imagination of music as\nFigure 3.T o p o g r a p h i c v i s u a l i z a t i o n o f t h e t o p 4 p r i n c i p l ecomponents with percentage of the explained signal variance.Channel positions in the 64-channel EEG layout are shownas dots. Colors are interpolated based on the channel weights.The PCA was computed onA:t h eg r a n da v e r a g eERPso fall perception trials,B:t h eg r a n da v e r a g eERPso fa l lc u e dimagination trials,C:t h ec o n c a t e n a t e dp e r c e p t i o nt r i a l s ,D:t h econcatenated cued imagination trials.described in [11, 25].Schaefer et al. [26] were able to use the unique time courseof the component responsible for the most variance to differen-tiate between stimuli. Analyzing the signals corresponding tothe principle components, we have not yet been able to repro-duce a significant stimulus classification accuracy. This couldbe caused by our much smaller number of trials, which arealso substantially longer than those used by [26]. Furthermore,the cross-correlation between the stimulus envelopes and thecomponent waveforms were much lower (often below 0.1) thanreported in [26].4.3 Grand Average Beat ERPsIn the previous section, we computed ERPs based on the trialonsets. Similarly, it is also possible to analyze beat events.Using the dynamic beat tracker [4] provided by the librosa8library, we obtained beat annotations for all beats within the au-dio stimuli. To this end, the beat tracker was initialized with theknown tempo of each stimulus. The quality of the automaticannotations was verified through sonification.Knowing the beat positions allows to analyze the respectiveEEG segments in the perception condition. For this analysis,the EEG data was additionally filtered with a low-pass at 8Hz to remove alpha band activity (8–12 Hz).Figure 4shows8https://github.com/bmcfee/librosaProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 767Figure 4.G r a n da v e r a g eb e a tE R Pf o rt h ep e r c e p t i o nt r i a l s( 1 6 5 1 5b e a t s ) .A l lt i m e sa r er e l a t i v et ot h eb e a to n s e t .L e f t :I n d i v i d u a lchannels and mean over time. Right: Topographic visualization for discrete time points (equally spaced at 1/30s interval).\nFigure 5.G r a n da v e r a g eb e a tE R Pf o rt h ec u e di m a g i n a t i o ntrials (16515 beats). All times are relative to the beat onset.Note the difference in amplitude compared toFigure 4.the grand average ERP for all beats except the cue clicks9in all perception trials of all subjects except P05. Here weconsidered epochs, i.e., EEG segments of interest, from 200 msbefore until 300 ms after each beat marker. Before averaginginto the ERP, we applied a baseline correction of each epochby subtracting the signal mean computed from the 200 mssub-segment before the beat marker.The ERP has a negative dip that coincides with the beatonset time at 0 ms. Any auditory processing related to the beatwould occur much later. A possible explanation is that the dipis caused by the anticipation of the beat. However, this requiresfurther investigation. There might be potential to use this effectas the basis for an MIIR beat or tempo tracker. For comparison,the respective grand average ERP for the cued imaginationtrials is shown inFigure 5.T h i sE R Pl o o k sv e r yd i f f e r e n tf r o mthe one for the perception conditions. Most notably the ampli-tude scale is very low. This outcome was probably caused bythe imprecise time locking. In order to compute meaningfulERPs, the precise event times (beat onsets) need to be known.However, small tempo variations during imagination are verylikely and thus the beat onsets are most likely not exact.9Cue clicks were excluded because these isolated auditory events illicitad i f f e r e n tb r a i nr e s p o n s et h a nb e a t se m b e d d e di n t oas t r e a mo fm u s i c .5. CONCLUSIONS AND OUTLOOKWe have introduced OpenMIIR – an open EEG dataset in-tended to enable MIR researchers to venture into the domainof music imagery and develop novel methods without the needfor special EEG equipment. We plan to add new EEG record-ings with further subjects to the dataset and possibly adaptthe experimental settings as we learn more about the problem.In our first experiments using this dataset, we were able topartly reproduce the identification of overlapping componentsbetween music perception and imagination as reported earlier.Will it one day be possible to just think of a song and themusic player will start its playback? If this could be achieved,it would require the intense interdisciplinary collaboration be-tween MIR researchers and neuroscientists. We hope that theOpenMIIR dataset will facilitate such a collaboration and con-tribute to new developments in this emerging field for research.Acknowledgments:This work has been supported by a fel-lowship within the Postdoc-Program of the German AcademicExchange Service (DAAD), the Canada Excellence ResearchChairs (CERC) Program, an National Sciences and Engineer-ing Research Council (NSERC) Discovery Grant, an OntarioEarly Researcher Award, and the James S. McDonnell Founda-tion. The authors would further like to thank the study partici-pants, and the anonymous ISMIR reviewers for the constructivefeedback on the paper.6. REFERENCES[1]R. Cabredo, R. S. Legaspi, P. S. Inventado, and M. Numao.An Emotion Model for Music Using Brain Waves. InProceedings of the 13th International Society for MusicInformation Retrieval Conference (ISMIR’12),p a g e s265–270, 2012.[2]L. K. Cirelli, D. Bosnyak, F. C. Manning, C. Spinelli,C. Marie, T. Fujioka, A. Ghahremani, and L. J. Trainor.Beat-induced fluctuations in auditory cortical beta-bandactivity: Using EEG to measure age-related changes.Frontiers in Psychology,5 ( J u l ) : 1 – 9 ,2 0 1 4 .[3]S. Deng, R. Srinivasan, and M. D’Zmura. Corticalsignatures of heard and imagined speech envelopes.Technical report, DTIC, 2013.768 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[4]D. P. W. Ellis. Beat Tracking by Dynamic Programming.Journal of New Music Research,3 6 ( 1 ) : 5 1 – 6 0 ,2 0 0 7 .[5]T. Fujioka, L. J. Trainor, E. W. Large, and B. Ross. Betaand gamma rhythms in human auditory cortex duringmusical beat processing.Annals of the New York Academyof Sciences,1 1 6 9 : 8 9 – 9 2 ,2 0 0 9 .[6]T. Fujioka, L. J. Trainor, E. W. Large, and B. Ross.Internalized Timing of Isochronous Sounds Is Repre-sented in Neuromagnetic Beta Oscillations.Journal ofNeuroscience,3 2 ( 5 ) : 1 7 9 1 – 1 8 0 2 ,2 0 1 2 .[7]E. Geiser, E. Ziegler, L. Jancke, and M. Meyer. Early elec-trophysiological correlates of meter and rhythm processingin music perception.Cortex,4 5 ( 1 ) : 9 3 – 1 0 2 ,2 0 0 9 .[8]A. Gramfort, M. Luessi, E. Larson, D. A. Engemann,D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks,L. Parkkonen, and M. H¨am¨al¨ainen. MEG and EEG dataanalysis with MNE-Python.Frontiers in Neuroscience,7, 2013.[9]A. Gramfort, M. Luessi, E. Larson, D. A. Engemann,D. Strohmeier, C. Brodbeck, L. Parkkonen, and M. S.H¨am¨al¨ainen. MNE software for processing MEG andEEG data.NeuroImage,8 6 ( 0 ) : 4 4 6–4 6 0 ,2 0 1 4 .[10]A. R. Halpern, R. J. Zatorre, M. Bouffard, and J. A.Johnson. Behavioral and neural correlates of perceivedand imagined musical timbre.Neuropsychologia,42(9):1281–92, 2004.[11]S. Herholz, A. Halpern, and R. Zatorre. Neuronal correlatesof perception, imagery, and memory for familiar tunes.Journal of cognitive neuroscience,2 4 ( 6 ) : 1 3 8 2 – 9 7 ,2 0 1 2 .[12]T. L. Hubbard. Auditory imagery: empirical findings.Psychological Bulletin,1 3 6 ( 2 ) : 3 0 2 – 3 2 9 ,2 0 1 0 .[13]J. R. Iversen, B. H. Repp, and A. D. Patel. Top-downcontrol of rhythm perception modulates early auditoryresponses.Annals of the New York Academy of Sciences,1169:58–73, 2009.[14]B. Kaneshiro and J. P. Dmochowski. Neuroimagingmethods for music information retrieval: Current findingsand future prospects. InProceedings of the 16th Interna-tional Society for Music Information Retrieval Conference(ISMIR’15),2 0 1 5 .[15]T.-W. Lee, M. Girolami, and T. J. Sejnowski. IndependentComponent Analysis Using an Extended InfomaxAlgorithm for Mixed Subgaussian and SupergaussianSources.Neural Computation,1 1 ( 2 ) : 4 1 7 – 4 4 1 ,1 9 9 9 .[16]Y.-P. Lin, T.-P. Jung, and J.-H. Chen. EEG dynamicsduring music appreciation. InAnnual InternationalConference of the IEEE Engineering in Medicine andBiology Society (EMBC’09),p a g e s5 3 1 6 – 5 3 1 9 ,2 0 0 9 .[17]H. Merchant, J. Grahn, L. J. Trainor, M. Rohrmeier, andW. T. Fitch. Finding a beat: a neural perspective acrosshumans and non-human primates.Philosophical Trans-actions of the Royal Society B: Biological Sciences,2 0 1 5 .[18]D. M¨ullensiefen, B. Gingras, J. Musil, and L. Stewart. TheMusicality of Non-Musicians: An Index for AssessingMusical Sophistication in the General Population.PLoSONE,9 ( 2 ) ,2 0 1 4 .[19]S. Nozaradan, I. Peretz, M. Missal, and A. Mouraux.Tagging the neuronal entrainment to beat and meter.TheJournal of Neuroscience,3 1 ( 2 8 ) : 1 0 2 3 4 – 1 0 2 4 0 ,2 0 1 1 .[20]S. Nozaradan, I. Peretz, and A. Mouraux. SelectiveNeuronal Entrainment to the Beat and Meter Embeddedin a Musical Rhythm.The Journal of Neuroscience,32(49):17572–17581, 2012.[21]J. A. O’Sullivan, A. J. Power, N. Mesgarani, S. Rajaram,J. J. Foxe, B. G. Shinn-Cunningham, M. Slaney, S. A.Shamma, and E. C. Lalor. Attentional Selection in a Cock-tail Party Environment Can Be Decoded from Single-TrialEEG.Cerebral Cortex,( 2 5 ) : 1 6 9 7 – 1 7 0 6 ,2 0 1 5 .[22]F. Perrin, J. Pernier, O. Bertrand, and J. F. Echallier. Spher-ical splines for scalp potential and current density mapping.Electroencephalography and Clinical Neurophysiology,72(2):184–187, 1989.[23]R. Schaefer.Measuring the mind’s ear EEG of musicimagery.P h Dt h e s i s ,R a d b o u dU n i v e r s i t yN i j m e g e n ,2 0 1 1 .[24]R. Schaefer, Y . Blokland, J. Farquhar, and P. Desain. Singletrial classification of perceived and imagined music fromEEG. InProceedings of the 2009 Berlin BCI Workshop.2009.[25]R. S. Schaefer, P. Desain, and J. Farquhar. Shared process-ing of perception and imagery of music in decomposedEEG.NeuroImage,7 0 : 3 1 7 – 3 2 6 ,2 0 1 3 .[26]R. S. Schaefer, J. Farquhar, Y. Blokland, M. Sadakata,and P. Desain. Name that tune: Decoding music from thelistening brain.NeuroImage,5 6 ( 2 ) : 8 4 3 – 8 4 9 ,2 0 1 1 .[27]J. S. Snyder and E. W. Large. Gamma-band activityreflects the metric structure of rhythmic tone sequences.Cognitive Brain Research,2 4 : 1 1 7 – 1 2 6 ,2 0 0 5 .[28]S. Stober, D. J. Cameron, and J. A. Grahn. Usingconvolutional neural networks to recognize rhythm stimulifrom electroencephalography recordings. InAdvancesin Neural Information Processing Systems 27 (NIPS’14),pages 1449–1457, 2014.[29]S. Stober and J. Thompson. Music imagery informationretrieval: Bringing the song on your mind back to your ears.In13th International Conference on Music Information Re-trieval (ISMIR’12) - Late-Breaking & Demo Papers,2 0 1 2 .[30]M. Teplan. Fundamentals of EEG measurement.Measurement science review,2 ( 2 ) : 1 – 1 1 ,2 0 0 2 .[31]G. Tzanetakis, A. Kapur, and M. Benning. Query-by-Beat-Boxing: Music Retrieval For The DJ. InProceedings ofthe 5th International Conference on Music InformationRetrieval (ISMIR’04),p a g e s1 7 0 – 1 7 7 ,2 0 0 4 .[32]R. J. Vlek, R. S. Schaefer, C. C. A. M. Gielen, J. D. R.Farquhar, and P. Desain. Shared mechanisms in per-ception and imagery of auditory accents.ClinicalNeurophysiology,1 2 2 ( 8 ) : 1 5 2 6 – 1 5 3 2 ,2 0 1 1 .[33]J. Willander and S. Baraldi. Development of a new clarityof auditory imagery scale.Behaviour Research Methods,42(3):785–590, 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 769"
    },
    {
        "title": "Temporal Music Context Identification with User Listening Data.",
        "author": [
            "Cameron Summers",
            "Phillip Popp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415938",
        "url": "https://doi.org/10.5281/zenodo.1415938",
        "ee": "https://zenodo.org/records/1415938/files/SummersP15.pdf",
        "abstract": "The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly im- portant. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, man- ually labeling track-temporal context associations is diffi- cult, time consuming, and costly. This paper examines track-day contexts with the pur- pose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known hol- idays, Halloween, Saint Patrick’s Day, and July 4th, preci- sion@50 was 95%, 99%, and 73%, respectively.",
        "zenodo_id": 1415938,
        "dblp_key": "conf/ismir/SummersP15",
        "keywords": [
            "music",
            "context",
            "listener",
            "relationship",
            "temporal",
            "catalogues",
            "manually",
            "labeling",
            "discovery",
            "automated"
        ],
        "content": "TEMPORAL MUSIC CONTEXT IDENTIFICATION WITH USERLISTENING DATACameron SummersGracenotecsummers@gracenote.comPhillip PoppGracenoteppopp@gracenote.comABSTRACTThe times when music is played can indicate context forlisteners. From the peaceful song for waking up eachmorning to the traditional song for celebrating a holiday toan up-beat song for enjoying the summer, the relationshipbetween the music and the temporal context is clearly im-portant. For music search and recommendation systems,an understanding of these relationships provides a richerenvironment to discover and listen. But with the largenumber of tracks available in music catalogues today, man-ually labeling track-temporal context associations is difﬁ-cult, time consuming, and costly.This paper examines track-day contexts with the pur-pose of identifying relationships with speciﬁc musictracks. Improvements are made to an existing methodfor classifying Christmas tracks and a generalization tothe approach is shown that allows automated discoveryof music for any day of the year. Analyzing the top 50tracks obtained from this method for three well-known hol-idays, Halloween, Saint Patrick’s Day, and July 4th, preci-sion@50 was 95%, 99%, and 73%, respectively.1. INTRODUCTIONWith the ever increasing amount of recorded music, struc-tured metadata is important to organize it. For holiday mu-sic, there is some metadata that indicates an associationwith a music track, often Christmas [1], but comprehensivelabeling for other holidays is still lacking. One reason forthis is the varying nature of holiday music. Across geogra-phies, cultures, and time, what music is used to celebrateholidays changes dramatically. There is a bit of a para-dox as to whether a holiday track is so because the artistrecorded it for that purpose or the listeners use it to cele-brate1. Given this complex landscape of holiday music,manual labeling of a large number of music tracks is difﬁ-cult, time consuming, and costly. Methods for automatedlabeling are desirable for large scale organization, further1The interpretation of the authors of what is truly holiday music is thelatter.c\u0000Cameron Summers, Phillip Popp.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Cameron Summers, Phillip Popp.“Temporal Music Context Identiﬁcation with User Listening Data”, 16thInternational Society for Music Information Retrieval Conference, 2015.improving the capabilities of music search and recommen-dation systems.One automated approach is using text search of tracknames or album names for keywords also associated withthe target holiday [2]. For example, tracks with the key-words ”winter” or ”spooky” may be likely associated withChristmas or Halloween, respectively. This approach hasdrawbacks, however. First, it requires experts to createkeywords lists, which can be costly or difﬁcult, particularlyfor music in different languages. Second, the keywordsdo not guarantee correct track-holiday association, particu-larly for ambiguous words like ”whiskey”, which could belinked contextually to Saint Patrick’s Day or simply drink-ing beverages. This problem is compounded when usingmultiple keywords, as is required for a comprehensive setof tracks.Another automated approach for labeling holiday musicis through user crowdsourcing. LastFM (last.fm), for ex-ample, allows users to add their own tags to music tracks,which include tags for some holidays like Halloween [3].This has the advantages of outsourcing the work of label-ing and getting a better representation of the holiday musicpreferences of a larger number of listeners. But this toohas drawbacks. Users tend to only label popular tracksand artists, leading to imbalanced coverage. The qualityof these tags can suffer to due misspellings, synonyms, bi-ases, or dishonest labeling. And the users providing tagsare still typically a small subset of the total users of a ser-vice [4].Alternatively, leveraging user listening data avoids thequality issues associated with user tagging and keyword as-sociation, can utilize the entire user base, and is languageagnostic. Researchers have studied temporal dynamics ofuser data previously to understand context. [5] examinedtemporal context to improve biosurveillance. [6] and [7]classiﬁed web search queries using features in the pop-ularity signal over time and in music, [8], [9], and [10]show the usefulness of temporal analysis in recommenda-tions systems. An approach proposed by [11] exploits userlistening data to automatically label tracks as associatedwith Christmas. However, the approach performs poorlyfor other holidays in our experiments. In this paper weshow that the methodology in [11] can be improved andgeneralized to discover tracks associated with other holi-days throughout the year.59Figure 1. Mean listen ratesRfor tracks above 1,500 totallisten threshold with ”Christmas” in track or album name(solid line) and tracks without (dotted line) for December18, 2012 - January 1, 2013.2. METHODOLOGY[11] hypothesized that the listening signals of two tracks,one associated with a holiday and one not, will have dif-fering and detectable patterns on and around that holiday.This can readily be seen for Christmas tracks and non-Christmas tracks in Figure 1. In this section we show themethodology in [11] for detecting Christmas tracks andpropose improvements.2.1 Listening RatesThe form of the raw data is listening events in which aknown user has listened to a known track at a date andtime. If a track is associated with a day, we expect usersto engage more relative to other time periods. The signalused in [11] can be described as user engagement,Eij=UXk=1cijk(1)which is the total number of listens for all users for trackiin time periodj.In Eqn (1),cijkis an element ofC, andC2RT⇥W⇥UwhereTis the number of tracks,Wis the number of timeperiods, andUis the number of users. To account for dif-ferences between popularity of tracks, theEwas normal-ized across the periods of time as described byRij=EijPWl=1Eil(2)which were the listen rates used to train the Christmasmodel.We propose a new signal based on absolute user engage-ment,ˆE. Given the functionf(x)=(1,ifx>00,otherwise(3)ˆEijis the number of users who listened to a trackiintime periodjand is calculated byˆEij=UXk=1f(cijk).(4)Number of Records4,819,992,847Number of Users1,648,796Number of Tracks13,227,376Date RangeJanuary 2012 - February 2013Table 1. Dataset of listening records.This is similarly normalized across time periods to getnew listen ratesˆRij=ˆEijPWl=1ˆEil.(5)The intuition is to limit the effect of repeat plays amongindividual users. In estimating cultural preferences, thereis likely more information gained when 100 users listento a track once than when one user listens to a track 100times.2.2 DetectionFor detection, [11] ﬁt a multi-variate Gaussian with listenrates of only Christmas tracks with parameters✓Christmascomposed of mean,µRj, and covariance matrix,⌃Rj.Given a new track with listen rates for the same time peri-ods,x=[R1,R2,. . . ,RW], the metric for detection wasP(x|✓Christmas)=WYj=1N(x|µRj,⌃Rj).(6)We propose another metric using the posterior probabil-ity from a direct application of Bayes’ Rule inP(✓c|x)=P(x|✓c)⇤P(c)P(x|✓c)⇤P(c)+P(x|✓n)⇤P(n)(7)wherecsubscript represents Christmas andnsubscriptrepresents non-Christmas. This includes a model for non-Christmas tracks and prior probabilitiesP(c)andP(n),which represent the proportion of Christmas and non-Christmas tracks in matrixC, respectively. The priors inparticular are important because of the small number ofChristmas tracks in the dataset.P(x|✓n)is calculated fromEqn (6) whereµRjand⌃Rjare calculated using all non-Christmas tracks.2.3 DatasetThis study uses the same internal Gracenote dataset ofonline radio listening records in North America as [11].Some basic information is shown in Table 1. Each recordin the dataset represents one listen of a track by one userand provides User ID, Date, Time, and Track ID. Trackmetadata is also available such as track name, album name,and artist name.60 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Min. Listens All Tracks Christmas Tracks1,500 338,406 4,732500 767,116 10,647200 1,397,032 18,170100 2,087,863 26,13410 5,906,307 68,5821 10,207,335 118,515Table 2. Track distribution at each threshold of minimumlistens.3. CHRISTMAS3.1 ExperimentsIn these experiments, we compared the performance of thesignals and prediction metrics in Section 2. As in [11], wegenerated a ground truth of Christmas tracks by searchingfor keyword ”Christmas” in track names and album names.We deﬁned the window radius,rw, as the number of con-secutive days before and after the target holiday, December25, 2012, such that the window lengthW=2⇤rw+1.Since [11] showed an increase in performance with in-creasing popularity of tracks, we use the same thresholdsof minimum listens in the dataset (1,500, 500, 200, 100,10, 1) for direct comparison. Table 2 shows the distribu-tion of tracks for each threshold.For each listen rate in Section 2.1, a matrix was con-structed from the dataset using tracks above the speciﬁedthreshold, all users, andWdays. We variedWby choos-ingrwranging 1 to 30 to capture the signal up to one monthbefore and a month after December 25. The matrix wasrandomized and split into train (60%) and test (40%) setson the ﬁrst dimension. Two single component GaussianMixture Models, for Christmas and non-Christmas, weretrained with the training set in a supervised manner witheach training example a track and features the listen ratesfor each day in the signal window. Classiﬁcation was per-formed with each metric in Section 2.2 on the test set andthe area under the Receiver Operating Characteristic (AU-ROC) was calculated for evaluation.3.2 ResultsFigure 2 and Figure 3 show the AUROC against the win-dow radius for the proposed listen rate,ˆR, and each predic-tion metric. Observing the difference in y-axis scale, themost notable difference between the ﬁgures is an increasein performance across all thresholds and signal lengthsfor the posterior probability. In particular, the lowest twothresholds have quite large increases of about 0.15 at eachsignal length.Among tracks with the strongest listening signals, thereis a small decay with increased window length. In contrast,the weakest listening signals show a large boost in perfor-mance with increased signal length. Similar plots for listenratesRare not shown because they track very closely andmostly just below the trends forˆR. Lastly, Table 3 showsthe maximum AUROC value for each threshold across sig-\nFigure 2. AUROC for each listen threshold for listen rateˆRand prediction metricP(x|✓c)\nFigure 3. AUROC for each listen threshold for listen rateˆRand prediction metricP(✓c|x)nal lengths as a measure of overall performance. The pro-posed signal and prediction metric give the highest AU-ROC for the top four thresholds, and the signal from [11]with the proposed prediction metric have slightly higherAUROC for the bottom two thresholds.3.3 AnalysisThe posterior probability performs better than the likeli-hood because the inclusion of a non-Christmas model pro-vides additional discriminative information. There is a lotof complexity in the non-Christmas tracks that is not mod-eled well by a single Gaussian with a mostly uniform dis-tribution as shown in Figure 1. This suggests that incor-porating models for other common signal shapes such asthose of newly released tracks might further improve per-formance.The signal length effects the performance in differentways. Tracks with the strongest listening signals performbest more with localized time window. We believe this isdue primarily to higher variability of listening rates lead-ing up to Christmas. The Christmas holiday is celebratedProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 61P(x|✓c)P(✓c|x)ThresholdRˆRRˆR1,5000.9870.9910.9890.9925000.9750.9810.9780.9832000.9640.9690.9690.9731000.9500.9560.9580.963100.8510.8500.8920.88810.6800.6820.7840.783Table 3. Best AUROC for any signal window.for many days before, and the signals during this time maybe less stable than nearby December 25th. Tracks with theweakest listening signals perform best with a larger timewindow performs. This is likely because there is more in-formation available with a longer signal, even if a smallamount. Tracks with 50 plays in the dataset average onlyone play in ten days so capturing enough discriminatoryinformation for detection requires a longer signal window.The proposed signal of user counts,ˆR, has a smoothingeffect over the signal of play counts,R, boosting perfor-mance. With tracks of stronger signals this appears to bemore discriminating as shown in Table 3. But tracks withweaker signals, this seems to remove some useful infor-mation, which would explain why the play counts,R, per-form slightly better at the two lowest thresholds. No singleconﬁguration appears to give optimal performance for thistask.4. HOLIDAY GENERALIZATIONWe are interested in detecting track-temporal context as-sociations for many days other than Christmas. Directlyrepeating the procedure in Section 3 for other holidays pro-duced poor results on the dataset in Section 1. We believethis is because the ground truth generated from keywords ismuch less clean. Since many other holidays have a smallermusic repertoire than Christmas, discriminative keywordslike the holiday names generate too few tracks with stronglistening signals for model training. And less discrimina-tive keywords inadvertently include tracks not associatedwith the holiday, similarly compromising training.Instead, the Christmas model in Section 2 can be rein-terpreted as a holiday model with parameters✓holidaycom-posed of the same mean,µRj, and covariance matrix,⌃Rjas Eqn (6). Now a new track with listen rate signal ofthe same length,W, centered on adifferent target holi-day,x=[R1,R2,. . . ,RW], can be detected with Eqn (6)or Eqn (7).4.1 ExperimentsIn these experiments, we show the performance of detec-tion on three other holidays. Since the dataset in Section 1is from users in North America, we chose Halloween, SaintPatrick’s Day, and U.S. Independence Day as they are well-known holidays in North America and likely to have musicassociations. For the best results, we use only tracks withSaint Patrick’s U.S. Independence Halloween95% 73% 99%Table 4. Average precision@50 for holiday track detec-tion.strong listening signals - above 1,500 total listens in thedataset - and the best performing listen rate and predictionmetric from Section 3,ˆRand Eqn (7).We constructed the training set feature matrix usingW= 15, implyingrw=7andˆRi8is the listen ratefor trackion December 25, 2012. Again, we trained twosingle component Gaussian Mixture Models, holiday andnon-holiday, in a supervised manner. We constructed thetest set feature matrix similarly withW= 15, meaningˆRi8is the listen rate for trackion the target holiday.We calculated the probability of each track in the testset with Eqn (7) and ranked the tracks from highest proba-bility to lowest for analysis. We chose precision@k to pro-vide a general measure of relevance of tracks. We set k=50with the assumption that there are at least 50 tracks trulyassociated with each holiday in order to better attribute er-rors in detection to the methodology. Three music contentexperts examined each list and labeled tracks as relevant tothe holiday or not. We averaged the results to get a singlevalue of precision@50 for each holiday.4.2 ResultsTable 4 shows the average precision@50 of holiday trackdetection as indicated by three music experts. Halloweenand Saint Patrick’s Day had high values at 99% and 95%,respectively, and U.S. Independence Day was lower at73%. The mean probability of the all tracks according tothe holiday model was 99.9%. The distribution of incor-rect tracks for U.S. Independence Day is skewed towardthe bottom of the list.The top 10 tracks for each holiday are shown inSection 4.2.1 - Section 4.2.3 to further characterize the re-sults. All of these tracks had a probability of 1.0 accordingto the holiday model. The ordering for each track is trackname, artist name.4.2.1 Top 10 Saint Patrick’s Tracks1.When Irish Eyes Are Smiling, Bing Crosby2.Maloney Wants A Drink, The Clancy Brothers3.Sally MacLennane, The Pogues4.When Irish Eyes Are Smiling, The Irish Folk5.Danny Boy, Irish Drinking Songs6.Water Is Alright In Tay, The Clancy Brothers7.Whiskey In The Jar, The Clancy Brothers8.A Pair of Brown Eyes, The Pogues9.The Black Velvet Band, Irish Drinking Songs10.Grace, Jim McCann4.2.2 Top 10 U.S. Independence Tracks1.America, Barry White2.Independence Day, Elliott Smith62 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20153.Proud to be an American, Tiki4.Stars And Stripes Forever, John Philip Sousa5.Justice And Independence ’85, John Mellencamp6.4th of July, X7.Our Country (Rock Version), John Mellencamp8.America the Beautiful, Blake Shelton and MirandaLambert9.This Is My Country, The Impressions10.God Bless The U.S.A., Lee Greenwood4.2.3 Top 10 Halloween Tracks1.Purple People Eater, Halloween Hit Factory2.”Dr. Who” Theme Song, Mannheim Steamroller3.Graveyard Of The Living Dead, Halloween SoundEffects4.Werewolves - Scary Halloween Sound Effects, Hal-loween Sound Effects5.Dracula’s Organ - Scary Halloween Sound Effects,Halloween Sound Effects6.Creatures Of The Night (Original Mix), MannheimSteamroller7.This Is Halloween, The Countdown Kids8.Hall of Screams - Scary Halloween Sound Effects,Halloween Sound Effects9.Scary Halloween Haunted House, Sound Fx10.Grimly Fiendish (Album Edit Version), The Damned5. ANALYSIS - HOLIDAYThe high values for precision@50, particularly those ofHalloween and Saint Patrick’s, show that a model trainedwith user data around Christmas is effective in identify-ing daily music-temporal context associations. The lowerprecision@50 of U.S. Independence Day and the incorrecttracks being skewed towards the bottom of the list suggeststhat our assumption of at least 50 associated tracks for theholiday may be incorrect. Flaws in the methodology couldalso be the cause.In particular, the assumption that Christmas listeningsignals have a distribution that matches those other holi-days closely is likely ﬂawed. Looking at Christmas signalin Figure 1 and the Saint Patrick’s signal in Figure 4, theyare similar but do not match exactly. The Christmas trackshave two peaks, December 24 and December 25, and theSaint Patrick’s tracks have a single peak on March 17. Withshorter signal lengths, this difference is pronounced andgives poor results for detecting other holiday tracks. Thisis why the experiments in Section 4.1 usedrw=7, andnot the optimal from Section 3,rw=3.Among the incorrect U.S. Independence tracks, nearlyone-third were from a single album by electro-punk bandFrittenbude. This highlights other possible reasons for in-creased engagement such as marketing pushes. This albumappears to have been released in the summer of 2012 andthe synchronized rise and fall of the album’s initial listen-ing could be one explanation. In this case and other one-time events, album releases happen just once and couldbe separated from the more cyclical holiday listening withmultiple years of data.\nFigure 4. Mean listen ratesRfor top 100 predicted SaintPatrick’s Day tracks from March 10, 2012 to March 24,2012.The effectiveness of general holiday detection im-plies one obvious commercial application: an automated,”always-on” seasonal radio station. With multiple years ofdata, the results likely could be improved and characterizedby their change over time. Also, the addition of locationdata could highlight geographical differences for improvedrecommendations. For example, since our dataset is pri-marily North America, the tracks in Section 4.2.1 may bepoor recommendations to users celebrating Saint Patrick’sin Ireland or other parts of the world.6. FUTURE WORKThe issues with matching the signal shapes of Christmastracks to other holidays suggest room for improvement.Artiﬁcial templates or hand labeling a holiday ground truthcould estimate the target distributions more accurately. Al-though labeling track-temporal context associations withuser data has advantages over the other automated methodsas outlined in Section 1, combining these methods couldproduce superior results. Lastly, applying this method-ology at additional time resolutions (e.g hours, weeks,months) or exploring how these contexts interact with userdata (e.g. age, geography, personality) could further enrichthe user listening experience.7. CONCLUSIONThis study showed improvements to previous method fordetecting Christmas tracks from user listening data andgeneralized the method to detect tracks for other holidays.The proposed improvements showed small increases ofabout 0.01 maximum AUROC for the most popular tracksbut larger improvements of about 0.1 maximum AUROCfor less popular tracks. Detection of Halloween, SaintPatrick’s Day, and July 4th tracks was promising with pre-cision@50 at 95%, 99%, and 73%, respectively.8. REFERENCES[1]Christmas comes early to TheEcho Nest (The Echo Nest Blog)http://blog.echonest.com/post/35845347430/christmas-comes-early-to-the-echo-nest[2]Top 50 Love Songs of All Time (Billboard)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 63http://www.billboard.com/articles/list/1538839/top-50-love-songs-of-all-time[3]Last.fm (Last.fm) http://www.last.fm/charts/toptags[4]P. Lamere. ”Social Tagging and Music Information Re-trieval.”Journal of New Music Research, Vol. 37 No. 2pp. 101-114, 2008.[5]Reis, Ben Y., Marcello Pagano, and Kenneth D. Mandl.”Using temporal context to improve biosurveillance.”Proceedings of the National Academy of Sciences100.4 (2003): 1961-1965.[6]Kulkarni, Anagha, et al. ”Understanding temporalquery dynamics.” Proceedings of the fourth ACM in-ternational conference on Web search and data mining.ACM, 2011.[7]M. Shokouhi: Shokouhi, Milad. ”Detecting seasonalqueries by time-series analysis.” Proceedings of the34th international ACM SIGIR conference on Re-search and development in Information Retrieval.ACM, 2011.[8]Park, Chan Ho, and Minsuk Kahng. ”Temporal dynam-ics in music listening behavior: A case study of on-line music service.” Computer and Information Science(ICIS), 2010 IEEE/ACIS 9th International Conferenceon. IEEE, 2010.[9]Carneiro, Mrio Joo Teixeira. ”Towards the discoveryof temporal patterns in music listening using Last.fmproﬁles.” Dissertation, 2012.[10]Hidasi, Balzs, and Domonkos Tikk. ”Context-awarerecommendations from implicit data via scalable ten-sor factorization.” arXiv preprint arXiv:1309.7611,2013.[11]C. Summers, P. Popp. ”Large Scale Discovery of Sea-sonal Music with User Data.”User Modeling, Adapta-tion and Personalization: 23rd International Confer-ence, UMAP 2015, Extended Proceedings. Springer,2015.64 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Relating Natural Language Text to Musical Passages.",
        "author": [
            "Richard F. E. Sutcliffe",
            "Tim Crawford",
            "Chris Fox",
            "Deane L. Root",
            "Eduard H. Hovy",
            "Richard Lewis 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415270",
        "url": "https://doi.org/10.5281/zenodo.1415270",
        "ee": "https://zenodo.org/records/1415270/files/SutcliffeCFRHL15.pdf",
        "abstract": "There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long- term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made.",
        "zenodo_id": 1415270,
        "dblp_key": "conf/ismir/SutcliffeCFRHL15",
        "keywords": [
            "musicological literature",
            "musical passages",
            "MusicXML score",
            "scholarly texts",
            "shared evaluation",
            "passages identification",
            "MusicXML",
            "Natural language phrases",
            "passages reference",
            "Musicological analysis"
        ],
        "content": "Relating Natural Language Text to Musical Passages Richard Sutcliffe Tim Crawford Chris Fox School of CSEE University of Essex Colchester, UK rsutcl@essex.ac.uk Dept of Computing Goldsmiths, University of London t.crawford@gold.ac.uk School of CSEE University of Essex Colchester, UK foxcj@essex.ac.uk Deane L. Root Eduard Hovy Richard Lewis Department of Music University of Pittsburgh Pittsburgh, PA, USA dlr@pitt.edu Lang Technologies Inst Carnegie-Mellon Univ Pittsburgh, PA, USA hovy@cmu.edu Department of Computing Goldsmiths, University of London richard.lewis@ gold.ac.uk ABSTRACT There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long-term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe t h e  first two years of the evaluation and finally appraise the results to establish what progress we have made. 1. INTRODUCTION A traditional Information Retrieval (IR) system takes as input a short textual query and a document collection and returns a list of documents which match the query [27]. By combining IR with Natural Language Processing (NLP) the field of Question Answering was born [13], leading to systems which could take a query as input and produce an exact answer [17-20,24]. In the meantime, Music Information Retrieval (MIR) has become a very active area in which various kinds of query are matched against music recordings or electronic forms of score such as MEI [11] (inspired by TEI [25]) or MusicXML [15]. However, music involves text as well as scores; there is a vast body of textual information concerned with Western classical music. First and foremost, Grove’s Dictionary of Music and Musicians has developed from a four-volume printed dictionary published in 1879-1889 into Grove Online which contains around 50,000 signed articles and 30,000 biographies contributed by over 6,000 scholars [6]. In addition, there are countless scholarly books, journal articles and conference papers as well as numerous online sources such as the Wikipedia. All these sources contain detailed analyses of musical works which necessarily make reference to specific passages in scores. Our long-term objective is to investigate whether these references – expressed in a natural language such as English – can be automatically matched to the musical passages to which they refer. In pursuit of our objective we organised in 2014 [23, 10] and 2015 [to appear] shared evaluations called C@merata (Cl@ssical Music Extraction of Relevant Aspects by Text Analysis) – http://csee.essex.ac.uk/ camerata/ – in which a number of participants each built a system which could take as input a question in English and a score in MusicXML and identify one or more passages in the score which matched the question. We describe those evaluations and the rationale behind them. We first outline the background to this work and its origins in Question Answering (QA). Second, we present an analysis of text examples, taken from the writings of three important musicologists, which refer to musical passages. Third and Fourth we describe the two C@merata campaigns. Finally we discuss what we have learned and draw some conclusions. 2. BACKGROUND TO OUR EVALUATIONS Our work is derived from three existing areas of research. First, the considerable body of MIR work concerned with finding passages in music scores based on inputs of various kinds, e.g. [5]. Secondly, the Music Information Retrieval Evaluation Exchange has been organised by J. Stephen Downie since 2005 [4,12]. These landmark evaluations have been concerned with many different tasks over the years and are related to parallel evaluations concerning IR and NLP at TREC [26], CLEF [1] and NTCIR [16]. While MIREX has often been concerned with audio-based systems, it has regularly featured score-based tasks which, in the light of our work, could be combined with natural \n © R. Sutcliffe, T. Crawford, C. Fox, D.L. Root, E. Hovy andR. Lewis. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: R. Sutcliffe, T. Crawford, C. Fox, D.L. Root, E. Hovy and R. Lewis. “Relating Natural Language Text to Musical Passages”, 16th International Society for Music Information Retrieval Conference, 2015. 524   language input. Thirdly, there have been QA tracks at CLEF, starting in 2003 [20]. However, these were not concerned with music until 2011. In that year, the Question Answering for Machine Reading (QA4MRE) task featured difficult multiple-choice questions in four domains, one being Music and Society [18]. Four documents in this domain were used, each taken from transcripts of talks delivered at the TED Conferences. In the 2012 task [19], the music texts used were drawn from Wikipedia, Project Gutenburg and the 1911 Encyclopedia Britannica. Finally, in 2013, the four documents were taken with permission from Grove Online [6]. This gave us the idea of combining text processing with core processing. 3. REFERENCES TO SCORES IN MUSIC TEXTS In this section, we motivate our work by providing a short description of the references to musical passages in three important text sources. The first is an analysis of the Beethoven Symphonies by Antony Hopkins (Chapter 2: Symphony No. 1 in C Major Op. 21) [7] (henceforth ah). The second is the study of Domenico Scarlatti by Ralph Kirkpatrick (Chapter 10: Scarlatti's Harmony, Section Cadential vs. Diatonic Movement of Harmony) [9] (henceforth rk). The third is the entry for Anton Bruckner by Deryck Cooke (Section 7. Music) [2] from the New Grove Dictionary of Music and Musicians [21] (henceforth dc). We extracted phrases from the above works by hand − 261 in all − and organised them into 14 categories: notes, intervals, scales, melodies, rhythms, tempi, dynamics, keys, harmony, counterpoint, texture & instrumentation, bar numbers, passages & sections and structures & sequences. Furthermore, they are classed as Specific or Vague. Examples of each category can be seen in Table 1, with two Specific and two Vague for each phrase type. The source is indicated in square brackets: [ah26] means ah (i.e. Hopkins) p26; [dc364lh] means dc (i.e. Cooke) p364 in Grove, left hand column. It is important to note that the categories in Table 1 are for illustration only and are neither exhaustive nor mutually exclusive. The examples are given purely to illustrate the kinds of references to musical passages which one might find in a musicological text. Moreover, the binary categorisation into Specific and Vague is also purely for illustration purposes as specificity lies on a scale. We now draw some conclusions from this table. The first point to note is that the references vary i n  specificity; some are clear and unambiguous (C#-D rising semitone, D major, eight-part choir, bars 189-198); others are much more difficult to pin down (alien F#, disturbing syncopations, anguished D minor chromaticism, varied alternation of two long-drawn themes). Secondly, however, all the phrases are meaningful – an expert familar with the works concerned is likely to be able to identify the points mentioned in the score with a fair accuracy (high Precision even if not necessarily high Recall). This suggests that they are interesting and worthwhile to study. Thirdly, some categories of phrase lend  themselves  to Category S/V Examples Notes S [ah26] giant unison G from the entire orchestra [rk220] based on nothing else but A, D, E, and A V [ah12] alien F# in the ascending scale [dc364lh] pedal point Intervals S [ah24] C#-D rising semitone [dc363lh] an ascending diminished fifth V [ah19] fragment of five rising crotchets [dc364lh] themes based on falling octaves Scales S [dc363lh] parts entering successively on the degrees of the ascending scale of D major [dc363rh] old church modes ... Phrygian and Lydian V [ah28] the initial scale [ah29] little scales dart to and fro Melodies S [ah13] semiquaver descent in bar 18 [ah19] fragment of five rising crotchets V [ah19] Second Subject appearing in the tonic key [dc363rh] the chorale themes in the symphonies Rhythms S [ah15] quaver pattern [ah25] repeated crotchet chords V [ah18] disturbing syncopations [dc364lh] hammering ostinatos Tempi S [ah11] slow tempo [dc366lh] slow movements V [ah28] rustic oom-pah bass [dc364rh] intense and long-drawn string cantabile Dynam- ics S [ah26] violins in bar 126 come in FF [ah29] sudden fortissimo outburst V [ah29] sudden roaring [dc364lh] murmering tremolando Keys S [ah10] D major [dc366lh] in Bb minor V [rk221] modulatory excursion of the second half [dc363rh] unusual key changes Harmony S [rk221] major dominant [dc364lh] tonic triad of E major V [rk220] departure from three-chord harmony [dc363lh] anguished D minor chromaticism Counter- point S [ah23] cellos provide a delicate countertune [dc363lh] parts entering successively on the degrees of the ascending scale of D major V [rk220] dominated by diatonic movement of parts [dc363lh] bold polyphonic imitation of a single point Texture, Instru- menation S [dc363lh] eight-part choir [dc363rh] a piece of unison plainsong V [ah29] decked with garlands of scales from flutes, clarinets and bassoons [dc364rh] a faint background sound, emerging almost imperceptibly out of silence Bar Numbers S [ah15] bars 189-198 [rk220] measure thirteen to measure fifteen V [ah24] sixteen or at most thirty-two bars long [dc365rh] over periods of 16, 32 or even 64 bars Passages, Sections S [dc363rh] whose slow movement and finale [dc364rh] far-ranging first movement V [rk220] series of small sequential passages [dc362rh] a passage from the Gloria Structures, Sequences S [ah18] First Subject [rk221] Phrygian cadence V [dc365rh] exposition (nearly always built on three subject groups rather than two) [dc366rh] varied alternation of two long-drawn themes Table 1. F ou rteen  ty pes  of  ref erring  ex press ion s, categorised into Specific (S) and Vague (V). rather simple and clear expression. Examples include Notes (G), Intervals (ascending diminished fifth), Scales (D major), Rhythms (repeated crotchet chords), Dynamics (FF), Keys (Bb minor) and bar numbers (measure thirteen). If we set ourselves the task of Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 525   searching for such passages in a score, we are likely to be quite successful. Fourthly, some categories of phrase tend conversely to be complex and often imprecise as well. Examples include Texture & Instrumentation (a faint background sound, emerging almost imperceptibly out of silence), Passages  &  Sections  (a  passage  from  the  Gloria)  and Structures & Sequences (exposition (nearly always built on three subject groups rather than two)). Western classical music excels in structure and in harmony, s o  treatment of these topics tends to be particularly interesting and important. The richness and ambiguity of language are its strengths in this context as a great deal can be suggested in relatively few words. Moreover, t o  the expert, the references remain quite clear, though a considerable amount of knowledge and background information is being brought to bear. Fifthly, it is interesting to observe that many of the examples in Table 1 are noun phrases; this construct can express very complicated and detailed concepts in a musicological text. Sixthly and finally, phrases in natural language can never be replaced by expressions in a pattern language (such as regular expressions applied over text strings). Such expressions are by their nature unambiguous and in practical contexts they are usually concise. Therefore, the study of natural language in musicology is not made unnecessary by the existence of such languages. On the other hand, such expression languages are extremely useful and worthwhile [28]; one possible application of them here is to map a natural language phrase onto a pattern (possibly extremely complex) in such an expression language in order to initiate a search. In the next section we will describe our evaluations. 4. THE 2014 C@MERATA TASK 4.1 Input Provided In a QA evaluation such as ResPubliQA [17], the input is normally a short question such as ‘Who is President o f  the United States’ and the output is an exact answer such as ‘Barack H. Obama’. As we have discussed earlier, many of the real examples in Table 1 are in fact noun phrases. So it seemed reasonable to use a noun phrase as the input for an initial evaluation, rather than a complete question. The top of Table 2 shows the question types which were adopted. For all the types mentioned below, there are several examples in the right hand column. As we observed above, entries in the Notes category of Table 1 are some of the simplest and clearest. As this was a new task, it was decided to include three simple query types in the evaluation which correspond broadly to Note: simple_pitch, simple_length and pitch_and_length. Perf_spec queries combine a note with some performance indication. Stave_spec queries restrict t h e  answer to a particular stave in the score which may b e  specified in various ways, including the instrument concerned, the hand being used (for keyboard music) or the clef on which the music appears. Similarly, word_spec queries link a note to the word which is sung on it in one of the parts. Question Types for 2014 Task Type No Examples simple_pitch 30 G5, E, A natural, C flat, F#4, F2 sharp simple_length 30 dotted quarter note, quarter note rest, semiquaver rest, whole note, semibreve pitch_and_length 30 D# crotchet, half note C, quarter note B5, semiquaver G#, half note Db, quaver F# perf_spec 10 D sharp trill, fermata A natural, staccato B flat, marcato D flat, F trill, down bow E stave_spec 20 D4 in the right hand, half note D in the viola, treble clef A sharp, F3 sharp in the ″alt″, quarter note F in the Alto word_spec 5 word \"Se\" on an A flat, minim on the word ″Der″, minim B on the word ″im″, G on the word ″praise″ followed_by 30 crotchet followed by semibreve, D followed by G, quarter note G followed by eighth note G, dotted quaver E followed by semiquaver F sharp, crotchet rest followed by crotchet, dotted quarter note followed by A4 melodic_interval 19 melodic octave, rising major sixth, melodic descending fifth, falling major third, melodic rising minor third, octave leap, falling tone, melodic fourth harmonic_interval 11 harmonic major sixth, harmonic second, nineteenth, seventh, harmonic fifth, harmonic octave, major seventeenth cadence_spec 5 perfect cadence triad_spec 5 tonic triad, Ib triad, triad in first inversion, Ia triad texture_spec 5 polyphony, melody with accompaniment, monophony, homophony All 200  Question Types for 2015 Task Type No Examples 1_melod 40 D4 minim, eighth note in measure 9 1_melod qualified by perf, instr, clef, time, key 40 trill on a quaver A; G# in the Cello part in measures 29-39; sixteenth note C# in the left hand; half note E3 in 2/2; sixteenth note G in G minor in measures 1-5 n_melod 20 F# E G F# A; Do Mi Do Sol Do Mi Sol Do in bars 1-20; twenty semiquavers; five note melody in bars 1-10 n_melod qualified by perf, instr, clef, time, key 20 two staccato quarter notes in the Violin 1; crotchet, crotchet rest, crotchet rest, crotchet, crotchet rest, crotchet, crotchet, crotchet, crotchet, crotchet in the Timpani; melodic octave leap in the bass clef in measures 70-80; G4 B4 E5 in 3/4; rising G minor arpeggio 1_harm possibly qualified by perf, instr, clef, time, key 20 eighth note chord Bb, C, E; chord of D minor in measures 109-110; harmonic minor sixth in the Violas; dotted minim chord in the left hand texture 6 monophonic passage; homophony in measures 1-14; polyphony in measures 10-14; Alberti bass in measures 0-4 follow possibly qualified on either or both sides by perf, instr, clef, time, key 40 quavers F4 E4 in the oboe followed by quavers E2 G#2 in the bass clef; quarter note minor third followed by eighth note unison; C followed by mordent Bb; chord C4 G4 C5 E5 then a quaver; three eighth notes in the Violin I followed by twelve sixteenth notes in the Violin II in measures 87-92 synch possibly qualified in either or both parts by perf, instr, clef, time, key 14 four eighth notes against a half note; crotchet D3 on the word “je” against a minim D2; four staccato quavers in the Violoncello against a minim chord Ab3 C4 F4 in the Harpsichord All 200  Table 2. Summary of question types in tasks. 526 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   \n Q:  G flat A:  [ 4/4, 2, 67:5-67:5 ], [ 4/4, 2, 71:2-71:2 ] Q: semibreve A:  [ 4/4, 1, 76:1-76:4 ] Q:  minim F A:  [ 4/4, 1, 67:1-67:2 ] Q:  minim C in the bass A:  [ 4/4, 1, 72:1-72:2 ], [ 4/4, 1, 72:3-72:4 ] Q:  crotchet followed by semibreve A:  [ 4/4, 1, 75:4-76:4 ] Q:  melodic octave A:  [ 4/4, 2, 69:5-69:8 ], [ 4/4, 2, 72:1-72:8 ], [ 4/4, 2, 73:5-73:8 ],  [ 4/4, 2, 74:5-74:8 ], [ 4/4, 2, 75:5-75:8 ] Figure 1. Extract from Scarlatti K466 with questions and answers from the 2014 task. So far, all the query types are simple notes in isolation. Queries of type followed_by specify two adjacent notes. As Table 1 showed, intervals are discussed in real texts, so we wished to include some queries of this type. We divided them into two kinds, melodic and harmonic. melodic_interval specifies two adjacent notes on the same stave which are a specified distance apart. Conversely, a harmonic_interval specifies two simultaneous notes. Unlike melodic intervals, harmonic intervals were permitted to occur across staves because they are integral to the concept of harmony which is often created by instruments or voices in different parts. Intervals a r e  considered harmonic by default, thus ‘fifth’ is assumed to be a harmonic fifth. The last three question types were more experimental, though still being relatively straightforward and unambiguous in musical terms. cadence_spec requires a  cadence to be identified. A triad_spec specifies triads in various forms of notation. Finally, texture_spec states the required texture to be found. Referring back to Table 1, cadences touch upon Structures & Sequences and Triads are a fundamental element of Harmony. There were 200 queries in a fixed distribution as shown in the middle column of Table 2. The four simplest query types (simple_pitch, simple_length, pitch_and_length, followed_by) were the most numerous in the test set with 30 each. After this came stave_spec and melodic interval with twenty each followed by perf_spec and harmonic_interval with ten each. (One melodic interval was changed for a harmonic_interval at a  late  stage,  so  in fact there were nineteen of the former  \n Q:  dotted minim F#4 A:  [ 3/4, 1, 65:1-65:3 ] Q: F4 crotchet in the oboe A:  [ 3/4, 2, 64:3-64:4 ] Q:  minim A2 in 3/4 time A:  [ 3/4, 1, 62:2-62:3 ], [ 3/4, 1, 64:2-64:3 ] Q:  chord D2 E5 G5 in bars 54-58 A:  [ 3/4, 2, 57:1-57:1 ] Q:  quavers F3 A3 followed by crotchet A4 in the violin A:  [ 3/4, 1, 57:2-57:3 ] Q:  four quavers in the violin against a minim in the bass clef A:  [ 3/4, 1, 62:2-62:3 ], [ 3/4, 1, 64:2-64:3 ] Figure 2. Extract from Bach BWV1047 Andante with questions and answers from the 2015 task. and eleven of the latter.) Finally, there were five each of word_spec, cadence_spec, triad_spec and texture_spec. Thus some more experimental types of query were represented in the task but played a relatively minor role. In summary, most of the question types used in 2014 were straightforward and were derived from Notes, Intervals and (partly) Harmony, Texture & Instrumentation and Structures & Sequences. Other phrase types of Table 1 were not catered for. 4.2 Output Required As we have seen, an input query was simply a short noun phrase. To make the evaluation as simple as possible, an answer was defined to be a subsection of a score, starting and ending at a particular place. The answer was not required to specify which stave (or staves) contained the answer. Initially, we planned to measure beats in a bar in terms of the shortest note (hemidemisemiquaver, one sixteenth of a crotchet). However, this does not allow for triplets (where, say, a crotchet is divided into three) or any other sort of n-tuplet. So instead, we adopted the concept of divisions f r o m  M u s i c X M L .  T h e  d i v i s i o n s  v a l u e  i s  t h e  number of beats into which the crotchet is divided. A  suitable value depends on what we wish to demarcate as an answer. So for simplicity, we specified for each query the divisions value to be used for the answers. Based on these ideas we developed the concept of a passage w h i c h  w o u l d  c o n t a i n ,  f o r  b o t h  s t a r t  a n d  e n d ,  a  time signature, a divisions value, and a bar and beat. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 527   The start bar and beat is where the passage is defined to commence. More precisely, the passage begins in the denoted bar immediately before th e s tart beat, m easu red from the beginning of the bar in the unit of time denoted by the stated divisions value. Similarly, the passage is defined to end immediately after t h e  e n d  b e a t .  W e  adopted this before-the-start and after-the-end after careful thought and discussion. The advantage of it is that it is intuitive: As can be seen in Figure 1, above, the first two crotchets in bar 67 are denoted 67:1-67:2 which can be understood at a glance. We developed three equivalent ways of stating a passage: Ascii Long Form, Ascii Short Form and XML form. The Ascii forms are convenient for discussions in papers etc. while the XML form is useful as the input to, and output from programs. Here is an example in short form: [4/4,1,1:1-2:4]. The time signature is 4/4 and divisions value is 1. The passage starts in bar 1 before the first crotchet (i.e. 1:1) and ends in bar two after the fourth crotchet (i.e. 2:4). We take bar numbers from the MusicXML score. We use the XML format for specifying the test queries for participants as well as for the queries plus correct answers (often called the Gold Standard in QA).  In summary, our passage specifies two vertical lines drawn through the score and does not distinguish between the different staves. We thus assume that any answer can be exactly demarcated in this way. We will return to this point in the conclusions. 4.3 Evaluation Precision, Recall and F-Measure are commonly used in IR and NLP [27]. We wished to determine all the correct answer passages by hand to produce a Gold Standard and then to compare the results returned by a system to that. It is useful to have both strict and lenient measures in an evaluation. At the fourth TREC QA track onwards  (starting in 2002) there were four judgements of each answer, Right, ineXact, Unsupported and Wrong [29]. In the TREC context a correct answer could be ‘Bill Clinton’ while an ineXact one could be ‘Clinton’ or perhaps ‘Bill Clinto’. Unsupported answers were Right but not shown to be so from a document in the collection. We decided that a passage returned which began at the right bar and beat within the bar and also ended at t h e  right bar and beat within the bar was correct. On the other hand, an answer which started and ended at the right bar (but not necessarily the right beat in the bar) was s t i l l  very useful and could be considered the equivalent of TREC’s ineXact. If an expert is looking for a particular cadence, for example, and is told the bar numbers, they can usually see it at a glance. However, searching through hundreds of bars looking for the cadence is time consuming. The concept of Unsupported is not applicable to our task. The measures were thus defined as follows: Beat Precision ( B P )  i s  t h e  n u m b e r  o f  b e a t - c o r r e c t  passages returned by a system divided by the number of passages (correct or incorrect) returned. Beat Recall ( B R )  i s  t h e  n u m b e r  o f  b e a t - c o r r e c t  passages returned by a system divided by the total number of answer passages known to exist. 2014 Results BP BR BF MP MR MF Maximum 0.713 0.904 0.797 0.764 0.967 0.854 Minimum 0.113 0.150 0.185 0.155 0.154 0.226 Average 0.420 0.654 0.483 0.460 0.734 0.534 2015 Results BP BR BF MP MR MF Maximum 0.817 0.739 0.620 0.817 0.809 0.656 Minimum 0.061 0.175 0.108 0.073 0.175 0.129 Average 0.351 0.564 0.348 0.370 0.619 0.375 Table 3. Results of the 2014 & 2015 tasks. As is usual, Beat F-Score (BF) is the harmonic mean of BP and BR. Measure Precision (MP) is the number of bar-correct passages returned by a system divided by the number of passages (correct or incorrect) returned. Measure Recall ( M R )  i s  t h e  n u m b e r  o f  b a r - c o r r e c t  passages returned by a system divided by the total number of answer passages known to exist. Finally, Measure F-Score (MF) is the harmonic mean of MP and MR. 4.4 Scores After consideration of several notations including kern [8], MusicXML was chosen because it is widely used and is supported by music21 [3] and musescore [14]. Twenty MusicXML scores were used and ten questions were set on each, forming the question type distribution of Table 2. We incorporated both European (crotchet, bar etc) and American (quarter note, measure etc) terms into the task by setting American queries for ten of the twenty scores and English queries for the rest. Scores for 2014 were chosen from the Renaissance and Baroque in order to avoid more heavily-scored works from the Classical period onwards. The composers chosen were Bach, Carissimi, Charpentier, Corelli, F. Cutting, Dowland, Lully, Monteverdi, Purcell, A. Scarlatti, D. Scarlatti, Tallis, Telemann, Vivaldi and S. L. Weiss. Scores were chosen on a predefined distribution: six on two staves, six on three staves, four on one stave and two each on four staves and five staves.  There were works for solo cello, harpsichord and lute; one, three, four and five voices; soprano or cello and harpsichord; two violins and cello; two violins, viola and two cellos. The scores were obtained from two sources. Most came from musescore.com. Two Bach chorales were used and both came from www.jsbchorales.net. We required scores to have a license ‘to share’ rather than just ‘for personal use’. Moreover, we required scores to be well presented, transcribed in a scholarly manner and provided in valid MusicXML Version 2 or lower. 4.5 Questions Each score was sent to one of the organisers who was asked to set questions according to the target distribution of Table 2. It was specified for each score whether t h e  questions were to be in American or English. For each question, answers were to be provided in the Ascii short form for specifying passages. The organiser in question 528 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   was asked to find all answers for all the questions. The question data was returned in an Ascii format which incorporates the score filename, the questions, the answers in Short Ascii form and also any comments concerning the questions or answers. On receipt of the files, the questions and answers were checked by a second expert who noted any changes or observations using comments in the Ascii file. The second expert also carried out an independent search for answer passages within the scores. When all changes were checked and validated, the complete set of twenty Ascii files was transformed automatically into XML format in order to form the Gold Standard for the task.  4.6 Participants, Runs and Results The task was announced in January 2014. Five participants registered; two were from Ireland and the other three came from Australia, England and India. Participants had one week to complete their runs starting from 16th June 2014. Each participant was allowed to submit up to three runs. The overall results are shown in Table 3. The best  BF (strict) score was 0.797 which was remarkably good. Averages for BF and MF are 0.483 and 0.534 so systems scored better under lenient measures than under strict measures but the difference is not large − only 11%. Concerning the top run, the difference between MF=0.854 and BF=0.797 is only 7%. So if a system finds the correct bar, it tends to find the exact beat in the bar as well. Generally, the average figures suggest that participants had all made a very good attempt at building a system for this very complicated task.  4.7 Approaches to the Task Concerning software, most participants opted to use Python and to adapt a baseline system using music21 [3] which we wrote and distributed [22]. Others used their own tools in Lisp or C. Only basic NLP was used. Typically the query was scanned looking for terms (e.g. down bow) and converting them to concepts (down_bow). Some systems adopted a QA approach and assigned the query to a pre-define set of types, each with its method of solution. Others converted the concepts to a structured representation by parsing the concepts. The final stage was a search of the score. Some varied the representation of the score according to the query type (e.g. using music21 chordify for cadence questions). As all answers to a given query were defined to lie in exactly one of the scores, no one opted to use any inverted indexing of the music data. 5. THE 2015 C@MERATA TASK 5.1 Changes from 2014 This year’s campaign has just concluded. The use of MusicXML scores, the XML formats for questions and answers, the passage concept and the evaluation measures remained the same in 2015. However, there was a wider range of score types from the Renaissance to the early Romantic periods, scores were more complicated – up to nineteen staves – and questions were differently organised and generally more difficult (see Table 2). For example, an n_melod question can specify quite complicated melodies while the synch type can link two simultaneous features. 5.2 Participants, Runs and Results The same five participated as in 2014. The maximum BF was 0.620 and the average BF was 0.348 (Table 3), both lower than last year. However, the task was considerably harder and the participants did very well.  6. DISCUSSION AND CONCLUSIONS First, in both years, participants were able to build a working system and submit valid runs. Second, all systems could make a good attempt at answering at least one of the question types.  Third, the best systems (see Table 3) achieved very good results and several others were not far behind. Fourth, the technical basis of the task was shown to be sound and all the steps of the campaigns were fulfilled. Fifth, the development of strict measures (BP, BR, BF) and lenient measures (MP, MR, MF) specifically for this task worked well. Sixth, the ability to evaluate runs automatically showed the practicality and scalability of the evaluation. There were also some shortcomings; first, our passage concept does not distinguish between staves. Suppose a minim F starts in the first beat of bar 1 in the treble clef and in the second beat of bar 1 in the bass clef (of a keyboard work). The two answer passages thus overlap which is anomalous. On the other hand, consider a texture such as homophony where some instruments have rests for some or all of the passage − are those instruments part of the passage or not? Second, not all passages of interest in a score can be demarcated exactly. For example, a polyphonic passage may commence in a madrigal when a homophonic section is still drawing to a close. If we say ‘most’ parts must be participating in polyphony is that the start of it, or must ‘all’ participate? Also, what about the start and end of a triad? Sometimes the bass note is only established after the other notes. Third, some ‘passages’ may turn out to have no length. Consider the perfect cadence. The V chord can be set up in many different ways such that it can be hard to say where exactly that chord starts. Then, the onset of th e I chord can be equally ambiguous: there may be a trill on the V; or the I in the treble may be set up either before or after the bass moves to I. For the future, we would consider defining a perfect cadence as a point in a score, not a passage; the instant the bass moves from V to I. Finally, consider again Table 1 (real phrases) against Table 2 (actual phrases used in 2014 and 2015). There is a considerable difference in complexity and subtlety. Many of our queries were simple notes which present few problems for either NLP or MIR. Future campaigns can include more complex query types which delve further into the subtleties of musical language while still b e i n g  practical for use in MIR. Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 529   7. REFERENCES [1] CLEF (2014). http://www.clef-initiative.eu/. [2] Cooke, D. (1995). Bruckner, (Joseph) Anton. In S. Sadie (ed), New Grove Dictionary of Music and Musicians, Volume 3, Section 7. Music (p362-366). London, UK: Macmillan. [3] Cuthbert, M. S., & Ariza C. (2010). music21: a toolkit for computer-aided musicology and symbolic music data. Proc. International Symposium on Music Information Retrieval (Utrecht, The Netherlands, August 09 - 13, 2010), p637-642. [4] Downie, J. S. (2008). The Music Information Retrieval Evaluation Exchange (2005-2007): A window into music information retrieval research.Acoustical Science and Technology 29 (4): 247-255. Available at: http://dx.doi.org/10.1250/ast.29.247 [5] Ganseman, J., Scheunders, P., & D'haes, W. (2008). Using XQuery on MusicXML databases for musicological analysis. Proc. International Symposium on Music Information Retrieval, p433-438. [6] Grove Music Online (2015). http://www.oxfordmusiconline.com/public/ [7] Hopkins, A. (1982). The Nine Symphonies of Beethoven. London: Pan Books. [8] Huron, D. (1997). Humdrum and Kern: Selective Feature Encoding. In ‘Beyond MIDI’, ed. E. Selfridge-Field (p375-401). Cambridge, MA: MIT Press. [9] Kirkpatrick, R. (1953). Domenico Scarlatti. Princeton, NJ: Princeton University Press. [10] Larson, M., Riegler, M. A., Miro, X. A., Korshunov, P., Petkos, G., Soleymani, M., Choi, J., Schedl, M., Ionescu, B., Eskevich, M., Jones, G., & Sutcliffe, R. F. E. (2014). Proc. MediaEval 2014 Workshop, Barcelona, Spain, October 16-17 2014. http://ceur-ws.org/Vol-1263/. [11] MEI (2014). Music Encoding Initiative. http://music-encoding.org/home. [12] Mirex (2014). http://www.music-ir.org/mirex/wiki/ MIREX_HOME [13] Mollá, D., & Vicedo, J. L. (2007). Question answering in restricted domains: An overview. Comput. Linguist., 33(1):41-61. [14] Musescore (2014). Music Composition and Notation Software. http://musescore.org/. [15] MusicXML (2014). http://www.musicxml.com/. [16] NTCIR (2014). http://research.nii.ac.jp/ntcir/index-en.html. [17] Peñas, A., Forner, P., Sutcliffe, R., Rodrigo, A . ,  Forascu, C., Alegria, I., Giampiccolo, D., Moreau, N., & Osenova, P. (2009). Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation Notebook of the Cross Language Evaluation Forum, CLEF 2009, Corfu, Greece, 30 September - 2 October. [18] Peñas, A., Hovy, E., Forner, P., Rodrigo, A., Sutcliffe, R., Forascu, C., Sporleder, C. (2011). Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation. Proc. QA4MRE-2011. Held as part of CLEF 2011. [19] Peñas, A., Hovy, E., Forner, P., Rodrigo, A., Sutcliffe, R., Sporleder, C., Forascu, C., Benajiba, Y., Osenova, P. (2012). Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation. Proc. QA4MRE-2012. Held as part of CLEF 2012. [20] Peñas, A., Magnini, B., Forner, P., Sutcliffe, R . ,  Rodrigo, A., & Giampiccolo, D. (2012). Question Answering at the Cross-Language Evaluation Forum 2003-2010. Language Resources and Evaluation Journal, 46(2), 177-217. [21] Sadie, S. (eds) (1995).  The New Grove Dictionary of Music and Musicians. London, UK: Macmillan. [22] Sutcliffe, R. F. E. (2014). A Description of the C@merata Baseline System in Python 2.7 for Answering Natural Language Queries on MusicXML Scores. University of Essex Technical Report, 21st May, 2014. [23] Sutcliffe, R. F. E., Crawford, T., Fox, C., Root, D. L., & Hovy, E. (2014). The C@merata Task at MediaEval 2014: Natural language queries on classical music scores. Proc. MediaEval 2014 Workshop, Barcelona, Spain, October 16-17 2014. http://ceur-ws.org/Vol-1263/. [24] Sutcliffe, R., Peñas, A., Hovy, E., Forner, P., Rodrigo, A., Forascu, C., Benajiba, Y., & Osenova, P. (2013). Overview of QA4MRE Main Task at CLEF 2013. Proc. QA4MRE-2013.  [25] TEI (2014). Text Encoding Initiative. http://www.tei-c.org/index.xml. [26] TREC (2014). http://trec.nist.gov/. [27] van Rijsbergen, K. J. (1979). Information Retrieval. London, UK: Butterworth. http://www. dcs.gla.ac.uk/Keith/ Preface.html [28] Viglianti, R. (2015). Enhancing Music Notation Addressability. http://mith.umd.edu/ research/ project/ enhancing-music-notation-addressability/. [29] Voorhees, E. M. (2002). Overview of the TREC 2002 Question Answering Track. http://trec.nist.gov/pubs/trec11/ papers/QA11.pdf   530 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "A Hierarchical Bayesian Framework for Score-Informed Source Separation of Piano Music Signals.",
        "author": [
            "Wai Man Szeto",
            "Kin Hong Wong"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417061",
        "url": "https://doi.org/10.5281/zenodo.1417061",
        "ee": "https://zenodo.org/records/1417061/files/SzetoW15.pdf",
        "abstract": "Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone sig- nals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achiev- ing significantly better quality than those reported in related work done previously.",
        "zenodo_id": 1417061,
        "dblp_key": "conf/ismir/SzetoW15",
        "keywords": [
            "score-informed",
            "monaural",
            "source separation",
            "piano tone",
            "sinusoidal modeling",
            "instrument-specific",
            "Bayesian framework",
            "overlapping partials",
            "quality separation",
            "related work"
        ],
        "content": "AH I E R A R C H I C A LB A Y E S I A NF R A M E W O R KF O RSCORE-INFORMED SOURCE SEPARATION OF PIANO MUSIC SIGNALSWai Man SZETOOffice of University General EducationThe Chinese University of Hong Kongwmszeto@cuhk.edu.hkKin Hong WONGDepartment of Computer Science and EngineeringThe Chinese University of Hong Kongkhwong@cse.cuhk.edu.hkABSTRACTHere we propose a score-informed monaural source separationsystem to extract every tone from a mixture of piano tone sig-nals. Two sinusoidal models in our earlier work are employedin the above-mentioned system to represent piano tones: theGeneral Model and the Piano Model. The General Model, avariant of sinusoidal modeling, can represent a single tone withhigh modeling quality, yet it fails to separate mixtures of tonesdue to the overlapping partials. The Piano Model, on the otherhand, is an instrument-specific model tailored for piano. Itsmodeling quality is lower but it can learn from training data(consisting entirely of isolated tones), resolve the overlappingpartials and thus separate the mixtures. We formulate a newhierarchical Bayesian framework to run both Models in thesource separation process so that the mixtures with overlappingpartials can be separated with high quality. The results showthat our proposed system gives robust and accurate separationof piano tone signal mixtures (including octaves) while achiev-ing significantly better quality than those reported in relatedwork done previously.1. INTRODUCTIONHere we propose a score-informed monaural source separationsystem under a new hierarchical Bayesian framework to ex-tract every tone from a mixture of piano tone signals with highseparation quality. Two sinusoidal models in our earlier workin [14, 15] are employed in the above mentioned system torepresent piano tones. Sinusoidal modeling is commonly usedin many existing monaural source separation systems to modelpitched musical sounds [6, 7, 9, 11, 16]. The major difficulty ofsource separation (SS) is to resolve overlapping partials.Existing systems are based on assumptions on the generalproperties of pitched musical sounds. For example, the spectralenvelope of tones is assumed to be smooth (as in [7, 16]), orthat the amplitude envelope of each partial from the same notetends to be similar [11] (known as common amplitude modula-tion (CAM)), or that the amplitude envelope of a partial evolvessimilarly among different notes of the same musical instrumentc\u0000Wai Man SZETO, Kin Hong WONG.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Wai Man SZETO, Kin Hong WONG. “AHierarchical Bayesian Framework for Score-Informed Source Separationof Piano Music Signals”, 16th International Society for Music InformationRetrieval Conference, 2015.in [9]. Yet these assumptions may not be suitable for SS ofpiano mixtures as explained in [15]. A very recent work in [17]can resolve two closed partials but it may not work on octaves,in which the partials of the upper tone are totally immersedwithin the frequencies of the lower tone. Moreover, it assumesthat partials are exact multiples of the fundamental frequency.This assumption is not valid for piano because piano tones areonly quasi-harmonic [1].Instead of formulating similar assumptions, we limit inputmixtures to piano music signals. This allows us to design apiano-specific model called the Piano Model (PM) to resolveoverlapping partials in [15]. In piano music, a particular pitchtends to appear more than once. The tones of the same pitchshare some common characteristics which can be capturedby PM. Our system is based on two requirements. First, thepitches in the mixtures should reappear as isolated tones in thetarget recording. Second, the piano music is performed withoutpedaling. Then the isolated tones can be used as the trainingdata for PM to resolve the overlapping partials even for octaves.Although PM can resolve the overlapping partials, its mod-eling quality of single piano tones is lower than our GeneralModel (GM) in [14]. However, GM cannot be directly appliedto SS because it fails to separate mixtures of tones due to theoverlapping partials. Here we formulate a new hierarchicalBayesian framework to run both PM and GM in the SS processso that the mixtures with overlapping partials can be separatedwith high quality. The separation process is divided into thetraining stage and the SS stage. Given the estimated PM param-eters and the training data, we can, in the SS stage, set the priordistributions of the GM parameters to favor the proper regionsof values under the Bayesian framework, estimate the GMparameters successfully even the case of overlapping partials,and reconstruct the individual tones in the mixtures with highquality. We hope that our system could shed some light on theempirical study of expressiveness in music performance [5]by comparing the subtleties of various artists’ performances,based on individual tones extracted by SS.2. SIGNAL MODELSHere an individual tone (the sound of hitting one piano key) isconsidered as a particular sound source of the correspondingpitch. When multiple piano keys are pressed, a mixture signalis generated. We model a mixture signal as a sum of its cor-responding individual tones asy(t)=PKk=1xk(t)wherey(t)is the observed mixture signal in the time domain,Kis the155number of tones in the mixture,xk(t)is thekth individual tonein the mixture, andtis the time in seconds. We assume thatthe score has been known so that the pitch and the duration ofeachxk(t)are given (music transcription systems [2, 10] canbe used here). The goal of our research is to recover the signalof each individual tonexk(t)from the mixture signaly(t)viathe signal models GM and PM.2.1 General Model (GM)In [14], we present a frame-wise sinusoidal model called GMto represent a piano tone. For a piano tone, the frequencies ofthe partials are stable so the frequencies can be fixed acrossframes. The number of partials can also be fixed for a tone. InGM, the estimated tonebxk,r,w h i c hi st h ee s t i m a t eo ft h ekthtone in a mixture at therth frame, can be written as:bxk,r[l]=MkXm=1w[l](↵k,m,rcos(2⇡fk,mtl)+\u0000k,m,rsin(2⇡fk,mtl))(1)whereMkis the number of partials,↵k,m,ris the amplitudeof the cosine component,\u0000k,m,ris the amplitude of the sinecomponent,fk,mis the frequency,w[l]is the window functionwith the window lengthLandl=0,...,L\u00001,a n dtlis the timein second at the indexlsotl=l/fsandfsis the samplingfrequency in Hz. The overlap-and-add method in [18] can beused to reconstruct the entire signal from GM.Based on the above model, the estimated mixturebyr[l]attherth frame is the sum of each estimated tonebxk,r[l]suchthatbyr[l]=PKk=1bxk,r[l].T h eo b s e r v e dm i x t u r ei st h es u mo fthe estimated mixture and the noise term soyr[l]=byr[l]+vr[l]wherevr[l]is the noise component. To estimate the parametersin each frame, it is convenient to rewrite the model in (1) intothe matrix form. LetHkbe the frequency matrix of thekthtone in the form ofHk[l,u]=8<:w[l]cos(2⇡fk,utl)if1uMk,w[l]sin(2⇡fk,u\u0000Mktl)ifMk+1u2Mk(2)and we also letfkbe the frequency vector containing allfk,u.The amplitudes of the cosine and sine terms of thekth toneat therth frame can be expressed as a column vectorgk,rdefined bygk,r[u]=(↵k,u,rif1uMk,\u0000k,u\u0000Mk,rifMk+1u2Mk.(3)For the mixture, the frequency matrices from each tone areconcatenated into the matrixH=[H1···HK]and allfkareconcatenated into the column vectorf=⇥fT1···fTK⇤T.T h eamplitude vectors of each tone can also be concatenated intoac o l u m nv e c t o rgr=⇥gT1,r···gTK,r⇤T.T h ee s t i m a t e dm i x t u r eatrth frame can be expressed asbyr=Hgrand the estimatedmixture is related to the observed mixture as below:yr=Hgr+vr(4)wherevris the noise term. It is modeled as the zero-meanGaussian noise with the variance\u00002vr.The observed mixture signal can be expressed in the formofY=[y1···yR].T h e nt h ee s t i m a t e dm i x t u r ef o ra l lf r a m e scan be written asbY=HG(5)wherebY=[by1···byR],G=[g1···gR]andRis the number offrames. All GM parameters can be grouped into⇥={f,G}.The goal of our SS is to estimate both the frequency matrixHand the amplitude matrixGso that each individual tone canbe reconstructed. However,His often rank deficient. Thishappens when some of the partials from different tones in themixture are overlapping. This implies that if only the mixturein such case is given, it is impossible to separate the mixtureinto its individual tones unless more information is provided.This problem can be solved by using the training data as theprior information under the Bayesian framework in Section 3.2.2 Piano Model (PM)In [15], we propose PM to resolve the overlapping partialsby exploring the common properties of recurring tones. PMemploys a time-varying sum-of-sinusoidal signal model forpiano tones, and it describes a tone in an entire duration insteadof a single analysis frame asbxk(tn)=MkXm=1a(tn;ck,'k,m)·cos(2⇡fk,mtn+\u0000k,m)(6)whereMkis the number of partials of thekth tone,fk,mand\u0000k,mare the frequency and the phase respectively, anda(tn;ck,'k,m)is the time-varying amplitude of the partialstated in [15] where the envelope parameters'k,mcontrolthe envelope surface against the intensityckand the timetn.The intensityckis assigned to be the peak amplitude of theobserved time-domain signal of the tone. The onset of eachtone in the mixture may not be exactly the same so a time-shift factor is introduced for each tone in the estimated mixtureby(tn)=PMkk=1bxk(tn\u0000⌧k)where⌧kis the time shift in seconds.All parameters in PM for thekth tone can be groupedinto a parameter set kso k={'k,m,fk,m,\u0000k,m,ck,⌧k}and ={ 1,..., K}.T h e P M p a r a m e t e r s kcan be di-vided into two groups: the invariant PM parameters k,I={'k,m,fk,m,\u0000k,m}and the varying PM parameters k,V={ck,⌧k}.T h ei n v a r i a n tP Mp a r a m e t e r sc o n t a i np a r a m e t e r si n -variant to instances of the same pitch and they are estimatedfrom the training data. The varying PM parameters consist ofparameters which may vary across instances. Given a mixture,only the varying PM parameters of the mixture are requiredto be estimated if the invariant PM parameters have been esti-mated from the training data.In both GM and PM, we have assumed that the number ofpartialsMkof each tone is known. The number of partialsMkis fixed for all experiments. The details of findingMkcan befound in [14].3. BAYESIAN FRAMEWORK FOR SSThis section will explain how the Bayesian framework inte-grates the two models in the previous section and incorporatesthe training data to resolve overlapping partials. Given themixtureyand the training dataX,t h eg o a lo fB a y e s i a nS Swith GM is to find theMaximum A Posterior(MAP) solutionb⇥ythat maximizes the posteriorp(⇥y|y,X)where⇥yis the156 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015⇥true⇥yp(⇥y|X)(b) Prior\n⇥true⇥yp(y|⇥y)(a) Likelihood\n⇥true⇥yp(⇥y|y,X)(c) PosteriorFigure 1.( a )T h el i k e l i h o o df u n c t i o n .( b )T h ep r i o r .( c )T h eposterior. This schematic diagram shows that an appropriateprior gives the desirable MAP solution. The vertical line showsthe true value of⇥y.GM parameter set fory.B yB a y e s ’t h e o r e m ,t h ep o s t e r i o rc a nbe written in the formp(⇥y|y,X)/p(y|⇥y)p(⇥y|X).The key issue of Bayesian SS is how to set up the priorp(⇥y|X).I f o v e r l a p p i n g p a r t i a l s a r e p r e s e n t , t h e f r e q u e n c ymatrixHis rank deficient and many choices of⇥can give sim-ilar values of the likelihoodp(y|⇥y).H e n c e ,t h e r ea r em a n ypeaks in the likelihood function as shown in the schematicdiagram (Figure 1(a)). In order to find the desirable MAPsolution, it is advantageous that the prior distribution has a highdensity around the correct value of⇥y.I n F i g u r e 1 ( b ) , t h eprior is appropriate so that the MAP solution, i.e. the peak ofthe posterior, can be located correctly as depicted in Figure1(c). In short, an appropriate prior of the GM parameters iscrucial for resolving the overlapping partials. It can be foundby using the training data and the estimated PM parameters.The priorp(⇥y|X)expresses the probability distributionof⇥ygiven the training dataXand before the mixtureyisobserved. The functional form ofp(⇥y|X)can be formulatedin terms of PM. The PM parameter set yof the mixtureyis divided into two sets: the invariant PM parameter set y,Iand the varying PM parameter set y,V.F o rt h et r a i n i n gd a t aX,t h eP Mp a r a m e t e rs e t Xis divided into the invariant PMparameter set X,Iand the varying PM parameter set X,V.Note that both the mixture and the training data share the sameset of the invariant PM parameters. The subscriptsyandXfor the invariant PM parameters can be omitted for clarity so I= y,I= X,I.The posteriorp(⇥y|y,X)of the GM parameters can belinked up with the PM parameters by using marginalization:p(⇥y|y,X)=ZZp(⇥y, y,V, I|y,X)d y,Vd I.(7)Note that the noise variance\u00002vrof the mixture in (4) is omit-ted in the derivation for clarity. Then by the product rule ofprobability, (7) can be put intop(⇥y|y,X)=ZZp(⇥y|y,X, y,V, I)p( y,V, I|y,X)d y,Vd I(8)where the first term is the posterior of⇥yand the second isthe posterior of y,Vand I.However, finding the MAP solution involves evaluatingthe integration over all possible values of y,Vand Iin(8). PM is a highly dimensional and nonlinear model thatmakes the integration analytically infeasible. Different ap-proximation techniques can be used to find the MAP solution.General Model(GM)Piano Model(PM)InvariantPMparametersPiano Model(PM)VaryingPMparametersTrainingSource separationbªIbªy;Vp(ªIjX)p(ªy;Vjy;bªI)p(£yjy;X;bªy;V;bªI)Input:mixture  yInput:trainingdataXOutput:GM parametersb£yFigure 2.B a y e s i a nf r a m e w o r kf o rS S .For computational efficiency, here we have used the evidenceapproximation [12, 13]. Following the derivation of the evi-dence approximation in [3, p. 408], we assume that the pos-teriorp( y,V, I|y,X)is sharply peaked around their mostprobable valuesb y,Vandb I.T h e n ( 8 ) c a n b e w r i t t e n a sp(⇥y|y,X)⇡p(⇥y|y,X,b y,V,b I).Hence, the MAP solutionb⇥yis the maximum of the pos-teriorp(⇥y|y,X,b y,V,b I).T h ee s t i m a t i o no fb y,Vandb Ican be done as follows: (i)b y,Vis estimated by maximizingthe posteriorp( y,V|y,X)via the evidence approximationwhich givesp( y,V|y,X)⇡p( y,V|y,b I)(note thatXisomitted because y,Vis independent ofXifb Iis given);(ii)b Iis estimated by maximizing the posteriorp( I|y,X)that can be approximated by using the training data only sop( I|y,X)⇡p( I|X).According to these results, the whole SS process is sum-marized in Figure 2. The whole process is divided into thefollowing two stages:1.Training.G i v e n t h e t r a i n i n g d a t aX,f i n dt h em o s tprobable value of the invariant PM parametersb Iofp( I|X).2.SS.G i v e nt h em i x t u r ey,t h et r a i n i n gd a t aXand theinvariant PM parametersb I,S Sf u n c t i o n si nt w os t e p s :(a)SS with PM.G i v e nyandb I,f i n dt h em o s tp r o b -able value of the varying PM parametersb y,Vofp( y,V|y,b I).(b)SS with GM.G i v e ny,X,b y,Vandb I,f i n dt h eMAP solutionb⇥yofp(⇥y|y,X,b y,V,b I).4. TRAINING AND SS WITH PMThe goal of the training stage is to find the most probableinvariant PM parametersb Ithat maximize the posterior ofthe invariant PM parametersp( I|X)given the training dataX.B y B a y e s ’ t h e o r e m , t h e p o s t e r i o r c a n b e r e w r i t t e n a sp( I|X)/p(X| I)p( I).The priorp( I)reflects our priorknowledge of the invariant PM parameters I.T h ev a l u e so f Igreatly vary from different pitches and pianos. If we havelittle idea on suitable values for a parameter, it is safe to assignap r i o rw h i c hi si n s e n s i t i v et ot h ev a l u e so ft h a tp a r a m e t e r[ 4 ] .Then maximizing the posteriorp( I|X)is effectively equiv-alent to maximize the likelihoodp(X| I).T h e d e t a i l s o ffinding the solutionb Ican be found in [15].Given the invariant PM parametersb Iand the mixturey,we perform SS with PM as shown in Figure 2. The goal of SSProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 157with PM is to find the most probable varying PM parametersb y,Vthat maximize the posterior of the varying PM param-etersp( y,V|y,b I).B y B a y e s ’ t h e o r e m , t h e p o s t e r i o r c a nbe rewritten asp( y,V|y,b I)/p(y| y,V,b I)p( y,V).T h epriorp( y,V)reflects our prior knowledge of the invariant PMparameters y,V.T h ev a l u e so f y,Vgreatly vary from differ-ent playings. Hence, we choose an insensitive prior for y,Vas I.T h e nm a x i m i z i n gt h ep o s t e r i o rp( y,V|y,b I)is again ef-fectively equivalent to maximize the likelihoodp(y| y,V,b I).The details of findingb y,Vare also presented in [15].5. SS WITH GMThe process of SS with GM is divided into the following twosteps: (1) estimate the hyperparameters, and (2) given the hy-perparameters, find the MAP solutionb⇥y.W ew i l lf o c u so nthe second step first.5.1 Find the MAP solutionThe MAP solutionb⇥yis found by maximizing the posteriorp(⇥y|y,X,b y,V,b I).T h e G M p a r a m e t e r s⇥yinclude theamplitude matrixGand the frequenciesf.A ni t e r a t i v eu p d a t escheme is designed to find the MAP solution: (1) givenf,updateG,a n d( 2 )g i v e nG,u p d a t ef.S t e p s1t o2a r er e p e a t e duntil convergence. The iterative update starts with the inputfrequencies from the estimated frequencies in PM in Section 4.The frequencies in PM are close to those in GM. We find that10 iterations are enough for convergence. In the followings, theiterative update scheme will be discussed in details.5.1.1 Step 1: update the amplitude matrixGEachgrin the amplitude matrixGcan be estimated indepen-dently. Given the estimated frequenciesbf,n o ww er e w r i t et h eposterior ofgrintop(gr|yr,X,bf,b y,V,b I,b\u00002vr).T h eg o a lo fthis step is to find the MAP solutionbgrwhich maximizes theposterior ofgr.B yB a y e s ’t h e o r e m ,t h ep o s t e r i o ro fgrcan beexpressed in the form ofp(gr|yr,X,bf,b y,V,b I,b\u00002vr)/p(yr|gr,bf,b\u00002vr)p(gr|X,b y,V,b I)(9)whereb\u00002vrrepresents the estimated variance of the zero-meanGaussian noise in (4).The priorp(gr|X,b y,V,b I)in (9) represents the priordistribution ofgrconditioned on the training dataXand thePM parametersb y,Vandb I.I ti sm o d e l e da saG a u s s i a nw i t hthe meanbµgrand the covariance matrixb⌃gr.I nt h i ss e c t i o n ,it is assumed that the hyperparametersb\u00002vr,bµgrandb⌃grhavebeen estimated and their values are known. The estimation ofthese hyperparameters fromX,b y,Vandb Iwill be discussedin Section 5.2. Note that eachgrhas its own set ofbµgrandb⌃grso the MAP solution of eachgrcan be found independently.Asbyr=Hgris a linear model for givenH,a n db o t ht h enoise and the prior are Gaussian, the resulting posterior ofgris also Gaussian. Therefore, the MAP solutionbgris equal tothe posterior mean. By using the result in [4, p. 153], the MAPsolution isbgr=⇣b⌃\u00001gr+b\u0000\u00002vrHTH⌘\u00001⇣b⌃\u00001grbµgr+b\u0000\u00002vrHTyr⌘.(10)5.1.2 Step 2: update the frequenciesfGiven the estimated amplitude matrixbGin Step 1, the goalof Step 2 is to find the MAP solutionbfwhich maximizes theposteriorp(f|Y,X,bG,b y,V,b I,b\u00002v).H o w e v e r , t h e m o d e lbY=HGin (5) is nonlinear withf.B a s e do no u rw o r ki n[ 1 4 ] ,we vectorize the matrixbYintobYvecand then linearizebYvecby using Taylor’s expansion sobYvec(f)⇡bYvec(fcur)+Z(fcur)(f\u0000fcur)(11)wherebYvec(f)is the estimate depending on the new frequencyvectorfwhich is to be updated, andbYvec(fcur)is the esti-mate depending on the current estimate offcur.T h e m a t r i xZ=Z(fcur)is the Jacobian matrix@bYvec/@fevaluated atfcurandZ=⇥ZT1···ZTr···ZTR⇤T.T h e m a t r i xZris the Ja-cobian matrix@byr/@fatrth frame for all tones andZr=[Z1,r···Zk,r···ZK,r].T h e nt h eJ a c o b i a nm a t r i xZk,ratrthframe forkth tone isZk,r[l,m]=@ˆyr[l]@fk,m=2⇡tlw[l](\u0000↵k,m,rsin(2⇡fk,mtl)+\u0000k,m,rcos(2⇡fk,mtl)).(12)Hence, each element inZcan be computed from (12).Following the prior distribution ofgr,t h ep r i o rd i s t r i b u t i o noffis also modeled as a Gaussian with the meanbµfand thecovariance matrixb⌃f.B ya p p l y i n g( 1 1 )t ot h er e s u l ti n[ 4 ,p .93], the MAP solutionbfisbf=⇣b⌃\u00001f+ZTb⌃\u00001vZ⌘\u00001⇣b⌃\u00001fbµf+ZTb⌃\u00001v⇣Yvec\u0000bYvec+Zfcur⌘⌘(13)whereZ=Z(fcur),bYvec=bYvec(fcur),a n dt h ec o v a r i a n c ematrixb⌃v=d i a g (b\u00002v11L,...,b\u00002vR1L)and1Ldenotes theL-dimensional column vector filled with 1’s. In the next section,we will show how to find the hyperparameters which are crucialfor resolving overlapping partials.5.2 Estimation of the hyperparametersGiven the training dataX,w ef i r s te s t i m a t et h eG Mp a r a m e t e r sfor each isolated tone inXby the method in [14]. Togetherwith the estimated PM parametersb y,Vandb Ifound in Sec-tion 4, we will estimate the hyperparametersb\u00002vr,bµgr,b⌃gr,bµfandb⌃f.5.2.1 Estimation of the noise variance\u00002vrTo estimate the noise variance\u00002vrofyrin (4), we model thenoise variance of an isolated tonexk,rat a frame is directlyproportional to the signal power. Then the noise variance ofxk,ris\u00002vk,r=¯\u00002vk||xk,r||2where¯\u00002vkis the proportionalityconstant for pitchpkand it can be determined by the trainingdataXwhich may contain multiple instances of the same pitch.Letxik,r,Xbe a frame of an isolated tone inXwhere the indexidenotes theith instance of the pitchpk.T h e n¯\u00002vkcan beestimated by¯\u00002vk=1IkRkLIkXi=1RikXr=1L\u00001Xl=00@xik,r,X[l]\u0000ˆxik,r,X[l]\u0000\u0000\u0000\u0000\u0000\u0000xik,r,X\u0000\u0000\u0000\u0000\u0000\u00001A2(14)158 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015whereIkis the number of instances of pitchpkinX,Rikisthe number of frames in theith instance of the pitchpkandRk=PIki=1Rik,a n dˆxik,r,Xis the estimate ofxik,r,Xand isfound by using the method in [14].The noise variance of the mixtureyris\u00002vr=KXk=1\u00002vk,r,y=KXk=1¯\u00002vk||xk,r,y||2(15)wherexk,r,yis thekth individual tone in the mixture, and\u00002vk,r,yis its noise variance. However,xk,r,yis not known. Inorder to estimate\u00002vr,w ea p p r o x i m a t e||xk,r,y||2into||xk,r,y||2⇡ bckPKk=1bck!||yr||2(16)where the estimated intensitybckin PM determines the pro-portion of||xk,r,y||2in||yr||2.S u b s t i t u t i n g( 1 6 )i n t o( 1 5 ) ,w eestimate the noise varianceb\u00002vrin the mixtureyrin the form ofb\u00002vr=KXk=1 bck¯\u00002vkPKk=1bck!||yr||2.(17)5.2.2 Estimation of the prior distribution of the amplitudesgrThe prior distributionp(gr|bµgr,b⌃gr)ofgris modeled as theGaussian with the meanbµgrand the covarianceb⌃gr.B o t hbµgrandb⌃grdepend onb y,Vandb I.T h i sd e p e n d e n c ec a nbe formulated by converting the PM parametersb y,Vandb Iinto the GM parameters. Lett0rbe the time at the center of therth frame so thatt0r=( (r\u00001)D+0.5L)/fswhereDis thehop size in samples. Evaluating the envelope function of PMin (6) at the center of therth frame, we can find the estimatedamplitudebak,m,r,y,PM=a(t0r;bck,b'm)wherebckandb'mareincluded inb y,Vandb Irespectively.The phase at the center ofrth frame can be calculated fromb y,Vandb Ibyb\u0000k,m,r,y,PM=2⇡bfk,m,PM(t0r\u0000b⌧k)+b\u0000k,m,PM(18)where the frequencybfk,m,y,PMand the phaseb\u0000k,m,PMare in-cluded inb I,a n dt h et i m es h i f tb⌧kis included inb y,V.T h e nbak,m,r,y,PMandb\u0000k,m,r,y,PMin PM can be transformed intothe amplitude of cosineb↵k,m,r,y,PMand the amplitude of sineb\u0000k,m,r,y,PMin GM. The meanbµgrof the prior is assigned tobe these estimated amplitudes from PM so thatbµ↵k,m,r=b↵k,m,r,y,PM=bak,m,r,y,PMcosb\u0000k,m,r,y,PM(19)bµ\u0000k,m,r=b\u0000k,m,r,y,PM=\u0000bak,m,r,y,PMsinb\u0000k,m,r,y,PM(20)wherebµ↵k,m,randbµ\u0000k,m,rare the elements inbµgrand theyfollow the ordering in (3).The covarianceb⌃grmeasures the deviation between thevalues ofgrestimated by PM and those estimated by GM. Itis modeled as a diagonal matrix of which the diagonal is filledwith the variancesb\u00002↵k,m,randb\u00002\u0000k,m,rand follows the order-ing in (3). We assume that the variancesb\u00002↵k,m,randb\u00002\u0000k,m,rare identical and they are directly proportional to the power ofthe partial amplitude. This givesb\u00002↵k,m,r=b\u00002\u0000k,m,r=¯\u00002Gk(bak,m,r,y,PM)2(21)where¯\u00002Gkis the proportionality constant and it can be deter-mined by the training dataXas below.Letb↵ik,m,r,X,GMandb\u0000ik,m,r,X,GMbe the amplitudes in GMforXand they have been estimated by the method in [14]. Letb↵ik,m,r,X,PMandb\u0000ik,m,r,X,PMbe the amplitudes in GM forXand they are converted from the PM estimate. The conversionfrom the PM estimate to the GM estimate forXfollows thatfor the mixtureyin (19) and (20). Letbaik,m,r,X,PMbe thepartial amplitude in PM thenbaik,m,r,X,PM=r⇣b↵ik,m,r,X,GM⌘2+⇣b\u0000ik,m,r,X,GM⌘2.(22)Following (21), we can estimate¯\u00002GkfromXby¯\u00002Gk=12IkMkRkIkXi=1MkXm=1RikXr=18<: \u0000b↵ik,m,rbaik,m,r,X,PM!2+ \u0000b\u0000ik,m,rbaik,m,r,X,PM!29=;(23)where\u0000b↵ik,m,r=b↵ik,m,r,X,GM\u0000b↵ik,m,r,X,PMand\u0000b\u0000ik,m,r=b\u0000ik,m,r,X,GM\u0000b\u0000ik,m,r,X,PM.Note that the priorp(gr|bµgr,b⌃gr)reflects the differencebetween the individual tones estimated by GM and PM. AsPM gives satisfactory quality of estimation in [15], the differ-ence should be small enough to make the prior distributionp(gr|bµgr,b⌃gr)has a high density around the correct value ofgras shown in the schematic diagram in Figure 1. Hence, over-lapping partials can be resolved and higher quality of SS can beobtained. It will be verified and explained in the experiments.5.2.3 Estimation of the prior distribution of frequenciesfThe prior distributionp(f|bµf,b⌃f)offis modeled as the Gaus-sian with the meanbµfand the covarianceb⌃f.T h em e a nbµfis set to the estimated frequencies in PM fromb Iso thatbµfk,m=ˆfk,m,PM(24)wherebµfk,mare the elements inbµf.F o l l o w i n gt h ed e r i v a t i o nofb⌃gr,w ea l s oa s s u m et h a tb⌃fis a diagonal matrix of whichthe diagonal is filled with each varianceb\u00002fk,m.T h ev a r i a n c eb\u00002fk,mis modeled to be directly proportional to the square ofthe frequency in PM. This givesb\u00002fk,m=¯\u00002fk⇣bfk,m,PM⌘2(25)where¯\u00002fkis the proportionality constant which can also bedetermined by the training dataX.T h ee s t i m a t eo f¯\u00002fkis¯\u00002fk=1MkMkXm=1 bfk,m,X,GM\u0000bfk,m,PMbfk,m,PM!2(26)wherebfk,m,X,GMis the estimated frequency in GM forXandit can be estimated by using the method in [14]. Note that thereis no subscriptXinbfk,m,PMbecausebfk,m,PMare the invariantPM parameters so the training data and the mixture share thesame set ofbfk,m,PM.In summary, after estimating the hyperparametersb\u00002vrin(17),bµgrin (19) and (20),b⌃grin (21),bµfin (24) andb⌃fin(25), we can find the MAP solutionb⇥yof GM by iterativelyupdating the amplitude matrixGin (10) and the frequenciesfin (13). In the next section, experimental results will beProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 159presented to show the performance of the whole SS process.6. EXPERIMENTS6.1 Data set and experimental setupWe used the same data set in [15] for comparing the perfor-mance. The data set contains 25 mixtures. Each mixture wasgenerated by mixing the isolated tones in the recorded pianodatabases [8, 15], taken from 4 different pianos. Only tonesfrom the same piano were used to form a mixture. The pitchesin each mixture correspond to a chord randomly selected from11 piano pieces in the RWC database [8]. The number of tones(represented byK)i no u rs e l e c t e dm i x t u r e sr a n g e sf r o m1t o6: 1 tone (8 mixtures), 2 tones (6), 3 tones (5), 4 tones (4),5t o n e s( 1 )a n d6t o n e s( 1 ) .T h e s e2 5m i x t u r e sc o n s i s to f6 2tones. 7 mixtures contain one pair of octaves, 2 (K=5andK=6)c o n t a i n2p a i r so fo c t a v e s .F o rt h et r a i n i n gd a t a ,t w oinstances of each pitch are available soIk=2.T h e f i r s t 0 . 5second of the mixtures and the training data were used in theexperiments. All data were downsampled to 11.025 kHz forfaster processing. The window setting in GM is as follows: thewindow function is the hamming window with length 11.61ms (L=128)a n d5 0 %o v e r l a p .T h et i t l e so ft h ep i a n op i e c e sused and details of the selected mixtures are available on thewebsite of this paper (link available at the end of this section).6.2 ResultsThe performance of our SS system is evaluated by the signal-to-noise ratio (SNR) defined bySNR=10log10Pnx(tn)2Pn(x(tn)\u0000bx(tn))2(27)wherex(tn)is the isolated tone in the time domain beforemixing andbx(tn)is the estimated tone in the time domain. Theisolated tones give the ground truth for evaluation.6.2.1 Evaluation on modeling qualityWe followed the procedures in [15] to evaluate the modelingquality, i.e. the quality of PM and GM to represent an isolatedtone before mixing. The isolated tones of the 25 mixtureswere inputted into our proposed SS system including both PMand GM. The outputs of our system were the estimated tonesreconstructed from PM and GM. If the parameters obtainedin PM and GM are accurate, they can regenerate the originaltones in high quality. The result is that the average SNRs ofPM and GM are 11.15 dB and 17.38 dB respectively. Theaverage SNR of GM is much higher than that of PM. This isbecause GM is more flexible to represent piano tones.6.2.2 Comparing with other systems for separation qualityThe procedures in [15] were followed to evaluate the separationquality, i.e. the quality of PM and GM separating a mixtureinto its individual tones. We also compared PM and GM withar e c e n tS Ss y s t e mi n[ 1 1 ] ,i nw h i c hL i ,W o o d r u f fa n dW a n gbuilt their system (Li’s system) based on CAM mentioned inSection 1. It uses the non-overlapping partials to estimate theoverlapping partials of the same note. The implementation ofSNR (dB)PMGMLiAll mixtures10.8813.516.63K=211.7615.2612.072K610.9713.155.40Upper tones in octaves10.9512.771.57Table 1.C o m p a r i s o no fL i ’ ss y s t e ma n do u rP Ma n dG M .\n123456051015\nNumber of tones KAverage SNR (dB) PMGMLiFigure 3.A v e r a g e S N R a g a i n s t t h e n u m b e r o f t o n e sKforPM, GM, and Li’s system.Li’s system was provided by the authors. The true fundamentalfrequency of each tone was supplied to Li’s system.The results are shown in Table 1. For the 25 mixtures, theaverage SNRs of PM, GM and Li’ system are 10.88 dB, 13.51dB and 6.63 dB respectively. Both PM and GM outperformLi’s system. A significant improvement is in the octave casesas shown in the table. Li’s system is unable to resolve theoverlapping partials of the upper tones in octaves because non-overlapping partials are not available. On the other hand, bothPM and GM are able to reconstruct the upper tone in an octave.The overlapping partials were successfully resolved even formixtures containing 2 pairs of octaves of C3, G3, C4, E4, G4(K=5)a n do fF]3, C4, F4, C5, D5, F5 (K=6).The average SNR against the number of tonesKis plottedin Figure 3. The average SNR of Li’s system decreases muchmore rapidly than PM and GM. Our system can make useof the training data to give higher separation quality. Someaudio files in the experiments are selected for demonstrationpurpose. The audio files, titles of piano pieces used, details ofthe selected mixtures and mathematical notations used in thispaper are available at http://www.cse.cuhk.edu.hk/⇠khwong/www2/conference/ismir2015/ismir2015.html.7. CONCLUSIONSHere we have proposed a score-informed monaural SS systemto extract each tone from a mixture of piano tone signals. Twosinusoidal models, PM and GM, are employed to represent pi-ano tones in the system. We formulate a hierarchical Bayesianframework to run both Models in the SS process so that themixtures with overlapping partials can be resolved with highquality. Experiments show that our proposed system givesrobust and accurate separations of mixtures and improves theseparation quality significantly comparing to the previous work.160 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20158. REFERENCES[1]A. Askenfelt, editor.Five Lectures on the Acoustics of thePiano.R o y a lS w e d i s hA c a d e m yo fM u s i c ,1 9 9 0 .A v a i l a b l eonline at http://www.speech.kth.se/music/5lectures/.[2]T. Berg-Kirkpatrick, J. Andreas, and D. Klein. Unsuper-vised transcription of piano music. InAdvances in NeuralInformation Processing Systems,p a g e s1 5 3 8 – 1 5 4 6 ,2 0 1 4 .[3]C. M. Bishop.Neural Network for Pattern Recognition.Oxford University Press, New York, 1995.[4]C. M. Bishop.Pattern Recognition and Machine Learning.Springer, New York, 2006.[5]E. Schubert D. Fabian, R. Timmers, editor.Expressivenessin Music Performance: Empirical Approaches AcrossStyles and Cultures.O x f o r dU n i v e r s i t yP r e s s ,2 0 1 4 .[6]M. Davy, S. Godsill, and J. Idier. Bayesian analysis ofpolyphonic western tonal music.Journal of the AcousticalSociety of America,1 1 9 ( 4 ) : 2 4 9 8 – 2 5 1 7 ,A p r i l2 0 0 6 .[7]M. R. Every and J. E. Szymanski. Separation of syn-chronous pitched notes by spectral filtering of harmonics.IEEE Transactions on Audio, Speech & LanguageProcessing,1 4 ( 5 ) : 1 8 4 5 – 1 8 5 6 ,2 0 0 6 .[8]M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.Rwc music database: Music genre database and musicalinstrument sound database. InProceedings of the 4thInternational Conference on Music Information Retrieval(ISMIR 2003),p a g e s2 2 9 – 2 3 0 ,O c t o b e r2 0 0 3 .[9]Jinyu Han and B. Pardo. Reconstructing completelyoverlapped notes from musical mixtures. InAcoustics,Speech and Signal Processing (ICASSP), 2011 IEEEInternational Conference on,p a g e s2 4 9 – 2 5 2 ,2 0 1 1 .[10]A. Klapuri and M. Davy, editors.Signal ProcessingMethods for Music Transcription.S p r i n g e r ,2 0 0 6 .[11]Y. Li, J. Woodruff, and D. Wang. Monaural musicalsound separation based on pitch and common amplitudemodulation.IEEE Transactions on Audio, Speech, andLanguage Processing,1 7 ( 7 ) : 1 3 6 1 – 1 3 7 1 ,2 0 0 9 .[12]D. J. C. MacKay. Bayesian interpolation.NeuralComputation,4 : 4 1 5 – 4 4 7 ,1 9 9 2 .[13]D. J. C. MacKay. A practical bayesian frameworkfor backpropagation networks.Neural Computation,4:448–472, 1992.[14]W. M. Szeto and K. H. Wong. Sinusoidal modeling forpiano tones. In2013 IEEE International Conferenceon Signal Processing, Communication and Computing(ICSPCC),p a g e s1 – 6 ,K u n m i n g ,C h i n a ,A u g u s t2 0 1 3 .Available online at http://www.cse.cuhk.edu.hk/⇠khwong/www2/conference/ismir2015/ismir2015.html.[15]W. M. Szeto and K. H. Wong. Source separation andanalysis of piano music signals using instrument-specificsinusoidal model. InProceedings of the 16th InternationalConference on Digital Audio Effects (DAFx-13),p a g e s109–116, Maynooth, Ireland, September 2013. Availableonline at http://www.cse.cuhk.edu.hk/⇠khwong/www2/conference/ismir2015/ismir2015.html.[16]T. Virtanen.Sound Source Separation in Monaural MusicSignals.P h Dt h e s i s ,T a m p e r eU n i v e r s i t yo fT e c h n o l o g y ,Finland, November 2006.[17]M. Zivanovic. Harmonic bandwidth companding forseparation of overlapping harmonics in pitched signals.IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing,2 3 ( 5 ) : 8 9 8 – 9 0 8 ,M a y2 0 1 5 .[18]U. Z¨oler, editor.DAFX - Digital Audio Effects.W i l e y ,2 n dedition, 2011.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 161"
    },
    {
        "title": "Cover Song Identification with Timbral Shape Sequences.",
        "author": [
            "Christopher J. Tralie",
            "Paul Bendich"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416824",
        "url": "https://doi.org/10.5281/zenodo.1416824",
        "ee": "https://zenodo.org/records/1416824/files/TralieB15.pdf",
        "abstract": "We introduce a novel low level feature for identifying cover songs which quantifies the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different ver- sions of the same song, these point clouds are approxi- mately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features.",
        "zenodo_id": 1416824,
        "dblp_key": "conf/ismir/TralieB15",
        "keywords": [
            "low level feature",
            "cover songs",
            "smoothed frequency spectrum",
            "sliding window representation",
            "point cloud",
            "time-ordered point cloud",
            "MFCC embeddings",
            "relative shape sequence",
            "Covers 80 dataset",
            "note sequences"
        ],
        "content": "COVER SONG IDENTIFICATION WITH TIMBRAL SHAPE SEQUENCESChristopher J. TralieDuke University Department ofElectrical and Computer Engineeringchris.tralie@gmail.comPaul BendichDuke University Department ofMathematicsbendich@math.duke.eduABSTRACTWe introduce a novel low level feature for identifying coversongs which quantiﬁes the relative changes in the smoothedfrequency spectrum of a song. Our key insight is that asliding window representation of a chunk of audio can beviewed as a time-ordered point cloud in high dimensions.For corresponding chunks of audio between different ver-sions of the same song, these point clouds are approxi-mately rotated, translated, and scaled copies of each other.If we treat MFCC embeddings as point clouds and castthe problem as a relative shape sequence, we are able tocorrectly identify 42/80 cover songs in the “Covers 80”dataset. By contrast, all other work to date on cover songsexclusively relies on matching note sequences from Chromaderived features.1. INTRODUCTIONAutomatic cover song identiﬁcation is a surprisingly difﬁ-cult classical problem that has long been of interest to themusic information retrieval community [5]. This problemis signiﬁcantly more challenging than traditional audio ﬁn-gerprinting because a combination of tempo changes, mu-sical key transpositions, embellishments in time and ex-pression, and changes in vocals and instrumentation canall occur simultaneously between the original version of asong and its cover. Hence, low level features used in thistask need to be robust to all of these phenomena, ruling outraw forms of popular features such as MFCC, CQT, andChroma.One prior approach, as reviewed in Section 2, is to com-pare beat-synchronous sequences of chroma vectors be-tween candidate covers. The beat-syncing helps this beinvariant to tempo, but it is still not invariant to key. How-ever, many schemes have been proposed to deal with this,up to and including a brute force check over all key trans-positions.Chroma representations factor out some timbral infor-mation by folding together all octaves, which is sensiblegiven the effect that different instruments and recording en-vironments have on timbre. However, valuable non-pitchc\u0000Christopher J. Tralie, Paul Bendich.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Christopher J. Tralie, Paul Bendich.“Cover Song Identiﬁcation with Timbral Shape Sequences”, 16th Inter-national Society for Music Information Retrieval Conference, 2015.information which is preserved between cover versions,such as spectral ﬁngerprints from drum patterns, is ob-scured in Chroma representation. This motivated us to takeanother look at whether timbral-based features could beused at all for this problem. Our idea is that even if ab-solute timbral information is vastly different between twoversions of the same song, therelative evolutionof timbreover time should be comparable.With careful centering and normalization within smallwindows to combat differences in global timbral drift be-tween the two songs, we are indeed able to design shapefeatures which are approximately invariant to cover. Thesefeatures, which are based on self-similarity matrices ofMFCC coefﬁcients, can be used on their own to effectivelyscore cover songs. This, in turn, demonstrates that even ifabsolute pitch is obscured and blurred, cover song identiﬁ-cation is still possible.Section 2 reviews prior work in cover song identiﬁca-tion. Our method is described in detail by Sections 3 and4. Finally, we report results on the “Covers 80” benchmarkdataset [7] in Section 5, and we apply our algorithm to therecent “Blurred Lines” copyright controversy.2. PRIOR WORKTo the best of our knowledge, all prior low level feature de-sign for cover song identiﬁcation has focused on Chroma-based representations alone. The cover songs problemstatement began with the work of [5], which used FFT-based cross-correlation of all key transpositions of beat-synchronous chroma between two songs. A follow-upwork [8] showed that high passing such cross-correlationcan lead to better results. In general, however, cross-correlation is not robust to changes in timing, and it isalso a global alignment technique. Serra [22] extended thisinitial work by considering dynamic programming localalignment of chroma sequences, with follow-up work andrigorous parameter testing and an “optimal key transposi-tion index” estimation presented in [23]. The same authorsalso showed that a delay embedding of statistics spanningmultiple beats before local alignment improves classiﬁca-tion accuracy [25]. In a different approach, [14] comparedmodeled covariance statistics of all chroma bins, as wellas comparing covariance statistics for all pairwise differ-ences of beat-level chroma features, which is not unlike the“bag of words” and bigram representations, respectively,in text analysis. Other work tried to model sequences of38chords [2] as a slightly higher level feature than chroma.Slightly later work concentrated on fusing the results ofmusic separated into melody and accompaniment [11] andmelody, bass line, and harmony [21], showing improve-ments over matching chroma on the raw audio. The mostrecent work on cover song identiﬁcation has focused onfast techniques for large scale pitch-based cover song iden-tiﬁcation, using a sparse set of approximate nearest neigh-bors [28] and low dimensional projections [12]. Authorsin [9] and [17] also use themagnitudeof the 2D FourierTransform of a sequences of chroma vectors treated as animage, so the resulting coefﬁcients will be automaticallyinvariant to key and time shifting without any extra com-putation, at the cost of some discriminative power.Outside of cover song identiﬁcation, there are otherworks which examine gappy sequences of MFCC in mu-sic, such as [4]. However, these works look at matchedsequences of MFCC-like features in their original featurespace. By contrast, in our work, we examine therela-tiveshape of such features. Finally, we are not the ﬁrstto consider shape in an applied musical context. For in-stance, [29] turns sequences of notes in sheet music intoplane curves, whose curvature is then examined. To ourknowledge, however, we are the ﬁrst to explicitly modelshape in musical audio for version identiﬁcation.3. TIME ORDERED POINT CLOUDS FROMBLOCKS OF AUDIOThe ﬁrst step of our algorithm uses a timbre-based methodto turn a block of audio into what we call atime-orderedpoint cloud.We can then compare to other time-orderedpoint clouds in a rotation, translation, and scale invariantmanner using normalized Euclidean Self-Similarity matri-ces (Section 3.3). The goal is to then match up the relativeshape of musical trajectories between cover versions.3.1 Point Clouds from Blocks and WindowsWe start with a song, which is a function of timef(t)thathas been discretized as some vectorX. In the followingdiscussion, the symbolX(a, b)means the song portion be-ginning at timet=aand ending at timet=b. GivenX, there are many ways to summarize a chunk of audiow2X, which we call awindow, as a point in some featurespace. We use the classical Mel-Frequency Cepstral coef-ﬁcient representation [3], which is based on a perceptuallymotivated log frequency and log power short-time Fouriertransform that preserves timbral information. In our appli-cation, we perform an MFCC with 20 coefﬁcients, givingrise to a 20-dimensional point.MFCC(w)2R20(1)Given a longer chunk of audio, which we call ablock,we can use the above embedding on a collection ofKwindows that cover the block to construct a collection ofpoints, or apoint cloud, representing that block. More for-mally, given a block covering a range[t1,t2], we want a setof window intervals[ai,bi], withi=1..K, so that•ai<bi•ai<ai+1,bi<bi+1•[Ki=1[ai,bi]=[t1,t2]Wheret1,t2,ai, andbiare all discrete time indices intothe sampled audioX. Hence, our ﬁnal operator takes a setof time-ordered intervals{[a1,b1],[a2,b2],. . . ,[aK,bK]}which cover a block[t1,t2]and turns them into aK-dimensional point cloud inR20PC({[a1,b1],. . . ,[aK,bK]})={MFCC(X(a1,b1)),. . . ,MFCC(X(aK,bK))}(2)3.2 Beat-Synchronous BlocksAs many others in the MIR community have done, includ-ing [5] and [8] for the cover songs application, we com-pute our features synchronized within beat intervals. Weuse a simple dynamic programming beat tracker developedin [6]. Similarly to [8], we bias the beat tracker with threeinitial tempo levels: 60BPM, 120BPM, and 180BPM, andwe compare the embeddings from all three levels againsteach other when comparing two songs, taking the bestscore out of the 9 combinations. This is to mitigate the ten-dency of the beat tracker to double or halve the true beatintervals of different versions of the same song when thereare tempo changes between the two. The trade-off is ofcourse additional computation. We should note that othercover song works, such as [23], avoid beat tracking stepaltogether, hence bypassing these problems. However, it isimportant for us to align our sequences as well as possiblein time so that shape features are in correspondence, andthis is a straightforward way to do so.Given a set of beat intervals, the union of which makesup the entire song, we take blocks to be all contiguousgroups ofBbeat intervals. In other words, we create asequence of overlapping blocksX1,X2,. . .such thatXiismade up ofBtime-contiguous beat intervals, andXiandXi+1differ only by the starting beat ofXiand the ﬁn-ishing beat ofXi+1. Hence, givenNbeat intervals, thereareN\u0000B+1blocks total. Note that computing an em-bedding over more than one beat is similar in spirit to thechroma delay embedding approach in [25]. Intuitively, ex-amining patterns over a group of beats gives more informa-tion than one beat alone, the effect of which is empiricallyevaluated in Section 5. For all blocks, we take the win-dow sizeWto be the length of the average tempo period,and we advance the window intervals evenly from the be-ginning of the block to the end of a block with ahop sizeH=W/200. Hence, there is a99.5%overlap betweenwindows. We were inspired by theory on raw 1D timeseries signals [18], which shows that matching the win-dow length to be just under the length of the period in adelay embedding maximizes the roundness of the embed-ding. Here we would like to match beat-level periodicitiesand ﬂuctuations therein, so it is sensible to choose a win-dow size corresponding to the tempo. This is in contrastto most other applications that use MFCC sliding windowembeddings, which use a much smaller window size on theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 39(a) Window size 0.05 seconds\n(b) Window size 0.5 secondsFigure 1. A screenshot from our GUI showing PCA onthe sliding window representation of an 8-beat block fromthe hook of Robert Palmer’s “Addicted To Love” with twodifferent window sizes. Cool colors indicate windows to-wards the beginning of the block, and hot colors indicatewindows towards the end.order of 10s of milliseconds, generally with a50%overlap,to ensure that the frequency statistics are stationary in eachwindow. In our application, however, we have found thata longer window size makes our self similarity matrices(Section 3.3) smoother, allowing for more reliable matchesof beat-level musical trajectories, while having more win-dows per beat (high overlap) leads to more robust matchingof SSMs using L2 (Section 4.1).Figure 1 shows the ﬁrst three principal components ofan MFCC embedding with a traditional small window sizeversus our longer window embedding to show the smooth-ing effect.3.3 Euclidean Self-Similarity MatricesFor each beat-synchronous blockXlspanningBbeats,we have a 20-dimensional point cloud extracted from thesliding window MFCC representation. Given such a time-ordered point cloud, there is a natural way to create an im-age which represents the shape of this point cloud in a rota-tion and translation invariant way, called theself-similaritymatrix(SSM) representation.Deﬁnition 1.A EuclideanSelf-Similarity Matrix (SSM)over an ordered point cloudXl2RM⇥kis anM⇥MmatrixDso thatDij=||Xl[i]\u0000Xl[j]||2(3)In other words, an SMM is an image representing allpairwise distances between points in a point cloud orderedby time. SSMs have been used extensively in the MIRcommunity already, spearheaded by the work of Foote in2000 for note segmentation in time [10]. They are nowoften used in general segmentation tasks [24] [15]. Theyhave also been successfully applied in other communities,such as computer vision to recognize activity classes invideos from different points of view and by different ac-tors [13]. Inspired by this work, we use self-similarity ma-trices as isometry invariant descriptors of local shape inour sliding windows of beat blocks, with the goal of cap-turing relative shape. In our case, the “activities” are mu-sical expressions over small intervals, and the “actors” aredifferent performers or groups of instruments.\nThe Beatles\nFive Man Acoustical JamTime\nTimeTimeTime(a) A block of 4 beats with 400 windows sliding in the song “We Can WorkIt Out” by The Beatles with a cover by Five Man Acoustical Jam\nNeil Young\nAnnie LennoxTime\nTimeTimeTime\n(b) A block of 4 beats with 400 windows sliding in the song “Don’t Let ItBring You Down” by Neil Young with a cover by Annie Lennox.Figure 2. Two examples of MFCC SSM blocks whichwere matched between a song and its cover in the cov-ers80 dataset. Hot colors indicate windows in the blockare far from each other, and cool colors indicate that theyare close.To help normalize for loudness and other changes in re-lationships between instruments, we ﬁrst center the pointcloud within each block on its mean and scale each pointto have unit norm before computing the SSM. That is, wecompute the SSM onˆXl, whereˆXl=⇢x\u0000mean(x)||x\u0000mean(x)||2:x2Xl\u0000(4)Also, not every beat block has the same number of sam-ples due to natural variations of tempo in real songs. Thus,to allow comparisons between all blocks, we resize eachSSM to a common image dimensiond⇥d, which is a pa-rameter chosen in advance, the effects of which are ex-plored empirically in Section 5.Figure 2 shows examples of SSMs of 4-beat blockspulled from the Covers80 dataset that our algorithmmatches between two different versions of the same song.Visually, similarities in the matched regions are evident. Inparticular, viewing the images as height functions, manyof the critical points are close to each other. The “We CanWork It Out” example shows how this can work even forlive performances, where the overall acoustics are quitedifferent. Even more strikingly, the “Don’t Let It BringYou Down” example shows how similar shape patternsemerge even with an opposite gender singer and radicallydifferent instrumentation. Of course, in both examples,40 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015there are subtle differences due to embellishments, localtime stretching, and imperfect normalization between thedifferent versions, but as we show in Section 5, there areoften enough similarities to match up blocks correctly inpractice.4. GLOBAL COMPARISON OF TWO SONGSOnce all of the beat-synchronous SSMs have been ex-tracted from two songs, we do a global comparison be-tween all SSMs from two songs to score them as covermatches. Figure 3 shows a block diagram of our system.After extracting beat-synchronous timbral shape featureson SSMs, we then extract a binary cross-similarity matrixbased on the L2 distance between all pairs of self-similaritymatrices between two songs. We subsequently apply theSmith Waterman algorithm on the binary cross-similaritymatrix to score a match between the two songs.4.1 Binary Cross-Similarity And Local Alignment\n(a) Full cross-similarity matrix(CSM)\n(b)212⇥212Binary cross-similarity matrix (BM) with=0.05\n(c) Smith Waterman with localconstraints: Score 93.1Figure 4. Cross-similarity matrix and Smith Waterman onMFCC-based SSMs for a true cover song pair of “We CanWork It Out” by The Beatles and Five Man Acoustical Jam.Given a set ofNbeat-synchronous block SSMs for asong A and a set ofMbeat-synchronous block SSMs for asong B, we compute a song-level matching between songA and B by comparing all pairs of SSMs between the twosongs. For this we create anN⇥Mcross-similarity matrix(CSM), whereCSMij=||SSMAi\u0000SSMBj||2(5)is the Frobenius norm (L2 image norm) between the SSMfor theithbeat block from song A and the SSM forjthbeat block for song B. Given this cross-similarity infor-mation, we then compute a binary cross similarity matrixBM. A binary matrix is necessary so that we can apply theSmith Waterman local alignment algorithm [27] to scorethe match between song A and B, since Smith Watermanonly works on a discrete, quantized alphabet, not real val-ues [23]. To computeBM, we take the mutual fractionnearest neighbors between song A and song B, as in [25].That is,BMij=1ifCSMijis within theMthsmall-est values in rowiof the CSM and ifCSMijis withintheNthsmallest values in columnjof the CSM, and 0otherwise. As in [25], we found that a dynamic distancethreshold for mutual nearest neighbors per element workedsigniﬁcantly better than a ﬁxed distance threshold for theentire matrix.\n(a) Full cross-similarity matrix(CSM)\n(b)212⇥185Binary cross-similarity matrix (BM) with=0.05\n(c) Smith Waterman with localconstraints: Score 8Figure 5. Cross-similarity matrix and Smith Waterman onMFCC-based SSMs for two songs that are not covers ofeach other: “We Can Work It Out” by The Beatles and“Yesterday” by En V ogue.Once we have theBMmatrix, we can feed it to theSmith Waterman algorithm, which ﬁnds the best localalignment between the two songs, allowing for time shift-ing and gaps. Local alignment is a more appropriate choicethan global alignment for the cover songs problem, sinceit is possible that different versions of the same song mayhave intros, outros, or bridge sections that were not presentin the original song, but otherwise there are many sec-tions in common. We choose a version of Smith Watermanwith diagonal constraints, which was shown to work wellfor aligning binary cross-similarity matrices for chroma incover song identiﬁcation [23]. In particular, we recursivelycompute a matrixDso thatDij=max8>>>>>>>>>>>>>><>>>>>>>>>>>>>>:Di\u00001,j\u00001+( 2\u0000(Bi\u00001,j\u00001)\u00001)+\u0000(Bi\u00002,j\u00002,Bi\u00001,j\u00001),Di\u00002,j\u00001+( 2\u0000(Bi\u00001,j\u00001)\u00001)+\u0000(Bi\u00003,j\u00002,Bi\u00001,j\u00001),Di\u00001,j\u00002+( 2\u0000(Bi\u00001,j\u00001)\u00001)+\u0000(Bi\u00002,j\u00003,Bi\u00001,j\u00001),09>>>>>>>>>>>>>>=>>>>>>>>>>>>>>;(6)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 41Song ABeat TrackingTempo Bias A, B (60/120/180 bmp)Beat-Synchronous MFCC Sliding Window BlocksBeatsPerBlock (B)Self-Similarity Matrix ComputationImage Resize Dimension dBinary Cross SimilarityA to B with MutualNearest NeighborsFraction of Neighbors KappaSmith WatermanLocal AlignmentSong BABABABFinal ScoreMatching A T o BABFigure 3. A block diagram of our system for computing a cover song similarity score of two songs using timbral features.\ni, ji-1,j-1i-1,j-2i-2,j-1i-2,j-2i-3,j-2i-2,j-3\nFigure 6. Constrained local matching paths considered inSmith Waterman, as prescribed by [23].where\u0000is the Kronecker delta function and\u0000(a, b)=8<:0b = 1-0.5 b = 0,a=1-0.7 b = 0,a=09=;(7)The(2\u0000(Bi\u00001,j\u00001)\u00001)term in each line is such thatthere will be a +1 score for a match and a -1 score fora mismatch. The\u0000function is the so-called “afﬁne gappenalty” which gives a score of\u00000.5\u00000.7(g\u00001)for agap of lengthg. The local constraints are to bias SmithWaterman to choosing paths along near-diagonals ofBM.This is important since in musical applications, we do notexpect large gaps in time in one song that are not in theother, which would show up as horizontal or vertical pathsthrough theBMmatrix. Rather, we prefer gaps that occurnearly simultaneously in time for a poorly matched beat orset of beats in an otherwise well-matching section. Fig-ure 6 shows a visual representation of the paths consideredthroughBM.Figure 4 shows an example of a CSM,BM, and result-ing Smith Waterman for a true cover song pair. Severallong diagonals are visible, indicating large chunks of thetwo songs are in correspondence, and this gives rise to alarge score of93.1between the two songs. Figure 5 showsthe CSM,B, and Smith Waterman for two songs which arenot versions of each other. By contrast, there are no longdiagonals, and this pair only receives a score of 8.5. RESULTSTo benchmark our algorithm, we apply it to the standard“Covers 80” dataset [7], which consists of 80 sets of twoversions of the same song, most of which are pop songsfrom the past three decades. There are designated two setsof songs A and B, each with exactly one version of everypair. To benchmark our algorithm on this dataset, we fol-low the scheme in [5] and [8]. That is, given a song fromset A, compute the Smith Waterman score from all songsfrom set B and declare the cover song to be the one withthe maximum score. Note that a random classiﬁer wouldonly get 1/80 in this scheme. The best scores reported onthis dataset are 72/80 [20], using a support vector machineon several different chroma-derived features.Table 1 shows the correctly identiﬁed songs based onthe maximum score, given variations of the parameters wehave in our algorithm. We achieve a maximum score of42/80for a variety of parameter combinations. The near-est neighbor fractionand the dimension of the SSM im-age have very little effect, but increasing the number ofbeats per block has a positive effect on the performance.The stability ofanddare encouraging from a robustnessstandpoint, and the positive effect increasing the numberof beats per block suggests that the shape of medium scalemusical expressions are more discriminative than smallerones.Table 1. The number of songs that are correctly rankedas the most similar in the Covers 80 dataset, varyingparamters.is the nearest neighbor fraction,Bis the num-ber of beats per block, anddis the resized dimension of theEuclidean Self-Similarity images.Kappa = 0.05B=8B = 10B = 12B = 14d = 10030333640d = 20031333639d = 30031343640Kappa = 0.1B=8B = 10B = 12B = 14d = 10035394142d = 20036384242d = 30036384141Kappa = 0.15B=8B = 10B = 12B = 14d = 10036424142d = 20036414142d = 30038424241In addition to the Covers 80 benchmark, we apply ourcover songs score to a recent popular music controversy,the “Blurred Lines” controversy [16]. Marvin Gaye’s es-tate argues that Robin Thicke’s recent pop song “BlurredLines” is a copyright infringement of Gaye’s “Got To GiveIt Up.” Though the note sequences differ between the twosongs, ruling out any chance of a high chroma-based score,Robin Thicke has said that his song was meant to “evoke anera” (Marvin Gaye’s era) and that he derived signiﬁcant in-spiration from “Got To Give It Up” speciﬁcally [16]. With-out making a statement about any legal implications, wenote that our timbral shape-based score between “BlurredLines” and “Got To Give It Up” is in the99.9thpercentileof all scores between songs in group A and group B in the42 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015(a) Shape-based timbre\n(b) Chroma delay embeddingFigure 7. Corresponding portions of the binary cross-similarity matrix between Marvin Gaye’s “Got To Give ItUp” and Robin Thicke’s “Blurred Lines” for both shape-based timbre (our technique) and chroma delay embeddingCovers 80 dataset, for=0.1,B= 14, andd= 200.Unsurprisingly, when comparing “Blurred Lines” with allother songs in the Covers 80 database plus “Got To Give ItUp,” “Got To Give It Up” was the highest ranked. For ref-erence, binary cross similarity matrices are shown in Fig-ure 7, both for our timbre shape based technique and thedelay embedding chroma technique in [25]. The timbre-based cross-similarity matrix is densely populated with di-agonals, while the pitch-based one is not.6. CONCLUSIONS AND FUTURE WORKWe show that timbral information in the form of MFCCcan indeed be used for cover song identiﬁcation. Mostprior approaches have used Chroma-based features aver-aged over intervals. By contrast, we show that an analysisof the ﬁne relative shape of MFCC features over intervalsis another way to achieve good performance. This opensup the possibility for MFCC to be used in much more ﬂex-ible music information retrieval scenarios than traditionalaudio ﬁngerprinting.On the more technical side, we should note that forcomparing shape, L2 of SSMs for cross-similarity is fairlysimple and not robust to local re-parameterizations in timebetween versions, though we tried many other isometryinvariant shape descriptors that were signiﬁcantly slowerand yielded inferior performance in initial implementation.In particular, we tried curvature descriptors (ratio of arclength to chord length), Gromov-Hausdorff distance af-ter fractional iterative closest points aligning MFCC blockcurves [19], and Earth Mover’s distance between SSMs[26]. If we are able to ﬁnd another shape descriptor whichperforms better than our current scheme but is slower, wemay still be able to make it computationally feasible byusing the “Generalized Patch Match” algorithm [1] to re-duce the number of pairwise block comparisons neededby exploiting coherence in time. This is similar in spiritto the approximate nearest neighbors schemes proposedin [28] for large scale cover song identiﬁcation, and wecould adapt their sparse Smith Waterman algorithm to ourproblem. In an initial implementation of generalized patchmatch for our current scheme, we found we only needed toquery about 15% of the block pairs.7. SUPPLEMENTARY MATERIALWe have documented our code and uploaded directions forperforming all experiments run in this paper. We also cre-ated an open source graphical user interface which can beused to interactively view cross-similarity matrices and toexamine the shape of blocks of audio after 3D PCA usingOpenGL. All code can be found in the ISMIR2015 direc-tory atgithub.com/ctralie/PublicationsCode.8. ACKNOWLEDGEMENTSChris Tralie was supported under NSF-DMS 1045133 andan NSF Graduate Fellowship. Paul Bendich was supportedby NSF 144749. John Harer and Guillermo Sapiro arethanked for valuable feedback. The authors would also liketo thank the Information Initative at Duke (iiD) for stimu-lating this collaboration.9. REFERENCES[1]Connelly Barnes, Eli Shechtman, Dan B Goldman,and Adam Finkelstein. The generalized patchmatchcorrespondence algorithm. InComputer Vision–ECCV2010, pages 29–43. Springer, 2010.[2]Juan Pablo Bello. Audio-based cover song retrieval us-ing approximate chord sequences: Testing shifts, gaps,swaps and beats. InISMIR, volume 7, pages 239–244,2007.[3]Bruce P Bogert, Michael JR Healy, and John W Tukey.The quefrency alanysis of time series for echoes:Cepstrum, pseudo-autocovariance, cross-cepstrum andsaphe cracking. InProceedings of the symposium ontime series analysis, volume 15, pages 209–243. chap-ter, 1963.[4]Michael Casey and Malcolm Slaney. The impor-tance of sequences in musical similarity. InAcoustics,Speech and Signal Processing, 2006. ICASSP 2006Proceedings. 2006 IEEE International Conference on,volume 5, pages V–V . IEEE, 2006.[5]Daniel PW Ellis. Identifying’cover songs’ with beat-synchronous chroma features.MIREX 2006, pages 1–4, 2006.[6]Daniel PW Ellis. Beat tracking by dynamic program-ming.Journal of New Music Research, 36(1):51–60,2007.[7]Daniel PW Ellis. The “covers80” cover songdata set.URL: http://labrosa. ee. columbia.edu/projects/coversongs/covers80, 2007.[8]Daniel PW Ellis and Courtenay Valentine Cotton. The2007 labrosa cover song detection system.MIREX2007, 2007.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 43[9]Daniel PW Ellis and Bertin-Mahieux Thierry. Large-scale cover song recognition using the 2d fourier trans-form magnitude. InThe 13th international society formusic information retrieval conference, pages 241–246, 2012.[10]Jonathan Foote. Automatic audio segmentation usinga measure of audio novelty. InMultimedia and Expo,2000. ICME 2000. 2000 IEEE International Confer-ence on, volume 1, pages 452–455. IEEE, 2000.[11]R´emi Foucard, J-L Durrieu, Mathieu Lagrange, andGa¨el Richard. Multimodal similarity between musi-cal streams for cover version detection. InAcousticsSpeech and Signal Processing (ICASSP), 2010 IEEEInternational Conference on, pages 5514–5517. IEEE,2010.[12]Eric J Humphrey, Oriol Nieto, and Juan Pablo Bello.Data driven and discriminative projections for large-scale cover song identiﬁcation. InISMIR, pages 149–154, 2013.[13]Imran N Junejo, Emilie Dexter, Ivan Laptev, andPatrick P´erez. Cross-view action recognition from tem-poral self-similarities. InProceedings of the 10th Eu-ropean Conference on Computer Vision: Part II, pages293–306. Springer-Verlag, 2008.[14]Samuel Kim, Erdem Unal, and Shrikanth Narayanan.Music ﬁngerprint extraction for classical music coversong identiﬁcation. InMultimedia and Expo, 2008IEEE International Conference on, pages 1261–1264.IEEE, 2008.[15]Brian McFee and Daniel PW Ellis. Analyzing songstructure with spectral clustering. In15th InternationalSociety for Music Information Retrieval (ISMIR) Con-ference, 2014.[16]Emily Miao and Nicole E Grimm. The blurred linesof what constitutes copyright infringement of music:Robin thicke v. marvin gayes estate.WESTLAW J. IN-TELLECTUAL PROP ., 20:1, 2013.[17]Oriol Nieto and Juan Pablo Bello. Music segmentsimilarity using 2d-fourier magnitude coefﬁcients. InAcoustics, Speech and Signal Processing (ICASSP),2014 IEEE International Conference on, pages 664–668. IEEE, 2014.[18]Jose A Perea and John Harer. Sliding windows and per-sistence: An application of topological methods to sig-nal analysis.Foundations of Computational Mathemat-ics, pages 1–40, 2013.[19]Jeff M Phillips, Ran Liu, and Carlo Tomasi. Outlier ro-bust icp for minimizing fractional rmsd. In3-D DigitalImaging and Modeling, 2007. 3DIM’07. Sixth Interna-tional Conference on, pages 427–434. IEEE, 2007.[20]Suman Ravuri and Daniel PW Ellis. Cover song de-tection: from high scores to general classiﬁcation.InAcoustics Speech and Signal Processing (ICASSP),2010 IEEE International Conference on, pages 65–68.IEEE, 2010.[21]Justin Salamon, Joan Serr`a, and Emilia G´omez.Melody, bass line, and harmony representations formusic version identiﬁcation. InProceedings of the 21stinternational conference companion on World WideWeb, pages 887–894. ACM, 2012.[22]J Serra. Music similarity based on sequences of de-scriptors: tonal features applied to audio cover songidentiﬁcation.Department of Information and Com-munication Technologies, Universitat Pompeu Fabra,Barcelona, Spain, 2007.[23]Joan Serra, Emilia G´omez, Perfecto Herrera, andXavier Serra. Chroma binary similarity and localalignment applied to cover song identiﬁcation.Audio,Speech, and Language Processing, IEEE Transactionson, 16(6):1138–1151, 2008.[24]Joan Serra, Meinard M¨uller, Peter Grosche, andJosep Lluis Arcos. Unsupervised detection of musicboundaries by time series structure features. InTwenty-Sixth AAAI Conference on Artiﬁcial Intelligence, 2012.[25]Joan Serra, Xavier Serra, and Ralph G Andrzejak.Cross recurrence quantiﬁcation for cover song identi-ﬁcation.New Journal of Physics, 11(9):093017, 2009.[26]Sameer Shirdhonkar and David W Jacobs. Approxi-mate earth movers distance in linear time. InComputerVision and Pattern Recognition, 2008. CVPR 2008.IEEE Conference on, pages 1–8. IEEE, 2008.[27]Temple F Smith and Michael S Waterman. Identiﬁca-tion of common molecular subsequences.Journal ofmolecular biology, 147(1):195–197, 1981.[28]Romain Tavenard, Herv´eJ´egou, and Mathieu La-grange. Efﬁcient cover song identiﬁcation using ap-proximate nearest neighbors. 2012.[29]Juli´an Urbano, Juan Llor´ens, Jorge Morato, and SoniaS´anchez-Cuadrado. Melodic similarity through shapesimilarity. InExploring music contents, pages 338–355. Springer, 2011.44 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Design and Evaluation of a Probabilistic Music Projection Interface.",
        "author": [
            "Beatrix Vad",
            "Daniel Boland",
            "John Williamson 0001",
            "Roderick Murray-Smith",
            "Peter Berg Steffensen"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416670",
        "url": "https://doi.org/10.5281/zenodo.1416670",
        "ee": "https://zenodo.org/records/1416670/files/VadBWMS15.pdf",
        "abstract": "We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist gener- ation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34- dimensional feature space. We use a nonlinear dimen- sionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilis- tic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users’ homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist gener- ation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of sub- jective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the prob- abilistic highlighting mode.",
        "zenodo_id": 1416670,
        "dblp_key": "conf/ismir/VadBWMS15",
        "keywords": [
            "probabilistic interface",
            "music exploration",
            "casual playlist generation",
            "low-level audio features",
            "34-dimensional feature space",
            "nonlinear dimensionality reduction",
            "music maps",
            "visualizations of probabilistic mappings",
            "user feedback",
            "mouse activity logs"
        ],
        "content": "DESIGN AND EVALUATION OF A PROBABILISTIC MUSICPROJECTION INTERFACEBeatrix Vad1Daniel Boland1John Williamson1Roderick Murray-Smith1Peter Berg Steffensen21School of Computing Science, University of Glasgow, United Kingdom2Syntonetic A/S, Copenhagen, Denmarkmail@bea-vad.de,{daniel,jhw,rod}@dcs.gla.ac.uk, pbs@syntonetic.comABSTRACTWe describe the design and evaluation of a probabilisticinterface for music exploration and casual playlist gener-ation. Predicted subjective features, such as mood andgenre, inferred from low-level audio features create a 34-dimensional feature space. We use a nonlinear dimen-sionality reduction algorithm to create 2D music maps oftracks, and augment these with visualisations of probabilis-tic mappings of selected features and their uncertainty.We evaluated the system in a longitudinal trial in users’homes over several weeks. Users said they had fun with theinterface and liked the casual nature of the playlist gener-ation. Users preferred to generate playlists from a localneighbourhood of the map, rather than from a trajectory,using neighbourhood selection more than three times moreoften than path selection. Probabilistic highlighting of sub-jective features led to more focused exploration in mouseactivity logs, and 6 of 8 users said they preferred the prob-abilistic highlighting mode.1. INTRODUCTIONTo perform information retrieval on music, we typicallyrely on either meta data or on ‘intelligent’ signal process-ing of the content. These approaches create huge featurevectors and as the feature space expands it becomes harderto interact with. A projection-based interface can providean overview over the collection as a whole, while show-ing detailed information about individual items in context.Our aim is to build an interactive music exploration tool,which offers interaction at a range of levels of engagement,which can foster directed exploration of music spaces, ca-sual selection and serendipitous playback. It should pro-vide a consistent, understandable and salient layout of mu-sic in which users can learn music locations, select musicand generate playlists. It should promote (re-)discovery ofmusic and accommodate widely varying collections.c\u0000Beatrix Vad, Daniel Boland, John Williamson, RoderickMurray-Smith, Peter Berg Steffensen. Licensed under a Creative Com-mons Attribution 4.0 International License (CC BY 4.0).Attribution:Beatrix Vad, Daniel Boland, John Williamson, Roderick Murray-Smith,Peter Berg Steffensen. “Design and evaluation of a probabilistic musicprojection interface”, 16th International Society for Music InformationRetrieval Conference, 2015.To address these goals we built and evaluated a systemto interact with 2D music maps, based on dimensionally-reduced inferred subjective aspects such as mood andgenre. This is achieved using a ﬂexible pipeline of acousticfeature extraction, nonlinear dimensionality reduction andprobabilistic feature mapping. The features are generatedby the commercial Moodagent Proﬁling Service1for eachsong, computed automatically from low-level acoustic fea-tures, based on a machine-learning system which learnsfeature ratings from a small training set of human subjec-tive classiﬁcations. These inferred features are uncertain.Subgenres of e.g. electronic music are hard for expert hu-mans to distinguish, and even more so for an algorithmusing low-level features [24]. This motivates representingthe uncertainty of features in the interaction.It is not straightforward to evaluate systems based oninteracting with such high-dimensional data. This is nota pure visualisation task. Promoting understanding is sec-ondary to offering a compelling user experience, where theuser has a sense of control. How do we evaluate projec-tions, especially if the user’s success criterion is just toplay something ’good enough’ with minimal effort? Weevaluated our system to answer:1. Can a single interface enable casual, implicit and fo-cused interaction for music retrieval?2. Which interface features better enable people to nav-igate and explore large music collections?3. Can users create viable mental models of a high-dimensional music space via a 2D map?2. BACKGROUND2.1 Arranging music collections on ﬁxed dimensionsA music retrieval interface based on a 2D scatter plot withone axis ranging from slow to fast and the other from darkto bright on the timbre dimension is presented in [10].The authors show this visualisation reduces time to se-lect suitable tracks compared to a traditional list view. [11]presents a 2D display of music based on the establishedarousal-valence (A V) diagram of emotions [20], with A Vjudgements obtained from user ratings. An online explo-ration toolmusicovery.com[6] enables users to selecta mood in the A V space and starts a radio stream based1http://www.moodagent.com/134Figure 1. (a) An audio collection, described by a large set of features automatically extracted from the content. (b)visualisation of this high-dimensional dataset in two dimensions using dimensionality reduction (c) probabilistic modelsshowing the distribution of speciﬁc features in the low dimensional space (d) combining dimensionality reduction withthese models to build an interactive exploration interface.on the input. These use two predeﬁned dimensions thatare easy to interpret, however they do not allow a broaderinterpretation of musical characteristics based on richerfeature sets. [13] ﬁnds that music listening is often basedupon mood. The investigation of musical preferences in [9]shows most private collections consist of a wide range ofstyles and approaches to categorisation.2.2 Music visualisations via dimensionality reduction“Islands of Music” [17] visualises music collections us-ing a landscape metaphor. They use rhythmic patterns ina set of frequency bands to create a Self-Organizing Map(SOM), a map of music for users to explore. Similarly, [16]introduce the SOM-based PlaySOM and PocketSOM in-terfaces. Features are again based on rhythm and 2D em-bedding. An interesting visualisation feature is the use of“gradient ﬁelds” to illustrate the distribution of featuresover the map. Playlist generation is enabled with a rect-angular marquee and path selection. Elevations are basedon the density of songs in the locality, so clustered songsform islands with mountains. A collection of 359 pieceswas used to evaluate the system and song similarities weresubjectively evaluated. An immersive 3D environment formusic exploration, again using a SOM is described in [14].An addition to previous approaches is an integrated feed-back loop that allows users to reposition songs, alter theterrain and position landmarks. The users’ sense of simi-larity is modelled and the map gradually adapted. Both theSOM landscape and acoustic clues improved search timesper song.SongWords [2] is an interactive tabletop application tobrowse music based on lyrics. It combines a SOM witha zoomable user interface. The app is evaluated in a userstudy with personal music collections of ca. 1000 items.One reported issue was that only the item positions de-scribed the map’s distribution of characteristics. Users hadto infer the structure of the space from individual items.“Rush 2” explores interaction styles from manual to auto-matic [1]. They use similarity measures to create playlistsautomatically by selecting a seed song.A detailed overview of music visualisation approachesand the MusicGalaxy system is contributed with [23]. Thiswork introduces adaptive methods for music visualisation,allowing users to adjust weightings in the projection. Italso explores the use of a lens so that users could zoom intoparts of the music space. Most notably, it receives a signif-icant amount of user evaluation. The lack of such evalua-tions in the ﬁeld of MIR has been noted in [21], which callsfor a user-centred approach to MIR. The work in this pa-per thus includes an ‘in-the-wild’ longitudinal evaluation,bringing HCI methodology to bear in MIR.2.3 Interaction with music visualisationsPath drawings on a music visualisation, enabling high-level control over songs and progression of createdplaylists can be found in [26]. Casual interaction has re-cently started receiving attention from the HCI commu-nity [18], outlining how interactions can occur at varyinglevels of engagement. A radio-like interface that adapts touser engagement is introduced by [3, 4]. It allows users tointeract with a stream of music at varying levels of control,from casual mood-setting to engaged interaction. Musicvisualisations can also span engagement – from broad se-lections in an overview to speciﬁc zoomed-in selections.3. PROBABILISTIC MUSIC INTERFACEAs shown in Figure 1, the interface builds on featuresderived from raw acoustic characteristics and transformsthese into a mood-based visualisation, where nearby songswill have a similar subjective “feeling”. Our feature extrac-tion service provides over thirty predicted subjective fea-tures for each song including mood, genre, style, vocals,instrument, beat, tempo, energy and other attributes. Thefeatures associated with moods chosen for highlighting inthe visualisation includeHappy, Angry, SadandTender.These were identiﬁed as relevant moods from social tagsin [12].Erotic, FearandTempo(not strictly a mood) werealso included. The features were investigated in [5].Given our large number of features, we need dimension-ality reduction to compress the data from|F|dimensionsto|D|dimensions. The goal of this step is to preserve sub-jective similarities between songs and maintain coherentstructure in the dataset. For interaction, we reduce downProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 135to 2D. We tried our system with a number of dimension-ality reduction techniques including PCA and SOM. Wechose the t-distributed stochastic neighbour embedding (t-SNE, [25]) model for non-linear dimensionality reductionto generate a map entangling a global overview of clustersof similar songs and yet locally minimise false positives.To provide additional information about the composi-tion of the low-dimensional space, we developedproba-bilistic modelsto visualise high dimensional features in thelow-dimensional space. This probabilistic back-projectiongives users insight into the structure of the layout, but alsointo the uncertainties associated with the classiﬁcations.On top of the pipeline (Figure 1), we built an efﬁcient, scal-able web-based UI which can handle music collections up-wards of 20000 songs. The tracks can be seen as randomvariables drawn from a probabilistic distribution with re-spect to a speciﬁc feature. The distribution parameters canbe estimated and used for prediction, allowing smoothedinterpolation of features as shown in Figure 2. We usedGaussian Process (GP) priors [19], a powerful nonpara-metric Bayesian regression method. We applied a squaredexponential covariance function on the 2D(x, y)coordi-nates, predicting the mood featuresPfover the map. TheGP can also infer the uncertainty\u00002fof the predicted fea-ture relevance for each point [22].\nFigure 2. Gaussian Process predictions of features. Or-ange denotes the “happy” feature distribution and blue de-notes “tender”. The greyscale surface shows the uncer-tainty; lighter is more certain and darker is less certain.3.1 Interface designTo present the inferred subjective results to the users,the GP mean and standard deviation is evaluated over a200⇥200grid covering the 2D music space. A continu-ously colouredbackground highlightingis created whereareas of high feature scores stand out above areas withhigher uncertainty or lower scores. To highlight areaswith high prediction scores and low uncertainty, a non-linear transform is used:↵f=P2f\u0000\u00002f, for each moodfeaturef, having a standard deviation\u0000fand a predictedfeature valuePf. The clusters in the music space can beemphasised as in the upper part of Figure 3 by colouringareas with the colour associated with the highest score; i.e.argmax(↵f)– a winner-takes-all view. This not only di-vides the space into discrete mood areas but also showsnuanced gradients of mood inﬂuences within those areas.However, once a user starts to dynamically explore a spe-ciﬁc area of the space, the system transitions toimplicitbackground highlightingsuch that the background distri-bution of the mood with the highest value near the cursoris blended in dynamically as in the lower plots of Figure 3,giving the user more subtle insights into the nature of thespace.Tracks are represented as circles in a scatter plot, wheresize can convey information, e.g. the popularity of a song,without disturbing the spatial layout. To support visualclustering, colour highlights the highest scoring mood fea-ture of each song, and transparency conveys the featurescore. However, the number of diverging, bright coloursfor categorisation is limited. Murch [15] states that a “re-focus” is needed to perceive different pure colours, somatched pairs of bright and desaturated colours are cho-sen for the correlated pairstender/sad,happy/eroticandangry/fear.\nFigure 3. Top: The interactive web interface in its ‘win-ner takes all’ overview colouring. A path playlist selectionas well as a neighbourhood selection is visible in the moodspace. Bottom: Background highlighting for the featuresangry,tenderanderotic. Compared with the overviewcolouring, the subtle ﬂuctuations of features are apparent.3.2 Interaction with the interfaceAs the visualisation can handle very large numbers ofitems, asemantic zoomwas integrated, where the size ofeach element is ﬁxed. This coalesces items on zoom outand disentangles items on zoom in.Further insight into the nature of the space is given bytheadaptive area explorationtool which visualises the lo-cal item density. In contrast to previous work we do notuse a ﬁxed selection area but one based on thek-nearest-neighbours to the mouse cursor. Points are highlightedas the mouse is moved, creating a dynamically expandingand collapsing highlight, responding to the structure of thespace. Thek-nn visualisation adapts to zoom level; when136 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015zoomed out,kis large; when zoomed in, we support fo-cused interaction, with a smallerk.Focus and context:To make the music map explorationmore concrete, a hover box is displayed with informationabout the nearest item to the cursor, including artist name,title, and album art (see Figure 3). It shows a mini barchart of the song’s mood features. As this is ﬁxed onscreen,users can explore and observe changes in the mood chart,giving them insight into the high-dimensional space.3.3 Playlist generationNeighbourhood selectionis a quick and casual interactionmetaphor for creating a playlist from theknearest neigh-bours. Songs are ranked according to their query point dis-tance. This enables the directed selection of clusters inthe space, even if the cluster is asymmetric. By adjustingzoom level (and thusk),k-NN selection can include all in-cluster items while omitting items separated from the per-ceived cluster. This feature could be enhanced by addingan✏-environment similar to the density-based clustering al-gorithm DBSCAN [7]. Fast rendering and NN search wasimplemented using quadtree spatial indexing [8].Path selectionenables space-spanning selections.Drawing a path creates a playlist which ‘sticks’ to nearbyitems along the way. The local density of items is con-trolled by modulating velocity, so faster trajectory sectionsstick to fewer songs than slow ones. This ‘dynamic attach-ment’ offers control over the composition of playlists with-out visual clutter. E.g. a user can create a playlist startingin thehappyarea, then gradually migrating towardstender.4. USER EVALUATIONThe evaluation was based on theresearch questions:1. How do users perceive the low-dimensional mood spaceprojection? 2. Is the mood-based visualisation useful inmusic exploration and selection? 3. Which techniques dousers develop to create playlists?A pilot study evaluated the viability of the system andguided the design of the main longitudinal “in the wild”user study, which was conducted to extract detailed us-age behaviour over the course of several weeks. Adaptingto a new media interface involves understanding how per-sonal preferences and personal media collections are rep-resented. Longitudinal study is essential for capturing thebehaviour that develops over time, beyond superﬁcial aes-thetic reactions and can – in contrast to Lab-based study –cover common use cases (choose tracks for a party, playsomething in the background while studying).Eight participants (1 female, 7 male, 5 from the UK and3 from Germany, undergraduate and research students) –each with their own Spotify account and personal musiccollections – were recruited. The mood interface was usedto visualise the personal music collection of the partici-pants. The participants used the interface at home as theirmusic player to whatever extent and in whatever way theywanted. Two participants also used the system at work.All subjects used a desktop to access the interface. As areward and to facilitate use together with the Spotify WebPlayer, participants were given a voucher for a three monthpremium subscription of Spotify.The Shannon entropyHof the 6 mood features of eachuser’s music collection gives an impression of the diver-sity of content. Using the maximum mood feature for eachsong,H=\u0000Pipilog2pi, wherepi=Nmoodi/N.12345678H2.51 2.36 2.49 2.42 1.84 2.38 1.93 2.5N3679 2623 4218 3656 2738 2205 1577 3781Table 1. EntropyH, no. tracksNof users’ collections.The study took place in two blocks, each with nomi-nally four days of usage, although the actual duration var-ied slightly. One of the key aims was to ﬁnd out if the prob-abilistic background highlighting provides an enhanced ex-perience, so the study was comprised of two conditions ina counterbalanced within-subjects arrangement:AMusic Map without background highlighting.BMusic Map with background highlighting: The proba-bilistic models are included, with the composite view ofthe mood distribution as well as dynamic mood highlight-ing on cursor movements. Each participant was randomlyassigned either conditionAin week 1 followed byBinweek 2 or vice versa. At the beginning of each condi-tion and the end of the study, questionnaires were adminis-tered to capture participants’ experience with the interface.Interface events, including playlist generation, navigationand all mouse events (incl. movements) were recorded.5. RESULTSMost participants used the software extensively, generat-ing an average of 21 playlists per user per week, as shownin Table 2. On average, users actively interacted with thesystem for 77 minutes each week (roughly 20 minutes aday) – time spent passively listening to playlists is not in-cluded in this ﬁgure. Both groups generated more playlistsin week 1 than in week 2, as they explored the system.User 1 2 3 4 5 6 7 8Np,A27 164 4 7 18 8 5 25Np,B46 39 3 7 5 17 18 53Table 2. No. playlists generated per user for cond. A & B.Users 1-5 had A in week 1, while 6-8 had A in week 2.5.1 Mood perceptionAfter each condition, users were asked to rate their satis-faction with interacting via the mood space. The overallopinion was encouraging. The majority of participants re-ported that they felt their collection was ordered in a mean-ingful way. Six stated that the mood-based categorisationmade sense. Initially, the distinction of different musictypes was not rated as consistently over all conditions. Thismight be due to the fact that people usually discuss musicin terms of genres rather than moods. However, the dif-ﬁculty rating of mood changed over sessions. While sixusers rated mood-based categorisation as difﬁcult at theProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 137start, only three participants still rated mood as difﬁcultto categorise by the second week. This suggests that userscan quickly learn the unfamiliar mood-based model.5.2 Interactions with the Mood SpaceBrowsing the Space:Analysis of mouse movements pro-vided insight into how participants explored mood spaces.Heatmaps were generated showing the accumulated mousepositions in each condition (Figure 4). Participants ex-plored the space more thoroughly in week one of thestudy. Some participants concentrated exploration onsmall pockets, while others explored the whole space rela-tively evenly.\nFigure 4. Heatmaps of interaction (mouse activity) of user3 in week 1 (left) and week 2 (right). Interaction becomesmore focused in the second week.The browse/select ratio dropped noticeably for the sec-ond week for users with conditionAﬁrst, as shown in Ta-ble 3. This suggests that participants browsed much morefor each playlist in the ﬁrst part of the study, and were moretargeted in the second part. The browsing could have beeneither curiosity-driven exploration, or a frustrating experi-ence, because the non-linear nature of the mapping madethe space difﬁcult for the users to predict the response tomovements in the space. However, from Table 3 we cansee that users who had the highlights in the ﬁrst weekseemed to have much more focused navigation from thestart, and did not decrease their browsing much in week 2when they lost the highlighting mechanism.Condition A Condition BWeek 11218.39(483.49)447.94(176.85)Week 2319.25(29.27)576.5(416.72)Table 3. Browse select ratios (std. dev. in parentheses) forweek 1 and week 2 of the experiment, in cond. A and B.Selections and Playlist Creation:Figure 5 shows playlistsfrom two different participants in conditionB. User 1(left) created playlists by neighbourhood selection, andalso drew a few trajectory playlists. User 7 (right) movedin over a more diverse set with a number of trajectoryplaylists. The paths partially follow the contours of thebackground highlight, which suggests this user exploredcontrasts in the mood space on these boundaries.Neighbourhood selection was used more often (341neighbourhood selections and 105 path selections). A rise\nFigure 5. Created playlists under condition B for user 1(H=2.51) and user 7 (H=1.93). Note the differentclass layouts for the collections with high/low entropyH.in the use of path selections, and a decline in neighbour-hood selections can be seen in conditionBversusA. InconditionA, ﬁve times more neighbourhood than path se-lections were recorded, and only twice as many in condi-tionB(see Table 4). This could be explained by the back-ground distributions suggesting mood inﬂuence changegradually over the space. This information may encourageusers to create trajectory playlists that gradually changefrom one mood intensity to another.Selection A B TotalPath 42 63 105Neighbourhood 216 125 341Neighbourhood/Path 5.1⇥2.0⇥3.3⇥Table 4. Usage of the two different selection types ineach condition. The neighbourhood/path ratio shows theincreased use of the path tool in condition B.5.3 Qualitative feedbackBackground Highlighting:We asked whether backgroundhighlighting was valuable to the users. The answer wasclearly in favour of background highlighting: 6/8 usersvalued the highlighting, one user was indifferent and onepreferred the version without highlighting. The reasonsgiven in favour of the highlighting were that they couldmore easily identify different regions and remember spe-ciﬁc “locales” in the mood space. They recognised thatsongs had different mood inﬂuences and enjoyed follow-ing the colour highlights to areas of different intensity. Oneuser stated that he liked the vividness of the implicit high-lighting. The user who preferred no highlighting found it acleaner look that was less confusing. 6 participants statedthat they did not ﬁnd the highlighting confusing. 7 par-ticipants answered that it did not distract from the playlistcreation task. Qualitative feedback also indicated a prefer-ence for highlighting:”[with highlighting] I could easieridentify how the mood was distributed over my library”,”coloured areas provided some kind of ’map’ and ’land-marks’ in the galaxy”.Preference for neighbourhood versus path playlists:Thedomination of neighbourhood versus path playlists in thelogged data is supported by feedback from questionnaires,138 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015which shows that users were generally happier with neigh-bourhood selection than the more novel path selectiontechnique. The attitude towards the path selection differedbetween conditions. Participants were more satisﬁed withpath selection in conditionB, with interactive backgroundhighlighting.InA, four participants agreed the path playlistwas effective, and three disagreed. After conditionB, how-ever, 5 users agreed and only one disagreed.Advantages of the Interface:The subjective feedback re-vealed that users had fun exploring the mood space andenjoyed the casual creation of playlists.”fun to explorethe galaxy”, ”easy generation of decent playlists”Usersalso appreciated the casual nature of the interface:”Itwas very easy to take a hands-off approach”, ”I didn’thave to think about speciﬁc songs”.Users made spe-ciﬁc observations indicating that they were engaged in theexploration task and learned the structure of the map, al-though this varied among users.”I discovered that mostolder jazz pieces were clustered in the uppermost corner”,”It was easy to memorize such ﬁndings [...] the galaxythus became a more and more personal space”.Satisfac-tion with the quality of selections was high, although someparticipants found stray tracks that did not ﬁt with neigh-bouring songs.”The detected mood was a bit off for afew songs”.Several users stated that they appreciated theconsistency of created playlists and the diversity of differ-ent artists, in contrast to their usual artist-based listening.There was concern that playlists did not offer enough di-versity”some songs that dominated the playlists”, ”toomuch weight given to ’archive’ material”, ”some way toreorder the playlists to keep them fresh”,while others en-joyed this aspect:”I rediscovered many songs I had notlistened to in a long time”.Shared versus personal:Visualising a shared (i.e. inter-user) mood space with personal collections embedded wasnot rated very important by most users (only one userthought this important). However, personalisation of thespace was rated of high importance by half of the users.Ensuring that nearby songs are subjectively similar was ad-ditionally rated as important by the majority of participants(ﬁve users). These user priorities led to trade-offs betweenvery large music maps and maps reliably uncovering in-trinsic clusters of similar items.Improvement requests:The most requested missing featurewas a text search feature. The use of Spotify for playbackalso led to a disjointed user experience which would beeasily improved on in a fully integrated mood-map musicplayer. Users also requested the integration of recommen-dations and the ability to compare different mood spaces.6. CONCLUSIONS AND FUTURE WORKWe presented an interactive tool for music exploration,with musical mood and genre inferred directly from tracks.It features probabilistic representations of multivariablepredictions of subjective characteristics of the music togive users subtle, nuanced visualisations of the map. Theseexplicitly represent the vagueness and overlap among fea-tures. The user-based, in-the-wild evaluation of this novelhighlighting technique provided answers to the initial re-search questions:Can users create viable mental models of the music space?The feedback from the ‘in-the-wild’ evaluation indicatesthat people enjoyed using these novel interfaces on theirown collections, at home, and that mood-based categorisa-tion can usefully describe personal collections, even if ini-tially unfamiliar. Analysis of logged data revealed distinctstrategies in experiencing the mood space. Some usersexplored diverse parts of the mood space and switchedamong them, while others quickly homed in on areas ofinterest and then concentrated on those. The question-naire responses suggest they learned the composition of thespace and used it more constructively in the later sessions.Users make plausible mental models of the visualisation –they know where the favourite songs are – and can use thismodel to discover music and formulate playlists.Which interface features enable people to navigate and ex-plore the music space?Interactive background highlight-ing seemed to reduce the need to browse intensively withthe mouse (Table 3). Subjective feedback conﬁrmed that ithelped understand the music space with 6/8 users prefer-ring it over no highlighting. Most users did not feel dis-turbed by the implicitly changing background highlight-ing. Both the neighbourhood and path playlist generatorswere used by the participants, although neighbourhood se-lections were subjectively preferred and were made threetimes more often than path selections. Subjective feedbackhighlights the contrast between interfaces which adapt toan individual user taste or reﬂect a global model, in whichall users can collaborate, share and discuss music, tradinggreater relevance versus greater communicability. Simi-larly, how can we adapt individual user maps as the user’smusical horizons are expanded via the exploratory inter-face? Users’ preference of comparing visualisations overinteracting in one large music space hints that an alignmentof visualisations is a valid solution to this problem.Can a single interface enable casual, implicit and focusedinteraction?Users valued the ability to vary the levelof engagement. Their feedback also suggested that in-corporating preview and control over the playing time ofplaylists would be useful, e.g. move towards “happy” over35 minutes. A recurring theme was that playlists tendedto be repetitive. One solution would be to allow the jitter-ing of playlist trajectories and to do this jittering in high-dimensional space. The low-dimensional path then speci-ﬁes a prior in the high-dimensional music space which canbe perturbed to explore alternative expressions of that path.Post-evaluation:An enhanced version with a text search function was dis-tributed at the end of the study. The encouraging result wasthat a month later, 3 of 8 participants still returned to theinterface on a regular basis – once every few days, with oneuser generating 68 new playlists in the following weeks.AcknowledgmentsPartially supported by Danish Councilfor Strategic Research: CoSound project, 11-115328Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1397. REFERENCES[1] Dominikus Baur, Bernhard Hering, Sebastian Boring,and Andreas Butz. Who needs interaction anyway? Ex-ploring mobile playlist creation from manual to auto-matic.Proc. 16th Int. Conf. on Intelligent User Inter-faces, pages 291–294, 2011.[2] Dominikus Baur, Bartholomaus Steinmayr, and An-dreas Butz. SongWords: Exploring music collectionsthrough lyrics. InProc. ISMIR), pages 531–536, 2010.[3] Daniel Boland, Ross McLachlan, and RoderickMurray-Smith. Inferring Music Selections for CasualMusic Interaction.EuroHCIR, pages 15–18, 2013.[4] Daniel Boland, Ross McLachlan, and RoderickMurray-Smith. Engaging with mobile music retrieval.InMobileHCI 2015, Copenhagen, 2015.[5] Daniel Boland and Roderick Murray-Smith.Information-theoretic measures of music listeningbehaviour. InProc. ISMIR, Taipei, 2014.[6] Vincent Castaignet and Frederic Vavrille. (23.04.2014)http://musicovery.com/.[7] Martin Ester, Hans P Kriegel, Jorg Sander, and Xi-aowei Xu. A Density-Based Algorithm for Discover-ing Clusters in Large Spatial Databases with Noise. InSecond Int. Conf. on Knowledge Discovery and DataMining, pages 226–231, 1996.[8] Raphael A. Finkel and Jon Louis Bentley. Quad treesa data structure for retrieval on composite keys.ActaInformatica, 4(1):1–9, 1974.[9] Alinka Greasley, Alexandra Lamont, and John Slo-boda. Exploring Musical Preferences: An In-DepthQualitative Study of Adults’ Liking for Music in TheirPersonal Collections.Qualitative Research in Psychol-ogy, 10:402–427, 2013.[10] Jiajun Zhu and Lie Lu. Perceptual Visualization of aMusic Collection. In2005 IEEE Int. Conf. on Multi-media and Expo, pages 1058–1061. IEEE, 2005.[11] JungHyun Kim, Seungjae Lee, SungMin Kim, andWon Young Yoo. Music mood classiﬁcation modelbased on arousal-valence values.13th Int. Conf. on Ad-vanced Comm. Technology, pages 292–295, 2011.[12] Cyril Laurier, Mohamed Sordo, Joan Serr`a, and Per-fecto Herrera. Music mood representations from socialtags. InProc. ISMIR, pages 381–386, 2009.[13] Adam J Lonsdale and Adrian C North. Why do we lis-ten to music? A uses and gratiﬁcations analysis.BritishJournal of Psychology, 102(1):108–134, 2011.[14] Matthias L¨ubbers, Dominik and Jarke. Adaptive Mul-timodal Exploration of Music Collections. InProc. IS-MIR, pages 195–200, 2009.[15] Gerald M. Murch. Physiological principles for the ef-fective use of color.Computer Graphics and Applica-tions, IEEE, 4(11):48–55, 1984.[16] Robert Neumayer, Michael Dittenbach, and AndreasRauber. PlaySOM and PocketSOMPlayer, alternativeinterfaces to large music collections. InProc. ISMIR,pages 618–623, 2005.[17] Elias Pampalk, Andreas Rauber, and Dieter Merkl.Content-based organization and visualization of mu-sic archives.Proc. 10th ACM Int. Conf. on Multimedia,page 570, 2002.[18] Henning Pohl and Roderick Murray-Smith. Focusedand casual interactions: allowing users to vary theirlevel of engagement. InProc. ACM SIGCHI Conf. onHuman Factors in Computing Systems, pages 2223–2232, 2013.[19] Carl Edward Rasmussen and Christopher K. I.Williams.Gaussian Processes for Machine Learning.MIT Press, 2006.[20] James A. Russell. A circumplex model of affect.Jour-nal of Personality and Social Psychology, 39:1161–1178, 1980.[21] Markus Schedl and Arthur Flexer. Putting the User inthe Center of Music Information Retrieval. InProc. IS-MIR, Porto, Portugal, 2012.[22] Devinderjit Sivia and John Skilling. Data Analysis: ABayesian Tutorial.Technometrics, 40(2):155, 1998.[23] Sebastian Stober.Adaptive Methods for User-CenteredOrganization of Music Collections. PhD thesis, Otto-von-Guericke-Universitt Magdeburg, 2011.[24] Bob L. Sturm. A simple method to determine if a musicinformation retrieval system is a horse.IEEE Transac-tions on Multimedia, 16(6):1636–1644, 2014.[25] Laurens van der Maaten and Geoffrey Hinton. Visual-izing Data using t-SNE.Journal of Machine LearningResearch, 9:2579–2605, 2008.[26] Rob van Gulik and Fabio Vignoli. Visual playlist gen-eration on the artist map. InProc. ISMIR, pages 520–523, 2005.140 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Improving Music Recommendations with a Weighted Factorization of the Tagging Activity.",
        "author": [
            "Andreu Vall",
            "Marcin Skowron",
            "Peter Knees",
            "Markus Schedl"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416802",
        "url": "https://doi.org/10.5281/zenodo.1416802",
        "ee": "https://zenodo.org/records/1416802/files/VallSKS15.pdf",
        "abstract": "Collaborative filtering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix fac- torization techniques that fuses the implicit feedback de- rived from the users’ listening activity with the tags that users have given to musical items. In contrast to exist- ing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging ac- tivity. We evaluate the model for the task of artist recom- mendation, using the expected percentile rank as metric, extended with confidence intervals to enable the compar- ison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different rec- ommender systems.",
        "zenodo_id": 1416802,
        "dblp_key": "conf/ismir/VallSKS15",
        "keywords": [
            "Collaborative filtering",
            "implicit feedback",
            "music recommendations",
            "matrix factorization",
            "weighted factorization",
            "tags",
            "artist recommendation",
            "evaluation methodology",
            "confidence intervals",
            "twofold contribution"
        ],
        "content": "IMPROVING MUSIC RECOMMENDATIONS WITH A WEIGHTEDFACTORIZATION OF THE TAGGING ACTIVITYAndreu Vall Marcin Skowron Peter Knees Markus SchedlDepartment of Computational Perception, Johannes Kepler University, Linz, Austria{andreu.vall, marcin.skowron, peter.knees, markus.schedl}@jku.atABSTRACTCollaborative ﬁltering systems for music recommendationsare often based on implicit feedback derived from listeningactivity. Hybrid approaches further incorporate additionalsources of information in order to improve the quality ofthe recommendations. In the context of a music streamingservice, we present a hybrid model based on matrix fac-torization techniques that fuses the implicit feedback de-rived from the users’ listening activity with the tags thatusers have given to musical items. In contrast to exist-ing work, we introduce a novel approach to exploit tagsby performing a weighted factorization of the tagging ac-tivity. We evaluate the model for the task of artist recom-mendation, using the expected percentile rank as metric,extended with conﬁdence intervals to enable the compar-ison between models. Thus, our contribution is twofold:(1) we introduce a novel model that uses tags to improvemusic recommendations and (2) we extend the evaluationmethodology to compare the performance of different rec-ommender systems.1. INTRODUCTION AND RELATED WORKWe provide the motivation of our work together with a re-view of the relevant related work, divided into three parts.First, we introduce the types of user feedback under con-sideration. Then, we present the family of models we useto build recommender systems. Finally, we review theevaluation methodology.1.1 Explicit, Implicit and One-Class FeedbackThe interactions between users and items provide a use-ful source of data to produce recommendations [16]. It iscommonly accepted to distinguish betweenexplicit feed-backandimplicit feedback, depending on whether the useractively provides feedback about an item or this is trackedfrom the user’s interaction with the system [1]. Examplesof explicit feedback are rating a movie, giving a ”like” toa blog post, or tagging an artist, because the user activelyc\u0000Andreu Vall, Marcin Skowron, Peter Knees, MarkusSchedl. Licensed under a Creative Commons Attribution 4.0 Inter-national License (CC BY 4.0).Attribution:Andreu Vall, MarcinSkowron, Peter Knees, Markus Schedl. “Improving music recommen-dations with a weighted factorization of the tagging activity”, 16th Inter-national Society for Music Information Retrieval Conference, 2015.provides an opinion. In contrast, the listening histories ofusers in a music streaming service are an example of im-plicit feedback.The standard approach to make use of implicit feed-back is to count or aggregate all the interactions for eachuser-item pair [5, 7, 8], yielding a user-item-count table.In structure, this is identical to an explicit feedback user-item-rating table. We henceforth refer to such data struc-ture asuser-item interactions matrix, regardless of the typeof feedback (implicit or explicit).In some cases a user-item interaction can express bothpositive and negative opinions, in other cases it only re-ﬂects positive (or active) examples. Ratings in a1to5scale conventionally range from strongly disliking an itemto strongly liking it. However, tracking whether a user vis-ited or not a website, only provides a binary feedback de-scribing action or inaction. Binary feedback is often re-ferred to asone-class feedback[12, 15, 17], and examplesof it can be found both in explicit and in implicit feed-back. For example, a user-item interactions matrix (be itfrom explicit or implicit feedback) contains intrinsically asource of one-class feedback, revealing which user-itempairs were observed and which not.Inaction must not be confused with a negative opinion,because a user may not have interacted with an item for avariety of reasons, not necessarily because of lack of inter-est. Social tags also exhibit this property, and treating thiscorrectly will be a key point of the presented model.1.2 Matrix Factorization for Collaborative FilteringCollaborative ﬁltering is a widely used recommendationmethod which aims at recommending the most relevantitems to a user based on relations learned from previous in-teractions between users and items [16]. The factorizationof the user-item interactions matrix into latent factors ma-trices is a well established technique to implement collab-orative recommender systems, both for explicit feedbackand implicit feedback datasets [7, 10, 15]. Compared toother methods, it has the advantage of uncovering latentdata structures by solving an optimization problem, insteadof using problem-speciﬁc and manually-designed features.Speciﬁc collaborative systems for implicit feedback databased on matrix factorization techniques are presented in[7, 15]. The key technique is to use appropriate weights inthe low-rank approximation of the user-item interactionsmatrix. More speciﬁcally, even if the weighting schemesare different, both [7, 15] assign higher conﬁdence to the65observed user-item pairs and lower (but still positive) con-ﬁdence to the unobserved user-item pairs. This is impor-tant to handle the uncertainty derived from the one-classproperty described before. We will insist on this point later,because our improved treatment of the tagging activity willrest on the same principle.1.3 Hybrid Recommender SystemsIn collaborative ﬁltering implementations based on matrixfactorization techniques, hybrid models can be based onthe simultaneous factorization of the user-item interactionsmatrix, together with other data for users and items [5,13].The motivation for that is that latent factors summarizinguser and item properties should be reinforced, or better de-scribed, if other data sources related to the same users anditems are involved in the optimization problem.The tags that users assign to musical items –or otherforms of textual data, like user proﬁles or genre annota-tions for the items– are an obvious example of potentiallyuseful additional information. In this line, the research pre-sented in [5] is a valid starting point, dealing with implicitfeedback data and hybridized with user and item proﬁles,built on the basis of tf-idf weights calculated for each user,each item and each considered word in a dictionary.Tagging information is an explicit source of feedback(because users actively provide it) that exhibits, at the sametime, the one-class property described before; the tags as-signed to musical items are only positive examples (evenif the meaning of a tag is semantically negative). A par-ticular tag may not have been applied to a musical item,but this does not imply that the tag is not suited to describethat musical item. This property of social tags is also re-ferred to asweak labeling[18]. It is reasonable to assumethough, that the more often a tag has been applied to a mu-sical item, the more it should be trusted. Similarly, if a userapplies a tag very often, it may be assumed that the tag isto some extent relevant for her to describe musical items.To address the uncertainty that arises from this wide rangeof possibilities, we propose to exploit the tagging activitywith a weighted matrix factorization scheme similar to theone applied for collaborative ﬁltering in implicit feedbackdatasets. Observed tags can be given higher conﬁdence.Unobserved tags can be given lower conﬁdence, but stillpositive, so that they are not ignored in the recommenda-tion system.1.4 Evaluation of Recommender SystemsThe Netﬂix Prize [3] has motivated an important progressin the domain of collaborative ﬁltering, but probably due tothe speciﬁc approach considered in the challenge, researchhas centered on attaining maximum levels of accuracy inthe prediction of ratings. However, improvements in pre-dictive accuracy do not always translate to improved usersatisfaction [14].To make the evaluation task more similar to a real usecase (although still in an off-line experiment), [9] evalu-ates different recommender systems on the basis of issuedranked lists of recommendations. A recommender able torank ﬁrst the relevant items should be considered betterthan a recommender that is not able to do so. An exten-sion of this evaluation methodology to deal with implicitfeedback datasets is proposed in [7] and applied in [8, 12].It consists in a central tendency measure, calledexpectedpercentile rank, assessing how good is the recommender atidentifying relevant items.The expected percentile rank is a valid metric to mea-sure the average behavior of a single recommender system,but in order to compare the performance of different rec-ommender systems, considering only mean values can beinaccurate. We propose to use bootstrapping techniquesto examine the distribution of the expected percentile rankand test for signiﬁcant differences between models.2. METHODOLOGYThis work is framed in the context of music streaming ser-vices in which users interact with musical items, mainlylistening to music, but also through the free input of textdescribing them. We focus on the task of artist recom-mendations. The listening data is aggregated at the artistlevel, obtaining a user-artist-count matrix of implicit feed-back. The tagging activity yields a user-artist-tag matrix ofone-class feedback, processed to obtain: a user-tag-countmatrix, describing how many times a user applied a tag,and an artist-tag-count matrix, describing how many timesa tag was applied to an artist. The proposed model isactually ﬂexible regarding the tagging activity data. Inour experiments, we successfully use a collection of topused tags (not a complete list of all the used tags) togetherwith weights describing the tag relevance (instead of actualcounts).2.1 Recommender System ModelsWe compare three recommender systems. The ﬁrst is astandard collaborative ﬁltering model for implicit feedbackdata. The second is a hybrid model incorporating textualdata, that we modify for the speciﬁc task of using tags.Finally, we introduce a novel model, able to improve thequality of the recommendations through a weighted fac-torization of the tagging activity.2.1.1 Implicit Feedback Matrix Factorization (MF)We use the approach described in [7] to perform collabo-rative ﬁltering on implicit feedback data. It consists in aweighted low-rank approximation of the user-artist-countmatrix, adjusting the conﬁdence of each user-artist pair as afunction of the count. Given a system withNusers andMartists, the counts for each user-artist pair are tabulated in amatrixR2NN⇥M, where users are stored row-wise andartists column-wise. A binary matrixeRis deﬁned, suchthat for each useruand each artistaeRua=(1ifRua>00ifRua=0,(1)and the following weight function is deﬁned asw(⌘,x)=1+⌘log(1 +x).(2)66 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Other weight functions can be deﬁned and may better suiteach speciﬁc problem and distribution of the data. Wechoose a logarithmic relation (instead of the also commonlinear relation used in [5, 7, 8]) to counteract the long-taildistribution of the data, where a majority of users have asmall percentage of the total observed interactions. How-ever, the detailed optimization of this function is not withinthe scope of this work.Finally, the matrix factorization consists in ﬁnding twoD-rank matricesP2RN⇥DandQ2RM⇥D(rows arelatent features for users and artists respectively) minimiz-ing the following cost function:JMF(P,Q)=Xua2Rw(↵, Rua)⇣eRua\u0000PuQTa⌘2+\u0000\u0000kPk2F+kQk2F\u0000.(3)MatrixeRis reconstructed usingPandQ.eRuais the en-try ofeRcorresponding to useruand artista.Puis therow ofPcorresponding to useru, andQais the row ofQcorresponding to artista. The squared reconstructionerror is weighted using a function of the actual counts inRuaaccording to equation (2) and it is summed over allthe user-artist pairs.1The parameter↵contributes to theweight function and is determined by grid search. A reg-ularization term involving the Frobenius norm ofPandQis added to prevent the model from over-ﬁtting. The regu-larization parameter\u0000is also determined by grid search.2.1.2 Implicit Feedback Matrix Factorization withTagging Activity (TMF)Equation (3) is extended in [5] to incorporate textual infor-mation. We present a modiﬁcation of this model to specif-ically deal with tags. Given a system whereTtags havebeen used, the counts for each user-tag pair are stored in amatrixTU2NN⇥T, where rows correspond to users andcolumns correspond to tags. The counts for each artist-tagpair are stored in a matrixTA2NM⇥T, where rows corre-spond to artists and columns correspond to tags. The mod-iﬁed model factorizes togethereR,TUandTAinto threeD-rank matricesP2RN⇥D,Q2RM⇥D,X2RT⇥D(rows are latent features for users, artists and tags respec-tively) minimizing the following cost function:JTMF(P,Q,X)=Xua2Rw(↵, Rua)⇣eRua\u0000PuQTa⌘2+µ1Xut2TU\u0000TUut\u0000PuXTt\u00002+µ2Xat2TA\u0000TAat\u0000QaXTt\u00002+\u0000⇣kPk2F+kQk2F+kXk2F⌘.(4)The ﬁrst term is identical as in (3). The second and thirdterms account for the contribution of tags.Xtis the row1As described in [7], this includes the zero entries ofRas well.ofXcorresponding to tagt. MatricesTUandTAare re-constructed usingP,QandX, and the squared reconstruc-tion errors are summed over all user-tag pairs and artist-tagpairs. The parametersµ1,µ2account for the contributionof each term to the cost function, and are determined bygrid search. The regularization term is analogous as in (3).This formulation modiﬁes the one described in [5], inthat it factorizesTUandTAusing a single shared tags’factor matrixX, instead of two dedicated factor matri-ces. The tagging activity consists of user-artist-tag obser-vations. Even if we use separated user-tag-count and artist-tag-count matrices as inputs for the model, the tags must befactorized in the same space of latent features.This model factorizes the user-tag and artist-tag rawcounts. If, for example, an artist-tag pair has never beenobserved, the model will try to ﬁt a value of0counts for it.This seems an unsuited model, because we know that a tagthat has not been applied may still be relevant.2.1.3 Implicit Feedback Matrix Factorization withWeighted Tagging Activity (WTMF)We introduce a novel approach to improve the hybridiza-tion with tagging activity, by using a weighted factoriza-tion scheme similar to the one used for implicit feedbackdata. The observed user-tag and artist-tag pairs are givenhigh conﬁdence and therefore have a higher contributionto the cost function. The unobserved user-tag and artist-tag pairs are given low conﬁdence. They become less rel-evant in the cost function, and at the same time the modelhas more freedom to ﬁt them. As the results in Section 3.3demonstrate, this is a better approach to model the weaklabeling property of social tags.We deﬁne binary matriceseTUandeTA, such that foreach useru, each artistaand each tagteTUut=(1ifTUut>00ifTUut=0eTAat=(1ifTAat>00ifTAat=0.(5)We factorize togethereR,eTUandeTAinto threeD-rankmatricesP2RN⇥D,Q2RM⇥DandX2RT⇥D(rowsare latent features for users, artists and tags respectively)minimizing the following cost function:JWTMF(P,Q,X)=Xua2Rw(↵, Rua)⇣eRua\u0000PuQTa⌘2+µ1Xut2TUw(\u0000,TUut)⇣eTUut\u0000PuXTt⌘2+µ2Xat2TAw(\u0000,TAat)⇣eTAat\u0000QaXTt⌘2+\u0000⇣kPk2F+kQk2F+kXk2F⌘.(6)The equation is similar to (4), but now all the terms in-volve a weighted factorization. Note that the second andProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 67third terms have speciﬁc weight coefﬁcients\u0000and\u0000, de-termined by grid search.2.2 Parameter EstimationAlternating Least Squares (ALS) is usually the preferredmethod to minimize the objective functions of models basedon matrix factorization [2, 5–8, 15, 19]. ALS is an iterativemethod, where subsequently all but one of the factor ma-trices are kept ﬁxed. This results in quadratic functionsthat approximate the original one. At each step, the costvalue is expected to move closer to a local minimum andthe process is repeated until convergence. Since the ap-proximated functions are quadratic, the exact solution forthe factors can be computed in closed form.For each of the presented models, we provide the exactsolution for the factors of each userustored inPu, eachartistastored inQaand each tagtstored inXt. We in-troduce some additional notation.Rru,Rca,TUra,TUct,TAra,TActrefer to theuth,ath,tthrow or column (r, c) of the cor-responding matrix (R, TU,TA).2We also need to deﬁnethe following matrices:•WruR2RM⇥Mis a diagonal matrix with the weightscomputed for theuthrow ofRin the diagonal•WcaR2RN⇥Nis a diagonal matrix with the weightscomputed for theathcolumn ofRin the diagonal•WruTU2RT⇥Tis a diagonal matrix with the weightscomputed for theuthrow ofTUin the diagonal•WctTU2RN⇥Nis a diagonal matrix with the weightscomputed for thetthcolumn ofTUin the diagonal•WraTA2RT⇥Tis a diagonal matrix with the weightscomputed for theathrow ofTAin the diagonal•WctTA2RM⇥Mis a diagonal matrix with the weightscomputed for thetthcolumn ofTAin the diagonal2.2.1 Solution forJMFFor each useruand artista, the latent factors are given by(Pu=\u0000QTWruRQ+\u0000I\u0000\u00001\u0000QTWruRRTru\u0000Qa=\u0000PTWcaRP+\u0000I\u0000\u00001\u0000PTWcaRRTca\u0000(7)2.2.2 Solution forJTMFFor each useru, artistaand tagt, the latent factors aregiven by8>>>>>>>>>><>>>>>>>>>>:Pu=\u0000QTWruRQ+µ1XTX+\u0000I\u0000\u00001\u0000QTWruRRTru+µ1XTTUTra\u0000Qa=\u0000PTWcaRP+µ2XTX+\u0000I\u0000\u00001\u0000PTWcaRRTca+µ2XTTATra\u0000Xt=\u0000µ1PTP+µ2QTQ+\u0000\u0000\u00001\u0000µ1PTTUTct+µ2QTTATct\u0000(8)\n2TUandTAmay be further transposed, readingTUTandTAT.2.2.3 Solution forJWTMFFor each useru, artistaand tagt, the latent factors aregiven by8>>>>>>>>>><>>>>>>>>>>:Pu=\u0000QTWruRQ+µ1XTWruTUX+\u0000I\u0000\u00001\u0000QTWruRRTru+µ1XTWruTUTUTra\u0000Qa=\u0000PTWcaRP+µ2XTWraTAX+\u0000I\u0000\u00001\u0000PTWcaRRTca+µ2XTWraTATATra\u0000Xt=\u0000µ1PTWctTUP+µ2QTWctTAQ+\u0000\u0000\u00001\u0000µ1PTWctTUTUTct+µ2QTWctTATATct\u0000(9)2.3 Producing RecommendationsThe technique employed to produce recommendations isthe same for all the models. Once the factor matricesP,QandXare learned, the user-artist preferences are predictedasZ=PQT. Note that the tags’ factor matrixXis notdirectly involved in the prediction, although it contributedto a better estimation ofPandQ. The new matrixZisexpected to be a reconstruction ofeRfor the observed user-artist pairs. For unobserved entries,Zis expected to revealpotential preferences on the basis of the learned user andartist factors. The closer a predicted user-artist preferenceis to1, the more conﬁdence we have that it corresponds toan interesting artist for the user. For each useru, a recom-mendation list is prepared showing the artists with higherpredicted preference values inZu.3. EXPERIMENTAL STUDY3.1 DatasetWe compare the different models on a dataset of Last.fmlistening histories, top tags used by users and top tags ap-plied to artists, collected through the Last.fm API.3Thecombination of the standard Taste Proﬁle Subset4with theLast.fm tags dataset5would seem a preferable choice, butthe absence of users’ tagging activity makes it unsuited.The dataset is built as a stable subset of a running crawlof Last.fm listening events. The original crawl includesonly users with non-empty country information, non-emptygender information and a value in the age ﬁeld between10and80years, although such ﬁltering is actually not needed.There is no constraint on the minimum or maximum num-ber of artists a user has listened to. However, we only in-clude users such that at least95%of their listened artistshave a valid MusicBrainz6identiﬁer, which is required toaccurately crawl the artists’ tagging activity. This does notbias the dataset towards popular artists, because the Mu-sicBrainz is an open and collaborative platform, includ-ing a wide variety of artists. The users’ tagging activityis fetched with the Last.fm user names.3http://www.last.fm/api4http://labrosa.ee.columbia.edu/millionsong/tasteproﬁle5http://labrosa.ee.columbia.edu/millionsong/lastfm6https://musicbrainz.org/68 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015# listened artists # users1\u000010 6411\u000020 8421\u000030 12231\u000040 7741\u000050 9650\u0000100 466101\u00002,332 1,993total2,902Table 1: Distribution of users per numberof listened artists.The dataset includes21,852,559listening events, re-lating to2,902users and71,223artists, yielding687,833non-zero user-artist-count entries. This corresponds to amatrix density of roughly0.3%. Table 1 shows the distri-bution of users as a function of the number of artists theylistened to.The top tags for each user (if any) are provided togetherwith a count variable describing how many times the userapplied it. The top tags applied to an artist (if any) are pro-vided together with a percentage relative to the most fre-quently applied tag [11]. Because the API functions onlyreturn the top tags, we only observe a partial set of the tag-ging activity. In addition, although the user is presentedwith previously used tags, she can always input free text.To overcome these limitations, we perform regularizationand simpliﬁcation operations to the tag strings, namely: re-placements of genre abbreviations with their extended ver-sion, spelling corrections, removal of non-alphanumericcharacters and mapping of different spelling variants to aunique tag string, resulting in a uniﬁed set of tokens. Afterthis process is applied to the fetched tags, we are left with630unique tags for600users and12,902unique tags for67,332artists, among which494unique tags are identiﬁedas identical between the user and the artist list. Note thattags were found for most of the artists, but only for20%ofthe users. Probably, only a small subset of active users usethe tagging functionality.The whole matrix of user-artist counts is used, althoughnot all users or artists have related tagging activity. Tagsare a complement whenever they are available.3.2 Evaluation MethodologyThe most reliable evaluation method for a recommendersystem is an actual large-scale on-line experiment, wherereal users interact with the system [16]. This requires acomplex infrastructure which, unfortunately, is not withinthe scope of this work. Since we only have access to his-torical data, we can not measure how new recommenda-tions would be perceived by the users. Furthermore, incontrast to explicit feedback applications, accuracy met-rics for predicted ratings are not meaningful for implicitfeedback. Therefore, we adopt the evaluation approachproposed in [9] and adapted in [7] to deal with implicitfeedback datasets in a recall-oriented setting and we addi-tionally propose an extension to it.The observed user-artist pairs are split into training andtest sets to perform5-fold cross validation, letting eachuser have approximately80%of the listened artists in thetraining set and20%in the test set. For each user-artistpairu, aassigned to the test set, a random list of artists(not includinga) is drawn. The list is then ranked accord-ing to the preferences of useru, learned from the trainingset as explained in Section 2. Finally,ais inserted in thesorted list, and its percentile rank within the list is storedasrankua.7Ifais ranked among the top positions of thelist, then its percentile rank is close to0%. If it is rankedin last positions, then its percentile rank is close to100%.After this process is done over all the splits,rankuaisknown for all the observed user-artist pairs in the dataset.Then, following [5, 7, 8], theexpected percentile rankisdeﬁned as the weighted average ofrankuawith weightsgiven by the user-artist counts:rank=Xua2RRuarankuaXua2RRua.(10)Correctly ranking a highly relevant artist is more impor-tant than correctly ranking a less relevant artist. Likewise,failing to recommend a highly relevant artist is worse thanfailing to recommend a less relevant one. Values ofrankclose to0%indicate that the recommender is able to cor-rectly rank the relevant artists. Producing ranked lists uni-formly at random results in an expected percentile rank of50%. Ranking all the relevant items in the last position ofthe list results in an expected percentile rank of100%.We extend the evaluation methodology by building con-ﬁdence intervals ofrank. This allow us to test for signif-icant differences in the performance of models. We usebasic bootstrap conﬁdence intervals, based on the boot-strap distribution of the expected percentile rank (see [4]).For all the observed user-artist pairs in the dataset, randomsamples with replacement and with the same size as thedataset are drawn. For each sample of user-artist pairs, theexpected percentile rank is computed. We repeat this step1,000times to obtain the bootstrap distribution ofrank.We then build95%conﬁdence intervals ofrankusing thebasic bootstrap scheme described in [4].3.3 Model ComparisonThe models are evaluated and compared for a varying num-ber of latent factorsD, and for a varying number of train-ing iterations. On the one hand, we ﬁx the number of itera-tions to10and evaluate the models with5,10,20,50,100latent factors. On the other hand, we ﬁx the number offactors to10and evaluate the models for5,10,20,50,100training iterations. We choose10factors and10training7Lists of any length may be prepared, and the percentile rank providesa uniﬁed scale. We use lists of100artists in our experiments. Accordingto our experience, longer lists do not yield signiﬁcant differences.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 69MF\nTMFWTMF1.5%2.0%2.5%3.0%3.5%\n5 10 20 50 100Number of FactorsExpected Percentile RankFigure 1: Model comparison for different number of latentfactors. The dots correspond torankand the error barsdisplay95%basic bootstrap conﬁdence intervals. The dif-ferent models are dodged to avoid overlapping. The topand center lines correspond to the baseline models. Thelowest corresponds to the presented model.iterations as a basic setting, because they balance well per-formance and computational requirements.For each model and each combination of factors and it-erations, we tune the parameters↵,\u0000,\u0000,µ1,µ2and\u0000bygrid search. We choose the set of values that provides low-est expected percentile rank, computed by5-fold cross val-idation as described in Section 3.2. Figures 1 and 2 showthe results for different number of factors and iterations re-spectively.Note that all models, including the plain matrix factor-ization model, provide very good results, with values ofexpected percentile rank under4%. This implies that, onaverage, the models are able to rank relevant artists amongthe top4positions of a list of100random artists.The performance of TMF and WTMF improves signif-icantly when more latent factors are used (see Figure 1).The presented model outperforms the baselines, althoughfor100factors the difference between TMF and WTMFis small. We examine this case. We compute a95%ba-sic bootstrap conﬁdence interval for the difference ofrankand it does not include0. We conclude that the differencein performance is still signiﬁcant. For lower number offactors the differences between the presented model andthe baselines are remarkable. Good performance at inex-pensive computational requirements is a crucial property,especially for large-scale implementations.Increasing the number of training iterations results insmaller improvements (see Figure 2). Our model clearlyoutperforms the baselines in this set of experiments too,with a difference of nearly1%in expected percentile rank.With the basic setting of10factors, TMF can not fully ex-ploit the tagging activity and performs comparably to MF.For experiments with20or more training iterations theyperform exactly as well, because the grid search processﬁnds that discarding the tagging activity yields best results.MFTMF\nWTMF2.1%2.4%2.7%3.0%3.3%\n5 10 20 50 100Number of IterationsExpected Percentile RankFigure 2: Model comparison for different number of train-ing iterations. The dots correspond torankand the errorbars display95%basic bootstrap conﬁdence intervals. Thedifferent models are dodged to avoid overlapping. The toplines correspond to the baseline models. The lowest corre-sponds to the presented model.TMF performs slightly better than MF with5training iter-ations. The performance of TMF does not improve mono-tonically with more training iterations, although the modelis not over-ﬁtting. This is because after5iterations thecost function of TMF reaches a ﬂat region close to a localminimum, resulting in small performance variations.4. CONCLUSIONS AND FURTHER RESEARCHIn this paper we presented a novel model to incorporatetagging activity into implicit feedback recommender sys-tems. Our approach proves to work better than previoushybrid models, based on experiments conducted with realdata from Last.fm, a well-known music streaming service.We extended the common evaluation methodology com-puting basic bootstrap conﬁdence intervals for the expectedpercentile rank. This allows us to test for signiﬁcant differ-ences in the performance of models.As future work, we will evaluate the robustness of thepresented model for different recommendation tasks. Weare particularly interested in the task of song recommenda-tions, but we will also experiment in ﬁelds other than mu-sic, like movies or websites. Another interesting questionis the effect of the size and connectedness of the taggingdata on the ﬁnal quality of the recommendations. We willinvestigate how rich and linked together needs to be thetagging activity in order to enhance the recommendations.This could provide indications of when can the model besuccessfully utilized, or which kind of processing of thetag strings is required to make the tagging activity helpful.5. ACKNOWLEDGMENTSThis research is supported by the Austrian Science Fund(FWF) under project no. P25655 and the EU FP7 throughprojects 601166 (PHENICX) and 610591 (GiantSteps).70 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20156. REFERENCES[1]Gediminas Adomavicius and Alexander Tuzhilin. To-ward the next generation of recommender systems: asurvey of the state-of-the-art and possible extensions.IEEE Transactions on Knowledge and Data Engineer-ing, 17(6):734–749, June 2005.[2]Robert M. Bell and Yehuda Koren. Scalable collabo-rative ﬁltering with jointly derived neighborhood inter-polation weights. InProc. ICDM, pages 43–52. IEEE,2007.[3]James Bennett and Stan Lanning. The netﬂix prize. InProc. KDDCup, page 35, 2007.[4]Thomas J. DiCiccio and Bradley Efron. Bootstrap con-ﬁdence intervals.Statistical science, pages 189–212,1996.[5]Yi Fang and Luo Si. Matrix co-factorization for recom-mendation with rich side information and implicit feed-back. InProc. HETREC, pages 65–69. ACM, 2011.[6]K. Ruben Gabriel and S. Zamir. Lower Rank Ap-proximation of Matrices by Least Squares with AnyChoice of Weights.Technometrics, 21(4):489, Novem-ber 1979.[7]Yifan Hu, Yehuda Koren, and Chris V olinsky. Collab-orative ﬁltering for implicit feedback datasets. InProc.ICDM, pages 263–272. IEEE, 2008.[8]Christopher C. Johnson. Logistic Matrix Factorizationfor Implicit Feedback Data. 2014.[9]Yehuda Koren. Factorization meets the neighborhood:a multifaceted collaborative ﬁltering model. InProc.SIGKDD, pages 426–434. ACM, 2008.[10]Yehuda Koren, Robert Bell, and Chris V olinsky. Ma-trix factorization techniques for recommender systems.Computer, 42(8):30–37, 2009.[11]M. Levy and M. Sandler. Music information retrievalusing social tags and audio.IEEE Transactions on Mul-timedia, 11(3):383–395, April 2009.[12]Yanen Li, Jia Hu, ChengXiang Zhai, and Ye Chen. Im-proving one-class collaborative ﬁltering by incorporat-ing rich user information. InProc. CIKM, pages 959–968. ACM, 2010.[13]Hao Ma, Tom Chao Zhou, Michael R. Lyu, and IrwinKing. Improving recommender systems by incorporat-ing social contextual information.ACM Transactionson Information Systems, 29(2):1–23, April 2011.[14]Sean M. McNee, John Riedl, and Joseph A. Konstan.Being accurate is not enough: How accuracy metricshave hurt recommender systems. InProc. CHI’06 Ex-tended Abstracts, pages 1097–1101. ACM, 2006.[15]Rong Pan, Yunhong Zhou, Bin Cao, Nathan Nan Liu,Rajan Lukose, Martin Scholz, and Qiang Yang. One-class collaborative ﬁltering. InProc. ICDM, pages502–511. IEEE, 2008.[16]Francesco Ricci, Lior Rokach, Bracha Shapira, andPaul B Kantor, editors.Recommender systems hand-book. Springer, 2011.[17]Vikas Sindhwani, Serhat S. Bucak, Jianying Hu, andAleksandra Mojsilovic. One-class matrix completionwith low-density factorizations. InProc. ICDM, pages1055–1060. IEEE, 2010.[18]Douglas Turnbull, Luke Barrington, and Gert Lanck-riet. Five approaches to collecting tags for music. InProc. ISMIR, volume 8, pages 225–230, 2008.[19]Yunhong Zhou, Dennis Wilkinson, Robert Schreiber,and Rong Pan. Large-scale parallel collaborative ﬁlter-ing for the netﬂix prize. InAlgorithmic Aspects in In-formation and Management, pages 337–348. Springer,2008.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 71"
    },
    {
        "title": "Evaluation of Album Effect for Feature Selection in Music Genre Recognition.",
        "author": [
            "Igor Vatolkin",
            "Günter Rudolph",
            "Claus Weihs"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416328",
        "url": "https://doi.org/10.5281/zenodo.1416328",
        "ee": "https://zenodo.org/records/1416328/files/VatolkinRW15.pdf",
        "abstract": "With an increasing number of available music characteris- tics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another ad- vantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to “over- optimisation” when data in the optimisation set is too dif- ferent from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called “album effect” meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of gen- res. For that case we examine the performance of classifi- cation models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classifica- tion performance on the second set decreases. Neverthe- less, in almost all cases the feature selection remains ben- eficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance.",
        "zenodo_id": 1416328,
        "dblp_key": "conf/ismir/VatolkinRW15",
        "keywords": [
            "feature selection",
            "music genre recognition",
            "optimisation",
            "album effect",
            "classification performance",
            "ensemble of classifiers",
            "robust generalisation",
            "MFCCs",
            "relevant properties",
            "redundant ones"
        ],
        "content": "EVALUATION OF ALBUM EFFECT FOR FEATURE SELECTION INMUSIC GENRE RECOGNITIONIgor VatolkinTU DortmundDepartment of Computer Scienceigor.vatolkin@udo.eduG¨unter RudolphTU DortmundDepartment of Computer Scienceguenter.rudolph@udo.eduClaus WeihsTU DortmundFaculty of Statisticsclaus.weihs@udo.eduABSTRACTWith an increasing number of available music characteris-tics, feature selection becomes more important for variouscategorisation tasks, helping to identify relevant featuresand remove irrelevant and redundant ones. Another ad-vantage is the decrease of runtime and storage demands.However, sometimes feature selection may lead to “over-optimisation” when data in the optimisation set is too dif-ferent from data in the independent validation set. In thispaper, we extend our previous work on feature selectionfor music genre recognition and focus on so-called “albumeffect” meaning that optimised classiﬁcation models mayoveremphasize relevant characteristics of particular artistsand albums rather than learning relevant properties of gen-res. For that case we examine the performance of classiﬁ-cation models on two validation sets after the optimisationwith feature selection: the ﬁrst set with tracks not used fortraining and feature selection but randomly selected fromthe same albums, and the second set with tracks selectedfrom other albums. As it can be expected, the classiﬁca-tion performance on the second set decreases. Neverthe-less, in almost all cases the feature selection remains ben-eﬁcial compared to complete feature sets and a baselineusing MFCCs, if applied for an ensemble of classiﬁers,proving robust generalisation performance.1. INTRODUCTIONAmong many different scenarios for automatic classiﬁca-tion of music data (we refer to [4] for an introduction tocontent-based music information retrieval and an overviewof related tasks), the recognition of high-level music cate-gories such as music genres and styles is one of the mostprominent and user-related applications. Probably the ﬁrststudy on automatic categorisation of music was addressedto distinguish between several classical and popular pieces[22]. After the seminal work of Tzanetakis and Cook onclassifying musical data into a hierarchy of 25 music gen-res and speech categories [38] many efforts were spent toc\u0000Igor Vatolkin, G¨unter Rudolph, Claus Weihs.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Igor Vatolkin, G¨unter Rudolph, ClausWeihs. “Evaluation of Album Effect for Feature Selection in Music GenreRecognition”, 16th International Society for Music Information RetrievalConference, 2015.enhance the methods, develop new features, and integrateactual techniques from machine learning research [42]. [37]lists several hundreds of studies related only to the recog-nition of genres. Since 2005, audio genre classiﬁcationbelongs to tasks of the annual MIREX contest [6].The operating principle of supervised classiﬁcation isbased on two stages: the training of a classiﬁcation modelCTand its applicationCon uncategorised data:CT:\u0000X2RF⇥TTR,yL2RTTR\u00007! M,C:\u0000X2RF⇥T,M\u00007!yP2RT. (1)Given a set ofFnumeric data characteristics, or fea-tures, forTTRdata instances (also referred to as classiﬁ-cation windows) resulting in the feature matrixX, and thecorresponding labelsyL, the training stage identiﬁes rele-vant dependencies between features and labels and storesthem as a modelM. Some approaches are based on the es-timation of probability-based distribution of features (NaiveBayes) or boundaries between data instances of differentcategories (support vector machines); for an overview ofclassiﬁcation approaches see, e.g., [13, 43]. Once the clas-siﬁcation models are saved, they can be applied to classifyTunlabelled data instances represented by the sameFpre-viously extracted features.Music classiﬁcation can be carried out using featuresfrom different sources. For instance, the score allows aprecise estimation of harmonic, instrumental, and rhyth-mic descriptors of music pieces, but it is not always avail-able for popular music. Meta data, cultural features, ortags provide another source of information, but are some-times incomplete or erroneous. Audio features can be ex-tracted for every digitised music piece, and many classiﬁ-cation approaches are limited to or focused on this kind offeatures [9, 18, 19, 21, 27, 34, 36, 38, 40]. Another advan-tage of these characteristics is that they are not dependenton the popularity of a song, availability of the score, or In-ternet connection for the download of metadata. Even ifaudio features typically require high computing efforts fortheir extraction, these costs can be reduced to a certain de-gree if the extraction is done ofﬂine or on a server farm. Inthat case only the time for the training and the applicationof classiﬁcation models will inﬂuence a user’s satisfactionduring the deﬁnition of new categorisation tasks. For thesereasons we have limited the scope of this study to audiofeatures only.169Having a large number of available descriptors at hand,individual features may be very important or completelyuseless depending on a categorisation task. As the com-bination of features from different sources may increasethe classiﬁcation quality (e.g., as shown for audio, sym-bolic, and cultural features in [26]), the inclusion of fea-tures from many sources would lead to an increased num-ber of both relevant and irrelevant features. If the numberof irrelevant features would become too high, the classiﬁ-cation quality may suffer because the probability increasesthat some irrelevant features are identiﬁed as relevant bychance [13, 43]. A solution is to start with a sufﬁcientlylarge initial feature set and to remove irrelevant and noisycharacteristics for a current category by means of featureselection (FS). Other beneﬁts of FS are that classiﬁcationmodels created with less features often require less storagespace, the classiﬁcation is done faster, and the danger ofoverﬁtting towards the training set may be reduced using aproper evaluation of models and feature sets [3].In our previous work we have applied feature selectionfor the recognition of music genres and styles and mea-sured a signiﬁcant increase of classiﬁcation performancecompared to complete feature sets [39]. For the ﬁnal eval-uation of models optimised with feature selection, we usedan independent validation set with tracks not used for modeltraining and feature selection. The motivation for an in-dependent evaluation in music classiﬁcation is discussedin [9]. However, strictly observed, the validation set usedin our previous experiments was not completely indepen-dent: due to the limited size of our music database, mu-sic pieces for validation were different from training andoptimisation sets, but randomly selected from the same al-bums. Therefore, a danger existed that optimised classiﬁ-cation models would have an especially high performanceon music pieces of the same artists and albums.Such effect was observed in [30] for the recognitionof genres. Also the tags of songs belonging to the samealbums may have higher co-occurrences as inspected in[20]. Further investigations showed interesting results onthe difference between album and artist effect for musicdatabases of different sizes [10] as well as varying im-pact of artist ﬁlter with regard to music from different ge-ographic locations around the world [15]. However, noneof these studies explicitly evaluated the sensitivity of FS toartist/album effect using a large number of features. Suchevaluations can be promising in future, in particular be-cause both latter papers stated differences in measured artisteffect for different feature groups, even if the overall num-bers of integrated features were not very high.Thus, the idea behind this study was to re-evaluate themeasured advantage of feature selection using a new “album-independent” validation set and to estimate the album ef-fect for different music categories. In the next section,we outline basic concepts of feature selection and refer toseveral applications. Section 3 describes the setup of thestudy. In Section 4, the results and the album effect onfeature selection are discussed. We conclude with a briefsummary of the work and outline steps for future research.2. FEATURE SELECTIONFor an exhaustive introduction into feature selection meth-ods see [12]. In general, the task of feature selection is toﬁnd an optimal feature subset indicated by the binary vec-torq(qi=1for thei-th feature to be selected, otherwiseqi=0), so that some relevance function, or evaluation cri-terionm(e.g., classiﬁcation error) is minimised. The func-tions to maximise (e.g., accuracy) can be easily adapted forminimisation. We deﬁne the task of feature selection as:q⇤= arg minq[m(yL,yP,\u0000(x,q))],(2)where\u0000(x,q)corresponds to the subset of the originalfeature vectorx.yL2[0; 1]are the labelled category re-lationships of classiﬁcation instances, andyP2[0; 1]arethe predicted category relationships. Note that in generalmmay not necessarily depend on labels, e.g., if the corre-lation between features is used as selection criterion, or iflabels are not available (as in unsupervised classiﬁcation).Feature selection with regard to only one evaluation cri-terion may lead to a decrease of performance for otherones. For example, classiﬁcation models built with toomany features may have smaller classiﬁcation errors fora speciﬁc data set, but be slower and have a poor general-isation performance on other data. Therefore, several rel-evance functions or objectivesm1,. . . ,mOmay be consid-ered for simultaneous optimisation:q⇤= arg minq[m1(yL,yP,\u0000(x,q)),. . . ,mO(yL,yP,\u0000(x,q))].(3)In literature, individual features are often referred to asrelevant or redundant w.r.t. the performance of a Bayesianclassiﬁer which predicts labels based on a probabilistic dis-tribution of feature vectors. For a given feature setX,afeature subsetX0⇢Xis calledrelevant, iff its removalwill decrease the performance of a Bayesian classiﬁer:P(yP|yL=yP,X)<P(yP|yL=yP,X\\X0)andP(yP|yL6=yP,X)>P(yP|yL6=yP,X\\X0). (4)Aredundantfeature subsetX0can be replaced withoutdecrease of a Bayesian classiﬁer’s performance by at leastone subsetS, which does not containX0:9S✓X,X0\\S=;:P(yP|X)=P(yP|S).(5)The equations (4)–(5) can be adapted to any relevancefunction, describing a decrease of performance after the re-moval of relevant features and retaining it after the removalof redundant features.FS is a very complex task: forFfeatures, the numberof all possible non-empty feature subsets is2F\u00001, and therelated problems were described as NP-hard [1,14]. There-fore, metaheuristics like evolutionary algorithms (EA) [33]which simulate the natural evolution based on principlesof recombination (keeping the positive characteristics ofsolutions) and mutation (exploring the search space using170 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015some random procedure) are a possible remedy. EAs haveproven their ability to solve many complex optimisationtasks, among others for data mining and classiﬁcation [28].The ﬁrst application of EAs for FS was introduced in [35],and EAs were recommended for sets with more than 100features in [16] after the comparison of 18 FS methods.FS has been already often applied for music classiﬁ-cation, for example for the recognition of musical instru-ments (44 features) [5], moods (66 features) [34], or sev-eral classiﬁcation tasks (between 60 and 1140 features)[25]. Evolutionary FS was integrated in music classiﬁ-cation for the ﬁrst time in [11] and was applied also inlater studies, e.g., [8, 27, 36]. The ﬁrst application of evo-lutionary multi-objective algorithms to FS for the simul-taneous minimisation of the number of features and mis-classiﬁcation rate was proposed in [7]. In music classiﬁ-cation, multi-objective evolutionary feature selection wasintroduced in [40] for genre categorisation and later for therecognition of instruments [41].In the following, we will describe the study to measurethe impact of two-objective FS (minimisation of the clas-siﬁcation error and the number of features) on the classiﬁ-cation into music genres and styles. Classiﬁcation resultsare compared to models built with full feature sets and abaseline with MFCCs. Further, we will investigate the sen-sitivity of the proposed method to the album effect.3. EXPERIMENTAL SETUP3.1 Categorisation TasksWe distinguish between music genres and styles providedby AllMusicGuide, where a track may belong to one mu-sic genre and up to several music styles which are morespeciﬁc and are typically harder to predict.Our main database for experiments consists of 120 al-bums with approximately one third of commercial popu-lar music (45 Pop/Rock albums) as well as tracks of sev-eral other genres for a better evaluation of generalisationperformance (15 albums of each genre Classic, Electronic,Jazz, Rap, and R&B). For the evaluation of the album ef-fect the database was extended with 120 songs from al-bums of other artists but the same genre and similar styledistribution. It is important to mention that we use ourown database, because many publicly available ones werenot well suited for this work. Several databases containonly segments of songs so that it is not possible to extractfeatures from long frames (e.g., structural complexity, seethe next section). Others are strongly biased towards cer-tain genres or are expensive because of a large share ofcommercial music. These problems could be in principleavoided using data sets with features only (e.g., Echo Nestdescriptors). However, a sufﬁciently large number of audiofeatures is necessary to measure the impact of feature se-lection, and many descriptors are developed by ourselvesbeing not available in freely distributed feature sets.We distinguish between training, optimisation, and twotest sets (all of them are disjunct on track level, i.e. it isnot permitted to have the same track in more than oneset). Each classiﬁcation model istrainedfrom 20 tracks,10 of which belong to the category to predict (positive ex-amples), and 10 do not belong to it (negative examples).These small training sets are motivated by the real-worldsituation, where a listener would like to omit high effortsfor the labelling of ground truth. On the other side, mu-sic pieces have strong variations on different levels (instru-mentation, vocal segments, harmony, etc.) and we buildclassiﬁcation instances from music intervals of 4 s with 2s overlap, so that 20 tracks contribute to more than 2,000classiﬁcation instances. The data set for the identiﬁcationof relevant features is theoptimisationset of 120 songs,each of them selected randomly from the 120 albums. Theﬁnal evaluation of feature sets after feature selection isdone either on 120testtracks randomly selected from theoriginal albums (test set TS) or 120 tracks from other artists(test set TSAI). Thus, the overall number of tracks for eachclassiﬁcation experiment was equal to 260. The exact listsof tracks are available on our web site1.3.2 FeaturesTwo large audio feature sets are used as baselines to com-pare them with sets optimised by means of feature selec-tion. For exact deﬁnitions and references please see [39].The third baseline set is built with MFCCs which are oftenused for music classiﬁcation [18].The ﬁrst large set comprises low-level audio signal de-scriptors. Such features can be roughly grouped into tim-bre, rhythmic, and pitch characteristics [38]. We extendthis categorisation to ‘timbre and energy’, ‘chroma andharmony’, ‘temporal and correlation characteristics’, and‘rhythm’. Table 1 provides examples of features for differ-ent extraction domains and lists numbers of correspondingfeature dimensions. Because we estimate the mean and thestandard deviation of each feature vector in a classiﬁcationwindow, the original number of 318 dimensions leads to636 features used for the training of categorisation models.The second set contains semantic audio features whichare closely related to music theory and are listed in Ta-ble 2. They can be assigned to four main groups accord-ing to their properties and the extraction procedure. Theﬁrst group consists of chroma-related, harmony, and chordcharacteristics. The second one comprises temporal, rhyth-mic, and structural characteristics. The third group (in-struments, moods, and various high-level characteristics)relates to features estimated with supervised classiﬁcationmodels previously optimised as described in [39]. The lastgroup was extracted using the concept of structural com-plexity [24]. Here, selected interpretable musical charac-teristics (instrumentation, harmonic properties, etc.) arerepresented by a vector of base features, and estimatedstatistics describe the temporal progress of these vectorsover large texture frames.1https://ls11-www.cs.uni-dortmund.de/rudolph/mi#musictestdatabaseProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 171Table 1. Low-level audio featuresGroups and examples of featuresNo.TIMBRE AND ENERGY-TIME DOMAINLinear prediction coefﬁcients, low energy,peak characteristics17TIMBRE AND ENERGY-SPECTRAL DOMAINVarious spectral characteristics (bandwidth, centroid,etc.), tristimulus, sub-band energy ratio29TIMBRE AND ENERGY-CEPSTRAL DOMAINMFCCs, delta MFCCs,CMRARE modulation features [21]101TIMBRE AND ENERGY-PHASE DOMAINAngles and distances [27]2TIMBRE AND ENERGY-E R BANDBARK DOMAINSBark scale magnitudes, charact. of ERB bands [17]53CHROMA AND HARMONYCharact. of spectral peaks, fundamental frequency,chroma, chroma DCT-reduced log pitch (CRP) [29]101TEMPORAL AND CORRELATION CHARACTERISTICSCharacteristics of periodicity peaks3RHYTHMCharacteristics of ﬂuctuation patterns [17]12Table 2. Semantic audio featuresGroups and examples of featuresNo.CHROMA AND HARMONYConsonance [23], tonal centroid [17],strengths of major and minor keys [17]129CHORD STATISTICSNumber of different chords and chord changes in 10s, shares of the most frequent chords [39]5TEMPO,RHYTHM AND STRUCTUREDuration of music piece, estimated number of beat,tatum, and onset events per minute, tempo, segmen-tation characteristics after [31]9INSTRUMENTSIdentiﬁcation of guitar, piano, wind, and strings [41]32MOODSAggressive, conﬁdent, energetic, etc. [39]64VARIOUS HIGH-LEVEL CHARACTERISTICSSinging characteristics, effects distortion,characteristics of melodic range [32]128STRUCTURAL COMPLEXITYChord, harmony, instruments, tempo and rhythmcomplexity [39]703.3 Algorithms and EvaluationThe exhaustive tuning of classiﬁcation methods was be-yond the scope of this study - however it was importantto test the impact of feature selection and the album effectusing classiﬁers with different operating methods. Afterpreliminary studies, we selected four algorithms. Deci-sion tree C4.5 provides interpretable models and alreadyincludes internal feature pruning, but is rather slow. Ran-dom forest (RF) creates a large number of unpruned treesbased on a randomly drawn subset of features. It is oftensuperior to C4.5 w.r.t. classiﬁcation quality and is faster,but classiﬁcation models are not the same if trained an-other time and are not interpretable. Naive Bayes (NB) isvery fast and leads to comprehensible models, especially ifthey are created from interpretable semantic features. Onthe other side, it is a probabilistic method which treats fea-ture distributions independently from each other, and clas-siﬁcation performance is usually lower. Finally, supportvector machine (SVM) is in many cases the state-of-theart method, which achieves the best classiﬁcation results.However, for the best performance it requires parametertuning, is slower than other methods, and models have alower interpretability.The following two criteria are minimised during featureselection. Because of imbalanced distribution of songs inthe optimisation and test sets, the balanced relative errormBREmeasures classiﬁcation quality:mBRE=12✓FNTP+FN+FPTN+FP◆,(6)whereTPis a number of true positives (tracks belong-ing to a category and predicted as belonging to it),TNisa number of true negatives (tracks not belonging to a cate-gory and predicted as not belonging to it),FPis a numberof false positives (tracks not belonging to a category andpredicted as belonging to it), andFNis a number of falsenegatives (tracks belonging to a category and predicted asnot belonging to it).The predicted relationships of tracks to categories areestimated by major voting across all corresponding classi-ﬁcation windows:yP(x1,. . . ,xTS;j)=&PTSi=1yP(xi)TS\u00000.5',(7)whereTSis the number of classiﬁcation instances in thesongjandxidescribes the feature vector of instancei.The second optimisation criterion is the selected featureratemSFR:mSFR=|\u0000(x,q)||X|,(8)where|\u0000(x,q)|is the number of selected features and|X|the number of all features.mSFRis a rough estima-tor for runtime and storage demands (classiﬁcation usinga model with more features is typically slower), but mayalso correlate with the generalisation performance of clas-siﬁcation models: models built with less features have alower tendency to be overﬁtted towards the training set ifthe optimisation of feature selection is done using an inde-pendent song set.The feature selection method itself is based on a multi-objective evolutionary algorithm SMS-EMOA [2]. Theoutput is the set of non-comparable feature subsets: theﬁrst with the largestmSFRand smallestmBRE, and thelast with the smallestmSFRand largestmBRE2. Becausewe focus here on the measurement of album effect havingregard to classiﬁcation error, the discussion of results in thenext section is based on subsets with the smallestmBRE.These subsets contain smallest errors achieved for as smallfeature subsets as possible.2As we minimise bothmSFRandmBRE, an example of two non-comparable (also referred to asnon-dominated) subsets is, e.g., a subsetwithmSFR=0.05,mBRE=0.20and another one withmSFR=0.10,mBRE=0.15. The ﬁrst subset is built with less features and thesecond one has a smaller classiﬁcation error.172 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Table 3. Errors of optimised feature sets and comparison to baselines (smaller values are better). For details see the text.Categorisation tasksData setLOW-LEVEL FEATURESSEMANTIC FEATURESemBRE\u0000LL\u0000MFCCemBRE\u0000SEM\u0000MFCCRECOGNITION OF GENRESClassicTS0.012741.9133.070.013737.4335.68TSAI0.017539.9532.470.027057.2050.09ElectronicTS0.092866.1948.910.119159.2562.78TSAI0.104082.4756.370.127556.5269.11JazzTS0.049766.8947.560.060569.8657.89TSAI0.1192107.0089.420.111349.4783.50PopTS0.129174.7168.340.127043.9467.23TSAI0.159941.2075.530.135327.9163.91RapTS0.050870.2656.310.065076.2972.06TSAI0.047572.7488.290.057956.54107.62R&BTS0.157089.8274.090.148476.8569.93TSAI0.133784.7356.290.148673.6762.57RECOGNITION OF STYLESAdultContemporaryTS0.119267.3155.940.134457.0263.07TSAI0.190664.8381.450.186064.2779.49AlbumRockTS0.090065.4146.680.106651.1555.29TSAI0.122561.4749.040.161750.7364.73AlternativePopRockTS0.106670.9249.670.109254.1950.89TSAI0.174671.4781.400.181864.1084.76ClubDanceTS0.155182.4174.350.138955.0266.59TSAI0.139870.2554.690.146564.6557.32HeavyMetalTS0.083959.2592.200.077856.2585.49TSAI0.119259.2285.260.099155.0670.89ProgRockTS0.107264.0047.430.097353.5243.04TSAI0.178057.6865.560.203959.8575.10SoftRockTS0.110467.2845.190.119753.1349.00TSAI0.175269.9165.280.149862.3955.81UrbanTS0.103876.9574.670.083757.0660.22TSAI0.154159.0958.810.155351.8759.274. DISCUSSION OF RESULTS4.1 Table with ResultsTable 3 provides the summary of results and is organisedas follows. The ﬁrst column lists categorisation tasks. Thesecond column indicates whether the album-dependent testset TS or album-independent test set TSAI was used forthe ﬁnal validation. Columns 3-5 describe results withlow-level features. In the column 3 the “mean best” er-roremBREis listed. Themeanis here calculated across10 statistical repetitions: because evolutionary FS is basedon random decisions, the results are not the same for eachrun. So the value ofemBRE=0.0127corresponds totheexpectedbestmBREafter the application of FS. Thebestmeans that we take into account feature subsets withthe smallestmBREand the largestmSFRacross compro-mise solutions identiﬁed with a multi-objective selectionapproach (see the previous section).Entries in columns 4 and 5 measure the relative reduc-tion ofemBREcompared to complete set of low-level fea-tures,\u0000LL, and set of MFCCs,\u0000MFCC. Smaller valuesare better. For example, in the ﬁrst lineemBRE=0.0127corresponds to 41.91% of the error of the model which usesall low-level descriptors (mBRE=0.03033). Similarly,3Please note that we use an ensemble of four classiﬁers and selectthe best one for each task. Using a complete feature set for the categoryClassic leads tomBRE=0.0303if trained with random forest; for ex-ample, using naive Bayes leads tomBRE=0.0695, so that the error ofemBREis reduced to 33.07% of the error of the model builtwith MFCCs only.Columns 6-8 contain values ofemBREfor models builtwith semantic features and the reduction of error comparedto full set of semantic features\u0000SEMand\u0000MFCC.4.2 Album Effect and Two Cases where FeatureSelection FailsAs it could be expected, classiﬁcation errors increase formost of categories if we switch from the test set TS toTSAI. The advantage of optimised feature subsets com-pared to baselines (columns 4,5,7,8) is often decreased,but not always. For instance, despite of a larger error forAdultContemporary using TSAI (0.1906 against 0.1192),the advantage of optimised low-level feature subsets com-pared to the model with all low-level features is slightlyincreased (64.83 against 67.31, smaller value is better), butnot if compared to the model built with MFCCs (81.45against 55.94).A more important observation is that in all but two casesoptimised models are better than baselines (only two en-tries in columns 4,5,7,8 are above 100%) which meansthat feature subsets after FS lead to a robust reduction oferror even if ﬁnally validated on the test set from inde-the optimised combination “feature subset and classiﬁer” is even strongerreduced if compared to a simple application of naive Bayes together withall low-level features.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 173pendent artists and albums. The ﬁrst exception is Jazz(value of 107.00 in the 4th column): here the full\u0000LLset(mBRE=0.1114) leads to a slightly smaller error than theoptimised set (mBRE=0.1192). This can be explained bythe choice of music: in the artist-independent validationsong set the category Jazz was represented rather by Eu-ropean Jazz, where the training and optimisation set con-tained rather American Jazz4. Another exception relatesto the error of optimised subsets with semantic featurescompared to MFCCs for Rap (value of 107.62, column 8).This matches well the theoretical reason that MFCCs areparticularly successful for the recognition of speech. Thesmallest error for Rap is achieved using the optimised setwith low-level features (and MFCCs belong to this set):mBRE=0.0475.4.3 A Further Danger for Feature Selection (orAdvantage of Ensembles)In all but two explained situations FS led to smaller errors.However, this statement holds for classiﬁcation with fourmethods. Using an ensemble of classiﬁers makes oftensense, and in our previous work we have already observedthat there is no “winner” for all categories [39]. To exam-ine whether the feature selection was successful for indi-vidual combinations of a classiﬁer and a task we comparedthe results to baselines by means of Wilcoxon test. If nostatistical advantage against a baseline has been observedfor at least one of four classiﬁers, the corresponding entryin Table 3 is marked with an italic font. If the baseline waseven better for at least one classiﬁer, the entry is markedwith a bold font. Particularly some models with MFCCsseem to provide a better generalisation performance ratherthan optimised feature subsets. This happens only if testset TSAI is used for the validation. In other words, opti-mising feature selection with an individual classiﬁer maylead to overﬁtting–but in our study this case was avoidedusing an ensemble of several classiﬁers.4.4 A Remark on ResourcesBeside possible problems for feature selection discussedabove, it should not be forgotten that FS provides a strongadvantage against large sets of features because it helpsto reduce storage and runtime demands. The advantageof smaller feature sets is that the classiﬁcation is typicallyfaster5. When the time expensive feature selection may berun once for each new music category, the automatic clas-siﬁcation based on the optimised feature set can be appliedon new songs over and over again. It is hard to preciselymeasure the reduction of computing demands, especiallyfor experiments on different machines. As a rough mea-4We came to this explanation after the studies were accomplished.The uniform sampling of European and American Jazz tracks for optimi-sation and validation sets could be a better decision, but in that case itwould not be possible to exactly compare the results to [39].5As we could see, a set of MFCCs is also small and is sometimessuccessful, so the reduction of demands on resources is not very stronghere. However, all but one values in columns 5 and 8 are below 100%, andit is probably not the best idea to build classiﬁcation models with MFCCsonly for all possible classiﬁcation tasks (styles, tags, moods, etc.)sure we may estimate the decrease of runtime of the lastFS iteration compared to the ﬁrst iteration (in each itera-tion, a classiﬁcation model is trained and validated). As anexample, the mean of runtime of the last iteration dividedby runtime of the ﬁrst iteration for the category Classicis 15.08 for the low-level feature set and 12.57 for the se-mantic set (classiﬁcation with C4.5), 34.55 and 31.72 (RF),20.88 and 8.56 (NB), and 12.68 and 10.54 (SVM).5. CONCLUSIONS AND OUTLOOKIn this work we have examined whether the success of fea-ture selection in music classiﬁcation suffers from an “al-bum effect”, so that the properties of albums and artistsrather than of target categories like genres and styles arelearned. As it could be expected, the danger of such over-ﬁtting exists, and the performance is typically reduced ifthe validation set is built with tracks of other artists. How-ever, if there are enough available features at hand, and fea-ture selection is applied using an ensemble of classiﬁers, inall but two cases the optimised subsets helped to build clas-siﬁcation models not only with less features, but also withsmaller classiﬁcation errors compared to baselines. Thesetwo cases could be theoretically explained and do not de-tract the general sense of feature selection - but they un-derline the consequence that any signiﬁcant achievementsin classiﬁcation domain raise and fall with the design ofdata sets. A very simple case observed in this study wasthat the classiﬁcation models optimised to recognise par-ticularly American Jazz were not best suited to recogniseEuropean Jazz. In future we plan to continue our workinvestigating advantages and dangers of feature selectionfor music classiﬁcation. In particular, the application onpublicly available data sets is important for a reliable com-parison of results. However, this is a hard task which re-quires compromises, e.g., limiting the set of features onlyto available Echo Nest descriptors. Further optimisation ofalgorithm parameters (e.g., larger ensembles, various ker-nels for SVMs) is another promising direction.6. REFERENCES[1]E. Amaldi and V . Kann. On the approximability of mini-mizing nonzero variables or unsatisﬁed relations in linearsystems.Theoretical Computer Science, 209(1-2):237–260,1998.[2]N. Beume, B. Naujoks, and M. Emmerich. SMS-EMOA:Multiobjective selection based on dominated hypervolume.European Journal of Operational Research, 181(3):1653–1669, 2007.[3]B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs.Resampling methods for meta-model validation with rec-ommendations for evolutionary computation.EvolutionaryComputation, 20(2):249–275, 2012.[4]M. A. Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes,and M. Slaney. Content-based music information retrieval:Current directions and future challenges.Proceedings of theIEEE, 96(4):668–696, 2008.[5]J. D. Deng, C. Simmermacher, and S. Craneﬁeld. A study onfeature analysis for musical instrument classiﬁcation.IEEETransactions on Systems, Man, and Cybernetics, Part B: Cy-bernetics, 38(2):429–438, 2008.174 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015[6]J. S. Downie, A. F. Ehmann, M. Bay, and M. C. Jones. TheMusic Information Retrieval Evaluation eXchange: Some ob-servations and insights. In Z. W. Ras and A. Wieczorkowska,editors,Advances in Music Information Retrieval, pages 93–115. Springer, 2010.[7]C. Emmanouilidis, A. Hunter, and J. MacIntyre. A mul-tiobjective evolutionary setting for feature selection anda commonality-based crossover operator. InProc. IEEECongress on Evolutionary Computation (CEC), volume 1,pages 309–316, 2000.[8]S. Essid, G. Richard, and B. David. Musical instru-ment recognition by pairwise classiﬁcation strategies.IEEETransactions on Audio, Speech, and Language Processing,14(4):1401–1412, 2006.[9]R. Fiebrink and I. Fujinaga. Feature selection pitfalls and mu-sic classiﬁcation. InProc. 7th Int’l Conf. on Music Informa-tion Retrieval (ISMIR), pages 340–341, 2006.[10]A. Flexer and D. Schnitzer. Effects of album and artist ﬁltersin audio similarity computed for very large music databases.Computer Music Journal, 34(3):20–28, 2010.[11]I. Fujinaga. Machine recognition of timbre using steady-statetone of acoustic musical instruments. InProc. Int’l ComputerMusic Conf. (ICMC), pages 207–210, 1998.[12]I. Guyon, M. Nikravesh, S. Gunn, and L. A. Zadeh, editors.Feature Extraction. Foundations and Applications, volume207 ofStudies in Fuzziness and Soft Computing. Springer,Berlin Heidelberg, 2006.[13]T. Hastie, R. Tibshirani, and J. Friedman.The Elements ofStatistical Learning. Springer, New York, 2009.[14]R. Kohavi and G. H. John. Wrappers for feature subset selec-tion.Artiﬁcial Intelligence, 97(1-2):273–324, 1997.[15]A. Kruspe, H. Lukashevich, and J. Abeßer. Artist ﬁltering fornon-western music classiﬁcation. InProc. 6th Audio MostlyConference (AM), pages 82–86, 2011.[16]M. Kudo and J. Sklansky. Comparison of algorithms thatselect features for pattern classiﬁers.Pattern Recognition,33(1):25–41, 2000.[17]O. Lartillot.MIRtoolbox 1.4 User’s Manual. Finnish Centreof Excellence in Interdisciplinary Music Research and SwissCenter for Affective Sciences, 2012. Online resource.[18]B. Logan. Mel frequency cepstral coefﬁcients for music mod-eling. InProc. 1st Int’l Symp. on Music Information Retrieval(ISMIR), 2000.[19]M. I. Mandel and D. Ellis. Song-level features and supportvector machines for music classiﬁcation. InProc. 6th Int’lConf. on Music Information Retrieval (ISMIR), pages 594–599, 2005.[20]M. I. Mandel, R. Pascanu, D. Eck, Y . Bengio, L. M. Aiello,R. Schifanella, and F. Menczer. Contextual tag inference.ACM Transactions on Multimedia Computing, Communica-tions, and Applications (TOMCCAP), 7(Suppl.):32, 2011.[21]R. Martin and A. M. Nagathil. Cepstral modulation ratio re-gression (CMRARE) parameters for audio signal analysisand classiﬁcation. InProc. IEEE Int’l Conf. on Acoustics,Speech, and Signal Processing (ICASSP), pages 321–324,2009.[22]B. Matityaho and M. Furst. Neural network based modelfor classiﬁcation of music type. InProc. 18th Convention ofElectrical and Electronics Engineers in Israel, pages 4.3.4/1–4.3.4/5, 1995.[23]M. Mauch and S. Dixon. Approximate note transcription forthe improved identiﬁcation of difﬁcult chords. InProc. 11thInt’l Society for Music Information Retrieval Conf. (ISMIR),pages 135–140, 2010.[24]M. Mauch and M. Levy. Structural change on multiple timescales as a correlate of musical complexity. InProc. 12th Int’lSociety for Music Information Retrieval Conf. (ISMIR), pages489–494, 2011.[25]R. Mayer, A. Rauber, P. J. Ponce de Le´on, C. P´erez-Sancho,and J. M. I˜nesta. Feature selection in a cartesian ensembleof feature subspace classiﬁers for music categorisation. InProc. 3rd Int’l Workshop on Machine Learning and Music(MML), pages 53–56, 2010.[26]C. McKay.Automatic Music Classiﬁcation with jMIR. PhDthesis, McGill University, 2010.[27]I. Mierswa and K. Morik. Automatic feature extraction forclassifying audio data.Machine Learning Journal, 58(2-3):127–149, 2005.[28]A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, andC. A. Coello Coello. A survey of multiobjective evolution-ary algorithms for data mining: Part I.IEEE Transactions onEvolutionary Computation, 18(1):4–19, 2014.[29]M. M¨uller and S. Ewert. Chroma Toolbox: MATLAB im-plementations for extracting variants of chroma-based audiofeatures. InProc. 12th Int’l Conf. on Music Information Re-trieval (ISMIR), pages 215–220, 2011.[30]E. Pampalk, A. Flexer, and G. Widmer. Improvements ofaudio-based music similarity and genre classiﬁcaton. InProc. 6th Int’l Conf. on Music Information Retrieval (ISMIR),pages 628–633, 2005.[31]J. Paulus and A. Klapuri. Music structure analysis using aprobabilistic ﬁtness measure and an integrated musicologi-cal model. InProc. 9th Int’l Conf. on Music Information Re-trieval (ISMIR), pages 369–374, 2008.[32]G. R¨otter, I. Vatolkin, and C. Weihs. Computational predic-tion of high-level descriptors of music personal categories.In B. Lausen, D. van den Poel, and A. Ultsch, editors,Algorithms from and for Nature and Life, pages 529–537.Springer, 2013.[33]G. Rozenberg, T. B¨ack, and J. N. Kok, editors.Handbook ofNatural Computing. Springer, Berlin Heidelberg, 2012.[34]P. Saari, T. Eerola, and O. Lartillot. Generalizability and sim-plicity as criteria in feature selection: Application to moodclassiﬁcation in music.IEEE Transactions on Audio, Speech,and Language Processing, 19(6):1802–1812, 2011.[35]W. W. Siedlecki and J. Sklansky. A note on genetic algorithmsfor large-scale feature selection.Pattern Recognition Letters,10(5):335–347, 1989.[36]C. N. Silla Jr., A. L. Koerich, and C. A. A. Kaestner. A featureselection approach for automatic music genre classiﬁcation.International Journal of Semantic Computing, 3(2):183–208,2009.[37]B. Sturm. A survey of evaluation in music genre recogni-tion. InProc. 10th Int’l Workshop on Adaptive MultimediaRetrieval (AMR), 2012.[38]G. Tzanetakis and P. Cook. Musical genre classiﬁcation ofaudio signals.IEEE Transactions on Speech and Audio Pro-cessing, 10(5):293–302, 2002.[39]I. Vatolkin.Improving Supervised Music Classication byMeans of Multi-Objective Evolutionary Feature Selection.PhD thesis, Dep. of Computer Science, TU Dortmund, 2013.[40]I. Vatolkin, M. Preuß, and G. Rudolph. Multi-objective fea-ture selection in music genre and style recognition tasks. InProc. 13th Annual Genetic and Evolutionary ComputationConf. (GECCO), pages 411–418, 2011.[41]I. Vatolkin, M. Preuß, G. Rudolph, M. Eichhoff, andC. Weihs. Multi-objective evolutionary feature selection forinstrument recognition in polyphonic audio mixtures.SoftComputing, 16(12):2027–2047, 2012.[42]C. Weihs, U. Ligges, F. M¨orchen, and D. M¨ullensiefen. Clas-siﬁcation in music research.Advances in Data Analysis andClassiﬁcation, 1(3):255–291, 2007.[43]I. H. Witten and E. Frank.Data Mining: Practical Ma-chine Learning Tools and Techniques. Elsevier, San Fran-cisco, 2005.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 175"
    },
    {
        "title": "Evaluating Conflict Management Mechanisms for Online Social Jukeboxes.",
        "author": [
            "Felipe Vieira",
            "Nazareno Andrade"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416232",
        "url": "https://doi.org/10.5281/zenodo.1416232",
        "ee": "https://zenodo.org/records/1416232/files/VieiraA15.pdf",
        "abstract": "Social music listening is a prevalent and often fruitful ex- perience. Social jukeboxes are systems that enable so- cial music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conflict- ing interests. Because of that, virtually all social juke- boxes incorporate conflict management mechanisms. In contrast with their widespread use, however, little atten- tion has been given to evaluating how different conflict management mechanisms function to preserve the positive experience of music listeners. This paper presents an ex- periment with three conflict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners’ satisfaction and to exam- ine their impressions and views on conflict, conflict man- agement mechanisms, and social jukeboxing.",
        "zenodo_id": 1416232,
        "dblp_key": "conf/ismir/VieiraA15",
        "keywords": [
            "Social music listening",
            "social jukeboxes",
            "collaborative music selection",
            "diverse music tastes",
            "conflict management",
            "positive experience",
            "mixed-methods approach",
            "listeners satisfaction",
            "impressions and views",
            "conflict management mechanisms"
        ],
        "content": "EVALUATING CONFLICT MANAGEMENT MECHANISMS FOR ONLINESOCIAL JUKEBOXESFelipe Vieira Nazareno AndradeDepartment of Systems and Computing, Universidade Federal de Campina Grande, Brazil{felipev, nazareno}@lsd.ufcg.edu.brABSTRACTSocial music listening is a prevalent and often fruitful ex-perience. Social jukeboxes are systems that enable so-cial music listening with listeners collaboratively choosingthe music to be played. Naturally, because music tastesare diverse, using social jukeboxes often involves conﬂict-ing interests. Because of that, virtually all social juke-boxes incorporate conﬂict management mechanisms. Incontrast with their widespread use, however, little atten-tion has been given to evaluating how different conﬂictmanagement mechanisms function to preserve the positiveexperience of music listeners. This paper presents an ex-periment with three conﬂict management mechanisms andthree groups of listeners. The mechanisms were chosento represent those most commonly used in the state of thepractice. Our study employs a mixed-methods approach toquantitatively analyze listeners’ satisfaction and to exam-ine their impressions and views on conﬂict, conﬂict man-agement mechanisms, and social jukeboxing.1. INTRODUCTIONThe act of listening to music together is ubiquitous. Inmany situations, the choice of the music to be played for agroup is done by an authority, such as a performer or a DJ;in other situations, groups rely on more democratic choicesthrough social jukeboxes. Such devices have varying im-plementations in industry and have received attention fromacademia. In the latter, research has observed systemswhich arbitrate the selection of songs in gyms consider-ing the musical tastes of those attending the gym [15], andsystems that democratize the choice of music to be playedin parties [10, 17], public spaces [16] and in cars [18]. Inindustry, Plug.DJ [3] (three million registered accounts),the recently shut down Soundrop [6] (peaked at nearly 49thousand monthly active users) and the mobile applica-tions Noispot [1], PlayMySong [2], Rockbot [5], and Se-cret.DJ [7] (all of them with more than ten thousand down-loads on virtual stores) are some commercial systems thatpresently have a signiﬁcant user base.Because people are often affected by the music heardin an environment [11], sharing the choice of music to beheard may lead to pleasant or dissatisfying experiences. In-c\u0000Felipe Vieira, Nazareno Andrade. Licensed under a Cre-ative Commons Attribution 4.0 International License (CC BY 4.0).Attri-bution:Felipe Vieira, Nazareno Andrade. “Evaluating conﬂict manage-ment mechanisms for online social jukeboxes”, 16th International Societyfor Music Information Retrieval Conference, 2015.deed, in the presence of diverse musical tastes, it is likelythat there will be conﬂicts in choosing music collectively.In the simplest case, one member of the group may like agenre or speciﬁc songs disliked by others. Even for par-ticipants that share similar tastes, one of them may be ata given moment interested in relaxing songs, while an-other participant is interested in increasing arousal. Toryet al. [10] and O’Hara et al. [16] have documented exam-ples of such conﬂicts in the context of social jukeboxes.To prevent that conﬂicts cause unpleasant experiences,it is central that social jukeboxes have mechanisms thatmanage such conﬂicts. Some of the aforementioned sys-tems rely on voting to allow users to communicate theirpreferences. In part of these systems, this feedback alsoserves as an input to choose music based on the prefer-ence of the majority. However, in spite of the necessaryand common use of conﬂict management mechanisms insocial jukeboxes, there has been little or no comparativescientiﬁc evaluation of such mechanisms.This work contributes to ﬁlling this gap by studying theuse of three conﬂict management mechanisms in the samesocial jukeboxing system. The three mechanisms studiedare present in multiple solutions in the state of the practiceof social jukeboxes, and aim to represent signiﬁcant pointsin the design space of conﬂict management mechanisms.Experiments were conducted with three user groups, eachusing the social jukebox in their natural settings. Our eval-uation uses a mixed methods approach combining quan-titative measures of user satisfaction and textured impres-sions stemming from semi-structured interviews in combi-nation with observation reports and chat logs.By analysing user satisfaction data, our results conﬁrmthat in spite of conceptual differences, the three conﬂictmanagement mechanisms provide a signiﬁcant gain in usersatisfaction when compared to a baseline social jukeboxwith no mechanism. Moreover, the up/downvoting mech-anism provides the highest satisfaction among the mech-anisms we experiment with. A qualitative analysis of in-terviews, observation notes, and chat logs suggests that theeffectiveness of voting is related to its interaction demandsand the feedback it provides. Furthermore, analysing suchdata highlights other fonts of conﬂicts and opportunitiesfor the design of new conﬂict management mechanisms.2. ONLINE SOCIAL JUKEBOXES ANDCONFLICTAkin to the jukebox metaphor, in online social jukeboxesusers add songs to a queue to be played. This choice of190songs is the primary source of conﬂict, as users may dis-agree on the best song to be played in a given moment. Ex-amining the industry and social jukeboxes in the researchliterature, we identify three mechanisms most often used tomanage conﬂict:like/dislikefeedback,up/down votingofsongs in a queue, and askipfeature. Like/dislike is presentin all systems mentioned except Jukola, up/down voting isused in Soundrop, Noispot, Rockbot and Jukola, and skipis implemented in Plug.DJ, Noispot and Jukola.These three conﬂict management mechanisms can beeasily evidenced in the observed jukeboxes. Like/dislikefeedback is comprised of messages from users about thesong currently playing. As such, it does not directly orimmediately affect the music playing; in the presence ofconﬂict it only conveys to the person responsible for thesong the desire that future choices are different. This isthe less intrusive mechanism. Up/down voting, in turn, al-lows for the group to change the order of songs that willbe played next. If users downvote a song, this both com-municates their negative preference and delays the songstart. This delaying represents a more intrusive approachto manage conﬂicts by avoiding songs that will not satisfysome participants. Finally, skipping gives the group meansto directly interfere in a song that is presently playing.Allowing users to express their appreciation for somecontent is a widespread feature in social media. Cheng etal. [13] have found that in large-scale systems, this typeof mechanism can lead to signiﬁcant changes in the au-thor’s future behaviour by attaching more quality to thecontent shared after negative feedback. The mechanismof affecting the next song to be played by up/down vot-ing on the queue items is perhaps the most straightforwardmechanism of democratizing music choice. It also resem-bles approaches applied in different settings such as socialQ&A or media aggregating sites such as Reddit [4], whereusers are able to choose which shared content is going to bemost evident in the website by up/down voting posts. Thepossibility of abruptly stopping a song execution throughskip seems to be more speciﬁc of social music systems, buthas been recognized as valuable to avoid mood-breakingsongs [18] and to prevent frequent users from the frustra-tion of hearing the same song multiple times [16].It is worthwhile mentioning that although there are anumber of conﬂict management mechanisms used in socialjukebox systems, to the best of our knowledge there hasbeen no experimental study that compares the effectivenessof conﬂict management mechanisms for these systems.3. THREE CHOSEN MECHANISMS IN ANONLINE JUKEBOXGiven the state of the practice observed in conﬂict manage-ment for social jukeboxes, we opted to experiment withthe three mechanisms identiﬁed as most often employed:like/dislike feedback, up/down voting and skip. Thesemechanisms were implemented in a social jukebox devel-oped by the authors and named WePlay, which has its basicinterface shown in Figure 1.WePlay allows for a group of users to synchronouslylisten to music coming from a shared queue of songs towhich all can contribute. Each user can contribute as manysongs as desired by searching these songs on YouTube andadding to a queue visible by all. The queue lists songs, butnot the users who contributed the songs. Besides featuresavailable to the users, WePlay also allows an experimenterto alternate the conﬂict management mechanism exposedto users at will. The implementation of the three mecha-nisms is detailed next.\nFigure 1. The interface of WePlay, the social jukebox sys-tem used in our experiments3.1 Like/DislikeSimilarly to prevalent mechanisms in online social me-dia, when this mechanism is available, users have accessto like and dislike buttons next to the name of the songpresently playing, as shown in Figure 2. Similar to the so-cial jukebox systems we observed, this explicit feedbackdoes not directly control which song will play next. In-stead, it serves as a message to the user who queued thesong stating how welcome that song has been consideredby current listeners. In WePlay, only one immutable feed-back may be provided per song. Moreover, the number oflikes and dislikes is visible for all listeners, but no listenerhas access to the list of users who liked or disliked a song.Finally, when this mechanism is enabled, users are able tosee a list of previously played songs and the feedback theyreceived.3.2 Up/down votingBy using the up/down voting mechanism in WePlay, userscan vote up or down songs in the queue. Users can cast onevote per song, also immutable. After each vote, songs areordered according to their balance, calculated as the dif-ference between its positive and negative votes. The queueinterface is depicted in Figure 3. Neither voters nor currentbalance are shown in the interface, but the highest-rankedsong is always highlighted. In the event of a tie, the times-tamp is considered the tiebreaker, awarding highest rank tothe song ﬁrst suggested to the system.3.3 SkipThis mechanism allows the jukebox users to collectivelyskip the current song. If enough users manifest such will,Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 191Figure 2. The like/dislike mechanism and the feedbackhistory\nFigure 3. The up/downvoting mechanismthe song is then immediately skipped and the next songfrom the queue starts playing. To manifest opinions aboutthe song, users can cast positive or negative votes about it.Considering the number of listenersn, the number of posi-tive votesp, and the negative votess, if the overall satisfac-tiono=( (p\u0000s)/n)+1of the current song reaches a valuebelow the threshold of 0.5, the song is skipped, followingthe skip mechanism idealized heuristically by the originalauthors of a side project which was adapted to result in ourWePlay and maintained due to the similarity of the originaluse of the system and our experimental scenario.4. EXPERIMENTAL SETUPThree different groups were recruited to participate in ourexperiments. Recruitment was done using the social net-work of authors, primarily targeting groups of potentialusers of an online social jukebox that would be availableover multiple consecutive days for the experiments. Partic-ipants from the ﬁrst two groups are undergraduate students,graduate students or researchers working in the same uni-versity as the authors, totalizing 18 participants (16 malesand 2 females, average age 25.2). Participants in thesegroups are work colleagues who used the system duringnormal workdays, and were collocated in the same or adja-cent rooms during the experiment. The third group is com-prised by friends of one of the authors (10 males, averageage 25) who have known each other for years, work in di-verse ﬁelds, used the system during leisure time, and werelocated in different places in a common city. In all threegroups, our goal is to study the use of the social jukeboxsystem integrated in the subjects’ routine, aiming at theecological validity in social listening research suggestedby North [9].Each experiment lasted for the period of ﬁve days. Allparticipants were submitted to a brieﬁng explaining howthe system would work and describing the experimentdynamics (e.g. what conﬂict management mechanismswould be enabled on each day). None of the participantswas aware of the speciﬁc details of the research. All partic-ipants were informed that the goal of the experiment was toevaluate multiple designs of the social jukebox, and agreedto use the system for the duration of the experiment, andto have data collected during this time to be used in theresearch. In the week after each experiment, users wereinterviewed about their experience using semi-structuredinterviews. Four, seven and seven participants were inter-viewed respectively on groups one, two and three, totalling18 interviews. Interviews lasted on average 15 minutes.During the experiment, each group ﬁrst interacted withthe social jukebox using no conﬂict management mecha-nism in a situation we dub baseline. After the baseline, theother conﬂict resolution mechanisms were available one ata time and in the same order for all groups, for one com-plete day each. On the ﬁfth day, all three mechanismswere available for participants, in a setting we call com-bined mechanisms. During the complete experiment, par-ticipants and the experimenter shared a text chat room us-ing Google Hangouts. This communication channel wasmeant primarily for the experimenter to answer questions,but also hosted diverse conversations among participantsduring the experiments.The social jukebox used in the experiments is instru-mented to provide detailed usage information through logs.Furthermore, to gauge participant’s overall satisfactionwith the system, the jukebox asked participants to pro-vide every 30 minutes their level of satisfaction througha 5-point likert scale in a form which asked users to ex-plicitly state theirsatisfaction with recently played songs.Although the action of listening to music is often a back-ground task and users could forget to answer this request,whenever a new request was made, participants were re-minded to answer the from through the group chat.5. CONFLICT SITUATIONSAs expected, the interviews and our observation of systemusage revealed conﬂict situations. Overall, our data showssome conﬂicts related to a participant having an aversion toa song proposed by another participant. Such aversion maybe related to one’s musical identity [8] (Everytime she sug-gested I immediately voted negative, because of her musi-cal taste1) and were perceived to affect satisfaction (Therewas a moment when I felt upset about the songs. They wereputting some songs like funk, and I don’t really like funk.But it was a radio, and it was in a democracy style, so Ihad to listen to that.orIn some moments I was very dis-satisﬁed. There were some songs I cannot stand... Somemusical styles.).1Quotes from the interviews are presented henceforth in italics andparenthesis. All quotes were translated from Portuguese to English bythe authors.192 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 201512345\nbaselinelike/dislikeup/downvotingskipcombinedscenariosatisfaction\nFigure 4. Distribution of median satisfaction reported byusers in each of the scenarios. The violin glyphs encodedensity. Error bars represent 95% conﬁdence intervals forthemedians. In comparing the intervals, one should takeinto account that samples are paired; this pairing resultsin, up/down voting having a signiﬁcantly higher mediansatisfaction than the combined scenario, and in skip signif-icantly outperforming the baseline.A second and minor source of conﬂict relevant to themechanisms we experimented with is related to gamingthe mechanisms and trolling. Participants reported theirtendency to game the voting mechanism, and trolling be-havior by users.6. CONFLICT MANAGEMENT AND USERSATISFACTIONOur quantitative data contains multiple satisfaction ratingsfor each participant in each of the ﬁve different designs:baseline, like/dislike, up/downvoting, skip, and all mech-anisms. In the following, each participant’s satisfaction issummarized as the median of the ratings provided in eachdesign.Albeit conceptually categorical, likert scales data in theform employed in our experiment can be reliably usedin numerical statistical tests [14, 19]. A normality testhowever points that the satisfaction data is not normal,(Shapiro-Wilk test,p<.01for all ﬁve scenarios). Thisobservation combined with the sample size (N<30forall samples) leads us to use non-parametric tests to com-pare participant satisfactions.Participants’ satisfaction and the 95% conﬁdence in-terval for the median satisfaction across participants areshown in Figure 4. It is readily apparent that like/dislike,up/downvoting, and the combine mechanisms all lead tosigniﬁcantly higher user satisfaction than the baseline sys-tem. A rank-sum comparison using Mann-Whitney pairedone-tailed tests reveals that all mechanisms provide sig-niﬁcantly higher satisfaction than the baseline (p<.02for all designs. Like/dislike:V= 276, up/downvoting:V= 276, skip:V= 117.5).Comparing the mechanisms among themselves, we seethat up/down voting has at the same time the highest me-dian satisfaction and the smallest dispersion in satisfac-tion values. The overall higher satisfaction of participantswhen using the voting mechanism is also conﬁrmed by arank-sum comparison with the combined scenario (Mann-Whitney paired one-tailed,p=.02,V= 75). Since theparticipants in group 3 had different backgrounds to thosein the other two groups, the previous statistical tests wererepeated withholding data pertaining to group 3, The re-sults of this test have similar outcomes.Next to up/down voting, the combination of mecha-nisms resulted in the second highest satisfaction scores.This may reﬂect the availability of the high-performingup/down voting mechanism in the combination. The sec-ond best performing sole mechanism is like/dislike. Themechanism that provided the smallest increase in satisfac-tion in our experiments was skipping.Finally, Figure 5 compares satisfactions reported byparticipants in the different groups. The general patternis the same for all three groups. This is so in spite of therelatively different context in which group 3 used the sys-tem.Together, our quantitative results suggest that the beststrategy for a designer considering implementing a con-ﬂict management mechanism in a social jukebox system isto focus on up/down voting. In the next section we elab-orate on the reasons behind participants’ preferences, andon other relevant episodes in the experiments.7. IMPRESSIONS ABOUT CONFLICTMANAGEMENTBesides the quantitative data, we now turn to analyze col-lected interviews, observations taken by the experimenters,and chat history among participants. The qualitative datawas explored using Grounded Theory [12] methods forcoding and categorizing quotes, and to analyse the emer-gent themes.7.1 Mechanisms’ effectivenessAn overall positive effect of the conﬂict managementmechanisms reported by users is the possibility of com-municating of one’s identity and preferences to negotiate acommon ground and reduce conﬂict (... and I found it veryinteresting the little window on the bottom of the screenwhere we could see our latest ratings. It’s useful whenyou’re choosing your next song and you don’t want to picka song nobody likes).Focusing on up/down voting, this mechanism seems tooffer a particularly convenient trade-off between express-ing preferences on multiple songs and having to often in-terrupt other tasks to use the social jukebox (... and I alsothought the songs list[with the up/ down votes]very in-teresting because we could express our opinions and goback to our main activities, avoiding to open the system allthe time, focusing on our jobs and still making our voicesProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 19312345\n12345\n12345123\nbaselinelike/dislikeup/downvotingskipcombinedscenariosatisfaction\nFigure 5. Distribution of median satisfaction reported byusers in each of the scenarios, with users divided in thethree experimental groups.active inside the system). Both like/dislike and skip mustbe performed on a song while it was playing, and thus re-quired more frequent interaction with the system (I used[the like/dislike feature] on almost all songs, except whenI was really busy with work).In our experiments, positive feedback was seen as moreusual, and negative feedback as related to more extremecases (I only used the dislike feature when a song was areally really bad choice or if the other guy was clearlytrolling), or to constantly send explicit messages to userswith mismatched musical tastes about the incoherence oftheir choices (There’s no signiﬁcant difference between mypositive and negative voting, I guess, except that when [aparticipant’s name] suggested. Then I always voted nega-tive due to her musical taste).The skip feature as implemented in our experiments hada major limitation related to presence. Our system ac-counted for listeners as active if they are logged in, and de-manded that a proportion of active listeners voted for skip-ping to actually skip the song. Because music listening wasa background activity, and participants interleaved this ac-tivity with attention to other tasks or even being temporar-ily physically away from the computer, there were ofteninsufﬁcient votes for skipping (It was hard to see the skipfeature happening. It barely happened, and in a rare mo-ment when [the song] was skipped I think it only happenedbecause that song had a really big rejectionorAlthoughI think the skip is a great idea it almost didn’t happened,and when it happened I thought it was because our roomwasn’t full yet and a few negative feedbacks were enoughto skip the song, which sadly came to be a song of mine...).7.2 Gaming and trollingGaming conﬂict resolution and trolling are two often re-ported phenomena in online communities. In our experi-ments, both behaviours happened and were commented onduring interviews. For example, one participant reportedstrategies for imposing their choices on the group:Therewere several times when I tried to downvote all songs ex-cept mine’s, so I could just upvote any of my songs andplace them at the top, playing it before the other’s choices.It didn’t succeed because the guys discovered my strategyand started to downvote my songs. I tried also to dislikeall the other’s songs, hoping that the system skipped those,but that didn’t happen.On a different occasion, because participants weremostly friends, there were participants who posted non-sense songs or repeatedly posted a song related to somememe as a joke with the group. Although subverting therules and joking may reinforce social ties, in our experi-ments it had detrimental effects (There was a time whenwe had a song related to a viral, and because of that thesong got repeated over and over again, so as I couldn’thandle it I took my earphone off and put it on a little laterto hear some new songs, if that was the case.). Anotheruser clearly stated he was motivated by jokingly annoyingothers (It was my fuel. When it annoyed people, I’d put thesong again).7.3 Design SuggestionsAfter being exposed to four situations in conﬂict manage-ment, participants were also asked about their views aboutthe design space of social jukeboxes.A participant suggested that more mechanisms to com-municate musical identity may be of use, and that perhapsallowing one to specify such identity explicitly could con-tribute to reduce conﬂict by enabling semi-automatic songchoice (I think a good way [to increase conﬂict manage-ment] is to allow user proﬁling, something like: an userhas three musical preferences, so when he starts using thesystem he could be asked to ﬁll a form stating those threechoices, and after that the system could check who is on-line and select the next song according to the intersectionof musical tastes).Further room to increase the convenience of express-ing preferences in the system when music is a backgroundtask was also mentioned. A participant suggested the useof smartphones for enabling interaction in such cases (Itwould be great if we have a tool to facilitate the votingprocess, because we can only vote at the web page, and194 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015sometimes we are[on our desk but]not using our PC butwe keep listening the songs, so if we could, for example,vote in a song using our smartphone that could make thedemocratic process even better).A more challenging suggestion to experiment with thatwas mentioned by multiple participants is the possibility ofpunishing users perceived as trolls (It came to a point whenI had enough of [another participant’s name]’s songs. Ireally wish he was unable to suggest songs, so the systemcould at least enable the chance of banning a song whichreceived too much negative votes, but actually I think itwould be even greater if we could ”mute” a speciﬁc user,removing the access to the features and only allowing himto listen the songs suggested by the others).8. IMPLICATIONS FOR DESIGNOur experiment evaluates three commonly used conﬂictmanagement mechanisms in online social jukeboxes. To-gether, the quantitative and qualitative results point formultiple implications for designers of social jukeboxes.In our experience, conﬂicts were relatively easy to re-produce. Participants of all of our experiments were al-ready friends or colleagues, and had multiple communi-cation channels besides the social jukebox. Yet, conﬂictsrelated to incompatibility in music tastes and different in-tentions in listening to music on a given moment were re-ported to inﬂuence satisfaction with the music listening ex-perience.Quantitatively, all three conﬂict management mecha-nisms led to signiﬁcant improvements in user satisfactionwith the music played in the experiments. It is also notablethat the mechanisms provided such increase in the pres-ence of conversations both face-to-face and through onlinechat to manage the same conﬂicts. This result suggests thatsimple mechanisms effectively complement more texturedsocial interactions to negotiate this type of conﬂict.Comparing mechanisms, our results point that up/downvoting songs on the queue leads to the highest overall usersatisfaction. Both the median and minimum satisfactionof participants were the highest with this mechanism. Aqualitative analysis points that up and downvoting seemsto be on a sweet spot of the design space as it allowsfor conveniently sparse batch interactions with the sys-tem, combined with an informative log of past song evalu-ations. These results, together with the ease of implement-ing up and down voting recommends that present design-ers consider this mechanism. Moreover, it suggests thatconﬂict resolution mechanisms for background music lis-tening take into account the frequency of interaction withthe system.With respect to the log of past evaluations, our analysissuggests it has a constructive role in preventing conﬂicts.Our experiment does not allow for isolating its effect, butsuggests this and other mechanisms that allow users or thegroup to express their taste are likely to contribute to con-ﬂict management. Indeed, this direction is similar to thecommon behaviour of stating group norms explicitly inmany online communities.Other relevant aspects that arose in our analyses werethe limited effect of the skip mechanisms and the presenceof gaming and trolling. The former is chieﬂy related todifﬁculties in detecting and communicating user presencewhile music was a background task. As a result, the sys-tem perceived too many users as active, and participantsfelt that voting for skipping was not effective. Detectingwhich users are presently interacting with the system anddevising a skipping policy more easily understandable maylead to different results, and our mechanism allow limitedconclusions in this perspective.With respect to gaming and trolling, our experimentshighlight that these phenomena happen in social music lis-tening even for small-scale scenarios. From our observa-tions, the mechanisms we experimented with were robustto gaming. Trolling in our setting was related to jokes froma user that reduced the satisfaction of others – which nev-ertheless were reported as trolling in the interviews. Theinterviews suggest that mechanisms to regulate such be-haviours may contribute to the success of online socialjukeboxes.9. LIMITATIONS AND FUTURE WORKThis work contributes preliminary ﬁndings to an under-standing of the effectiveness of multiple points in the de-sign space of conﬂict management mechanisms for onlinesocial jukeboxes. In doing so, it has a number of limita-tions and leaves open questions for future work.An issue that markedly limits the generalizability of ourﬁndings is related to the characteristics of our sample ofusers. All participants where already acquainted, and byand large male. Replicating our experiment with moregroups with different compositions is a direct and neces-sary extension of this work. This is necessary to examinethe degree to which the context, closeness, and size of thegroup affect our results. Moreover, understanding whetherand how direct conversation interferes with conﬂict man-agement also seems like a promising avenue of research.Another point that demands further study is the analy-sis of other policies for each of the mechanisms examinedhere. Other policies for consolidating votes, skip requests,and like and dislike feedback may be more suitable for cer-tain contexts. Also, experimenting with other policies forskipping seems particularly relevant, given the feedbackfrom the participants in our experiments.Finally, our experience highlights and commends for fu-ture work the beneﬁts of conducting similar research in anaturalistic setting. Observing participants use the systemin their normal routine, and participating in social listen-ing with colleagues and friends helped unveil a number ofrelevant observations in our research.10. ACKNOWLEDGEMENTSWe would like like to thank the developers of R´adio LSD,on which WePlay was based – particularly Abmar Barros– and the participants of our experiment.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 19511. REFERENCES[1] Noispot website. http://noispot.com/. Accessed: 2015-04-23.[2] Playmysong website. http://www.playmysong.com/.Accessed: 2015-04-23.[3] Plug.dj website. https://plug.dj/. Accessed: 2015-04-23.[4] Reddit website. http://www.reddit.com/. Accessed:2015-04-23.[5] Rockbot website. https://rockbot.com/. Accessed:2015-04-23.[6] Soundrop website. http://soundrop.fm/. Accessed:2015-04-23.[7] Soundrop website. http://www.secretdj.com/. Ac-cessed: 2015-04-23.[8] J. Hargreaves A. North, D. Hargreaves. The functionsof music in everyday life: Redeﬁning the social in mu-sic psychology. InPsychology of Music, pages 84–95,1999.[9] J. Hargreaves A. North, D. Hargreaves. Uses of musicin everyday lifes. InMusic Perception, pages 41–77.,2004.[10] M. Tory. D. Sprague, F. Wu. Music selection using thepartyvote democratic jukebox. InProceedings of AVI2008, 2008.[11] T. DeNora.Music and everyday life. Cambridge: Cam-bridge University Press., 2000.[12] Monique Hennink, Inge Hutter, and Ajay Bailey.Qual-itative research methods. Sage, 2010.[13] J. Leskovec J. Cheng, C. Danescu-Niculescu-Mizil.How community feedback shapes user behavior. InICWSM, 2014.[14] D. Dodou J. de Winter. Five-point likert items: t testversus mann-whitney-wilcoxon. InPract. Assess. Res.Eval. 15 (11), 1–12, 2010.[15] T. Anagnost J. McCarthy. Musicfx: An arbiter ofgroup preferences for computer supported collabora-tive workouts. InProceedings of the ACM 1998 Conf.on Comp. Support. Coop. Work (CSCW 98), pages363–372, 1998.[16] M. Jansen A. Unger H. Jeffries P. Macer K. O’Hara,M. Lipson. Jukola: Democratic music choice in a pub-lic space. InProceedings of DIS 2004, Boston, MA,2004.[17] D. Nichols S. Cunningham. Exploring social music be-havior: An investigation of music selection at parties.InProceedings of ISMIR 2009, pages 747–752., 2009.[18] B. Bainbridge H. Ali S. Jo Cunningham, D.M. Nichols. Social music in cars. InProceedings of15th International Society for Music Information Re-trieval Conference, 2014.[19] C. Wu. An empirical study on the transformation oflikert-scale data to numerical scores. InApplied Math-ematical Sciences, Vol. l No. 58, 2851-2861, 2007.196 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Music Pattern Discovery with Variable Markov Oracle: A Unified Approach to Symbolic and Audio Representations.",
        "author": [
            "Cheng-i Wang",
            "Jennifer Hsu",
            "Shlomo Dubnov"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416480",
        "url": "https://doi.org/10.5281/zenodo.1416480",
        "ee": "https://zenodo.org/records/1416480/files/WangHD15.pdf",
        "abstract": "This paper presents a framework for automatically discov- ering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and au- dio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Vari- able Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an ap- propriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance.",
        "zenodo_id": 1416480,
        "dblp_key": "conf/ismir/WangHD15",
        "keywords": [
            "framework",
            "polyphonic music piece",
            "automatic discovery",
            "symbolic and audio representations",
            "heuristics",
            "musical knowledge",
            "pattern discovery framework",
            "Variable Markov Oracle",
            "pattern-finding algorithm",
            "state of the art performance"
        ],
        "content": "MUSIC PATTERN DISCOVERY WITH VARIABLE MARKOV ORACLE:A UNIFIED APPROACH TO SYMBOLIC AND AUDIOREPRESENTATIONSCheng-i Wang, Jennifer Hsu and Shlomo DubnovMusic DepartmentUniversity of California, San Diego{chw160, jsh008, sdubnov}@ucsd.eduABSTRACTThis paper presents a framework for automatically discov-ering patterns in a polyphonic music piece. The proposedframework is capable of handling both symbolic and au-dio representations. Chroma features are post-processedwith heuristics stemming from musical knowledge and fedinto the pattern discovery framework. The pattern-ﬁndingalgorithm is based onVariable Markov Oracle. TheVari-able Markov Oracledata structure is capable of locatingrepeated sufﬁxes within a time series, thus making it an ap-propriate tool for the pattern discovery task. Evaluation ofthe proposed framework is performed on the JKU PatternsDevelopment Dataset with state of the art performance.1. INTRODUCTIONAutomatic discovery of musical patterns (motifs, themes,sections, etc.) is a task deﬁned as identifying salient musi-cal ideas that repeat at least once within a piece [3,11] withcomputational algorithms. In contrast to “segments” foundin the music segmentation task [14], the patterns foundhere may overlap with each other and may not cover theentire piece. In addition, the occurrences of these patternscould be inexact in terms of harmonization, rhythmic pat-tern, melodic contours, etc. Lastly, hierarchical relationsbetween motifs, themes and sections are also desired out-puts of the pattern discovery task.Two major approaches for symbolic representations arethe string-based and the geometric methods. A string-basedmethod treats a symbolic music sequence as a string of to-kens and applies string pattern discovery algorithms on thesequence [2, 18]. A geometric method views musical pat-terns as shapes appearing on a score and enables inexactpattern matching as similar shapes imply different occur-rences of one pattern [4, 16]. For a comprehensive reviewof pattern discovery with symbolic representations, readersare directed to [11]. For audio representations, geometricc\u0000Cheng-i Wang, Jennifer Hsu and Shlomo Dubnov.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Cheng-i Wang, Jennifer Hsu andShlomo Dubnov. “Music Pattern Discovery with Variable Markov Ora-cle: A Uniﬁed Approach to Symbolic and Audio Representations”, 16thInternational Society for Music Information Retrieval Conference, 2015.methods for symbolic representations have been extendedto handle audio signals by multiF0-estimation with beattracking techniques [5]. Approaches adopted from mu-sic segmentation tasks using self-similarity matrices andgreedy search algorithms are proposed in [19, 20]. Mostof the research involving audio representations has beenfocused on “deadpan audio” rendered from MIDI. In [5],the pattern discovery task is extended to live performanceaudio recordings with a single recording for each musicpiece. In the current study, instead of directly applying theproposed framework on performance recordings, multiplerecordings are gathered for each musical piece to aid thepattern discovery on deadpan audio.In this paper, the work presented in [25] focusing onpattern discovery on deadpan audio is extended to handlesymbolic representations. The framework proposed in thispaper can be seen as a string-based method in which inputfeatures are symbolized. The framework consists of twoblocks:1)feature extraction with post-processing routinesand2)the pattern ﬁnding algorithm. For both symbolicand audio representations, chroma features are extractedand post-processed based on musical heuristics, such asmodulation, beat-aggregation, etc. The core of the patternﬁnding algorithm is aVariable Markov Oracle(VMO). AVMOis a data structure capable of symbolizing a signal byclustering the observations in a signal, and is derived fromtheFactor Oracle(FO) [13] andAudio Oracle(AO) [9]structures. TheFOstructure is a variant of a sufﬁx treedata structure and is devised for retrieving patterns froma symbolic sequence [13]. AnAOis the signal extensionof aFO, and is capable of indexing repeated sub-clips ofa signal sampled at discrete times.AOs have been appliedto audio query [6] and audio structure discovery [8]. TheVMOdata structure was ﬁrst proposed in [24] as an efﬁ-cient audio query-matching algorithm. This paper showsthe capability of using aVMOto ﬁnd repeated sub-clips ina signal in an unsupervised manner.This paper is structured as follows: section 2 introducestheVMOdata structure and the accompanying pattern ﬁnd-ing algorithm. Section 3 documents the experiments onsymbolic and audio representations as well as the dataset,feature extraction, and task setup. Section 4 provides anevaluation of the experiment. Last, future work, observa-tions and insights are discussed in section 5.1760123456789 10 11ab bcabcdabc00101220123bccdd\n2233LRS valuesSuffix LinksaabcbcbcFigure 1. (Top) AVMOstructure with symbolized sig-nal{a, b, b, c, a, b, c, d, a, b, c}, upper (solid) arrows repre-sent forward links with symbols for each frame and lower(dashed) are sufﬁx links. Values outside of each circle arethelrsvalue for each state. (Bottom) A visualization ofhow patterns{a, b, c}and{b, c}are related tolrsandsfx.2. VARIABLE MARKOV ORACLEAVMOsymbolizes a time seriesO, sampled at timet,into a symbolic sequenceQ=q1,q2,...,qt,...,qT, withTstates and with frameO[t]labeled by a symbolqt. Thesymbols are formed by tracking sufﬁx links along the statesin an oracle structure. An oracle structure (eitherFO,AOorVMO) carries three kinds of links: forward link, sufﬁxlink and reverse sufﬁx link. A sufﬁx link is a backwardpointer that links statettokwitht>k, without a label,and is denoted bysfx[t]=k.sfx[t]=k()the longest repeated sufﬁx of{q1,q2,...,qt}is recognized ink.Sufﬁx links are used to ﬁnd repeated sufﬁxes inQ. In orderto track the longest repeated sufﬁx at each time indext, thelength of the longest repeated sufﬁx at each statet(denotedaslrs[t]) is computed by the algorithm described in [13].A reverse sufﬁx link,rsfx[k]=t, is the sufﬁx link inthe reverse direction.sfx,lrsandrsfxallow for theproposed pattern discovery algorithm described in section2.2.Forward links are links with labels and are used to re-trieve any of the factors fromQ. Since forward links arenot used in the proposed algorithm, readers are referredto [13] for details.The last piece for the construction of aVMOis a thresh-old value,✓.✓is used to determine if the incomingO[t]issimilar to one of the frames following the sufﬁx link be-ginning att\u00001. Two frames,O[i]andO[j], are assignedthe same symbol if|O[i]\u0000O[j]|✓. In extreme cases,aVMOmay assign different symbols to every frame inO(✓excessively low), or aVMOmay assign the same sym-bol to every frame inO(✓excessively high). In these twocases, theVMOstructure is incapable of capturing any pat-terns (repeated sufﬁxes) in the signal. The optimal✓can befound by calculating theInformation Rate(IR), a music in-formation dynamics measure, and this process is describedin section 2.1. An example of an oracle structure with ex-treme✓values is shown in Fig. 2.The on-line construction algorithms ofVMOare intro-Figure 2. Two oracle structures with extreme values of✓. The characters near each forward link represent the as-signed labels. (Top) The oracle structure with✓=0orextremely low✓value. (Bottom) The oracle structure witha very high✓value. In both cases the oracles are not ableto capture any structure in the time series.duced in [24] and not repeated here. Fig. 1 shows an ex-ample of a constructedVMOand howlrsandsfxarerelated to pattern discovery. The symbols formed by gath-ering states connected by sufﬁx links share the followingproperties :1)the pairwise distance between states con-nected by sufﬁx links is less than✓,2)the symbolizedsignal formed by the oracle can be interpreted as a sam-ple from a variable-order Markov model because the statesconnected by sufﬁx links share common sufﬁxes with vari-able length,3)each state is labeled by a single symbol be-cause each state has a single sufﬁx link,4)the alphabetsize of the assigned symbols is unknown before the con-struction and is determined by✓.2.1 Model Selection via Information RateThe same input signal may be associated with multipleVMOs with different sufﬁx structures and different sym-bolized sequences if different✓values are used to constructtheVMOs. To select the one symbolized sequence withthe most informative patterns,IRis used as the criterion inmodel selection between different structures generated bydifferent✓values.IRis an information theoretic measurecapable of measuring the information content of a time se-ries [7] in terms of the predictability of its source processon the present observation given past ones. In the contextof pattern discovery with aVMO,aVMOwith higherIRvalue captures more of the repeating sub-clips (ex. pat-terns, motives, themes, gestures, etc) than the ones withlowerIRvalues.TheVMOstructure uses the same approach as theAOstructure [8] to calculateIR. LetxN1={x1,x2,...,xN}denote time seriesxwithNobservations,H(x)the en-tropy ofx, the deﬁnition ofIRisIR(xn\u000011,xn)=H(xn)\u0000H(xn|xn\u000011).(1)IRis the mutual information between the present and pastobservations and is maximized when there is a balancebetween variations and repetitions in the symbolized sig-nal. The value ofIRcan be approximated by replacing theentropy terms in (1) with complexity measures associatedwith a compression algorithm. These complexity measuresProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 177Figure 3.IRvalues are shown on the vertical axis while✓are on the horizontal axis. The solid blue curve showsthe relationship betweenIRand✓, and the dashed blackline indicates the chosen✓by locating the maximumIRvalue. Empirically,IRcurves exhibit quasi-concave func-tion shapes, thus a global maximum can be located.Algorithm 1Pattern Discovery usingVMORequire:VMO, V , of lengthTand minimum pattern lengthL.Ensure:sfx,rsfx,lrs2V1:InitializePt t randPt t rL e nas empty lists.2:InitializeprevSfx=\u00001,K=03:fori=T:Ldo4:pttrF ound=Fa ls e5:ifi\u0000lrs[i]+1>sfx[i]^sfx[i]6=0^lrs[i]\u0000Lthen6:if9k2{1,...,K},sfx[i]2Pt t r[k]then7:AppenditoPt t r[k]8:Pt t rL e n[k] min(lrs[i],Pt t r L e n[k])9:pttrF ound=Tru e10:end if11:ifprevSfx\u0000sfx[i]6=1^pttrF ound==Fa ls ethen12:Append{sfx[i],i ,rsfx[i]}toPt t r13:Appendmin{lrs[{sfx[i],i ,rsfx[i]}]}toPt t rL e n14:K K+115:end if16:prevSfx sfx[i]17:else18:prevSfx \u0000119:end if20:end for21:returnPt t r ,Pt t rL e n ,Kare the number of bits used to compressxnindependentlyand compressxnusing the past observationsxn\u000011. Theformulation of combining the lossless compression algo-rithm,Compror[12], withAOandIRis provided in [8]. Avisualization of the sum ofIRvalues versus different✓s onone of the music pieces tested in this paper is depicted inFig. 3.2.2 Pattern DiscoveryAlgorithm 1 shows theVMO-based algorithm for the auto-matic pattern discovery task. The idea behind Algorithm1 is to track patterns by followingsfxandlrs.sfxprovides the locations of patterns, andlrsindicates thelength of these patterns. In line 5 of Algorithm 1, checksare made so that redundant patterns are avoided, and thelengths of patterns are larger than a user-deﬁned minimumL. From line 6 to 10, the algorithm recognizes occurrencesof established patterns, and from line 11 to 15 it detectsnew patterns and stores them intoPt t randPt t rLen.Algorithm 1 returnsPt t r ,Pt t rLenandK.Pt t ris alist of lists with eachPt t r[k],k2{1,2,...,K}, a listcontaining the ending indices of different occurrences ofthekth pattern found.Kis the total number of patternsfound.Pt t rLenhasKvalues representing the length ofthekth pattern inPt t r.3. EXPERIMENTSThe dataset chosen for the music pattern discovery is theJKU Pattern Development Dataset (JKU-PDD) [3]. Thisdataset consists of ﬁve polyphonic classical music piecesor movements in both symbolic and audio representations.The ground truth of repeated patterns (motifs, themes, sec-tions) for each piece is annotated by musicologists. Thedetails of the experimental setup are provided in the fol-lowing sections.3.1 Feature ExtractionFor the automatic musical pattern discovery task, the chro-magram is the input feature to Algorithm 1 for both thesymbolic and audio representations. The chromagram is afeature that characterizes harmonic content and is a com-monly used in musical structure discovery [1].3.1.1 Symbolic RepresentationFor the experiments described in this paper, the symbolicrepresentation chosen is MIDI, but other symbolic repre-sentations may be used instead. The chromagram derivedfrom the symbolic representation is referred to as the “MIDIchromagram”.The MIDI chromagram is similar to the MIDI histogramdescribed in [23] and represents the presence of pitch classesduring each time frame. To create a MIDI chromagramwith quantizationbin terms of MIDI whole note beats,frame sizeM, and hop sizeh, the MIDI ﬁle is ﬁrst parsedinto a matrix where each column is a MIDI beat quantizedbyband each row is a MIDI note number(0\u0000127). Foreach analysis frame, the velocities are summed overMMIDI beats, and then folded and summed along the MIDInotes to create a single octave of velocities. In other words,all velocities that correspond to MIDI notes that share thesame modulo 12 are summed. The analysis frame thenhopshMIDI beats forward in time, repeats the foldingand summing, and continues on until the end of the MIDImatrix is reached. The bottom plot in Fig. 4 is an exam-ple of the MIDI chromagram extracted from the Beethovenminuet in the JKU-PDD.3.1.2 Audio RecordingThe routines for extracting the chromagram from an audiorecording used in this paper is as follows. For a mono au-dio recording sampled at44.1kHz, the recording is ﬁrstdownsampled to11025Hz. Next, a spectrogram is calcu-lated using a Hann window of length8192with128sam-ples overlap. Then the constant-Q transform of the spec-trogram is calculated with frequency analysis ranging be-tweenfmin= 27.5Hz tofmax= 5512.5Hz and12binsper octave. Finally, the chromagram is obtained by foldingthe constant-Q transformed spectrogram into a single oc-tave to represent how energy is distributed among the12pitch classes.To achieve the pattern discovery on a music metricallevel, the chroma frames are aggregated with a median ﬁl-ter according to the beat locations found by a beat tracker178 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 4. Features, found patterns, and ground truthfor the Beethoven minuet in the JKU-PDD. 1. Beat-synchronous chromagram from the deadpan audio record-ing. 2. Patterns found by Algorithm 1 using the chroma-gram shown above. 3. Ground truth from JKU-PDD. 4.Patterns found by Algorithm 1 using the MIDI chroma-gram. 5. Quantized MIDI chromagram. For 2., 3. and4., each row is a pattern place holder with dark regionsrepresenting the occurrences on the timeline. The orderof found patterns is manually sorted to best align with theground truth for visualization purpose. Notice the hierar-chical relations of patterns embedded in the ground truthand found from the algorithms.[10] conforming to the music metrical grid. For ﬁner rhyth-mic resolution, each beat identiﬁed is spliced into two sub-beats before chroma frame aggregation. Last, the sub-beat-synchronous chromagram is whitened with alogfunction.Whitening boosts the harmonic tones implied by the mo-tifs so that the difference between the same motif with andwithout harmonization is reduced. See the top plot in Fig.4 for an example of the the beat-synchronous chromagramextracted from the Beethoven minuet in the JKU-PDD.3.2 Repeated Themes DiscoveryFor both symbolic and audio representations, after thechroma feature sequenceOis extracted from the musicpiece as described in section 3.1.1 and 3.1.2,✓2(0.0,2.0]is used to construct multipleVMOs withO. TheL2\u0000normis used to calculate the distance between incoming obser-vations and the ones stored in aVMO. The singleVMOwith the highestIRis fed into Algorithm 1 withLto ﬁndpatterns and their occurrences. Instead of settingL=5for all pieces as in [25],Lis set according tolrsasL=\u0000TPTt=1lrs[t], whereLis adaptive to the averagelength of repeated sufﬁxes found in the piece.\u0000is a scalingparameter which is set to0.5empirically.To consider transposition (moving patterns up or downby a constant pitch interval), the distance function usedforVMOstructures is a cost function with transpositioninvariance. For a transposition invariant cost function, acyclic permutation with offsetkon ann-dimensional vec-torx=(x0,x1,...,xn\u00001)is deﬁned ascpk(x): ={xi!x(i+kmodn),8i2(0,1,...,n\u00001)},and the transposition invariant dissimilaritydbetween twovectorsxandyis deﬁned as,d=m i nk{kx\u0000cpk(y)k2}.n= 12for the chroma vector, and the cost function is usedduring theVMOconstruction.In addition to the basic chromagram, a stacked chroma-gram using time-delay embedding withMsteps of historyas in [22] is also used. Experiments reveal that choicesforb,M, andhfor both the MIDI chromagram and thestacked MIDI chromagram can greatly alter the accuracyof patterns discovered. The values used in the experimentswere quantization sizesb=[18,116,132], frame sizeM=[1,8,16,32], and hop lengthsh=[ 1,2,4]whereMandhare described in terms of MIDI beats of sizeb. It wasfound that the stacked MIDI chromagram withb=132,M= 16, andh=2resulted in the best pattern discov-ery. For the audio representation, there is no signiﬁcantdifference in terms of the patterns found or the evaluationmetrics between regular and stacked chromagrams.Fig. 4 shows the chromagram found from audio andMIDI for the Beethoven minuet in the JKU-PDD alongwith the patterns found by theVMOstructure and theground truth patterns. The patterns found by the audio andsymbolic representations share similarities and visually re-semble the ground truth patterns. In section 4, quantitativemeasures for evaluating the patterns found by theVMOareexplained and reported.3.3 Performance Recordings to Aid Pattern DiscoveryFive performance recordings for each of the pieces includedin the JKU-PDD are collected in order to further explorethe discovery of repeated themes. The motivation behindthis experiment is to explore the notion that music perfor-mances contain information about how performers inter-pret the musical structure embedded in the score [21] andto examine whether or not the patterns found on deadpanaudio could be improved with the addition of such infor-mation.For each of the performance recordings, the chroma-gram is extracted and aggregated along the beats as de-scribed in section 3.1.2. Dynamic Time Warping [17] isused to align the beat-synchronous chromagram from theperformance audio with the beat-synchronous chromagramof the deadpan audio. Since motif annotations on theseperformance recordings do not exist yet, the alignment be-tween the deadpan audio and performance recordings arenecessary so that the patterns found from the performanceProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 179Figure 5. 1. Ground truth from JKU-PDD. 2. Patternsfound from deadpan audio with theVMO.3\u00007. Patternsfound from the ﬁve performances. 8. Patterns from dead-pan and performance audio.recordings can be compared to the ground truth or addedto the found patterns from the deadpan audio. The draw-back of the alignment is that timing variations contain-ing the performer’s structural interpretation are lost. Al-though timing variations are lost in this experiment, ve-locity variations applied across time and different voicesare retained. The aligned performance audio chromagramis then whitened, normalized and fed into theVMOpat-tern ﬁnding algorithm. For patterns found across multipleperformances of one piece, the intersection of patterns forany two performances of one piece that are longer thanLare kept and added to the found patterns from the deadpanaudio. Fig. 5 is an example of how incorporating perfor-mance recordings can change the discovered patterns fromdeadpan audio.4. EVALUATIONThe evaluation follows the metrics proposed in the Mu-sic Information Retrieval Evaluation eXchange (MIREX)[3]. Three metrics are considered for inexact pattern dis-covery. For each metric, standardF1score, deﬁned asF1=2PR(P+R), precisionPand recallRare calculated. Theﬁrst metric is the establishment score (est) which measureshow each ground truth pattern is identiﬁed and covered bythe algorithm. The establishment score takes inexactnessinto account and does not consider occurrences. The sec-ond metric is the occurrence score (o(c)) with a thresh-oldc. The occurrence score measures how well the al-gorithm performs in ﬁnding occurrences of each pattern.The thresholdcdetermines whether or not an occurrenceshould be counted. The higher the value forc, the lowerthe tolerance.c={0.5,0.75}are used in standard MIREXevaluation. The last metric is the three-layer score that con-siders both the establishment and occurrence score. The re-sults of the proposed framework are listed in Table 1 alongwith a comparison to previous work.From the evaluations for both symbolic and audio rep-resentations, the establishment scores are generally lowerthan the occurrence scores, meaning that the proposed al-gorithm is better at ﬁnding occurrences of established pat-terns than ﬁnding all possible patterns. With the symbolicrepresentation, the standardFest,Fo(.75), andF3scoresare better than previously published results. The estab-lishment, occurrence, and three-layer precision scores arealso as good as or better than previous algorithms [5, 15].The recall scores reveal that this is a part of the algorithmthat could be improved as previous algorithms all scoredhigher on recall than the proposed algorithm. Similar to thesymbolic results, the proposed audio algorithm achieveshighF1and precision scores for the establishment, oc-currence, and three-layer scores. The recall of the audioalgorithm is higher than previously reported results [5, 19,20]. The recall rates of the proposed framework are infe-rior when compared to the precision scores and previouswork in symbolic representation. This may occur becausechroma features were used and the folding of the constant-Q spectrogram discards information contained in differentvoices.The inclusion of performance recordings is the effortmade in this work to improve both the coverage and accu-racy of the pattern discovery framework for audio repre-sentations. Due to space limitations, the detailed metricsfor each piece in the JKU-PDD is not shown here. Theeffects of including performance recordings are describedhere. The establishment recall rate and occurrence pre-cision rate with threshold0.5are improved when perfor-mance recordings are included, but in general the patterndiscovery task is not improved because the decrease in es-tablishment precision rate is larger than the improvementon recall rates. This result indicates that more patterns andtheir occurrences could be discovered if different versionsof the same piece are used in the pattern discovery task, butmore false positive patterns will be found.The proposed pattern ﬁnding algorithm completed inless time than previously reported algorithms on both sym-bolic and audio representations. Although theVMOdatastructure is used for both the proposed symbolic and au-dio algorithms, there is a discrepancy in the time that ittakes to ﬁnd the patterns for all ﬁve songs. The audio al-gorithm takes much less time because the analysis framesare larger than the frames used in the symbolic representa-tion (32th note versus8th note relatively). Thus, there areless frames to analyze with the audio representation andbuilding aVMOtakes less time.Fig. 6 is a summary of the three-layerF1scores foreach of the 5 pieces in the JKU-PDD for the proposed au-dio and symbolic frameworks along with the current stateof the art results. The small quantization value for theMIDI representation leads to a higher score in the caseof the Beethoven and Chopin pieces. The proposed audio180 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015AlgorithmFestPestRestFo(.5)Po(.5)Ro(.5)Fo(.75)Po(.75)Ro(.75)F3P3R3Time (s)VMO symbolic60.79 74.5756.9471.9279.5468.7875.98 75.9875.9956.68 68.9853.564333[5]33.7 21.578.076.578.374.7\u0000\u0000\u0000\u0000\u0000\u0000\u0000[15]50.20 43.60 63.8063.20 57.00 71.6068.40 65.40 76.4044.20 40.4054.407297VMO deadpan56.15 66.857.8367.7872.9364.370.58 72.81 68.6650.6 61.3652.2596deadpan + real52.76 53.2 58.2567.3574.4263.3170.51 72.73 68.5848.25 50.252.84\u0000[20]49.8 54.96 51.7338.73 34.98 45.1731.79 37.58 27.6132.01 35.12 35.28454[5]23.94 14.960.956.87 62.9 51.9\u0000\u0000\u0000\u0000\u0000\u0000\u0000[19]41.43 40.83 46.4323.18 26.6 20.9424.87 32.08 21.2428.23 30.43 31.92196Table 1. Results from various algorithms on the JKU-PDD for both symbolic (upper three) and audio (bottom four)representations. Scores are averaged across pieces. Missing values were not reported in their original publications.\nFigure 6. Three-layerF1score (F3in Table 1) for theproposed audio and symbolic method on the 5 pieces inthe JKU-PDD plotted along with state of the art results.and symbolic framework have the highestF1value on theBeethoven minuet and the lowestF1value with the BachFugue. When looking at the proposed method along withcurrent the state of the art results, it is evident that the BachFugue and the Gibbons piece are songs where patterns areembedded in different voices, and that the Beethoven piecehas more consistent repeated phrases. The algorithm forsymbolic data described in [15] performs better with Bachand Gibbons in comparison toVMOand [20], most likelybecause of its capability to discover patterns embedded indifferent yet simultaneous voices.In summary, our method has improved upon theF1andPscores as well as time to ﬁnd patterns. The patternsfound using audio and symbolic representations are similarand the evaluation scores reﬂect this similarity. Improvingrecall and allowing for inexact occurrences should be a fo-cus for future studies. Source codes and details about theexperiments are accessible via Github1.5. DISCUSSIONIn this work, a framework for automatic pattern discoveryfrom a polyphonic music piece based on aVMOis pro-posed and shown to achieve state of the art performance onthe JKU-PDD dataset. With both the regular and stackedMIDI chromagram, a smaller quantization valuebresultsin better pattern discovery because ﬁner details are cap-tured with smaller quantization. From the results, it seemsthat a larger frame sizeMfor smaller quantizationbre-sulted in better pattern ﬁnding. For hop sizeh, it is ob-served thath=2results in a hop of a16th note which1https://github.com/wangsix/VMO_repeated_themes_discoveryis the shortest note in the JKU-PDD ground truth annota-tions. Results from both the audio and MIDI representa-tions show that the recall of discovered themes could beimproved. Although it is possible for aVMOto identifyinexact patterns from the input feature sequence with sym-bolization from✓, different occurrences of the same patternare sometimes not recognized because chroma features dis-card information from various voices in the music piece.Our framework could be improved if the feature used al-lows for separation of voices from polyphonic MIDI andaudio. Incorporating techniques for identifying multiplevoices in polyphonic audio would improve the proposedframework.In addition to the proposed framework for both sym-bolic and audio representations, using multiple perfor-mance recordings in the repeated themes discovery taskfor deadpan audio is another novelty presented in this pa-per. The work done in this paper differs from [5] in that theperformance audio recordings are used as supplements todeadpan audio and not analyzed as separate musical enti-ties. The original intention behind using deadpan audio forrepeated themes discovery is to allow for the use of audiosignal processing techniques, but deadpan audio containsthe same amount of information as its symbolic counter-part with less accessibility because of its representation.This is evident by the similarity between the MIREX met-rics for the MIDI and deadpan audio since similar tech-niques are applied. Performance recordings, on the otherhand, contain expressive performance variations on phras-ing and segmentation. In this paper, it is shown thatadding performance recordings to the proposed frameworkachieved improvements on some of the standard metrics.The next step for advancing the repeated themes discov-ery task is to annotate the performance recordings so thatthese recordings can be used as a dataset directly withoutreferencing back to the deadpan audio version. By observ-ing the results from the pattern ﬁnding with performancerecordings, the patterns found for each performance showinformative cues as to how each rendition of the same piecediffers from the others visually (Fig. 5). These visualiza-tions are interesting discoveries on their own, even with-out a comparison to ground truth annotations, and couldbe further investigated for use in expressive performanceanalysis and music structural segmentation.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 1816. REFERENCES[1]Juan Pablo Bello. Measuring structural similarity inmusic.Audio, Speech, and Language Processing, IEEETransactions on, 19(7):2013–2025, 2011.[2]Emilios Cambouropoulos, Maxime Crochemore,Costas S Iliopoulos, Manal Mohamed, and Marie-France Sagot. All maximal-pairs in step–leap repre-sentation of melodic sequence.Information Sciences,177(9):1954–1962, 2007.[3]Tom Collins. Discovery of repeated themes andsections.Retrieved 4th May, http://www.music-ir.org/mirex/wiki/2013:DiscoveryofRepeatedThemes&Sections, 2013.[4]Tom Collins, Andreas Arzt, Sebastian Flossmann, andGerhard Widmer. SIARCT-CFP: Improving precisionand the discovery of inexact musical patterns in point-set representations. InISMIR, pages 549–554, 2013.[5]Tom Collins, Sebastian B¨ock, Florian Krebs, and Ger-hard Widmer. Bridging the audio-symbolic gap: Thediscovery of repeated note content directly from poly-phonic music audio. InAudio Engineering SocietyConference: 53rd International Conference: SemanticAudio. Audio Engineering Society, 2014.[6]Arshia Cont, Shlomo Dubnov, G´erard Assayag, et al.Guidage: A fast audio query guided assemblage. InIn-ternational Computer Music Conference, 2007.[7]Shlomo Dubnov. Spectral anticipations.Computer Mu-sic Journal, 30(2):63–83, 2006.[8]Shlomo Dubnov, G´erard Assayag, and Arshia Cont.Audio oracle analysis of musical information rate. InSemantic Computing (ICSC), 2011 Fifth IEEE Inter-national Conference on, pages 567–571. IEEE, 2011.[9]Shlomo Dubnov, Gerard Assayag, Arshia Cont, et al.Audio oracle: A new algorithm for fast learning of au-dio structures. InInternational Computer Music Con-ference, 2007.[10]Daniel P. W. Ellis. Beat tracking by dynamic program-ming.Journal of New Music Research, 36(1):51–60,2007.[11]Berit Janssen, W. Bas de Haas, Anja V olk, and Pe-ter Kranenburg. Discovering repeated patterns in mu-sic: potentials, challenges, open questions. In10thInternational Symposium on Computer Music Multi-disciplinary Research. Laboratoire de M´ecanique etd’Acoustique, 2013.[12]Arnaud Lefebvre and Thierry Lecroq. Compror: on-line lossless data compression with a factor oracle.In-formation Processing Letters, 83(1):1–6, 2002.[13]Arnaud Lefebvre, Thierry Lecroq, and Jo¨el Alexandre.An improved algorithm for ﬁnding longest repeats witha modiﬁed factor oracle.Journal of Automata, Lan-guages and Combinatorics, 8(4):647–657, 2003.[14]Brian McFee and Daniel P. W. Ellis. Analyzing songstructure with spectral clustering. InThe 15th Interna-tional Society for Music Information Retrieval Confer-ence, pages 405–410, 2014.[15]David Meredith. COSIATEC and SIATECCompress:Pattern discovery by geometric compression. InInter-national Society for Music Information Retrieval Con-ference, 2013.[16]David Meredith, Kjell Lemstr¨om, and Geraint A. Wig-gins. Algorithms for discovering repeated patterns inmultidimensional representations of polyphonic music.Journal of New Music Research, 31(4):321–345, 2002.[17]Meinard M¨uller. Dynamic time warping.Informationretrieval for music and motion, pages 69–84, 2007.[18]Oriol Nieto and Morwaread Farbood. Perceptual eval-uation of automatically extracted musical motives.InProceedings of the 12th International Conferenceon Music Perception and Cognition, pages 723–727,2012.[19]Oriol Nieto and Morwaread Farbood. MIREX 2013:Discovering musical patterns using audio structuralsegmentation techniques.Music Information RetrievalEvaluation eXchange, Curitiba, Brazil, 2013.[20]Oriol Nieto and Morwaread Farbood. Identifying poly-phonic patterns from audio recordings using music seg-mentation techniques. InThe 15th International Soci-ety for Music Information Retrieval Conference, 2014.[21]John Rink, Neta Spiro, and Nicolas Gold. Motive, ges-ture, and the analysis of performance.New Perspec-tives on Music and Gesture, pages 267–292, 2011.[22]Joan Serr`a, Meinard Mueller, Peter Grosche, andJosep Ll Arcos. Unsupervised music structure annota-tion by time series structure features and segment sim-ilarity.IEEE Transactions on Multimedia, 16(5):1229–1240, 2014.[23]George Tzanetakis, Andrey Ermolinskyi, and PerryCook. Pitch histograms in audio and symbolic musicinformation retrieval.Journal of New Music Research,32(2):143–152, 2003.[24]Cheng-i Wang and Shlomo Dubnov. Guided musicsynthesis with variable markov oracle. InThe 3rd Inter-national Workshop on Musical Metacreation, 10th Ar-tiﬁcial Intelligence and Interactive Digital Entertain-ment Conference, 2014.[25]Cheng-i Wang and Shlomo Dubnov. Pattern discoveryfrom audio recordings by variable markov oracle: Amusic information dynamics approach. InAcoustics,Speech, and Signal Processing (ICASSP), 2015 IEEEInternational Conference on. IEEE, 2015.182 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "On the Impact of Key Detection Performance for Identifying Classical Music Styles.",
        "author": [
            "Christof Weiß",
            "Maximilian Schaab"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416246",
        "url": "https://doi.org/10.5281/zenodo.1416246",
        "ee": "https://zenodo.org/records/1416246/files/WeissS15.pdf",
        "abstract": "We study the automatic identification of Western classi- cal music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece’s global key for estimating key-related pitch classes. First, we present four automatic key detec- tion systems. We compare their performance on suitable datasets of classical music and optimize the algorithms’ free parameters. Using a second dataset, we evaluate au- tomatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calcu- late global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individ- ual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for clas- sification. Furthermore, we evaluate the impact of key de- tection performance on the classification results. Addition- ally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms.",
        "zenodo_id": 1416246,
        "dblp_key": "conf/ismir/WeissS15",
        "keywords": [
            "chroma histograms",
            "global key",
            "pitch classes",
            "style periods",
            "Baroque",
            "Classical",
            "Romantic",
            "Modern",
            "automatic key detection",
            "classification features"
        ],
        "content": "ON THE IMPACT OF KEY DETECTION PERFORMANCEFOR IDENTIFYING CLASSICAL MUSIC STYLESChristof WeißFraunhofer Institute for Digital Media Technology Ilmenauchristof.weiss@idmt.fraunhofer.deMaximilian SchaabFraunhofer Institute for Digital Media Technology IlmenauABSTRACTWe study the automatic identiﬁcation of Western classi-cal music styles by directly using chroma histograms asclassiﬁcation features. Thereby, we evaluate the beneﬁtsof knowing a piece’s global key for estimating key-relatedpitch classes. First, we present four automatic key detec-tion systems. We compare their performance on suitabledatasets of classical music and optimize the algorithms’free parameters. Using a second dataset, we evaluate au-tomatic classiﬁcation into the four style periods Baroque,Classical, Romantic, and Modern. To that end, we calcu-late global chroma statistics of each audio track. We thensplit up the tracks according to major and minor keys andcircularly shift the chroma histograms with respect to thetonic note. Based on these features, we train two individ-ual classiﬁer models for major and minor keys. We test theefﬁciency of four chroma extraction algorithms for clas-siﬁcation. Furthermore, we evaluate the impact of key de-tection performance on the classiﬁcation results. Addition-ally, we compare the key-related chroma features to otherchroma-based features. We obtain improved performancewhen using an efﬁcient key detection method for shiftingthe chroma histograms.1. INTRODUCTIONIn the ﬁeld of Music Information Retrieval (MIR), a con-siderable amount of research has been performed to clas-sify music audio recordings according to different cate-gories [3,29]. Beyond top-levelgenressuch as Rock, Jazz,or Classical, several attempts towards resolvingsubgenreshave been made. We dedicate ourselves to the subgenreclassiﬁcation of Western classical music which has beenaddressed sparsely in previous work.There are plenty of possibilities to organize classicalmusic archives. Apart from the speciﬁc artists—soloistsor ensembles—, timbral properties such as the predomi-nant instrument(s) may serve as categories [26]. We thinkthat the rather abstract concept pfmusical styleprovides ac\u0000Christof Weiß, Maximilian Schaab.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Christof Weiß, Maximilian Schaab.“On the Impact of Key Detection Performancefor Identifying Classical Music Styles”, 16th International Society forMusic Information Retrieval Conference, 2015.more appropriate subgenre taxonomy. The speciﬁc appli-cation of this idea leads to the task of composer identiﬁca-tion [4, 11, 15, 22]. Beyond such a detailed taxonomy, werestrict ourselves to more general categories—thehistori-cal periodsBaroque, Classical, Romantic, and Modern.1This naturally constitutes a simpliﬁcation but may providea convenient starting point for ﬁner analyses [6].Several researchers have published studies on the ba-sis of symbolic data such as score or MIDI representa-tions [1, 8, 10, 11, 15, 22, 25]. However, we ﬁnd some ben-eﬁts when directly dealing with audio recordings. First,the audio incorporates more information than the score byrepresenting the “sounding reality” of the music to a higherdegree.2Second, audio-based methods enable nice appli-cations for organizing and browsing today’s large archivesof classical music. Morevoer, such archives provide pre-cious possibilities for data-driven musicological researchin a new quantitative dimension.Studies based on symbolic data often make use of mu-sical properties such as the use of speciﬁc intervals [1] orchords [22]. Sometimes, characteristics of polyphony andvoice leading are considered as well [1,11]. Other methodsrely on more fundamental properties of harmony such asthe occurrence of pitch classes [10] and pitch class sets [8].Usually, researchers statistically analyze these character-istics to obtain classiﬁcation features. These features arethen used as input for machine learning (ML) classiﬁers.There are several limitations for harmonic analysis ofaudio based on state-of-the-art signal processing algo-rithms. Due to the restricted perfomance of automatic mu-sic transcription3, we build our method upon chroma fea-tures that have been shown to suitably represent the pitchclass content of audio [7, 19]. Using chroma features, sev-eral musical characteristics such as voice leading proper-ties or interval and chord inversions cannot be resolved.Furthermore, acoustic phenomena such as overtones andtimbre show considerable effect on the chroma features.Scholars proposed several attempts to approach these prob-lems by enhancing the robustness of chroma [7,13,14,17].Researches have proposed several chroma-based fea-1Here, theModernclass refers to 20th century art music with somestylistic distance from romantic music.2This observation particularly matters for older music such as theBaroque style, where numerous conventions for practical performancewere known by the interpreters without notating them in the scores.3In particular, these algorithms highly depend on the orchestration.On that account, automatic transcription is not reliable when dealing withmixed music for piano, orchestra, and voices.45ture types for classifying musical genres and styles. Tzane-takis uses the predominant pitch class, its relative ampli-tude, and the size of the predominant interval as features[29]. Others extract chords from audio and classify basedon the chord types and progressions [24]. In [32], inter-val and chord types are estimated from different chromaresolutions. Furthermore, measures for quantifying tonalcomplexity have been tested as classiﬁcation features [33].In this paper, we want to test a simpler approach thatdirectly useschroma histogramsas input for a classiﬁer(Section 2). For tonal music, the chroma distribution ofa piece is mainly inﬂuenced by themusical key. Usually,the notes of the underlying scale and the most prominentchords obtain high values—such as the tonic note or thedominant note. We therefore test the beneﬁts of knowingthe global key for classifying with chroma features. To thatend, we ﬁrst compare four key detection methods (Sec-tion 2.2) on suitable datasets of classical music and opti-mize the algorithms’ parameters (Section 3.2). Second, weperform classiﬁcation experiments on a separate dataset of1600 classical pieces (Section 3.3). As classiﬁcation fea-tures, we use chroma histograms that are shifted on thebasis of different key algorithms or ground truth key anno-tations. We test the inﬂuence of considering the key as wellas the effect of training separate models for major and mi-nor keys. Finally, we compare these features’ perfomanceagainst other chroma-based features introduced in earlierwork [32, 33].2. PROPOSED METHODIn Western classical music, tonality and harmony play acentral part for establishing musical form, expression, andstyle. The use of speciﬁc pitches, intervals, and chords—aswell as their progressions—constitute typical style mark-ers. They hierarchically depend on each other and con-tribute to the chroma distribution of a piece. Beyond thehigh importance of the global key, modulations to otherkeys entail the use of different chords and pitches. Thatway, sections in foreign keys considerably contribute tothe global chroma histogram—depending on their length.Apart from such harmonic characteristics, instrumentationand timbre may affect the shape of the chroma distribution.Let us consider a simple major triad: Depending on the in-strumentation, the root, third, or ﬁfth note may be morepronounced leading to different chroma vectors.Some of these differences may serve to resolve subtlerstylistic differences. In Figure 1, we show two chromahistograms of symphony movements by Schumann andBrahms, both in a major key and centered to their respec-tive tonic note (CandF). Though these composers havemuch in common—a part of their lifetime, the culturalbackground, and several inspiring persons—the piecesconsiderably differ in their pitch class histograms. Onereason may be the more complex harmony in Brahms’music—the chromatic pitch classes such asF],C], andA[are enhanced compared to Schumann’s equivalents.Moreover, Brahms’ instrumentation often emphasizes thechords’ third notes. This could explain the increased val-Schumann, 2nd symphony, 1st mvmt.(C major)\nEbBbFCGDAEBF#C#G#00.20.40.6Brahms, 3rd symphony, 1st mvmt.(F major)\nAbEbBbFCGDAEBF#C#00.20.40.6\nFigure 1. Chroma histograms for Schumann’s 2nd sym-phony, 1st movement inC major(upper plot) and Brahms3rd symphony, 1st movement inF major(lower plot). Thehistograms are arranged according to the circle of ﬁfths andcentered to the respective tonic note. We normalize the dis-tributions to the`2norm in order to ensure comparability.ues forD,A, andE—the triad thirds of the main chordsB[M(subdominant),FM(tonic), andCM(dominant), re-spectively. Another explanation for this observation couldbe a modulation to the local keyA majorfor a consid-erable amount of time. Such modulations to third-relatedmediant keysare common in late romantic music.To describe such characteristics, therelativepitchclasses are important. Therefore, we need informationabout the global key. Sometimes, this metadata is providedin musical archives. However, such annotations are oftenincomplete. For work cycles and multi-movement works,we usually ﬁnd only one key (“Symphony inF major”)which single movements may differ from. For those rea-sons, we test automatic methods for audio key detectionand evaluate the inﬂuence of their performance on the over-all classiﬁcation results. We also compare automatic keydetection to the use of ground truth key annotations.Apart from the tonic note, the mode (major / minor) isof high importance, since the harmonic structure of minorpieces fundamentally differs from the one in major. Tothat end, we split up our data and train a separate modelfor each mode. Section 2.3 outlines the details of this idea.2.1 Chroma FeaturesIn audio signal processing, chroma features have beenshown to suitably represent tonal characteristics [7, 19].For a chromagram, the spectrogram bins are mappedinto a series of 12-dimensional chroma vectorsc=(c0,c1,...,c11)T2R12. These vectors represent the en-ergy of the pitch classes that are independent from the oc-tave. To reduce the inﬂuence of overtones and timbralcharacteristics, several chroma extraction methods havebeen proposed. We consider six different approaches:46 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015(i)CP. This algorithm [21] is based on a multirate ﬁlterbank and published in the Chroma Toolbox [18]. Weuse theChroma Pitchas our baseline feature.(ii)CLP. Jiang et al. found improvement of chord recog-nition when using logarithmic compression beforeoctave summarization. We use theChroma Loga-rithmic Pitchwith compression parameter⌘= 1000which performed best in [9].(iii)CRP.M¨uller and Ewert proposed a method to elim-inate timbral information using the Discrete CosineTransform—Chroma-DCT-Reduced Log Pitch[17].(iv)HPCP. TheseHarmonic Pitch Class Proﬁlescon-sider the overtones for the chroma computation [7].(v)EPCP. In [27],Enhanced Pitch Class Proﬁles[13]performed best in a chord matching experiment. Thisalgorithm makes use of an iterative procedure (har-monic product spectrum). We use three iterations.(vi)NNLS. Mauch introduced an approximate transcrip-tion step based on aNon-Negative Least Squaresalgorithm [14]. The resulting chroma features ledto a considerable boost of chord recognition perfor-mance. The code is published as a “Vamp” plugin.4We compute the initial chroma features with a resolutionof 10 Hz. In order to eliminate the inﬂuence of dynamics,we normalize to the`1norm so that||c||1=PN\u00001n=0|cn|=1.(1)2.2 Key Detection AlgorithmsFor automatic key detection, we compare four approachesthat have been tested successfully on classical music data.(1)Template matching. For this standard method, thedistance between a chroma histogram and a key pro-ﬁle is computed for each of the 24 keys. The proﬁleminimizing the distance gives the global key [28].(2)Proﬁle learning. Van de Par et al. improved the pro-ﬁle matching algorithm by using a learning procedurefor the key proﬁles [30]. Furthermore, they emphasizethe beginning and ending section of the pieces. We ex-tend this idea by separately weighting beginning andending section. Therefore, we introduce new parame-ters\u0000and\u0000to emphasize the beginning and ending,respectively—along with the parameter↵from [30].(3)Symmetry model. Another class of key ﬁnding algo-rithms makes use of geometrical pitch models [2, 5].We use the symmetry model by Gatzsche and Mehnertthat was evaluated for key detection in [16].(4)Final chord. The algorithm proposed in [31] consid-ers the ﬁnal chord to estimate the tonic note of theglobal key—combined with a proﬁle matching for es-timating the mode. This algorithm was tested on threedatasets of classical music.4http://isophonics.net/nnls-chroma2.3 Classiﬁcation FeaturesThe basic idea of this paper is to directly use chroma his-tograms for classiﬁcation of music styles. We thereforesum up theMchroma vectorsc1,...,cMof a piece inorder to obtain a`1normalized chroma histogramh:bh=PMi=1ci,h=bh/||bh||1(2)In order to compare the impact of the chroma compu-tation method, we use four different chroma algorithmsfrom the ones presented in Section 2.1: CP, CLP, EPCP,and NNLS.As the main contribution of our work, we want to eval-uate the relevance of key information for classiﬁcation.To this end, we test different combinations of key esti-mation and classiﬁcation algorithms. Using 3-fold cross-validation, we randomly split our dataset into a trainingfold (2/3) and a test fold (1/3). For the training stage, theground truth key annotations are used to split up the datainto pieces in major and minor modes. With the same keyinformation, we circularly rotate the chroma histograms sothat the tonic note is on the ﬁrst position:hrotatedk=h(k\u0000k⇤)m o d1 2,(3)withk2[0 : 11]andk⇤denoting the chroma index of thetonic note (k⇤=0forC, etc.). For testing, we use one ofthe four automatic key detection algorithms presented inSection 2.2. With this key information, we split up the testdata according to the mode and again rotate each chromahistogram with respect to the tonic note. The full process-ing chain of our approach is shown in Figure 2.To compare against existing methods, we use othertypes of chroma-based classiﬁcation features. In [32], a setoftemplate-based featuresfor estimating the occurence ofinterval and chord types has been proposed. To this end,chroma features are smoothed to different temporal reso-lutions followed by a multiplication of chroma values ac-cording to interval and chord templates. Another group—tonal complexity features—makes use of statistical mea-sures on the chroma distribution in order to estimate thetonal complexity of the music on different time scales [33].3. EVALUATIONIn order to estimate the classiﬁcation performance on un-seen data, we apply a two-step evaluation strategy. First,we test the key detection performance of the four meth-ods presented in Section 2.2 and optimize the algorithms’free parameters (Section 3.2). Second, we perform classi-ﬁcation experiments on a different dataset using a RandomForest classiﬁer with chroma histograms as input features.We train separate models for major and minor pieces, re-spectively. For estimating the importance of the algo-rithm’s elements, we conduct several baseline experiments.3.1 DatasetsIn our studies, we make use of different datasets. Toevaluate key detection performance and optimize param-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 47  CLASS Baroque Classical Romantic Modern  Data (Chroma features) Training data Test data Maj Train Min Train Maj Test Min Test Maj Model Min Model classify Training Key 1 Key 2 classify Figure 2. Flow diagram for the classiﬁcation procedure. For performing cross-validation, the data is split up into trainingand test set. Each set is sorted with respect to the mode by using different key algorithms or ground truth key annotations(key 1 / key 2), respectively. The trained models for major and minor keys are then used to classify the respective test data.eters, we use three datasets of classical music record-ings with corresponding key annotations. This data hasbeen used for evaluating key detection in published work[23, 30, 31]. The ﬁrst set (Symph) comprises classical andromantic symphonies—each with all movements—from11 composers containing115 tracksin total. The sec-ond one—a selection fromSaarland Music Data WesternMusic(SMD) [20]—includes music for solo instruments,orchestra, and chamber music. The key annotations for126 selected tracksthat show clear tonality are availableon the corresponding website.5Third, we recompiled adataset of piano music recordings (Pno) used for key de-tection in [23, 30]. This data comprises237 piano pieces(Bach, Brahms, Chopin and Shostakovich). We considerthese datasets as training data for the key detection step,thus justifying the overﬁtting procedure for the parameters.For the classiﬁcation experiments, we make use of an-other dataset (Cross-Era) containing1600 audio record-ingsof classical music as used in [32, 33]. The data isbalanced with respect to the historical periods (each 400tracks for the Baroque, Classical, Romantic, and Mod-ern period) and instrumentation (200 piano pieces and 200orchestral pieces per class). We collected expert annota-tions for the key of 1200 tracks. The modern class hasnot been considered due to a high amount of atonal pieces.For atonal pieces, we assume little inﬂuence of key detec-tion on classiﬁcation with chroma histograms.6The datais not balanced with respect to the key or the mode (ma-jor/minor). We show the key distribution in Figure 3.3.2 Key Detection ExperimentsFor estimating the optimal parameters, we run each algo-rithm with different parameter settings in a stepwise fash-ion. To that end, we optimize each parameter by maximiz-ing the weighted total performance⇤t⇤t= (115 ⇤Symph+ 126 ⇤SMD+ 237 ⇤Pno)/478(4)5http://www.mpi-inf.mpg.de/resources/SMD6For example, a dodecaphonic piece of music shows nearly equalpitch class distribution. Thus, its chroma distribution is practically in-variant to cyclic shifts.160120804004080120160\nC#G#D# A#FCGDAEBF#a#fcgdaebf#c#g#d#Figure 3. Key distribution (annotations) of the periodsBaroque, Classical, and Romantic (1200pieces) in thedatasetCross-Era. Major keys are shown in black and up-ward direction, minor keys are in grey downwards. Thetonic notes are arranged according to the circle of ﬁfths.and ﬁx the remaining parameters to default or best ﬁt val-ues. For the basic chroma features, we test the six typespresented in Section 2.1. We obtain the following resultsfor the different algorithms:(1)Template matching. We test three pairs (maj / min) ofproﬁles proposed by Krumhansl [12], Temperley [28],and Gomez [7]. In our study, the latter ones performedbest. Though these proﬁles have been developed incombination with HPCP features, NNLS features out-performed these features (84.7 %) followed by CLP.(2)Proﬁle learning. For the proﬁle training, we per-formed a cross-validation with 98 % training data, 2 %test data, and 5000 repetions following [30]. We foundbest performance for CLP chroma features (92.3 %)—closely followed by NNLS—together with parameters↵=2,\u0000=1, and\u0000=0.25. We could not reach theresult presented in [30] (98 % on thePnodataset). Asa reason for this, we assume that the speciﬁc chromafeatures presented in that work (including a maskingmodel) provide additional beneﬁts.48 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20150.50.60.70.80.91.0\nCPCLPCRPHPCPEPCPNNLSPno Datset \n0.50.60.70.80.91.0\nCPCLPCRPHPCPEPCPNNLSSMD Dataset \n0.50.60.70.80.91.0\nCPCLPCRPHPCPEPCPNNLSSymph Dataset Final ChordProfile LearningTemplate MatchingSymmetry ModelFigure 4. Evaluation of different key detection algorithms. Here, we show the individual key recognition accuracies forthe three datasets of classical music. As the basic feature, we compare six types of chroma features.(3)Symmetry model. This algorithm worked best in con-junction with NNLS chroma. The optimal pitch set en-ergy threshold was found atfTR=0.12. The angularvector value came out best atwsym=0.53leading toa total performance of 82.6 %.(4)Final chord. The ﬁnal chord algorithm obtained opti-mal results on the basis of CP chroma features. Forthe parameters,N= 19ﬁnal frames, a root-scaleweight exponent ofs=0.9, an energy threshold offe=0.19 %, and the weight exponents templatem(2)have come out best (93.7 % accuracy).The overall results for the key detection evaluation areshown in Figure 4 for the individual datasets. All al-gorithms considerably depend on the chroma extractionmethod—especially when the data includes piano music(Pno,SMD). NNLS features often obtained the best re-sults and seem to be the most stable basis for key detec-tion methods. EPCP features are not a good choice forthis purpose. The proﬁle learning and the ﬁnal chord algo-rithms performed similarly. Hereby, the ﬁrst one is ratherdata-dependent whereas the ﬁnal chord algorithms requiresa ﬁne parameter tuning. In the following, we use the ﬁ-nal chord algorithm that showed a slightly better total rate(93.7 %) compared to the proﬁle training method (92.3 %).Finally, we test the four key detection methods on asubset of theCross-Eradataset (Section 3.1) using 1200tracks with key annotations. For each method, we use thefeature and parameter setting performing best in the previ-ous experiments.7We obtain a performance of83.9 %forthe template matching algorithm (1),87.1 %for the pro-ﬁle learning (2),80.4 %for the symmetry model (3), and85.4 %for the ﬁnal chord based method (4). Comparedto the optimization datasets, the performance is worse andthe differences between the methods are smaller. Proﬁlelearning and ﬁnal chord stay with best results. However,the learning strategy (2) seems to be more robust than theparameter-dependent ﬁnal chord algorithm (4).3.3 Classiﬁcation ExperimentsBy using the method and parameters performing best inSection 3.2, we now test the inﬂuence of key detectionon automatic style classiﬁcation based on theCross-Eradataset. We use a Random Forest (RF) classiﬁer. In or-der to avoid problems due to thecurse of dimensionality,7For the proﬁle learning approach, the proﬁles are also trained on thepreviously used datasetsSymph,SMD, andPno.Figure 5. Classiﬁcation accuracies for different typesof chroma features for classiﬁcation, four classes,key 1=key 2=ﬁnal chord. Bars and error marks indicatemean and standard deviation over 100 initializations of thecross-validation. Here, we do not use LDA (only twelve-dimensional features).we transform the feature space using Linear DiscriminantAnalysis (LDA) with three output dimensions. For eval-uation, we conduct a 3-fold cross-validation. We use thechroma histograms over the full piece as classiﬁcation fea-tures. As our basic idea, we rotate the chroma histogramsto the tonic note (Section 2.3). In the ideal setting, we usethe ground truth key annotations for the training data (key1). For the test data (key 2), we use the automatically de-tected key from the ﬁnal chord algorithm (see Section 3.2).Major and minor keys exhibit very different tonal struc-tures resulting in distinct typical chroma distributions. Themode-related properties in the chroma distribution mayheavily overlay the more subtle differences originatingfrom style. We therefore split up the data into major andminor pieces by using key annotations (training set,key 1)or automatic key detection (test set,key 2), respectively.On the resulting training data sets, we train separate classi-ﬁcation models for major and minor keys. The test data isthen classiﬁed into style periods using the appropriate clas-siﬁer model. This procedure is visualized in Figure 2. Wethen repeat the classiﬁcation by using the next fold as testdata. The whole cross-validation is performed 100 timeswith new random initialization of the folds.First, we test the inﬂuence of the speciﬁc chroma fea-ture implementation on the classiﬁcation performance. Inthis experiment, we use the automatic key (ﬁnal chord al-gorithm) for both training and test. The results are shownin Figure 5. Classiﬁcation performance considerably de-pends on the chroma type. Here, logarithmic compression(CLP)—enhancing weak components—does not improveclassiﬁcation performance. CP and EPCP features performProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 49Figure 7. Classiﬁcation accuracies for different combinations of chroma-based features, four classes, NNLS features. Thevarying dimensionality of the feature collections is reduced to three dimensions by using LDA.\nFigure 6. Classiﬁcation accuracies based on different keydetection methods (forkey 1andkey 2), three classes,NNLS features. Here, we do not use LDA transformation.Table 1. Classiﬁcation results for different key methodcombinations, three classes, NNLS features.Key 1 Key 2Major MinorGround truth Final chord70.1 % 66.7 %Final chord Final chord69.4 % 71.5 %similar whereas NNLS features outperform the others byseveral percentage points. We therefore use NNLS chromafeatures for the remaining experiments.Next, we evaluate the dependence of the key-relatedchroma features on the performance of the automatic keydetection. To this end, we once use each of the four meth-ods from Section 3.2 both for training and test data. Sincewe have no ground truth key annotations for the modernera, we just perform classiﬁcation of the remaining threeclasses (1200 pieces). Classiﬁcation results are similarwith all key methods (Figure 6). For proﬁle learning andﬁnal chord key detection, the results partly outperform theclassiﬁcation based on ground truth key annotations. Weconclude that some of the errors in key detection may havebeneﬁcial effects on classiﬁcation performance. Compar-ing the classiﬁcation results with the key detection per-fomance onCross-Era, we ﬁnd similar behaviour. Thus,a good key detection leads to better classiﬁcation, some-times outperforming the use of ground truth key annota-tions. When using ground truth key for training (key 1)and an automatic method for testing (key 2), performancevalues change but do not generally increase (Table 3.3).In the last study, we compare different types of clas-siﬁcation features (Figure 7). For thebaseline chromaexperiment, we do not use any key information but usethe original (absolute) NNLS histograms as classiﬁca-tion features—without Major / Minor discrimination (onemodel for all).Baseline Maj / Minmakes use of groundtruth key annotations for mode selection. This does notlead to increased classiﬁcation results. For thekey-relatedchromamethod, we use NNLS rotated with respect to theﬁnal chord key, for training and test.8The use of key de-tection boosts classiﬁcation results by almost 10 %. Next,we combine the key-related chroma histograms with otherchroma-based features such as tonalcomplexityortem-plate-based features (Section 2.3) leading to improvementsof almost 20 %. Combining all three types of features doesnot further increase classiﬁcation accuracies.When comparing our results with the outcome of [32,33], we do not obtain a general performance boost throughadding key-related chroma features. Both complexity [33]and template features [32] alone performed similar inthe respective experiments—compared to combining themwith our features. However, we already obtain remarkableresults with key-related features only. These features canbe computed with a high computational efﬁency.9As themain difference, complexity and template features capturelocal properties whereas global chroma histograms do not.4. CONCLUSIONWe evaluated four automatic key detection methods andoptimized their parameters using three datasets of classicalmusic. On a separate dataset, we performed style classi-ﬁcation experiments using key-related chroma histogramsas classiﬁcation features. With such features, the use ofan efﬁcient key detection algorithm improves classiﬁcationaccuracy. Thus, automatic key detection constitutes a use-ful step for such music classiﬁcation systems. However,involving local chroma-based features leads to a better per-formance than only using global chroma histograms.Acknowledgments:C. W. has been supported by theFoundation of German Business (Stiftung der DeutschenWirtschaft). He thanks Daniel G¨artner for fruitful discus-sions and Judith Wolff for contributing to key annotations.8The difference between this result and the NNLS performance in Fig-ure 5 is due to using LDA transformation here.9Since we only use global chroma, a very coarse time resolution forthe time-frequency transform could be applied.50 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20155. REFERENCES[1]Eric Backer and Peter van Kranenburg. On Musical Stylome-try: A Pattern Recognition Approach.Pattern Recognition Letters,26(3):299–309, 2005.[2]Ching-Hua Chuan and Elaine Chew. Polyphonic Audio Key FindingUsing the Spiral Array CEG Algorithm. InProceedings of the IEEEInternational Conference on Multimedia and Expo, 2005.[3]Roger B. Dannenberg, Belinda Thom, and David Watson. A MachineLearning Approach to Musical Style Recognition. InProceedings ofthe International Computer Music Conference (ICMC), 1997.[4]Ofer Dor and Yoram Reich. An Evaluation of Musical Score Charac-teristics for Automatic Classiﬁcation of Composers.Computer MusicJournal, 35(3):86–97, 2011.[5]Gabriel Gatzsche, Markus Mehnert, David Gatzsche, and KarlheinzBrandenburg. A Symmetry Based Approach for Musical TonalityAnalysis. InProceedings of the 8th International Society for MusicInformation Retrieval Conference (ISMIR), pages 207–210, 2007.[6]Iriving Godt. Style Periods of Music History Considered Analyti-cally.College Music Symposium, 24, 1984.[7]Emilia G´omez.Tonal Description of Music Audio Signals. PhD the-sis, Universitat Pompeu Fabra, Barcelona, 2006.[8]Aline Honingh and Rens Bod. Pitch Class Set Categories as AnalysisTools for Degrees of Tonality. InProceedings of the 11th Interna-tional Society for Music Information Retrieval Conference (ISMIR),pages 459–464, 2010.[9]Nanzhu Jiang, Peter Grosche, Verena Konz, and Meinard M¨uller. An-alyzing Chroma Feature Types for Automated Chord Recognition. InProceedings of the 42nd AES International Conference on SemanticAudio, pages 285–294, 2011.[10]Francis J. Kiernan. Score-based Style Recognition Using ArtiﬁcialNeural Networks. InProceedings of the 1st International Symposiumon Music Information Retrieval (ISMIR), 2000.[11]Peter van Kranenburg. Composer Attribution by Quantifying Com-positional Strategies. InProceedings of the 7th International Societyfor Music Information Retrieval Conference (ISMIR), 2006.[12]Carol L. Krumhansl.Cognitive Foundations of Musical Pitch. OxfordPsychology Series. Oxford University Press, 1990.[13]Kyogu Lee. Automatic Chord Recognition from Audio Using En-hanced Pitch Class Proﬁle. InProceedings of the International Com-puter Music Conference (ICMC), 2006.[14]Matthias Mauch and Simon Dixon. Approximate Note Transcriptionfor the Improved Identiﬁcation of Difﬁcult Chords. InProceedings ofthe 11th International Society for Music Information Retrieval Con-ference (ISMIR), pages 135–140, 2010.[15]Lesley Mearns and Simon Dixon. Characterisation of Composer StyleUsing High Level Musical Features. InProceedings of the 11th In-ternational Society for Music Information Retrieval Conference (IS-MIR), 2010.[16]Markus Mehnert, Gabriel Gatzsche, and Daniel Arndt. SymmetryModel Based Key Finding. InProceedings of the 126th AES Con-vention, 2009.[17]Meinard M¨uller and Sebastian Ewert. Towards Timbre-Invariant Au-dio Features for Harmony-Based Music.IEEE Transactions on Au-dio, Speech, and Language Processing, 18(3):649–662, 2010.[18]Meinard M¨uller and Sebastian Ewert. Chroma Toolbox: MATLABImplementations for Extracting Variants of Chroma-Based AudioFeatures. InProceedings of the 12th International Society for MusicInformation Retrieval Conference (ISMIR), pages 215–220, 2011.[19]Meinard M¨uller, Frank Kurth, and Michael Clausen. Chroma-BasedStatistical Audio Features for Audio Matching. InProceedings Work-shop on Applications of Signal Processing (WASPAA), pages 275–278, 2005.[20]Meinard M¨uller, Verena Konz, Wolfgang Bogler, and Vlora Ariﬁ-M¨uller. Saarland Music Data. InProceedings of the 12th Interna-tional Society for Music Information Retrieval Conference (ISMIR),2011.[21]Meinard M¨uller, Frank Kurth, and Michael Clausen. Audio Matchingvia Chroma-Based Statistical Features. InProceedings of the 6th In-ternational Society for Music Information Retrieval Conference (IS-MIR), pages 288–295, 2005.[22]Mitsunori Ogihara and Tao Li. N-Gram Chord Proﬁles for ComposerStyle Identiﬁcation. InProceedings of the 9th International Societyfor Music Information Retrieval Conference (ISMIR), 2008.[23]Steffen Pauws. Musical key extraction from audio. InProceedings ofthe 5th International Society for Music Information Retrieval Confer-ence (ISMIR), 2004.[24]Carlos P´erez-Sancho, D. Rizo, Jos´e Manuel I˜nesta, Pedro Jos´ePonce de Le´on, S. Kersten, and Rafael Ramirez. Genre Classiﬁcationof Music by Tonal Harmony.Intelligent Data Analysis, 14(5):533–545, 2010.[25]Pedro Jos´e Ponce de Le´on and Jos´e Manuel I˜nesta. A Pattern Recog-nition Approach For Music Style Identiﬁcation Using Shallow Statis-tical Descriptors.IEEE Transactions on System, Man and Cybernet-ics - Part C : Applications and Reviews, 37(2):248–257, 2007.[26]Christian Simmermacher, Da Deng, and Stephen Craneﬁeld. Fea-ture Analysis and Classiﬁcation of Classical Musical Instruments:An Empirical Study. InAdvances in Data Mining. Applications inMedicine, Web Mining, Marketing, Image and Signal Mining, vol-ume 4065 ofLecture Notes in Computer Science, pages 444–458.Springer, Berlin and Heidelberg, 2006.[27]Michael Stein, B. M. Schubert, Matthias Gruhne, Gabriel Gatzsche,and Markus Mehnert. Evaluation and Comparison of Audio ChromaFeature Extraction Methods. InProceedings of the 126th AES Con-vention, 2009.[28]David Temperley.The Cognition of Basic Musical Structures. MITPress, 2001.[29]George Tzanetakis and Perry Cook. Musical Genre Classiﬁcation ofAudio Signals.IEEE Transactions on Speech and Audio Processing,10(5):293–302, 2002.[30]Steven van de Par, Martin F. McKinney, and Andr´e Redert. MusicalKey Extraction From Audio Using Proﬁle Training. InProceedingsof the 7th International Society for Music Information Retrieval Con-ference (ISMIR), 2006.[31]Christof Weiß. Global Key Extraction from Classical Music AudioRecordings Based on the Final Chord. InProceedings of the 10thSound and Music Computing Conference (SMC), 2013.[32]Christof Weiß, Matthias Mauch, and Simon Dixon. Timbre-InvariantAudio Features for Style Analysis of Classical Music. InProceedingsof the Joint Conference 40th ICMC and 11th SMC, 2014.[33]Christof Weiß and Meinard M¨uller. Tonal Complexity Features forStyle Classiﬁcation of Classical Music. InProceedings of the IEEEInternational Conference on Acoustics, Speech, and Signal Process-ing (ICASSP), 2015.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 51"
    },
    {
        "title": "Comparative Music Similarity Modelling Using Transfer Learning Across User Groups.",
        "author": [
            "Daniel Wolff",
            "Andrew MacFarlane 0001",
            "Tillman Weyde"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417835",
        "url": "https://doi.org/10.5281/zenodo.1417835",
        "ee": "https://zenodo.org/records/1417835/files/WolffMW15.pdf",
        "abstract": "We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-The- oretic Metric Learning (RITML) algorithm adapts a Maha- lanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use in- formation from larger datasets to build better models for more specific datasets, such as user groups from differ- ent cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are rele- vant for the specific models when compared to the general user data. We to this end introduce the new CASimIR dataset, the first openly available relative similarity dataset with user attributes. With two age-related subsets, we show that trans- fer learning with RITML leads to better age-specific mod- els. RITML here improves learning on small datasets. Us- ing the larger MagnaTagATune dataset, we show that RITML performs as well as state-of-the-art algorithms in terms of general similarity estimation.",
        "zenodo_id": 1417835,
        "dblp_key": "conf/ismir/WolffMW15",
        "keywords": [
            "transfer learning",
            "music similarity models",
            "relative user data",
            "Maha-lobian distance",
            "ITML algorithm",
            "information-theoretic metric learning",
            "template model",
            "regularization",
            "CASimIR dataset",
            "age-related subsets"
        ],
        "content": "COMPARATIVE MUSIC SIMILARITY MODELLING USING TRANSFERLEARNING ACROSS USER GROUPSDaniel Wolff, Andrew MacFarlane and Tillman WeydeMusic Informatics Research Group – Department of Computer ScienceCity University London{daniel.wolff.1, a.macfarlane-1, t.e.weyde}@city.ac.ukABSTRACTWe introduce a new application of transfer learning fortraining and comparing music similarity models based onrelative user data: The proposed Relative Information-The-oretic Metric Learning (RITML) algorithm adapts a Maha-lanobis distance using an iterative application of the ITMLalgorithm, thereby extending it to relative similarity data.RITML supports transfer learning by training models withrespect to a given template model that can provide priorinformation for regularisation. With this feature we use in-formation from larger datasets to build better models formore speciﬁc datasets, such as user groups from differ-ent cultures or of different age. We then evaluate whatmodel parameters, in this case acoustic features, are rele-vant for the speciﬁc models when compared to the generaluser data.We to this end introduce the new CASimIR dataset, theﬁrst openly available relative similarity dataset with userattributes. With two age-related subsets, we show that trans-fer learning with RITML leads to better age-speciﬁc mod-els. RITML here improves learning on small datasets. Us-ing the larger MagnaTagATune dataset, we show that RITMLperforms as well as state-of-the-art algorithms in terms ofgeneral similarity estimation.1. INTRODUCTIONMusic similarity models are a central part of many ap-plications in music research, particularly Music Informa-tion Retrieval (MIR). When training similarity models, itturns out that learnt models vary considerably for differ-ent data sets and application scenarios. Recently, context-sensitive models have been introduced, e.g. for the task ofmusic recommendation (Stober [9] provides an overview).The main problem with context-sensitive similarity mod-els is currently to obtain enough data to train the modelsfor each context. Transfer learning promises to enable ef-fective training of models for speciﬁc contexts by includ-ing information from related datasets. We here present anc\u0000Daniel Wolff, Andrew MacFarlane and Tillman Weyde.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Daniel Wolff, Andrew MacFarlaneand Tillman Weyde. “Comparative Music Similarity Modelling usingTransfer Learning Across User Groups”, 16th International Society forMusic Information Retrieval Conference, 2015.approach of transfer learning in music similarity that im-proves results of specialised models, using ourW0-RITMLextension of Information-Theoretic Metric Learning (ITML).The template-based optimisation inW0-RITML allows fora comparison of the general and specialised models – itderives the latter from the former – which we suggest asa tool for comparative analysis of similarity data by (e.g.cultural) provenance.We are particularly interested in modelling relative similar-ity ratings collected from participants during Games With aPurpose (GWAPs). Using similarity data from user groupspromises to provide tailored model performance and theopportunity to compare such groups via the trained sim-ilarity models. The new CASimIR dataset presented inSection3contains such similarity ratings and informationabout the contributing subjects. We use this extra data togroup users and here exemplarily train age-speciﬁc musicsimilarity models based on age-bounded subsets. How-ever, the relatively small size of the CASimIR dataset re-quires a different approach to training the group-speciﬁcmodels as existing algorithms are not sufﬁciently effectivefor this purpose.We contribute a solution to this problem with a novel genericalgorithm for transfer learning with similarity models: TheRITML algorithm (see Section5.2) extends on ITML toallow for learning a Mahalanobis metric from relative sim-ilarity data like in CASimIR. WithW0-RITML, informa-tion learnt from remaining data can be successfully trans-ferred to an age-bounded dataset via a Mahalanobis ma-trix. This transfer-learning increases performance on smalldatasets and provides interpretable values in the Mahala-nobis matrix. The Mahalanobis matrix provides a compactrepresentation of similarity information in a dataset. Thisis useful in scenarios where the music data is difﬁcult toaccess due to its data volume or copyright restrictions. TheCASimIR dataset and code used in this paper are availableonline1.2. RESEARCH BACKGROUNDTransfer learning relates to many areas and approaches inmachine learning. A general overview of transfer learningis given in Pan and Yang [6]. In their categorisation, ourtask is an inductive knowledge transfer from one similaritymodelling task to another via model parameters. Note that1http://mirg.city.ac.uk/datasets/ismir2015dw24in our example the tasks differ only in the dataset, but ourmethod can also be used for more divergent tasks.In MIR, transfer learning is a relatively new method. In2013, [2] described multi-task learning using a shared la-tent representation for auto-tagging, genre classiﬁcation andgenre-based music similarity. This representation includesboth the features and the labels for the different tasks. Inexperiments on several datasets they showed improvementof classiﬁcation accuracy and modelling similarity accord-ing to genre.We here work with relative similarity ratings from humansin our new CASimIR dataset for group-speciﬁc modelling.Furthermore, we use the MagnaTagATune dataset [3] forcomparison on non-speciﬁc similarity learning. Here, theSupport Vector Machine (SVM) approach developed bySchultz and Joachims [7] and applied in [10,11] is usedas state-of-the art baseline.Another state-of-the-art algorithm for learning from rela-tive similarity data is Metric Learning To Rank (MLR).McFee et al. [4] introduce MLR for parametrising a lin-ear combination of content-based features using collabo-rative ﬁltering data. Their post-training analysis of featureweights revealed that tags relating to genre or radio stationswere assigned greater weights than those related to musictheoretical terms.3. A DATASET FOR USER-AWARE SIMILARITYIn order to perform a related analysis and comparisons ofmodels between different user groups, we have collectedthe CASimIR datasets using Spot the Odd Song Out [13],an online2multi-player Game With a Purpose (GWAP).The similarity module of the Spot the Odd Song Out gamecollects relative similarity data using an odd-one-out sur-vey: From a set of three music clips, participants are askedto choose the clip most dissimilar to the remaining clips,i.e. theodd song out. The game motivates players byrewarding blind agreement. For various reasons, includ-ing personal data protection, little music annotation data ispublicly available with information about the provider ofthe data and their context.Although the game can collect anonymised personal in-formation including gender, nationality, spoken languagesand musical experience, the amount and type informationavailable varies between participants, as data provision isvoluntary. Our overarching goal is to study the relationbetween similarity and culture and we thus link annota-tions to cultural proﬁles rather than indexing speciﬁc par-ticipants. With this paper we publish the ﬁrst set of simi-larity data with anonymised proﬁles.3.1 Constraints from Relative Similarity RatingsThe MagnaTagATune and CASimIR datasets both containrelative similarity ratings. A participant’s rating ofCkas2http://mirg.city.ac.uk/camir/game/the odd one out (of the tripletCi,Cj,Ck) results in 2 rela-tive similarity constraints: clipsCiandCjare more similarthanCiandCk, and clipsCjandCiare more similar thanCjandCk. These constraints are denoted as(i, j, k)and(j, i, k), respectively which are contained in the constraintsetˆQ.Human ratings regularly produce inconsistent constraints.We use the graph representation of the similarity data assuggested by [5] to analyse and ﬁlter inconsistencies: Eachconstraint(i, j, k)is represented by an edge connectingtwo vertices(i, j)↵ijk!(i, k)corresponding to two clippairs, with the edge weight↵ijk=1. When combiningall constraints in a graph, the weights↵ijkare accumu-lated. Inconsistencies then appear as cycles in the graph,which in their most common form are of length 2:(i, j)↵ijk\u0000↵ikj(i, k).We remedy such cycles by removing the edge with thesmaller weight and assigning the weight|↵ijk\u0000↵ikj|tothe remaining edge. For both the MagnaTagATune andCASimIR datasets this already creates a cycle-free graphQas no larger cycles remain. The cycle-free setsQareused in this study for training and evaluation.Compared to the MagnaTagATune dataset, the CASimIRdataset features more frequent recurrences of clips betweenthe triplets presented to the users. Recurring clips relate thecorresponding similarity data, and result in large connectedcomponents in the CASimIR similarity graph: While themaximal number of clips directly or transitively relatedto each other through similarity data in the MagnaTagA-Tune dataset was 3 (see [11]), most clips in the CASimIRsimilarity data are related to at least 5 other clips. Therepetition of clips across triplets results in fewer uniquereferenced clips: the current CASimIR similarity datasetcontains only 180 clips referenced by 2102 ratings, whileMagnaTagATune references 2000 ratings with about 500clips, and has 1019 clips with 7650 ratings in total.3.2 Analysis of Age-bounded Similarity RatingsThe additional participant attributes allow us to select sub-sets of similarity data according to speciﬁc proﬁles of theparticipants. This enables the training of more speciﬁcmodels that support better similarity predictions for the rel-evant group of users, and allows for comparison of differ-ent models.As an example of group-based similarity modelling wechoose age as a separating criterion on the CASimIR simi-larity data from over 256 participants: We divide the com-plete set of similarity ratingsRinto twoage-boundedsub-setsR25of data provided by participants not older than25 years andR>25containing data of older participants.The boundary of 25 years was chosen as the best approxi-mation to equal sizes of the subsets (data input is only in 5year bands). As shown in Table1, the number of ratings ishigher for theR25dataset.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 25RR25R>25R{(25)R{(>25)ratings210291964411831458constr.914723576732809clips180171163175176Table 1. Number of votes, unique constraints and refer-enced clips, after ﬁltering inconsistencies, per dataset.539 similarity ratings are not associated to a valid age andstored separately inR;. For the two age-bounded datasets,we furthermore deﬁne complementary datasetsR{(25)andR{(>25)combining the remaining similarity data, e.g.R{(25)=R>25[R;. These complementary sets will be used fortraining of template models for transfer learning.After splitting, the above (sub)sets of ratings are trans-ferred into constraints (see Section3.1) and separately ﬁl-tered for inconsistencies. We now use the correspondingsets of unique constraintsQ,Q25,Q>25,Q{(25)andQ{(>25)for training and testing of models. The number ofconstraints are also noted in Table1, together with the totalnumber of clips referenced by the constraint sets. Due tomultiple ratings referring to the same constraint and ﬁlter-ing the constraint count is lower than the number of ratings.4. SIMILARITY MODELLINGThe computational representations of music through fea-tures, related to physical, musical, and cultural attributesdetermine the basis of similarity models. Both the Magna-TagATune and CASimIR datasets contain pre-computedfeatures created by The Echo Nest API. For our exper-iments with CASimIR we derive acoustic features fromthis data which are aggregated to the clip-level. The 41-dimensional features contain 12 chroma and 12 timbre fea-tures, both aggregated via averaging, 2 weight vectors andfurther features after [8,11]:chroma timbresegmentDurationMean temposegmentDurationVariance beatVariancetimeLoudnessMaxMean tatumloudness tatumConﬁdenceloudnessMaxMean numTatumsPerBeatloudnessMaxVariance timeSignatureloudnessBeginMean timeSignatureStabilityloudnessBeginVariance –Table 2. Features used in our experiments.For experiments with the MagnaTagATune dataset we willuse the similar features provided in [12] which contain pre-processed tag information in addition to the acoustic fea-tures described above. For the CASimIR dataset, using un-processed tags from Last.fm did not increase performancein earlier experiments due to very sparse tag assignments.Therefore, our experiments on CASimIR use acoustic fea-tures only. For a clipCi, we refer to its feature vector asxi2RN.4.1 Mahalanobis DistancesWe use the inverse of the distance of two feature vectors asthe similarity of the two corresponding clips. The mathe-matical form of the Mahalanobis distance is used to spec-ify a parametrised distance measure. Given two featurevectorsxi,xj2RN, the distance can be expressed asdW(xi,xj)=q(xi\u0000xj)|W(xi\u0000xj),whereW2RN⇥Nis a square matrix parametrising thedistance function: theMahalanobis matrix.dWqualiﬁesas a metric ifWis positive deﬁnite and symmetric.5. MODEL TRAINING WITH RITMLWe now discuss our algorithm which can adapt Mahala-nobis distances in order to ﬁt relative similarity data. It isbased on the ITML algorithm as described below, whichcannot be used directly with relative similarity data. In-stead, ITML requires upper or lower bounds on the sim-ilarity of two clips, e.g.dW(xi,xj)<mi,jfor similarclips. In Section5.2we will iteratively derive such con-straints during the RITML optimisation process.5.1 Information-Theoretic Metric LearningDavis et al. [1] describe Information-Theoretic Metric Lear-ning (ITML) for learning a Mahalanobis distance from ab-solute distance constraints (e.g. requiringdW(xi,xj)<0.5). A particularly interesting feature of ITML is thata template Mahalanobis matrixW02Rn⇥ncan be pro-vided for regularisation. ThisW0can be from a metric thatis predeﬁned or learnt on a different dataset. IfW0is notspeciﬁed, the identity transform is used. The regularisationof ITML exploits an interpretation of Mahalanobis matri-ces as multivariate Gaussian distributions: The distancebetween two Mahalanobis distance functions parametrisedbyWandW0is measured by the relative entropy of thecorresponding distributions, which in [1] uses the LogDetdivergenceDld:Dld(W, W0)=tr(WW\u000010)\u0000log det(WW\u000010)\u0000n=2⇤KL(P(xi;W0)kP(xi;W)).KL refers to the Kullback-Leibler divergence. For detailsof the transformation see [1]. Given the constraints in formof similar (Rs) and dissimilar (Rd) clip indices as well asupper and lower boundsuij,lij, the optimisation problemis then posed as follows:ITML(W, ⇠, c, Rs,Rd)=argminW⌫0,⇠Dld(W, W0)+c·Dld(diag(⇠),diag(⇠0))s.t. tr(WdLi,j(dLi,j)|)⇠ij8(i, j)2Rstr(WdLi,j(dLi,j)|)\u0000⇠ij8(i, j)2RdwithdLi,j=(xi\u0000xj).26 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Here,⇠ijare slack variables enabling and controlling theviolation of individual constraints. The⇠ijare initialised togiven upper boundsuij, if(i, j)2Rsor lower boundslij,if(i, j)2Rd. During optimisation, they are regularised bycomparison to the template slack⇠0using triangular matri-ces diag(⇠)and diag(⇠0).5.2 Relative Learning with RITMLIn order to allow for training with relative similarity con-straints, we present Relative Information-Theoretic MetricLearning (RITML) based on ITML. Motivated by [14], weembed ITML into an iterative adaptation of the upper andlower bounds.We start with a training set of relative constraints(i, j, k)2Qt. We require standard ITML parameters such asc, aswell as the relative learning parameters including shrink-age factor⌘, margin⌧and number of cycleskat the be-ginning. We use the identity matrix for the templateW0.During iterationm, the active training set of violated con-straintsQmis calculated asQm={(i, j, k)2Qt|dWm(xi,xj)>dWm(xi,xk)}.Qmis then further divided into the sets of similar and dis-similar constraintsRmsandRmd:Rms={(i, j)|(i, j, k)2Qm}Rmd={(i, k)|(i, j, k)2Qm},Afterwards, absolute distance constraints⇠ijfor the fol-lowing ITML instance are acquired by adding a margin⌧to the average distance valuesµ=dWm(xi,xj)+dWm(xi,xk)2of the clip pairs:⇠mij=(µ\u0000⌧(i, j)2Rmsµ+⌧(i, j)2Rmd8(i, j, k)2QmNow, with⇠mcontaining the upper and lower bounds,\u0000Wcan be calculated using\u0000W=ITML(Wm,⇠m,\u0000,Rms,Rmd)(1)and the ﬁnal Mahalanobis matrix is accumulated over iter-ations using the model update functionWm+1=m⇤Wm+⌘⇤\u0000Wm+1.In order for the algorithm to converge, the cardinality of theactive training set|Qm|needs to decrease. In our experi-ments,k= 200training iterations are usually sufﬁcient.Otherwise an early stopping of the algorithm takes place if|Qm|does not decrease for50iterations. In this case theWmfor the smallest|Qm|within the last 50 iterations isreturned. RITML does not guaranteedWto be a metric.Algorithm 1:Relative Training with RITMLData:ConstraintsQt, featuresxi, template matrixW0,regularisation factorc, shrinkage factor⌘, margin⌧, number of cycleskm=0;whilemk^Q⇤6=;doUpdate training setsQm,RmsandRmd;Update absolute constraints⇠m;Calculate parameter change\u0000W;CalculateWm+1;m = m+1 ;endreturnMahalanobis matrixWk5.3 Transfer Learning withW0-RITMLThe property that motivates our usage of RITML is thatit enablestransfer learning: If a speciﬁc starting value ortemplate ofW0other than the identity matrix is provided,the optimisation tends to produce results close to the pro-videdW0. In order to sustain this effect for large numbersof iterations we modify Equation (1) such that regularisa-tion is ﬁxed towardsW0instead of the Euclidean distance:\u0000W=ITML(W0,⇠m,\u0000,Rms,Rmd)This constitutes theW0-RITML algorithm for transfer learn-ing with Mahalanobis matrices.6. EXPERIMENTSFor all our experiments we use the 10-fold cross-validationwithinductive samplingas described in [11]: Instead of di-viding the similarity constraints themselves into test/trainingsets, the data are divided on the basis of connected clustersin the similarity data. This approach prevents the recur-rence of clips from a training-set in the corresponding testset. It also leads to a greater variance in test-set sizes forCASimIR where the clusters of connected similarity dataare larger.We evaluate the algorithms’ performance based on the per-centage of training and test constraints fulﬁlled by the trainedmodel. Our main focus is on the test-set results as we areinterested how well the learnt models generalise to unseendata. As a baseline we use the Euclidean distance on thefeatures. We have tested results for statistical signiﬁcanceusing the Wilcoxon signed rank test on cross-validationfolds’ results with a threshold ofp<5%.Both SVM as implemented insvmlight[7] and RITML havehyper-parameters affecting the performance on differentdatasets. The results reported here were selected on thebasis of best test-set performances after a grid-search overa range of value combinations identiﬁed as reasonable inpreliminary experiments: The regularisation trade-offcis aparameter common to SVM, RITML andW0-RITML witha similar effective range: we explored ac2[0.001,10]us-ing an approximately logarithmic scale. For RITML andProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 27W0-RITML we additionally used⌧2{10\u00004,10\u00003...,10\u00001,0.5,1...10}and⌘2{0.1,0.15...0.95}.6.1 Comparing the Performance of RITMLFor a comparable evaluation of RITML we chose the Magna-TagATune-based dataset and constraint sampling publishedin [12]. Their evaluation compares various algorithms forlearning a Mahalanobis metric using two different sam-plings. The inductive sampling used here corresponds tothesampling Bin their text. Table3shows the results onMagnaTagATune and on the complete CASimIR dataset(Q).AlgorithmMagnaTagATuneCASimIREuclidean59.80 / 59.7759.75 / 59.82RITML71.12 / 73.4164.23/93.36SVM71.20/ 85.7563.22 / 69.11MLR68.90 /100.062.79 / 73.37Table 3. Comparison of Test / Training set performance onthe MagnaTagATune and CASimIR datasets for baseline,RITML and SVM. Reported are the number of constraintsfulﬁlled by the learnt distance measures.For MagnaTagATune, RITML achieves similar generalisa-tion results as SVM (with parameters SVM:c=0.7andRITML:c=1,⌘=0.85,⌧=0.5), while MLR over-ﬁts to the training data. For both the MagnaTagATuneand CASimIR datasets all methods perform signiﬁcantlybetter than the baseline. The RITML results are thereforecomparable to the state-of-the-art. The training results onMagnaTagATune with SVM and MLR are far better thanthe test results, indicating overﬁtting, which does not oc-cur for RITML. Interestingly, on the CASimIR dataset, thesituation between RITML and SVM is reversed. Resultspublished by [11] for acoustic-only features on MagnaTag-ATune show a performance of 66% on MagnaTagATune,but the lower performance on CASimIR can be explainedby the smaller number of training examples.6.2 Transfer LearningA core motivation for transfer learning is the training onhighly specialised but small datasets. To evaluate theW0-RITML method for transfer learning, we ﬁrstly comparedthe SVM and RITML algorithms with the baseline on theage-bounded datasetsQ>25andQ25in Table4. Therightmost column shows the average performance acrossboth age-bounded datasets. Expectedly, on these smallerdatasets generalisation results for RITML as well as thereference SVM and MLR are lower than on the whole CA-SimIR. Only for RITML an increase of 4.37% from thebaseline is notable for the slightly largerQ>25which im-proves the average score for RITML.We now apply transfer learning to improve generalisationresults on the age-bounded sets. The overall process is de-picted in Figure1. First, an similarity modelling experi-ment is performed on both of the complementary subsetsAlgorithmQ25Q>25AverageEuclidean59.32 / 59.9559.15 / 59.6359.23 / 59.79RITML63.69 /75.8761.02 / 67.9562.35 / 71.91SVM61.56 / 72.7861.34 / 71.4361.45 / 72.10MLR62.06 / 75.7962.58 /78.4762.32 /77.13W0-Direct63.96 / 66.1764.82 / 69.5764.39 / 67.87W0-RITML65.53/ 70.8267.07/ 73.2266.30/ 72.02Table 4. Comparison of Test / Training set performanceon the age-bounded datasets. Training on single datasets(top 3 rows) and transfer learning withW0-RITML andW0-Direct.ComplementDatasetQ{(>25)TemplatemodelW0Age-speciﬁcmodelWRITMLW0-RITMLAge-boundedDatasetQ>25Figure 1. Flow diagram for transfer learning, exempliﬁedfor theQ>25dataset.Q{(>25)andQ{(25)using cross-validation with trainingand test data from only these sets. Comparing the indi-vidual results for validation folds we choose the Maha-lanobis matrices with the greatest test-set performance astemplate matrixW0. The template matrixW0learnt onQ{(25)is then used for transfer learning onQ25, us-ingW0-RITML. For comparison of the effectiveness of theﬁne-tuning withW0-RITML, we report the performanceachieved with the unmodiﬁedW0onQ25asW0-Direct.This process is repeated analogously forQ25by applyingthe template matrixW0fromQ{(25)onQ25.The highlighted lower columns of Table4show the resultsfor transfer learning: RowW0-Direct reports the directperformances of the template Mahalanobis matricesW0.The results of ﬁne-tuning these models withW0-RITMLare reported in the last row. We here ﬁnd that using the ma-trices trained on the larger datasets, and thus transfer learn-ing, generally improves results. Only the results forW0-RITML provide gains>6.21%that are statistically signif-icant when compared to the baseline. As the average resultofW0-RITML also signiﬁcantly outperforms the averageSVM performance,W0-RITML works best for adaptingmodels to specialised datasets.A drawback of RITML is that it is computationally de-manding: For theQdataset, RITML uses 50 seconds whereSVM converges in 5 seconds. On the other hand, SVMlearns a diagonalWwhich reduces the number of param-eters and model ﬂexibility.6.3 Model ComparisonIn order to identify speciﬁcities of theQ>25dataset incomparison to the remainingQ{(>25), we now analyse changesmade to the template matrixW0in the ﬁne-tuning pro-cess. Instead of starting from the Euclidean metric, modelslearnt from theW0-RITML method have a model already28 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015adapted to similarity data as basis.Figure2shows the relative differenceˆW\u0000ˆW0of the Ma-halanobis matrix before (W0) and after (W) ﬁne tuning.As the ﬁne tuning process rescales the similarity measureand therebyW, the matrices have been normalised to theinterval of[0,1]via3ˆW=W\u0000mini,j(wij)maxi,j(W\u0000mini,jwij)ij.(2)The axes of the ﬁgure correspond to feature types, whichfor better overview have been grouped into chroma, timbreand ranges of the features in Table2. The template matrixW0in Figure3ahas large values only in the diagonal andhomogeneous small values off the diagonal. In comparisonto this, Figure2shows that speciﬁc combinations of tim-bre features (in the bottom centre) with (B)eat and tempostatistics were raised in importance byW0-RITML, result-ing in the ﬁnal matrixWas shown in Figure3b. Also,the centre of the matrix shows increased values for combi-nations of different timbre coefﬁcients. The strongest in-creases (20-24%) in weights are reported for the off-dia-gonal ﬁelds ofC11C1,T6T5,B4T4andB4T5, whereC, Trelate to chroma and timbre coefﬁcients andB4refers tothe tatumConﬁdence feature. Weights are increased mainlyat the cost of diagonal elements, and suggest at a speciali-sation of the model to the speciﬁcities of theQ>25similar-ity subset. For this data collected from users aged over 25,the analysedW0-RITML model with stronger inﬂuence ofthe timbre and beat-statistics features performs best in ourevaluation.\nC············T············S·L·····B······C············T············S·L·····B······Figure 2. Learnt model difference forW0-RITML onQ>25. Axis labels represent ranges of feature types:(C)hroma, (T)imbre, as well as (S)egment, (L)oudness and(B)eat+Tempo statistics. Dark red / blue colours corre-spond to strong weight increase / decrease.3Subtraction and division are applied toWin a point-wise manner.\nC············T············S·L·····B······C············T············S·L·····B······(a)\nC············T············S·L·····B······C············T············S·L·····B······(b)Figure 3.(a)Template matrixW0before and(b)ﬁnal ma-trixWafter ﬁne-tuning withW0-RITML onQ>25. Thelatter shows higher variance in off-diagonal entries for thespecialised model. Axis labels represent ranges of fea-ture types: (C)hroma, (T)imbre, as well as (S)egment,(L)oudness and (B)eat+Tempo statistics. Dark red colourscorrespond to strong weight increase, light yellow to de-crease.7. CONCLUSION & FUTURE WORKWe presented a method for analysing music similarity dataof different user groups via models trained with transferlearning. To this end, the new RITML algorithm was de-veloped extending ITML to relative similarity data. A keyfeature of RITML is that it enables transfer learning withtemplate Mahalanobis matrices viaW0-RITML. Our eval-uation of the algorithm was performed on two datasets:The evaluation on the commonly used MagnaTagATunedataset showed that RITML performs comparably to state-of-the-art algorithms for metric learning.For evaluation of transfer learning withW0-RITML weprovide the CASimIR similarity dataset, the ﬁrst open datasetcontaining user attributes associated to relative similaritydata. Tests on the whole CASimIR dataset corroboratedour ﬁnding that RITML competes with current similaritylearning methods. Our analysis ofW0-RITML was per-formed on age-bounded subsets of the dataset. Resultsshowed that transfer learning withW0-RITML outperformsthe standard SVM algorithm on small datasets.Our comparison of models allowed us to point out speciﬁcfeatures and combinations that determine similarity in userdata. For this ﬁrst evaluation we chose age to group users.We hope this will motivate further research in comparisonof similarity models and adaptation to data with regard tocultural and user context.For future work we are interested in collecting larger sim-ilarity datasets, and applying the methods introduced herefor improved validation of results and the analysis of morespeciﬁc user groups. The set-up used for our experimentsmotivates transfer learning across the MagnaTagATune andCASimIR datasets withW0-RITML for further analysis ofthe transferability of similarity information via Mahalano-bis matrices.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 298. REFERENCES[1]Jason V . Davis, B. Kulis, Prateek Jain, Suvrit Sra,and Inderjit S. Dhillon. Information-theoretic met-ric learning. InProc. of ICML ’07, pages 209–216,New York, NY , USA, 2007. ACM.[2]Philippe Hamel, Matthew E. P. Davies, KazuyoshiYoshii, and Masataka Goto. Transfer learning in mir:Sharing learned latent representations for music au-dio classiﬁcation and similarity. In Alceu de SouzaBritto Jr., Fabien Gouyon, and Simon Dixon, editors,ISMIR, pages 9–14, 2013.[3]Edith Law and Luis V on Ahn. Input-agreement:A new mechanism for collecting data using humancomputation games. InProc. of CHI. ACM Press,2009.[4]B. McFee, L. Barrington, and G. Lanckriet. Learn-ing similarity from collaborative ﬁlters. InProc. ofISMIR 2010, pages 345–350, 2010.[5]Brian McFee and Gert R. G. Lanckriet. Partial or-der embedding with multiple kernels. InProc. of the26th International Conference on Machine Learning(ICML’09), pages 721–728, June 2009.[6]Sinno Jialin Pan and Qiang Yang. A survey on trans-fer learning.IEEE Transactions on Knowledge andData Engineering, 22(10):1345–1359, October 2010.[7]M. Schultz and T. Joachims. Learning a distance met-ric from relative comparisons. InAdvances in NeuralInformation Processing Systems (NIPS). MIT Press,2003.[8]Malcolm Slaney, Kilian Q. Weinberger, and WilliamWhite. Learning a metric for music similarity. InJuan Pablo Bello, Elaine Chew, and Douglas Turn-bull, editors,Proc. of ISMIR 2008, pages 313–318,2008.[9]Sebastian Stober.Adaptive Methods for User-Centered Organization of Music Collections. PhDthesis, Otto-von-Guericke-University, Magdeburg,Germany, Nov 2011. published by Dr. Hut Verlag,ISBN 978-3-8439-0229-8.[10]Sebastian Stober and Andreas N¨urnberger. Similar-ity adaptation in an exploratory retrieval scenario. InProc. of AMR 2010, Linz, Austria, Aug 2010.[11]Daniel Wolff and Tillman Weyde. Learning musicsimilarity from relative user ratings.Information Re-trieval, pages 1–28, 2013.[12]Daniel Wolff, Sebastian Stober, Andreas N¨urnberger,and Tillman Weyde. A systematic comparison of mu-sic similarity adaptation approaches. InProc. of IS-MIR 2012, pages 103–108, 2012.[13]Daniel Wolff, Guillaume Bellec, Anders Friberg, An-drew MacFarlane, and Tillman Weyde. Creating au-dio based experiments as social web games with thecasimir framework. InProc. of AES 53rd Interna-tional Conference: Semantic Audio, Jan 2014.[14]Zhaohui Zheng, Keke Chen, Gordon Sun, andHongyuan Zha. A regression framework for learn-ing ranking functions using relative relevance judg-ments. InProc. of SIGIR ’07, pages 287–294, NewYork, NY , USA, 2007. ACM.30 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Drum Transcription Using Partially Fixed Non-Negative Matrix Factorization with Template Adaptation.",
        "author": [
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417839",
        "url": "https://doi.org/10.5281/zenodo.1417839",
        "ee": "https://zenodo.org/records/1417839/files/WuL15.pdf",
        "abstract": "In this paper, a template adaptive drum transcription algo- rithm using partially fixed Non-negative Matrix Factoriza- tion (NMF) is presented. The proposed method detects per- cussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undefined entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adap- tation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems.",
        "zenodo_id": 1417839,
        "dblp_key": "conf/ismir/WuL15",
        "keywords": [
            "template adaptive",
            "drum transcription",
            "Non-negative Matrix Factorization",
            "percussive events",
            "complex mixtures",
            "minimal training set",
            "perceptrons",
            "harmonic dictionary",
            "standard NMF procedure",
            "percussive dictionary"
        ],
        "content": "DRUM TRANSCRIPTION USING PARTIALLY FIXED NON-NEGATIVEMATRIX FACTORIZATION WITH TEMPLATE ADAPTATIONChih-Wei Wu, Alexander LerchGeorgia Institute of Technology, Center for Music Technology{cwu307, alexander.lerch}@gatech.eduABSTRACTIn this paper, a template adaptive drum transcription algo-rithm using partially ﬁxed Non-negative Matrix Factoriza-tion (NMF) is presented. The proposed method detects per-cussive events in complex mixtures of music with a minimaltraining set. The algorithm decomposes the music signalinto two dictionaries: a percussive dictionary initializedwith pre-deﬁned drum templates and a harmonic dictionaryinitialized with undeﬁned entries. The harmonic dictionaryis adapted to the non-percussive music content in a standardNMF procedure. The percussive dictionary is adapted toeach individual signal in an iterative scheme: it is ﬁxedduring the decomposition process, and is updated based onthe result of the previous convergence. Two template adap-tation methods are proposed to provide more ﬂexibility androbustness in the case of unknown data. The performanceof the proposed system has been evaluated and comparedto state of the art systems. The results show that templateadaptation improves the transcription performance, and thedetection accuracy is in the same range as more complexsystems.1. INTRODUCTIONBeing one of the most intensively researched areas in MusicInformation Retrieval (MIR), automatic music transcrip-tion is often considered the core technology that wouldenable high-level representations of music signals with thepotential of improving virtually any MIR system. A com-plete transcription system comprises many sub-tasks suchas multi-pitch detection, onset detection, instrument recog-nition, and rhythm extraction [2]. While the main focusis mostly on pitched instruments, a considerable amountof publications deal with the transcription of percussivesounds in mixtures of tonal and percussive instruments.The drum track in popular music conveys information abouttempo, rhythm, style, and possibly the structure of a song.A drum transcription system enables applications in ac-tive listening [27], music education, and interactive musicperformance.c\u0000Chih-Wei Wu, Alexander Lerch.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Chih-Wei Wu, Alexander Lerch. “Drum Tran-scription Using Partially Fixed Non-Negative Matrix Factorization withTemplate Adaptation”, 16th International Society for Music InformationRetrieval Conference, 2015.This study explores the application of the popular tran-scription method NMF for drum transcription in polyphonicmusic. A standard NMF approach for music transcriptiondecomposes a signal into a dictionary matrix, which con-sists of multiple pre-deﬁned templates, and an activationmatrix, which contains the activity of the correspondingtemplates. In this paper, we propose to transcribe drumevents using a signal-adaptive method based on NMF.The paper is structured as follows:Section 2providesan overview of the research in this area. InSection 3wepresent our approach; evaluation results are being presentedand discussed inSection 4.Section 5provides a summary,conclusion, and directions of future work.2. RELATED WORKDrum transcription is a task that requires instrument iden-tiﬁcation and onset detection for percussive sounds. Totranscribe signals containing only drum sounds, standardapproaches with a feature extractor and a subsequent clas-siﬁer are able to produce results with high accuracy [11].For most use cases, however, a drum transcription systemis expected to work on mixtures of percussive and har-monic sound sources. Gillet and Richard propose to cat-egorize automatic drum transcription systems into threecategories:(i)segment and classify[4, 7, 22], for whichthe audio signal is segmented into a series of events us-ing onset detection, and each event is classiﬁed based theextracted temporal or spectral features, (ii)separate and de-tect[1,6,15,17], which assumes music to be a superpositionof different sound sources; by decomposing the signal intosource templates with corresponding activation functions,the content can be transcribed by analyzing the activities ofeach template, and (iii)match and adapt[28,29], identify-ing the drum events using a template matching method inwhich the templates are searched for the closest match andadapted in an iterative process.Methods extended from these three types of approacheshave been presented as well. Paulus and Klapuri proposedto use Hidden Markov Models (HMM) for drum transcrip-tion [16]. This method models temporal connections be-tween drum events and detect the drum based on the prob-abilistic model. However, the method needs to train onmultiple drum sequences, thus, a large dataset is needed toobtain a generic model. Another recent approach is to usebar information to classify the audio signal into differentpredeﬁned drum patterns [23]. This approach requires addi-257tional information of the bar locations and a large dictionary,which can be impractical in some use cases.Among the above mentioned methods, the second typeof approaches (separate and detect), frequently using NMF-related methods, has the advantage of joint estimation ofmultiple instruments and easy interpretation of the results.However, when NMF is applied to the task of drum tran-scription, the following challenges have to be faced:First, the number of sound sources and notes within amusic recording is usually unknown. To optimally decom-pose a signal, this number is necessary for determining therankrof the dictionary. This problem would be less severewhen the sound sources of the target signal are given [14].However, in most cases, this prior information is difﬁcult toacquire. One solution is to build a dictionary that containsmore source templates than the target signal. Benetos etal. used a probabilistic extension of NMF (ProbabilisticLatent Component Analysis, PLCA) to jointly transcribepitched and unpitched sounds in polyphonic music with arelatively large pre-trained dictionary [3]. Although thismethod can provide harmonic and percussive contents ofthe music simultaneously, its robustness against unknownsources still needs to be evaluated.Second, without any prior knowledge, it can be hard toidentify the corresponding instrument of every template inthe dictionary matrix [26]. This problem becomes moresevere when the rank is selected too high or too low. Helenand Virtanen trained an SVM to separate drum templatesfrom harmonic templates; the rank number was derivedempirically during the factorization process [10]. The iden-tiﬁed drum templates and their corresponding activationcould later be used to reconstruct the drum signal, resultingin a system for drum source separation. Their approachrequires a signiﬁcant amount of training data for the clas-siﬁer and, more importantly, the results can be expectedto be very susceptible to choice of rank. Yoo et al. pro-posed a co-factorization algorithm [26] to simultaneouslyfactorize a drum track and a polyphonic signal. They usedthe dictionary matrix from the drum track to identify thedrum templates in the polyphonic signal. This approachensures that the drum templates in both dictionary matricesare estimated only from the drum track, resulting in properisolation of the harmonic templates from the drum tem-plates. Since their system aims at drum separation, they canwork at higher ranks. For drum transcription, however, thisapproach is not directly applicable because the correspond-ing instrument of the templates in the dictionary matrix isunknown.Third, a suitable penalty term or sparsity constraint fordetecting percussive instruments still needs to be investi-gated. In general, these constraints are the additional termsin the NMF cost function that will facilitate the differentproperties (e.g., the sparseness) in the resulting activationmatrix. Virtanen proposed to use constraints for temporalcontinuity and sparseness [24]. He reported that by usingthe temporal continuity criterion, the detection accuracyand SNR of the pitched sounds can be improved in thesource separation task, whereas no signiﬁcant improvementMusic  Signals\u0001Transcription\u0001PFNMF\u0001Onset Detection\u0001Drum  Signals\u0001STFT\u0001Template Extraction\u0001STFT\u0001Template Adaptation\u0001Figure 1. Flowchart of the drum transcription systemis shown with the sparseness constraint.Another issue is the adaptability of the extracted tem-plates. When using supervised NMF, the algorithm losesits adaptability and might fail when the target signal is verydifferent from the pre-trained dictionary. Dittmar and Gart-ner proposed to use semi-adaptive bases during the NMFdecomposition process [5]. However, their results indicatethat the semi-adaptive process did not improve the perfor-mance of the transcription accuracy compared to ﬁxed bases.Furthermore, no results were reported for the transcriptionperformance in polyphonic mixtures.3. METHOD3.1 ImplementationFigure 1shows the ﬂow chart of the implemented system.The STFT of the signals will be calculated using a Hannwindow with a window length and a hop size of2048and512, respectively, and the sample rate is 44.1 kHz. Theresulting magnitude spectrogram is used as the input rep-resentation. A pre-trained dictionary matrixWDwill beconstructed from the training set, which consists of isolateddrum sounds. Next, the initial drum dictionary will be usedin the partially ﬁxed NMF (PFNMF) process and updatedby the selected template adaptation methods described inSection 3.3. Finally, the activation matrixHDis processedto determine the onset positions and their correspondingclasses.The initial drum dictionary matrixWDis generated froma subset of the ENST dataset, which contains audio tracksof 5 to 6 single hits for each drum, performed by threedrummers. For every drum class, one track per drummer iscollected as training data. The onset position of these singlehits was determined using the annotated ground truth. Thetemplate spectrum is a median spectrum of all individualevents of one drum class in the training set. The templatesare extracted for the three classes: Hi-Hat (HH), Bass Drum(BD) and Snare Drum (SD).High values in the activation matrixHDindicate thepresence of a drum event. More speciﬁcally, the activitydifference of each row of the activation matrix could beconsidered as the onset novelty function of each individualdrum. We use a median ﬁlter as a standard approach tocreate a signal-adaptive threshold for peak picking [13]. Inthis paper, the window length and the offset coefﬁcient\u0000ofthe median adaptive threshold are set to be 0.1 s and 0.12 forevery track. The Matlab implementation of the presentedsystem is available online.11https://github.com/cwu307/NmfDrumToolbox258 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015V\u0001\u0002\u0001WD\u0001AWH\u0001m × n\u0001!\u0001!\u0001HD\u0001HH\u0001m \u0003 rD\u0001m \u0003 rH\u0001r \u0003 r\u0001rD \u0003 n\u0001rH \u0003 n\u0001Figure 2. Illustration of the factorization process.W:dictionary matrix,H: activation matrix; SubscriptD: drum,subscriptH: harmonic components.Ais the weightingmatrix.3.2 Algorithm DescriptionThe basic concept of NMF can be expressed asV⇡WHwith non-negativity constraints, in whichVis am⇥nmatrix,Wis am⇥rdictionary matrix, andHis ar⇥nactivation matrix, withrbeing the rank of the NMF decom-position. In most audio applications,Vis the spectrogramwithmfrequency bins andnframes,Wcontains the mag-nitude spectra of the salient components, andHindicatesthe activation of these components with respect to time [20].The matricesWandHare estimated through an iterativeprocess that minimizes a distance measure between thetarget spectrogramVand its approximation [12].In this paper, we propose a signal adaptive method totranscribe drum events in polyphonic signals. The idea ofusing NMF with prior knowledge of the target source withinthe mixture has been applied to source separation tasks[21] and multipitch analysis [18]. The method describedhere is based on similar ideas but with different emphasis:(i) we focus on a real world scenario in which users onlyhave limited amount of training samples that are slightlydifferent from the target source, (ii) we propose to use asmall dictionary matrix which is both efﬁcient and easilyinterpretable, and (iii) the proposed method is able to adaptto different content in the polyphonic mixtures.PFNMF [25] is a method inspired by [26] for drum tran-scription task.Figure 2visualizes the concept: the matricesWandHare split into the matricesWDandWH, andHDandHH, respectively. Instead of using co-factorization, thealgorithm initializes the matrixWDwith drum templatesand does not modify it during the factorization process. ThematricesWH,HH, andHDare initialized randomly. TherankrDofWDandHDdepends on the number of tem-plates (i.e., instruments) provided, and the rankrHcan bearbitrarily chosen. The total rankr=rD+rH.Ais ar⇥rdiagonal weighting matrix, which contains weightingcoefﬁcients for every template to balance the drum and har-monic dictionaries in the NMF cost function (as discussedinSection 4.3.1). In our experiment, the coefﬁcients areset to be↵=(rD+rH)/rDfor each drum template and\u0000=rH/(rD+rH)for each harmonic template. This set-ting is to increase the weighting of drum templates andslightly decrease the weighting of harmonic templates asrHbecomes larger. WhenrH=0, the algorithm reduces tothe original NMF.The distance measure used is KL-divergence, in whichDKL(x|y)=x·log (x/y)+(y\u0000x). The NMF costfunction as shown in Eq.(1)is minimized by applyinggradient decent and multiplicative update rules.J=DKL(V|↵WDHD+\u0000WHHH)(1)The matricesWH,HH, andHDwill be updated accord-ing to Eqs. (2)–(4):HD HDWTD(V/(↵WDHD+\u0000WHHH))WTD(2)WH WH(V/(↵WDHD+\u0000WHHH))HTHHTH(3)HH HHWTH(V/(↵WDHD+\u0000WHHH))WTH(4)To summarize, the presented method before templateadaptation consists of the following steps:1.Construct am⇥rDdictionary matrixWD, withrDbeing the number of drum components to be detected.2.Given a pre-deﬁned rankrH, initialize am⇥rHmatrixWH,arD⇥nmatrixHDand arH⇥nmatrixHH.3.NormalizeWDandWH.4.UpdateHD,WH, andHHusing Eqs. (2)–(4).5.Calculate the cost of the current iteration using Eq.(1).6.Repeat step 3 to step 5 until convergence.The time positions of the drum events can then be extractedby applying a simple onset detection on the rows of matrixHD.3.3 Template AdaptationPrevious approaches to include template adaptation in drumtranscription process can be found in [5, 29]. These ap-proaches usually start with seed templates and graduallyadapt them to the optimal templates. In this paper, wepropose two methods for template adaptation with PFNMF.Both methods have the same criterion to stop iterating whenthe error between two consecutive iterations changes by lessthan0.1%or the number of iterations exceeds 20. How-ever, the adaptation process typically converges after 5–10iterations.3.3.1 Method 1: Complementary UpdateIn the ﬁrst method (referred to as AM1), the drum dictio-naryWDis updated based on the cross-correlation betweenthe activationsHHand of each individual drum inHD.PFNMF starts by randomly initializing aWHwith rankrH.AlthoughWHtends to adapt to the harmonic content, it maystill contain entries that belong to percussive instrumentsdue to a mismatch between the initialized drum templatesand the target sources. This will result in cross-talk (si-multaneous activation) betweenHHandHDand generatea less pronounced activation. However, these harmonictemplates may also provide complementary information tothe original drum templates. To identify these entries, thenormalized cross-correlation betweenHHandHDfor eachindividual drum is computed using Eq. (5)⇢x,y=Pnj=1x(j)·y(j)kxk2·kyk2,(5)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 259wherexandyrepresent different activation vectors, andnis the number of samples in the activation vectors. A thresh-old⇢thresis deﬁned for identiﬁcation of related entries, andthe drum templateWDcan be updated using Eq.(6), whereW(i)H(i=1,. . . ,S)are the entries with their corresponding⇢x,yhigher than⇢thres, andSis the number of the selectedentries. Since a low⇢threscan introduce too much adapta-tion and vice versa, a⇢thres= 0.5 is chosen heuristically.The amount of adaptation also depends on the coefﬁcient\u0000=12k, which decreases as iteration numberkincreases.W0D=( 1\u0000\u0000)WD+\u00001SSXi=1⇣⇢(i)W(i)H⌘(6)3.3.2 Method 2: Alternate UpdateIn the second method (referred to as AM2), the drum tem-plateWDis adapted by alternatively ﬁxingWDandHDduring the decomposition process. The adaptation processstarts by ﬁxingWD, and PFNMF will try to ﬁt the bestactivationHDto approximate the drum part in the music.OnceHDis determined, a new iteration of PFNMF can bestarted by ﬁxingHDand allowWD,WHandHHto update.This constraint will guide the algorithm to ﬁt better drumtemplates based on the detected activationHD. The updaterule forWDis shown in Eq. (7).WD WD(V/(↵WDHD+\u0000WHHH))HTDHTD(7)4. EVALUATION4.1 Dataset DescriptionThe experiments have been conducted on two differentdatasets. The ﬁrst one is theminus onesubset from theENST drum dataset [8]. This dataset consists of recordingsfrom three different drummers performing on their owndrum kits. The set for each drummer contains individualhits, short phrases of drum beats, drum solos, and short ex-cerpts played with accompaniments. The minus one subsethas 64 tracks of polyphonic music, and the sampling rateof every track is 44.1 kHz. Each track in this subset hasa length of approximately 70 s with varying style. Morespeciﬁcally, the subset contains various drum playing tech-niques such as ghost notes, ﬂam, and drag; these techniquesare considered difﬁcult to identify with existing drum tran-scription systems [9]. The accompaniments are mixed withtheir corresponding drum tracks using a scaling factor of1/3 and 2/3 in order to reproduce the evaluation settings asused in [16].The second dataset, used for cross-dataset validation, isIDMT-SMT-Drums [5]. This dataset consists of 95 drumloop recordings from three drum kits (RealDrum, Wave-Drum and TechnoDrum). The sampling rate of every trackis 44.1 kHz, and the total duration of the dataset is approxi-mately two hours. This dataset also contains isolated drumhits for training. However, in our experiments, the isolatedsounds are not used.4.2 Evaluation ProcedureWe evaluate the proposed system for both monophonic(drum only) and polyphonic mixtures. The same set ofaudio tracks is used with and without accompaniments. Athree-fold cross-validation is applied to the evaluation pro-cess. Single drum hits collected from two drummers areused to train the system, and complete mixtures from thethird drummer are used to test the system. The process re-peats three times to test every drummer in the dataset. Thisprocess is the same as described in [16], and the purposeis to prevent the system from seeing the test data. Notethat the training data used in the system are single drumhits, and the number of onsets is signiﬁcantly fewer thanthe test data. Typically, the training data only consists of 10to 12 single hits for each drum class. This is similar to thereal-world use case, where the users may have access onlyto a limited number of training samples.The evaluation metrics follow the standard calculationof the precision (P), recall (R), and F-measure (F). To beconsistent with [9], an onset is considered to be a matchwith the ground truth if the time deviation in between is lessor equal to 50 ms. It should be noted that some authors usemore restrictive settings, compare e.g. the 30 ms as usedin [16].4.3 Evaluation Results4.3.1 Rank IndependenceIn an initial test to determine the rankrHof the PFNMF,rH=5,10,20,40,80,160have been tested in polyphonicsignals with and without a weighting matrix. As shown inFigure 3, a general trend of decreasing performance canbe observed whenrH>5without a weighting matrix.With a weighting matrix, however, the performance slightlyincreases for both HH and SD, and slightly decreases for BDas therHincreases. The results demonstrate the robustnessof the proposed system against the rank selection when aweighting matrix is introduced.By increasing the rankrH, a largerWHwill be initializedto better adapt to the target signal, however, this unbalancedincrease in templates would also decrease the weight of thedrum templates in the optimization process, thus reducingthe impact of the percussive templates on the NMF costfunction. This effect is reduced by the weighting matrixAwhich balances the weights between drum and harmonictemplates.4.3.2 Threshold SelectionThe transcription results can be obtained after applyingonset detection on each drum activation (seeSection 3.1).However, the performance varies according to the selectionof the signal-adaptive threshold. To evaluate the inﬂuenceof different thresholds, the average F-measure of all drumswith different offset coefﬁcient\u0000on IDMT-SMT-Drumsdataset is shown inFigure 4. A general trend of paraboliccurve can be observed. This is in agreement with the ﬁnd-ings of Dittmar et al. [5]. One major difference is that inmost regions of the curve, both AM1 and AM2 outperform260 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Figure 3. Average F-measure versus harmonic rankrHin(Top) without weighting matrix (Bottom) with weightingmatrix\nFigure 4. Evaluation results for IDMT-SMT-Drums datasetusing (a) PFNMF (Solid circle) (b) AM1 (Dash diamond)(c)AM2 (Dotted square)PFNMF. This veriﬁes that template adaptation process doeshelp the algorithm in the case of the unknown sounds (tem-plates and the test signals are from two different datasets).The overall performance is slightly lower than [5] due to themismatch in templates and target signals. However, the F-measures of AM1 can reach74.0%,93.2%and73.4%forHH, BD, SD, respectively, which indicates the applicabilityof the proposed method across datasets.4.3.3 ResultsTable 1shows the evaluation results on ENST drum datasetminus onesubsetwithout accompaniments. For comparison,we also list the results of Gillet et al. [9] and Paulus etal. [16]. All the compared methods use the same datasetwith identical mixing settings (1/3 for accompaniments and2/3 for drum tracks). Since the target signals contain onlydrum sounds, the rankrHcan be small. In this experiment,rHis set to 10 for absorbing drum sounds other than HH,BD and SD. The results show that our proposed method isable to transcribe drum events with an average F-measureof77.9%using AM2. This result is higher than the73.8%reported in [9], and at the same level as reported in [16].Table 2shows the evaluation results on ENST drumdatasetminus onesubsetwith accompaniments. The com-pared methods are the same as described above. Since thetarget signals contain both percussive and harmonic parts,rHis set to 50. The results show that our proposed methodachieves an average F-measure =72.2%using AM2, whichis higher than67.8%[9] and at a similar range as the72.7%,reported in [16].In general, our methods outperform [9] for all instru-ments except the snare drum. The possible reason is thatmany of the playing technique variations are applied to thesnare (e.g., ghost note, rim shot, with/without snare on),and a single snare drum template cannot cover all the pos-sibilities even with template adaptation. In the polyphonicdataset, our proposed methods perform better on BD andSD but slightly worse on HH compared to the HMM basedmethod [16]. Since Paulus et al. [16] trained and tested theirsystem using the same ENST dataset, the music played byall three drummers is highly correlated because of the sameaccompaniments used. This may lead to a tendency of over-ﬁtting the transition probability in this dataset. For all themethods, the performances drop from the monophonic tothe polyphonic dataset, especially for BD and SD. This isan unsurprising trend. The less prominent decrease for HHmight be due to the fact that the typical frequency range ofHH is more separated from other instruments than BD andSD, thus is more robust against the presence of tonal sounds.In the case of template adaptation, a general trend of in-crease in precision and decrease in recall can be observed.One explanation is that once a better representation of thedrum templates is found, the system might become moreselective, leading toward a reduction in both false positivesand true positives.AM1 seems to perform better than AM2 on BD in bothmonophonic and polyphonic dataset. One possible expla-nation is that bass drum usually appears on the downbeats,which tends to have higher correlation with other entries inharmonic activation matrix. This means BD has a higherchance of being adapted to better templates using AM1.AM2 uses a more generalized adaptation process and per-forms better on HH and SD. However, it is more computa-tionally demanding since it adapts the templates constantly,whereas AM1 only adapts when the correlation is above thethreshold. To sum up, both template adaptation methodsperform at the similar level, and the best ﬁt of either methodfor speciﬁc types of music still needs to be investigated.5. CONCLUSIONWe have presented a drum transcription system for bothmonophonic and polyphonic music using partially ﬁxedNMF with template adaptation. The system is robust againstrank changes, and the evaluation results show that the twopresented template adaptation methods improve the preci-sion of the system, leading toward better performance. Theproposed method is able to achieve average F-measures of77.9% and 72.2% in monophonic and polyphonic musicrespectively for detecting 3 classes of drums.The presented method has the following advantages:Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 261Method Metric HH BD SD MeanPFNMFP 0.918 0.886 0.825 0.876R 0.705 0.938 0.453 0.698F 0.797 0.911 0.5850.764AM1P 0.909 0.955 0.837 0.900R 0.682 0.927 0.473 0.694F 0.7790.9400.6040.774AM2P 0.928 0.914 0.854 0.898R 0.703 0.927 0.483 0.704F 0.799 0.920 0.6170.779Gillet et al. [9]P 0.736 0.798 0.710 0.748R 0.865 0.700 0.642 0.735F 0.795 0.7450.674 0.738Paulus et al. [16]P 0.838 0.941 0.750 0.806R 0.849 0.921 0.567 0.843F0.8430.930 0.6450.779Table 1. Evaluation results for ENST drum datasetminus onesubsetwithoutaccompanimentsMethod Metric HH BD SD MeanPFNMFP 0.902 0.714 0.684 0.766R 0.706 0.862 0.464 0.677F 0.792 0.781 0.5520.708AM1P 0.904 0.781 0.758 0.814R 0.679 0.856 0.45 0.661F 0.7750.8160.5640.719AM2P 0.908 0.774 0.726 0.802R 0.694 0.855 0.466 0.671F 0.786 0.812 0.5670.722Gillet et al. [9]P 0.702 0.744 0.619 0.688R 0.818 0.653 0.552 0.674F 0.755 0.6950.583 0.678Paulus et al. [16]P 0.847 0.802 0.663 0.770R 0.826 0.815 0.453 0.698F0.8360.808 0.5380.727Table 2. Evaluation results for ENST drum datasetminus onesubsetwithaccompanimentsFirst, the system only requires a few training samples fortemplate extraction, and these templates can adapt towardthe target sources gradually. This makes the system moreapplicably to the real world use case. Second, adjustmentof the parameterrHallows the algorithm to work with poly-phonic music, and the use of a weighting matrix preventsthe performance from dropping asrHincreases. Third, thecross-dataset evaluation results indicate a robustness againsttemplate mismatches, possibly allowing the application insituations with minimum prior knowledge. Last but notleast, the evaluation results indicate that the F-measure ofthe proposed methods is at the same level as state-of-the artsystems with a lower model complexity.Possible directions for future work include the automaticestimation ofrHfor any given signal using a probabilisticapproach similar to [19]; this might be a solution for thesystem to optimally select the rank. Furthermore, a moredetailed analysis of playing techniques might be necessarytoward a more complete drum transcription system. Finally,different penalty terms for the NMF cost function, such assparsity, temporal continuity [24], or rankrHmight be takeninto account for better adjustment of the current method.6. REFERENCES[1]David S Alves, Jouni Paulus, and Jos´e Fonseca. Drumtranscription from multichannel recordings with non-negative matrix factorization. InProc. of the EuropeanSignal Processing Conference (EUSIPCO), Glasgow,2009.[2]Emmanouil Benetos, Simon Dixon, Dimitrios Gian-noulis, Holger Kirchhoff, and Anssi Klapuri. Automaticmusic transcription: Challenges and future directions.Journal of Intelligent Information Systems, 41(3):407–434, December 2013.[3]Emmanouil Benetos, Sebastian Ewert, and TillmanWeyde. Automatic transcription of pitched and un-pitched sounds from polyphonic music. InProc. of theInternational Conference on Acoustics Speech and Sig-nal Processing (ICASSP), 2014.[4]Christian Dittmar. Drum detection from polyphonic au-dio via detailed analysis of the time frequency domain.InProc. of the Music Information Retrieval EvaluationeXchange (MIREX), 2005.[5]Christian Dittmar and Daniel G¨artner. Real-time Tran-scription and Separation of Drum Recording Based onNMF Decomposition. InProc. of the International Con-ference on Digital Audio Effects (DAFX), pages 1–8,2014.[6]Derry FitzGerald, Bob Lawlor, and Eugene Coyle.Drum transcription in the presence of pitched instru-ments using prior subspace analysis. InProc. of the262 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Irish Signals & Systems Conference (ISSC), Limerick,2003.[7]Olivier Gillet and Ga¨el Richard. Automatic transcriptionof drum loops. InProc. of the International Conferenceon Acoustics, Speech, and Signal Processing (ICASSP),volume 4, pages 269–272, May 2004.[8]Olivier Gillet and Ga¨el Richard. ENST-Drums: an ex-tensive audio-visual database for drum signals process-ing. InProc. of the International Conference on MusicInformation Retrieval (ISMIR), Victoria, 2006.[9]Olivier Gillet and Ga¨el Richard. Transcription and sep-aration of drum signals from polyphonic music.IEEEtransactions on Audio, Speech, and Language Process-ing, 16(3):529—540, March 2008.[10]Marko Helen and Tuomas Virtanen. Separation ofdrums from polyphonic music using non-negative ma-trix factorization and support vector machine. InProc.of the European Signal Processing Conference (EU-SIPCO), Antalya, 2005.[11]Perfecto Herrera, Amaury Dehamel, and FabienGouyon. Automatic labeling of unpitched percussionsounds. InProc. of the 114th Audio Engineering SocietyConvention. AES, March 2003.[12]Daniel D Lee and H Sebastian Seung. Algorithms fornon-negative matrix factorization. InAdvances in Neu-ral Information Processing Systems (NIPS), 2000.[13]Alexander Lerch.An Introduction to Audio ContentAnalysis: Applications in Signal Processing and MusicInformatics. John Wiley & Sons, 2012.[14]Henry Lindsay-Smith, Skot McDonald, and Mark San-dler. Drumkit Transcription via Convolutive NMF. InProc. of the International Conference on Digital AudioEffects (DAFX), pages 15–18, 2012.[15]Arnaud Moreau and Arthur Flexer. Drum transcriptionin polyphonic music using non-negative matrix factori-sation. InProc. of the International Conference on Mu-sic Information Retrieval (ISMIR), pages 353—354,2007.[16]Jouni Paulus and Anssi Klapuri. Drum Sound Detec-tion in Polyphonic Music with Hidden Markov Models.EURASIP Journal on Audio, Speech, and Music Pro-cessing, pages 1–9, 2009.[17]Jouni Paulus and Tuomas Virtanen. Drum transcriptionwith non-negative spectrogram factorisation. InProc.of the European Signal Processing Conference (EU-SIPCO), page 4, Antalya, 2005.[18]Stanislaw A. Raczyski, Nobutaka Ono, and ShigekiSagayama. Multipitch analysis with harmonic nonneg-ative matrix approximation. InProc. of InternationalConference on Music Information Retrieval (ISMIR),2007.[19]Mikkel N. Schmidt and Morten Mø rup. Inﬁnite non-negative matrix factorization. InProc. of the EuropeanSignal Processing Conference (EUSIPCO), 2010.[20]Paris Smaragdis and Judith C Brown. Non-negativematrix factorization for polyphonic music transcription.InProc. of the Workshop on Applications of SignalProcessing to Audio and Acoustics (WASPAA), NewPaltz, 2003. .[21]Paris Smaragdis, Bhiksha Raj, and MadhusudanaShashanka. Supervised and semi-supervised separationof sounds from single-channel mixtures. InProc. of the7th international conference on Independent componentanalysis and signal separation, pages 414–421, 2007.[22]Koen Tanghe, Sven Degroeve, and Bernard De Baets.An algorithm for detecting and labeling drum events inpolyphonic music. InProc. of the Music InformationRetrieval Evaluation eXchange (MIREX), 2005.[23]Lucas Thompson, Matthias Mauch, and Simon Dixon.Drum Transcription via Classiﬁcation of Bar-LevelRhythmic Patterns. InProc. of the International Confer-ence on Music Information Retrieval (ISMIR), 2014.[24]Tuomas Virtanen. Monaural sound source separationby nonnegative matrix factorization with temporal con-tinuity and sparseness criteria.IEEE transactions onaudio, speech, and language processing, 15(3):1066–1074, 2007.[25]Chih-Wei Wu and Alexander Lerch. Drum Transcrip-tion using Partially Fixed Non-Negative Matrix Fac-torization. InProc. of the European Signal ProcessingConference (EUSIPCO), Nice, France, 2015.[26]Jiho Yoo, Minje Kim, Kyeongok Kang, and SeungjinChoi. Nonnegative matrix partial co-factorization fordrum source separation. InProc. of the InternationalConference on Acoustics Speech and Signal Processing(ICASSP), pages 1942—1945, Dallas, 2010. .[27]Kazuyoshi Yoshii, Masataka Goto, and Kazunori Ko-matani. Drumix: An audio player with real-time drum-part rearrangement functions for active music listening.IPSJ Digital Courier, 3:134—144, 2007.[28]Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G.Okuno. Automatic drum sound description for real-world music using template adaptation and matchingmethods. InProc. of the International Conference onMusic Information Retrieval (ISMIR), Barcelona, 2004.[29]Kazuyoshi Yoshii, Masataka Goto, and Hiroshi GOkuno. Drum sound recognition for polyphonic audiosignals by adaptation and matching of spectrogram tem-plates with harmonic structure suppression.IEEE trans-actions on Audio, Speech and Language Processing,15(1):333—345, January 2007.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 263"
    },
    {
        "title": "Spectral Learning for Expressive Interactive Ensemble Music Performance.",
        "author": [
            "Guangyu Xia",
            "Yun Wang",
            "Roger B. Dannenberg",
            "Geoffrey Gordon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415806",
        "url": "https://doi.org/10.5281/zenodo.1415806",
        "ee": "https://zenodo.org/records/1415806/files/XiaWDG15.pdf",
        "abstract": "We apply machine learning to a database of recorded en- semble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co- evolving time series and learn their interactive relation- ship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspond- ence not only between different performers but also be- tween the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to gener- ate a more human-like interaction.",
        "zenodo_id": 1415806,
        "dblp_key": "conf/ismir/XiaWDG15",
        "keywords": [
            "machine learning",
            "database",
            "ensemble performances",
            "artificial performer",
            "expressive timing",
            "dynamics",
            "piano duet",
            "interaction",
            "musical expression",
            "co-evolving time series"
        ],
        "content": "SPECTRAL LEARNING FOR EXPRESSIVE INTERACTIVE ENSEMBLE MUSIC PERFORMANCE Guangyu Xia Yun Wang Roger Dannenberg Geoffrey Gordon  School of Computer Science, Carnegie Mellon University, USA {gxia,yunwang,rbd,ggordon}@cs.cmu.edu  ABSTRACT We apply machine learning to a database of recorded en-semble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co-evolving time series and learn their interactive relation-ship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspond-ence not only between different performers but also be-tween the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to gener-ate a more human-like interaction. 1. INTRODUCTION Ensemble musicians achieve shared musical interpreta-tions when performing together. Each musician performs expressively, deviating from a mechanical rendition of the music notation along the dimensions of pitch, dura-tion, tempo, onset times, and others. While creating this musical interpretation, musicians in an ensemble must listen to other interpretations and work to achieve an or-ganic, coordinated whole. For example, expressive timing deviations by each member of the ensemble are con-strained by the overall necessity of ensemble synchroni-zation. In practice, it is almost impossible to achieve sat-isfactory interpretations on the first performance. There-fore, musicians spend time in rehearsal to become famil-iar with the interpretation of each other while setting the “communication protocols” of musical expression. For example, when should each musician play rubato, and when should each keep a steady beat? What is the desired trend and balance of dynamics? It is important to notice that these protocols are usually complex and implicit in the sense that they are hard to express via explicit rules. (Musicians in a large ensemble even need a conductor to help set the protocols.) However, musicians are able to learn these protocols very effectively. After a few re-hearsals, they are prepared to handle new situations that do not even occur in rehearsals, which indicates that the learning procedure goes beyond mere memorization. Although many studies have been done on musical ex-pression in solo pieces, the analysis of interactive ensem-ble music performance is relatively new and has mainly focused on mechanisms used for synchronization, includ-ing gesture. Ensemble human-computer interaction is still out of the scope of most expressive performance studies, and the interaction between synchronization and individ-ual expressivity is poorly understood. From the synthesis perspective, though score following and automatic ac-companiment have been practiced for decades, many re-searchers still refer to this as the “score following” prob-lem, as if all timing and performance information derives from the (human) soloist and there is no performance problem. Even the term “automatic accompaniment” di-minishes the complex collaborative role of performers playing together by suggesting that the (human) soloist is primary and the (computer) accompanist is secondary. In professional settings, even piano accompaniment is usual-ly referred to as “collaborative piano” to highlight its im-portance. To successfully synthesize interactive music performance, all performers should be equal with respect to musical expression, including the artificial performers. Thus, there is a large gap between music practice and computer music research on the topic of expressive inter-active ensemble music performance. We aim to address this gap by mimicking human rehearsals, i.e., learn the communication protocols of musical expression from re-hearsal data. For this paper, we consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. In other words, our goal is to build an arti-ficial pianist that can interact with a human pianist ex-pressively, and is capable of responding to the musical nuance of the human pianist. To build the artificial pianist, we first model different performers’ musical expression as co-evolving time se-ries and design a function approximation to reveal the in-teractive relationship between the two pianists. In particu-lar, we assume musical expression is related to hidden mental states and characterize the piano duet performance as a linear dynamic system (LDS). Second, we learn the parameters of the LDS from multiple rehearsals using a spectral method. Third, given the learned parameters, the artificial pianist can generate an expressive performance by interacting with a human pianist. Finally, we conduct evaluation by comparing the computer-generated perfor-mances with human performances. At the same time, we \n © Guangyu Xia, Yun Wang, Roger Dannenberg, Geoffrey Gordon. Licensed under a Creative Commons Attribution 4.0 Interna-tional License (CC BY 4.0). Attribution: Guangyu Xia, Yun Wang, Roger Dannenberg, Geoffrey Gordon. “Spectral Learning for Expres-sive Interactive Ensemble Performance”, 16th International Society for Music Information Retrieval Conference, 2015. 816   inspect how training set size and the performer’s style affect the results.  The next section presents related work. Section 3 de-scribes the model. Section 4 describes a spectral learning procedure. Section 5 shows the experimental results. 2. RELATED WORK The related work comes from three different research fields: Expressive Performance, where we see the same focus of musical expression; Automatic Accompaniment, where we see the same application of human-computer interactive performance; and Music Psychology, where we see musicology insights and use them to help design better computational models. For detailed historical re-views of expressive performance and automatic accom-paniment, we point the readers to [14] and [27], respec-tively. Here, we only review recent work that has strong connections to probabilistic modeling.  2.1 Expressive Performance Expressive performance studies how to automatically render a musical performance based on a static score. To achieve this goal, probabilistic approaches learn the con-ditional distribution of the performance given the score, and then generate new performances by sampling from the learned models. Grindlay and Helmbold [9] use hid-den Markov models (HMM) and learn the parameters by a modified version of the Expectation-Maximization al-gorithm. Kim et al. [13] use a conditional random field (CRF) and learn the parameters by stochastic gradient descent. Most recently, Flossmann et al. [7] use a very straightforward linear Gaussian model to generate the musical expression of every note independently, and then use a modification of the Viterbi algorithm to achieve a smoother global performance. All these studies successfully incorporate musical ex-pression with time-series models, which serve as good bases for our work. Notice that our work considers not only the relationship between score and performance but also the interaction between different performers. From an optimization point of view, these works aim to opti-mize a performance given a score, while our work aims to solve this optimization problem under the constraints cre-ated by the performance of other musicians. Also, we are dealing with a real-time scenario that does not allow any backward smoothing.  2.2 Automatic Accompaniment Given a pre-defined score, automatic accompaniment sys-tems follow human performance in real time and output the accompaniment by strictly following human’s tempo. Among them, Raphael’s Music Plus One [19] and IRCAM’s AnteScofo system [5] are very relevant to our work in the sense that they both use computational mod-els to characterize the expressive timing of human musi-cians. However, the goal is still limited to temporal syn-chronization; the computer’s musical expression in inter-active performance is not yet considered. 2.3 Music Psychology Most related work in Music Psychology, referred to as sensorimotor synchronization (SMS) and entrainment, studies adaptive timing behavior. Generally, these works try to discover common performance patterns and high-level descriptive models that could be connected with un-derlying brain mechanisms. (See Keller’s book chapter [11] for a comprehensive overview.) Though the discov-ered statistics and models are not “generative” and hence cannot be directly adopted to synthesize artificial perfor-mances, we can gain much musicology insight from their discoveries to design our computational models.  SMS studies how musicians tap or play the piano by following machine generated beats [15-18, 21, 25]. In most cases, the tempo curve of the machine is pre-defined and the focus is on how humans keep track of different tempo changes. Among them, Repp, Keller [21] and Ma-tes [18] argue that adaptive timing requires error correc-tion processes and use a “phase/period correction” model to fit the timing error. The experiments show that the er-ror correction process can be decoupled into period cor-rection (larger scale tempo change) and phase correction (local timing adjustment). This discovery suggests that it is possible to predict timing errors based on timing fea-tures on different scales. Compared to SMS, entrainment studies consider more realistic and difficult two-way interactive rhythmic pro-cesses [1, 8, 10-11, 20, 22, 26]. Among them, Goebl [8] investigated the influences of audio feedback in a piano duet setting and claims that there exist bidirectional ad-justments during full feedback despite the leader/follower instruction. Repp  [20] does further analysis and discov-ers that the timing errors are auto-correlated and that how much musicians adapt to each other depends on the music context, such as melody and rhythm. Keller [11] claims that entrainment not only results in coordination of sounds and movements, but also of mental states. These arguments suggest that it is possible to predict the timing errors (and other musical expressions) by regressions based on different music contexts, and that hidden varia-bles can be introduced to represent mental states. 3. MODEL SPECIFICATION 3.1 Linear Dynamic System (LDS) We use a linear dynamic system (LDS), as shown in Fig-ure 1, to characterize the interactive relationship between the two performers in the expressive piano duet. Here, !=!!!,!!,…,!! denotes the 2nd piano’s musical ex-pression, !=!!!,!!,…,!! denotes a combination of the 1st piano’s musical expression and score information, and !=!!,!!,…,!! denotes the hidden mental states of the 2nd pianist that influence the performance. The key Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 817   idea is to reveal that the 2nd piano’s musical expression is not static. It is not only influenced by the 1st piano’s per-formance but also keeps its own character and continuity over time. \n Figure 1. The graphical representation of the LDS, in which grey nodes represent hidden variables. Formally, the evolution of the LDS is described by the following linear equations:  !!=!!!!!+!!!+!!!!!!!~!(0,!)!!!!!!!!! (1)  !!=!!!+!!!+!!!!!!!!!!!!~!(0,!)!!!!!!!!!! (2) Here, !!∈ℝ! and its two dimensions correspond to expressive timing and dynamics, respectively, !!∈ℝ!, which is a much higher dimensional vector (we describe the design of !! in detail in Section 3.3), and !!∈ℝ!, which is a relatively lower dimensional vector. A, B, C, and D are the main parameters of the LDS. Once they are learned, we can predict the performance of the 2nd piano based on the performance of the 1st piano. 3.2 Performance Sampling   Notice that the LDS is indexed by the discrete variable t. One question arises: should t represent note index or score time? Inspired by Todd’s work [23], we assume that musical expression evolves with score time rather than note indices, and therefore define t as score time. Since music notes have different durations, we “sample” the performed notes (of both the 1st piano and the 2nd pi-ano) at the resolution of a half beat, as shown in Figure 2. \n Figure 2. An illustration of performance sampling. To be more specific, if a note’s starting time aligns with a half beat and its inter-onset-interval (IOI) is equal to or greater than one beat, we replace the note by a series of eighth notes, each having the same pitch, dynamic, and duration-to-IOI ratio as the original note. Note that we still play the notes as originally written; the sampled rep-resentation is only for learning and prediction. 3.3 Input Features Design To show the design of !!, we introduce an auxiliary nota-tion !=!!!,!!,…,!! to denote the raw score infor-mation and musical expression of the 1st piano and de-scribe the mapping from ! to each component of !! in rest of this section. Note that !! is based on sampled score and performance. 3.3.1 Score Features High Pitch Contour:  For the chords within a certain time window up to and including t, extract the highest-pitch notes and fit the pitches by a quadratic curve. Then, high pitch contour for t is defined as the coefficients of the curve. Formally: !!!!\"!≝!argmin!!!!!!!!!\"!!\"#$!−!\"#$!!−!+!!!!!! where p is a context length parameter and !\"#$! is the quadratic function parameterized by !.  Low Pitch Contour: Similar to high pitch contour, we compute !!!\"# for low pitch contour. Beat Phase: The relative location of t within a measure. Formally: BeatPhase!≝(!!!\"#!MeasureLen)/MeasureLen 3.3.2 The 1st Piano Performance Features Tempo Context: Tempi of the p closest notes directly before t. This is a timing feature on a relatively large time scale. Formally: TempoContext!≝!!!!!\"#$%,!!!!!!!\"#$%,⋯,!!!!!\"#$%! Here, the tempo of a note is defined as the slope of the least-squares linear regression between the performance onsets and the score onsets of q preceding notes.  Onsets Deviation Context: A description of how much the p closest notes’ onsets deviate from their tempo curves. Compared to the tempo context, this is a timing feature on a relatively small scale. Formally: OnsetsDeviationContext!≝!!!!!\"#$%#&$'()%(*\",!!!!!!!\"#$%#&$'()%(*\",⋯,!!!!!\"#$%#&$'()%(*\"! Duration Context: Durations of the p closest notes di-rectly before t. Formally: DurationContext!!≝!!!!!\"#,!!!!!!!\"#,⋯,!!!!!\"#! Dynamic Context: MIDI velocities of the p closest notes directly before t. Formally: DynamicContext!!≝!!!!!\"#,!!!!!!!\"#,⋯,!!!!!\"#! The input feature, !!, is a concatenation of the above features. We have also tried other features and mappings (e.g., rhythm context, phrase location, and down beat), \n818 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   and finally picked the ones above through experimenta-tion. 4. SPECTRAL LEARNING PROCEDURE To learn the model, we use a spectral method, which is rooted in control theory [24] and then further developed in the machine learning field [2]. Spectral methods have proved to be both fast and effective in many applications [3][4]. Generally speaking, a spectral method learns hid-den states by predicting the performance future from fea-tures of the past, but forcing this prediction to go through a low-rank bottleneck. In this section, we present the main learning procedure with some underlying intuitions, using the notation of Section 3.1.  Step 0: Construction of Hankel matrices We learn the model in parallel for fast computation. In order to describe the learning procedure more concisely, we need some auxiliary notations. For any time series !=[!!,!!,…,!!], the “history” and “future” Hankel ma-trices are defined as follows: !!≝!!…!!!!⋮⋱⋮!!!…!!!!!!!,!!≝!!!!!…!!!!!⋮⋱⋮!!…!!!! Also, the “one-step-extended future” and “one-step-shifted future” Hankel matrices are defined as follows:  !!!≝!!!!!…!!!!!⋮⋱⋮!!!!…!!,!!!≝!!!!!…!!!!!!!⋮⋱⋮!!!!…!! Here, d is an even integer indicating the size of a sliding window. Note that corresponding columns of !! and !! are “history-future” pairs within sliding windows of size d; compared with !!!, !!! is just missing the first row. We will use the Hankel matrices of both U and Y in the fol-lowing steps. Step 1: Oblique projections If the true model is LDS, i.e., everything is linear Gaussi-an, the expected future observations can be expressed lin-early by history observations, history inputs, and future inputs. Formally: !(!!|!!,!!,!!)=[!!!!!!!!!!!]!!!!!!!!!!!!!!!!(3) Here, !=[!!!!!!!!!!!] is the linear coefficient that could be solved by: !=!!!!!!!!!!!=!!!!!!!!!!!!!!!!!!!!!!!!!!(4) where † denotes the Moore-Penrose pseudo-inverse. However, since in a real-time scenario the future input, !!, is unknown, we can only partially explain future ob-servations based on the history. In other words, we care about the best estimation of future observations but just based on the history observations and inputs. Formally: !!!≝!!!!!!0=!!!!!!!!0!!!!0!!!!!!!!!!!!!!(5) where !!!is referred to as the oblique projection of !! “along” !! and “onto” !!!!. In this step, we also use the same technique to compute!!!! and just throw out its first row to obtain !!!. Step 2: State estimation by singular value decomposi-tion (SVD)  If we knew the true parameters of the LDS, the oblique projections and the hidden states would have the follow-ing relationship: !!=!!!!≝!!!\"⋮!!!!!!!!!!!!,!!!!!,…!,!!!!!!!!!!!!(6) !!!=!!!!!≝!!\"⋮!!!!!!!!!!!!,!!!!!,…!,!!!!!!!!!!!(7) Intuitively, the information from the history observa-tions and inputs “concentrate” on the nearest future hid-den state and then spread out onto future observations. Therefore, if we perform SVD on the oblique projections and throw out small singular values, we essentially en-force a bottleneck on the graphical model representation, learning compact, low-dimensional states. Formally, let !!=!Λ!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!(8) and delete small numbers in Λ and corresponding col-umns in ! and !. Since LDS is defined up to a linear transformation, we could estimate the hidden states by: !!=!Λ!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!9 !!=!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!(10) !!!!=!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!(11) Step 3: Parameter estimation Once we have estimated the hidden states, the parameters can be estimated from the following two equations:  !!!!=!!!+!!!!+!!!!!!! (12)  !!=!!!+!!!+!!!!!!! (13) Here, !! and !! are the 1st rows of !! and !!, i.e., !!=!!!!!!,!!!!!,…,!!!!!, !!=!!!!!!,!!!!!,…,!!!!!. Similarly,!!! is the 1st row of !!!, i.e., !!!=!!!!!!,!!!!!,…,!!!!!!!. In summary, the spectral method does three regres-sions. The first two estimate the hidden states by oblique projections and SVD. The third one estimates the parame-Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 819   ters. The oblique projections can be seen as de-noising the latent states by using past observations, while the SVD adds low-rank constraints. As opposed to maximum likelihood estimation (MLE), the spectral method is a method-of-moments estimator that does not need any random initialization or iterations. Also note that we are making a number of arbitrary choices here (e.g., using equal window sizes for history and future), not attempt-ing to give a full description of how to use spectral meth-ods. (See Van Overschee & De Moor’s book [24] for the details and variations of the learning methods.) 5. EXPERIMENTS 5.1 Dataset We created a dataset [27] that contains three piano duets: Danny Boy, Serenade (by Schubert), and Ashokan Fare-well. All pieces are in MIDI format and contain two parts: a monophonic 1st piano part and a polyphonic 2nd piano part. Each piece is performed 35 to 42 times in different musical interpretations by 5 to 6 pairs of musicians. (Each pair performs each piece of music 7 times.) 5.2 Methods for Comparison  We use three methods for comparison: linear regression, neural network, and the timing estimation often used in automatic accompaniment systems [6]. The first two methods use the same set of features as in the spectral methods, while the 3rd method does not contain any learn-ing procedure and is considered as the baseline. Linear regression: Referring to the notation in Section 3, the linear regression method simply solves the follow-ing equation: !=!\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!14 Like the LDS, this method uses the performance of 1st piano to estimate that of the 2nd piano, but it does not use any hidden states or attempt to enforce self-consistency in the musical expression of the 2nd pianist’s performance. Neural network: We use a simple neural network with a single hidden layer. The hidden layer consists of 10 neu-rons and uses rectified linear units (ReLUs) to produce non-linearity; the single output neuron is linear. Denoting the activation of the hidden units by Z, the neural network represents the following relationship between U and Y:  !=!!!!+!!! 15  !=!!!+!!!!!!!!! (16) where !!=0,!<0!,!≥0!!!!!!!!!!!!!!!!!!!!!!!!!!!17 The neural network is trained by the minibatch stochastic gradient descent (SGD) algorithm, using the mean abso-lute error as the cost function. The parameters of the neu-ral network (W1, b1, W2, b2) are initialized randomly, after which they are tuned with 30 epochs of SGD. Each mini-batch consists of one rehearsal. The learning rate decays from 0.1 to 0.05 in an exponential fashion during the training. We report the average absolute and relative er-rors across five runs with different random initializations on the test set.  This method can be seen as an attempt to improve the linear regression method using non-linear function ap-proximation, but it also doesn’t consider the self-consistency in the musical expression of the 2nd pianist’s performance. Baseline: The baseline method assumes that local tempo and dynamics are stable. For timing, it estimates a linear mapping between real time and score time by fitting a straight line to 4 recently performed note onsets of the 1st piano. This mapping is then used to estimate the timing of the next note of the 2nd piano. For dynamics, it uses the dynamics of the last performed note of the 1st piano as the estimator. \nFigure 3. A local view of the absolute timing residuals of the LDS approach. \n Figure 4. A local view of the absolute dynamics residu-als of the LDS approach. 5.3 A Local View of the LDS Method Figure 3 and Figure 4 show a local view of the expressive timing and dynamics cross-validation result, respectively, for Danny Boy. (To have a clear view, we just compare LDS with the baseline here. We show the results of all the methods on all the pieces later.) For both figures, the x-axis represents score time and the y-axis represents ab-solute residual between the prediction and human per-formance. Therefore, small numbers mean better results. The curve with circle markers represents the baseline ap-proach, while the curve with “x” markers represents the LDS approach trained with only 4 randomly selected re-hearsals of the same piece performed by other perform-ers. We can see that the LDS approach performs much 253035404500.050.10.15\nScore time (sec)Time residual (sec)\n  BLLDS\n2530354045051015202530\nScore time(sec)Dynamics residual (MIDI velocity\n  BLLDS820 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015   better than the baseline approach with only 4 training re-hearsals, which indicates that the algorithm is both accu-rate and robust. 5.4 A Global View of All Methods The curves in the previous two figures are a measurement over different performances. If we average the absolute residual across an entire piece of music, we get a single number that describes a method’s performance for that piece. I.e., how much on average is the prediction of a method different from the human performance for each note? Figure 5 and Figure 6 show this average absolute residual for timing and dynamics, respectively, for all the methods and pieces combinations with different training set sizes. \n Figure 5. A global view of absolute timing residuals for all pieces and methods. (Smaller is better.)  \n Figure 6. A global view of absolute dynamics residuals for all pieces and methods. (Smaller is better.) In both figures, the x-axis represents different methods with different training set sizes, the y-axis represents the average absolute residual, and different colors represent different pieces. For example, the grey bar above the la-bel “NN-4” in Figure 5 is the average absolute timing re-sidual for Serenade by using the neural network approach with 4 training rehearsals.  We see that for expressive timing, both neural network and LDS outperform simple linear regression, and the LDS performs the best regardless of the music piece or training set size. This indicates that the constraint of pre-ceding notes (self-consistency) captured by LDS is play-ing an important role in timing prediction. For expressive dynamics, the difference between different methods is less significant. We see no benefit by using a neural net-work. But when the training set size is small, LDS still outperforms linear regression. (Which is quite interesting because LDS learns more parameters than linear regres-sion.) 5.5 Performer’s Effect Finally, we inspect whether there is any gain by training a performer-specific model. In other words, we only learn from the rehearsals performed by the same pair of musi-cians. Since each pair of musicians only performs 7 times for each piece, we randomly choose 4 from the 7 perfor-mances to make a fair comparison against the results in Figure 5 and Figure 6. \n Figure 7. A global view of the performer-specific model. Figure 7 shows a comparison between performer-specific model and different-performer model. In both sub-graphs, the bars above “LDS-4same” are the results for perform-er-specific model, while the bars above “LDS-4” are the same as in Figure 5 and Figure 6. Note that they are both cross-validation results and the only difference is the training set. We see that the performer-specific model achieves better results, especially when the different-performer model is not doing a good job. 6. CONCLUSIONS AND FUTURE WORK In conclusion, we have applied a spectral method to learn the interactive relationship in expressive piano duet per-formances from multiple rehearsals. Compared to other methods, we have made better predictions based on only 4 rehearsals, and we have been able to further improve the results using a performer-specific model. Our best model is able to shrink the timing residual by nearly 60 milliseconds and shrink the dynamic residual by about 8 MIDI velocity units compared to the baseline algorithm, especially when the baseline algorithm behaves poorly. In the future, we would like to incorporate some non-linear function approximations with the current graphical representation of the model. An ideal case would be to combine the dynamical system with a neural network, which calls for new spectral learning algorithms. Also, we would like to be more thorough in the evaluations. Rather than just inspecting the absolute difference be-tween computer-generated performance and human per-formances, we plan to also compare computed-generated results with typical variation in human performances and use subjective evaluation. BLLR−4NN−4LDS−4LR−8NN−8LDS−8LR−16NN−16LDS−1600.050.10.15\nMethods and training sizeTime Residual (sec)\n  Danny BoySerenadeAshokan Farewell\nBLLR−4NN−4LDS−4LR−8NN−8LDS−8LR−16NN−16LDS−1605101520\nMethods and training sizeDynamics Residual (MIDI velocity)  Danny BoySerenadeAshokan FarewellLDS−4LDS−4same00.020.040.060.080.10.12Timing residual (sec)\n  \nLDS−4LDS−4same051015Dynamics residual (MIDI velocity)  Danny BoySerenadeAshokan FarewellDanny BoySerenadeAshokan FarewellProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 821   7. REFERENCES [1] C. Bartlette, D. Headlam, M. Bocko, and G. Velikic, “Effect of Network Latency on Interactive Musical Performance,” Music Perception, pp. 49–62, 2006. [2] B. Boots, Spectral Approaches to Learning Predictive Representations (No. CMU-ML-12-108). Carnegie Mellon Univ., School of Computer Science, 2012. [3] B. Boots and G. Gordon, “An Online Spectral Learning Algorithm for Partially Observable Nonlinear Dynamical Systems,” Proceedings of the National Conference on Artificial Intelligence, 2011. [4] B. Boots, S. Siddiqi, and G. Gordon, “Closing the Learning-planning Loop with Predictive State Representations,” The International Journal of Robotics Research, pp. 954-966, 2011. [5] A. Cont, “ANTESCOFO: Anticipatory Synchronization and Control of Interactive Parameters In Computer Music,” Proceedings of International Computer Music Conference, pp. 33-40, 2011. [6] R. Dannenberg, “An Online Algorithm for Real-Time Accompaniment,” Proceedings of the International Computer Music Conference, pp. 193-198, 1984. [7] S. Flossmann, M. Grachten, and G. Widmer, “Expressive Performance Rendering with Probabilistic Models,” Guide to Computing for Expressive Music Performance, Springer, pp. 75–98, 2013. [8] W. Goebl and C. Palmer, “Synchronization of Timing and Motion Among Performing Musicians,” Music Perception, pp. 427– 438, 2009.  [9] G. Grindlay and D. Helmbold, “Modeling, Analyzing, and Synthesizing Expressive Piano Performance with Graphical Models,” Machine Learning, pp. 361-387, 2006. [10] M. Hove, M. Spivey, and L. Krumhansl, “Compatibility of Motion Facilitates Visuomotor Synchronization,” Journal of Experimental Psychology: Human Perception and Performance, pp. 1525-1534, 2010. [11] P. Keller, “Joint Action in Music Performances,” Enacting Intersubjectivity: A Cognitive and Social Perspective to the Study of Interactions Amsterdan, The Netherlands: IOS Press, pp. 205-221, 2008. [12] P. Keller, G. Knoblich, and B. Repp, “Pianists Duet Better When They Play with Themselves: On the Possible Role of Action Simulation in Synchronization,” Consciousness and Cognition, pp. 102–111, 2007. [13] T. Kim, F. Satoru, N. Takuya, and S. Shigeki, \"Polyhymnia: An Automatic Piano Performance System with Statistical Modeling of Polyphonic Expression and Musical Symbol Interpretation,\" Proceedings of the International Conference on New Interfaces for Musical Expression, pp. 96-99, 2011. [14] A. Kirke and E. R. Miranda, “A Survey of Computer Systems for Expressive Music Performance,” ACM Surveys 42(1): Article 3, 2009.  [15] E. Large and J. Kolen, “Resonance and the Perception of Musical Meter”. Connection Science, pp. 177–208, 1994.  [16] E. Large and C. Palmer, “Perceiving Temporal Regularity in Music,” Cognitive Science, pp. 1–37, 2002. [17] E. Large and C. Palmer, “Temporal Coordination and Adaptation to Rate Change in Music Performance,” Journal of Experimental Psychology: Human Perception and Performance, pp. 1292-1309, 2011. [18] J. Mates, “A Model of Synchronization of Motor Acts to a Stimulus Sequence:  Timing and Error Correction,” Biological Cybernetics, pp. 463– 473, 1994. [19] C. Raphae, “Music Plus One and Machine Learning,” Proceedings of International Conference on Machine Learning, pp. 21-28, 2010. [20] B. Repp and P. Keller, “Sensorimotor Synchronization with Adaptively Timed Sequences,” Human Movement Science, pp. 423-456, 2008. [21] B. Repp and P. Keller, “Adaptation to Tempo Changes in Sensorimotor Synchronization: Effects of Intention, Attention, and Awareness,” Quarterly Journal of Experimental Psychology, pp. 499-521, 2004. [22] G. Schöner, “Timing, Clocks, and Dynamical Systems,” Brain and Cognition, pp. 31-51, 2002. [23] P. Todd, \"A Connectionist Approach to Algorithmic Composition,\" Computer Music Journal, pp. 27-43, 1989. [24] P. Van Overschee and B. De Moor, Subspace Identification for Linear Systems: Theory, Implementation, applications. Kluwer Academic Publishers, 1996. [25] D. Vorberg and H. Schulze, “A Two-level Timing Model for Synchronization,” Journal of Mathematical Psychology, pp. 56–87, 2002. [26] A. Wing, “Voluntary Timing and Brain Function: an Information Processing Approach,” Brain and Cognition, pp. 7-30, 2002. [27] G. Xia and R. Dannenberg, “Duet Interaction: Learning Musicianship for Automatic Accompaniment,” Proceedings of the International Conference on New Interfaces for Musical Expression, 2015. 822 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Infinite Superimposed Discrete All-Pole Modeling for Multipitch Analysis of Wavelet Spectrograms.",
        "author": [
            "Kazuyoshi Yoshii",
            "Katsutoshi Itoyama",
            "Masataka Goto"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417575",
        "url": "https://doi.org/10.5281/zenodo.1417575",
        "ee": "https://zenodo.org/records/1417575/files/YoshiiIG15.pdf",
        "abstract": "This paper presents a statistical multipich analyzer based on a source-filter model that decomposes a target music audio signal in terms of three major kinds of sound quan- tities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral enve- lope of an isolated sound is represented by an all-pole filter, linear predictive coding (LPC) can be used for filter esti- mation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral enve- lope, the whole spectrum is used for filter estimation. To solve this problem, we propose an infinite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed har- monic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparamet- ric Bayesian source-filter model is formulated in the log- frequency domain that better suits the frequency character- istics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain.",
        "zenodo_id": 1417575,
        "dblp_key": "conf/ismir/YoshiiIG15",
        "keywords": [
            "pitch",
            "timbre",
            "intensity",
            "source-filter model",
            "spectral envelope",
            "all-pole filter",
            "linear predictive coding",
            "infinite superimposed discrete all-pole",
            "nonparametric Bayesian",
            "log-frequency domain"
        ],
        "content": "INFINITE SUPERIMPOSED DISCRETE ALL-POLE MODELINGFOR MULTIPITCH ANALYSIS OF WA VELET SPECTROGRAMSKazuyoshi Yoshii1Katsutoshi Itoyama1Masataka Goto21Graduate School of Informatics, Kyoto University, Japan2National Institute of Advanced IndustrialS c i e n c ea n dT e c h n o l o g y( A I S T ) ,J a p a n{yoshii,itoyama}@kuis.kyoto-u.ac.jp m.goto@aist.go.jpABSTRACTThis paper presents a statistical multipich analyzer basedon a source-ﬁlter model that decomposes a target musicaudio signal in terms of three major kinds of sound quan-tities: pitch (fundamental frequency: F0), timbre (spectralenvelope), and intensity (amplitude). If the spectral enve-lope of an isolated sound is represented by an all-pole ﬁlter,linear predictive coding (LPC) can be used for ﬁlter esti-mation in the linear-frequency domain. The main problemof LPC is that although only the amplitudes of harmonicpartials are reliable samples drawn from the spectral enve-lope, the whole spectrum is used for ﬁlter estimation. Tosolve this problem, we propose aninﬁnite superimposeddiscrete all-pole(iSDAP) model that, given a music signal,can estimate an appropriate number of superimposed har-monic structures whose harmonic partials are drawn fromal i m i t e dn u m b e ro fs p e c t r a le n v e l o p e s . O u rn o n p a r a m e t -ric Bayesian source-ﬁlter model is formulated in the log-frequency domain that better suits the frequency character-istics of human audition. Experimental results showed thatthe proposed model outperformed the counterpart modelformulated in the linear frequency domain.1. INTRODUCTIONStatistical modeling of music audio signals based on ma-chine learning techniques is a hot topic in the ﬁeld of musicsignal analysis. In particular, nonnegative matrix factoriza-tion (NMF) has often been used for multiple fundamentalfrequency (F0) estimation (multipitch analysis) and sourceseparation [1–4, 7, 14, 15, 17, 21–26]. The standard NMFapproximates a nonnegative spectrogram (matrix) as theproduct of two nonnegative matrices: a set of basis spec-tra and a set of the corresponding activations. An efﬁcientmultiplicative-updating (MU) algorithm was proposed forminimizing a cost function that measures the approxima-tion error [18]. This was later found to be maximum like-lihood estimation of a particular probabilistic model [5].Statistical source-ﬁlter models, which were inspired bythe simpliﬁed model of the speech production mechanism,\nc\u0000Kazuyoshi Yoshii, Katsutoshi Itoyama, Masataka Goto.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Kazuyoshi Yoshii, Katsutoshi Itoyama,Masataka Goto. “Inﬁnite Superimposed Discrete All-pole Modeling forMultipitch Analysis of Wavelet Spectrograms”, 16th International Soci-ety for Music Information Retrieval Conference, 2015.)BSNPOJD\u0001TUSVDUVSFT⊗\nʜʜHBJOTCBTFT⊕⊗\"MM\u000eQPMF\u0001USBOTGFS\u0001GVODUJPOT\n0CTFSWFE\u0001TQFDUSVN\u0001PG\u0001FBDI\u0001GSBNF-PH\u0001GSFRVFODZ\u0001<DFOUT>-PH\u0001GSFRVFODZ\u0001<DFOUT>TPVSDFT\u0001\tQJUDIFT\nGJMUFST\u0001\tUJNCSFT\n0OMZ\u0001UIF\u0001XFJHIUT\u0001PG\u0001IBSNPOJD\u0001QBSUJBMT\u0001BSF\u0001TDBMFE\u0001BDDPSEJOH\u0001UP\u0001B\u0001GJMUFS%\"1\nFigure 1.O v e r v i e w o f i n ﬁ n i t e s u p e r i m p o s e d d i s c r e t e a l l -pole (iSDAP) modeling: We take the inﬁnite limit as boththe numbers of sources and ﬁlters,IandJ,g ot oi n ﬁ n i t y .have often been proposed for representing musical instru-ment sounds [7,14,25]. The pitches and timbres of musicalinstrument sounds are well characterized by ﬁne structures(sources) and spectral envelopes (ﬁlters) in the frequencydomain. Since the human auditory system is sensitive tospectral peaks and formants, the spectral envelope of eachframe is usually modeled by an all-pole frequency transferfunction (frequency response of an autoregressive (AR) ﬁl-ter) [14]. A classical method of all-pole spectral envelopeestimation called linear predictive coding (LPC) [16] cor-responds to maximum likelihood estimation of a particularprobabilistic model under a strong assumption that sourcesignals have the ﬂat spectrum (white noise).The composite autoregressive (CAR) modeling [17] isap r o m i s i n gs t a t i s t i c a la p p r o a c ht h a to v e r c o m e st h el i m i -tation of classical source-ﬁlter modeling in the frameworkof NMF. A given audio spectrogram is decomposed intospeciﬁed numbers of ﬁne structures (sources) and spectralenvelopes (ﬁlters). A key feature of this approach is thatsource spectra themselves can be estimated (not limitedto white noise) at the same time as all-pole spectral enve-lope estimation. The probabilistic interpretation of source-ﬁlter NMF makes it possible to formulate a nonparamet-ric Bayesian extension calledinﬁniteCAR (iCAR) model-ing that can automatically choose the appropriate numbersof sources and ﬁlters according to a given audio spectro-gram [26]. Another useful extension is to restrict sourcespectra to harmonic structures by using parametric func-tions [26]. The F0s of source spectra can be estimated in aprincipled maximum-likelihood framework.Conventional methods of source-ﬁlter NMF includingCAR [7,14,17,25,26] have two major problems as follows:861. All the frequency bins are taken into account forspectral envelope estimation although only the am-plitudes of harmonic partials can be regarded as re-liable samples from spectral envelopes.2. Linear-frequency spectrograms given by short-timeFourier transform (STFT) are used for all-pole mod-eling although log-frequency spectrograms given bywavelet or constant-Q transform better suit the fre-quency characteristics of human audition.To solve these problems, we propose a new statisticalapproach to source-ﬁlter NMF calledinﬁnite superimposeddiscrete-all pole(iSDAP) modeling. Our approach is basedon a well-known technique calleddiscrete all-pole(DAP)modeling [8] that takes into account only the peaks of har-monic partials for spectral envelope estimation. To dealwith polyphonic music audio signals, however, we need toseparate individual harmonic structures and estimate theirF0s (positions of discrete harmonic partials). A major con-tribution of this study is to extend the DAP modeling fordealing with an arbitrary number of superimposed harmonicstructures in a similar way to the iCAR modeling. This en-ables us to decompose a log-frequency spectrogram intoappropriate numbers of pitches (F0s), timbres (spectral en-velopes), and their volumes by leveraging the frequency-scale-free characteristics of the DAP modeling.2. RELATED WORKThis section reviews probabilistic models of source-ﬁlterdecomposition, NMF, and source-ﬁlter NMF as a basis offormulating the iSDAP model. Most conventional modelsare formulated in the linear frequency (STFT) domain.2.1 Linear Predictive Coding (All-pole Modeling)The linear predictive coding (LPC) [16] is a signal model-ing method that can be used for estimating the spectral en-velope of an observed spectrum. The underlying assump-tion is that the corresponding audio signalx={xm}1m=\u00001(a local signal{xm}Mm=1is inﬁnitely repeated) follows aP-order autoregressive (AR) process as follows:xm=\u0000PXp=1apxm\u0000p+smi.e.,PXp=0apxm\u0000p=sm,(1)wherea=[a0,···,aP]Tis a vector of AR coefﬁcients(a0=1)a n d{sm}Mm=1is a set of prediction errors. Eq. (1)can be interpreted in terms of source-ﬁlter modeling,i.e.,whenxis a speech signal,sis an excitation signal gener-ated by the vocal cords (source) andarepresents the reso-nance characteristics of the vocal tract (ﬁlter).Eq. (1) can be regarded as a linear system (governed bya)t h a tt a k e ssas input and then givesxas output. SinceEq. (1) is a convolution ofawithx,w ec a ns a yA(z)X(z)=S(z)i.e.,X(z)=S(z)F(z),(2)whereX(z)andS(z)are thez-transforms ofxands,r e -spectively, which are given byX(z)=1Xm=\u00001xmz\u0000mandS(z)=1Xm=\u00001smz\u0000m,(3)\u0013\u0011\u0011\u000e\u0015\u0011\u000e\u0013\u0011\u000e\u0017\u0011\u000e\u0019\u0011\u000e\u0012\u0011\u0011\u000e\u0012\u0015\u0011\u0019 <L)[>\u0018\u0017\u0016\u0015\u0014\u0013\u0012\u000e\u0012\u0013\u0011<E#>%JTDSFUF\u0001BMM\u000eQPMF\u0001NPEFMJOH \t%\"1\n-JOFBS\u0001QSFEJDUJWF\u0001DPEJOH\u0001\t-1$\n%\"1\u0001JT\u0001OPU\u0001CJBTFE\u0001UPXBSET\u0001IBSNPOJD\u0001QBSUJBMT\n0CTFSWFE\u0001TQFDUSVN\tIBSNPOJD\u0001TUSVDUVSF\nFigure 2.S p e c t r a le n v e l o p e se s t i m a t e db yL P Ca n dD A P .andF(z)def=1A(Z)is an all-pole transfer function given byF(z)=1A(z)=1PPp=0apz\u0000p.(4)Letting2⇡mM=!mand substitutingz=ei!minto Eq. (2),we get the Fourier-transform representation as follows:X(ei!m)=S(ei!m)F(ei!m),(5)where{X(ei!m)}Mm=1is the complex spectrum of the ob-served signalx,{S(ei!m)}Mm=1is that of the source signals,a n d{F(ei!m)}Mm=1is the frequency characteristics ofthe all-pole transfer function.The goal of LPC is to estimate a set of AR coefﬁcientsaunder a strong unrealistic assumption that the source sig-nalsis Gaussian white noise. This means thatS(ei!m)iscomplex Gaussian-distributed as follows:S(ei!m)⇠Nc(0,\u00002),(6)where\u00002is the power of the white spectrum of the sourcesignals.U s i n gE q .( 5 )a n dE q .( 6 ) ,w eg e tX(ei!m)⇠Nc(0,\u00002|F(ei!m)|2).(7)LettingXm=|X(ei!m)|2andFm=|F(ei!m)|2,w ebrieﬂy rewrite Eq. (7) as follows:Xm⇠Exponential(\u00002Fm),(8)where{Xm}Mm=1is thepowerspectrum of the observedsignalxand{Fm}Mm=1is the spectral envelope of{Xm}Mm=1,as shown in Figure 2. Eq. (8) deﬁnes the probabilistic modelof LPC.{Fm}Mm=1(i.e.,a)a n d\u00002can be estimated in amaximum-likelihood manner [16].The main problem of LPC is that if we analyze a pitchedsound derived from a periodic source signal (e.g.,v i b r a t i o nof strings), the estimated envelope{Fm}Mm=1loosely ﬁtsthe observed spectrum{Xm}Mm=1and its peaks (formants)tend to be biased to the positions of harmonic partials. Thisis because allMfrequency bins are used for all-pole mod-eling although in reality only the amplitudes of harmonicpartials can be considered to be reliable samples from thespectral envelope.2.2 Discrete All-pole ModelingThe discrete all-pole (DAP) modeling [8] is a well-knownspectral envelope estimation method that was proposed forsolving the problem of LPC. Since DAP is an extension ofLPC, the probabilistic model of DAP has the same formas Eq. (8). A key feature of DAP is that Eq. (8) is deﬁnedover only a partial set of frequency bins,⌦, as follows:Xm⇠Exponential(\u00002Fm)m2⌦,(9)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 87where if⌦={1,···,M},D A Pr e d u c e st oL P C .T oe s t i -mate the spectral envelope of a harmonic spectrum, we cantake into account only the discrete frequencies of harmonicpartials. The estimated envelope passes close to the peaksof harmonic partials (Figure 2). To maximize the likeli-hood given by Eq. (9), an efﬁcient algorithm was proposedfor alternately optimizingaand\u00002[8]. It was later foundas a multiplicative updating algorithm [1, 14].The main limitation of DAP is that the F0 and its over-tones of an observed spectrum{Xm}Mm=1should be givenin advance for deﬁning a set of discrete frequencies to beconsidered,⌦. To analyze polyphonic music audio signalsconsisting of superimposed harmonic structures, we needto separate harmonic structures and estimate their F0s.2.3 Composite Autoregressive ModelingThe composite autoregressive (CAR) modeling [17] is avariant of source-ﬁlter NMF that is used for decomposing alinear-frequency mixture spectrogram intoIﬁne structures(sources) andJspectral envelopes (ﬁlters), as shown inFigure 3. LetXbe anM⇥Npower spectrogram, whereMis the number of frequency bins andNis the number offrames. The nonnegative matrixXis factorized into threekinds of “factors”S,F,a n dHas follows:Xmn⇡IXi=1JXj=1SimFjmHnijdef=Ymn,(10)where{Sim}Mm=1is the linear-frequency power spectrumof sourcei,{Fjm}Mm=1is that of ﬁlterj,a n dHnijis thegain of a pair of sourceiand ﬁlterjat framen.A l lt h e s evariables should be estimated fromX.2.3.1 Original FormulationThe probabilistic model of CAR can be formulated by pre-cisely modeling source signals in a statistical manner. Toavoid the unrealistic assumption of LPC that each sourcesignal is Gaussian white noise (Eq. (6)), we assumeSi(ei!m)⇠Nc(0,Sim),(11)where{Si(ei!m)}Mm=1is the complex spectrum of sourcei.U s i n gE q .( 5 )a n dE q .( 1 1 ) ,w eg e tXijmn(ei!m)⇠Nc(0,SimFjmHnij),(12)where{Xijmn(ei!m)}Mm=1is alatentcomplex spectrumgenerated from sourceiand ﬁlterjat framen.U s i n gt h ereproducing property of the Gaussian and Eq. (10), we getXmn(ei!m)⇠Nc(0,Ymn),(13)where{Xmn(ei!m)}Mm=1is theobservedcomplex spec-trum at framen.E q .( 1 3 )i se q u i v a l e n tt oXmn⇠Exponential(Ymn),(14)whereE[Xmn]=Ymnis satisﬁed and{Xmn}Mm=1and{Ymn}Mm=1are thepowerspectra of framen.This means that the Itakura-Saito (IS) divergence is the-oretically justiﬁed as a cost function that evaluates the errorbetweenXmandYmin Eq. (10) [17]. In general, however,optimization algorithms tend to get stuck in bad local min-ima because the IS divergence is not convex w.r.t.Ymn.)BSNPOJD\u0001TUSVDUVSFT⊗\nʜʜ\"MM\u000eQPMF\u0001USBOTGFS\u0001GVODUJPOT\n-JOFBS\u0001GSFRVFODZ\u0001<)[>-JOFBS\u0001GSFRVFODZ\u0001<)[>TPVSDFT\u0001\tQJUDIFT\nGJMUFST\u0001\tUJNCSFT\n5IF\u0001XIPMF\u0001IBSNPOJD\u0001TUSVDUVSF\u0001JT\u0001TDBMFE\u0001BDDPSEJOH\u0001UP\u0001B\u0001GJMUFS-1$HBJOTCBTFT⊕⊗\n0CTFSWFE\u0001TQFDUSVN\u0001PG\u0001FBDI\u0001GSBNFFigure 3. Overview of composite autoregressive (CAR)modeling deﬁned in the linear frequency domain.2.3.2 Several ExtensionsAnother probabilistic model of CAR was proposed by us-ing the Kullback-Leibler (KL) divergence instead of the ISdivergence as a cost function for a practical reason [26].Instead of Eq. (14), we assumeXmn⇠Poisson(Ymn),(15)whereE[Xmn]=Ymnholds.{Xmn}Mm=1and{Ymn}Mm=1are theamplitudespectra of framenbecause KL-NMFmodels are usually formulated in the amplitude domain byassuming the amplitude additivity [10, 18].An o n p a r a m e t r i cB a y e s i a ne x t e n s i o nc a l l e dinﬁniteCARenables us to automatically estimate appropriate numbersof sources and ﬁlters according to the observationX[26].This technique is based on gamma process NMF [15].Another extension of CAR is to force the amplitudespectrum of each source{Sim}Mm=1to have a harmonicstructure [26]. If the source signal is a train of periodic im-pulses (an idealized model of the vocal chords),{Sim}Mm=1has a harmonic structure consisting of equally-spaced har-monic partials with the same weight. The optimal valueof the F0 can be estimated such that the likelihood givenby Eq. (15) is maximized. This technique of F0 estimationhas a potential to solve the limitation of DAP.3. PROPOSED MODELThis section presents a nonparametric Bayesian approachcalledinﬁnite superimposed discrete all-pole(iSDAP) mod-eling for source-ﬁlter decomposition of wavelet spectro-grams. Our model can estimate multiple F0s at each frameand discover several kinds of instrument timbres (all-polespectral envelopes) from polyphonic music audio signals.To achieve this, we integrate the technique of discrete all-pole (DAP) modeling [8] into the framework of compositeautoregressive (CAR) modeling [17, 26] in a probabilisticmanner. The iSDAP model can be regarded as a Bayesianextension of log-frequency source-ﬁlter NMF based on asingle ﬁlter [19], and has all of the following features:1.Superimposed DAP modeling:O u rm o d e lc a ne s t i -mate the spectral envelope of each of harmonic struc-ture contained in mixed sounds. The original DAPmodel can deal with only isolated sounds [8].2.Precise F0 modeling: Each frame is allowed to con-tain a unique set of F0s (sources) for capturing ﬁne88 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015ﬂuctuations of F0s (e.g.,v i b r a t o ) .T h eo r i g i n a lC A Rmodels [17,26] assume that a common set of sourcespectra (semitone-level F0s) is shared over all frames.3.Log-frequency modeling:O u r s o u r c e - ﬁ l t e r m o d e lcan deal with wavelet spectrograms that suit the char-acteristics of human audition by leveraging an ad-vantage of DAP modeling that only discrete frequen-cies are required for spectral envelope estimation.4.Bayesian nonparametrics:O u rm o d e lc a ne s t i m a t eeffective numbers of sources and ﬁlters according toa given spectrogram by allowing unbounded (inﬁnitein theory) numbers of sources and ﬁlters to be used.3.1 Model FormulationWe explain a probabilistic model of iSDAP. LetXbe anM⇥Nlog-frequency amplitude spectrogram withMfre-quency bins andNframes. The nonnegative matrixXisfactorized in a similar way to Eq. (10) as follows:Xmn⇠Poisson0@I!1Xi=1J!1Xj=1✓ni\u0000jWnijmHnij1A,(16)where✓niis the local weight of sourceiat framen,\u0000jis the global weight of ﬁlterj,a n dHnijis the gain of apair of sourceiand ﬁlterjat framen.{Wnijm}Mm=1isthe amplitude spectrum derived from the source-ﬁlter pairat framen.N o t e t h a t✓niandWnijmare allowed to varyover time to represent the F0 ﬂuctuation unlike Eq. (10).We aim to perform sparse learning of weight vectors✓n=[✓n1,···,✓nI]Tand\u0000=[\u00001,···,\u0000J]Twhen the numberof sourcesIand the number of ﬁltersJgo to inﬁnity.3.1.1 Parametric FunctionsAs shown in Figure 4, we force the amplitude spectrum{Wnijm}Mm=1to have a harmonic structure as follows:Wnijm=RXr=1SmnirFnijr,(17)whereRis the number of harmonic partials and{Smnir}Mm=1is the monomodal spectrum of ther-th harmonic partial ofsourceiat framengiven bySmnir=e x p✓\u000012\u00002(fm\u0000(µni+1 2 0 0l o g2r))2◆,(18)whereµniis the F0 [cents] of sourceiat framen,fmis thelog-frequency [cents] corresponding to them-th bin, and\u00002indicates energy diffusion around harmonic partials.We then represent the weights of discrete harmonic par-tials,{Fnijr}Rr=1,b yu s i n ga na l l - p o l et r a n s f e rf u n c t i o ni nthe log frequency domain as follows:Fnijr=1\u0000\u0000\u0000PPp=0ajpe\u0000!nirpi\u0000\u0000\u0000=\u0000aTjU(!nir)aj\u0000\u000012,(19)whereaj⌘[aj0,···,ajP]T,!niris a normalized fre-quency [rad] corresponding to ther-th harmonic partial ofsourceiat framen,a n dU(!)is a(P+1)⇥(P+1)matrixwith[U(!)]pq=c o s (!(p\u0000q)).N o t et h a tFnijrindicatesthe value of amplitude (not power). The Poisson likelihood-PH\u000eGSFRVFODZ\u0001<DFOUT>\"NQMJUVEFɾɾɾɾɾɾ5IF\u0001BMM\u000eQPMF\u0001TQFDUSBM\u0001FOWFMPQF\u0001EFUFSNJOFT\u0001UIF\u0001XFJHIUT\u0001PG\u0001IBSNPOJD\u0001QBSUJBMTFigure 4. Composition of sourceiand ﬁlterjat framenin the log-frequency domain.(KL-NMF) is considered to ﬁt the amplitude domain ratherthan the power domain [19].3.1.2 Prior DistributionsWe put gamma process (GaP) priors on inﬁnite-dimensionalvectors✓nand\u0000as in [15, 26]. Speciﬁcally, we put inde-pendent gamma priors on elements of✓nand\u0000as follows:✓ni⇠Gamma⇣↵✓I,↵✓⌘,\u0000j⇠Gamma⇣↵\u0000J,↵\u0000⌘,(20)where↵✓and↵\u0000are hyperparameters called concentrationparameters. AsJdiverges to inﬁnity, the vector\u0000approx-imates an inﬁnite vector drawn from a GaP with↵\u0000.I t i sproven that the effective number of ﬁlters,J+,s u c ht h a t\u0000j>✏for some number✏>0is almost surely ﬁnite [15].IfJis sufﬁciently larger than↵\u0000(Jis often called a trun-cation level in weak-limit approximation to inﬁnite model-ing), the GaP can be well approximated. The same reason-ing can be applied to the GaP on✓n.O n t h e o t h e r h a n d ,we put a standard Gamma prior onHnijas follows:Hnij⇠Gamma(aH,bH),(21)whereaHandbHare shape and rate hyperparameters.3.2 Variational InferenceThe posterior over random variablesp(✓,\u0000,H|X;µ,a)and parametersµandaare determined such that alowerboundLof the log-evidencelogp(X;µ,a)is maximized.Since this cannot be analytically computed, we use an ap-proximate method called variational Bayes (VB), which re-stricts the posterior to a factorized form given byq(✓,\u0000,H)=Yniq(✓ni)Yjq(\u0000j)Ynijq(Hnij).(22)Iteratively updating this posterior, we can monotonicallyincrease a lower bound of the log-evidence given bylogp(X;µ,a)\u0000E[logp(X|✓,\u0000,H;µ,a)]+E[logp(✓)] +E[logp(\u0000)] +E[logp(H)]\u0000E[logq(✓)]\u0000E[logq(\u0000)]\u0000E[logq(H)]⌘L0,(23)where the ﬁrst term can be further lower bounded by Jensen’sinequality on the concave logarithmic function as follows:E[logp(X|✓,\u0000,H;µ,a)]=PmnXmnEhlogPijr✓ni\u0000jSmnirFnijrHniji\u0000PmnijrE⇥✓ni\u0000jSmnirFnijrHnij⇤+const.\u0000Pmnijr\u0000mnijrXmnEhlog✓ni\u0000jSmnirFnijrHnij\u0000mnijri\u0000PmnijrE⇥✓ni\u0000jSmnirFnijrHnij⇤+const. (24)Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 89where\u0000mnijris a normalized auxiliary variable such thatPijr\u0000mnijr=1.T h ee q u a l i t yh o l d s(i.e., the lower boundofL0is maximized) if and only if\u0000mnijr/exp(E[log(✓ni\u0000jSmnirFnijrHnij)]).(25)Using Eq. (24), the objective function of our model to bemaximized,L, is obtained as the lower bound ofL0.F o rconvenience, we deﬁneXmnijrandYmnijrasXmnijr=\u0000mnijrXmn,(26)Ymnijr=E[✓ni\u0000jSmnirFnijrHnij].(27)3.3 Variational Bayesian Updating of✓,\u0000,a n dHThe VB updating rules are given byq(✓)/exp(Eq(\u0000,H)[logp(X,✓,\u0000,H;µ,a)]),q(\u0000)/exp(Eq(✓,H)[logp(X,✓,\u0000,H;µ,a)]),(28)q(H)/exp(Eq(✓,\u0000)[logp(X,✓,\u0000,H;µ,a)]).The variational posterior of each random variable is set tobe the same family as its prior distribution as follows:q(✓ni)=Gamma(a✓ni,b✓ni),q(\u0000j)=Gamma(a\u0000j,b\u0000j),q(Hnij)=Gamma(aHnij,bHnij).(29)The variational parameters are given bya✓ni=↵✓I+XmjrXmnijr,b✓ni=↵✓+XmjrE[\u0000jHnij]Wnijm,a\u0000j=↵\u0000J+XmnirXmnijr,b\u0000j=↵\u0000+XmnirE[✓niHnij]Wnijm,aHnij=aH+XmrXmnijr,bHnij=bH+XmrE[✓ni\u0000j]Wnijm.To estimate the effective number of ﬁltersJ+,w ep e r -form sparse learning. IfE[\u0000j]becomes sufﬁciently smallfor some ﬁlterj,w ed e g e n e r a t ei ta n dJ J\u00001.As i m i -lar treatment is applied toE[✓ni].T h ep r o p o s e dv a r i a t i o n a lalgorithm is gradually accelerated per iteration.3.4 Multiplicative Updating ofµandaTo estimate parametersµanda,w eu s et h em u l t i p l i c a t i v eupdate (MU) algorithm as in [1,14]. In general, to optimizex,w er e p r e s e n tt h ep a r t i a ld e r i v a t i v eo fa“ c o s t ”f u n c t i o nCwith respect toxas the difference of two positive terms,i.e.,@C@x=R\u0000R0.A nu p d a t i n gr u l eo fxis then given byx R0Rx.N o t et h a txbecomes constant if the derivative iszero, and is updated in the opposite direction of the deriva-tive. In this study the cost function is the negative lowerbound of the log-evidence,\u0000L.First, we represent the partial derivative of\u0000Lwith re-spect toµmias\u0000@L@µni=Rni\u0000R0ni,w h e r eRniandR0niarepositive terms given byRni=Xmjr(µni+1 2 0 0l o g2r)Xmnijr+fmYmnijr,(30)R0ni=XmjrfmXmnijr+(µni+1 2 0 0l o g2r)Ymnijr,(31)The updating rule ofµniis given byµni R\u00001niR0niµni.(32)As in [1, 14], we then represent the partial derivative of\u0000Lwith respect toajas\u0000@L@aj=(Rj\u0000R0j)aj,w h e r eRjandR0jare positive deﬁnite matrices given byRj=XmnirXmnijrF2nijrU(!nir),(33)R0j=XmnirYmnijrF2nijrU(!nir).(34)The updating rule ofajis given byaj R\u00001jR0jaj.(35)Finally, we forcibly adjust the scale of the ﬁlterFnijrsuchthat↵j0=1for normalizing the ﬁlter. Although this stepviolates the convergence of the optimization algorithm, itwas empirically found to work well.3.5 Binary Piano-roll EstimationTo perform multipitch analysis,i.e., make a binary piano-roll representation, we need to judge the existence of eachsemitone-level pitch at each frame. Using a trained model,we calculate an activation matrixV={Vkn}88,Nk=1,n=1overpitchkand framen(continuous-valued piano-roll repre-sentatione.g.,t h em i d d l eﬁ g u r eo fF i g u r e5 )b ya c c u m u -lating the expected amplitude of the ﬁrst partial of sourcei,PjE[✓ni\u0000jFnij1Hnij],i n t oVknindicated byµni.F i -nally, the activation matrixVis normalized such that allthe elements sum to unity,i.e.,PknVkn=1.There are several approaches to binary piano-roll esti-mation. The common approach is to make a binary deci-sion based on a threshold⌘.A n o t h e ra p p r o a c hi st od e ﬁ n eah i d d e nM a r k o vm o d e l( H M M )a n du s et h eV i t e r b i - s e a r c halgorithm for estimating a sequence of hidden binary states{Zkn}Nn=1from a sequence of pitch-existence likelihoods{Vpkn}Nn=1for each pitchk,w h e r epcontrols the dynamicrange. In our implementation,p=0.2and the transitionmatrix is[0.8,0.2; 0.01,0.99]in the Matlab notation.4. EVALUATIONWe report comparative experiments that were conductedfor evaluating the performance of the iSDAP model in mul-tipitch analysis of piano music. Since the proposed modelassumes that input mixture signals contain only harmonicsounds, we also tested the use of harmonic and percussivesource separation (HPSS) [12] as a preprocessor.4.1 Experimental ConditionsWe used 30 pieces (labeled as ”ENSTDkCl”) selected fromthe MAPS database [9] that contain stereo signals sam-pled at 44.1 [kHz]. The audio signals were converted tomonaural signals and truncated to 30 [s] from the begin-ning as in [2, 4, 21, 22, 24]. The amplitude spectrogram ofeach piece over the frequency bins ranging from 0 [cents](16.325 [Hz]) to 12000 [cents] (16717 [Hz]) was obtainedby performing the wavelet transform with a Gabor wavelet,af r e q u e n c yi n t e r v a lo f1 0[ c e n t s ] ,a n das h i f t i n gi n t e r v a lof 10 [ms],i.e.,M=1 2 0 0andN=3 0 0 0.T h e o t h e rquantities wereI=8 8,J=3,R=2 0,P=1 3,a n d\u0000=2 5.T h e p r i o r s w e r e s e t t o b e l e s s i n f o r m a t i v e ,i.e.,90 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015\u0013\u0012\u0012\u0011\u00190CTFSWBUJPO\u00013FDPOTUSVDUJPO\u00011JUDI\u0001BDUJWBUJPO'JOBM\u0001PVUQVU\u0001(SPVOE\u0001USVUI<DFOUT>\u0011\u0013\u0015\u0011\u0011\u0015\u0019\u0011\u0011\u0018\u0013\u0011\u0011\u001a\u0017\u0011\u0011\u0012\u0013\u0011\u0011\u0011\u0011\u0013\u0015\u0011\u0011\u0015\u0019\u0011\u0011\u0018\u0013\u0011\u0011\u001a\u0017\u0011\u0011\n\u0013\u0012\u0012\u0011\u0019\n\u0013\u0012\u0012\u0011\u0019\u0012\u0013\u0011\u0011\u0011\n<.*%*\u0001OPUF\u0001OVNCFS>\n\u0014\u0011\u0001<T>\u0013\u0016\u0013\u0011\u0012\u0016\u0012\u0011\u0016'\u000eNFBTVSF\u001b\u0001\u0017\u001a\u000f\u0015\u0006Figure 5.A n a l y s i so fM U S - m z3333ENSTDkCl.↵✓=↵\u0000=aH=1,a n dbH=Eemp[Xmn]\u00001.S i n c eXcontained only piano sounds, the truncation levelJ=3worked well (two ﬁlters were degenerated in this experi-ment,i.e.,J+=1). The values of{µni}Ii=1were initial-ized as the frequencies corresponding to the 88 keys rang-ing from 900 [cents] to 9600 [cents]. The value of each↵jp(1pP)was drawn from a Gaussian with a zeromean and a small variance of 0.01. The variational poste-riors were initialized as the corresponding priors.The proposed model was tested under possible combi-nations of preprocessing (with or without HPSS) and post-processing (thresholding or Viterbi decoding). HPSS wasperformed in the log-frequency domain. The model with asingle ﬁlter (J=J+=1)w a sa l s ot e s t e di nas u p e r v i s e dsetting. A set of ﬁlter coefﬁcientsa1was pretrained from264 isolated sounds of the same or different piano (ENST-DkCl in a closed test or SptkBGCl in an open test) by usingLPC, and kept constant during multipitch analysis.The estimation results were evaluated in terms of theframe-level recall/precision rates and F-measure as in [24]:R=PncnPnrn,P=PncnPnen,F=2RPR+P,(36)wherern,en,a n dcnare the numbers of ground truth, es-timated and correct pitches on framen,r e s p e c t i v e l y . T h ethreshold⌘was determined as⌘=1 0\u00001.3without HPSSand⌘=1 0\u00001.5with HPSS.4.2 Experimental ResultsThe experimental results shown in Figure 5 and Table 1 in-dicate the great potential of the iSDAP model. The modelsupervised in the open condition (67.3%) signiﬁcantly out-performed the iCAR model formulated in the linear fre-quency domain (48.4%) [26] and tied with the state-of-the-art methods,e.g.,h a r m o n i cN M F( 6 7 . 7 % )[ 2 4 ] ,N M Fwith group sparsity (71.3%) [21], and NMF with HellingerFilter learning HPSS HMMRPFUnsupervised 55.3 57.9 56.6362.2 60.2 61.2362.4 64.3 63.43367.4 64.2 65.8Supervised362.4 67.0 64.4(open test)3369.9 64.5 67.3Supervised359.4 69.1 63.9(close test)3367.4 67.8 67.6Table 1.E x p e r i m e n t a lr e s u l t so fm u l t i p i t c ha n a l y s i sf o r3 0piano pieces labeled as ENSTDkCl.\n\u0012\u0013\u0011\u0011\u0011\u001a\u0017\u0011\u0011\u0018\u0013\u0011\u0011\u0015\u0019\u0011\u0011\u0013\u0015\u0011\u0011\u0011\u0012\u0015\u0011\u0012\u0013\u0011\u0019\u0011\u0012\u0011\u0011\u0017\u0011\u0015\u0011\u0013\u0011\u000e\u0013\u0011\u0011<E#>\n<DFOUT>\"MM\u000eQPMF\u0001GJMUFS\u0001MFBSOFE\u0001GSPN\u0001l&/4,%L$Mz\u0001JTPMBUFE\u0001QJBOP\u0001TPVOET\"MM\u000eQPMF\u0001GJMUFS\u0001MFBSOFE\u0001GSPN\u0001l4UQL#($Mz\u0001JTPMBUFE\u0001QJBOP\u0001TPVOET\"MM\u000eQPMF\u0001GJMUFS\u0001MFBSOFE\u0001GSPNl.64\u000eN[@\u0014\u0014\u0014@\u0014@&/45%L$Mz\u0001EBUB\tTVQFSJNQPTFE\u0001QJBOP\u0001TPVOET\nFigure 6.A l l - p o l eﬁ l t e r sl e a r n e df r o mi s o l a t e ds o u n d so rapiano piece (mixed sounds) in the log-frequency domain.sparse coding (66.5%) [22]). While many recent methodsneed to pretrain a dictionary of basis spectra for reasonabledecomposition [2,4,21,24], our model works well (65.8%)even in the completely unsupervised condition. As shownin Figure 6, a ﬁlter learned from a music signal droppedfaster than the pretrained ﬁlters because the model failed tocapture higher-order overtones even in the log-frequencydomain due to the strong inharmonicity of piano sounds.Nonetheless, the learned ﬁlter acted as an effective con-straint on the relative weights of harmonic partials.There would be much room for improving the perfor-mance. KL-NMF [18] and IS-NMF [10] are special casesof\u0000-divergence NMF [11, 20] with\u0000=1,0,r e s p e c t i v e l y .It was reported that the use of an intermediate divergencewith\u0000=0.5signiﬁcantly improves the performance byabout 5% [24]. Similar ﬁndings were reported in the con-text of source separation [13]. This calls for the use of theTweedie likelihood instead of the Poisson likelihood [6].5. CONCLUSIONWe presented a new nonparametric Bayesian approach tosource-ﬁlter NMF called inﬁnite superimposed discrete all-pole (iSDAP) modeling that can decompose awaveletspec-trogram into three kinds of factors,i.e.,h a r m o n i cs o u r c e s ,all-pole ﬁlters, and time-varying gains of source-ﬁlter pairs.Our model clearly outperformed its counterpart called theiCAR model formulated in the linear frequency domain.One important research direction is to build a uniﬁed modelof harmonic and percussive sounds. To bridge the gap be-tween multipitch analysis and music transcription, we planto incorporate a prior distribution on the time-frequencypositions of musical notes into a Bayesian framework.Acknowledgment:This study was partially supported by JSTOngaCREST Project, JSPS KAKENHI 24220006, 26700020,and 26280089, and Kayamori Foundation.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 916. REFERENCES[1] R. Badeau and A. Ozerov. Multiplicative updates formodeling mixtures of non-stationary signals in thetime-frequency domain. InEuropean Signal Process-ing Conference (EUSIPCO),2 0 1 3 .[2] E. Benetos, R. Badeau, T. Weyde, and G. Richard.Template adaptation for improving automatic musictranscription. InInternational Society for Music Infor-mation Retrieval Conf. (ISMIR),p a g e s1 7 5 – 1 8 0 ,2 0 1 4 .[3] N. J. Bryan, G. Mysore, and G. Wang. Source sep-aration of polyphonic music with interactive user-feedback on a piano roll display. InInternational So-ciety for Music Information Retrieval Conference (IS-MIR),p a g e s1 1 9 – 1 2 4 ,2 0 1 3 .[4] J. J. Carabias-Orti, T. Virtanen, P. Vera-Candeas,N. Ruiz-Reyes, and F. J. Ca˜nadas-Quesada. Musi-cal instrument sound multi-excitation model for non-negative spectrogram factorization.IEEE Journal ofSelected Topics in Signal Proc.,5 ( 6 ) : 1 1 4 4 – 1 1 5 8 ,2 0 1 1 .[5] A. T. Cemgil. Bayesian inference for nonnegative ma-trix factorisation models.Computational Intelligenceand Neuroscience,2 0 0 9 : A r t i c l eI D7 8 5 1 5 2 ,2 0 0 9 .[6] U. S¸ims¸ekli, A. Cemgil, and Y . K. Yılmaz. Learningthe\u0000-divergence in Tweedie compound Poisson matrixfactorization models. InInternational Conference onMachine Learning (ICML),p a g e s1 4 0 9 – 1 4 1 7 ,2 0 1 3 .[7] J.-L. Durrieu, G. Richard, B. David, and C. F´evotte.Source/ﬁlter model for unsupervised main melody ex-traction from polyphonic audio signals.IEEE Trans-actions on Audio, Speech, and Language Processing,18(3):564–575, 2010.[8] A. El-Jaroudi and J. Makhoul. Discrete all-polemodeling.IEEE Transactions on Signal Processing,39(2):411–423, 1991.[9] V. Emiya, R. Badeau, and B. David. Multipitch esti-mation of piano sounds using a new probabilistic spec-tral smoothness principle.IEEE Transactions on Au-dio, Speech and Language Processing,1 8 ( 6 ) : 1 6 4 3 –1654, 2010.[10] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegativematrix factorization with the Itakura-Saito divergence:With application to music analysis.Neural Computa-tion,2 1 ( 3 ) : 7 9 3 – 8 3 0 ,2 0 0 9 .[11] C. F´evotte and J. Idier. Algorithms for nonnegativematrix factorization with the beta-divergence.NeuralComputation,2 3 ( 9 ) : 2 4 2 1 – 2 4 5 6 ,2 0 1 1 .[12] D. FitzGerald. Harmonic/percussive separation usingmedian ﬁltering. InInternational Conference on Digi-tal Audio Effects (DAFx),2 0 1 0 .[13] D. FitzGerald, M. Cranitch, and E. Coyle. On the useof the beta divergence for musical source separation. InIrish Signals and Systems Conf.,p a g e s1 – 6 ,2 0 0 8 .[14] R. Hennequin, R. Badeau, and B. David. NMF withtime-frequency activations to model nonstationary au-dio events.IEEE Transactions on Audio, Speech, andLanguage Processing,1 9 ( 4 ) : 7 4 4 – 7 5 3 ,2 0 1 1 .[15] M. Hoffman, D. Blei, and P. Cook. Bayesian nonpara-metric matrix factorization for recorded music. InIn-ternational Conference on Machine Learning (ICML),pages 439–446, 2010.[16] F. Itakura and S. Saito. Analysis synthesis telephonybased on the maximum likelihood method. InInterna-tional Congress on Acoustics (ICA),p a g e sC 1 7 – C 2 0 ,1968.[17] H. Kameoka and K. Kashino. Composite autoregres-sive system for sparse source-ﬁlter representation ofspeech. InIEEE International Symposium on Circuitsand Systems (ISCAS),p a g e s2 4 7 7 – 2 4 8 0 ,2 0 0 9 .[18] D. Lee and H. Seung. Algorithms for non-negativematrix factorization. InNeural Information ProcessingSystems (NIPS),p a g e s5 5 6 – 5 6 2 ,2 0 0 0 .[19] T. Nakamura, K. Shikata, N. Takamune, andH. Kameoka. Harmonic-temporal factor decomposi-tion incorporating music prior information for in-formed monaural source separation. InInternationalSociety for Music Information Retrieval Conference(ISMIR),p a g e s6 2 3 – 6 2 8 ,2 0 1 4 .[20] M. Nakano, H. Kameoka, J. Le Roux, Y. Kitano,N. Ono, and S. Sagayama. Convergence-guaranteedmultiplicative algorithms for non-negative matrix fac-torization with beta divergence. InInternational Work-shop on Machine Learning for Signal Processing(MLSP),p a g e s2 8 3 – 2 8 8 ,2 0 1 0 .[21] K. O’Hanlon and M. D. Plumbley. Polyphonic pianotranscription using non-negative matrix factorisationwith group sparsity. InInternational Conference onAcoustics, Speech, and Signal Processing (ICASSP),pages 2214–2218, 2014.[22] K. O’Hanlon, M. Sandler, and M. D. Plumbley. Ma-trix factorisation incorporating greedy Hellinger sparsecoding applied to polyphonic music transcription. InInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP),p a g e s3 1 1 2 – 3 1 1 6 ,2 0 1 5 .[23] P. Smaragdis, C. F´evotte, G. Mysore, N. Moham-madiha, and M. Hoffman. Dynamic source separationusing nonnegative factorizations: A uniﬁed view.IEEESignal Processing Magazine,3 1 ( 3 ) : 6 6 – 7 5 ,2 0 1 4 .[24] E. Vincent, N. Bertin, and R. Badeau. Adaptive har-monic spectral decomposition for multiple pitch esti-mation.IEEE Transactions on Audio, Speech and Lan-guage Processing,1 8 ( 3 ) : 5 2 8 – 5 3 7 ,2 0 1 0 .[25] T. Virtanen and A. Klapuri. Analysis of polyphonic au-dio using source-ﬁlter model and non-negative matrixfactorization. InNIPS Workshop on Advances in Mod-els for Acoustic Processing,2 0 0 9 .[26] K. Yoshii and M. Goto. Inﬁnite composite autoregres-sive models for music signal analysis. InInternationalSociety for Music Information Retrieval Conference(ISMIR),p a g e s7 9 – 8 4 ,2 0 1 2 .92 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Conceptual Blending in Music Cadences: A Formal Model and Subjective Evaluation.",
        "author": [
            "Asterios I. Zacharakis",
            "Maximos A. Kaliakatsos-Papakostas",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416056",
        "url": "https://doi.org/10.5281/zenodo.1416056",
        "ee": "https://zenodo.org/records/1416056/files/ZacharakisKC15.pdf",
        "abstract": "Conceptual blending is a cognitive theory whereby ele- ments from diverse, but structurally-related, mental spaces are ‘blended’ giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorith- mic formalisation for conceptual blending applied to har- monic concepts. More specifically, it investigates the abil- ity of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fun- damental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected re- lationships between the produced blends. Then, a prelimi- nary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spa- tial configurations for both behavioural data and dissimi- larity estimations by the algorithm. Comparison of the two configurations showed that the system is capable of mak- ing fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaning- ful blends based on self-evaluation of its outcome.",
        "zenodo_id": 1416056,
        "dblp_key": "conf/ismir/ZacharakisKC15",
        "keywords": [
            "conceptual blending",
            "cognitive theory",
            "elements blending",
            "mental spaces",
            "harmonic concepts",
            "algorithmic formalisation",
            "harmonic cadences",
            "subjective evaluation",
            "pairwise dissimilarity listening test",
            "multidimensional scaling analysis"
        ],
        "content": "CONCEPTUAL BLENDING IN MUSIC CADENCES: A FORMAL MODELAND SUBJECTIVE EVALUATION.Asterios ZacharakisSchool of Music Studies,Aristotle University ofThessaloniki, Greeceaszachar@mus.auth.grMaximos Kaliakatsos-PapakostasSchool of Music Studies,Aristotle University ofThessaloniki, Greecemaxk@mus.auth.grEmilios CambouropoulosSchool of Music Studies,Aristotle University ofThessaloniki, Greeceemilios@mus.auth.grABSTRACTConceptual blending is a cognitive theory whereby ele-ments from diverse, but structurally-related, mental spacesare ‘blended’ giving rise to new conceptual spaces. Thisstudy focuses on structural blending utilising an algorith-mic formalisation for conceptual blending applied to har-monic concepts. More speciﬁcally, it investigates the abil-ity of the system to produce meaningful blends betweenharmonic cadences, which arguably constitute the most fun-damental harmonic concept. The system creates a varietyof blends combining elements of the penultimate chords oftwo input cadences and it further estimates the expected re-lationships between the produced blends. Then, a prelimi-nary subjective evaluation of the proposed blending systemis presented. A pairwise dissimilarity listening test wasconducted using original and blended cadences as stimuli.Subsequent multidimensional scaling analysis produced spa-tial conﬁgurations for both behavioural data and dissimi-larity estimations by the algorithm. Comparison of the twoconﬁgurations showed that the system is capable of mak-ing fair predictions of the perceived dissimilarities betweenthe blended cadences. This implies that this conceptualblending approach is able to create perceptually meaning-ful blends based on self-evaluation of its outcome.1. INTRODUCTIONConceptual blending is a cognitive theory developed byFauconier and Turner [8] whereby elements from diverse,but structurally-related, mental spaces are ‘blended’ givingrise to new conceptual spaces that often possess new pow-erful interpretative properties allowing better understand-ing of known concepts or the emergence of novel conceptsaltogether. The general framework within which the cur-rent work is placed, comprises a formal model for concep-tual blending [7] based on Goguen’s initial ideas of a Uni-ﬁed Concept Theory [9, 18]. This model incorporates im-c\u0000Asterios Zacharakis, Maximos Kaliakatsos-Papakostas,Emilios Cambouropoulos.Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0).Attribution:Asterios Zacharakis, MaximosKaliakatsos-Papakostas, Emilios Cambouropoulos. “Conceptual Blend-ing in Music Cadences: A formal model and subjective evaluation.”, 16thInternational Society for Music Information Retrieval Conference, 2015.portant interdisciplinary research advances from cognitivescience, artiﬁcial intelligence, formal methods and com-putational creativity. To substantiate its potential, a proof-of-concept autonomous computational creative system thatperforms melodic harmonisation is being developed.Musical meaning is to a large extent self-referential;themes, motives, rhythmic patterns, harmonic progressionsand so on emerge via self-reference rather than externalreference to non-musical concepts. Since musical mean-ing largely relies on structure and since conceptual blend-ing involves mapping between different conceptual struc-tures, music seems to be an ideal domain for conceptualblending (musical cross-domain blending is discussed byAntovi´c [1], Cook [6], Zbikowski [24]). Indeed, structuralconceptual blending is omnipresent in music making: fromindividual pieces harmoniously combining music charac-teristics of different pieces/styles, to entire musical styleshaving emerged as a result of blending between diversemusic idioms.Suppose we have a basic tonal ontology where onlydiatonic notes are allowed and dissonances in chords aremostly forbidden (except possibly using minor 7th inter-vals as in the dominant seventh chord). We assume thatsome basic cadences have been established as salient har-monic functions around which the harmonic language ofthe idiom(s) has been developed, such as the authentic/per-fect cadence, the half cadence, the plagal cadence and,even, older 15th century modal cadences such as the Phry-gian cadence (Figure 1). The main question to be ad-dressed is the following: Is it possible for a computationalsystem to enrich its learned tonal ontology by inventing‘new’ meaningful cadences based on blending betweenknown cadences?\nFigure 1: Conceptual blending between the basic perfectand Phrygian cadences gives rise to the Tritone Substitu-tion progression/cadence. The Backdoor progression canalso be derived as a weaker blend since less attributes ofthe two input spaces are retained leading to a lower ratingby the system.141Figure 1 presents a conceptual blending example wherethe perfect and the 15th century modal Phrygian cadencesare used as input spaces. These have been chosen in thisexample as they are both ﬁnal cadences to the tonic andat the same time, they are very different (i.e. the Phry-gian mode does not have an upward leading note to thetonic but rather a downward ‘leading note’ from the IIbto the I). Initially, these cadences are formally describedas simply pitch classes with reference to a tonal centre (Ctonality was adopted in this case). Assuming that the ﬁ-nal chord is always a common tonic chord, blending takesplace by combining pitches of the penultimate chords be-tween different cadences. Rather than mere combinationof pitches, other characteristic attributes of a cadence arealso taken into account. Weights/priorities that reﬂect rel-ative prominence (e.g., root, upwards or downwards lead-ing note, dissonant note that requires resolution – see Fig-ure 1 where lines of variable thickness illustrate relativestrength of voice-leading connections in cadences) are as-signed to each chord note according to a human expert.The ‘blended’ penultimate chord is also constrained tocomply with a certain chord type such as the major or mi-nor chord (in this instance the characteristic major chordwith minor seventh was preferred).Let us examine the particular blending example be-tween the perfect and the Phrygian cadences more closely.Notes from the two input penultimate chords of the twocadence types create a large number of possible combina-tions. We start with combinations (at least 3 notes) of thehighest priority/salience notes (notes connected with boldlines in Figure 1). Many of these combinations are nottriadic chords and may be ﬁltered out using a set of con-straints (in this instance our constraint is to have a chordthat is a standard characteristic tonal chord such as a dom-inant seventh), while also a note completion step mightbe required (adding notes to incomplete blending results)if the examined combination incorporates too few notes;more details regarding constraints will be given in sub-section 2.1. Among the accepted blends, the most highlyrated one based on priorities values is the tritone substi-tution progression (IIb7-I) of jazz harmony. This simpleblending mechanism ‘invents’ a chord progression that em-bodies characteristics of the Phrygian cadence (root/bassdownward motion by semitone) and the dominant seventhchord (resolution of tritone). Thus, it creates a new har-monic ‘concept’ that was actually introduced in jazz, cen-turies later than the original input cadences. The BackdoorProgression also appears in the potential blends albeit withmuch lower priority (i.e. a much weaker blend) – see Fig-ure 1. A number of other applications and uses of harmonicblending [11] and, more speciﬁcally, chord blending arereported in [7].Following the above, a challenging question that needsto be addressed concerns the evaluation of the outcomeproduced by such a creative system. The mere deﬁnitionof creativity is problematic and not commonly accepted asmany authors approach it from different perspectives (e.g.,[3,5,17,21], for a comprehensive discussion see [10]). Ap-plications of computational creativity to music pose theextra issue of aesthetic quality judgement since creativ-ity may not always be accompanied by aesthetic value andvice versa. In terms of assessing a creative system, thetwo usual approaches are to either directly evaluate the ﬁ-nal product or to evaluate the productive mechanism [16].The present work is concluded by an empirical experimentthat attempts to address the former by shedding some lighton how the system’s output is perceived leaving -for themoment- the issue of aesthetic value intact.To this end, the output of the cadence blending systemdescribed in the following section is used to set up a pre-liminary subjective evaluation of the conceptual blendingalgorithm applied to cadence invention. As stated previ-ously, the computational system is capable of creating a va-riety of blends combining elements of two input cadencesand it further estimates the expected relationships betweenthe produced blends. A number of blends between the per-fect and the Phrygian cadence were produced in order totest the ability of the cadence blending system to accu-rately predict their perceived relations (i.e. the function-ality of the blends) using an ‘objective’ distance metric(see subsection 2.2). To achieve this, a pairwise dissim-ilarity listening test for the nine cadences (two original,four blends and three miscellaneous) was designed andconducted. Subsequent multidimensional scaling (MDS)analysis was utilised to obtain geometric conﬁgurations forboth behaviourally acquired pairwise distances and dissim-ilarity estimations by the algorithm. Comparison of thetwo conﬁgurations showed that the system can model theperceptual space quite accurately.2. FORMAL CONCEPTUAL BLENDING MODELThis section begins with a description of the conceptualblending mechanism utilised by the system for cadenceconstruction. It then proceeds with a consideration of anaive distance metric for pairs of cadences based on repre-sentation of cadences according to the system.2.1 Cadence generation through chord blendingA cadence is described as a progression of (at least) twochords that conclude a phrase, section or piece of mu-sic [2]. In our case we have examined the simplest caseof two chords, a penultimate and a ﬁnal chord. If the ﬁnal– destination – chord is considered ﬁxed, then blending be-tween two cadences can occur by blending the penultimatechords of the cadences. The penultimate chords shouldtherefore be described in a way that reﬂects the ‘functional’role of their constitutive components. To this end, ‘chord-type’ properties of the penultimate chords (i.e. characteris-tics of type such as major, minor etc.) should be consideredin combination with ‘key-related’ characteristics (i.e. theirrelations to the ﬁnal chord). For instance, a ‘chord-type’and distinctive characteristic of the penultimate chord inthe perfect cadence (V7) is the fact that it includes a tri-tone (between the third and the minor seventh), while two‘key-related’ important characteristics are a) the fact that142 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015it includes the leading tone to the tonic (expressed as thepitch class 11 relative to the local key) and b) that its rootmoves by a perfect ﬁfth to the tonic. Additionally, the spec-iﬁcation of cadences (penultimate chords) should incorpo-rate priority values, taking into account the fact that not allcharacteristics (‘chord-type’ or ‘key-related’) are equallysalient.The blending framework employed in this paper forproducing novel cadences through concept blending hasbeen presented in [7]. This framework follows Goguen’sproposal to model conceptual spaces as algebraic spec-iﬁcations, while the utilised speciﬁcations deﬁned in avariant ofCommon Algebraic Speciﬁcation Language(CASL) [14] are extended with priority values associatedto axioms. These speciﬁcations incorporate symbols asbasic building blocks, over which more reﬁned speciﬁca-tions are constructed, beginning from the sort‘Note’thatis utilised to built the sort‘Chord’. The sortChordrep-resents the penultimate chord of the cadence which is infact the notion of the cadence as previously described. ANotecan receive values between 0 and 11, indicating the12 pitch classes. In addition, a ‘+’ operator is consideredfor arithmetics of addition in a modulo 12. For example,7+9=4denotes that a sixth plus a ﬁfth is a major third.AChordspeciﬁcation incorporates two kinds of attribu-tes that relate to the aforementioned ‘chord-type’ and ‘key-related’ attributes, respectively‘chordNote’and‘keyNote’.The‘chordNote’property indicates semitone distances be-tween the chord’s root and the notes comprising the chord,e.g., a major chord with minor seventh has the follow-ing relative notes:[0,4,7,10]. On the other hand, the‘keyNote’property indicates semitone distance betweenthe scale’s root note and the notes comprising the chord,e.g., a major chord with minor seventh and with chord rooton the ﬁfth degree of the major scale (i.e. pitch class7) hasthe following key-related notes:[7,11,2,5].The salient characteristics of penultimate chords, andin extension of cadences, are deﬁned for the two inputspaces by employing human knowledge1. The salienceof a penultimate chord property is input to the system as apriorityvalue which is then directly linked to this property.The output of conceptual blending, i.e. a conceptual blend,should incorporate the most salient features of the two in-put spaces – reﬂected by higher priority values. Additionalconstraints that concern further knowledge about chordsare imposed. For the system employed in this paper, pre-sented in more detail in [7], the additional constraints con-cern the facts that a chord should not have a major and aminor third (‘chordNote3 and 4) at the same time, it shouldnot have a minor second (‘chordNote’1) and it should nothave both a perfect and a diminished ﬁfth (‘chordNote’6and7) at the same time. When a newblendoid2emerges,these constraints are enforced in the form of aconsistency1In this study, for convenience, they are determined manually by amusic expert.2The termblendoidrefers to a possible result of blending, which, how-ever, is not necessarily consistent or optimal. Additional criteria eithervalidate or discard the consistency of a blendoid as well as evaluate it asoptimal (based on ‘blending optimality principles’ or on domain-speciﬁccharacteristics inherited to the blendoid).checkon the chord speciﬁcation. Thereby,inconsistentblends are discarded.The input cadences that have been selected to demon-strate blending of harmonic concepts were theperfectandthePhrygian, with their attributes and priorities depictedin Table 1. For both cadences, the highest priorities areassigned in such a way that the most musically salient as-pects of the penultimate chords are highlighted. For theperfect cadence, the most highlighted features include theleading note (keyNote: 11) to the tonic and the fact that itstype includes a tritone (chordNote: 4 andchordNote: 10).For the Phrygian cadence, the musically salient feature isthe descending leading note (keyNote: 1) to the tonic.perfect Phrygianattribute priority attribute prioritykeyNote:7p:2keyNote: 10p:1keyNote: 11p:3keyNote:1p:3keyNote:2p:1keyNote:5p:2keyNote:5p:2chordNote:0p:1chordNote:0p:1chordNote:4p:3chordNote:3p:1chordNote:7p:1chordNote:7p:1chordNote: 10p:3Table 1: Attributes and priorities (higher values indicatehigher priority) considered in the blending system for theinput penultimate chords in the perfect and Phrygian ca-dences. Common attributes of both cadences (the genericspace [7]) appear in boxes.tritone backdoorattribute priority attribute prioritykeyNote:1p:3keyNote: 10p:1keyNote: 11p:3keyNote:2p:1keyNote:5p:2keyNote:5p:2keyNote:8p:1 keyNote:8p:1chordNote:0p:1chordNote:0p:1chordNote:4p:3chordNote:4p:3chordNote:7p:1chordNote:7p:1chordNote: 10p:3chordNote: 10p:3Table 2: Attributes and priorities (higher values indicatehigher priority) in the tritone substitution and backdoor ca-dences that result as blends from the perfect and Phrygiancadences. The completion step adds thekeyNote:8.The computational chord blending framework com-bines thesalienceof chord features and core ideas of thenotion ofAmalgams[15], resulting in a process that itera-tively produces blendoids with descending salience in theircharacteristics. However, the produced blendoids poten-tially require completion, i.e. additional reasoning mech-anisms that ﬁll-in incomplete properties. Let us considerthe example of the tritone substitution cadence blend toelucidate the completion step, as demonstrated in Table 2.The tritone substitution cadence is acquired by preserv-ing the most salientkeyNoteattributes (with priority 3)from both input spaces:[1,5,11], and all thechordNoteattributes of the perfect cadence:[0,4,7,10]. However,Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 143the utilisation of the pitch classes[1,5,11]does not sat-isfy the requirements for a full dominant seventh chord oftype[0,4,7,10]. The completions step for the pilot studypresented in [7] is performed manually, although it is pos-sible to develop an automatic completion algorithm basedon the chord root provided by the utilisation of the Gen-eral Chord Type (GCT) [4] algorithm. For instance, inthe tritone substitution, pitch class 1 is assigned as a rootnote, a fact that leads to the completion of the pitch class(keyNote:) 8 as a perfect ﬁfth (to match thechordNote:7). The backdoor cadence preserves thekeyNoteattributesspaces:[2,5,10], which are not the ones with the highestpriorities, and again all thechordNoteattributes of the per-fect cadence:[0,4,7,10]. Similarly, the completion stepassigns the pitch class 10 as a root note, while the require-ment for a minor seventh (chordNote: 10) leads to im-porting the pitch class (keyNote:) 8 into the blend. Sinceno background knowledge about the role of the attributekeyNote: 8 is given, the ‘default’ priority 1 is inserted,which will also be the case for all the examples in this pa-per: if attributes emerge through completion that have notbeen modelled in the input spaces, the default priority 1 isassigned.2.2 Model-based distance metricA naive method to compute the distances between pairs ofcadences is by comparing their common features with theset of all their distinct features. In our case, since the ﬁnalchord is always the same minor tonic, the comparison boilsdown to the features of their penultimate chords. Thereby,the more features these chords have in common, the moresimilar the cadences should be. For two cadences,CiandCjtwo sets are considered: theintersection,\\(Ci,Cj),and theunion,[(Ci,Cj)of their penultimate chord fea-tures. The intersection is the set of their common featuresand the union is the sum of all the features appearing inboth cadences without repetitions. For instance, for the ca-dences indexed 1 and 3 in Table 3:\\(C1,C3)=[ [ 5,11],[0,4,7,10]],[(C1,C3)=[ [ 1,2,5,7,8,11],[0,4,7,10]].The considered distance based on the intersection andunion of the features of penultimate chords is computed bydividing the number of elements in the intersection withthe number of elements in the union. IfN(X)is the num-ber of elements in a setX, then the distance between twocadences is computed asd(Ci,Cj)=N(\\(Ci,Cj))N([(Ci,Cj)).In the aforementioned example,d(C1,C3)=6/10.3. EMPIRICAL EVALUATIONIn order to investigate the functionality of the blended ca-dences (i.e. the perceived relationships between them) weconducted a pairwise dissimilarity rating listening experi-ment using as stimuli the nine selected cadences describedbelow. This approach is widely adopted in psychoacousticsbecause it enables the construction of perceptual spacesby employing multidimensional scaling (MDS) analysis onthe obtained dissimilarity matrices.3.1 StimuliThe stimulus set consisted of the two input cadences (per-fect and Phrygian), four blends of the input spaces andthree miscellaneous cadences (Figure 2). More speciﬁ-cally, seven selected blends were as follows: blend 3 wasthe tritone substitution progression, blends 4 and 5 werethe backdoor progression (the latter without seventh), ca-dence 6 was a plagal cadence (it was input manually as acadence instance that was not a blend and was rather dif-ferent to the two input cadences), cadence 7 contained aminor dominant penultimate chord, cadence 8 was essen-tially a French-sixth chord-type (similar in principle to thetritone substitution) and cadence 9 was a manually con-structed non-blend chromatic chord. Note that all the ca-dences were assumed to be in C minor and each cadencewas preceeded by the notes C and F to reinforce perceptionof tonal context – the only chord that changed in each stim-ulus was the penultimate chord. Table 3 illustrates the ca-dences used in the subjective experiment with thekeyNoteandchordNotefeatures grouped in two arrays. Therefore,since the system is able to produce blended cadences ac-cording to these features (keyNoteandchordNote), the sim-ilarity between two cadences in terms of the system’s mod-elling should depend merely on them.\nFigure 2: Score annotation of the two input cadences (1-2), 4 blends of the input spaces (3-6) and 3 miscellaneouscadences (7-9).3.2 ParticipantsFifteen listeners (aged 19-48, mean age: 26.5, 8 female)participated in the listening test. All reported normal hear-ing and long term music practice (years on average: 18.7,range: 6 to 43). Participants were students in the Depart-ment of Music Studies of the Aristotle University of Thes-saloniki. All participants were naive about the purpose ofthe test.144 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015input blends miscellaneousindex 1 2 3 4 5 6 7 8 9keyNote[7,11,2,5] [10,1,5] [1,5,8,11] [10,2,5,8] [10,2,5] [2,5,9,0] [7,10,2] [1,5,7,11] [3,7,10,1]chordNote[0,4,7,10] [0,3,7] [0,4,7,10] [0,4,7,10] [0,4,7] [0,3,7,10] [0,3,7] [0,4,6,10] [0,4,7,10]Table 3: The penultimate cadence chords for the experiments along with their features and their respective indexes.3.3 ProcedureIn the pairwise dissimilarity listening test, participantswere asked to compare all the pairs among the nine soundstimulus set using the free magnitude estimation method[23]. Therefore, they rated the perceptual distances offorty-ﬁve pairs (same pairs included) by freely typing ina number of their choice to represent dissimilarity of eachpair (i.e., an unbounded scale) with 0 indicating a samepair.Listeners became familiar with the different cadencesduring an initial presentation of the stimulus set in randomorder. This was followed by a brief training stage wherelisteners rated four selected pairs of stimuli. For the mainpart of the experiment participants were allowed to listen toeach pair of sounds as many times as needed prior to sub-mitting their rating. The pairs were presented in randomorder and listeners were advised to retain a consistent rat-ing strategy throughout the experiment. In total, the listen-ing test sessions, including instructions and breaks, lastedaround twenty minutes for most of the participants.4. EXPERIMENTAL RESULTSThe proposed formal conceptual blending framework en-ables the generation of multiple cadences with differentvalues of ‘importance’, as reﬂected by the priorities of theattributes preserved into the penultimate chords of the re-sulting cadences. For the purpose of this study, the system-wise ‘objective’ distance metric between cadences (seesubsection 2.2) is merely based on the common featuresof the penultimate chords, not taking priority values intoaccount. The aim of this study is to examine whetherthe pairwise distances between several cadences, as ex-pressed by this ‘objective distance’ is aligned with the cog-nitive/perceptual distances that musically trained partici-pants assign.A non-metric, weighted individual differences scaling(INDSCAL) MDS analysis as offered by the SPSS PROX-SCAL (proximity scaling) algorithm [13] was applied tothe dissimilarity matrices. INDSCAL computes weightsthat represent the importance attributed to each perceptualdimension by each participant and then uses these weightsto reconstruct a common perceptual space. Additionally,the ‘ordinal’ option applies a rank ordering transforma-tion to the raw dissimilarities within each participant’s re-sponses. The non-metric approach was adopted since it hasbeen proven robust to the presence of monotonic transfor-mations or random error in the data [19, 22].A two-dimensional solution of the behavioural datawith the following goodness of ﬁt measures: Stress-I: .2281st dimension-1 0 12nd dimension-1.5-1-0.500.511.52123456\n789(a)\n1st dimension-0.5 0 0.5 12nd dimension-1-0.500.51123456789(b)Figure 3: The perceptual (a) and the algorithmic (b) spa-tial conﬁgurations for the nine selected cadences. The ca-dences are labelled according to the indexes of table 3.and Dispersion Accounted for (DAF): .9473was favoured.Considering the number of objects in combination with thenumber of dimensions, the achieved Stress-I value does notimply an adequate ﬁt between the MDS model produceddisparities and the actual distances reported by the partic-ipants. This fact can be attributed to the high level of un-certainty present in the subjective responses. However, thesatisfactory interpretability of the two dimensional conﬁg-uration (as will be shown below) supports the acceptanceof this solution.The dissimilarity matrix that was produced by the dis-tance metrics of the cadence-blending-system was alsoanalysed through non-metric MDS. The two-dimensional3Stress-I is a measure of missﬁt where a lower value indicates a betterﬁt (with a minimum of zero) and DAF is a measure of ﬁt where a highervalue indicates a better ﬁt (with a maximum of one).Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 145solution featured both acceptable Stress-I (.123) and DAF(.985). The conﬁgurations of both spaces are shown in Fig-ure 3.Visual inspection of the perceptual space reveals thatprior expectations regarding cadence positioning are gen-erally fulﬁlled. The perfect (no.1) and the Phrygian (no.2)input cadences are positioned far away from each other onthe 1st dimension. This dimension could be interpreted as‘modal vs tonal’ since negative values coincide with ab-sence of the leading note[11]while positive values sig-nify presence of the leading note. Cadences no.4 (back-door with seventh) and 5 (backdoor without seventh) arenaturally closely related. The clustering of no.4 and no.5with the Phrygian could be explained by their shared notes[5,10]and also by the absence of the leading note[11]that moves them away from the perfect cadence territory.Also, the close positioning of cadences no.3 (tritone sub-stitution) and no.8 is explained by the fact that the formeris a German-sixth-type while the latter is a French-sixth-type both sharing three basic notes[1,5,11]. These twocadences are additionally positioned more closely to theperfect cadence (no.1) than to the Phrygian showing thatalthough the tritone substitution is created by incorporat-ing the most salient attributes of the two input cadences(see subsection 2.1), it is not perceived as being equidistantbetween them. This can be explained by the fact that bothno.3 and no.8 take the leading note[11]and the seventh[5](that needs to be resolved) from the perfect cadence butonly take note[1](base of the Phrygian) from the Phry-gian. Cadence no.6 -the plagal- is positioned in the middlebetween the perfect and Phrygian along dimension 1 but isexpectedly an outlier along dimension 2.The comparison between the perceptual and algorith-mic conﬁgurations was performed using Tucker’s congru-ence coefﬁcient [20]. As a guideline, for the congruencecoefﬁcient, values larger than .92 are considered good/fair,and values larger than .95 practically show equality be-tween conﬁgurations [12]. In our case, the congruence co-efﬁcient between the perceptual and the algorithmic spacewas computed to be .944 indicating that the system canmake a very good estimation of the relationships betweencadences.5. CONCLUSIONSAccording to the theory of conceptual blending developedby Fauconier and Turner, novel conceptual spaces can becreated by blending elements from diverse input concep-tual spaces. Based on this theory and its category-theoreticinterpretation proposed by Goguen this study presentedinitial developments of a system for blending between har-monic structures, using cadence blending as a proof of con-cept. To this end, two input spaces with simple formalisa-tions of the perfect and the Phrygian cadences were usedto produce several blended cadences.The two input spaces along with the produced blends,and other cadences, were subjected to a pairwise dissim-ilarity rating listening test and subsequent MDS analysisin order to evaluate the output produced by the cadenceblending system. The basic aim of the study was to ex-amine whether perceptual distances between pairs of ca-dences, as rated by the participants, were actually reﬂectedby an objective distance metric that related to the formal-isation of cadences in the blending system. Indeed, thecomparative results showed that the system is capable ofmaking fair predictions of the perceived dissimilarities be-tween the blended cadences. Given the uncertainty intro-duced by both the demanding nature of the behaviouraltask and the MDS analyses for the two sets of data, thisresult is deemed rather satisfactory and leads to the follow-ing implications:1.The presented cadence description framework ismeaningful. Although the representation of knowl-edge in cadences is very elementary (just describingthe penultimate chords with their absolute and rela-tive notes), the derived results align with human per-ception/cognition.2.The utilised blending methodology producesconsis-tentresults in the sense that resulting blends do in-deed match the perceptual/cognitive attributes of theinput spaces.The utilisation of more sophisticated system-orientedmetrics is expected to increase the accuracy of the self-evaluation process within the system so as to producemeaningful results for a wider combination of input ca-dences (also ending in different ﬁnal chords) or even formore complex harmonic structures. As an obvious nextstep, the parameters of the system distance metric can bereﬁned to optimise the ﬁt between the algorithm’s predic-tion and the actual perception of cadence dissimilarities.Cadence blending is a proof-of-concept example of thecomputational framework for conceptual blending that isbeing developed in the context of the COINVENT project[18]. Overall, the results of the subjective experiment,even with this elementary representation of cadences, in-dicate the effectiveness of this framework towards creatingmeaningful output. The long term objective is the appli-cation of the computational blending approach for devel-oping melodic harmonisation methodologies that facilitatestructural blending between harmonies of diverse music id-ioms. This will require the development of ontologies ca-pable of describing signiﬁcantly more complex harmonicconcepts compared to a simple harmonic cadence. At thesame time, the employed subjective evaluation will needto be enriched by more elaborate experiments that will notonly be able to assess the aesthetic value and functional-ity of the blends but to also address the challenge of ratinglonger stimuli.6. ACKNOWLEDGEMENTSThis work is funded by the COINVENT project. Theproject COINVENT acknowledges the ﬁnancial support ofthe Future and Emerging Technologies (FET) programmewithin the Seventh Framework Programme for Researchof the European Commission, under FET-Open grant num-ber: 611553.146 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20157. REFERENCES[1]M. Antovi´c. Musical metaphor revisited: Primitives,universals and conceptual blending.Universals andConceptual Blending, 2011.[2]B. Benward and M. N. Saker.Music in theory andpractice, volume I, page 359. McGraw-Hill, 7th edi-tion, 2003.[3]M. A. Boden.The creative mind: Myths and mecha-nisms. Psychology Press, 2004.[4]E. Cambouropoulos, M. Kaliakatsos-Papakostas, andC. Tsougras. An idiom-independent representation ofchords for computational music analysis and genera-tion. InProceeding of the joint 11th Sound and Mu-sic Computing Conference (SMC) and 40th Interna-tional Computer Music Conference (ICMC), ICMC–SMC 2014, 2014.[5]T. Collins, R. Laney, A. Willis, and P. H. Garth-waite. Developing and evaluating computational mod-els of musical style.Artiﬁcial Intelligence for Engi-neering Design, Analysis and Manufacturing, availableon CJO2015:1–28, 2015.[6]N. Cook. Theorizing musical meaning.Music TheorySpectrum, 23(2):170–195, 2001.[7]M. Eppe, R. Confalonier, E. Maclean, M. Kaliakatsos-Papakostas, E. Cambouropoulos, M. Schorlemmer,M. Codescu, and K.U. K¨uhnberger. Computational in-vention of cadences and chord progressions by concep-tual chord-blending. InInternational Joint Conferenceon Artiﬁcial Intelligence (IJCAI) 2015, accepted forpublication, 2015.[8]G. Fauconnier and M. Turner.The Way We Think: Con-ceptual Blending And The Mind’s Hidden Complexi-ties. Basic Books, New York, reprint edition, 2003.[9]J. Goguen. Mathematical Models of Cognitive Spaceand Time. In Daniel Andler, Yoshinori Ogawa, Mit-suhiro Okada, and Shigeru Watanabe, editors,Reason-ing and Cognition, volume 2 ofInterdisciplinary Con-ference Series on Reasoning Studies. Keio UniversityPress, 2006.[10]A. K. Jordanous.Evaluating computational creativity:a standardised procedure for evaluating creative sys-tems and its application. PhD thesis, University of Sus-sex, 2012.[11]M. Kaliakatsos-Papakostas, E. Cambouropoulos,K. K¨uhnberger, O. Kutz, and A. Smaill. ConceptInvention and Music: Creating Novel Harmoniesvia Conceptual Blending. InIn Proceedings of the9th Conference on Interdisciplinary Musicology(CIM2014), CIM2014, December 2014.[12]U. Lorenzo-Seva and J. M. F. Ten Berge. Tucker’scongruence coefﬁcient as a meaningful index of fac-tor similarity.Methodology: European Journal of Re-search Methods for the Behavioral and Social Sci-ences, 2(2):57–64, 2006.[13]J. J. Meulman and W. J. Heiser.PASW Categories 18,Chapter 3. SPSS Inc., Chicago, 2008.[14]P. D. Mosses.CASL Reference Manual – The CompleteDocumentation of the Common Algebraic SpeciﬁcationLanguage, volume 2960. Springer, 2004.[15]S. Onta˜n´on and E. Plaza. Amalgams: A Formal Ap-proach for Combining Multiple Case Solutions. InProceedings of the 18th International Conference onCase-Based Reasoning Research and Development,ICCBR’10, pages 257–271, Berlin, Heidelberg, 2010.Springer-Verlag.[16]M. Pearce and G. Wiggins. Towards a framework forthe evaluation of machine compositions. InProceed-ings of the AISB’01 Symposium on Artiﬁcial Intelli-gence and Creativity in the Arts and Sciences, pages22–32, 2001.[17]M. Pearce and G. Wiggins. Evaluating cognitive mod-els of musical composition. InProceedings of the 4thinternational joint workshop on computational creativ-ity, pages 73–80. Goldsmiths, University of London,2007.[18]M. Schorlemmer, A. Smaill, K.U. K¨uhnberger,O. Kutz, S. Colton, E. Cambouropoulos, and A. Pease.Coinvent: Towards a computational concept inventiontheory. In5th International Conference on Computa-tional Creativity (ICCC) 2014, June 2014.[19]R. N. Shepard. Metric structures in ordinal data.Jour-nal of Mathematical Psychology, 3:287–315, 1966.[20]L. R. Tucker. A method for synthesis of factor analy-sis studies. Technical report, Washington, DC: Depart-ment of the Army., 1951.[21]G. A. Wiggins. A preliminary framework for descrip-tion, analysis and comparison of creative systems.Knowledge-Based Systems, 19(7):449–458, 2006.[22]F. W. Young. Nonmetric multidimensional scal-ing: Recovery of metric information.Psychometrica,35:455–473, 1970.[23]A. Zacharakis, K. Pastiadis, and J. D. Reiss. An in-terlanguage uniﬁcation of musical timbre: bridgingsemantic, perceptual and acoustic dimensions.MusicPerception, 32(4), 2015.[24]L. M. Zbikowski.Conceptualizing Music: CognitiveStructure, Theory, and Analysis. Oxford UniversityPress, 2002.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 147"
    },
    {
        "title": "Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing.",
        "author": [
            "Shuo Zhang",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "https://zenodo.org/records/1285647/files/annotated_jingju_arias_1.0.zip",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/ZhangRS15",
        "keywords": [
            "Dataset",
            "Jingju",
            "Beijing Opera",
            "Arias",
            "Praat",
            "Shengqiang",
            "Role Types",
            "Melodic Pitch Contours",
            "Linguistic Tones",
            "Music Information Retrieval"
        ]
    },
    {
        "title": "Chord Detection Using Deep Learning.",
        "author": [
            "Xinquan Zhou",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416968",
        "url": "https://doi.org/10.5281/zenodo.1416968",
        "ee": "https://zenodo.org/records/1416968/files/ZhouL15.pdf",
        "abstract": "In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and configurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classification.",
        "zenodo_id": 1416968,
        "dblp_key": "conf/ismir/ZhouL15",
        "keywords": [
            "deep learning",
            "audio chord detection",
            "high-level features",
            "bottleneck architecture",
            "promising results",
            "state-of-the-art systems",
            "input pre-processing",
            "bottleneck architecture",
            "SVMs vs. HMMs",
            "chord classification"
        ],
        "content": "CHORD DETECTION USING DEEP LEARNINGXinquan ZhouCenter for Music TechnologyGeorgia Institute of Technologyroyzxq@gmail.comAlexander LerchCenter for Music TechnologyGeorgia Institute of Technologyalexander.lerch@gatech.eduABSTRACTIn this paper, we utilize deep learning to learn high-levelfeatures for audio chord detection. The learned features,obtained by a deep network in bottleneck architecture, givepromising results and outperform state-of-the-art systems.We present and evaluate the results for various methods andconﬁgurations, including input pre-processing, a bottleneckarchitecture, and SVMs vs. HMMs for chord classiﬁcation.1. INTRODUCTIONThe goal of automatic chord detection is the automaticrecognition of the chord progression in a music recording.It is an important task in the analysis of western musicand music transcription in general, and it can contributeto applications such as key detection, structural segmenta-tion, music similarity measures, and other semantic anal-ysis tasks. Despite early successes in chord detection byusing pitch chroma features [6] and Hidden Markov Models(HMMs) [26], recent attempts at further increasing the de-tection accuracy are only met with moderate success [4,28].In recent years, deep learning approaches have gainedsigniﬁcant interest in the machine learning community asa way of building hierarchical representations from largeamounts of data. Deep learning has been applied success-fully in various ﬁelds; for instance, a system for speechrecognition utilizing deep learning was able to outperformstate-of-the-art systems not using deep learning [10]. Sev-eral studies indicate that deep learning methods can bevery successful when applied to Music Information Re-trieval (MIR) tasks, especially when used for feature learn-ing [1,9,13,16]. Deep learning, with its potential to untanglecomplicated patterns in a large amount of data, should bewell suited for the task of chord detection.In this work, we investigate Deep Networks (DNs) forlearning high-level and more representative features in thecontext of chord detection, effectively replacing the widelyused pitch chroma intermediate representation. We presentindividual results for different pre-processing options suchas time splicing and ﬁltering (see Sect. 3.2), architectures(see Sect. 3.4), and output classiﬁers (see Sect. 4).c\u0000Xinquan Zhou, Alexander Lerch.Licensed under a Creative Commons Attribution 4.0 International License(CC BY 4.0).Attribution:Xinquan Zhou, Alexander Lerch. “ChordDetection Using Deep Learning”, 16th International Society for MusicInformation Retrieval Conference, 2015.2. RELATED WORKDuring the past decade, deep learning has been consideredby the machine learning community to be one of the mostinteresting and intriguing research topics. Deep architec-tures promise to remove the necessity of custom-designedand manually selected features as neural networks shouldbe more powerful in disentangling interacting factors andthus be able to create meaningful high-level representa-tions of the input data. Generally speaking, deep learningcombines deep neural networks with an unsupervised learn-ing model. Two major learning models are widely usedfor unsupervised learning: Restricted Boltzmann Machines(RBMs) [11] and Sparse Auto Encoders [24]. A deep archi-tecture comprises multiple stacked layers based on one ofthese two models. These layers can be trained one by one,a process that is referred to as “pre-training” the network.In this work, we employ RBMs to pre-train the deep archi-tecture in an unsupervised fashion; this is called a DeepBelief Network (DBN) [11]. DBNs, composed of a stackof RBMs, essentially share the same topology with generalneural networks: DBNs are generative probabilistic modelswith one visible layer and several hidden layers.Since Hinton et al. proposed a fast learning algorithmfor DBNs [11], it has been widely used for initializingdeep neural networks. In deep structures, each layer learnsrelationships between units in lower layers. The complexityof the system increases with an increasing number of RBMlayers, making the structure —in theory— more powerful.An extra softmax output layer can be added to the top ofthe network (seeEqn (6)) [18]; its output can be interpretedas the likelihood of each class.LeCun and Bengio introduced the idea of applying Con-volutional Neural Networks (CNNs) to images, speech, andother time-series signals [15]. This approach allows to dealwith the variability in time and space to a certain degree,as CNNs can be seen as a special type of neural networkin which the weights are shared across the input within acertain spatial or temporal area. The weights thus act as akernel ﬁlter applied to the input. CNNs have been particu-larly successful in image analysis. For example, Norouziet al. used Convolutional RBMs to learn shift-invariant fea-tures [22].The results of a network depend largely on the networkarchitecture. For example, Grezl et al. used a so-calledbottleneck architecture neural network to obtain featuresfor speech recognition and showed that these features im-prove the accuracy of the task [8]. The principle behind52Figure 1. Visualization of a bottleneck architecturethe bottleneck-shaped architecture is that the number ofneurons in the middle layer is lower than in the other lay-ers as shown in Fig. 1. A network with bottleneck canbe structured in two sections: (i) Section 1 from the ﬁrstlayer to the bottleneck layer, with a gradual decrease of thenumber of neurons per layer, functions as an encoding orcompression process which compacts relevant informationand discards redundant information, and (ii) Section 2 fromthe bottleneck layer to the last layer with a gradual increasein the number of neurons per layer. The function of thispart can be interpreted as a decoding process. An additionalbeneﬁt of bottleneck architectures is that they can reduceoverﬁtting by decreasing the system complexity.Recently, more researchers investigated deep learning inthe context of MIR. Lee et al. pioneered the application ofconvolutional deep learning for audio feature learning [16].Hamel et al. used the features learned from music with aDBN for both music genre classiﬁcation and music auto-tagging [9]; their system was successful in MIREX 2011with top-ranked results. Battenberg employed a conditionalDBN to analyze drum patterns [1]. The use of deep archi-tectures for chord detections, however, has not yet beenexplored, although modern neural networks have been em-ployed in this ﬁeld. For instance, Boulanger et al. inves-tigated recurrent neural networks [2] and Humphrey hasexplored CNNs [12,14]. While they also used the conceptof pre-training, their architectures have only two or 3 layersand thus cannot be called “deep”.The basic buildings blocks of most modern approachesto chord detection can be traced back to two seminal pub-lications: Fujishima introduced pitch chroma vectors ex-tracted from the audio as input feature for chord detec-tion [6] and Sheh et al. proposed to use HMMs for repre-senting chords as hidden states and to model the transitionprobability of chords [26]. Since then, there have been alot of studies using chroma features and HMMs for chorddetection [5, 23]. Examples for recent systems are Ni et al.,using a genre-independent chord estimation method basedon HMM and chroma features [21] and Cho and Bello,who used multi-band features and a multi-stream HMM forchord recognition [4]. Training HMMs with pitch chromafeatures arguably is the standard approach for this task andthe progress is less marked by major innovations but by\nFigure 2. The overview of our systemoptimizing and tuning speciﬁc components.3. SYSTEM OVERVIEWFigure 2 gives an overview of all components and process-ing steps of the presented system. The following sectionwill discuss all of these steps in detail.3.1 Input RepresentationThe input audio is converted to a sample rate of 11.025 kHz.Then, a Constant Q transform (CQT) is applied. TheCQT [3] is a perceptually inspired time-frequency transfor-mation for audio. The resulting frequency bins are equallyspaced on a logarithmic (“pitch”) scale. It has the advan-tage of providing a more musically and perceptually mean-ingful spectral representation than the DFT. We used animplementation of the CQT as a ﬁlterbank of Gabor ﬁlters,spaced at 36 bins per octave, i.e., 3 bins per semitone, yield-ing 180 bins representing a frequency range spanning from110 Hz to 3.520 kHz. Finally, we used Principal Compo-nent Analysis (PCA) for decorrelation, and applied Z-Scorenormalization [27].3.2 Pre-processingNeighboring frames of the input representation can be ex-pected to contain similar content, as chords will not changeon a frame-by-frame basis. In order to take into accountthe relationship between the current frame and previousand future frames, we investigate the application of severalpre-processing approaches.3.2.1 Time SplicingTime splicing is a simple way to extend the current framewith the data of neighboring frames by concatenating theframes into one larger superframe. In ﬁrst order time splic-ing, we concatenate the current frame, the previous frame,Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 53and the following frame. Thus, each superframe consistsof three neighboring frames. Since the same operation willbe applied to all frames, there will be overlap introducedbetween neighboring superframes.3.2.2 ConvolutionCNNs are extensively used in tasks with highly correlatedinputs (e.g., the recognition of hand-written digits). Manytime series show similar properties so that CNNs seemto be an appropriate choice in the context of audio, too.Essentially, CNNs have one or more convolutional layersbetween the input and lower layers of the neural network.The function of a convolutional layer can be interpreted asthe application of a linear ﬁlter plus a non-linear transfor-mation, sometimes also combined with a pooling operation:Y= pool(sigm(K⇤X+B)),(1)in whichYis the output of a convolutional layer,Kis thelinear kernel ﬁlter (i.e., the impulse response),Xis theinput,Bis the bias,sigm()is a non-linear transform, andpool()is a down-sampling operation. The uniqueness ofconvolutional networks stems from the convolution opera-tion applied to the inputX. Since, unfortunately, we had noaccess to a deep learning toolbox with support for the con-volution operation in the time domain, we opted to employan optional pre-processing step inspired by CNNs, namelyby applying ﬁlters to the input of the network. However,instead of learning the ﬁlters, we evaluate several manu-ally designed ﬁlters: a single-pole low pass ﬁlter and twoFIR low pass ﬁlters with exponentially shaped impulse re-sponses. The single pole low pass ﬁlter produces the outputyfor an inputx, given the parameter↵:yn=( 1\u0000↵)yn\u00001+↵xn(2)We apply anti-causal ﬁltering and ﬁlter the signal in bothdirections so that the resulting overall ﬁlter has a zero-phaseresponse.The other two low pass ﬁlters have exponential decayshaped impulse response. The difference equations aregiven in Eqn (3) and Eqn (4).y1(n)=NXk=1a\u0000k+1x(n\u0000N+k)(3)y2(n)=NXk=1a\u0000k+1x(n+N\u0000k)(4)The ﬁlter length isNandais the exponential base. Thesetwo ﬁlters are not centered around the current frame any-more but shifted byNframes. Their impulse responses aresymmetric to each other. One could interpret these ﬁltersas focusing on past and future frames, respectively. Thepresented ﬁlters will be referred to as “extension ﬁlters”.The ideas of splicing and convolution can be combined,as exempliﬁed in Fig. 3.Furthermore, similar to the process in CNNs, a maxi-mum pooling operation on the output of the spliced ﬁltersis optionally applied. The operation takes the maximumvalue among different ﬁlters per “bin”.\nFigure 3. Splicing output of different ﬁlters3.3 TrainingIt is impractical to train DNNs directly with back propaga-tion using gradient decent due to their deep structure andthe limited amount of training samples. Therefore, the net-work is usually initialized by an unsupervised pre-trainingstep. As our network consists of RBMs, Gibbs samplingcan be used for training [11]. The objective is to retain asmuch information as possible between input and output.The computation for layerlcan be represented as:Yl= sigm(WlXl+Bl),(5)which is identical to many traditional neural networks. Thus,a standard back propagation can be applied after pre-trainingto ﬁne-tune the network in a supervised manner. The losscriterion we use in this work is cross-entropy.3.4 ArchitectureWe investigate a deep network with6layers in two differentarchitectures. The common architecture features the sameamount of neurons in every layer, in our case1024. Thebottleneck architecture has256neurons in the middle layerand512neurons in the layers neighboring the middle layer.The remaining layers consist1024neurons each (compare[8]). A softmax output layer is stacked on top of botharchitectures as described by Eqn (6).softmax(Yl)=exp(Yl)PNk=1exp(Yk)(6)The network is implemented using the Kaldi package devel-oped by John Hopkins University [25].4. CLASSIFICATIONThe output of the softmax layer can be interpreted as thelikelihood of each chord class; simply taking the maximumwill provide a class decision (this method will be referredto asArgmax). Alternatively, the output can be treated asintermediate feature vector that can be used as an input toother classiﬁers for computing the ﬁnal decision.4.1 Support Vector MachineSupport Vector Machines (SVMs) are, as widely used classi-ﬁers with generally good performance. The SVM is trainedusing the output of the network as features, and the classiﬁ-cation is carried out frame by frame. The classiﬁcation isfollowed by a simple prediction smoothing.54 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 20154.2 Hidden Markov ModelHMMs are, as pointed out above, the standard classiﬁerfor automatic chord detection because the characteristicsof the task ﬁt the HMM approach well: Chords are hid-den states that can be estimated from observations (featurevectors extracted from the audio signal), and the likelihoodof chord transitions can be modeled with transition prob-abilities. Modiﬁed HMMs such as ergodic HMMs andkey-independent HMMs have been also explored for thistask [17, 23]. In this work we are mostly interested in theperformance comparison between high-level features, soa simple ﬁrst-order HMM is used. Given the probabilisticcharacteristic of the softmax output layer, it can be directlyas emission probabilities for the HMM. Therefore, there isno need to train the HMM using, e.g., the commonly usedBaum-Welch algorithm. Instead, the histogram of eachclass in our training is used as initial probabilities, and thebigram of chord transitions is used to compute the transi-tion probabilities. Finally, we employ the Viterbi decodingalgorithm to ﬁnd the globally optimal chord sequence.5. EVALUATION PROCEDURE5.1 DatasetOur dataset is a combination of several different datasets,yielding a 317-piece collection. The data is composed of•180songs from theBeatles dataset[19],•100songs from theRWC Pop dataset[7],•18songs from theZweieck dataset[19], and•19songs fromQueen dataset[19].The pre-processing as described in Sect. 3.2 ensures identi-cal input audio formats.5.2 MethodologyThe dataset is divided randomly into two parts: 80% for thetraining set and 20% for the test set. On the training scale,we use a frame-based strategy, which means we divide eachsong into frames, and treat each frame as an independenttraining sample. On average each song is divided intoabout 1200 frames resulting in approximately 300k trainingsamples and approximately 76k test samples.Within the training set, 10% of the data is used as a vali-dation set. For the post-processing, all data in the trainingset will be used to train the post-classiﬁer.Time constraints and the workload requirements for train-ing deep networks made a cross validation for evaluationimpractical.The chosen ground truth for classiﬁcation are major andminor triads for every root note, resulting in a dictionaryof24 + 1chord labels. Ground truth time-aligned chordsymbols are mapped to this major/minor dictionary:Chordmajmin⇢{N}[{S⇥maj, min}(7)withSrepresenting the 12 pitch classes (root notes) andNbeing the label for unknown chords. In the calculationof the detection accuracy, the following chord types aremapped to the corresponding major/minor in the dictionary:\nFigure 4. Chords histogramtriad major/minor and seventh major/minor. Other chordtypes are treated as unknown chords. For instance, G:majand G:maj7 are mapped to ‘G:maj’; G:dim and G:6 are allmapped to ‘N’. The histogram of chords in our dataset aftersuch mapping is shown in Fig. 4.5.3 Evaluation MetricThe used evaluation metric is the same as proposed in theaudio chord detection task for MIREX 2013: the WeightedChord Symbol Recall (WCSR). WCSR is deﬁned as thetotal duration of segments with correct prediction as formu-lated in Eqn (8):WCSR=1NnXk=1Ck,(8)in whichnis the number of test samples (songs),Nisthe total number of frames in all test samples, andCkisthe number of frames that are correctly detected in thekthsample.6. EXPERIMENTS6.1 Post-classiﬁersIn this experiment, the network is initialized with pre-training, followed by ﬁne tuning using back propagation.This conﬁguration will be referred to asDNDBN\u0000DNN.No pre-processing is applied to the data; the input is sim-ply the input representation (CQT followed by PCA) asdescribed in Sect. 3.1. The chosen architecture is the bottle-neck architecture. Three different classiﬁers are compared:the maximum of the softmax output (Argmax), an SVM,and an HMM.The results listed in Table 1 are unambiguous and un-surprising: the HMM with Viterbi decoding outperformsthe SVM; using HMMs with a model for transition prob-abilities is an appropriate approach to chord detection asit models the dynamic properties of chord progressions,which cannot be done with non-dynamic classiﬁers suchas SVMs. One noteworthy result is that the SVM does notProceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 55Training Scenario Classiﬁer WCSRDNDBN\u0000DNNArgmax() 0.648DNDBN\u0000DNNSVM 0.645DNDBN\u0000DNNHMM0.755Table 1. Chord detection performance using different post-classiﬁers↵Pre-processing WCSR0.25 Filtering 0.7580.25 Spliced Filters 0.9120.5 Filtering 0.7870.5 Spliced Filters 0.8570.75 Filtering 0.7980.75 Spliced Filters0.919Table 2. Chord detection performance using different ﬁlterparametersimprove the WCSR compared to the direct (Argmax) outputof the network. Apparently, the SVM is not able to improveseparability of the learned output features.6.2 Pre-processingAs stated in Sect. 3.2, we are interested in the applica-tion of different ﬁlters in the pre-processing stage. In theﬁrst experiment (Filtering), an anti-causal single pole ﬁl-ter (seeEqn (2)) is evaluated with the parameter↵set to0.25,0.5, and0.75, respectively. The second experiment(Spliced Filters), splices these ﬁlter outputs with the outputsof the extension ﬁlters as introduced in Sect. 3.2. Theseexperiments are carried out with theDNDBN\u0000DNNtrainingscenario, a bottleneck architecture, and an HMM classiﬁer.Table 2 lists the results of these pre-processing variants. Itcan be observed that the network trained with ﬁltered inputsslightly outperforms the network without pre-processing;splicing the ﬁltered input with the extension ﬁlter outputsincreases the results drastically.6.3 Architecture6.3.1 Common vs. BottleneckThe results of Grezl et al. indicate that a bottleneck architec-ture should be more suitable to learn high-level features thana common architecture and reduce overﬁtting [8]. In orderto verify these characteristics for our task, the performanceof both architectures is evaluated in comparison. The re-sults are listed in Table 3 for three pre-processing scenarios:no additional pre-processing (None),Spliced Filtersandspliced ﬁlters followed by a max pooling (Pooling). In or-der to allow conclusions about overﬁtting, both the WCSRof the test set and the training set are reported. All resultsare computed for theDNDBN\u0000DNNtraining scenario withHMM classiﬁers.The results show that the bottleneck architecture givessigniﬁcantly better results (p=0.023) on the test setArchitecture Pre-processingTrainingWCSRWCSRCommon None 0.843 0.703Bottleneck None 0.855 0.755Common Spliced Filters0.9850.876Bottleneck Spliced Filters 0.9360.919Common Pooling 0.965 0.875Bottleneck Pooling 0.960 0.916Table 3. Chord detection performance for different archi-tectures and pre-processing stepsLearning Targets WCSRSingle-Label — 25 Chord Classes0.919Multi-Label — 12 Pitch Classes 0.78Table 4. Chord detection performance for single-label vs.multi-label learning(WSCR). Note that this is not true for the training set (Train-ing WSCR), for which the common architecture achievesresults in the same range or better than the bottleneck archi-tecture. The difference between the results on the trainingset and the test set are thus much larger for the commonarchitecture than for the bottleneck architecture. The bot-tleneck architecture is clearly advantageous to use in thistask: it reduces complexity and thus the training workloadand increases the classiﬁcation performance signiﬁcantly.Furthermore, the comparison of classiﬁer performance be-tween training and test set in Table 3 clearly indicates thatthe common architecture tends to ﬁt more to the trainingdata, and is thus more prone to overﬁtting.6.3.2 Single-Label vs. Multi-LabelAs mentioned above, the pitch chroma is the standard fea-ture representation for audio chord detection. Since weuse the output of our deep network as feature, it seems anintuitive choice to learn pitch class information (and thus, apitch chroma) instead of the chord classes. By doing so, thenumber of outputs is reduced by a factor of two (or higherin the case of more chords), and there would also be a closerrelation between the output and the input representation,the CQT. Therefore, the abstraction and complexity of thetask might be decreased. It will, however, lead to anotherissue: the single-label output (one chord per output) willbe changed into a multi-label output (multiple pitches peroutput). Therefore, the learning has to be modiﬁed to allowmultiple simultaneous (pitch class) labels. The experimentis carried out with both Splicing and Filtering in the pre-processing, theDNDBN\u0000DNNtraining scenario, and HMMclassiﬁers. Table 4 lists the results.Boulanger-Lewandowski et al. report combining chromafeatures with chord labels for their recurrent neural networkand report a slightly improved result [2]. They do not, how-ever, provide a detailed description of this combination. Ascan be seen from the table, the result for multi-label train-ing is clearly lower than the result for single-label training.56 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015Method WCSRChordino 0.625Best Conﬁguration 0.919Best Conﬁguration with Max Pooling 0.916Table 5. Comparison of the performance of the best conﬁg-uration with ChordinoPossible reasons for bad performance include (i) difﬁcultieswith multi-target learning, since it increases the difﬁculty totrain; furthermore, our implementation of multi-label train-ing might be sub-optimal as the same posterior is assignedto each target without any information on the pitch classenergy, and (ii) the issue that not all pitches always soundsimultaneously in a chord (or might be missing altogether)might have larger impact on the multi-label training thanon the single-label training.6.4 Results & DiscussionIt is challenging to compare the results to previously pub-lished results due to varying evaluation methodologies, met-rics, and datasets. It seems that the results of Cho andBello [4], who reported a performance of about 76%, werecomputed with a comparable dataset. The recent MIREX re-sults on Chord Detection generally show lower accuracy butuse a different evaluation vocabulary. In order to providea baseline result to put results into perspective, we presentthe results of Chordino [20] with the default settings, com-puted on our dataset. It should be pointed out that thiscomparison is unfair as Chordino is able to detect as manyas 120 chords, compared to our 24. The label mappingstrategies are another signiﬁcant issue for Chordino. Ourlabel mapping results in nearly sixth of the total label being“N”, which might have negative impact on the Chordinoresults. The Chordino results are mapped to major/minorthe same way as the ground truth annotations. The resultsare shown in Table 5. In the table, theBest Conﬁgurationis using Bottleneck architecture, spliced ﬁlters(↵=0.75)as preprocessing, single label learning targets, and Viterbidecoding as post-classiﬁer. TheBest Conﬁguration withMax Poolingis the same as the best conﬁguration exceptapplying another max pooling layer after the spliced ﬁlters.The latter conﬁguration has a much reduced computationalworkload. The presented results are clearly competitivewith existing state-of-the-art systems.7. CONCLUSION & FUTURE WORKIn this work, we presented a system which applies deeplearning to the MIR task of automatic chord detection. Ourmodel is able to learn high-level probabilistic representa-tions for chords across various conﬁgurations. We haveshown that the use of a bottleneck architecture is advanta-geous as it reduces overﬁtting and increases classiﬁer perfor-mance, and that the choice of appropriate input ﬁltering andsplicing can signiﬁcantly increase classiﬁer performance.Learning a pitch class vector instead of chord likelihoodby incorporating multi-label learning proved to be less suc-cessful. The idea has, however, a certain appeal and wouldallow the number of output nodes to be independent of thenumber of chords to be detected. It is also conceivable toinvestigate a different option for the network output: in-stead of training chords or pitch classes we could — underthe assumption that we are only after chords comprisedof stacked third intervals — train the output with octave-independent third intervals in a multi-label scenario with24output nodes.8. REFERENCES[1]Eric Battenberg and David Wessel. Analyzing drumpatterns using conditional deep belief networks. InPro-ceedings of the International Conference on Music In-formation Retrieval (ISMIR), pages 37–42, 2012.[2]Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent. Audio chord recognition with recurrentneural networks. InProceedings of the InternationalConference on Music Information Retrieval (ISMIR),pages 335–340, 2013.[3]Judith C Brown. Calculation of a constant q spectraltransform.The Journal of the Acoustical Society ofAmerica, 89(1):425–434, 1991.[4]Taemin Cho and Juan P Bello. Mirex 2013: Large vo-cabulary chord recognition system using multi-bandfeatures and a multi-stream HMM.Music InformationRetrieval Evaluation eXchange (MIREX), 2013.[5]Taemin Cho, Ron J Weiss, and Juan Pablo Bello. Ex-ploring common variations in state of the art chordrecognition systems. InProceedings of the Sound andMusic Computing Conference (SMC), pages 1–8, 2010.[6]Takuya Fujishima. Realtime chord recognition of mu-sical sound: A system using common lisp music. InProceedings of the International Computer Music Con-ference (ICMC), volume 1999, pages 464–467, 1999.[7]Masataka Goto, Hiroki Hashiguchi, TakuichiNishimura, and Ryuichi Oka. Rwc music database:Popular, classical and jazz music databases. InProceedings of the International Conference on MusicInformation Retrieval (ISMIR), volume 2, pages287–288, 2002.[8]Frantisek Grezl, Martin Karaﬁ´at, Stanislav Kont´ar, andJ Cernocky. Probabilistic and bottle-neck features forlvcsr of meetings. InProceedings of the InternationalConference on Acoustics, Speech and Signal Processing(ICASSP), volume 4, pages IV–757. IEEE, 2007.[9]Philippe Hamel and Douglas Eck. Learning featuresfrom music audio with deep belief networks. InPro-ceedings of the International Conference on Music In-formation Retrieval (ISMIR), pages 339–344. Utrecht,The Netherlands, 2010.Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015 57[10]Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-nior, Vincent Vanhoucke, Patrick Nguyen, Tara NSainath, et al. Deep neural networks for acoustic mod-eling in speech recognition: The shared views of fourresearch groups.Signal Processing Magazine, IEEE,29(6):82–97, 2012.[11]Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.A fast learning algorithm for deep belief nets.Neuralcomputation, 18(7):1527–1554, 2006.[12]Eric J Humphrey and Juan Pablo Bello. Rethinkingautomatic chord recognition with convolutional neuralnetworks. InProceedings of the International Confer-ence on Machine Learning and Applications (ICMLA),volume 2, pages 357–362. IEEE, 2012.[13]Eric J Humphrey, Juan Pablo Bello, and Yann LeCun.Moving beyond feature design: Deep architectures andautomatic feature learning in music informatics. InPro-ceedings of the International Conference on Music In-formation Retrieval (ISMIR), pages 403–408, 2012.[14]Eric J Humphrey, Taemin Cho, and Juan Pablo Bello.Learning a robust tonnetz-space transform for automaticchord recognition. InProceedings of the InternationalConference on Acoustics, Speech and Signal Processing(ICASSP), pages 453–456. IEEE, 2012.[15]Yann LeCun and Yoshua Bengio. Convolutional net-works for images, speech, and time series.The hand-book of brain theory and neural networks, 3361:310,1995.[16]Honglak Lee, Peter Pham, Yan Largman, and Andrew YNg. Unsupervised feature learning for audio classiﬁca-tion using convolutional deep belief networks. InAd-vances in neural information processing systems, 2009.[17]Kyogu Lee and Malcolm Slaney. Acoustic chord tran-scription and key extraction from audio using key-dependent HMMs trained on synthesized audio.Audio,Speech, and Language Processing, IEEE Transactionson, 16(2):291–301, 2008.[18]Thomas M Martinetz, Stanislav G Berkovich, andKlaus J Schulten. Neural-gas’ network for vector quan-tization and its application to time-series prediction.Neural Networks, IEEE Transactions on, 4(4):558–569,1993.[19]Matthias Mauch, Chris Cannam, Matthew Davies, Si-mon Dixon, Christopher Harte, Sefki Kolozali, DanTidhar, and Mark Sandler. Omras2 metadata project2009. InProceedings of the International Conferenceon Music Information Retrieval (ISMIR), 2009.[20]Matthias Mauch and Simon Dixon. Approximate notetranscription for the improved identiﬁcation of difﬁcultchords. InProceedings of the International Conferenceon Music Information Retrieval (ISMIR), pages 135–140, 2010.[21]Yizhao Ni, Matt McVicar, Raul Santos-Rodriguez, andTijl De Bie. Using hyper-genre training to explore genreinformation for automatic chord estimation. InProceed-ings of the International Conference on Music Informa-tion Retrieval (ISMIR), pages 109–114, 2012.[22]Mohammad Norouzi, Mani Ranjbar, and Greg Mori.Stacks of convolutional restricted boltzmann machinesfor shift-invariant feature learning. InProceedings ofthe Conference on Computer Vision and Pattern Recog-nition (CVPR), pages 2735–2742. IEEE, 2009.[23]H´elene Papadopoulos and Geoffroy Peeters. Large-scalestudy of chord estimation algorithms based on chromarepresentation and hmm. InProceedings of the Interna-tional Workshop on Content-Based Multimedia Index-ing (CBMI), pages 53–60. IEEE, 2007.[24]Christopher Poultney, Sumit Chopra, Yann L Cun, et al.Efﬁcient learning of sparse representations with anenergy-based model. InAdvances in neural informa-tion processing systems, pages 1137–1144, 2006.[25]Daniel Povey, Arnab Ghoshal, Gilles Boulianne, LukasBurget, Ondrej Glembek, Nagendra Goel, Mirko Han-nemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,Jan Silovsky, Georg Stemmer, and Karel Vesely. Thekaldi speech recognition toolkit. InProceedings of theWorkshop on Automatic Speech Recognition and Under-standing. IEEE Signal Processing Society, December2011. IEEE Catalog No.: CFP11SRW-USB.[26]Alexander Sheh and Daniel PW Ellis. Chord segmenta-tion and recognition using em-trained hidden markovmodels.Proceedings of the International Conference onMusic Information Retrieval (ISMIR), pages 185–191,2003.[27]J Sola and J Sevilla. Importance of input data normaliza-tion for the application of neural networks to complexindustrial problems.Nuclear Science, IEEE Transac-tions on, 44(3):1464–1468, 1997.[28]Yushi Ueda, Yuuki Uchiyama, Takuya Nishimoto,Nobutaka Ono, and Shigeki Sagayama. Hmm-basedapproach for automatic chord detection using reﬁnedacoustic features. InProceedings of the InternationalConference on Acoustics, Speech and Signal Processing(ICASSP), pages 5518–5521. IEEE, 2010.58 Proceedings of the 16th ISMIR Conference, M´ alaga, Spain, October 26-30, 2015"
    },
    {
        "title": "Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015, Málaga, Spain, October 26-30, 2015",
        "author": [
            "Meinard Müller",
            "Frans Wiering"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2015"
    }
]