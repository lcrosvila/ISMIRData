[
    {
        "title": "Score-Informed Analysis of Intonation and Pitch Modulation in Jazz Solos.",
        "author": [
            "Jakob Abe\u00dfer",
            "Estefan\u00eda Cano",
            "Klaus Frieler",
            "Martin Pfleiderer",
            "Wolf-Georg Zaddach"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416836",
        "url": "https://doi.org/10.5281/zenodo.1416836",
        "ee": "https://zenodo.org/records/1416836/files/AbesserCFPZ15.pdf",
        "abstract": "The paper presents new approaches for analyzing the char- acteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on con- textual parameters such as pitch, duration, and tempo as well as the performing artist.",
        "zenodo_id": 1416836,
        "dblp_key": "conf/ismir/AbesserCFPZ15",
        "keywords": [
            "intonation",
            "pitch modulation",
            "woodwind",
            "brass solos",
            "jazz recordings",
            "score-informed analysis",
            "source separation",
            "fundamental frequency tracking",
            "audio splitting",
            "fundamental frequency contour"
        ]
    },
    {
        "title": "Emotion Based Segmentation of Musical Audio.",
        "author": [
            "Anna Aljanaki",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418201",
        "url": "https://doi.org/10.5281/zenodo.1418201",
        "ee": "https://zenodo.org/records/1418201/files/AljanakiWV15.pdf",
        "abstract": "The dominant approach to musical emotion variation detection tracks emotion over time continuously and usu- ally deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener\u2019s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is nec- essary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of dif- ferent segmentation methods as applied to a task of emo- tional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have struc- tural annotations from the SALAMI dataset. We investi- gate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agree- ment on emotional segmentation. Lastly, we evaluate dif- ferent unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best.",
        "zenodo_id": 1418201,
        "dblp_key": "conf/ismir/AljanakiWV15",
        "keywords": [
            "time resolutions",
            "emotional variation",
            "listeners point of view",
            "progression of emotionally stable segments",
            "segmenting music at emotional boundaries",
            "formal evaluation",
            "structural segmentation",
            "emotional segmentation",
            "inter-annotator agreement",
            "unsupervised segmentation methods"
        ]
    },
    {
        "title": "Real-Time Music Tracking Using Multiple Performances as a Reference.",
        "author": [
            "Andreas Arzt",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417205",
        "url": "https://doi.org/10.5281/zenodo.1417205",
        "ee": "https://zenodo.org/records/1417205/files/ArztW15.pdf",
        "abstract": "In general, algorithms for real-time music tracking di- rectly use a symbolic representation of the score, or a syn- thesised version thereof, as a reference for the on-line align- ment process. In this paper we present an alternative ap- proach. First, different performances of the piece in ques- tion are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking al- gorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience.",
        "zenodo_id": 1417205,
        "dblp_key": "conf/ismir/ArztW15",
        "keywords": [
            "symbolic representation",
            "on-line alignment",
            "alternative approach",
            "multiple instances",
            "live performance",
            "output combination",
            "current position",
            "score",
            "robustness",
            "precision"
        ]
    },
    {
        "title": "Corpus Analysis Tools for Computational Hook Discovery.",
        "author": [
            "Jan Van Balen",
            "John Ashley Burgoyne",
            "Dimitrios Bountouridis",
            "Daniel M\u00fcllensiefen",
            "Remco C. Veltkamp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415038",
        "url": "https://doi.org/10.5281/zenodo.1415038",
        "ee": "https://zenodo.org/records/1415038/files/BalenBBMV15.pdf",
        "abstract": "Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is op- timized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: au- dio bigram distributions, and the use of corpus-relative or \u201csecond-order\u201d descriptors. To test the real-world applica- bility of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening his- tory and experience of our participants. We find that our corpus-based audio features are able to explain a compa- rable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable.",
        "zenodo_id": 1415038,
        "dblp_key": "conf/ismir/BalenBBMV15",
        "keywords": [
            "corpus analysis",
            "interpretable results",
            "audio bigram distributions",
            "corpus-relative descriptors",
            "song recognition data",
            "cognitive adequacy",
            "model interpretation",
            "listening history",
            "symbolic features",
            "new insights"
        ]
    },
    {
        "title": "Predictive Power of Personality on Music-Genre Exclusivity.",
        "author": [
            "Jotthi Bansal",
            "Matthew Woolhouse"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417467",
        "url": "https://doi.org/10.5281/zenodo.1417467",
        "ee": "https://zenodo.org/records/1417467/files/BansalW15.pdf",
        "abstract": "Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this rela- tionship using a new methodology: genre dispersion among people\u2019s mobile-phone music collections. By analyzing the download behaviours of genre-defined user subgroups, we investigated the following questions: (1) do genre-pre- ferring subgroups show distinct patterns of genre consump- tion and genre exclusivity; (2) does genre exclusivity re- late to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusiv- ity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be \u201copen\u201d to differ- ent musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people\u2019s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wid- er musical tastes than those with low openness and agree- ableness. Our study corroborated previous research link- ing genre preference and personality, and revealed, in a novel way, the predictive power of personality on music- consumption.",
        "zenodo_id": 1417467,
        "dblp_key": "conf/ismir/BansalW15",
        "keywords": [
            "personality",
            "music preference",
            "genre dispersion",
            "mobile-phone music collections",
            "download behaviours",
            "Big Five personality factors",
            "genre exclusivity",
            "genre inclusivity",
            "openness",
            "agreeableness"
        ]
    },
    {
        "title": "Benford&apos;s Law for Music Analysis.",
        "author": [
            "Isabel Barbancho",
            "Lorenzo J. Tard\u00f3n",
            "Ana M. Barbancho",
            "Mateu Sbert"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417012",
        "url": "https://doi.org/10.5281/zenodo.1417012",
        "ee": "https://zenodo.org/records/1417012/files/BarbanchoTBS15.pdf",
        "abstract": "Benford\u2019s law defines a peculiar distribution of the lead- ing digits of a set of numbers. The behavior is logarith- mic, with the leading digit 1 reflecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the applica- tion of Benford\u2019s law to many different fields. In this paper, a novel exploitation of Benford\u2019s law for the analysis of au- dio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain au- dio dataset to Benford\u2019s law are presented. These new pro- posed features are succesfully tested in two concrete audio tasks: the detection of artificially assembled chords and the estimation of the quality of the MIDI conversions.",
        "zenodo_id": 1417012,
        "dblp_key": "conf/ismir/BarbanchoTBS15",
        "keywords": [
            "Benford\u2019s law",
            "peculiar distribution",
            "leading digits",
            "logarithmic",
            "probability of occurrence",
            "audio signals",
            "evaluation of agreement",
            "audio tasks",
            "artificially assembled chords",
            "estimation of MIDI conversions"
        ]
    },
    {
        "title": "An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.",
        "author": [
            "Emmanouil Benetos",
            "Tillman Weyde"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418017",
        "url": "https://doi.org/10.5281/zenodo.1418017",
        "ee": "https://zenodo.org/records/1418017/files/BenetosW15.pdf",
        "abstract": "In this paper, an efficient, general-purpose model for multi- ple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component anal- ysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computa- tional efficiency is achieved by supporting the use of pre- extracted and pre-shifted sound state templates. Two vari- ants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the ap- pearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models out- perform the state-of-the-art for multiple-instrument tran- scription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch de- tection performance compared with constant-Q represen- tations.",
        "zenodo_id": 1418017,
        "dblp_key": "conf/ismir/BenetosW15",
        "keywords": [
            "efficient",
            "general-purpose",
            "multiple instrument",
            "polyphonic music transcription",
            "probabilistic latent component analysis",
            "sound state spectral templates",
            "variable-Q transform",
            "pre-extracted and pre-shifted sound state templates",
            "hidden Markov model-based constraints",
            "benchmark transcription datasets"
        ]
    },
    {
        "title": "Melody Extraction by Contour Classification.",
        "author": [
            "Rachel M. Bittner",
            "Justin Salamon",
            "Slim Essid",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416322",
        "url": "https://doi.org/10.5281/zenodo.1416322",
        "ee": "https://zenodo.org/records/1416322/files/BittnerSEB15.pdf",
        "abstract": "Due to the scarcity of labeled data, most melody extrac- tion algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch con- tour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data- driven melodic contour selection. Specifically, a discrim- inative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track\u2019s extracted contours, and these scores are decoded to generate a single melody out- put. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the gen- erative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements.",
        "zenodo_id": 1416322,
        "dblp_key": "conf/ismir/BittnerSEB15",
        "keywords": [
            "scarcity of labeled data",
            "careful engineering",
            "pitch contour selection",
            "heuristics",
            "discriminative model",
            "binary classifier",
            "melodic contours",
            "generative model",
            "contour classification accuracy",
            "melody output"
        ]
    },
    {
        "title": "Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.",
        "author": [
            "Sebastian B\u00f6ck",
            "Florian Krebs",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416026",
        "url": "https://doi.org/10.5281/zenodo.1416026",
        "ee": "https://zenodo.org/records/1416026/files/BockKW15.pdf",
        "abstract": "In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike ex- isting (comb filter based) approaches, we do not use hand- crafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as in- put to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing differ- ent accent bands, modelling metrical relations, categoris- ing the excerpts into slow / fast or any other advanced pro- cessing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator\u2019s histogram peak.",
        "zenodo_id": 1416026,
        "dblp_key": "conf/ismir/BockKW15",
        "keywords": [
            "tempo estimation",
            "bank of resonating comb filters",
            "determining dominant periodicity",
            "recurrent neural network",
            "beat-level representation",
            "comb filter bank",
            "state-of-the-art performance",
            "nine of ten datasets",
            "advanced processing",
            "resonators histogram peak"
        ]
    },
    {
        "title": "An Audio to Score Alignment Framework Using Spectral Factorization and Dynamic Time Warping.",
        "author": [
            "Julio Jos\u00e9 Carabias-Orti",
            "Francisco J. Rodr\u00edguez-Serrano",
            "Pedro Vera-Candeas",
            "Nicol\u00e1s Ruiz-Reyes",
            "Francisco J. Ca\u00f1adas-Quesada"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418371",
        "url": "https://doi.org/10.5281/zenodo.1418371",
        "ee": "https://zenodo.org/records/1418371/files/Carabias-OrtiRV15.pdf",
        "abstract": "In this paper, we present an audio to score alignment framework based on spectral factorization and online Dy- namic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factoriza- tion (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposi- tion method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be inter- preted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance.",
        "zenodo_id": 1418371,
        "dblp_key": "conf/ismir/Carabias-OrtiRV15",
        "keywords": [
            "audio",
            "score",
            "alignment",
            "spectral",
            "factorization",
            "online",
            "Dynamic",
            "Time",
            "Warping",
            "NMF"
        ]
    },
    {
        "title": "Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC) Model for Pop Cover Song Recognition.",
        "author": [
            "Ning Chen 0007",
            "J. Stephen Downie",
            "Haidong Xiao",
            "Yu Zhu",
            "Jie Zhu 0006"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416096",
        "url": "https://doi.org/10.5281/zenodo.1416096",
        "ee": "https://zenodo.org/records/1416096/files/ChenDXZZ15.pdf",
        "abstract": "Most of the features of Cover Song Identification (CSI), for example, Pitch Class Profile (PCP) related features, are based on the musical facets shared among cover versions: melody evolution and harmonic progression. In this work, the perceptual feature was studied for CSI. Our idea was to modify the Perceptual Linear Prediction (PLP) model in the field of Automatic Speech Recognition (ASR) by (a) introducing new research achievements in psychophysics, and (b) considering the difference between speech and music signals to make it consistent with human hearing and more suitable for music signal analysis. Furthermore, the obtained Linear Prediction Coefficients (LPCs) were mapped to LPC cepstrum coefficients, on which liftering was applied, to boost the timbre invariance of the resultant feature: Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC). Experimental results showed that both LPC cepstrum coefficients mapping and cepstrum lif- tering were crucial in ensuring the identification power of the MPLPLC feature. The MPLPLC feature outperformed state-of-the-art features in the context of CSI and in re- sisting instrumental accompaniment variation. This study verifies that the mature techniques in the ASR or Compu- tational Auditory Scene Analysis (CASA) fields may be modified and included to enhance the performance of the Music Information Retrieval (MIR) scheme.",
        "zenodo_id": 1416096,
        "dblp_key": "conf/ismir/ChenDXZZ15",
        "keywords": [
            "Cover Song Identification",
            "Pitch Class Profile",
            "Perceptual Feature",
            "Perceptual Linear Prediction",
            "Psychophysics",
            "Automatic Speech Recognition",
            "Speech and Music Signals",
            "LPC cepstrum coefficients",
            "Liftering",
            "Music Information Retrieval"
        ]
    },
    {
        "title": "Electric Guitar Playing Technique Detection in Real-World Recording Based on F0 Sequence Pattern Recognition.",
        "author": [
            "Yuan-Ping Chen",
            "Li Su 0002",
            "Yi-Hsuan Yang"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414806",
        "url": "https://doi.org/10.5281/zenodo.1414806",
        "ee": "https://zenodo.org/records/1414806/files/ChenSY15.pdf",
        "abstract": "For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two- stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment.",
        "zenodo_id": 1414806,
        "dblp_key": "conf/ismir/ChenSY15",
        "keywords": [
            "playing techniques",
            "bend detection",
            "vibrato detection",
            "melody contour",
            "timbre features",
            "pitch features",
            "two-stage framework",
            "audio track",
            "accompaniment",
            "F-score"
        ]
    },
    {
        "title": "Hybrid Long- and Short-Term Models of Folk Melodies.",
        "author": [
            "Srikanth Cherla",
            "Son N. Tran",
            "Tillman Weyde",
            "Artur S. d&apos;Avila Garcez"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417315",
        "url": "https://doi.org/10.5281/zenodo.1417315",
        "ee": "https://zenodo.org/records/1417315/files/CherlaTWG15.pdf",
        "abstract": "In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melod- ies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are com- bined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connec- tionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our exper- iments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hy- brid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of mono- phonic chorale and folk melodies.",
        "zenodo_id": 1417315,
        "dblp_key": "conf/ismir/CherlaTWG15",
        "keywords": [
            "dynamic models",
            "predicting sequences",
            "musical pitch",
            "melodies",
            "long-term model",
            "short-term model",
            "ensemble",
            "relative entropies",
            "connectionist LTMs",
            "Recurrent Temporal Discriminative Restricted Boltzmann Machine"
        ]
    },
    {
        "title": "Singing Voice Separation from Monaural Music Based on Kernel Back-Fitting Using Beta-Order Spectral Amplitude Estimation.",
        "author": [
            "Hye-Seung Cho",
            "Jun-Yong Lee",
            "Hyoung-Gook Kim"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417044",
        "url": "https://doi.org/10.5281/zenodo.1417044",
        "ee": "https://zenodo.org/records/1417044/files/ChoLK15.pdf",
        "abstract": "Separating the leading singing voice from the musical",
        "zenodo_id": 1417044,
        "dblp_key": "conf/ismir/ChoLK15"
    },
    {
        "title": "Analysis of Intonation Trajectories in Solo Singing.",
        "author": [
            "Jiajie Dai",
            "Matthias Mauch",
            "Simon Dixon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417169",
        "url": "https://doi.org/10.5281/zenodo.1417169",
        "ee": "https://zenodo.org/records/1417169/files/DaiMD15.pdf",
        "abstract": "We present a new dataset for singing analysis and mod- elling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: \u201cEdelweiss\u201d, \u201cDo-Re- Mi\u201d and \u201cMy Favourite Things\u201d. 39 participants sang three repetitions of each excerpt without accompaniment, result- ing in a dataset of 21762 notes in 117 recordings. To ob- tain pitch estimates we used the Tony software\u2019s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers\u2019 pitch accuracy correlates signifi- cantly with self-reported singing skill and musical train- ing. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a sig- nificant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal ref- erence deviation (TRD). The data and software are freely available. 1",
        "zenodo_id": 1417169,
        "dblp_key": "conf/ismir/DaiMD15",
        "keywords": [
            "new dataset",
            "pitch accuracy",
            "pitch trajectories",
            "Tony software",
            "automatic transcription",
            "manual correction",
            "Singing analysis",
            "modelling",
            "self-reported singing skill",
            "musical training"
        ]
    },
    {
        "title": "The MIR Perspective on the Evolution of Dynamics in Mainstream Music.",
        "author": [
            "Emmanuel Deruty",
            "Fran\u00e7ois Pachet"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417995",
        "url": "https://doi.org/10.5281/zenodo.1417995",
        "ee": "https://zenodo.org/records/1417995/files/DerutyP15.pdf",
        "abstract": "Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called \u201cloudness war\u201d has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track\u2019s year of release, rather than to its genre.",
        "zenodo_id": 1417995,
        "dblp_key": "conf/ismir/DerutyP15",
        "keywords": [
            "Mainstream music",
            "MIR perspective",
            "Dynamics",
            "Genre",
            "Loudness war",
            "Peak year",
            "Track release",
            "Impact of genre",
            "Overriding dynamics",
            "Diachronic analysis"
        ]
    },
    {
        "title": "Theme And Variation Encodings with Roman Numerals (TAVERN): A New Data Set for Symbolic Music Analysis.",
        "author": [
            "Johanna Devaney",
            "Claire Arthur",
            "Nathaniel Condit-Schultz",
            "Kirsten Nisula"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417497",
        "url": "https://doi.org/10.5281/zenodo.1417497",
        "ee": "https://zenodo.org/records/1417497/files/DevaneyACN15.pdf",
        "abstract": "The Theme And Variation Encodings with Roman Nu- merals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the ef- fectiveness of automatic analysis algorithms in generaliz- ing across different musical textures. The pieces are en- coded in standard **kern format, with analyses jointly en- coded using an extension to **kern. The harmonic con- tent of the music was analyzed with both Roman numer- als and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, al- lowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level har- monic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase).",
        "zenodo_id": 1417497,
        "dblp_key": "conf/ismir/DevaneyACN15",
        "keywords": [
            "The Theme And Variation Encodings with Roman Numerals (TAVERN) dataset",
            "27 complete sets of theme and variations",
            "piano compositions by Mozart and Beethoven",
            "harmonic structures realized in different ways",
            "evaluation of automatic analysis algorithms",
            "kern format",
            "jointly encoded analyses",
            "harmonic content analysis",
            "Roman numerals and function labels",
            "musical phrases"
        ]
    },
    {
        "title": "Cross-Version Singing Voice Detection in Classical Opera Recordings.",
        "author": [
            "Christian Dittmar",
            "Bernhard Lehner",
            "Thomas Pr\u00e4tzlich",
            "Meinard M\u00fcller",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416958",
        "url": "https://doi.org/10.5281/zenodo.1416958",
        "ee": "https://zenodo.org/records/1416958/files/DittmarLPMW15.pdf",
        "abstract": "In the field of Music Information Retrieval (MIR), the au- tomated detection of the singing voice within a given mu- sic recording constitutes a challenging and important re- search problem. In this study, our goal is to find those seg- ments within a classical opera recording, where one or sev- eral singers are active. As our main contributions, we first propose a novel audio feature that extends a state-of-the- art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to im- prove the results in the case that the test data is not reflected well by the training data. Third, we show that a cross- version approach can help to stabilize the results even fur- ther. 1",
        "zenodo_id": 1416958,
        "dblp_key": "conf/ismir/DittmarLPMW15",
        "keywords": [
            "Music Information Retrieval",
            "singing voice detection",
            "classical opera recording",
            "audio feature",
            "state-of-the-art feature set",
            "singing voice detection in popular music recordings",
            "bootstrapping procedure",
            "cross-version approach",
            "stabilize the results",
            "improve the results"
        ]
    },
    {
        "title": "Automated Estimation of Ride Cymbal Swing Ratios in Jazz Recordings.",
        "author": [
            "Christian Dittmar",
            "Martin Pfleiderer",
            "Meinard M\u00fcller"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418149",
        "url": "https://doi.org/10.5281/zenodo.1418149",
        "ee": "https://zenodo.org/records/1418149/files/DittmarPM15.pdf",
        "abstract": "In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drum- mers\u2019 swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and eval- uation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical con- tribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for esti- mating swing ratios, circumventing the error-prone detec- tion of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research. 1",
        "zenodo_id": 1418149,
        "dblp_key": "conf/ismir/DittmarPM15",
        "keywords": [
            "microtiming",
            "drummers",
            "jazz recordings",
            "automatic analysis",
            "swing ratio",
            "RC onsets",
            "log-lag autocorrelation function (LLACF)",
            "jazz database",
            "experiment",
            "implications"
        ]
    },
    {
        "title": "Let it Bee - Towards NMF-Inspired Audio Mosaicing.",
        "author": [
            "Jonathan Driedger",
            "Thomas Pr\u00e4tzlich",
            "Meinard M\u00fcller"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415698",
        "url": "https://doi.org/10.5281/zenodo.1415698",
        "ee": "https://zenodo.org/records/1415698/files/DriedgerPM15.pdf",
        "abstract": "A swarm of bees buzzing \u201cLet it be\u201d by the Beatles or the wind gently howling the romantic \u201cGute Nacht\u201d by Schu- bert \u2013 these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automati- cally generating audio mosaics with the objective to pre- serve the source\u2019s timbre in the mosaic. Inspired by algo- rithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target record- ing. However, when applying the original NMF proce- dure, the resulting mosaic does not adequately reflect the source\u2019s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source\u2019s timbral character- istics in the resulting mosaic.",
        "zenodo_id": 1415698,
        "dblp_key": "conf/ismir/DriedgerPM15",
        "keywords": [
            "audio mosaics",
            "convey musical aspects",
            "preserve sources timbre",
            "non-negative matrix factorization",
            "activation matrix",
            "spectrogram",
            "sparse diagonal structures",
            "sources timbral characteristics",
            "iterative learning procedure",
            "extended update rules"
        ]
    },
    {
        "title": "Raga Verification in Carnatic Music Using Longest Common Segment Set.",
        "author": [
            "Shrey Dutta",
            "Krishnaraj Sekhar PV",
            "Hema A. Murthy"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417751",
        "url": "https://doi.org/10.5281/zenodo.1417751",
        "ee": "https://zenodo.org/records/1417751/files/DuttaPM15.pdf",
        "abstract": "There are at least 100 r\u00afagas that are regularly performed in Carnatic music concerts. The audience determines the identity of r\u00afagas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A r\u00afaga verification framework is therefore suggested. The r\u00afaga verification system assumes that a specific r\u00afaga is claimed based on similarity of movements and motivic pat- terns. The system then checks whether this claimed r\u00afaga is correct. For every r\u00afaga, a set of cohorts are chosen. A r\u00afaga and its cohorts are represented using pallavi lines of com- positions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a r\u00afaga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 r\u00afagas from Charsur Foundation 1 is used for analysis. An equal error rate (EER) of 12% is obtained. 1",
        "zenodo_id": 1417751,
        "dblp_key": "conf/ismir/DuttaPM15",
        "keywords": [
            "Carnatic music concerts",
            "audience determines r\u00afagas",
            "listener imitation",
            "r\u00afaga verification framework",
            "specific r\u00afaga claimed",
            "motivic patterns",
            "pallavi lines",
            "Longest Common Segment Set (LCSS)",
            "dataset of 30 r\u00afagas",
            "equal error rate (EER)"
        ]
    },
    {
        "title": "Musicology of Early Music with Europeana Tools and Services.",
        "author": [
            "Erik Duval",
            "Marnix van Berchum",
            "Anja Jentzsch",
            "Gonzalo Alberto Parra Chico",
            "Andreas Drakos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417509",
        "url": "https://doi.org/10.5281/zenodo.1417509",
        "ee": "https://zenodo.org/records/1417509/files/DuvalBJCD15.pdf",
        "abstract": "The Europeana repository hosts large collections of digit- ized music manuscripts and prints. This paper investi- gates how tools and services for this repository can ena- ble Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out re- search that is impossible to do without such tools or ser- vices. We report on the methodology, user-centered de- velopment of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their re- search. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Mu- sic Recognition and computer-supported analysis of mu- sic scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repos- itory.",
        "zenodo_id": 1417509,
        "dblp_key": "conf/ismir/DuvalBJCD15",
        "keywords": [
            "Early Music musicologists",
            "research tools",
            "data sharing",
            "workflow integration",
            "facets filtering",
            "music score analysis",
            "activity streams",
            "less linear organization",
            "repository consumption",
            "research results"
        ]
    },
    {
        "title": "Searching Lyrical Phrases in A-Capella Turkish Makam Recordings.",
        "author": [
            "Georgi Dzhambazov",
            "Sertan Sent\u00fcrk",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1287656",
        "url": "https://doi.org/10.5281/zenodo.1287656",
        "ee": "http://ismir2015.uma.es/articles/249_Paper.pdf",
        "abstract": "Turkish makam acapella sections dataset issung by professional singers and isa collection of recordings of compositions from the vocal form \u015fark\u0131.They are selected to be the same as the recordings in version two of http://compmusic.upf.edu/turkish-sarki\n\nThe main intention is to provide acapella counterpart to polyphonic recordings. \n\nTHE DATASET \n\nAudio music content\n\nThe collection has annotations with section, lyrics phrases and lyrics words. Each section, lyrics word and lyrical phrase is aligned to its corresponding segment in the audio.Annotations of secitons (arana\u011fme, zemin etc.) are taken fromhttps://github.com/MTG/turkish_makam_section_dataset\n\nFORMAT: All annotations in TextGrid (used in Praat)\n\nturkish-makam-acapella-sections-dataset-2.0.zip is organised by artist and makam_acapella-master_1.0.zip is organised by musicbrainz ID.\n\nUsing this dataset\n\nPlease cite one of the following publications if you use the dataset in your work:\n\n\nDzhambazov, G., Serra X.(2015).Modeling of Phoneme Durations for Alignment between Polyphonic Audio and Lyrics.Sound and Music Computing Conference 2015.\n\n\nOr\n\n\nDzhambazov, G., \u015eentrk S.,  Serra X. (2015). Searching Lyrical Phrases in A-Capella Turkish Makam Recordings. 16th International Society for Music Information Retrieval (ISMIR) Conference\n\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nIf you have any questions or comments about the dataset, please feel free to write to us.\n\nGeorgi Dzhambazov\nMusic Technology Group,\nUniversitat Pompeu Fabra,\nBarcelona, Spain\ngeorgi dot dzhambazov at upf dotedu\n\n\n\nhttp://compmusic.upf.edu/turkish-makam-acapella-sections-dataset",
        "zenodo_id": 1287656,
        "dblp_key": "conf/ismir/DzhambazovSS15"
    },
    {
        "title": "I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.",
        "author": [
            "Hamid Eghbal-zadeh",
            "Bernhard Lehner",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416762",
        "url": "https://doi.org/10.5281/zenodo.1416762",
        "ee": "https://zenodo.org/records/1416762/files/Eghbal-zadehLSW15.pdf",
        "abstract": "In this paper, we present a novel approach to extract song- level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefficient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame- level features. The i-vectors provide a low-dimensional and fixed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance be- tween i-vectors in order to predict the genre of songs. Second, for a supervised artist classification task we re- port the performance measures using multiple classifiers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similar- ity (which uses extra information such as rhythm). In artist classification using timbre descriptors, our method outper- formed the state of the art.",
        "zenodo_id": 1416762,
        "dblp_key": "conf/ismir/Eghbal-zadehLSW15",
        "keywords": [
            "timbral features",
            "Mel-frequency cepstral coefficient (MFCC)",
            "identity vectors",
            "factor analysis procedure",
            "unsupervised music similarity estimation",
            "genre prediction",
            "supervised artist classification",
            "performance measures",
            "multiple classifiers",
            "state of the art"
        ]
    },
    {
        "title": "Quantifying Lexical Novelty in Song Lyrics.",
        "author": [
            "Robert J. Ellis",
            "Zhe Xing",
            "Jiakun Fang",
            "Ye Wang 0007"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417577",
        "url": "https://doi.org/10.5281/zenodo.1417577",
        "ee": "https://zenodo.org/records/1417577/files/EllisXFW15.pdf",
        "abstract": "Novelty is an important psychological construct that affects both perceptual and behavioral processes.  Here, we propose a lexical novelty score (LNS) for a song\u2019s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/).  A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words.  An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist.  Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine\u2019s lists of \u201cAll-Time Top 100\u201d songs and artists had significantly lower LNSs than \u201cnon-top\u201d songs and artists.  An affirmative and highly consistent answer was found in both cases.  These results highlight the potential utility of the LNS as a feature for MIR.",
        "zenodo_id": 1417577,
        "dblp_key": "conf/ismir/EllisXFW15",
        "keywords": [
            "Novelty",
            "psychological",
            "construct",
            "perceptual",
            "processes",
            "lexical",
            "novelty",
            "score",
            "lyrics",
            "corpus"
        ]
    },
    {
        "title": "Improving Visualization of High-Dimensional Music Similarity Spaces.",
        "author": [
            "Arthur Flexer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416472",
        "url": "https://doi.org/10.5281/zenodo.1416472",
        "ee": "https://zenodo.org/records/1416472/files/Flexer15.pdf",
        "abstract": "Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that ap- pear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phe- nomenon impacts three popular approaches to compute two- dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces be- fore low dimensional projection.",
        "zenodo_id": 1416472,
        "dblp_key": "conf/ismir/Flexer15",
        "keywords": [
            "Visualizations",
            "Music databases",
            "Intuitive exploration",
            "Music similarity spaces",
            "Negative impact",
            "Hubs and anti-hubs",
            "Empirical study",
            "Three popular approaches",
            "Two-dimensional visualizations",
            "High-dimensional spaces"
        ]
    },
    {
        "title": "Correlating Extracted and Ground-Truth Harmonic Data in Music Retrieval Tasks.",
        "author": [
            "Dylan Freedman",
            "Eddie Kohler",
            "Hans Tutschku"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418111",
        "url": "https://doi.org/10.5281/zenodo.1418111",
        "ee": "https://zenodo.org/records/1418111/files/FreedmanKT15.pdf",
        "abstract": "We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground- truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use estab- lished chord identification algorithms to produce a cor- responding extracted chord label dataset. We imple- ment methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison param- eters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global opti- mization algorithm to stochastically search for the best pa- rameter set to maximize the rank correlation for two har- monic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and ex- tracted chord labels, and returns a rank correlation coeffi- cient. The second task queries the set of songs with fabri- cated chord progressions, ranks each query\u2019s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing.",
        "zenodo_id": 1418111,
        "dblp_key": "conf/ismir/FreedmanKT15",
        "keywords": [
            "traditional music information retrieval",
            "well-chosen parameters",
            "computationally extracted chord annotations",
            "ground-truth annotations",
            "established chord identification algorithms",
            "corresponding extracted chord label dataset",
            "local alignment scores",
            "harmonic retrieval tasks",
            "chord progression comparison parameters",
            "black-box global optimization algorithm"
        ]
    },
    {
        "title": "A Statistical View on the Expressive Timing of Piano Rolled Chords.",
        "author": [
            "Mutian Fu",
            "Guangyu Xia",
            "Roger B. Dannenberg",
            "Larry A. Wasserman"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417034",
        "url": "https://doi.org/10.5281/zenodo.1417034",
        "ee": "https://zenodo.org/records/1417034/files/FuXDW15.pdf",
        "abstract": "Rolled or arpeggiated chords are notated chords per- formed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano per- formance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the on- sets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equiva- lent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music perfor- mance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians.",
        "zenodo_id": 1417034,
        "dblp_key": "conf/ismir/FuXDW15",
        "keywords": [
            "Rolled or arpeggiated chords",
            "performed by playing the notes sequentially",
            "expressive timing",
            "piano performance",
            "equivalent onset",
            "onset span",
            "research questions",
            "local tempo estimation",
            "Analysis of Variance",
            "piano duet dataset"
        ]
    },
    {
        "title": "Efficient Melodic Query Based Audio Search for Hindustani Vocal Compositions.",
        "author": [
            "Kaustuv Kanti Ganguli",
            "Abhinav Rastogi",
            "Vedhas Pandit",
            "Prithvi Kantan",
            "Preeti Rao"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417203",
        "url": "https://doi.org/10.5281/zenodo.1417203",
        "ee": "https://zenodo.org/records/1417203/files/GanguliRPKR15.pdf",
        "abstract": "Time-series pattern matching methods that incorporate time warping have recently been used with varying de- grees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal mu- sic. While these methods perform effectively due to the minimal assumptions they place on the nature of the sam- pled pitch temporal trajectories, their practical applicabil- ity to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational com- plexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algo- rithms, the compressed representation of the pitch time series itself is not well understood given the pervasive- ness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by do- main knowledge to design the representation and to opti- mize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal composi- tions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at or- ders of magnitude reduction in complexity.",
        "zenodo_id": 1417203,
        "dblp_key": "conf/ismir/GanguliRPKR15",
        "keywords": [
            "time warping",
            "melodic phrases",
            "audio search",
            "pitch inflections",
            "dimensionality reduction",
            "string matching",
            "domain knowledge",
            "representation design",
            "parameter optimization",
            "audio query"
        ]
    },
    {
        "title": "Classical Music on the Web - User Interfaces and Data Representations.",
        "author": [
            "Martin Gasser",
            "Andreas Arzt",
            "Thassilo Gadermaier",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417717",
        "url": "https://doi.org/10.5281/zenodo.1417717",
        "ee": "https://zenodo.org/records/1417717/files/GasserAGGW15.pdf",
        "abstract": "We present a set of web-based user interfaces for explo- rative analysis and visualization of classical orchestral mu- sic and a web API that serves as a backend to those ap- plications; we describe use cases that motivated our devel- opments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval re- search groups and a world-renowned symphony orchestra. Furthermore, we describe two real-world applications that involve the work presented here. Firstly, our web ap- plications are used in the editorial stage of a periodically released subscription-based mobile app by the Royal Con- certgebouw Orchestra (RCO) 1 , which serves as a content- distribution channel for multi-modally enhanced record- ings of classical concerts. Secondly, our web API and user interfaces have been successfully used to provide real-time information (such as the score, and explanatory comments from musicologists) to the audience during a live concert of the RCO.",
        "zenodo_id": 1417717,
        "dblp_key": "conf/ismir/GasserAGGW15",
        "keywords": [
            "classical orchestral music",
            "exploratory analysis",
            "visualization",
            "web-based user interfaces",
            "backend API",
            "PHENICX project",
            "Royal Concertgebouw Orchestra",
            "subscription-based mobile app",
            "real-time information",
            "live concert"
        ]
    },
    {
        "title": "PAD and SAD: Two Awareness-Weighted Rhythmic Similarity Distances.",
        "author": [
            "Daniel G\u00f3mez-Mar\u00edn",
            "Sergi Jord\u00e0",
            "Perfecto Herrera"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416286",
        "url": "https://doi.org/10.5281/zenodo.1416286",
        "ee": "https://zenodo.org/records/1416286/files/Gomez-MarinJH15.pdf",
        "abstract": "Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we ob- served that measures of similarity may differ given the presence or absence of a pulse inducing sound and the im- portance of those measures is not constant along the pat- tern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the percep- tual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the tem- poral positions of the beat along the bar. We show that with these improvements, the correlation between the pre- viously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops.",
        "zenodo_id": 1416286,
        "dblp_key": "conf/ismir/Gomez-MarinJH15",
        "keywords": [
            "rhythm similarity",
            "perception of rhythms",
            "discriminating regions",
            "existing similarity metrics",
            "pulse inducing sound",
            "improvements",
            "homogeneous contribution",
            "temporal positions",
            "predictions based on metrics",
            "audio loops"
        ]
    },
    {
        "title": "Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations.",
        "author": [
            "Thomas Grill",
            "Jan Schl\u00fcter"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417461",
        "url": "https://doi.org/10.5281/zenodo.1417461",
        "ee": "https://zenodo.org/records/1417461/files/GrillS15.pdf",
        "abstract": "The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and hu- man annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectro- grams with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hier- archical nature of structural organization, we explore dif- ferent strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data.",
        "zenodo_id": 1417461,
        "dblp_key": "conf/ismir/GrillS15",
        "keywords": [
            "Convolutional Neural Networks",
            "spectrogram features",
            "self-similarity lag matrices",
            "structural boundaries",
            "audio features",
            "structural organization",
            "two-level annotations",
            "boundary recognition performance",
            "significant improvement",
            "algorithm prediction"
        ]
    },
    {
        "title": "Comparing Voice and Stream Segmentation Algorithms.",
        "author": [
            "Nicolas Guiomard-Kagan",
            "Mathieu Giraud",
            "Richard Groult",
            "Florence Lev\u00e9"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414916",
        "url": "https://doi.org/10.5281/zenodo.1414916",
        "ee": "https://zenodo.org/records/1414916/files/Guiomard-KaganG15.pdf",
        "abstract": "Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a bet- ter understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with poly- phonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Tem- perley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the al- gorithms on a corpus containing the 48 fugues of Well- Tempered Clavier by J. S. Bach as well as 97 files of pop- ular music containing actual polyphonic information. We discuss how to compare together voice and stream segmen- tation algorithms, and discuss their strengths and weak- nesses.",
        "zenodo_id": 1414916,
        "dblp_key": "conf/ismir/Guiomard-KaganG15",
        "keywords": [
            "polyphonic data",
            "musical score",
            "voice segmentation",
            "stream segmentation",
            "contiguous notes",
            "close pitches",
            "four algorithms",
            "corpus of fugues",
            "pop music",
            "evaluation metrics"
        ]
    },
    {
        "title": "Improving Melodic Similarity in Indian Art Music Using Culture-Specific Melodic Characteristics.",
        "author": [
            "Sankalp Gulati",
            "Joan Serr\u00e0",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418261",
        "url": "https://doi.org/10.5281/zenodo.1418261",
        "ee": "https://zenodo.org/records/1418261/files/GulatiSS15.pdf",
        "abstract": "Detecting the occurrences of r\u00afags\u2019 characteristic melodic phrases from polyphonic audio recordings is a fundamen- tal task for the analysis and retrieval of Indian art mu- sic. We propose an abstraction process and a complex- ity weighting scheme which improve melodic similarity by exploiting specific melodic characteristics in this music. In addition, we propose a tetrachord normalization to han- dle transposed phrase occurrences. The melodic abstrac- tion is based on the partial transcription of the steady re- gions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the dif- ferences in the melodic complexities of the phrases, a cru- cial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data compris- ing 625 annotated melodic phrases belonging to 10 differ- ent phrase categories. Results show that the proposed mel- odic abstraction and complexity weighting schemes sig- nificantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for deal- ing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the appli- cability of the proposed approach to other melody domi- nant music traditions such as Flamenco, Beijing opera and Turkish Makam music.",
        "zenodo_id": 1418261,
        "dblp_key": "conf/ismir/GulatiSS15",
        "keywords": [
            "fundamental task",
            "Indian art music",
            "melodic similarity",
            "Carnatic music",
            "melodic abstraction",
            "complexity weighting",
            "partial transcription",
            "duration truncation",
            "phrase detection accuracy",
            "tetrachord normalization"
        ]
    },
    {
        "title": "Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings.",
        "author": [
            "Swapnil Gupta",
            "Ajay Srinivasamurthy",
            "Manoj Kumar 0007",
            "Hema A. Murthy",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1267024",
        "url": "https://doi.org/10.5281/zenodo.1267024",
        "ee": "http://ismir2015.uma.es/articles/86_Paper.pdf",
        "abstract": "The Tabla Solo dataset is a parallel corpus comprising time-aligned syllabic scores and audio-recordings of 38 solo tabla compositions. The audio and scores for these recordings is from the instructional video DVD titled Shades of Tabla by Pt. Arvind Mulgaonkar.\n\nA companion page to the paper is here:http://compmusic.upf.edu/ismir-2015-tabla\n\nIntroduction\n\nIn Hindustani music, tabla is the main rhythm accompaniment (Examples of individual strokes of tabla can be obtained from here). To showcase the nuances of the t\u0101l (the rhythmic framework of Hindustani music) as well as the skill of the percussionist with the tabla, Hindustani music concerts feature a tabla solo. A tabla solo is intricate and elaborate, with a variety of pre-composed forms used for developing further elaborations. There are specific principles that govern these elaborations. Musical forms of tabla such as the th\u0113k\u0101, k\u0101yad\u0101, palat\u0101, \u0323 r\u0113l\u0101, p\u0113\u015bk\u0101r and gat \u0323are a part of the solo performance and have different functional and aesthetic roles in a solo performance. Harmonium or sarangi usually plays the role of a time-keeper in tabla solo performances.\n\n\n\nPercussion in Hindustani music is organized and orally transmitted with the use of onomatopoeic mnemonic syllables (called the b\u014dl) representative of the different strokes of tabla. Further, tabla has different stylistic schools called ghar\u0101n\u0101s. The repertoires of major ghar\u0101n\u0101s or schools of tabla differ in aspects such as the use of specific b\u014dls, the dynamics of strokes, ornamentation and rhythmic phrases. But there are also many similarities due to the fact that same forms and same standard phrases reappear across these repertoires.\n\nThe Dataset\n\nThe syllabic representation for tabla solos provide a meaningful representation for analysis. This dataset uses a such a representation. The dataset comprises audio recordings, scores and time aligned syllabic transcriptions for 38 tabla solo compositions of different forms in t\u012bnt\u0101l (a metrical cycle of 16 time units). The compositions are from the instructional video DVD Shades Of Tabla by Pandit Arvind Mulgaonkar, who is among the most renowned contemporary tabla maestros. Out of the 120 compositions in the DVD, we chose 38 representative compositions spanning all the ghar\u0101n\u0101s of tabla (Ajrada, Benaras, Dilli, Lucknow, Punjab, Farukhabad). The dataset contains about 17 minutes of audio with over 8200 syllables.\n\nAudio\n\nThe audio is extracted from the DVD video and segmented at the level of compositions from the full audio recording. The audio files are mono wav files, sampled at 44.1 kHz with a bit depth of 16 bits. All audios have a soft harmonium accompaniment.\n\nAnnotations\n\nThe booklet accompanying the DVD provides a syllabic transcription for each composition. We used Tesseract, an open source Optical Character Recognizer (OCR) engine to convert printed scores to a machine readable format. The scores obtained from OCR were manually verified and corrected for errors, adding the the vibh\u0101gs (sections) of the t\u0101l to the syllabic transcription. A time aligned syllabic transcription for each score and audio file pair was obtained using a spectral flux based onset detector followed by manual correction. The score for each composition has additional metadata describing ghar\u0101n\u0101, composer and its musical form.\n\nThe scores in the booklet consists of 41 different mnemonic syllables that are reduced and mapped to 18 syllables based on the timbral similarity between the syllables. The list of syllables along with their mapping can be found here: Syllable Mappings\n\nDataset Organization\n\nThe dataset consists of set of four files for each composition:\n\n\n\t\n\tWAV audio file (*.wav)\n\t\n\t\n\tThe syllable scores as retrieved from the booklet with the metadata (*.txt)\n\t\n\t\n\tTime-aligned non-mapped syllabic score with stroke onset times (*.csv)\n\t\n\t\n\tTime-aligned mapped syllabic score with stroke onset times (*.csv)\n\t\n\n\nPossible Uses of the Dataset\n\nThe dataset can be used for variety of of MIR tasks such as onset detection, percussion transcription, rhythm and percussion pattern analysis, and tabla stroke modeling.\n\nUsing this dataset\n\nIf you use the dataset in your work, please cite the following publication:\n\n\nS. Gupta, A. Srinivasamurthy, M. Kumar, H. A. Murthy, X. Serra. Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings. In Proc. of the 16th International Society for Music Information Retrieval Conference (ISMIR), 2015.\n\n\nhttp://hdl.handle.net/10230/25697\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nAjay Srinivasamurthy\nPhD Student, Music Technology Group\nUniversitat Pompeu Fabra,\nBarcelona, Spain\najays.murthy@upf.edu\n\nSwapnil Gupta\nMasters Student, Sound and Music Computing\nUniversitat Pompeu Fabra,\nBarcelona, Spain\nsuapnilgupta.iiith@gmail.com\n\nXavier Serra\nHead, Music Technology Group\nUniversitat Pompeu Fabra,\nBarcelona, Spain\nxavier.serra@upf.edu\n\n\n\nhttp://compmusic.upf.edu/tabla-solo-dataset",
        "zenodo_id": 1267024,
        "dblp_key": "conf/ismir/GuptaSKMS15",
        "keywords": [
            "Tabla Solo dataset",
            "parallel corpus",
            "time-aligned syllabic scores",
            "audio-recordings",
            "38 solo tabla compositions",
            "instructional video DVD",
            "Shades of Tabla",
            "Hindustani music concerts",
            "tabla solo",
            "intricate and elaborate"
        ]
    },
    {
        "title": "Automatic Handwritten Mensural Notation Interpreter: From Manuscript to MIDI Performance.",
        "author": [
            "Yu-Hui Huang",
            "Xuanli Chen",
            "Serafina Beck",
            "David Burn",
            "Luc Van Gool"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418267",
        "url": "https://doi.org/10.5281/zenodo.1418267",
        "ee": "https://zenodo.org/records/1418267/files/HuangCBBG15.pdf",
        "abstract": "This paper presents a novel automatic recognition frame- work for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promis- ing performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of- interest detection, ii) music symbol detection and classifi- cation, and iii) transcription to modern music. In addition to the output in modern notation, our system can gener- ate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources avail- able to non-specialists as well, as they can now access such ancient music in a better understandable form.",
        "zenodo_id": 1418267,
        "dblp_key": "conf/ismir/HuangCBBG15",
        "keywords": [
            "hand-written mensural music",
            "novel automatic recognition framework",
            "scanned manuscript",
            "modern music scores",
            "promising performance",
            "complete pipeline",
            "region-of-interest detection",
            "music symbol detection and classification",
            "transcription to modern music",
            "MIDI file generation"
        ]
    },
    {
        "title": "Four Timely Insights on Automatic Chord Estimation.",
        "author": [
            "Eric J. Humphrey",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417549",
        "url": "https://doi.org/10.5281/zenodo.1417549",
        "ee": "https://zenodo.org/records/1417549/files/HumphreyB15.pdf",
        "abstract": "Automatic chord estimation (ACE) is a hallmark re- search topic in content-based music informatics, but like many other tasks, system performance appears to be con- verging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite ar- guably achieving some of the highest results to date, both approaches plateau well short of having solved the prob- lem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that inval- idate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, stan- dard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conven- tional approaches conflate the competing goals of recogni- tion and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjec- tive, making the very notion of \u201cground truth\u201d annotations tenuous. Synthesizing these observations, this paper of- fers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large.",
        "zenodo_id": 1417549,
        "dblp_key": "conf/ismir/HumphreyB15",
        "keywords": [
            "Automatic chord estimation",
            "glass ceiling",
            "machine perception domains",
            "large-vocabulary ACE",
            "performance plateau",
            "music recordings",
            "harmony and tonality",
            "standard lexicons",
            "natural relationships",
            "conventional approaches"
        ]
    },
    {
        "title": "In Their Own Words: Using Text Analysis to Identify Musicologists&apos; Attitudes towards Technology.",
        "author": [
            "Charles Inskip",
            "Frans Wiering"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416422",
        "url": "https://doi.org/10.5281/zenodo.1416422",
        "ee": "https://zenodo.org/records/1416422/files/InskipW15.pdf",
        "abstract": "A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and pro- vides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point ap- proach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that ef- forts should be made into supporting development of their digital skills and providing usable, useful and relia- ble software created with a \u2018musicology-centred\u2019 design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level.",
        "zenodo_id": 1416422,
        "dblp_key": "conf/ismir/InskipW15",
        "keywords": [
            "online survey",
            "quantitative data",
            "qualitative data",
            "musicologists",
            "digital humanities",
            "technology use",
            "research practices",
            "resource format",
            "digital skills",
            "comments on rewards"
        ]
    },
    {
        "title": "Automatic Transcription of Ornamented Irish Traditional Flute Music Using Hidden Markov Models.",
        "author": [
            "Peter Jancovic",
            "M\u00fcnevver K\u00f6k\u00fcer",
            "Wrena Baptiste"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415116",
        "url": "https://doi.org/10.5281/zenodo.1415116",
        "ee": "https://zenodo.org/records/1415116/files/JancovicKB15.pdf",
        "abstract": "This paper presents an automatic system for note tran- scription of Irish traditional flute music containing orna- mentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hid- den Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cep- stral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evalu- ations are performed on monophonic flute recordings from Grey Larsen\u2019s CD. The performance of the system is eval- uated in terms of the transcription of notes as well as detec- tion of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset de- tection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency.",
        "zenodo_id": 1415116,
        "dblp_key": "conf/ismir/JancovicKB15",
        "keywords": [
            "automatic system",
            "note transcription",
            "Irish traditional flute music",
            "ornamentation",
            "hidden Markov models",
            "mel-frequency cepstral coefficients",
            "signal energy",
            "fundamental frequency",
            "F-measure",
            "onset detection"
        ]
    },
    {
        "title": "A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments.",
        "author": [
            "Berit Janssen",
            "Peter van Kranenburg",
            "Anja Volk"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418357",
        "url": "https://doi.org/10.5281/zenodo.1418357",
        "ee": "https://zenodo.org/records/1418357/files/JanssenKV15.pdf",
        "abstract": "To find occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to find occurrences of phrases in folk song melodies. We compare the similar- ity measures correlation distance, city-block distance, Eu- clidean distance and alignment, proposed for melody com- parison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic mel- odic similarity; moreover, wavelet transform and the ge- ometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the suc- cess of the different similarity measures through observing retrieval success in relation to human annotations. Our re- sults show that local alignment and SIAM perform on an almost equal level to human annotators.",
        "zenodo_id": 1418357,
        "dblp_key": "conf/ismir/JanssenKV15",
        "keywords": [
            "melodic segments",
            "themes",
            "phrases",
            "motifs",
            "human analysis",
            "large music corpora",
            "melodic similarity measures",
            "folk song melodies",
            "correlation distance",
            "city-block distance"
        ]
    },
    {
        "title": "Instrument Identification in Optical Music Recognition.",
        "author": [
            "Yucong Jiang",
            "Christopher Raphael"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416116",
        "url": "https://doi.org/10.5281/zenodo.1416116",
        "ee": "https://zenodo.org/records/1416116/files/JiangR15.pdf",
        "abstract": "We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instru- ments in the system. Each instrument may apply to sev- eral staves, may be represented with several possible text strings, and may appear at several possible positions rela- tive to the staves. We find the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character recog- nition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text la- beling for each system, as well as the character template models, thus adapting to the font at hand. Our implemen- tation alternately optimizes over the text label identifica- tion and re-estimates the character templates. Experiments are presented on 10 different scores showing a significant improvement due to adaptation.",
        "zenodo_id": 1416116,
        "dblp_key": "conf/ismir/JiangR15",
        "keywords": [
            "score",
            "instrument",
            "orchestra",
            "recognition",
            "staves",
            "optical",
            "music",
            "template",
            "character",
            "optimization"
        ]
    },
    {
        "title": "Graph-Based Rhythm Interpretation.",
        "author": [
            "Rong Jin 0004",
            "Christopher Raphael"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418287",
        "url": "https://doi.org/10.5281/zenodo.1418287",
        "ee": "https://zenodo.org/records/1418287/files/JinR15.pdf",
        "abstract": "We present a system that interprets the notated rhythm ob- tained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the con- nected graph. The graph identification problem is cast as an optimization where each potential edge is scored ac- cording to its plausibility. We seek the optimally scor- ing graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhyth- mic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets.",
        "zenodo_id": 1418287,
        "dblp_key": "conf/ismir/JinR15",
        "keywords": [
            "system",
            "interprets",
            "notated",
            "rhythm",
            "optical",
            "music",
            "recognition",
            "graph",
            "edges",
            "optimization"
        ]
    },
    {
        "title": "Evaluating the General Chord Type Representation in Tonal Music and Organising GCT Chord Labels in Functional Chord Categories.",
        "author": [
            "Maximos A. Kaliakatsos-Papakostas",
            "Asterios I. Zacharakis",
            "Costas Tsougras",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417379",
        "url": "https://doi.org/10.5281/zenodo.1417379",
        "ee": "https://zenodo.org/records/1417379/files/Kaliakatsos-Papakostas15.pdf",
        "abstract": "The General Chord Type (GCT) representation is ap- propriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descrip- tive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert an- notations (Kostka & Payne harmonic dataset). Addition- ally, novel methods for grouping and clustering chords, ac- cording to their GCT encoding and their functional role in chord sequences, are introduced. The results of both har- monic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for rep- resenting effectively harmony in computational systems.",
        "zenodo_id": 1417379,
        "dblp_key": "conf/ismir/Kaliakatsos-Papakostas15",
        "keywords": [
            "harmonic context",
            "tone simultaneities",
            "GCT representation",
            "harmonic rearrangement",
            "standard roman numeral chord type",
            "hierarchic harmonic systems",
            "tonal system",
            "post-tonal music",
            "traditional polyphonic systems",
            "GCT encoding"
        ]
    },
    {
        "title": "Neuroimaging Methods for Music Information Retrieval: Current Findings and Future Prospects.",
        "author": [
            "Blair Kaneshiro",
            "Jacek P. Dmochowski"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416082",
        "url": "https://doi.org/10.5281/zenodo.1416082",
        "ee": "https://zenodo.org/records/1416082/files/KaneshiroD15.pdf",
        "abstract": "Over the past decade and a half, music information re- trieval (MIR) has grown into a robust, cross-disciplinary field spanning a variety of research domains. Collabo- rations between MIR and neuroscience researchers, how- ever, are still rare, and to date only a few studies using approaches from one domain have successfully reached an audience in the other. In this paper, we take an initial step toward bridging these two fields by reviewing studies from the music neuroscience literature, with an emphasis on imaging modalities and analysis techniques that might be of practical interest to the MIR community. We show that certain approaches currently used in a neuroscientific setting align with those used in MIR research, and discuss implications for potential areas of future research. We ad- ditionally consider the impact of disparate research objec- tives between the two fields, and how such a discrepancy may have hindered cross-discipline output thus far. It is hoped that a heightened awareness of this literature will foster interaction and collaboration between MIR and neu- roscience researchers, leading to advances in both fields that would not have been achieved independently.",
        "zenodo_id": 1416082,
        "dblp_key": "conf/ismir/KaneshiroD15",
        "keywords": [
            "music information retrieval (MIR)",
            "collaborations between MIR and neuroscience",
            "imaging modalities",
            "analysis techniques",
            "bridging music neuroscience and MIR",
            "review of studies",
            "neuroscientific approaches",
            "practical interest to MIR",
            "future research directions",
            "disparate research objectives"
        ]
    },
    {
        "title": "An Iterative Multi Range Non-Negative Matrix Factorization Algorithm for Polyphonic Music Transcription.",
        "author": [
            "Anis Khlif",
            "Vidhyasaharan Sethu"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416686",
        "url": "https://doi.org/10.5281/zenodo.1416686",
        "ee": "https://zenodo.org/records/1416686/files/KhlifS15.pdf",
        "abstract": "This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is partic- ularly well suited to the task of automatic music tran- scription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time- frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the pro- posed algorithm proceeds iteratively by initially decom- posing a part of the time-frequency representation into a combination of a small subset of all possible notes then re- investing this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The res- olution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy.",
        "zenodo_id": 1416686,
        "dblp_key": "conf/ismir/KhlifS15",
        "keywords": [
            "Non-negative Matrix Factorisation (NMF)",
            "Automatic music transcription (AMT)",
            "Iterative algorithm",
            "Time-frequency representation",
            "Notes",
            "Resolution of dimensionality problem",
            "Transcription accuracy",
            "Octave",
            "Subset",
            "Large subset"
        ]
    },
    {
        "title": "Extending a Model of Monophonic Hierarchical Music Analysis to Homophony.",
        "author": [
            "Phillip B. Kirlin",
            "David L. Thomas"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416296",
        "url": "https://doi.org/10.5281/zenodo.1416296",
        "ee": "https://zenodo.org/records/1416296/files/KirlinT15.pdf",
        "abstract": "Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learn- ing algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the mu- sic, as opposed to taking all of the musical voices into ac- count. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present de- tails of the new model, an algorithm for determining the most probable analysis of the music, and a number of ex- periments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the result- ing analyses produced.",
        "zenodo_id": 1416296,
        "dblp_key": "conf/ismir/KirlinT15",
        "keywords": [
            "computers",
            "data-driven",
            "Schenkerian analysis",
            "machine learning",
            "music theory",
            "probabilistic models",
            "monophonic music",
            "homophonic music",
            "analysis",
            "rests"
        ]
    },
    {
        "title": "Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.",
        "author": [
            "Peter Knees",
            "\u00c1ngel Faraldo",
            "Perfecto Herrera",
            "Richard Vogl",
            "Sebastian B\u00f6ck",
            "Florian H\u00f6rschl\u00e4ger",
            "Mickael Le Goff"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.4153506",
        "url": "https://doi.org/10.5281/zenodo.4153506",
        "ee": "http://ismir2015.uma.es/articles/246_Paper.pdf",
        "abstract": "The GiantSteps+ EDMKey Dataset includes 600 two-minute sound excerpts from various EDM subgenres, annotated with single-key labels. This dataset focus in problematic Beatport excerpts, so it is biased, but it is interesting to test the robustness of key recognition systems. These 600 tracks have beenanalysed by Daniel G. Camhi and &Aacute;ngel Faraldo, providingpitch-class set descriptions, key and modalchanges, comments and confidence levels for theindividual tracks.\n\nThis dataset is a revision of the originalGiantSteps Key Dataset, available in Github (https://github.com/GiantSteps/giantsteps-key-dataset) and initially described in:\n\nKnees, P., Faraldo, &Aacute;., Herrera, P., Vogl, R., Bck, S., Hrschlger, F., Le Goff, M. (2015). Two Datasets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections. In Proceedings of the 16th International Society for Music Information Retrieval Conference, 364370. Mlaga, Spain.\n\nThe original audio samples belong to online audio snippets from Beatport,an online music store for DJ&#39;s and Electronic Dance Music Producers(http:\\\\www.beatport.com). If this dataset were used in further research, we would appreciate the citation of thecurrent DOI(10.5281/zenodo.1101082) and thefollowing doctoral dissertation, where a detailed description of the properties of this datasetcan be found:\n\n&Aacute;ngel Faraldo (2017).Tonality Estimation in Electronic Dance Music:A Computational and Musically Informed Examination. PhD Thesis. Universitat Pompeu Fabra, Barcelona.",
        "zenodo_id": 4153506,
        "dblp_key": "conf/ismir/KneesFHVBHG15"
    },
    {
        "title": "Corpus-Based Rhythmic Pattern Analysis of Ragtime Syncopation.",
        "author": [
            "Hendrik Vincent Koops",
            "Anja Volk",
            "W. Bas de Haas"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417213",
        "url": "https://doi.org/10.5281/zenodo.1417213",
        "ee": "https://zenodo.org/records/1417213/files/KoopsVH15.pdf",
        "abstract": "This paper presents a corpus-based study on rhythmic pat- terns in the RAG-collection of approximately 11.000 sym- bolically encoded ragtime pieces. While characteristic mu- sical features that define ragtime as a genre have been de- bated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. There- fore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee\u2019s model of syn- copation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short synco- pation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study con- firms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features.",
        "zenodo_id": 1417213,
        "dblp_key": "conf/ismir/KoopsVH15",
        "keywords": [
            "corpus-based study",
            "rhythmic patterns",
            "RAG-collection",
            "ragtime genre",
            "syncopation patterns",
            "Longuet-Higgins and Lees model",
            "genre-specific features",
            "onset density",
            "amount of syncopation",
            "musicological hypothesis"
        ]
    },
    {
        "title": "An Efficient State-Space Model for Joint Tempo and Meter Tracking.",
        "author": [
            "Florian Krebs",
            "Sebastian B\u00f6ck",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414966",
        "url": "https://doi.org/10.5281/zenodo.1414966",
        "ee": "https://zenodo.org/records/1414966/files/KrebsBW15.pdf",
        "abstract": "Dynamic Bayesian networks (e.g., Hidden Markov Mod- els) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhyth- mic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic pa- rameters from a piece of music. While this allows the mutual dependencies between these parameters to be ex- ploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We in- corporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets.",
        "zenodo_id": 1414966,
        "dblp_key": "conf/ismir/KrebsBW15",
        "keywords": [
            "Dynamic Bayesian networks",
            "Meter tracking",
            "Rhythmic parameters",
            "Joint inference",
            "Bar pointer model",
            "State-space discretisation",
            "Tempo transition model",
            "Beat and downbeat tracking",
            "Accuracy improvement",
            "Reduced complexity"
        ]
    },
    {
        "title": "Training Phoneme Models for Singing with &quot;Songified&quot; Speech Data.",
        "author": [
            "Anna M. Kruspe"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416858",
        "url": "https://doi.org/10.5281/zenodo.1416858",
        "ee": "https://zenodo.org/records/1416858/files/Kruspe15.pdf",
        "abstract": "Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several charac- teristics that differentiate it from speech. Therefore, algo- rithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recogni- tion of phonemes in singing. We noticed that this reco- gnition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. Howe- ver, such data does exist for speech. We therefore propose to make phoneme recognition mo- dels more robust for singing by training them on speech data that has artificially been made more \u201csong-like\u201d. We test two main modifications on speech data: Time stret- ching and pitch shifting. Artificial vibrato is also tested. We then evaluate models trained on different combinations of these modified speech recordings. The utilized mode- ling algorithms are Neural Networks and Deep Belief Net- works.",
        "zenodo_id": 1416858,
        "dblp_key": "conf/ismir/Kruspe15",
        "keywords": [
            "speech recognition",
            "singing characteristics",
            "speech algorithms",
            "phoneme recognition",
            "phoneme models",
            "speech data",
            "artificially made",
            "vibrato",
            "speech recordings",
            "modifications"
        ]
    },
    {
        "title": "Towards Support for Understanding Classical Music: Alignment of Content Descriptions on the Web.",
        "author": [
            "Taku Kuribayashi",
            "Yasuhito Asano",
            "Masatoshi Yoshikawa"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416138",
        "url": "https://doi.org/10.5281/zenodo.1416138",
        "ee": "https://zenodo.org/records/1416138/files/KuribayashiAY15.pdf",
        "abstract": "Supporting the understanding of classical music is an im- portant topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Re- cently, Kuribayashi et al. proposed a method for obtain- ing content descriptions from the web. However, the con- tent descriptions on a single page frequently explain a spe- cific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descrip- tions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create ap- plications systems to integrate descriptions with musical scores and performances.",
        "zenodo_id": 1416138,
        "dblp_key": "conf/ismir/KuribayashiAY15",
        "keywords": [
            "supporting",
            "classical",
            "music",
            "understanding",
            "research",
            "fields",
            "text",
            "analysis",
            "acoustics",
            "analysis"
        ]
    },
    {
        "title": "MIREX Grand Challenge 2014 User Experience: Qualitative Analysis of User Feedback.",
        "author": [
            "Jin Ha Lee 0001",
            "Xiao Hu 0001",
            "Kahyun Choi",
            "J. Stephen Downie"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417103",
        "url": "https://doi.org/10.5281/zenodo.1417103",
        "ee": "https://zenodo.org/records/1417103/files/LeeHCD15.pdf",
        "abstract": "Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evalua- tion eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algo- rithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users\u2019 overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the quali- tative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opin- ions, not fully captured by score ratings on the given cri- teria, and demonstrated the challenge of evaluating a va- riety of systems with different user goals. We conclude with a discussion on the implications of findings and rec- ommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility.",
        "zenodo_id": 1417103,
        "dblp_key": "conf/ismir/LeeHCD15",
        "keywords": [
            "Evaluation",
            "Music Information Retrieval",
            "MIREX",
            "User Experience",
            "Grand Challenge 2014",
            "System Interaction",
            "Qualitative Analysis",
            "Free Text Responses",
            "User Opinions",
            "Score Ratings"
        ]
    },
    {
        "title": "Automatic Mashup Creation by Considering both Vertical and Horizontal Mashabilities.",
        "author": [
            "Chuan-Lung Lee",
            "Yin-Tzu Lin",
            "Zun-Ren Yao",
            "Feng-Yi Lee",
            "Ja-Ling Wu"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416712",
        "url": "https://doi.org/10.5281/zenodo.1416712",
        "ee": "https://zenodo.org/records/1416712/files/LeeLYLW15.pdf",
        "abstract": "In this paper, we proposed a system to effectively create music mashups \u2013 a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by over- laying music segments on one single base track, the pro- posed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: \u201charmonic change balance\u201d and \u201cvolume weight\u201d have been considered. On the horizontal side, the meth- ods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encoun- tered and found the proper solution to each of them. Sub- jective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listen- ing enjoyment. Besides, by taking the newly proposed ver- tical mashability measurement into account, the improve- ment in user satisfaction is statistically significant.",
        "zenodo_id": 1416712,
        "dblp_key": "conf/ismir/LeeLYLW15",
        "keywords": [
            "music mashups",
            "re-created music",
            "mixing parts of multiple existing music pieces",
            "proposed system",
            "multiple background and lead track segments",
            "suitability between vertically overlaid tracks",
            "horizontal mashability",
            "harmonic change balance",
            "volume weight",
            "four levels of mashability"
        ]
    },
    {
        "title": "Understanding Users of Commercial Music Services through Personas: Design Implications.",
        "author": [
            "Jin Ha Lee 0001",
            "Rachel Price"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.232222",
        "url": "https://doi.org/10.5281/zenodo.232222",
        "ee": "http://ismir2015.uma.es/articles/12_Paper.pdf",
        "abstract": "Most of the previous literature on music users needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control.",
        "zenodo_id": 232222,
        "dblp_key": "conf/ismir/LeeP15",
        "keywords": [
            "qualitative user study",
            "emerging personas",
            "music information retrieval systems",
            "user engagement",
            "context-switching",
            "system design implications",
            "privacy concerns",
            "design for specialized uses",
            "user interactions",
            "demographic considerations"
        ]
    },
    {
        "title": "Monaural Blind Source Separation in the Context of Vocal Detection.",
        "author": [
            "Bernhard Lehner",
            "Gerhard Widmer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415940",
        "url": "https://doi.org/10.5281/zenodo.1415940",
        "ee": "https://zenodo.org/records/1415940/files/LehnerW15.pdf",
        "abstract": "In this paper, we evaluate the usefulness of several monau- ral blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the re- sults of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source sep- aration algorithms. In order to assess the performance of those strategies on an internal data set, we use two differ- ent feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post- processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of tra- ditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals.",
        "zenodo_id": 1415940,
        "dblp_key": "conf/ismir/LehnerW15",
        "keywords": [
            "monaural",
            "blind source separation",
            "vocal detection",
            "state-of-the-art",
            "source separation algorithms",
            "vocal detection",
            "feature sets",
            "classifiers",
            "post-processing technique",
            "source signals"
        ]
    },
    {
        "title": "Score Following for Piano Performances with Sustain-Pedal Effects.",
        "author": [
            "Bochen Li",
            "Zhiyao Duan"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418127",
        "url": "https://doi.org/10.5281/zenodo.1418127",
        "ee": "https://zenodo.org/records/1418127/files/LiD15.pdf",
        "abstract": "One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the us- age of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration nat- urally ceases. This makes the notes longer than their no- tated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of \u201cnew notes\u201d in the audio repre- sentation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previ- ous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Ex- periments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness.",
        "zenodo_id": 1418127,
        "dblp_key": "conf/ismir/LiD15",
        "keywords": [
            "score following",
            "audio frames",
            "score positions",
            "piano performances",
            "sustain pedal",
            "notes",
            "audio representation",
            "partial notes",
            "sustain-pedal effects",
            "alignment accuracy"
        ]
    },
    {
        "title": "Analysis of Expressive Musical Terms in Violin Using Score-Informed and Expression-Based Audio Features.",
        "author": [
            "Pei-Ching Li",
            "Li Su 0002",
            "Yi-Hsuan Yang",
            "Alvin W. Y. Su"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416784",
        "url": "https://doi.org/10.5281/zenodo.1416784",
        "ee": "https://zenodo.org/records/1416784/files/LiSYS15.pdf",
        "abstract": "The manipulation of different interpretational factors, in- cluding dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer- aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specif- ically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music infor- mation retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expres- sive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction.",
        "zenodo_id": 1416784,
        "dblp_key": "conf/ismir/LiSYS15",
        "keywords": [
            "interpretational factors",
            "dynamics",
            "duration",
            "vibrato",
            "expressive musical terms",
            "computer- aided music education",
            "novel task",
            "direct means",
            "score-informed scheme",
            "classification accuracy"
        ]
    },
    {
        "title": "Musical Offset Detection of Pitched Instruments: The Case of Violin.",
        "author": [
            "Che-Yuan Liang",
            "Li Su 0002",
            "Yi-Hsuan Yang",
            "Hsin-Ming Lin"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416834",
        "url": "https://doi.org/10.5281/zenodo.1416834",
        "ee": "https://zenodo.org/records/1416834/files/LiangSYL15.pdf",
        "abstract": "Musical offset detection is an integral part of a music sig- nal processing system that requires complete characteriza- tion of note events. However, unlike onset detection, off- set detection has seldom been the subject of an in-depth study in the music information retrieval community, possi- bly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. More- over, we conduct a case study of offset detection in vio- lin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset cover- ing 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly anno- tated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well- known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based",
        "zenodo_id": 1416834,
        "dblp_key": "conf/ismir/LiangSYL15",
        "keywords": [
            "Musical offset detection",
            "Note events",
            "Music signal processing",
            "Pitched non-percussive instruments",
            "Annotating offset times",
            "Evaluating offset detection",
            "Vio- lin recordings",
            "Energy-based methods",
            "Spectral flux-based methods",
            "Pitch-based methods"
        ]
    },
    {
        "title": "Content-Aware Collaborative Music Recommendation Using Pre-trained Neural Networks.",
        "author": [
            "Dawen Liang",
            "Minshu Zhan",
            "Daniel P. W. Ellis"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416308",
        "url": "https://doi.org/10.5281/zenodo.1416308",
        "ee": "https://zenodo.org/records/1416308/files/LiangZE15.pdf",
        "abstract": "Although content is fundamental to our music listening preferences, the leading performance in music recommen- dation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user\u2019s listening his- tory rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known \u201ccold-start\u201d prob- lem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content informa- tion into collaborative filtering methods have shown suc- cess in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative fil- tering model. Such a system still allows the user listening data to \u201cspeak for itself\u201d. The proposed system is evalu- ated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case.",
        "zenodo_id": 1416308,
        "dblp_key": "conf/ismir/LiangZE15",
        "keywords": [
            "content",
            "collaborative-filtering",
            "cold-start",
            "semantic tagging",
            "neural network",
            "Million Song Dataset",
            "user listening data",
            "prior",
            "performance",
            "recommendation"
        ]
    },
    {
        "title": "How Music Alters Decision Making - Impact of Music Stimuli on Emotional Classification.",
        "author": [
            "Elad Liebman",
            "Peter Stone",
            "Corey N. White"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414908",
        "url": "https://doi.org/10.5281/zenodo.1414908",
        "ee": "https://zenodo.org/records/1414908/files/LiebmanSW15.pdf",
        "abstract": "Numerous studies have demonstrated that mood can af- fect emotional processing. The goal of this study was to explore which components of the decision process are af- fected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, partici- pants decided whether words were emotionally positive or negative while listening to music that was chosen to in- duce positive or negative mood. The behavioral results show that the music manipulation was effective, as par- ticipants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumula- tion, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individ- ual aspects of music affect response patterns. Our results have implications for future studies of the connection be- tween music and mood.",
        "zenodo_id": 1414908,
        "dblp_key": "conf/ismir/LiebmanSW15",
        "keywords": [
            "mood",
            "emotional processing",
            "decision process",
            "stochastic sequential model",
            "drift-diffusion model",
            "positive or negative",
            "music manipulation",
            "bias",
            "starting point of evidence accumulation",
            "response patterns"
        ]
    },
    {
        "title": "Comparative Analysis of Orchestral Performance Recordings: An Image-Based Approach.",
        "author": [
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416248",
        "url": "https://doi.org/10.5281/zenodo.1416248",
        "ee": "https://zenodo.org/records/1416248/files/LiemH15.pdf",
        "abstract": "Traditionally, the computer-assisted comparison of mul- tiple performances of the same piece focused on perfor- mances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and artic- ulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While dif- ferences between interpretations in this genre may include commonly studied features on timing, dynamics and ar- ticulation, the timbre of the orchestra and choices of bal- ance within the ensemble are other important aspects dis- tinguishing different orchestral interpretations from one another. While it is hard to model these higher-level as- pects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Compo- nents Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pair- wise comparison, relative differences can be studied be- tween multiple performances in a corpus at once.",
        "zenodo_id": 1416248,
        "dblp_key": "conf/ismir/LiemH15",
        "keywords": [
            "computer-assisted comparison",
            "multiple performances",
            "same piece",
            "single instruments",
            "strong bias",
            "piano performances",
            "expressive performance features",
            "orchestra performances",
            "different orchestras",
            "different conductors"
        ]
    },
    {
        "title": "Detection of Common Mistakes in Novice Violin Playing.",
        "author": [
            "Yin-Jyun Luo",
            "Li Su 0004",
            "Yi-Hsuan Yang",
            "Tai-Shih Chi"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415968",
        "url": "https://doi.org/10.5281/zenodo.1415968",
        "ee": "https://zenodo.org/records/1415968/files/LuoSYC15.pdf",
        "abstract": "Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musi- cal instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset com- prising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are gener- ated from the same feature set with different scales, in- cluding two note-level representations and three segment- level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vec- tor machine for classification. Results show that the F- measures using different feature representations can vary up to 20% for two types of playing mistakes. It demon- strates the different sensitivities of each feature represen- tation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed.",
        "zenodo_id": 1415968,
        "dblp_key": "conf/ismir/LuoSYC15",
        "keywords": [
            "novice violin players",
            "four types of mistakes",
            "novel dataset",
            "violin experts",
            "Fisher score",
            "support vector machine",
            "legato notes",
            "different scales",
            "feature representations",
            "playing mistakes"
        ]
    },
    {
        "title": "Beat Histogram Features from NMF-Based Novelty Functions for Music Classification.",
        "author": [
            "Athanasios Lykartsis",
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418145",
        "url": "https://doi.org/10.5281/zenodo.1418145",
        "ee": "https://zenodo.org/records/1418145/files/LykartsisWL15.pdf",
        "abstract": "In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classification task. Musical excerpts are analyzed using an optimized, partially fixed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are eval- uated on two widely used genre datasets (GTZAN and Ball- room) using standard classification methods, concerning the achieved overall classification accuracy. Furthermore, their suitability in distinguishing between rhythmically sim- ilar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classification systems, while considering strictly drum patterns.",
        "zenodo_id": 1418145,
        "dblp_key": "conf/ismir/LykartsisWL15",
        "keywords": [
            "drum tracks",
            "polyphonic music",
            "genre classification",
            "Non-Negative Matrix Factorization",
            "beat histogram",
            "genre datasets",
            "GTZAN",
            "Ball-room",
            "rhythm features",
            "classification accuracy"
        ]
    },
    {
        "title": "Probabilistic Modular Bass Voice Leading in Melodic Harmonisation.",
        "author": [
            "Dimos Makris",
            "Maximos A. Kaliakatsos-Papakostas",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416374",
        "url": "https://doi.org/10.5281/zenodo.1416374",
        "ee": "https://zenodo.org/records/1416374/files/MakrisKC15.pdf",
        "abstract": "Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmoni- sation, since they capture statistical rules of the music id- ioms they are trained with. Proposed methodologies fo- cus either on specific aspects of harmony (e.g., generat- tion of many harmonic characteristics in a single proba- bilistic generative scheme. This paper addresses the prob- lem of assigning voice leading focussing on the bass voice, chord sequence, under the scope of a modular melodic har- monisation system where different aspects of the genera- tive process are arranged by different modules. The pro- posed technique defines the motion of the bass voice ac- cording to several statistical aspects: melody voice con- tour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are mod- ular, i.e., each criterion is defined by independent statisti- cal learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The pro- posed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic har- monisations).",
        "zenodo_id": 1416374,
        "dblp_key": "conf/ismir/MakrisKC15",
        "keywords": [
            "probabilistic methodologies",
            "automated music composition",
            "melodic harmonization",
            "voice leading",
            "bass voice",
            "chord sequence",
            "modular melodic harmonisation system",
            "voice motion",
            "statistical aspects",
            "novel blended melodic harmonisations"
        ]
    },
    {
        "title": "Specter: Combining Music Information Retrieval with Sound Spatialization.",
        "author": [
            "Bill Z. Manaris",
            "Seth Stoudenmier"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415596",
        "url": "https://doi.org/10.5281/zenodo.1415596",
        "ee": "https://zenodo.org/records/1415596/files/ManarisS15.pdf",
        "abstract": "Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile en- vironment to experiment with sound spatialization for music composition and live performance. Through vari- ous interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information re- trieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real- time generation, manipulation, and storing of sound tra- jectory scores. Finally, through Glaser, a sound manipula- tion instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vec- tor Based Amplitude Panning.  Various interfaces are dis- cussed, including a Kinect-based sensor system, a Leap- Motion-based hand-tracking interface, and a smartphone- based OSC controller.  Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter\u2019s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware.",
        "zenodo_id": 1415596,
        "dblp_key": "conf/ismir/ManarisS15",
        "keywords": [
            "Ambisonics",
            "JythonMusic",
            "Iannix",
            "Glaser",
            "Ambisonics",
            "Vector Based Amplitude Panning",
            "Migrant",
            "Specter",
            "Specter",
            "Specter"
        ]
    },
    {
        "title": "New Sonorities for Early Jazz Recordings Using Sound Source Separation and Automatic Mixing Tools.",
        "author": [
            "Daniel Matz",
            "Estefan\u00eda Cano",
            "Jakob Abe\u00dfer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416568",
        "url": "https://doi.org/10.5281/zenodo.1416568",
        "ee": "https://zenodo.org/records/1416568/files/MatzCA15.pdf",
        "abstract": "In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a pre- processing step of the mixing process. In addition to an ini- tial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive mul- ti-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different pro- cessing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound bal- ance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework.",
        "zenodo_id": 1416568,
        "dblp_key": "conf/ismir/MatzCA15",
        "keywords": [
            "framework",
            "automatic mixing",
            "early jazz recordings",
            "sound source separation",
            "pre-processing step",
            "mixing process",
            "six processing blocks",
            "cross-adaptive multi-track scaling",
            "cross-adaptive equalizer",
            "cross-adaptive dynamic spectral panning"
        ]
    },
    {
        "title": "A Software Framework for Musical Data Augmentation.",
        "author": [
            "Brian McFee",
            "Eric J. Humphrey",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1418365",
        "url": "https://doi.org/10.5281/zenodo.1418365",
        "ee": "https://zenodo.org/records/1418365/files/McFeeHB15.pdf",
        "abstract": "Predictive models for music annotation tasks are practi- cally limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of \u201cdata augmentation\u201d \u2014 supplementing a train- ing set with carefully perturbed samples \u2014 has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated per- turbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals.",
        "zenodo_id": 1418365,
        "dblp_key": "conf/ismir/McFeeHB15",
        "keywords": [
            "Predictive models",
            "pauca well-annotated training data",
            "data augmentation",
            "large-scale machine learning",
            "concept of data augmentation",
            "important component",
            "general software framework",
            "musically motivated perturbations",
            "task of recognizing instruments",
            "mixed signals"
        ]
    },
    {
        "title": "Hierarchical Evaluation of Segment Boundary Detection.",
        "author": [
            "Brian McFee",
            "Oriol Nieto",
            "Juan Pablo Bello"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414866",
        "url": "https://doi.org/10.5281/zenodo.1414866",
        "ee": "https://zenodo.org/records/1414866/files/McFeeNB15.pdf",
        "abstract": "Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and refined down to the short melodic ideas at the motivic level. However, typ- ical algorithmic approaches to structural annotation pro- duce flat temporal partitions of a track, which are com- monly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierar- chical structure annotations have been recently published, no techniques yet exist to measure an algorithm\u2019s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detec- tion with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical an- notations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset.",
        "zenodo_id": 1414866,
        "dblp_key": "conf/ismir/McFeeNB15",
        "keywords": [
            "hierarchical",
            "structure",
            "analysis",
            "algorithmic",
            "approaches",
            "flat",
            "temporal",
            "partitions",
            "motivic",
            "level"
        ]
    },
    {
        "title": "Put the Concert Attendee in the Spotlight. A User-Centered Design and Development Approach for Classical Concert Applications.",
        "author": [
            "Mark S. Melenhorst",
            "Cynthia C. S. Liem"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416820",
        "url": "https://doi.org/10.5281/zenodo.1416820",
        "ee": "https://zenodo.org/records/1416820/files/MelenhorstL15.pdf",
        "abstract": "As the importance of real-life use cases in the music in- formation retrieval (MIR) field is increasing, so does the importance of understanding user needs. The develop- ment of innovative real-life applications that draw on MIR technology requires a user-centered design and de- velopment approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applica- tions to enrich classical symphonic concerts. A user- driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications.",
        "zenodo_id": 1416820,
        "dblp_key": "conf/ismir/MelenhorstL15",
        "keywords": [
            "real-life use cases",
            "music information retrieval",
            "user-centered design",
            "innovative real-life applications",
            "user needs",
            "MIR technology",
            "user-driven approach",
            "orchestras",
            "concert experience",
            "informed user requirements"
        ]
    },
    {
        "title": "Improving Score-Informed Source Separation for Classical Music through Note Refinement.",
        "author": [
            "Marius Miron",
            "Julio Jos\u00e9 Carabias-Orti",
            "Jordi Janer"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417689",
        "url": "https://doi.org/10.5281/zenodo.1417689",
        "ee": "https://zenodo.org/records/1417689/files/MironCJ15.pdf",
        "abstract": "Signal decomposition methods such as Non-negative Ma- trix Factorization (NMF) demonstrated to be a suitable ap- proach for music signal processing applications, including sound source separation. To better control this decompo- sition, NMF has been extended using prior knowledge and parametric models. In fact, using score information con- siderably improved separation results. Nevertheless, one of the main problems of using score information is the mis- alignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, on- set and offset for each note present in the score. Note re- finement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) ma- trix. Decomposition is performed in a supervised way, us- ing training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory re- sults, especially in terms of SIR.",
        "zenodo_id": 1417689,
        "dblp_key": "conf/ismir/MironCJ15",
        "keywords": [
            "Non-negative Matrix Factorization",
            "Sound source separation",
            "Alignment between score and performance",
            "Audio-to-score alignment systems",
            "Note-level refinement",
            "Instrument-wise time activation matrix",
            "Supervised decomposition",
            "Training instrument models",
            "Coarsely-aligned score information",
            "Satisfactory results"
        ]
    },
    {
        "title": "Autoregressive Hidden Semi-Markov Model of Symbolic Music Performance for Score Following.",
        "author": [
            "Eita Nakamura",
            "Philippe Cuvillier",
            "Arshia Cont",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417030",
        "url": "https://doi.org/10.5281/zenodo.1417030",
        "ee": "https://zenodo.org/records/1417030/files/NakamuraCCOS15.pdf",
        "abstract": "A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score fol- lowing. Stochastic modelling has been one of the most suc- cessful strategies in this field. We describe the performance as a hierarchical process of performer\u2019s progression in the score and the production of performed notes, and repre- sent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and rea- sons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by com- paring the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more effi- cient and retains the advantages of the former model. The present model yields one of the state-of-the-art score fol- lowing algorithms for symbolic performance and can pos- sibly be applicable for other music recognition problems.",
        "zenodo_id": 1417030,
        "dblp_key": "conf/ismir/NakamuraCCOS15",
        "keywords": [
            "stochastic model",
            "symbolic performance",
            "polyphonic scores",
            "score following",
            "hidden semi-Markov model",
            "hidden Markov model",
            "performance progression",
            "produced notes",
            "accuracy comparison",
            "errors analysis"
        ]
    },
    {
        "title": "Beat and Downbeat Tracking Based on Rhythmic Patterns Applied to the Uruguayan Candombe Drumming.",
        "author": [
            "Leonardo O. Nunes",
            "Mart\u00edn Rocamora",
            "Luis Jure",
            "Luiz W. P. Biscainho"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415728",
        "url": "https://doi.org/10.5281/zenodo.1415728",
        "ee": "https://zenodo.org/records/1415728/files/NunesRJB15.pdf",
        "abstract": "Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or poly- rhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper pro- poses a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of an- notated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper.",
        "zenodo_id": 1415728,
        "dblp_key": "conf/ismir/NunesRJB15",
        "keywords": [
            "Computational analysis",
            "rhythmic/metrical structure",
            "music information retrieval",
            "explicit modeling",
            "characteristic rhythmic patterns",
            "beat-tracking algorithms",
            "syncopated or poly-rhythmic music",
            "Uruguayan Candombe drumming",
            "afro-rooted rhythm",
            "basic rhythmic patterns"
        ]
    },
    {
        "title": "FlaBase: Towards the Creation of a Flamenco Music Knowledge Base.",
        "author": [
            "Sergio Oramas",
            "Francisco G\u00f3mez 0001",
            "Emilia G\u00f3mez",
            "Joaqu\u00edn Mora"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417183",
        "url": "https://doi.org/10.5281/zenodo.1417183",
        "ee": "https://zenodo.org/records/1417183/files/OramasGGM15.pdf",
        "abstract": "Online information about flamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to flamenco music is gath- ered from general knowledge bases (e.g., Wikipedia, DB- pedia), music encyclopedias (e.g., MusicBrainz), and spe- cialized flamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identifiers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 pa- los (flamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The sys- tem can recognize and disambiguate FlaBase entity refer- ences in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a fla- menco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base.",
        "zenodo_id": 1417183,
        "dblp_key": "conf/ismir/OramasGGM15",
        "keywords": [
            "Online",
            "information",
            "flamenco",
            "music",
            "gathered",
            "general",
            "knowledge",
            "bases",
            "music",
            "encyclopedias"
        ]
    },
    {
        "title": "A Semantic-Based Approach for Artist Similarity.",
        "author": [
            "Sergio Oramas",
            "Mohamed Sordo",
            "Luis Espinosa Anke",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415976",
        "url": "https://doi.org/10.5281/zenodo.1415976",
        "ee": "https://zenodo.org/records/1415976/files/OramasSAS15.pdf",
        "abstract": "This paper describes and evaluates a method for comput- ing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting men- tions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge rep- resentation from these mentions in the form of a seman- tic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similar- ity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors.",
        "zenodo_id": 1415976,
        "dblp_key": "conf/ismir/OramasSAS15",
        "keywords": [
            "entity linking",
            "semantic information",
            "knowledge representation",
            "semantic graph",
            "vector-space model",
            "semantic similarity",
            "artist biographies",
            "Last.fm",
            "MIREX Audio and Music Similarity evaluation dataset",
            "Last.fm API"
        ]
    },
    {
        "title": "Combining Features for Cover Song Identification.",
        "author": [
            "Julien Osmalskyj",
            "Peter Foster",
            "Simon Dixon",
            "Jean-Jacques Embrechts"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417295",
        "url": "https://doi.org/10.5281/zenodo.1417295",
        "ee": "https://zenodo.org/records/1417295/files/OsmalskyjFDE15.pdf",
        "abstract": "In this paper, we evaluate a set of methods for combining features for cover song identification. We first create mul- tiple classifiers based on global tempo, duration, loudness, beats and chroma average features, training a random for- est for each feature. Subsequently, we evaluate standard combination rules for merging these single classifiers into a composite classifier based on global features. We further obtain two higher level classifiers based on chroma fea- tures: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classifiers with the composite classi- fier based on global features, we use standard rank aggre- gation methods adapted from the information retrieval lit- erature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multi- ple statistics. We observe that each combination rule out- performs single methods in terms of the total number of identified queries. Experiments with rank aggregation me- thods show an increase of up to 23.5 % of the number of identified queries, compared to single classifiers.",
        "zenodo_id": 1417295,
        "dblp_key": "conf/ismir/OsmalskyjFDE15",
        "keywords": [
            "evaluation",
            "cover song identification",
            "multiple classifiers",
            "random forest",
            "standard combination rules",
            "composite classifier",
            "global features",
            "chroma features",
            "cross-correlations",
            "rank aggregation methods"
        ]
    },
    {
        "title": "Improving MIDI Guitar&apos;s Accuracy with NMF and Neural Net.",
        "author": [
            "Masaki Otsuka",
            "Tetsuro Kitahara"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417531",
        "url": "https://doi.org/10.5281/zenodo.1417531",
        "ee": "https://zenodo.org/records/1417531/files/OtsukaK15.pdf",
        "abstract": "In this paper, we propose a method for improving the accu- racy of MIDI guitars. MIDI guitars are useful tools for var- ious purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have suf- ficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF\u2019s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for inte- grating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two out- puts. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively.",
        "zenodo_id": 1417531,
        "dblp_key": "conf/ismir/OtsukaK15",
        "keywords": [
            "MIDI guitars",
            "accuracy improvement",
            "audio transcription",
            "non-negative matrix factorization (NMF)",
            "neural network",
            "F-score",
            "policy of outputting common parts",
            "experimental results",
            "various purposes",
            "jam session system"
        ]
    },
    {
        "title": "Improving Optical Music Recognition by Combining Outputs from Multiple Sources.",
        "author": [
            "V\u00edctor Padilla Mart\u00edn-Caro",
            "Alex McLean",
            "Alan Marsden",
            "Kia Ng"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416258",
        "url": "https://doi.org/10.5281/zenodo.1416258",
        "ee": "https://zenodo.org/records/1416258/files/PadillaMMN15.pdf",
        "abstract": "Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commer- cial OMR programs when applied to images of different scores of the same piece of music. As a result of this pro- cedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate move- ments and sections and removes ossia staves which con- fuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrat- ing on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%.",
        "zenodo_id": 1416258,
        "dblp_key": "conf/ismir/PadillaMMN15",
        "keywords": [
            "image pre-processing",
            "commercial OMR programs",
            "combined output",
            "around 50% fewer errors",
            "split scores",
            "remove ossia staves",
            "align outputs",
            "reject outputs",
            "majority voting",
            "MusicXML format"
        ]
    },
    {
        "title": "A Toolkit for Live Annotation of Opera Performance: Experiences Capturing Wagner&apos;s Ring Cycle.",
        "author": [
            "Kevin R. Page",
            "Terhi Nurmikko-Fuller",
            "Carolin Rindfleisch",
            "David M. Weigl",
            "Richard Lewis 0001",
            "Laurence Dreyfus",
            "David De Roure"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415582",
        "url": "https://doi.org/10.5281/zenodo.1415582",
        "ee": "https://zenodo.org/records/1415582/files/PageNRWLDR15.pdf",
        "abstract": "Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists\u2019 study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufficiently capture the ephem- eral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musi- cologist throughout a complete multi-day production of Richard Wagner\u2019s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the perfor- mance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web tech- nologies, and of the practical challenges and compromises of fieldwork within a working theatre. Finally we con- sider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered.",
        "zenodo_id": 1415582,
        "dblp_key": "conf/ismir/PageNRWLDR15",
        "keywords": [
            "multimedia material",
            "musicologists study",
            "computational methods",
            "ephemeral phenomena",
            "stagings",
            "toolkit development",
            "tablet-based score interface",
            "scenic setting",
            "real-time use",
            "semantic web technologies"
        ]
    },
    {
        "title": "Harmonic-Percussive Source Separation Using Harmonicity and Sparsity Constraints.",
        "author": [
            "Jeongsoo Park 0001",
            "Kyogu Lee"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417323",
        "url": "https://doi.org/10.5281/zenodo.1417323",
        "ee": "https://zenodo.org/records/1417323/files/ParkL15.pdf",
        "abstract": "In this paper, we propose a novel approach to harmonic- percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying har- monic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observa- tion that the spectral distributions of harmonic and percus- sive signals differ \u2013 i.e., harmonic components have har- monic and sparse structure while percussive components are broadband \u2013 we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the pro- posed method outperforms the conventional methods.",
        "zenodo_id": 1417323,
        "dblp_key": "conf/ismir/ParkL15",
        "keywords": [
            "harmonic-percussive sound separation",
            "Non-negative Matrix Factorization",
            "sparsity",
            "harmonicity constraints",
            "conventional HPSS methods",
            "temporal continuity of harmonic components",
            "spectral continuity of percussive components",
            "time-varying harmonic signals",
            "percussive ones",
            "imposing different constraints"
        ]
    },
    {
        "title": "Song2Quartet: A System for Generating String Quartet Cover Songs from Polyphonic Audio of Popular Music.",
        "author": [
            "Graham Percival",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415014",
        "url": "https://doi.org/10.5281/zenodo.1415014",
        "ee": "https://zenodo.org/records/1415014/files/PercivalFG15.pdf",
        "abstract": "We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilis- tic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Pre- vious work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time- frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet pro- duces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet mu- sic while retaining some partial phrases and chord voicings from the target audio.",
        "zenodo_id": 1415014,
        "dblp_key": "conf/ismir/PercivalFG15",
        "keywords": [
            "Song2Quartet",
            "system for generating string quartet versions",
            "popular songs",
            "combining probabilistic models",
            "corpus of symbolic classical music",
            "target audio file",
            "add novelty to listening experience",
            "familiarity with string quartets",
            "audio features of the target song",
            "previously work in automatic arrangement"
        ]
    },
    {
        "title": "AcousticBrainz: A Community Platform for Gathering Music Information Obtained from Audio.",
        "author": [
            "Alastair Porter",
            "Dmitry Bogdanov",
            "Robert Kaye",
            "Roman Tsukanov",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414938",
        "url": "https://doi.org/10.5281/zenodo.1414938",
        "ee": "https://zenodo.org/records/1414938/files/PorterBKTS15.pdf",
        "abstract": "We introduce the AcousticBrainz project, an open plat- form for gathering music information. At its core, Acous- ticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Mu- sic Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis re- sults to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to var- ious sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classi- fiers aimed at adding musically relevant semantic informa- tion. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be ac- cessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this commu- nity that will define the actual uses and applications of its data.",
        "zenodo_id": 1414938,
        "dblp_key": "conf/ismir/PorterBKTS15",
        "keywords": [
            "AcousticBrainz",
            "open platform",
            "music information retrieval",
            "audio recordings",
            "MusicBrainz identifier",
            "MusicBrainz",
            "feature extractor",
            "Essentia audio analysis library",
            "classifiers",
            "community contribution"
        ]
    },
    {
        "title": "Modeling Genre with the Music Genome Project: Comparing Human-Labeled Attributes and Audio Features.",
        "author": [
            "Matthew Prockup",
            "Andreas F. Ehmann",
            "Fabien Gouyon",
            "Erik M. Schmidt",
            "\u00d2scar Celma",
            "Youngmoo E. Kim"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417523",
        "url": "https://doi.org/10.5281/zenodo.1417523",
        "ee": "https://zenodo.org/records/1417523/files/ProckupEGSCK15.pdf",
        "abstract": "Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly defined or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large ex- tent be objectively modeled via a combination of musi- cal attributes. We employ a data-driven approach utiliz- ing a subset of 48 hand-labeled musical attributes com- prising instrumentation, timbre, and rhythm across more than one million examples from Pandorar Internet Ra- dio\u2019s Music Genome Projectr. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven mod- els derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that com- bine audio features and learned musical attributes perform better than those derived from audio features alone.",
        "zenodo_id": 1417523,
        "dblp_key": "conf/ismir/ProckupEGSCK15",
        "keywords": [
            "music",
            "genres",
            "objectively",
            "modeling",
            "musical",
            "attributes",
            "data-driven",
            "Pandora",
            "Internet",
            "Radio"
        ]
    },
    {
        "title": "Large-Scale Content-Based Matching of MIDI and Audio Files.",
        "author": [
            "Colin Raffel",
            "Daniel P. W. Ellis"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417371",
        "url": "https://doi.org/10.5281/zenodo.1417371",
        "ee": "https://zenodo.org/records/1417371/files/RaffelE15.pdf",
        "abstract": "MIDI files, when paired with corresponding audio record- ings, can be used as ground truth for many music infor- mation retrieval tasks. We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content based solely on content, i.e., with- out using any metadata. The core of our approach is a con- volutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset.",
        "zenodo_id": 1417371,
        "dblp_key": "conf/ismir/RaffelE15",
        "keywords": [
            "MIDI files",
            "audio recordings",
            "ground truth",
            "music information retrieval",
            "corpus of audio content",
            "efficient matching",
            "audio content",
            "large-scale dynamic time warping",
            "corpus of MIDI files",
            "Million Song Dataset"
        ]
    },
    {
        "title": "Comparison of the Singing Style of Two Jingju Schools.",
        "author": [
            "Rafael Caro Repetto",
            "Rong Gong",
            "Nadine Kroher",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416692",
        "url": "https://doi.org/10.5281/zenodo.1416692",
        "ee": "https://zenodo.org/records/1416692/files/RepettoGKS15.pdf",
        "abstract": "Performing schools (liupai) in jingju (also known as Pe- king or Beijing opera) are one of the most important ele- ments for the appreciation of this genre among connois- seurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most re- nowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musico- logical literature, we have selected those that can be stud- ied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the de- scriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology.",
        "zenodo_id": 1416692,
        "dblp_key": "conf/ismir/RepettoGKS15",
        "keywords": [
            "performing schools",
            "jingju",
            "musicological descriptions",
            "singing style",
            "dan role-type",
            "Mei school",
            "Cheng school",
            "MIR techniques",
            "audio features",
            "jingju music research corpus"
        ]
    },
    {
        "title": "Image Quality Estimation for Multi-Score OMR.",
        "author": [
            "Dan Ringwalt",
            "Roger B. Dannenberg"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414890",
        "url": "https://doi.org/10.5281/zenodo.1414890",
        "ee": "https://zenodo.org/records/1414890/files/RingwaltD15.pdf",
        "abstract": "Optical music recognition (OMR) is the recognition of im- ages of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve ac- curacy. As a simpler alternative, we have developed fea- tures which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Fur- thermore, quality may be used to weight each score in an alignment, which should improve existing systems\u2019 robust- ness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but significantly corre- lated with the true accuracy. Improved features should be able to produce highly consistent results.",
        "zenodo_id": 1414890,
        "dblp_key": "conf/ismir/RingwaltD15",
        "keywords": [
            "Optical music recognition",
            "aligning results",
            "multi-score OMR",
            "predicting score quality",
            "selecting highest-quality score",
            "weighting scores in alignment",
            "commercial OMR software",
            "test set of MIDI recordings",
            "correlated with true accuracy",
            "improved robustness"
        ]
    },
    {
        "title": "Melodic Similarity in Traditional French-Canadian Instrumental Dance Tunes.",
        "author": [
            "Laura Risk",
            "Lillio Mok",
            "Andrew Hankinson",
            "Julie Cumming"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414858",
        "url": "https://doi.org/10.5281/zenodo.1414858",
        "ee": "https://zenodo.org/records/1414858/files/RiskMHC15.pdf",
        "abstract": "Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits.",
        "zenodo_id": 1414858,
        "dblp_key": "conf/ismir/RiskMHC15",
        "keywords": [
            "French-Canadian",
            "instrumental dance tunes",
            "varied and complex",
            "aural tradition",
            "few notated sources",
            "practitioners",
            "combination of tune segments",
            "personalized settings",
            "extreme amount of variation",
            "same tune or tune segment"
        ]
    },
    {
        "title": "Selective Acquisition Techniques for Enculturation-Based Melodic Phrase Segmentation.",
        "author": [
            "Marcelo E. Rodr\u00edguez-L\u00f3pez",
            "Anja Volk"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415064",
        "url": "https://doi.org/10.5281/zenodo.1415064",
        "ee": "https://zenodo.org/records/1415064/files/Rodriguez-Lopez15.pdf",
        "abstract": "Automatic melody segmentation is an important yet un- solved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous lis- tening experience plays a considerable role in the percep- tion of melodic segment structure. At present automatic melody segmenters that model listening experience com- monly do so using unsupervised statistical learning with \u2018non-selective\u2019 information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory. In this paper we investigate techniques for \u2018selective\u2019 information acquisition, i.e. our learning model uses a goal- oriented approach to select what to store in memory. We test the usefulness of the segmentations produced using se- lective acquisition learning in a melody classification ex- periment involving melodies of different cultures. Our re- sults show that the segments produced by our selective learner segmenters substantially improve classification ac- curacy when compared to segments produced by a non- selective learner segmenter, two local segmentation meth- ods, and two na\u00a8\u0131ve baselines.",
        "zenodo_id": 1415064,
        "dblp_key": "conf/ismir/Rodriguez-Lopez15",
        "keywords": [
            "Automatic melody segmentation",
            "Music Information Retrieval",
            "Listening experience",
            "Melodic segment structure",
            "Unsupervised statistical learning",
            "Non-selective information acquisition",
            "Goal-oriented approach",
            "Selective information acquisition",
            "Melody classification experiment",
            "Segments produced by selective learner"
        ]
    },
    {
        "title": "Automatic Tune Family Identification by Musical Sequence Alignment.",
        "author": [
            "Patrick E. Savage",
            "Quentin D. Atkinson"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417135",
        "url": "https://doi.org/10.5281/zenodo.1417135",
        "ee": "https://zenodo.org/records/1417135/files/SavageA15.pdf",
        "abstract": "Musics, like languages and genes, evolve through a pro- cess of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been ham- pered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein fam- ilies. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combina- tions of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combina- tion that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our ap- proach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale.",
        "zenodo_id": 1417135,
        "dblp_key": "conf/ismir/SavageA15",
        "keywords": [
            "Musical evolution",
            "Transmission",
            "Variation",
            "Selection",
            "Tune families",
            "Protein sequence alignment",
            "Automatic method",
            "Ground-truth dataset",
            "Expert musicologists",
            "Cross-cultural"
        ]
    },
    {
        "title": "Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.",
        "author": [
            "Jan Schl\u00fcter",
            "Thomas Grill"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417745",
        "url": "https://doi.org/10.5281/zenodo.1417745",
        "ee": "https://zenodo.org/records/1417745/files/SchluterG15.pdf",
        "abstract": "In computer vision, state-of-the-art object recognition sys- tems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically ex- plored for music signals. Using the problem of singing voice detection with neural networks as an example, we ap- ply a range of label-preserving audio transformations to as- sess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shift- ing to be the most helpful augmentation method. Com- bined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public data- sets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval.",
        "zenodo_id": 1417745,
        "dblp_key": "conf/ismir/SchluterG15",
        "keywords": [
            "label-preserving image transformations",
            "augmenting training datasets",
            "learn invariances",
            "improve generalization",
            "singing voice detection",
            "neural networks",
            "audio transformations",
            "pitch shifting",
            "time stretching",
            "random frequency filtering"
        ]
    },
    {
        "title": "Automatic Solf\u00e8ge Assessment.",
        "author": [
            "Rodrigo Schramm",
            "Helena de Souza Nunes",
            "Cl\u00e1udio Rosito Jung"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414720",
        "url": "https://doi.org/10.5281/zenodo.1414720",
        "ee": "https://zenodo.org/records/1414720/files/SchrammNJ15.pdf",
        "abstract": "This paper presents a note-by-note approach for auto- matic solf\u00e8ge assessment. The proposed system uses me- lodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The tech- nique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individ- ual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student.",
        "zenodo_id": 1414720,
        "dblp_key": "conf/ismir/SchrammNJ15",
        "keywords": [
            "melodic transcription",
            "solf\u00e8ge assessment",
            "audio signal",
            "melodic segments",
            "Bayesian classifier",
            "ground truth",
            "pitch accuracy",
            "onset accuracy",
            "offset accuracy",
            "expert listeners"
        ]
    },
    {
        "title": "Improving Genre Annotations for the Million Song Dataset.",
        "author": [
            "Hendrik Schreiber 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1414760",
        "url": "https://doi.org/10.5281/zenodo.1414760",
        "ee": "https://zenodo.org/records/1414760/files/Schreiber15.pdf",
        "abstract": "Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this pur- pose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multi- ple attempts have been made to add song-level genre anno- tations, which are required for supervised machine learn- ing tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating ad- ditional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top- level genres. These are most often used in MGR systems. We then combine multiple datasets using majority vot- ing. This both promises a more reliable ground truth and allows the evaluation of the newly generated and pre- existing datasets. To facilitate further research, all derived genre annotations are publicly available on our website.",
        "zenodo_id": 1414760,
        "dblp_key": "conf/ismir/Schreiber15",
        "keywords": [
            "Automatic music genre recognition",
            "Ground truth dataset",
            "Public dataset",
            "Mislabeling",
            "Duplications",
            "Million Song Dataset",
            "Feature and metadata",
            "Crowd-sourced genre labels",
            "Taxonomies",
            "Majority voting"
        ]
    },
    {
        "title": "Schematizing the Treatment of Dissonance in 16th-Century Counterpoint.",
        "author": [
            "Andie Sigler",
            "Jon Wild",
            "Eliot Handelman"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417369",
        "url": "https://doi.org/10.5281/zenodo.1417369",
        "ee": "https://zenodo.org/records/1417369/files/SiglerWH15.pdf",
        "abstract": "We describe a computational project concerning labeling of dissonance treatments \u2013 schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large cor- pus of 16th-century polyphonic music. We then apply struc- tural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possi- bilities.",
        "zenodo_id": 1417369,
        "dblp_key": "conf/ismir/SiglerWH15",
        "keywords": [
            "dissonance treatments",
            "schematic descriptions",
            "automatic score annotation",
            "database methods",
            "large corpus",
            "16th-century polyphonic music",
            "structural techniques",
            "coincidence of schemata",
            "extrapolate from found structures",
            "unused possibilities"
        ]
    },
    {
        "title": "Audio Chord Recognition with a Hybrid Recurrent Neural Network.",
        "author": [
            "Siddharth Sigtia",
            "Nicolas Boulanger-Lewandowski",
            "Simon Dixon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416594",
        "url": "https://doi.org/10.5281/zenodo.1416594",
        "ee": "https://zenodo.org/records/1416594/files/SigtiaBD15.pdf",
        "abstract": "In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language mod- els for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic sig- nal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a mod- ification to beam search using a hash table which yields im- proved results while reducing memory requirements by an order of magnitude, thus making the proposed model suit- able for real-time applications. We evaluate our model's performance on a dataset with publicly available annota- tions and demonstrate that the performance is comparable to existing state of the art approaches for chord recogni- tion.",
        "zenodo_id": 1416594,
        "dblp_key": "conf/ismir/SigtiaBD15",
        "keywords": [
            "audio chord estimation",
            "novel architecture",
            "recurrent neural network",
            "temporal dependencies",
            "feed forward deep neural networks",
            "discriminative features",
            "time-frequency representation",
            "beam search",
            "hash table",
            "real-time applications"
        ]
    },
    {
        "title": "Music Shapelets for Fast Cover Song Recognition.",
        "author": [
            "Diego Furtado Silva",
            "Vin\u00edcius M. A. de Souza",
            "Gustavo E. A. P. A. Batista"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416236",
        "url": "https://doi.org/10.5281/zenodo.1416236",
        "ee": "https://zenodo.org/records/1416236/files/SilvaSB15.pdf",
        "abstract": "A cover song is a new performance or recording of a pre- viously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new ver- sions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, tim- bre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve po- tential cover versions for a query recording. In this pa- per, we adapt the idea of time series shapelets for content- based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best de- scribe each song. We demonstrate that we can use such small segments to identify cover songs with higher identi- fication rates and more than one order of magnitude faster than methods that use features to describe the whole music.",
        "zenodo_id": 1416236,
        "dblp_key": "conf/ismir/SilvaSB15",
        "keywords": [
            "cover song",
            "automatic identification",
            "content-based music retrieval",
            "time series shapelets",
            "similarity search",
            "key differences",
            "tempo",
            "structure",
            "language",
            "identification rates"
        ]
    },
    {
        "title": "Analysis of the Evolution of Research Groups and Topics in the ISMIR Conference.",
        "author": [
            "Mohamed Sordo",
            "Mitsunori Ogihara",
            "Stefan Wuchty"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416354",
        "url": "https://doi.org/10.5281/zenodo.1416354",
        "ee": "https://zenodo.org/records/1416354/files/SordoOW15.pdf",
        "abstract": "We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we first investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of re- searchers, allowing us to investigate their evolution and topic dependence. Notably, we find that large groups last longer if they actively alter their membership. Further- more, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their mem- bership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage.",
        "zenodo_id": 1416354,
        "dblp_key": "conf/ismir/SordoOW15",
        "keywords": [
            "co-authorship network",
            "topological changes",
            "topics over time",
            "research groups",
            "evolution",
            "topic dependence",
            "active alteration",
            "membership",
            "research impact",
            "citation advantage"
        ]
    },
    {
        "title": "Particle Filters for Efficient Meter Tracking with Dynamic Bayesian Networks.",
        "author": [
            "Ajay Srinivasamurthy",
            "Andre Holzapfel",
            "Ali Taylan Cemgil",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416736",
        "url": "https://doi.org/10.5281/zenodo.1416736",
        "ee": "https://zenodo.org/records/1416736/files/Srinivasamurthy15.pdf",
        "abstract": "Recent approaches in meter tracking have successfully ap- plied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the appli- cation of exact inference is computationally demanding. More efficient approximate inference algorithms using par- ticle filters (PF) can be developed to overcome this limita- tion. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an exist- ing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Fil- ter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter track- ing accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We docu- ment that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music.",
        "zenodo_id": 1416736,
        "dblp_key": "conf/ismir/Srinivasamurthy15",
        "keywords": [
            "Bayesian models",
            "exact inference",
            "particle filters",
            "computational demands",
            "Carnatic music",
            "Ballroom dances",
            "meter tracking",
            "observation model",
            "corpora",
            "inference schemes"
        ]
    },
    {
        "title": "Towards Music Imagery Information Retrieval: Introducing the OpenMIIR Dataset of EEG Recordings from Music Perception and Imagination.",
        "author": [
            "Sebastian Stober",
            "Avital Sternin",
            "Adrian M. Owen",
            "Jessica A. Grahn"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416270",
        "url": "https://doi.org/10.5281/zenodo.1416270",
        "ee": "https://zenodo.org/records/1416270/files/StoberSOG15.pdf",
        "abstract": "Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments \u2013 each 7\u201316s long \u2013 taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data.",
        "zenodo_id": 1416270,
        "dblp_key": "conf/ismir/StoberSOG15",
        "keywords": [
            "music imagery information retrieval",
            "electroencephalography",
            "EEG recordings",
            "music perception",
            "music imagination",
            "public domain dataset",
            "music perception and imagination",
            "music fragments",
            "music genres",
            "musical dimensions"
        ]
    },
    {
        "title": "Temporal Music Context Identification with User Listening Data.",
        "author": [
            "Cameron Summers",
            "Phillip Popp"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415938",
        "url": "https://doi.org/10.5281/zenodo.1415938",
        "ee": "https://zenodo.org/records/1415938/files/SummersP15.pdf",
        "abstract": "The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly im- portant. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, man- ually labeling track-temporal context associations is diffi- cult, time consuming, and costly. This paper examines track-day contexts with the pur- pose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known hol- idays, Halloween, Saint Patrick\u2019s Day, and July 4th, preci- sion@50 was 95%, 99%, and 73%, respectively.",
        "zenodo_id": 1415938,
        "dblp_key": "conf/ismir/SummersP15",
        "keywords": [
            "music",
            "context",
            "listener",
            "relationship",
            "temporal",
            "catalogues",
            "manually",
            "labeling",
            "discovery",
            "automated"
        ]
    },
    {
        "title": "Relating Natural Language Text to Musical Passages.",
        "author": [
            "Richard F. E. Sutcliffe",
            "Tim Crawford",
            "Chris Fox",
            "Deane L. Root",
            "Eduard H. Hovy",
            "Richard Lewis 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415270",
        "url": "https://doi.org/10.5281/zenodo.1415270",
        "ee": "https://zenodo.org/records/1415270/files/SutcliffeCFRHL15.pdf",
        "abstract": "There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long- term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made.",
        "zenodo_id": 1415270,
        "dblp_key": "conf/ismir/SutcliffeCFRHL15",
        "keywords": [
            "musicological literature",
            "musical passages",
            "MusicXML score",
            "scholarly texts",
            "shared evaluation",
            "passages identification",
            "MusicXML",
            "Natural language phrases",
            "passages reference",
            "Musicological analysis"
        ]
    },
    {
        "title": "A Hierarchical Bayesian Framework for Score-Informed Source Separation of Piano Music Signals.",
        "author": [
            "Wai Man Szeto",
            "Kin Hong Wong"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417061",
        "url": "https://doi.org/10.5281/zenodo.1417061",
        "ee": "https://zenodo.org/records/1417061/files/SzetoW15.pdf",
        "abstract": "Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone sig- nals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achiev- ing significantly better quality than those reported in related work done previously.",
        "zenodo_id": 1417061,
        "dblp_key": "conf/ismir/SzetoW15",
        "keywords": [
            "score-informed",
            "monaural",
            "source separation",
            "piano tone",
            "sinusoidal modeling",
            "instrument-specific",
            "Bayesian framework",
            "overlapping partials",
            "quality separation",
            "related work"
        ]
    },
    {
        "title": "Cover Song Identification with Timbral Shape Sequences.",
        "author": [
            "Christopher J. Tralie",
            "Paul Bendich"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416824",
        "url": "https://doi.org/10.5281/zenodo.1416824",
        "ee": "https://zenodo.org/records/1416824/files/TralieB15.pdf",
        "abstract": "We introduce a novel low level feature for identifying cover songs which quantifies the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different ver- sions of the same song, these point clouds are approxi- mately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the \u201cCovers 80\u201d dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features.",
        "zenodo_id": 1416824,
        "dblp_key": "conf/ismir/TralieB15",
        "keywords": [
            "low level feature",
            "cover songs",
            "smoothed frequency spectrum",
            "sliding window representation",
            "point cloud",
            "time-ordered point cloud",
            "MFCC embeddings",
            "relative shape sequence",
            "Covers 80 dataset",
            "note sequences"
        ]
    },
    {
        "title": "Design and Evaluation of a Probabilistic Music Projection Interface.",
        "author": [
            "Beatrix Vad",
            "Daniel Boland",
            "John Williamson 0001",
            "Roderick Murray-Smith",
            "Peter Berg Steffensen"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416670",
        "url": "https://doi.org/10.5281/zenodo.1416670",
        "ee": "https://zenodo.org/records/1416670/files/VadBWMS15.pdf",
        "abstract": "We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist gener- ation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34- dimensional feature space. We use a nonlinear dimen- sionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilis- tic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users\u2019 homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist gener- ation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of sub- jective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the prob- abilistic highlighting mode.",
        "zenodo_id": 1416670,
        "dblp_key": "conf/ismir/VadBWMS15",
        "keywords": [
            "probabilistic interface",
            "music exploration",
            "casual playlist generation",
            "low-level audio features",
            "34-dimensional feature space",
            "nonlinear dimensionality reduction",
            "music maps",
            "visualizations of probabilistic mappings",
            "user feedback",
            "mouse activity logs"
        ]
    },
    {
        "title": "Improving Music Recommendations with a Weighted Factorization of the Tagging Activity.",
        "author": [
            "Andreu Vall",
            "Marcin Skowron",
            "Peter Knees",
            "Markus Schedl"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416802",
        "url": "https://doi.org/10.5281/zenodo.1416802",
        "ee": "https://zenodo.org/records/1416802/files/VallSKS15.pdf",
        "abstract": "Collaborative filtering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix fac- torization techniques that fuses the implicit feedback de- rived from the users\u2019 listening activity with the tags that users have given to musical items. In contrast to exist- ing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging ac- tivity. We evaluate the model for the task of artist recom- mendation, using the expected percentile rank as metric, extended with confidence intervals to enable the compar- ison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different rec- ommender systems.",
        "zenodo_id": 1416802,
        "dblp_key": "conf/ismir/VallSKS15",
        "keywords": [
            "Collaborative filtering",
            "implicit feedback",
            "music recommendations",
            "matrix factorization",
            "weighted factorization",
            "tags",
            "artist recommendation",
            "evaluation methodology",
            "confidence intervals",
            "twofold contribution"
        ]
    },
    {
        "title": "Evaluation of Album Effect for Feature Selection in Music Genre Recognition.",
        "author": [
            "Igor Vatolkin",
            "G\u00fcnter Rudolph",
            "Claus Weihs"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416328",
        "url": "https://doi.org/10.5281/zenodo.1416328",
        "ee": "https://zenodo.org/records/1416328/files/VatolkinRW15.pdf",
        "abstract": "With an increasing number of available music characteris- tics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another ad- vantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to \u201cover- optimisation\u201d when data in the optimisation set is too dif- ferent from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called \u201calbum effect\u201d meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of gen- res. For that case we examine the performance of classifi- cation models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classifica- tion performance on the second set decreases. Neverthe- less, in almost all cases the feature selection remains ben- eficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance.",
        "zenodo_id": 1416328,
        "dblp_key": "conf/ismir/VatolkinRW15",
        "keywords": [
            "feature selection",
            "music genre recognition",
            "optimisation",
            "album effect",
            "classification performance",
            "ensemble of classifiers",
            "robust generalisation",
            "MFCCs",
            "relevant properties",
            "redundant ones"
        ]
    },
    {
        "title": "Evaluating Conflict Management Mechanisms for Online Social Jukeboxes.",
        "author": [
            "Felipe Vieira",
            "Nazareno Andrade"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416232",
        "url": "https://doi.org/10.5281/zenodo.1416232",
        "ee": "https://zenodo.org/records/1416232/files/VieiraA15.pdf",
        "abstract": "Social music listening is a prevalent and often fruitful ex- perience. Social jukeboxes are systems that enable so- cial music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conflict- ing interests. Because of that, virtually all social juke- boxes incorporate conflict management mechanisms. In contrast with their widespread use, however, little atten- tion has been given to evaluating how different conflict management mechanisms function to preserve the positive experience of music listeners. This paper presents an ex- periment with three conflict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners\u2019 satisfaction and to exam- ine their impressions and views on conflict, conflict man- agement mechanisms, and social jukeboxing.",
        "zenodo_id": 1416232,
        "dblp_key": "conf/ismir/VieiraA15",
        "keywords": [
            "Social music listening",
            "social jukeboxes",
            "collaborative music selection",
            "diverse music tastes",
            "conflict management",
            "positive experience",
            "mixed-methods approach",
            "listeners satisfaction",
            "impressions and views",
            "conflict management mechanisms"
        ]
    },
    {
        "title": "Music Pattern Discovery with Variable Markov Oracle: A Unified Approach to Symbolic and Audio Representations.",
        "author": [
            "Cheng-i Wang",
            "Jennifer Hsu",
            "Shlomo Dubnov"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416480",
        "url": "https://doi.org/10.5281/zenodo.1416480",
        "ee": "https://zenodo.org/records/1416480/files/WangHD15.pdf",
        "abstract": "This paper presents a framework for automatically discov- ering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and au- dio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Vari- able Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an ap- propriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance.",
        "zenodo_id": 1416480,
        "dblp_key": "conf/ismir/WangHD15",
        "keywords": [
            "framework",
            "polyphonic music piece",
            "automatic discovery",
            "symbolic and audio representations",
            "heuristics",
            "musical knowledge",
            "pattern discovery framework",
            "Variable Markov Oracle",
            "pattern-finding algorithm",
            "state of the art performance"
        ]
    },
    {
        "title": "On the Impact of Key Detection Performance for Identifying Classical Music Styles.",
        "author": [
            "Christof Wei\u00df",
            "Maximilian Schaab"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416246",
        "url": "https://doi.org/10.5281/zenodo.1416246",
        "ee": "https://zenodo.org/records/1416246/files/WeissS15.pdf",
        "abstract": "We study the automatic identification of Western classi- cal music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece\u2019s global key for estimating key-related pitch classes. First, we present four automatic key detec- tion systems. We compare their performance on suitable datasets of classical music and optimize the algorithms\u2019 free parameters. Using a second dataset, we evaluate au- tomatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calcu- late global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individ- ual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for clas- sification. Furthermore, we evaluate the impact of key de- tection performance on the classification results. Addition- ally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms.",
        "zenodo_id": 1416246,
        "dblp_key": "conf/ismir/WeissS15",
        "keywords": [
            "chroma histograms",
            "global key",
            "pitch classes",
            "style periods",
            "Baroque",
            "Classical",
            "Romantic",
            "Modern",
            "automatic key detection",
            "classification features"
        ]
    },
    {
        "title": "Comparative Music Similarity Modelling Using Transfer Learning Across User Groups.",
        "author": [
            "Daniel Wolff",
            "Andrew MacFarlane 0001",
            "Tillman Weyde"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417835",
        "url": "https://doi.org/10.5281/zenodo.1417835",
        "ee": "https://zenodo.org/records/1417835/files/WolffMW15.pdf",
        "abstract": "We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-The- oretic Metric Learning (RITML) algorithm adapts a Maha- lanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use in- formation from larger datasets to build better models for more specific datasets, such as user groups from differ- ent cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are rele- vant for the specific models when compared to the general user data. We to this end introduce the new CASimIR dataset, the first openly available relative similarity dataset with user attributes. With two age-related subsets, we show that trans- fer learning with RITML leads to better age-specific mod- els. RITML here improves learning on small datasets. Us- ing the larger MagnaTagATune dataset, we show that RITML performs as well as state-of-the-art algorithms in terms of general similarity estimation.",
        "zenodo_id": 1417835,
        "dblp_key": "conf/ismir/WolffMW15",
        "keywords": [
            "transfer learning",
            "music similarity models",
            "relative user data",
            "Maha-lobian distance",
            "ITML algorithm",
            "information-theoretic metric learning",
            "template model",
            "regularization",
            "CASimIR dataset",
            "age-related subsets"
        ]
    },
    {
        "title": "Drum Transcription Using Partially Fixed Non-Negative Matrix Factorization with Template Adaptation.",
        "author": [
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417839",
        "url": "https://doi.org/10.5281/zenodo.1417839",
        "ee": "https://zenodo.org/records/1417839/files/WuL15.pdf",
        "abstract": "In this paper, a template adaptive drum transcription algo- rithm using partially fixed Non-negative Matrix Factoriza- tion (NMF) is presented. The proposed method detects per- cussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undefined entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adap- tation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems.",
        "zenodo_id": 1417839,
        "dblp_key": "conf/ismir/WuL15",
        "keywords": [
            "template adaptive",
            "drum transcription",
            "Non-negative Matrix Factorization",
            "percussive events",
            "complex mixtures",
            "minimal training set",
            "perceptrons",
            "harmonic dictionary",
            "standard NMF procedure",
            "percussive dictionary"
        ]
    },
    {
        "title": "Spectral Learning for Expressive Interactive Ensemble Music Performance.",
        "author": [
            "Guangyu Xia",
            "Yun Wang",
            "Roger B. Dannenberg",
            "Geoffrey Gordon"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1415806",
        "url": "https://doi.org/10.5281/zenodo.1415806",
        "ee": "https://zenodo.org/records/1415806/files/XiaWDG15.pdf",
        "abstract": "We apply machine learning to a database of recorded en- semble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers\u2019 musical expression as co- evolving time series and learn their interactive relation- ship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspond- ence not only between different performers but also be- tween the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to gener- ate a more human-like interaction.",
        "zenodo_id": 1415806,
        "dblp_key": "conf/ismir/XiaWDG15",
        "keywords": [
            "machine learning",
            "database",
            "ensemble performances",
            "artificial performer",
            "expressive timing",
            "dynamics",
            "piano duet",
            "interaction",
            "musical expression",
            "co-evolving time series"
        ]
    },
    {
        "title": "Infinite Superimposed Discrete All-Pole Modeling for Multipitch Analysis of Wavelet Spectrograms.",
        "author": [
            "Kazuyoshi Yoshii",
            "Katsutoshi Itoyama",
            "Masataka Goto"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1417575",
        "url": "https://doi.org/10.5281/zenodo.1417575",
        "ee": "https://zenodo.org/records/1417575/files/YoshiiIG15.pdf",
        "abstract": "This paper presents a statistical multipich analyzer based on a source-filter model that decomposes a target music audio signal in terms of three major kinds of sound quan- tities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral enve- lope of an isolated sound is represented by an all-pole filter, linear predictive coding (LPC) can be used for filter esti- mation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral enve- lope, the whole spectrum is used for filter estimation. To solve this problem, we propose an infinite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed har- monic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparamet- ric Bayesian source-filter model is formulated in the log- frequency domain that better suits the frequency character- istics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain.",
        "zenodo_id": 1417575,
        "dblp_key": "conf/ismir/YoshiiIG15",
        "keywords": [
            "pitch",
            "timbre",
            "intensity",
            "source-filter model",
            "spectral envelope",
            "all-pole filter",
            "linear predictive coding",
            "infinite superimposed discrete all-pole",
            "nonparametric Bayesian",
            "log-frequency domain"
        ]
    },
    {
        "title": "Conceptual Blending in Music Cadences: A Formal Model and Subjective Evaluation.",
        "author": [
            "Asterios I. Zacharakis",
            "Maximos A. Kaliakatsos-Papakostas",
            "Emilios Cambouropoulos"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416056",
        "url": "https://doi.org/10.5281/zenodo.1416056",
        "ee": "https://zenodo.org/records/1416056/files/ZacharakisKC15.pdf",
        "abstract": "Conceptual blending is a cognitive theory whereby ele- ments from diverse, but structurally-related, mental spaces are \u2018blended\u2019 giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorith- mic formalisation for conceptual blending applied to har- monic concepts. More specifically, it investigates the abil- ity of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fun- damental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected re- lationships between the produced blends. Then, a prelimi- nary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spa- tial configurations for both behavioural data and dissimi- larity estimations by the algorithm. Comparison of the two configurations showed that the system is capable of mak- ing fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaning- ful blends based on self-evaluation of its outcome.",
        "zenodo_id": 1416056,
        "dblp_key": "conf/ismir/ZacharakisKC15",
        "keywords": [
            "conceptual blending",
            "cognitive theory",
            "elements blending",
            "mental spaces",
            "harmonic concepts",
            "algorithmic formalisation",
            "harmonic cadences",
            "subjective evaluation",
            "pairwise dissimilarity listening test",
            "multidimensional scaling analysis"
        ]
    },
    {
        "title": "Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing.",
        "author": [
            "Shuo Zhang",
            "Rafael Caro Repetto",
            "Xavier Serra"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "http://ismir2015.uma.es/articles/282_Paper.pdf",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/ZhangRS15"
    },
    {
        "title": "Chord Detection Using Deep Learning.",
        "author": [
            "Xinquan Zhou",
            "Alexander Lerch 0001"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1416968",
        "url": "https://doi.org/10.5281/zenodo.1416968",
        "ee": "https://zenodo.org/records/1416968/files/ZhouL15.pdf",
        "abstract": "In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and configurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classification.",
        "zenodo_id": 1416968,
        "dblp_key": "conf/ismir/ZhouL15",
        "keywords": [
            "deep learning",
            "audio chord detection",
            "high-level features",
            "bottleneck architecture",
            "promising results",
            "state-of-the-art systems",
            "input pre-processing",
            "bottleneck architecture",
            "SVMs vs. HMMs",
            "chord classification"
        ]
    },
    {
        "title": "Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015, M\u00e1laga, Spain, October 26-30, 2015",
        "author": [
            "Meinard M\u00fcller",
            "Frans Wiering"
        ],
        "year": "2015",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": null,
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2015"
    }
]